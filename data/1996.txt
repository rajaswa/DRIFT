paper present approach automatic acquisition linguistic knowledge unstructured data acquire knowledge represent lexical knowledge representation language datr set transformation rule establish inheritance relationships default inference algorithm make basis components system since overall approach restrict special domain heuristic inference strategy use criteria evaluate quality datr theory different domains may require different criteria system apply linguistic learn task german noun inflection
present metagrammatical formalism generic rule give default interpretation grammar rule formalism introduce process dynamic bind interfacing level pure grammatical knowledge representation parse level present approach non constituent coordination within categorial grammars reformulate generic rule reformulation context free parsable reduce drastically search space associate parse task phenomena
interim report discuss possible guidelines assessment evaluation project develop speech language systems prepare request european commission dg xiii ad hoc study group make available form submit commission however report official european commission document reflect european commission policy official otherwise discussion terminology report focus combine user centre technology centre assessment meaningful comparisons make variety systems perform different task different domains report outline kind infra structure might require support comparative assessment evaluation heterogenous project also result questionnaire concern different approach evaluation
paper propose method measure semantic similarity word new tool text analysis similarity measure semantic network construct systematically subset english dictionary ldoce longman dictionary contemporary english spread activation network directly compute similarity two word longman define vocabulary indirectly similarity word ldoce similarity represent strength lexical cohesion semantic relation also provide valuable information similarity coherence texts
paper propose new indicator text structure call lexical cohesion profile lcp locate segment boundaries text text segment coherent scene word segment link together via lexical cohesion relations lcp record mutual similarity word sequence text similarity word represent cohesiveness compute use semantic network comparison text segment mark number subject show lcp closely correlate human judgments lcp may provide valuable information resolve anaphora ellipsis
possessive pronouns use determiners english equivalent would use japanese sentence mean paper propose heuristic method generate possessive pronouns even equivalent japanese method use information use possessive pronouns english treat lexical property nouns addition contextual information noun phrase referentiality subject main verb sentence noun phrase appear propose method implement ntt communication science laboratories japanese english machine translation system alt j e test set six thousand, two hundred sentence propose method increase number noun phrase appropriate possessive pronouns generate two hundred and sixty-three six hundred and nine cost generate eighty-three noun phrase inappropriate possessive pronouns
paper propose computationally feasible method measure context sensitive semantic distance word distance compute adaptive scale semantic space semantic space word vocabulary v represent multi dimensional vector obtain english dictionary principal component analysis give word set c specify context measure word distance dimension semantic space scale accord distribution c semantic space space thus transform distance word v become dependent context c evaluation word prediction task show propose measurement successfully extract context text
paper show necessity distinguish different referential use noun phrase machine translation argue differentiate generic referential ascriptive use noun phrase minimum necessary generate article number correctly translate japanese english heuristics determine differences propose japanese english machine translation system finally result use propose heuristics show raise percentage noun phrase generate correct use article number japanese english machine translation system alt j e sixty-five seventy-seven
report argue provision common software infrastructure nlp systems current trend language engineer research review motivation infrastructure relevant recent work discuss freely available system call gate describe build work
paper provide parse respect grammars express general tfs base formalism restriction ale motivation design abstract wam like machine formalism consider parse computational process use operational semantics guide design control structure abstract machine emphasize notion abstract type feature structure afss encode essential information tfss define unification afss rather tfss introduce explicit construct multi root feature structure mrss naturally extend tfss use represent phrasal sign well grammar rule also employ abstractions mrss give mathematical foundations need manipulate present simple bottom chart parser model computation grammars write tfs base formalism execute parser finally show parser correct
paper provide parse respect grammars express general tfs base formalism restriction ale motivation design abstract wam like machine formalism consider parse computational process use operational semantics guide design control structure abstract machine emphasize notion abstract type feature structure afss encode essential information tfss define unification afss rather tfss introduce explicit construct multi root feature structure mrss naturally extend tfss use represent phrasal sign well grammar rule also employ abstractions mrss give mathematical foundations need manipulate formally define grammars languages generate describe model computation correspond bottom chart parse grammars write tfs base formalism execute parser show computation correct respect independent definition finally discuss class grammars computations terminate prove termination guarantee line parsable grammars
natural language process nlp apply information retrieval ir filter problems may assign part speech tag term generally modify query document analytic model predict performance text filter system incorporate change suggest nlp allow us make precise statements average effect nlp operations ir provide model retrieval tag allow us compute performance change due syntactic parse allow us understand factor affect performance addition prediction performance tag upper lower bound retrieval performance derive give best worst effect include part speech tag empirical ground select set tag consider
serious think computational aspects situation theory start recent proposals direction viz prosit astl vary degrees divergence ontology theory believe program environment incorporate bona fide situation theoretic construct need describe recent baby sit implementation detail critical account prosit astl also offer order compare system pioneer influential frameworks
knowledge window style content location grammatical structure may use classify document originate within particular discipline may use place document theory versus practice spectrum distinction also study use type token ratio differentiate sublanguages statistical significance windows compute base presence term title abstract citations section headers well binary independent bi inverse document frequency idf weight characteristics windows study examine within window density wwd concentration sc concentration term various document field eg title abstract fulltext rate window occurrences begin end document fulltext differ academic field different syntactic structure sublanguages examine use consider discriminate specific academic discipline generally theory versus practice knowledge versus applications orient document
currently computational linguists cognitive scientists work area discourse dialogue argue subjective judgments reliable use several different statistics none easily interpretable comparable meanwhile researchers content analysis already experience difficulties come solution kappa statistic discuss wrong reliability measure currently use discourse dialogue work computational linguistics cognitive science argue would better field adopt techniques content analysis
present general framework base weight finite automata weight finite state transducers describe implement speech recognizers framework allow us represent uniformly information source data structure use recognition include context dependent units pronunciation dictionaries language model lattices furthermore general efficient algorithms use combine information source actual recognizers optimize application particular single composition algorithm use combine advance information source language model dictionaries combine acoustic observations information source dynamically recognition
phrase structure grammars effective model important syntactic semantic aspects natural languages computationally demand use language model real time speech recognition therefore finite state model use instead even though lack expressive power reconcile two alternatives design algorithm compute finite state approximations context free grammars context free equivalent augment phrase structure grammars approximation exact certain context free grammars generate regular languages include leave linear right linear context free grammars algorithm use build finite state language model limit domain speech recognition task
attempto control english ace allow domain specialists interactively formulate requirements specifications domain concepts ace accurately efficiently process computer expressive enough allow natural usage attempto system translate specification texts ace discourse representation structure optionally prolog translate specification texts incrementally add knowledge base knowledge base query ace verification execute simulation prototyping validation specification
derive formal specifications informal requirements difficult since one take account disparate conceptual worlds application domain software development bridge conceptual gap propose control natural language textual view formal specifications logic specification language attempto control english ace subset natural language accurately efficiently process computer expressive enough allow natural usage attempto system translate specifications ace discourse representation structure prolog result knowledge base query ace verification execute simulation prototyping validation specification
new tightly couple speech natural language integration model present tdnn base continuous possibly large vocabulary speech recognition system korean unlike popular n best techniques develop integrate mainly hmm base speech recognition natural language process word level obviously inadequate morphologically complex agglutinative languages model construct speak language system base morpheme level speech language integration integration scheme speak korean process engine skope design implement use tdnn base diphone recognition module integrate viterbi base lexical decode symbolic phonological morphological co analysis experiment result show speaker dependent continuous eojeol korean word recognition integrate morphological analysis achieve eight hundred and six success rate directly speech input middle level vocabularies
usefulness statistical approach suggest church et al one thousand, nine hundred and ninety-one evaluate extraction verb noun v n collocations german text corpora problematic issue method arise properties german language discuss various modifications method consider might improve extraction result german precision recall variant methods evaluate v n collocations contain support verbs consequences work extraction collocations german corpora discuss sufficiently large corpus six mio word tokens average error rate wrong extractions reduce twenty-two nine hundred and seventy-eight precision restrictive method however loss data almost fifty compare less restrictive method still eight hundred and seventy-six precision depend goal achieve emphasis put high recall lexicographic purpose high precision automatic lexical acquisition case unfortunately lead decrease correspond variable low recall still acceptable large corpora ie fifty one hundred million word available corpora special domains use addition data find machine readable collocation dictionaries
paper present constraint base morphological disambiguation approach applicable languages complex morphology specifically agglutinative languages productive inflectional derivational morphological phenomena certain respect approach motivate brill recent work observation transformational approach directly applicable languages like turkish system combine corpus independent hand craft constraint rule constraint rule learn via unsupervised learn train corpus additional statistical information corpus morphologically disambiguate hand craft rule linguistically motivate tune improve precision without sacrifice recall unsupervised learn process produce two set rule choose rule choose morphological parse lexical item satisfy constraint effectively discard parse ii delete rule delete parse satisfy constraint approach also use novel approach unknown word process employ secondary morphological processor recover relevant inflectional derivational information lexical item whose root unknown approach well one percent tokens remain unknown texts experiment result indicate combine hand craftedstatistical learn information source attain recall ninety-six ninety-seven percent correspond precision ninety-three ninety-four percent ambiguity one hundred and two one hundred and three parse per token
present constraint base case frame lexicon architecture bi directional map syntactic case frame semantic frame lexicon use semantic sense basic unit employ multi tiered constraint structure resolution syntactic information appropriate sense idiomatic usage valency change transformations morphologically mark passivized causativized form handle via lexical rule manipulate case frame templates system implement type feature system apply turkish
paper present efficient algorithm retrieve database tree tree match give query tree approximately within certain error tolerance natural language process applications search match example base translation systems retrieval lexical databases contain entries complex feature structure algorithm implement sparcstations large randomly generate synthetic tree databases tens thousands tree associatively search tree small error matter tenths second second
describe implement system robust domain independent syntactic parse english use unification base grammar part speech punctuation label couple probabilistic lr parser present evaluations system performance along several different dimension enable us assess contribution individual part make success system whole thus prioritise effort devote enhancement currently system able parse around eighty sentence substantial corpus general text contain number distinct genres random sample two hundred and fifty sentence system mean cross bracket rate seventy-one recall precision eighty-three eighty-four respectively evaluate manually disambiguate analyse
paper investigate model merge technique derive markov model text speech corpora model derive start large specific model successively combine state build smaller general model present methods reduce time complexity algorithm report experiment derive language model speech recognition task experiment show advantage model merge standard bigram approach merge model assign lower perplexity test set use considerably fewer state
grice maxims conversation grice one thousand, nine hundred and seventy-five frame directives follow speaker language paper argue consider point view natural language generation characterisation rather mislead desire behaviour fall quite naturally view language generation goal orient process argue position particular regard generation refer expressions
perspective statistical language model emphasize collocational aspect advocate suggest string generalize term class relationships instead class object single important characteristic model mechanism compare pattern pattern fully generalize natural definition syntactic class emerge subset relational class collocational syntactic class unambiguous partition traditional syntactic class
excellent result report data orient parse dop natural language texts bod one thousand, nine hundred and ninety-three unfortunately exist algorithms computationally intensive difficult implement previous algorithms expensive due two factor exponential number rule must generate use monte carlo parse algorithm paper solve first problem novel reduction dop model small equivalent probabilistic context free grammar solve second problem novel deterministic parse strategy maximize expect number correct constituents rather probability correct parse tree use optimizations experiment yield ninety-seven cross bracket rate eighty-eight zero cross bracket rate differ significantly result report bod comparable result duplication pereira schabes one thousand, nine hundred and ninety-two experiment data show bod result least partially due extremely fortuitous choice test data partially due use cleaner data researchers
paper present new parse algorithm linear index grammars ligs spirit one describe vijay shanker weir one thousand, nine hundred and ninety-three tree adjoin grammars lig l input string x length n build non ambiguous context free grammar whose sentence exclusively valid derivation sequence l lead x show grammar build cal ofn6 time individual parse extract linear time size extract parse tree though cal ofn6 upper bind improve previous result average case behave much better moreover practical parse time decrease statically perform computations
investigate use technique develop constraint program community call constraint propagation automatically make hpsg theory specific place linguistically motivate underspecification would lead inefficient process discuss two concrete hpsg examples show line constraint propagation help improve process efficiency
post process methods character recognition rely contextual information character word fragment level however due linguistic characteristics korean low level information alone sufficient high quality character recognition applications need much higher level contextual information improve recognition result paper present domain independent post process technique utilize multi level morphological syntactic semantic information well character level information propose post process system perform three level process candidate character set selection candidate eojeol korean word generation morphological analysis final single eojeol sequence selection linguistic evaluation require linguistic information probabilities automatically acquire statistical corpus analysis experimental result demonstrate effectiveness method yield error correction rate eight thousand and forty-six improve recognition rate nine thousand, five hundred and fifty-three post process rate seven hundred and twelve single best solution selection
paper present generalise two level implementation handle linear non linear morphological operations algorithm interpretation multi tape two level rule describe addition number issue arise develop non linear grammars discuss examples syriac
paper focus two disparate aspects german syntax perspective parallel grammar development part cooperative project present innovative approach auxiliaries multiple genitive nps german lfg base implementation present avoid unnessary structural complexity representation auxiliaries challenge traditional analysis auxiliaries raise verbs approach develop multiple genitive nps provide abstract language independent representation genitives associate nominalized verbs take together two approach represent step towards provide uniformly applicable treatments differ languages thus lighten burden machine translation
linguistic theories formulate architecture sc hpsg precise explicit since sc hpsg provide formally well define setup however query faithful implementation explicit theory large data structure specify make hard see relevant aspects reply give system furthermore system spend much time apply constraints never fail able enumerate specific answer paper want describe lazy evaluation result line compilation technique method evaluation use answer query sc hpsg system relevant aspects check output
paper establish framework various aspects prosodic morphology templatic morphology infixation handle two level theory use implement multi tape two level model paper provide new computational analysis root pattern morphology base prosody
address treatment metonymic expressions knowledge representation perspective context text understand system aim build conceptual representation texts accord domain model express knowledge representation formalism focus paper part semantic analyser deal semantic composition explain use domain model handle metonymy dynamically generally underlie semantic composition use knowledge descriptions attach concept ontology kind concept level multiple role qualia structure rely heuristic path search algorithm exploit graphic aspects conceptual graph formalism methods describe implement apply french texts medical domain
show general grammar may automatically adapt fast parse utterances specific domain mean constituent prune grammar specialization base explanation base learn methods together give order magnitude increase speed coverage loss entail grammar specialization reduce approximately half report previous work experiment describe suggest loss coverage reduce point longer cause significant performance degradation context real application
paper describe measure evaluate three determinants well probabilistic classifier perform give test set determinants appropriateness test set result one feature selection two formulation parametric form model three parameter estimation part model formulation procedure even break separate step tradeoffs explore paper relevant wide variety methods measure demonstrate large experiment use analyze result roughly three hundred classifiers perform word sense disambiguation
line compilation logic grammars use magic allow incorporation filter logic underlie grammar explicit definite clause characterization filter result magic compilation allow processor independent logically clean optimizations dynamic bottom process respect goal directedness two filter optimizations base program transformation technique unfold discuss practical theoretical interest
paper discuss machine translation english text turkish relatively free word order language present algorithms determine topic focus target sentence use salience center theory old vs new information contrastiveness discourse model order generate contextually appropriate word order target language
describe two semantically orient dependency structure formalisms form form form previously use machine translation interlingual representations without provide formal interpretation form introduce paper scoped version form define compositional semantics mechanism two type semantic composition basic complement incorporation modifier incorporation bind variables do time incorporation permit much flexibility composition order simple account semantic effect permute several incorporations
word unknown lexicon present substantial problem part speech tag paper present technique fully unsupervised statistical acquisition rule guess possible part speech unknown word three complementary set word guess rule induce lexicon raw corpus prefix morphological rule suffix morphological rule end guess rule learn perform brown corpus data rule set highly competitive performance produce compare state art
natural next step evolution constraint base grammar formalisms rewrite formalisms abstract fully away detail grammar mechanism express syntactic theories purely term properties class structure license focus structural properties languages rather mechanisms generate check structure exhibit properties model theoretic approach offer simpler significantly clearer expression theories potentially provide uniform formalization allow disparate theories compare basis properties discuss lkp monadic second order logical framework approach syntax distinctive virtue superficially expressive support direct statement linguistically significant syntactic properties well define strong generative capacity languages definable lkp iff strongly context free draw examples realms gpsg gb
paper present prune technique use reduce number paths search rule base bag generators type propose citepoznanskietal95 citepopowich95 prune search space generators important give computational cost bag generation technique rely connectivity constraint semantic indices associate lexical sign bag test algorithm range sentence show reductions generation time number edge construct
one problems part speech tag real word texts unknown lexicon word mikheev acl ninety-six cmp lg nine million, six hundred and four thousand and twenty-two technique fully unsupervised statistical acquisition rule guess possible part speech unknown word propose one simplification assume learn technique acquisition morphological rule obey simple concatenative regularities main word affix paper extend technique non concatenative case suffixation assess gain performance
paper describe architecture functionality main components workbench acquisition domain knowledge large text corpora workbench support incremental process corpus analysis start rough automatic extraction organization lexico semantic regularities end computer support analysis extract data semi automatic refinement obtain hypotheses workbench employ methods computational linguistics information retrieval knowledge engineer although workbench currently implementation components already implement performance illustrate sample engineer medical domain
paper describe algorithm compilation two level orthographic phonological rule notation finite state transducers notation alternative standard one derive koskenniemi work believe practical descriptive advantage quite widely use different interpretation efficient interpreters exist notation clear compile equivalent automata transparent way present paper show use conceptual tool provide kaplan kay regular relations calculus
short paper briefly discuss task nlg systems perform research interest occasionally find useful way introduce nlg potential project collaborators know nothing field
describe simple hpsg analysis partial verb phrase front argue present account adequate others make past years allow description constituents front position modifier remain non front part sentence problem ill form sign admit hpsg account partial verb phrase front know far explain solution suggest use difference combinatoric relations sign representation word order domains
paper show higher order colour unification form unification develop automate theorem prove provide general theory model interface interpretation process source linguistic non semantic information particular provide general theory primary occurrence restriction dalrymple shieber pereira one thousand, nine hundred and ninety-one analysis call
pulman show higher order unification hou use model interpretation focus paper extend unification base approach case often see test bed focus theory utterances multiple focus operators second occurrence expressions show result analysis favourably compare two prominent theories focus namely rooth alternative semantics krifka structure mean theory correctly generate interpretations alternative theories yield finally discuss formal properties approach argue even though hou need terminate class unification problems deal paper hou avoid shortcoming fact computationally tractable
paper characterize properties direct interpretation hpsg present advantage approach high level program languages constitute perspective efficient solution show multi paradigm approach contain particular constraint logic program offer mechanims close theory preserve fundamental properties take example life describe implementation main hpsg mechanisms
propose algorithm resolve anaphors tackle mainly problem intrasentential antecedents base methodology fact antecedents likely occur embed sentence sidner focus mechanism use basic algorithm complete approach propose algorithm test implement part conceptual analyser mainly process pronouns detail evaluation give
paper describe tactical generation turkish free constituent order language order constituents may change accord information structure sentence generate absence information regard information structure sentence ie topic focus background etc constituents sentence obey default order order almost freely changeable depend constraints text flow discourse use recursively structure finite state machine handle change constituent order implement right linear grammar backbone implementation environment genkit system develop carnegie mellon university center machine translation morphological realization implement use external morphological analysis generation component perform concrete morpheme selection handle morphographemic process
describe method automatic word sense disambiguation use text corpus machine readable dictionary mrd method base word similarity context similarity measure word consider similar appear similar contexts contexts similar contain similar word circularity definition resolve iterative converge process system learn corpus set typical usages sense polysemous word list mrd new instance polysemous word assign sense associate typical usage similar context experiment show method perform well learn even sparse train data
current work surface realization concentrate use general abstract algorithms interpret large reversible grammars little attention pay far many small simple applications require coverage small sublanguage different degrees sophistication system tg two describe paper smoothly integrate deep generation process integrate can text templates context free rule single formalism allow textual tabular output parameterized accord linguistic preferences feature base suitably restrict production system techniques generic backtrack regime
paper present way reduce complexity parse free coordination live coordinative count invariant property derivable sequence occurrence sensitive categorial grammar invariant exploit cut deterministically search space coordinate sentence minimal fraction invariant base inequalities show best one get presence coordination without proper parse implement categorial parser dutch result apply invariant parse coordination parser present
paper describe new statistical parser base probabilities dependencies head word parse tree standard bigram probability estimation techniques extend calculate probabilities dependencies pair word test use wall street journal data show method perform least well spatter magerman ninety-five jelinek et al ninety-four best publish result statistical parser task simplicity approach mean model train forty thousand sentence fifteen minutes beam search strategy parse speed improve two hundred sentence minute negligible loss accuracy
address problem automatically acquire case frame pattern selectional pattern large corpus data particular propose method learn dependencies case frame slot view problem learn case frame pattern learn multi dimensional discrete joint distributions random variables represent case slot formalize dependencies case slot probabilistic dependencies random variables since number parameters multi dimensional joint distribution exponential infeasible accurately estimate practice overcome difficulty settle approximate target joint distribution product low order component distributions base corpus data particular propose employ efficient learn algorithm base mdl principle realize task experimental result indicate certain class verbs accuracy achieve disambiguation experiment improve use acquire knowledge dependencies
address problem automatically construct thesaurus hierarchically cluster word base corpus data view problem cluster word estimate joint distribution cartesian product partition set nouns partition set verbs propose estimation algorithm use simulate anneal energy function base minimum description length mdl principle empirically compare performance method base mdl principle one base maximum likelihood estimator find former outperform latter also evaluate method conduct pp attachment disambiguation experiment use automatically construct thesaurus experimental result indicate improve accuracy disambiguation use thesaurus
describe substantial domain independent language process systems french spanish quickly develop manually adapt exist english language system sri core language engine explain adaptation process detail argue provide fairly general recipe convert grammar base system english correspond one romance language
study computational complexity parse problem variant lambek categorial grammar call semidirectional semidirectional lambek calculus sdl additional non directional abstraction rule allow formula abstract appear anywhere premise sequent leave hand side thus permit non peripheral extraction sdl grammars able generate context free language show parse problem semidirectional lambek grammar np complete reduction three partition problem
generation algorithm base active chart parse algorithm introduce use conjunction shake bake machine translation system concise prolog implementation algorithm provide performance comparisons shift reduce base algorithm give show chart generator much efficient generate possible sentence input specification
give new treatment tabular lr parse alternative tomita generalize lr algorithm advantage twofold firstly treatment conceptually attractive use simpler concepts grammar transformations standard tabulation techniques also know chart parse secondly static dynamic complexity parse space time significantly reduce
information retrieval important application area natural language process one encounter genuine challenge process large quantities unrestricted natural language text paper report application simple yet robust efficient noun phrase analysis techniques create better index phrase information retrieval particular describe hybrid approach extraction meaningful continuous discontinuous subcompounds complex noun phrase use corpus statistics linguistic heuristics result experiment show index base extract subcompounds improve recall precision information retrieval system noun phrase analysis techniques also potentially useful book index automatic thesaurus extraction
german plural system become focal point conflict theories language linguistic cognitive present simulation result three simple classifiers ordinary nearest neighbour algorithm nosofsky generalize context model gcm standard three layer backprop network predict plural class phonological representation singular german though absolutely minimal model term architecture input information nevertheless remarkably well nearest neighbour predict correct plural class accuracy seventy-two set twenty-four thousand, six hundred and forty nouns celex database subset eight thousand, five hundred and ninety-eight non compound nouns nearest neighbour gcm network score seven hundred and ten seven hundred and fifty eight hundred and thirty-five respectively novel items furthermore outperform hybrid pattern associator default rule model propose marcus et al one thousand, nine hundred and ninety-five data set
base empirical evidence free word order language german propose fundamental revision principles guide order discourse entities forward look center within center model claim grammatical role criteria replace indicators functional information structure utterances ie distinction context bind unbind discourse elements claim back empirical evaluation functional center
extend center model resolution intra sentential anaphora specify handle complex sentence empirical evaluation indicate functional information structure guide search antecedent within sentence
present easily reproducible term simple transformation offline parsable grammars result provably terminate parse program directly top interpretable prolog transformation consist two step one removal empty productions follow two leave recursion elimination relate leave corner parse grammar compile rather interpret parse program advantage guarantee termination presence empty productions generalize greibach normal form dcgs advantage implementation simplicity
examine terminological languages use manage linguistic data nl research development particular consider lexical semantics task characterize semantic verb class show language extend flag inconsistencies verb class definitions identify need new verb class identify appropriate linguistic hypotheses new verb behavior
present hybrid text understand methodology resolution textual ellipsis integrate conceptual criteria base well formedness conceptual strength role chain terminological knowledge base functional constraints reflect utterances information structure base distinction context bind unbind discourse elements methodological framework text ellipsis resolution center model adapt constraints
argue performance base design natural language grammars associate parsers order meet constraints pose real world natural language understand approach incorporate declarative procedural knowledge language language use within object orient specification framework discuss several message pass protocols real world text parse provide reason sacrifice completeness parse favor efficiency
present approach parallel natural language parse base concurrent object orient model computation depth first yet incomplete parse algorithm dependency grammar specify several restrictions degree parallelization discuss
paper describe approach automatic evaluation speech recognition understand capabilities speak dialogue system train time table information use word accuracy recognition concept accuracy understand performance judgement measure calculate compare modules output correct reference answer report evaluation result spontaneous speech corpus ten thousand utterances observe nearly linear relationship word accuracy concept accuracy
consider problem learn co occurrence information two word categories general two discrete random variables take value hierarchically classify domain particular consider problem learn association norm define axypx pxpy px joint distribution x px py marginal distributions induce px formulate problem sub task learn conditional distribution pxy exploit identity pxy axypx propose two step estimation method base mdl principle work follow first estimate px p1 use mdl estimate pxy fix apply mdl hypothesis class p1 b give class b representations association norm estimation therefore obtain side effect near optimal estimation pxy apply general framework problem acquire case frame pattern assume px ax give representable model base classification exist within exist thesaurus tree cut hence pxy represent product pair tree cut model devise efficient algorithm implement general strategy test method use actually acquire case frame pattern conduct disambiguation experiment use acquire knowledge experimental result show method improve upon exist methods
paper present model anaphor resolution within framework center model consideration incremental process mode introduce need manage structural ambiguity center level hence center framework refine account local global parse ambiguities propagate level center representations yield moderately adapt data structure center algorithm
note reply joshua goodman paper efficient algorithms parse dop model goodman one thousand, nine hundred and ninety-six cmp lg nine million, six hundred and four thousand and eight paper goodman make number claim work data orient parse model bod one thousand, nine hundred and ninety-two one thousand, nine hundred and ninety-six note show claim must mistake
synchronous rewrite productions two rewrite systems pair apply synchronously derivation pair string present new synchronous rewrite system argue handle certain phenomena cover exist synchronous systems also prove interest formal computational properties system
design lr parser base interleave atomic symbol process context free backbone grammar full constraints underlie unification grammar describe parser employ set reduce constraints derive unification grammar lr parse step gap thread simulate reduce applicability empty productions
general practical method handle sparse data avoid hold data iterative reestimation derive first principles test part speech tag task outperform delete interpolation context independent weight even latter use globally optimal parameter set determine posteriori
method give invert logic grammar display point view logical form rather word string lr compile techniques use allow recursive descent generation algorithm perform functor merge much way lr parser perform prefix merge improvement semantic head drive generator result much smaller search space amount semantic lookahead vary appropriate tradeoff point table size result nondeterminism find automatically do remove spurious nondeterminism input sufficiently close examples train corpus large portion input preserve completeness
many different metrics exist evaluate parse result include viterbi cross bracket rate zero cross bracket rate several others however parse algorithms include viterbi algorithm attempt optimize metric namely probability get correct label tree choose parse algorithm appropriate evaluation metric better performance achieve present two new algorithms label recall algorithm maximize expect label recall rate bracket recall algorithm maximize bracket recall rate experimental result give show two new algorithms improve performance viterbi algorithm many criteria especially ones optimize
paper address problem correct spell errors result valid though unintended word peace piece quiet quite also problem correct particular word usage errors amount number among corrections require contextual information handle conventional spell program unix spell first introduce method call trigrams use part speech trigrams encode context method use small number parameters compare previous methods base word trigrams however effectively unable distinguish among word part speech case alternative feature base method call bay perform better bay less effective trigrams distinction among word depend syntactic constraints hybrid method call tribayes introduce combine best previous two methods improvement performance tribayes components verify experimentally tribayes also compare grammar checker microsoft word find substantially higher performance
categorial grammars powerful rule like composition simple n word sentence exponentially many parse generate parse inefficient obscure whatever true semantic ambiguities input paper address problem fairly general form combinatory categorial grammar mean efficient correct easy implement normal form parse technique parser prove find exactly one parse semantic equivalence class allowable parse spurious ambiguity carefully define show safely completely eliminate
two class methods show useful resolve lexical ambiguity first rely presence particular word within distance ambiguous target word second use pattern word part speech tag around target word methods complementary coverage former capture lexical atmosphere discourse topic tense etc latter capture local syntax yarowsky exploit complementarity combine two methods use decision list idea pool evidence provide component methods solve target problem apply single strongest piece evidence whatever type happen paper take yarowsky work start point apply decision list problem context sensitive spell correction decision list find large outperform either component method however find improvements obtain take account single strongest piece evidence available evidence new hybrid method base bayesian classifiers present performance improvements demonstrate
paper hierarchical context definition add exist cluster algorithm order increase robustness result algorithm cluster contexts events separately use experiment different ways define context language model take account contexts range standard bigram trigram contexts part speech five grams although none model compete directly backoff trigram give nine improvement perplexity interpolate trigram moreover modify version algorithm lead performance increase original version twelve
natural language process task require lexical semantic information automate acquisition information would thus increase robustness portability nlp systems paper describe acquisition method make use fix correspondences derivational affix lexical semantic information one advantage method methods rely surface characteristics language necessary input currently available
increasingly inheritance hierarchies use reduce redundancy natural language process lexicons systems utilize inheritance hierarchies need able insert word optimal set class hierarchies paper formalize problem feature base default inheritance hierarchies since problem turn np complete present approximation algorithm show algorithm efficient perform well respect number standard problems default inheritance prototype implementation test lexical hierarchies produce encourage result work present also relevant type default hierarchies
paper present result experiment decide question authenticity supposedly spurious rhesus attic tragedy sometimes credit euripides experiment involve use statistics order test whether significant deviations distribution word categories rhesus work euripides find count frequencies word categories corpus part speech tagger greek implement special techniques reduce problem sparse data use result accuracy ca nine hundred and sixty-six
paper show account coordination construct use derivation structure lexicalize tree adjoin grammar ltag present notion derivation ltags preserve notion fix constituency ltag lexicon provide flexibility need coordination phenomena also discuss construction practical parser ltags handle coordination include case non constituent coordination
paper present method resolution lexical ambiguity nouns automatic evaluation brown corpus method rely use wide coverage noun taxonomy wordnet notion conceptual distance among concepts capture conceptual density formula develop purpose fully automatic method require hand cod lexical entries hand tag text kind train process result experiment automatically evaluate semcor sense tag version brown corpus
propose treatment coordination base concepts functor argument subcategorization formalization comprise two part conceptually independent one hand extend feature structure unification disjunctive set value order check compatibility satisfiability subcategorization requirements structure complement hand consider conjunction et head coordinate structure coordinate structure stem simply subcategorization specifications et general schemata head saturation part encode within hpsg use resource subcategorization principle extend
paper describe method compile constraint base grammar potentially efficient form process method take dependent disjunctions within constraint formula factor non interact group whenever possible determine independence group dependent disjunctions split smaller group exponential amount redundant information reduce runtime mean exponential amount process save well since performance algorithm process constraints dependent disjunctions highly determine input transformation present paper prove beneficial algorithms
paper present architecture generation speak monologues contextually appropriate intonation two tiered information structure representation use high level content plan sentence plan stag generation produce efficient coherent speech make certain discourse relationships explicit contrast appropriately salient system able produce appropriate intonational pattern generate systems rely solely word class give new distinctions
present extensive empirical comparison several smooth techniques domain language model include describe jelinek mercer one thousand, nine hundred and eighty katz one thousand, nine hundred and eighty-seven church gale one thousand, nine hundred and ninety-one investigate first time factor train data size corpus eg brown versus wall street journal n gram order bigram versus trigram affect relative performance methods measure cross entropy test data addition introduce two novel smooth techniques one variation jelinek mercer smooth one simple linear interpolation technique outperform exist methods
report development simple fast efficient inductive unsupervised semantic tagger chinese word pos hand tag corpus three hundred and forty-eight thousand word use corpus tag two step first possible semantic tag select semantic dictionarytong yi ci ci lin pos conditional probability semantic pos ie psp final semantic tag assign consider semantic tag current word semantic word conditional probability psw derive first step semantic bigram probabilities pss use second step final manual check show simple efficient algorithm hit rate ninety-one tagger tag one hundred and forty-two word per second use one hundred and twenty mhz pentium run foxpro run twenty-three time faster viterbi tagger
asymptote derive turing local reestimation formula population frequencies local reestimation formula derive zipf law asymptotic behavior population frequencies two show qualitatively different asymptotically nevertheless instance common class reestimation formula asymptote pair constitute upper lower bound convergence region cumulative frequency function rank tend infinity result demonstrate turing formula qualitatively different various extensions zipf law suggest smooth frequency estimate towards geometric distribution
thesis investigate three problems involve probabilistic model language smooth n gram model statistical grammar induction bilingual sentence alignment three problems employ model three different level language involve word base constituent base sentence base model respectively describe techniques improve model language level surpass performance exist algorithms problem approach three problems use three different frameworks relate frameworks bayesian paradigm show framework use appropriate give problem finally show research address two central issue probabilistic model sparse data problem problem induce hide structure
address problem structural disambiguation syntactic parse psycholinguistics number principles disambiguation propose notably lexical preference rule lpr right association principle rap attach low parallel principle alpp extension rap argue order improve disambiguation result necessary implement principles basis probabilistic methodology define three word probability implement lpr length probability implement rap alpp furthermore adopt back method combine two type probabilities experimental result indicate method effective attain accuracy eight hundred and ninety-two
paper use feature model semantics plural determiners present approach grammar check definiteness use neural network techniques semantics morphological category map learn apply textual encode technique one hundred and twenty-five occurences relevant category ten zero word narrative text learn surface semantics map apply learn generation function newly generate representations achieve correct category assignment many case eighty-seven result considerably better direct surface categorization approach fifty-four baseline always guess dominant category sixty discuss result could use multilingual nlp applications
report method compile decision tree weight finite state transducers key assumptions tree predictions specify rewrite symbols input string decision tree node stateable term regular expressions input string leaf node treat separate rule leave right contexts constructable decisions make traverse tree root leaf rule compile transducers use weight rewrite rule rule compilation algorithm describe mohri sproat one thousand, nine hundred and ninety-six
paper study computational complexity disambiguation probabilistic tree grammars context free grammars present proof follow problems np hard compute probable parse mpp sentence word graph compute probable sentence mps word graph np hardness compute mps word graph also hold stochastic context free grammars consequently existence deterministic polynomial time algorithms solve disambiguation problems highly improbable event
paper describe algorithm compute optimal structural descriptions optimality theory grammars context free position structure algorithm extend tesar dynamic program approach tesar 1994tesar one thousand, nine hundred and ninety-five compute optimal structural descriptions regular context free structure generalization context free structure create several complications overcome without compromise core dynamic program approach result algorithm time complexity cubic length input applicable grammars universal constraints exhibit context free locality
present iterative procedure build chinese language model lm segment chinese text word base word base chinese language model however construction chinese lm require word boundaries get chicken egg problem propose iterative procedure alternate two operations segment text word build lm start initial segment corpus lm base upon use viterbi liek algorithm segment another set data build lm base second set use result lm segment first corpus alternate procedure provide self organize way segmenter detect automatically unseen word correct segmentation errors preliminary experiment show alternate procedure improve accuracy segmentation discover unseen word surprisingly well result word base lm perplexity one hundred and eighty-eight general chinese corpus
paper present ongoing work data orient parse dop model previous work dop test clean set analyze part speech string penn treebank achieve excellent test result leave however two important question unanswered one dop perform test unedited data two dop use parse word string contain unknown word paper address question show parse result unedited data worse clean data although competitive compare model parse word string show hardness problem much depend unknown word previously unseen lexical categories know word give novel method parse word estimate probabilities unknown subtrees method general interest since show good performance obtain without use part speech tagger best knowledge method outperform statistical parsers test penn treebank word string
paper describe system lead us believe feasibility construct natural speak dialogue systems task orient domains specifically address issue robust interpretation speech presence recognition errors robustness achieve combination statistical error post correction syntactically semantically drive robust parse extensive use dialogue context present evaluation system use time completion quality final solution suggest native speakers english use system successfully virtually train
data orient parse dop annotate language corpus use stochastic grammar probable analysis new input sentence construct combine sub analyse corpus probable way approach succesfully use syntactic analysis use corpora syntactic annotations penn treebank corpus semantically annotate sentence use approach also generate probable semantic interpretation input sentence present paper explain semantic interpretation method summarize result preliminary experiment semantic annotations add syntactic annotations sentence atis corpus data orient semantic interpretation algorithm succesfully test semantically enrich corpus
paper present analysis conduct corpus software instructions french order establish whether task structure elements procedural representation users task alone sufficient control grammatical resources text generator show construct genre provide useful additional source control enable us resolve undetermined case
context dependent rewrite rule use many areas natural language speech process work computational phonology demonstrate give certain condition rewrite rule represent finite state transducers fsts describe new algorithm compile rewrite rule fsts show algorithm simpler efficient exist algorithms many applications demand ability compile weight rule weight fsts transducers generalize provide transition weight extend algorithm allow
paper discuss problem learn language unprocessed text speech signal concentrate problem learn lexicon particular argue representation language linguistic parameters like word build perturb composition exist parameters power representation demonstrate several examples text segmentation compression acquisition lexicon raw speech acquisition mappings text artificial representations mean
leave corner parse algorithm top filter report show efficient performance unification base systems however due nontermination parse leave recursive grammars top constraints must weaken paper general method maximize top constraints propose method provide procedure dynamically compute restrictor minimum set feature involve infinite loop every propagation path thus top constraints maximally propagate
paper introduce finite state calculus family direct replace operators contrast simple replace expression upper lower define karttunen acl ninety-five new direct version upper lower yield unambiguous transducer lower language consist single string transduce input string leave right make longest possible replacement point new type replacement expression upper prefix suffix yield transducer insert text around string instance upper symbol denote match part input remain unchanged prefix suffix regular expressions describe insertions expressions type upper prefix suffix may use compose deterministic parser local grammar sense gross one thousand, nine hundred and eighty-nine useful applications direct replacement include tokenization filter text stream
corpus base methods natural language process often use supervise train require expensive manual annotation train corpora paper investigate methods reduce annotation cost sample selection approach train learn program examine many unlabeled examples select label annotation informative stage avoid redundantly annotate examples contribute little new information paper extend previous work committee base sample selection probabilistic classifiers describe family methods committee base sample selection report experimental result task stochastic part speech tag find variants achieve significant reduction annotation cost though computational efficiency differ particular simplest method parameters tune give excellent result also show sample selection yield significant reduction size model use tagger
german joint research project verbmobil vm aim development speech speech translation system paper report research do group belong verbmobil subproject system architectures tp15 specific research areas construction parsers spontaneous speech investigations parallelization parse contribute development flexible communication architecture distribute control
paper present new approach word sense disambiguation wsd use exemplar base learn algorithm approach integrate diverse set knowledge source disambiguate word sense include part speech neighbor word morphological form unordered set surround word local collocations verb object syntactic relation test wsd program name sc lexas common data set use previous work well large sense tag corpus separately construct sc lexas achieve higher accuracy common data set perform better frequent heuristic highly ambiguous word large corpus tag refine sense sc wordnet
paper present grammar style checker demonstrator spanish greek native writers develop within project gramcheck besides brief grammar error typology spanish linguistically motivate approach detection diagnosis present base generalize use prolog extensions highly type unification base grammars demonstrator currently include full coverage agreement errors certain head argument relation issue also provide correction mean analysis transfer synthesis cycle finally future extensions current system discuss
constraint grammar rule induce corpora simple scheme base local information ie lexical bias next neighbour contexts extend use barriers reach eight hundred and seventy-three percent precision one hundred and twelve tag word nine hundred and eighty-two percent recall result compare favourably methods use similar task although mean good result achieve use original hand write rule develop several years time
generic system text categorization present use representative text corpus adapt process step feature extraction dimension reduction classification feature extraction automatically learn feature corpus reduce actual word form use statistical information corpus general linguistic knowledge dimension feature vector reduce linear transformation keep essential information classification principle minimum least square approach base polynomials describe system readily adapt new domains new languages application system reliable fast process completely automatically show text categorizer work successfully text generate document image analysis dia grind truth data
describe number experiment demonstrate usefulness prosodic information process module parse speak utterances feature base grammar employ empty categories show require certain prosodic properties position input presence empty category hypothesize derivation accomplish efficiently approach implement machine translation project verbmobil result significant reduction work load parser
present language model consist collection cost bidirectional finite state automata associate head word phrase model suitable incremental application lexical associations dynamic program search optimal dependency tree derivations also present model algorithm machine translation involve optimal tile dependency tree entries cost bilingual lexicon experimental result report compare methods assign cost function model conclude discussion adequacy annotate linguistic string representations machine translation
paper present statistical language translation model base collections small finite state machine call head automata model intend capture lexical sensitivity n gram model direct statistical translation model time take account hierarchical phrasal structure language two type head automata define relational head automata suitable translation transfer dependency tree head transducers suitable direct recursive lexical translation
paper extend calculus regular expressions new type replacement expressions enhance expressiveness simple replace operator define karttunen one thousand, nine hundred and ninety-five parallel replacement allow multiple replacements apply simultaneously input without interfere also allow replacement constrain number alternative contexts enhancements general replacement expressions versatile two level rule description complex morphological alternations
paper deal discovery representation use lexical rule lrs large scale semi automatic computational lexicon acquisition analysis base set lrs implement test basis spanish english business finance relate corpora show though use lrs justify come cost free semi automatic output check require even block preemtion procedures build nevertheless large scope lrs justify facilitate unavoidable process large scale semi automatic lexical acquisition also argue place lrs computational process complex issue
article present new semantic base transfer approach develop apply within verbmobil machine translation project give overview declarative transfer formalism together procedural realization approach discuss compare several approach mt literature
article give overview new semantic base transfer approach develop apply within verbmobil machine translation project present declarative transfer formalism discuss implementation
paper propose use pattern base context free grammars basis build machine translation mt systems adopt personal tool broad range users cyberspace society discuss major requirements tool include easy customization diverse domains efficiency translation algorithm scalability incremental improvement translation quality user interaction describe approach meet requirements
introduce memory base approach part speech tag memory base learn form supervise learn base similarity base reason part speech tag word particular context extrapolate similar case hold memory supervise learn approach useful tag corpus available example desire output tagger base corpus tagger generator automatically build tagger able tag new text way diminish development time construction tagger considerably memory base tag share advantage statistical machine learn approach additional advantage specific memory base approach include relatively small tag corpus size sufficient train ii incremental learn iii explanation capabilities iv flexible integration information case representations v non parametric nature vi reasonably good result unknown word without morphological analysis vii fast learn tag paper show large scale application memory base approach feasible obtain tag accuracy par know statistical approach attractive space time complexity properties use igtree tree base formalism index search huge case base use igtree additional advantage optimal context size disambiguation dynamically compute
describe case study application symbolic machine learn techniques discovery linguistic rule categories supervise rule induction algorithm use learn predict correct diminutive suffix give phonological representation dutch nouns system produce rule comparable rule propose linguists furthermore process learn morphological task phonemes use group phonologically relevant categories discuss relevance method linguistics language technology
paper define notion preventative expression discuss corpus study expressions instructional text discuss cod schema take account form function feature present measure inter coder reliability feature discuss correlations exist function form feature
build text plan resources hand time consume difficult certainly number plan architectures accompany plan libraries implement architectures may reuse new domain library plan typically one way address problem use machine learn techniques automate derivation plan resources new domains paper apply technique build micro plan rule preventative expressions instructional text
describe analyze evaluate experimentally new probabilistic model word sequence prediction natural language base prediction suffix tree psts use efficient data structure extend notion pst unbounded vocabularies also show use bayesian approach base recursive priors possible psts efficiently maintain tree mixtures mixtures provably practically better performance almost single model evaluate model several corpora low perplexity achieve relatively small pst mixture model suggest may advantageous alternative theoretically practically widely use n gram model
introduce method analyze complexity natural language process task predict difficulty new nlp task complexity measure derive kolmogorov complexity class automata mean automata whose purpose extract relevant piece information sentence natural language semantics define relative set question automaton answer paper show examples complexity estimate various nlp program task recipes complexity management position natural language process subdomain software engineer lay formal foundation
tsnlp project investigate various aspects construction maintenance application systematic test suit diagnostic evaluation tool nlp applications paper summarize motivation main result project besides solid methodological foundation tsnlp produce substantial multi purpose multi user test suit three european languages together set specialize tool facilitate construction extension maintenance retrieval customization test data tsnlp result include data technology make publicly available project present valuable linguistic resourc e potential provide wide spread pre standard diagnostic evaluation tool developers users nlp applications
paper focus mental state adjectives offer unify analysis theory generative lexicon pustejovsky one thousand, nine hundred and ninety-one one thousand, nine hundred and ninety-five show instead enumerate various syntactic constructions enter different sense arise possible give rich type semantic representation explain semantic syntactic polymorphism
paper propose novel strategy design enhance accuracy parser simplify complex sentence parse approach involve separate parse constituent sub sentence within complex sentence achieve divide conquer strategy first disambiguate roles link word sentence segment sentence base roles separate parse tree segment sub sentence noun phrase within synthesize form final parse evaluate effect strategy parse compare original performance dependency parser performance enhance divide conquer strategy test six hundred sentence ipsm ninety-five data set enhance parser saw considerable error reduction two hundred and twelve accuracy
morphological analysis important subtask text speech conversion hyphenation language engineer task traditional approach perform morphological analysis combine morpheme lexicon set linguistic rule heuristics find probable analysis contrast present inductive learn approach morphological analysis reformulate segmentation task report number experiment five inductive learn algorithms apply three variations task morphological analysis result show generalisation performance algorithms good ii lazy learn algorithm ib1 ig perform best three task conclude lazy learn morphological analysis classification task indeed viable approach moreover strong advantage traditional approach avoid knowledge acquisition bottleneck fast deterministic learn process language independent
purpose paper present method automatic classification dialogue utterances result apply method corpus superficial feature set train utterances call cue take basis find relevant utterance class extract rule assign class new utterances cue assume partially contribute communicative function utterance instead rely subjective judgments task find class rule opt use machine learn techniques guarantee objectivity
new scheme represent phonological change continuous speech recognition suggest phonological tag couple morphological tag design represent condition korean phonological change pairwise language model morphological phonological tag implement korean speech recognition system performance model verify tdnn base speech recognition experiment
multiplicative weight update algorithms winnow study extensively colt literature recently people start use applications paper apply winnow base algorithm task natural language context sensitive spell correction task fix spell errors happen result valid word substitute casual causal previous approach problem statistics base compare winnow one successful approach use bayesian classifiers find 1when standard heavily prune set feature use describe problem instance winnow perform comparably bayesian method 2when full unpruned set feature use winnow able exploit new feature convincingly outperform bay 3when test set encounter dissimilar train set winnow better bay adapt unfamiliar test set use strategy present combine learn train set unsupervised learn noisy test set
increase use new methods nlp nemlap conference series exemplify occur context wider shift nature concern discipline paper begin short review context significant trend field review motivate lead set requirements support software general utility nlp research development workers freely available system design meet requirements describe call gate general architecture text engineer information extraction ie sense define message understand conferences arpa citearp95 nlp application many new methods find home hobbs citehob93 jacobs ed citejac92 ie system base gate also available research purpose describe lastly review relate work
automate text generation require underlie knowledge base generate often difficult produce software documentation one domain part knowledge base may derive automatically paper describe drafter author support tool generate user centre software documentation particular describe part require knowledge base obtain automatically
paper propose mechanism learn pattern correspondences two languages corpus translate sentence pair propose mechanism use analogical reason two translations give pair translations similar part sentence source language must correspond similar part sentence target language similarly different part correspond respective part translate sentence correspondences similarities also differences learn form translation rule system test small train dataset produce promise result investigation
squib claim large scale automatic sense tag text last do high level accuracy far less complexity computational effort believe moreover do open class word carefully select oppose pair recent work describe two experiment one explore amount information relevant sense disambiguation contain part speech field entries longman dictionary contemporary english ldoce another practical experiment attempt sense disambiguation open class word text assign ldoce homographs sense tag use part speech information report ninety-two open class word successfully tag way plan extend work implement improve large scale tagger description include
thesis describe tactical generator turkish free constituent order language order constituents may change accord information structure sentence generate absence information regard information structure sentence ie topic focus background etc constituents sentence obey default order order almost freely changeable depend constraints text flow discourse use recursively structure finite state machine handle change constituent order implement right linear grammar backbone implementation environment genkit system develop carnegie mellon university center machine translation morphological realization implement use external morphological analysis generation component perform concrete morpheme selection handle morphographemic process
thesis present constraint base morphological disambiguation approach applicable languages complex morphology specifically agglutinative languages productive inflectional derivational morphological phenomena morphologically complex languages like turkish automatic morphological disambiguation involve select token morphological parse right set inflectional derivational markers system combine corpus independent hand craft constraint rule constraint rule learn via unsupervised learn train corpus additional statistical information obtain corpus morphologically disambiguate hand craft rule linguistically motivate tune improve precision without sacrifice recall certain respect approach motivate brill recent work observation transformational approach directly applicable languages like turkish approach also use novel approach unknown word process employ secondary morphological processor recover relevant inflectional derivational information lexical item whose root unknown approach well one tokens remain unknown texts experiment result indicate combine hand craft statistical learn information source attain recall ninety-six ninety-seven correspond precision ninety-three ninety-four ambiguity one hundred and two one hundred and three parse per token
paper discuss compositional semantics implement verbmobil speech speech translation system use lud description language underspecified discourse representation structure description language formal interpretation drt describe well implementation together architecture system entire syntactic semantic process module show linguistically sound theory formalism properly implement system near real time requirements
paper describe development use lexical semantic database verbmobil speech speech machine translation system motivation provide common information source distribute development semantics transfer semantic evaluation modules store lexical semantic information application independently database organize around set abstract semantic class use define semantic contributions lemmata vocabulary system automatically create semantic lexica check correctness semantic representations build semantic class model use inheritance hierarchy database implement use lexicon formalism lex4 develop project
german government bmbf fund project verbmobil semantic formalism language underspecified discourse representation structure lud use describe several drss allow underspecification deal japanese pose challenge problems paper treatment multiple discourse relation constructions sentential level show common japanese problem formalism problem distinguish discourse relations take widest scope compare scope take elements one hand underspecified among hand also state semantic constraint resolution multiple discourse relations seem prevail syntactic c command constraint
paper propose textual clue approach help metaphor detection order improve semantic process figure previous work domain study semantic regularities overlook obvious set regularities corpus base analysis show existence surface regularities relate metaphors clue characterize syntactic structure lexical markers present object orient model represent textual clue find representation design help choice semantic process term possible non literal mean prototype implement model currently development within incremental approach allow step step evaluations footnotethis work take part research project sponsor aupelf uref francophone agency education research
machine translation system say complete expressions correct accord source language grammar translate target language paper address completeness issue compositional machine translation general compositional machine translation context free grammars particular condition guarantee translation completeness context free grammars present
present novel approach lexical error recovery textual input advance robust tokenizer implement correct spell mistake also recover segmentation errors apart orthographic considerations take tokenizer also make use linguistic expectations extract train corpus idea arrange hide markov model hmm multiple layer hmms layer responsible different aspects process input report experimental evaluations alternative probabilistic language model guide lexical error recovery process
word level translational equivalences extract parallel texts surprisingly simple statistical techniques however techniques easily fool indirect associations pair unrelated word whose statistical properties resemble mutual translations indirect associations pollute result translation lexicons drastically reduce precision paper present iterative lexicon clean method iteration remain incorrect lexicon entries filter without significant degradation recall lexicon clean technique produce translation lexicons recall precision exceed ninety well dictionary size translation lexicons ninety-nine correct
paper look hopfield neural network use store recall pattern construct natural language sentence pattern recognition storage tool hopfield neural network receive much attention attention however mainly field statistical physics due model simple abstraction spin glass systems discussion make differences show bias correlation natural language sentence pattern randomly generate ones use previous experiment result give numerical simulations show auto associative competence network train natural language pattern
two different methods check satisfiability feature descriptions use functional uncertainty device namelycitekaplan88co citebackofen94jsc although one citebackofen94jsc solve satisfiability problem completely methods merit may happen one single description part first method appropriate part second apply paper present common framework allow one combine methods do present set rule simplify feature descriptions different methods describe different control rule set control specify order different rule must apply
texts exhibit considerable stylistic variation paper report experiment corpus document n seventy-five zero analyze use various simple stylistic metrics subset n one thousand corpus previously assess relevant answer give information retrieval query experiment show subset differ significantly rest corpus term stylistic metrics study
key problem description language structure explain contradictory properties specificity generality contrast pole formulaic prescription generative productivity argue possible accept analogy similarity basic mechanisms structural definition specific example discuss would possible use analogy define generative model syntactic structure
clear computational linguistics education research semantics tool provide graphical interface allow interactive construction semantic representations variety different formalisms use several construction methods clear develop part fracas project design encourage convergence different semantic formalisms montague grammar drt situation semantics clear system freely available www http coliuni sbde clear clearshtml
present paper original extension two data drive algorithms transcription sequence graphemes correspond sequence phonemes particular approach generalize algorithm originally propose dedina nusbaum dandn one thousand, nine hundred and ninety-one originally promote model human ability pronounce unknown word analogy familiar lexical items show dn algorithm perform comparatively poorly evaluate realistic test set extension allow us improve substantially performance analogy base model also suggest algorithms reformulate much general framework allow us anticipate useful extensions however consider inability define model important notions like lexical neighborhood conclude approach fail offer proper model analogical process involve read aloud
paper explore correlation center different form pronominal reference italian particular zero overt pronouns subject position correlations propose earlier work coling ninety verify analysis corpus naturally occur texts process extend previous analysis several ways example take possessives subordinate account also provide detail analysis continue transition specifically show pronouns use markedly different way continue precede another continue shift continue precede retain
paper examine discourse function different type subject perform italian within center framework build previous work coling90 account alternation null strong pronouns subject position extend previous analysis several ways example refine notion sc continue discuss center function full nps
paper give account phenomena pronominalization italian term center theory general introduction italian pronominal system review center show original rule extend modify finally show center account two phenomena first functional role utterance may override predictions center second null subject use refer whole discourse segment
statistical methods automatically identify dependent word pair ie dependent bigrams corpus natural language text traditionally perform use asymptotic test significance paper suggest fisher exact test appropriate test due skew sparse data sample typical problem theoretical experimental comparisons fisher exact test variety asymptotic test test pearson chi square test likelihood ratio chi square test present comparisons show fisher exact test reliable identify dependent word pair usefulness fisher exact test extend problems statistical natural language process skew sparse data appear rule natural language experiment present paper perform use proc freq sas system
quote speech often set punctuation mark particular quotation mark thus might seem quotation mark would extremely useful identify structure texts unfortunately situation quite clear work argue quotation mark adequate either identify constrain syntax quote speech useful information come presence quote verb either verb say punctual verb presence punctuation mark usually commas use lexicalize grammar license quote clauses text adjuncts distinction make direct indirect quote speech rather adjunct non adjunct quote clauses
present model text analysis text speech tts synthesis base weight finite state transducers serve text analysis module multilingual bell labs tts system transducers construct use lexical toolkit allow declarative descriptions lexicons morphological rule numeral expansion rule phonological rule inter alia date model apply eight languages spanish italian romanian french german russian mandarin japanese
thesis morphological description turkish encode use two level model description make phonological component contain two level morphophonemic rule lexicon component list lexical items encode morphotactic constraints word grammar express tabular form include verbal nominal paradigm vowel consonant harmony epenthesis reduplication etc describe detail cod two level notation loan word phonology model separately implementation make use lexc twolc xerox mechanisms integrate morphological analyzer lexical syntactic components discuss simple graphical user interface provide work underway use model classroom set teach turkish morphology non native speakers
paper propose analysis classifiers four major type unit metric group species base properties japanese english analysis make possible uniform straightforward treatment noun phrase head classifiers japanese english machine translation implement mt system alt j e although analysis base characteristics differences japanese english show also applicable unrelated language thai
paper outline lexical organization turkish make use lexical rule inflections derivations lexical category change control proliferation lexical entries lexical rule handle change grammatical roles enforce type constraints control map subcategorization frame valency change operations lexical inheritance hierarchy facilitate enforcement type constraints semantic compositions inflections derivations constrain properties term predicate design test part hpsg grammar turkish term performance run time execution rule seem far better alternative pre compilation latter cause exponential growth lexicon due intensive use inflections derivations turkish
study analyse turkish syntax informational point view sign base linguistic representation principles hpsg head drive phrase structure grammar theory adapt turkish basic informational elements nest inherently sort feature structure call sign implementation logic program tool ale attribute logic engine primarily design implement hpsg grammars use type structure hierarchy turkish language design syntactic phenomena subcategorization relative clauses constituent order variation adjuncts nomina l predicate complement modifier relations turkish analyze parser design implement ale
paper address method align english chinese bilingual news report china news service combine lexical satistical approach sentential structure differences english chinese match sentence level many work may result frequent match several sentence en masse view current work also attempt create shorter alignment pair permit finer match clauses texts possible current method base statiscal correlation sentence clause length texts time use obvious anchor number place name appear frequently news report lexcial cue
speech process require efficient methods algorithms finite state transducers show recently constitute useful abstract model lead highly efficient time space algorithms field present methods algorithms illustrate case speech recognition addition classical techniques describe many new algorithms minimization global local fly determinization weight automata efficient composition transducers methods currently use large vocabulary speech recognition systems show formalism algorithms use text speech applications relate areas language process morphology syntax local grammars efficient way tutorial self contain require specific computational linguistic knowledge classical result
paper elaborate design machine translation evaluation method aim determine degree mean original text preserve translation without look grammatical correctness constituent sentence basic idea human evaluator take sentence translate text sentence determine semantic relationship exist sentence immediately precede order minimise evaluator dependence relations sentence express term conjuncts connect rather explicit categories n sentence text result list n one sentence sentence relationships call text connectivity profile compare connectivity profile original text degree correspondence two would measure quality translation set essential conjuncts extract english japanese computer interface design support task insert fit conjuncts sentence pair place several set experiment perform
phonetic ambiguity confusibility bugbears form bottom data drive approach language process question input close enough target word pervade entire problem space speech recognition synthesis language acquisition speech compression language representation variety representations apply demonstrably inadequate least aspects problem paper review inadequacy examine several touchstone model phonetic ambiguity relate problems design solve good solution would among things efficient accurate precise universally applicable representation word ideally usable phonetic distance metric direct measurement distance word utterance pair none propose model provide complete solution problem general algorithmic theory phonetic distance unclear whether weakness representational technology fundamental difficulty problem statement
although confusion individual phonemes feature study analyze since miller nicely one thousand, nine hundred and fifty-five little work do extend predictive theory word level confusions pgpfone alphabet good touchstone problem develop word level confusion metrics paper present difficulties incur along propose solutions extension phonetic confusion result theoretical whole word phonetic distance metric propose solutions use conjunction set selection filter genetic algorithm automatically generate appropriate word list radio alphabet work illustrate principles pitfalls address numeric theory isolate word perception
propose analysis corrections model requirements corrections place context show analysis naturally extend interaction corrections pronominal anaphora one hand indefiniteness analysis build previous unification base approach nl semantics rely higher order unification equivalences form unification take account syntactic beta eta identity also denotational equivalence
paper discuss process conversants dialogue infer whether assertions proposals accept reject conversational partner expand previous work show logical consistency necessary indicator acceptance sufficient logical inconsistency sufficient indicator rejection necessary show conversants use information structure prosody well logical reason distinguish acceptances logically consistent rejections relate work previous work implicature default reason introduce three new class rejection sc implicature rejections sc epistemic rejections sc deliberation rejections show rejections infer result default inferences analyse would block context order account facts propose model common grind allow default inferences go show model originally propose account various form acceptance also model type rejection
cue phrase may use discourse sense explicitly signal discourse structure also sentential sense convey semantic rather structural information correctly classify cue phrase discourse sentential critical natural language process systems exploit discourse structure eg perform task anaphora resolution plan recognition paper explore use machine learn classify cue phrase discourse sentential two machine learn program cgrendel c45 use induce classification model set pre classify cue phrase feature text speech machine learn show effective technique automate generation classification model also improve upon previous result compare manually derive classification model already literature learn model often perform higher accuracy contain new linguistic insights data addition ability automatically construct classification model make easier comparatively analyze utility alternative feature representations data finally ease retrain make learn approach scalable flexible manual methods
exist natural language interfaces databases nlidbs design use snapshot database systems provide limit facilities manipulate time dependent data consequently nlidbs also provide limit support notion time database community become increasingly interest temporal database systems intend store manipulate principled manner information present also past future thesis develop principled framework construct english nlidbs temporal databases nlitdbs draw research tense aspect theories temporal logics temporal databases first explore temporal linguistic phenomena likely appear english question nlitdbs draw exist linguistic theories time formulate account large number phenomena simple enough embody practical nlitdbs exploit ideas temporal logics define temporal mean representation language top show hpsg grammar theory modify incorporate tense aspect account thesis map wide range english question involve time appropriate top expressions finally present prove correctness method translate top tsql2 tsql2 temporal extension sql ninety-two database language way establish sound route english question involve time general purpose temporal database language act principled framework build nlitdbs demonstrate framework workable employ develop prototype nlitdb implement use ale prolog
paper propose computational treatment resolution zero pronouns japanese discourse use adaptation center algorithm able factor language specific dependencies one parameter center algorithm previous analyse stipulate zero pronoun cospecifier must share grammatical function property sc subject sc nonsubject show property share stipulation unneeded addition propose notion sc topic ambiguity within center framework predict ambiguities occur japanese discourse analysis implications design language independent discourse modules natural language systems center algorithm implement hpsg natural language system english japanese grammars
paper three aim one generalize computational account discourse process call sc center two apply account discourse process japanese use computational systems machine translation language understand three provide insights effect syntactic factor japanese discourse interpretation argue discourse interpretation inferential process syntactic cue constrain process demonstrate argument respect interpretation sc zero unexpressed arguments verb japanese syntactic cue japanese discourse investigate morphological markers grammatical sc topic postposition wa well grammatical function sc subject ga sc object sc object2 ni addition investigate role speaker sc empathy viewpoint event describe syntactically indicate use verbal compound ie auxiliary use verbs kureta kita result base survey native speakers interpretation short discourse consist minimal pair vary one factor demonstrate syntactic cue indeed affect interpretation sc zero previously sc topic realize sc zero also contribute salience discourse entity propose discourse rule sc zero topic assignment show sc center provide constraints sc zero interpret sc zero topic
languages japanese use zero unexpressed arguments verb utterances shift topic involve risk mean intend speaker may transparent hearer however potentially undesirable conversational strategy often occur course naturally occur discourse chapter report empirical study two hundred and fifty utterances zero twenty japanese newspaper article utterance analyze term center transition form center realize refer expressions also examine lexical subcategorization information tense aspect order test hypothesis speaker expect hearer use information determine global discourse structure explain occurrence zero sc retain sc rough shift center transition claim zero use case shift center support contextual information lexical semantics tense aspect agreement feature propose algorithm center incorporate observations integrate center global discourse structure thus enhance ability non local pronoun resolution
goal thesis advance exploration statistical language learn design space pursuit goal thesis make two main theoretical contributions identify new class design specify architecture natural language analysis probabilities give semantic form rather superficial linguistic elements ii explore development mathematical theory predict expect accuracy statistical language learn systems term volume data use train theoretical work illustrate apply statistical language learn design analysis noun compound syntactic semantic analysis noun compound attempt use propose architecture empirical comparisons demonstrate propose syntactic model significantly better previously suggest approach performance human judge task propose semantic model first statistical approach problem exhibit significantly better accuracy baseline strategy result suggest new class design identify promise one experiment also serve highlight need widely applicable theory data requirements
first step corpus base multilingual nlp work construct detail map correspondence text translation several automatic methods task propose recent years yet even best methods err several typeset page smooth injective map recognizer simr new bitext map algorithm simr errors smaller previous front runner factor four robustness enable new commercial quality applications greedy nature algorithm make independent memory resources unlike bitext map algorithms simr allow cross correspondences account word order differences output convert quickly easily sentence alignment simr output use align two hundred megabytes canadian hansards publication linguistic data consortium
adomit algorithm automatic detection omissions translations algorithm rely solely geometric analysis bitext map use linguistic information property allow deal equally well omissions correspond linguistic units might result word process mishaps adomit prove discover many errors hand construct gold standard evaluate bitext map algorithms quantitative evaluation simulate omissions show even today poor bitext map technology adomit valuable quality control tool translators translation bureaus
verbs particular kind binary ambiguity carry normal full mean merely act prop nominal object suggest detectable pattern relationship verb act prop termsupport verb noun support task paper undertake develop model identify support verb particular noun extension nouns enumerate model disambiguate verb respect support status paper set basic model standard comparison propose complex model give result support model validity compare similar approach
paper present method large corpus analysis semantically classify entire clause particular use cooccurrence statistics among similar clauses determine aspectual class input clause process examine linguistic feature clauses relevant aspectual classification genetic algorithm determine combinations linguistic feature use task
probabilistic analogues regular context free grammars well know computational linguistics currently subject intensive research date however satisfactory probabilistic analogue attribute value grammars propose previous attempt fail define correct parameter estimation algorithm present paper define stochastic attribute value grammars give correct algorithm estimate parameters estimation algorithm adapt della pietra della pietra lafferty one thousand, nine hundred and ninety-five estimate model parameters necessary compute expectations certain function random field application discuss della pietra della pietra lafferty represent english orthographic constraints gibbs sample use estimate need expectations fact attribute value grammars generate constrain languages make gibbs sample inapplicable show variant gibbs sample metropolis hastings algorithm use instead
several methods propose process corpus induce tagset sub language represent corpus paper examine structure tag word classification method introduce mcmahon one thousand, nine hundred and ninety-four discuss mcmahon smith one thousand, nine hundred and ninety-five cmp lg nine million, five hundred and three thousand and eleven two major variations one non random initial assignment word class two move multiple word parallel together provide robust non random result speed increase two hundred four hundred and fifty cost slightly lower quality mcmahon method average quality two variations three retain information less frequent word four avoid reclustering close class propose study note speed increase quote relative implementation understand mcmahon algorithm take time measure hours days home pc revise version mcmahon smith one thousand, nine hundred and ninety-five paper appear june one thousand, nine hundred and ninety-six computational linguistics two hundred and twenty-two thousand, two hundred and seventeen two hundred and forty-seven refer time several weeks cluster five hundred and sixty-nine word sparc ipc
many applications necessary determine similarity two string widely use notion string similarity edit distance minimum number insertions deletions substitutions require transform one string report provide stochastic model string edit distance stochastic model allow us learn string edit distance function corpus examples illustrate utility approach apply difficult problem learn pronunciation word conversational speech application learn string edit distance one fourth error rate untrained levenshtein distance approach applicable string classification problem may solve use similarity function database label prototypes keywords string edit distance levenshtein distance stochastic transduction syntactic pattern recognition prototype dictionary spell correction string correction string similarity string classification speech recognition pronunciation model switchboard corpus
paper present integrate tool german morphology statistical part speech tag aim make well establish methods widely available software user friendly run pc download complete package include lexicon documentation world wide web compare performance tag systems tagger produce similar result
paper detail simple approach implementation optimality theory ot prince smolensky one thousand, nine hundred and ninety-three computer part reuse standard system software nutshell ot generate source implement binprolog program interpret context free specification gen structural grammar accord user supply input form result set textually flatten candidate tree representations pass constraint stage constraints implement finite state transducers specify sed stream editor script typically map ill form portion candidate violation mark evaluation candidates reduce simple sort violation mark annotate output leave con feed sort order candidates basis violation vector column line thereby bring optimal candidate top approach give rise ot simple first freely available software tool ot framework provide generic facilities gen constraint definition practical applicability demonstrate model ot analysis apparent subtractive pluralization upper hessian present golston wiese one thousand, nine hundred and ninety-six
thesis present computational theory unsupervised language acquisition precisely define procedures learn language ordinary speak write utterances explicit help teacher theory base heavily concepts borrow machine learn statistical estimation particular learn take place fit stochastic generative model language evidence much thesis devote explain condition must hold general learn strategy arrive linguistically desirable grammars thesis introduce variety technical innovations among common representation evidence grammars learn strategy separate content linguistic parameters representation algorithms base suffer search problems plague computational approach language acquisition theory test problems learn vocabularies grammars unsegmented text continuous speech mappings sound representations mean perform extremely well various objective criteria acquire knowledge cause assign almost exactly structure utterances humans work application data compression language model speech recognition machine translation information retrieval task rely either structural stochastic descriptions language
last years new approach language process start emerge become know various label data orient parse corpus base interpretation tree bank grammar cf van den berg et al one thousand, nine hundred and ninety-four bod one thousand, nine hundred and ninety-two ninety-six bod et al 1996a b bonnema one thousand, nine hundred and ninety-six charniak 1996a b goodman one thousand, nine hundred and ninety-six kaplan one thousand, nine hundred and ninety-six rajman 1995a b scha one thousand, nine hundred and ninety ninety-two sekine grishman one thousand, nine hundred and ninety-five sima et al one thousand, nine hundred and ninety-four sima one thousand, nine hundred and ninety-five ninety-six tugwell one thousand, nine hundred and ninety-five approach call data orient process dop embody assumption human language perception production work representations concrete past language experience rather abstract linguistic rule model instantiate approach therefore maintain large corpora linguistic representations previously occur utterances process new input utterance analyse utterance construct combine fragment corpus occurrence frequencies fragment use estimate analysis probable one paper give depth discussion data orient process model employ corpus label phrase structure tree review model instantiate dop approach many model also employ label phrase structure tree use different criteria extract fragment corpus employ different disambiguation strategies bod 1996b charniak 1996a b goodman one thousand, nine hundred and ninety-six rajman 1995a b sekine grishman one thousand, nine hundred and ninety-five sima one thousand, nine hundred and ninety-five ninety-six model use richer formalisms corpus annotations van den berg et al one thousand, nine hundred and ninety-four bod et al 1996a b bonnema one thousand, nine hundred and ninety-six kaplan one thousand, nine hundred and ninety-six tugwell one thousand, nine hundred and ninety-five
statistical language model assign probability string arbitrary length unfortunately possible gather reliable statistics string arbitrary length finite corpus therefore statistical language model must decide symbol string depend small finite number symbols string report propose new way model conditional independence markov model central feature nonuniform markov model make predictions vary lengths use contexts vary lengths experiment wall street journal reveal nonuniform model perform slightly better classic interpolate markov model result somewhat remarkable model contain identical number parameters whose value estimate similar manner difference two model combine statistics longer shorter string keywords nonuniform markov model interpolate markov model conditional independence statistical language model discrete time series
paper present hmm base speech recognition engine integration direct manipulation interfaces korean document editor speech recognition reduce typical tedious repetitive action inevitable standard guis graphic user interfaces system consist general speech recognition engine call abrain auditory brain speech commandable document editor call simple hear editor abrain phoneme base speech recognition engine show ninety-seven discrete command recognition rate eurobridge widget base document editor support speech command well direct manipulation interfaces
past thirty years considerable progress design natural language interfaces databases work concern snapshot databases limit facilities manipulate time vary information database community become increasingly interest temporal databases databases special support time dependent entries develop framework construct natural language interfaces temporal databases draw research temporal phenomena within logic linguistics central part framework logic like formal language call top capture semantics wide range english sentence implement hpsg base sentence analyser convert large set english query involve time top formulae formulate provably correct procedure translate top expressions query tsql2 temporal database language way establish sound route english general purpose temporal database language
paper describe experimental comparison seven different learn algorithms problem learn disambiguate mean word context algorithms test include statistical neural network decision tree rule base case base classification techniques specific problem test involve disambiguate six sense word line use word current proceed sentence context statistical neural network methods perform best particular problem discuss potential reason observe difference also discuss role bias machine learn importance explain performance differences observe specific problems
paper analyse language model speak dialogue systems access database use several language model obtain exploit dialogue predictions give better result use single model whole dialogue interaction reason several model create one specific system question request confirmation parameter use dialogue dependent language model increase performance recognition understand level especially answer system request moreover methods increase performance like automatic cluster vocabulary word use better acoustic model recognition affect improvements give dialogue dependent language model system use experiment dialogos italian speak dialogue system use access railway timetable information telephone experiment carry large corpus dialogues collect use dialogos
paper describe set metrics evaluation different dialogue management strategies implement real time speak language system set metrics propose offer useful insights evaluate particular choices dialogue management affect overall quality man machine dialogue evaluation make use establish metrics transaction success contextual appropriateness system answer calculation normal correction turn dialogue also define new metric implicit recovery allow measure ability dialogue manager deal errors different level analysis report evaluation data several experiment compare two different approach dialogue repair strategies use set metrics argue
paper present dialogos real time system human machine speak dialogue telephone task orient domains system test large trial inexperienced users prove robust enough allow spontaneous interactions users get good recognition performance ones get lower score robust behavior system achieve combine use specific language model recognition phase analysis tolerance toward spontaneous speech phenomena activity robust parser use pragmatic base dialogue knowledge integration different modules allow deal partial total breakdowns different level analysis report field trial data system evaluation result overall system submodules
maximum entropy model toolkit support parameter estimation prediction statistical language model maximum entropy framework maximum entropy framework provide constructive method obtain unique conditional distribution pyx satisfy set linear constraints maximize conditional entropy hpf respect empirical distribution fx maximum entropy distribution pyx also unique parametric representation class exponential model myx ryx zx numerator myx prodi alphaigixy product exponential weight alphai explambdai denominator zx sumy ryx require satisfy axioms probability manual explain build maximum entropy model discrete domains maximum entropy model toolkit memt first summarize step necessary implement language model use toolkit next discuss executables provide toolkit explain file format require toolkit finally review maximum entropy framework apply problem statistical language model keywords statistical language model maximum entropy exponential model improve iterative scale markov model trigger
view operations richardson purification algorithm discrete time dynamical process propose method overcome instability algorithm control chaos present theoretical analysis numerical result behavior performance stabilize algorithm