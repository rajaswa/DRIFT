present new method discover segmental discourse structure document categorize segment function demonstrate retrieval noun phrase pronominal form along zero sum weight scheme determine topicalize segmentation futhermore use term distribution aid identify role segment perform document finally present result evaluation term precision recall surpass earlier approach
outline utterances dialogs interpret use partial first order logic exploit capability logic talk truth status formulae define notion coherence utterances explain coherence relation serve construction tree represent segmentation dialog bdi model formalize basic assumptions dialog cooperative behaviour participants assumptions provide basis infer speech act coherence relations utterances attitudes dialog participants speech act prove useful determine dialog segment define notion complete expectations dialog participants finally sketch explicit segmentation signal cue phrase performatives cover dialog model
document describe sizable grammar english write tag formalism implement use xtag system report grammar describe herein supersede tag grammar describe earlier one thousand, nine hundred and ninety-five xtag technical report english grammar describe report base tag formalism extend include lexicalization unification base feature structure range syntactic phenomena handle large include auxiliaries include inversion copula raise small clause constructions topicalization relative clauses infinitives gerunds passives adjuncts clefts wh clefts pro constructions noun noun modifications extraposition determiner sequence genitives negation noun verb contractions sentential adjuncts imperatives technical report correspond xtag release eight thirty-one ninety-eight xtag grammar continuously update addition new analyse modification old ones online version report find xtag web page http wwwcisupennedu xtag
language model speech recognition typically use probability model form pran a1 a2 one stochastic grammars hand typically use assign structure utterances language model form construct grammars compute prefix probability sumw sigma pra1 w w represent possible terminations prefix a1 main result paper algorithm compute prefix probabilities give stochastic tree adjoin grammar tag algorithm achieve require computation ofn6 time probability subderivations derive word prefix contribute structurally derivation precomputed achieve termination algorithm enable exist corpus base estimation techniques stochastic tag use language model
much power probabilistic methods model language come ability compare several derivations string language important start point study cross derivational properties notion consistency probability model define probabilistic grammar say consistent probabilities assign string language sum one literature probabilistic context free grammars cfgs know precisely condition ensure consistency true give cfg paper derive condition give probabilistic tree adjoin grammar tag show consistent give simple algorithm check consistency give formal justification correctness condition derive use ensure probability model use tag check deficiency ie whether probability mass assign string generate
paper present new tree rewrite formalism call link share tree adjoin grammar lstag variant synchronous tag use lstag define approach towards coordination linguistic dependency distinguish notion constituency approach towards coordination explicitly distinguish dependencies constituency give better formal understand representation compare previous approach use tree rewrite systems conflate two issue
paper describe incremental generation parse table lr type parse tree adjoin languages tals algorithm present handle modifications input grammar update parser generate far paper lazy generation lr type parsers tals define parse table create need parse describe incremental parser generator tals respond modification input grammar update parse table build far
paper present morphy integrate tool german morphology part speech tag context sensitive lemmatization large lexicon three hundred and twenty thousand word form plus ability process german compound nouns guarantee wide morphological coverage syntactic ambiguities resolve standard statistical part speech tagger use output tagger lemmatizer determine correct root even ambiguous word form complete package freely available download world wide web
lexical acquisition system present paper incrementally update linguistic properties unknown word infer surround context parse sentence hpsg grammar german employ gradual information base concept unknownness provide uniform treatment range completely know maximally unknown lexical entries unknown information view revisable information either generalizable specializable update take place parse require modify lexical lookup revisable piece information identify grammar specify declarations provide access paths parse feature structure update mechanism revise correspond place lexical feature structure iff context actually provide new information revise generalizable information type union require work example demonstrate inferential capacity implement system
paper describe computational declarative approach prosodic morphology use inviolable constraints denote small finite candidate set filter restrictive incremental optimization mechanism new approach illustrate implement fragment modern hebrew verbs couch microcuf expressive constraint logic formalism generation parse word form propose novel line technique eliminate run time optimization produce finite state oracle efficiently restrict constraint interpreter search space byproduct unknown word analyze without special mechanisms unlike pure finite state transducer approach hybrid setup allow expressivity constraints specify eg token identity reduplication arithmetic constraints phonetics
paper address issue sc pos tagger evaluation evaluation usually perform compare tagger output reference test corpus assume error free currently use corpora contain noise cause obtain performance distortion real value analyze extent distortion may invalidate comparison taggers measure improvement give new system main conclusion rigorous test experimentation set design need reliably evaluate compare tagger accuracies
present bootstrapping method develop annotate corpus specially useful languages available resources method apply develop corpus spanish 5mw method consist take advantage collaboration two different pos taggers case taggers agree present higher accuracy use retrain taggers
report two corpora use evaluation component systems task one linear segmentation text two summary direct sentence extraction present characteristics corpora methods use collection user judgments overview application corpora evaluate component system finally discuss problems issue construction test set apply broadly construction evaluation resources language technologies
several methods know parse languages generate tree adjoin grammars tag ofn6 worst case run time paper investigate restrictions tag tag derivations need order lower ofn6 time complexity without introduce large runtime constants without lose generative power need capture syntactic constructions natural language handle unrestricted tag particular describe algorithm parse strict subclass tag ofn5 attempt show subclass retain enough generative power make useful general case
paper argue interlingual representation must explicitly represent part mean situation possibilities preferences necessary definite components mean constraints possibilities enable analysis generation nuance something require faithful translation furthermore representation mean word especially near synonyms crucial specify nuances word convey contexts
paper present partial solution component problem lexical choice choose synonym typical expect context apply new statistical approach represent context word lexical co occurrence network implementation train evaluate large corpus result show inclusion second order co occurrence relations improve performance implement lexical choice program
paper present result compare statistical tagger german base decision tree rule base brill tagger german use train corpus therefore tag set train taggers apply taggers test corpus compare respective behavior particular error rat taggers perform similarly error rate around five detail error analysis see rule base tagger problems unknown word statistical tagger result opposite tokens many ways ambiguous unknown word feed taggers help external lexicon gertwol system error rate rule base tagger drop forty-seven respective rate statistical taggers drop around thirty-seven combine taggers use output one tagger help lead improvement
paper present language model develop syntactic structure use extract meaningful information word history thus enable use long distance dependencies model assign probability every joint sequence word binary parse structure headword annotation operate leave right manner therefore usable automatic speech recognition model probabilistic parameterization set experiment mean evaluate predictive power present improvement standard trigram model achieve
paper present language model develop syntactic structure use extract meaningful information word history thus enable use long distance dependencies model assign probability every joint sequence word binary parse structure headword annotation model probabilistic parametrization set experiment mean evaluate predictive power present
thesis address problem automatically acquire lexical semantic knowledge especially case frame pattern large corpus data use acquire knowledge structural disambiguation approach adopt follow characteristics one divide problem three subproblems case slot generalization case dependency learn word cluster thesaurus construction two view subproblem statistical estimation define probability model subproblem three adopt minimum description length mdl principle learn strategy four employ efficient learn algorithms five view disambiguation problem statistical prediction major contributions thesis include one formalization lexical knowledge acquisition problem two development number learn methods lexical knowledge acquisition three development high performance disambiguation method
exist several methods calculate similarity curve sequence similarity value represent lexical cohesion successive text constituents eg paragraph methods decide locations fragment boundaries however scarce propose fragmentation method base dynamic program method theoretically sound guarantee provide optimal split basis similarity curve prefer fragment length cost function define method especially useful control fragment size importance
order support efficient development nl generation systems two orthogonal methods currently pursue emphasis one reusable general linguistically motivate surface realization components two simple task orient template base techniques paper argue application orient perspective benefit still limit order improve situation suggest evaluate shallow generation methods associate increase flexibility advise close connection domain motivate linguistic ontologies support quick adaptation new task domains rather reuse general resources method especially design generate report limit linguistic variations
describe simple variant interpolate markov model non emit state transition prove strictly powerful markov model importantly non emit model outperform classic interpolate model natural language texts wide range experimental condition modest increase computational requirements non emit model also much less prone overfitting keywords markov model interpolate markov model hide markov model mixture model non emit state transition state conditional interpolation statistical language model discrete time series brown corpus wall street journal
paper present method identify discourse marker usage spontaneous speech base machine learn discourse markers denote special pos tag thus process pos tag use identify discourse markers incorporate pos tag language model discourse markers identify speech recognition timeliness information use help predict follow word contrast approach alternative machine learn approach propose litman one thousand, nine hundred and ninety-six paper also argue discourse markers use help hearer predict role upcoming utterance play dialog thus discourse markers provide valuable evidence automatic dialog act prediction
memory base learn keep full memory learn material appear viable approach learn nlp task often superior generalisation accuracy eager learn approach abstract learn material investigate three partial memory base learn approach remove memory specific task instance type estimate exceptional three approach implement one heuristic function estimate exceptionality instance type typicality ii class prediction strength iii friendly neighbourhood size experiment perform memory base learn algorithm ib1 ig train english word pronunciation find remove instance type low prediction strength ii test method seriously harm generalisation accuracy conclude keep full memory type rather tokens exclude minority ambiguities appear performance preserve optimisations memory base learn
lead morpho phonological theories state art text speech systems assume word pronunciation learn perform without analyse several abstraction level eg morphological graphemic phonemic syllabic stress level challenge assumption case english word pronunciation use igtree inductive learn decision tree algorithms train test three word pronunciation systems number abstraction level implement sequence modules reduce five via three one latter system classify letter string directly map phonemes stress markers yield significantly better generalisation accuracies two multi module systems analyse empirical result indicate positive utility effect sequence modules outweigh cascade errors pass modules
paper present new context free parse algorithm base bidirectional strictly horizontal strategy incorporate strong top predictions derivations adjacencies functional point view parser able propagate syntactic constraints reduce parse ambiguity computational perspective algorithm include different techniques aim improvement manipulation representation structure use
paper describe conversion hide markov model finite state transducer closely approximate behavior stochastic model case transducer equivalent hmm conversion especially advantageous part speech tag result transducer compose transducers encode correction rule frequent tag errors speed tag also improve describe methods implement successfully test
thesis describe application relaxation label algorithm nlp disambiguation language model context constraint inspire constraint grammars constraints enable use real value statind compatibility technique apply pos tag shallow parse word sense disambigation experiment result report propose approach enable use multi feature constraint model simultaneous resolution several nl disambiguation task collaboration linguistic statistical model
report study interannotator agreement coreference task define message understand conference muc six muc seven base feedback annotators clarify simplify annotation specification perform analysis disagreement among several annotators conclude sixteen disagreements represent genuine disagreement coreference remainder case mostly typographical errors omissions easily reconcile initially measure interannotator agreement low 80s precision recall try improve upon run several experiment final experiment separate tag candidate noun phrase link actual coreferring expressions method show promise interannotator agreement climb low 90s need extensive validation result position research community broaden coreference task multiple languages possibly different kinds coreference
exist natural language database interfaces nldbs design use database systems provide limit facilities manipulate time dependent data support adequately temporal linguistic mechanisms verb tense temporal adverbials temporal subordinate clauses etc database community become increasingly interest temporal database systems intend store manipulate principled manner information present also past future interfacing temporal databases support temporal linguistic mechanisms become crucial present framework construct natural language interfaces temporal databases nltdbs draw research tense aspect theories temporal logics temporal databases framework consist temporal intermediate representation language call top hpsg grammar map wide range question involve temporal mechanisms appropriate top expressions provably correct method translate top tsql2 tsql2 recently propose temporal extension sql database language framework employ implement prototype nltdb use ale prolog
paper present statistical learn approach find name non recursive entities text per muc six definition ne task use variant standard hide markov model present justification problem approach detail discussion model finally successful result new approach
graph interpolation grammars declarative formalism operational semantics goal emulate salient feature human parser notably incrementality parse process define gigs incrementally build syntactic representation sentence successive lexeme read gig rule specify set parse configurations trigger application operation perform match configuration rule partly context sensitive furthermore reversible mean operations undo allow parse process nondeterministic two factor confer enough expressive power formalism parse natural languages
paper present novel formalization optimality theory unlike previous treatments optimality computational linguistics start ellison one thousand, nine hundred and ninety-four new approach require explicit mark count constraint violations base notion lenient composition define combination ordinary composition priority union underlie form output meet give constraint lenient composition enforce constraint none output candidates meet constraint lenient composition allow sake greater efficiency may leniently compose gen relation constraints single finite state transducer map underlie form directly optimal surface realizations vice versa without ever produce fail candidates see perspective optimality theory surprisingly similar two older strain finite state phonology classical rewrite systems two level model particular rank optimality constraints correspond order rewrite rule
paper discuss problem determinising finite state automata contain large number epsilon move experiment finite state approximations natural language grammars often give rise large automata large number epsilon move paper identify three subset construction algorithms treat epsilon move number experiment perform indicate algorithms differ considerably practice furthermore experiment suggest average number epsilon move per state use predict algorithm likely perform best give input automaton
resolution lexical ambiguity commonly term word sense disambiguation expect improve analytical accuracy task sensitive lexical semantics task include machine translation information retrieval parse natural language understand lexicography reflect growth utilization machine readable texts word sense disambiguation techniques explore variously context corpus base approach within one corpus base framework similarity base method systems use database example sentence manually annotate correct word sense give input systems search database similar example input lexical ambiguity word contain input resolve select sense annotation retrieve example research apply method resolution verbal polysemy similarity two examples compute weight average similarity complement govern target polysemous verb explore similarity base verb sense disambiguation focus follow three methods first propose weight schema verb complement similarity computation second similarity base techniques overhead manual supervision search large size database prohibitive resolve problem propose method select small number effective examples system usage finally efficiency system highly dependent similarity computation use maximize efficiency propose method integrate advantage previous methods similarity computation
investigate existence class zfc provably total recursive unary function give certain constraints apply result show sigma1 sound set theory zfcnotvdash pnp
paper present experiment learn valences subcategorization frame fifty million word text corpus base lexicalize probabilistic context free grammar distributions estimate use modify algorithm evaluate acquire lexicon comparison dictionary entropy measure result show model produce highly accurate frame distributions
paper present model linguistic description base group theory grammar model g grammar collection lexical expressions products logical form phonological form inverses phrasal descriptions obtain form products lexical expressions cancel contiguous elements inverses show applications model parse generation long distance movement quantifier scoping believe move free monoid vocabulary v standard formal language study free group v deep affinities linguistic phenomena classical algebra come surface consequences tap mathematical connections thus establish could considerable
model co occurrence bitext boolean predicate indicate whether give pair word tokens co occur correspond regions bitext space co occurrence precondition possibility two tokens might mutual translations model co occurrence glue bind methods map bitext correspondence methods estimate translation model integrate system exploit parallel texts different model co occurrence possible depend kind bitext map available language specific information available assumptions make nature translational equivalence although statistical translation model base model co occurrence model co occurrence correctly difficult may first appear
annotation style guide create blinker project university pennsylvania blinker project name bilingual linker gui create enable bilingual annotators link word tokens mutual translations parallel texts parallel text choose project bible probably easiest text obtain electronic form multiple languages languages involve english french languages project co ordinator familiar two sufficient number annotators likely find
bilingual annotators pay link roughly sixteen thousand correspond word line versions bible modern french modern english annotations freely available research community http wwwcisupennedu melamed annotations use several purpose first use standard data set develop test translation lexicons statistical translation model second researchers lexical semantics able mine annotations insights cross linguistic lexicalization pattern third annotations use research certain recently propose methods monolingual word sense disambiguation paper describe annotate texts specially design annotation tool strategies employ increase consistency annotations annotation process repeat five time different annotators inter annotator agreement rat indicate annotations reasonably reliable method easy replicate
parallel texts bitexts properties distinguish kinds parallel data first word translate one word second bitext correspondence noisy article present methods bias statistical translation model reflect properties analysis expect behavior bias presence sparse data predict result accurate model prediction confirm evaluation respect gold standard translation model bias fashion significantly accurate baseline knowledge poor model article also show statistical translation model take advantage various kinds pre exist knowledge might available particular language pair even simplest kinds language specific knowledge distinction content word function word show reliably boost translation model performance task statistical model inform pre exist knowledge model domain combine best rationalist empiricist traditions
inside outside probabilities typically use reestimating probabilistic context free grammars pcfgs forward backward probabilities typically use reestimating hmms show several novel use include improve parser accuracy match parse algorithms evaluation criteria speed dop parse five hundred time thirty time faster pcfg thresholding give accuracy level also give elegant state art grammar formalism use compute inside outside probabilities parser description formalism make easy derive inside outside formulas many others
since early sixties seventies know regular context free languages characterize definability monadic second order theory certain structure recently descriptive characterizations use obtain complexity result constraint principle base theories syntax provide uniform model theoretic framework explore relationship theories express disparate formal term result limit extent lack descriptive characterizations language class beyond context free recently show tree adjoin languages mildly generalize form characterize recognition automata operate three dimensional tree manifold three dimensional analog tree paper exploit automata theoretic result obtain characterization tree adjoin languages definability monadic second order theory three dimensional tree manifold open way extend tool model theoretic syntax level tals provide highly flexible mechanism define tag term logical constraints full version paper appear proceed coling acl ninety-eight project note
work motivate two long term goals understand humans learn language build program understand language use representation make relevant feature explicit prerequisite successful learn understand therefore choose represent relations individual word explicitly model lexical attraction define likelihood relations introduce new class probabilistic language model name lexical attraction model represent long distance relations word formalize new class model use information theory within framework lexical attraction develop unsupervised language acquisition program learn identify linguistic relations give sentence explicitly represent linguistic knowledge program lexical attraction initial grammar lexicon build input raw text learn process interdigitated processor use regularities detect learner impose structure input structure enable learner detect higher level regularities use bootstrapping procedure program train one hundred million word associate press material able achieve sixty precision fifty recall find relations content word use knowledge lexical attraction program identify correct relations syntactically ambiguous sentence saw statue liberty fly new york
experience critique system show system detect problems user performance multiple critique often produce analysis corpus actual critique reveal even though individual critique concise coherent set critique whole may exhibit several problems detract conciseness coherence consequently assimilation thus text planner need could integrate text plan individual communicative goals produce overall text plan represent concise coherent message paper present general rule base system accomplish task system take input emphset individual text plan represent rst style tree produce smaller set complex tree represent integrate message still achieve multiple communicative goals individual text plan domain independent rule use capture strategies across domains facility addition domain dependent rule enable system tune requirements particular domain system test corpus critique domain trauma care
position paper suggest progress automatic summarise demand better research methodology carefully focus research strategy order develop effective procedures necessary identify respond context factor ie input purpose output factor bear summarise evaluation paper analyse illustrate factor implications evaluation argue analysis together state art intrinsic difficulty summarise imply nearer term strategy concentrate shallow surface text analysis indicative summarise illustrate current work potentially productive research programme develop
paper report recognition component intelligent tutor system design help foreign language speakers learn standard english system model grammar learner instantiation system tailor signers american sign language asl discuss theoretical motivations system various difficulties encounter implementation well methods use overcome problems method capture ungrammaticalities involve use mal rule also call error productions however straightforward addition mal rule cause significant performance problems parser instance asl population strong tendency drop pronouns auxiliary verb able account sentence result explosion number possible parse sentence explosion leave unchecked greatly hamper performance system discuss handle take account expectations specific population capture unique user model different representations lexical items various point acquisition process model use mal rule obviate need multiple lexicons grammar evaluate ability correctly diagnose agreement problems actual sentence produce asl native speakers
paper present result study semantic constraints impose lexical choice certain contextual indicators show indicators compute correlations choice noun phrase description name entity automatically establish use supervise learn base correlation develop technique automatic lexical choice descriptions entities text generation discuss underlie relationship pragmatics choose appropriate description serve specific purpose automatically generate text semantics description present work framework general concept reuse linguistic structure automatically extract large corpora present formal evaluation approach conclude thoughts potential applications method
interpret natural language discourse level useful accurately recognize dialogue act suggest identify speaker intentions research explore utility machine learn method call transformation base learn tbl compute dialogue act tbl number advantage alternative approach application identify extensions tbl necessary order address limitations original algorithm particular demand discourse process use monte carlo strategy increase applicability tbl method select feature utterances use input improve performance tbl system currently test verbmobil corpora speak dialogues produce promise preliminary result
introduce significant improvement relatively new machine learn method call transformation base learn apply monte carlo strategy randomly sample space rule rather exhaustively analyze possible rule drastically reduce memory time cost algorithm without compromise accuracy unseen data enable transformation base learn apply wider range domains effectively consider larger number different feature feature interactions data addition monte carlo improvement decrease labor demand human developer longer need develop minimal set rule templates maintain tractability
conversational implicatures usually describe license disobey flout principle cooperation however specification principle prove computationally elusive paper suggest useful concept rationality concept specify explicitely plan term argue speakers perform utterances part optimal plan particular communicative goals assumption use hearer infer conversational implicatures implicit speaker utterance
conversational implicatures usually describe license disobey flout principle speaker cooperative dialogue however work fail distinguish case speaker flout principle case speaker either deceptive hold mistake belief paper demonstrate three different case distinguish term beliefs ascribe speaker utterance argue act distinguish speaker intention ascribe beliefs intend inference make hearer theory implement viewgen pre exist belief model system use medical counsel domain
task recognize dialogue act apply transformation base learn tbl machine learn algorithm circumvent sparse data problem extract value well motivate feature utterances speaker direction punctuation mark new feature call dialogue act cue find effective cue phrase word n grams practice present strategies construct set dialogue act cue automatically minimize entropy distribution dialogue act train corpus filter irrelevant dialogue act cue cluster semantically relate word addition address limitations tbl introduce monte carlo strategy train efficiently committee method compute confidence measure ideas combine work implementation label hold data accurately report system dialogue act tag task
paper present result first attempt apply transformation base learn discourse level natural language process task address two limitations standard algorithm develop monte carlo version transformation base learn make method tractable wider range problems without degradation accuracy devise committee method assign confidence measure tag produce transformation base learn paper describe advance present experimental evidence transformation base learn effective alternative approach decision tree n grams discourse task call dialogue act tag argue transformation base learn desirable feature make particularly appeal dialogue act tag task
paper describe grapheme phoneme conversion method use phoneme connectivity ccv conversion rule method consist mainly four modules include morpheme normalization phrase break detection morpheme phoneme conversion phoneme connectivity check morpheme normalization replace non korean symbols standard korean graphemes phrase break detector assign phrase break use part speech pos information morpheme phoneme conversion module morpheme phrase convert phonetic pattern look morpheme phonetic pattern dictionary contain candidate phonological change boundaries morphemes graphemes within morpheme group ccv pattern convert phonemes ccv conversion rule phoneme connectivity table support grammaticality check adjacent two phonetic morphemes experiment corpus four thousand, nine hundred and seventy-three sentence achieve nine hundred and ninety-nine grapheme phoneme conversion performance nine hundred and seventy-five sentence conversion performance full korean tts system implement use conversion method
paper introduce methodology use basic phase follow develop catalan wordnet shich lexical resources employ build methodology well tool make use think general way could apply language
study present rely integrate use different kinds knowledge order improve first guess accuracy non word context sensitive correction general unrestricted texts state art spell correction systems eg ispell apart detect spell errors also assist user offer set candidate corrections close misspell word base correction proposals ispell build several guessers combine different ways firstly evaluate possibilities select best ones corpus artificially generate type errors secondly best combinations test texts genuine spell errors result latter suggest expect automatic non word correction errors free run text eighty precision single proposal ninety-eight time one hundred and two proposals average
recognize shallow linguistic pattern basic syntactic relationships word common task apply natural language text process common practice approach task tedious manual definition possible pattern structure often form regular expressions finite automata paper present novel memory base learn method recognize shallow pattern new text base bracket train corpus train data store efficient suffix tree data structure generalization perform line recognition time compare subsequences new text positive negative evidence corpus way information train lose happen learn systems construct single generalize model time train paper present experimental result recognize noun phrase subject verb verb object pattern english since learn approach enable easy port new domains plan apply syntactic pattern languages sub language pattern information extraction
paper describe method ask statistical question large text corpus exemplify method address question percentage federal register document real document possible interest text researcher analyst estimate answer question evaluate two hundred document select corpus forty-five thousand, eight hundred and twenty federal register document stratify sample use reduce sample uncertainty estimate three thousand, one hundred document fewer one thousand stratification base observe characteristics real document sample procedure incorporate bayesian version neyman allocation possible application method establish baseline statistics use estimate recall rat information retrieval systems
research automatic acquisition lexical information corpora start produce large scale computational lexicons contain data relative frequencies subcategorisation alternatives individual verbal predicate however empirical question whether type frequency information practice improve accuracy statistical parser yet answer paper describe experiment wide coverage statistical grammar parser english subcategorisation frequencies acquire ten million word text show information significantly improve parse accuracy
word sense disambiguation algorithms exceptions make use one lexical knowledge source describe system perform unrestricted word sense disambiguation content word free text combine different knowledge source semantic preferences dictionary definitions subject domain cod along part speech tag usefulness source optimise mean learn algorithm also describe creation new sense tag corpus combine exist resources test accuracy approach corpus exceed ninety-two demonstrate viability word disambiguation rather restrict oneself small sample
paper present method combine set unsupervised algorithms order accurately build large taxonomies machine readable dictionary mrd aim profit conventional mrds explicit semantic cod propose system one perform fully automatic exraction taxonomic link mrd entries two rank extract relations way selective manual refinement allow test accuracy reach around one hundred depend degree coverage select show taxonomy build limit structure dictionaries ldoce
paper summarise set methodologies techniques fast construction multilingual wordnets english wordnet use approach backbone catalan spanish wordnets lexical knowledge resource several subtasks
explore fully lexicalize tree adjoin grammar discourse take basic elements monologic discourse simply clauses larger structure anchor variously realize discourse cue link intra sentential grammar suggest account different pattern discourse cue different structure operations suggest three separate source elements discourse mean one compositional semantics tie basic tree operations two presuppositional semantics carry cue phrase freely adjoin tree three general inference draw additional defeasible conclusions flesh convey compositionally
propose model determine hearer attentional state depend solely list salient discourse entities list order among elements list cover also function backward look center center model rank criteria list base distinction hearer old hearer new discourse entities incorporate preferences inter intra sentential anaphora model basis algorithm operate incrementally word word
describe corpus base investigation proposals dialogue first describe dri compliant cod scheme report inter coder reliability result next test several hypotheses constitute well form proposal
focus production efficient descriptions object action events define type efficiency textual economy exploit hearer recognition inferential link material elsewhere within sentence textual economy lead efficient descriptions material support inferences include satisfy independent communicative goals therefore overload pollack sense argue achieve textual economy impose strong requirements representation reason use generate sentence representation must support generator simultaneous consideration syntax semantics reason must enable generator assess quickly reliably stage hearer interpret current sentence incomplete syntax semantics show representational reason requirements meet spud system sentence plan realization
present approach anaphora resolution base focus algorithm implement within exist muc message understand conference information extraction system allow quantitative evaluation substantial corpus annotate real world texts extensions basic focus mechanism easily test result refinements mechanism resolution rule result compare result simpler heuristic base approach
present result two methods assess event profile news article function verb type unique contribution research focus role verbs rather nouns two algorithms present evaluate one show accurately discriminate document type semantic properties ie event profile initial method use wordnet miller et al one thousand, nine hundred and ninety produce multiple cross classification article primarily due bushy nature verb tree couple sense disambiguation problem second approach use english verb class alternations evca levin one thousand, nine hundred and ninety-three show monosemous categorization frequent verbs wsj make possible usefully discriminate document example result show article communication verbs predominate tend opinion piece whereas article high percentage agreement verbs tend mergers legal case evaluation perform result use kendall tau present convince evidence use verb semantic class discriminant document classification
center theory posit discourse center distinguish discourse entity topic discourse simplify version theory develop dynamic semantics framework result system mechanism center shift allow simple elegant analysis variety phenomena involve sloppy identity ellipsis paycheck pronouns
address problem cluster word construct thesaurus base co occurrence data use acquire word class improve accuracy syntactic disambiguation view problem estimate joint probability distribution specify joint probabilities word pair noun verb pair propose efficient algorithm base minimum description length mdl principle estimate probability distribution method natural extension propose brown et al ninety-two li abe ninety-six overcome drawbacks retain advantage combine cluster method disambiguation method li abe ninety-five derive disambiguation method make use automatically construct thesauruses hand make thesaurus overall disambiguation accuracy achieve method eight hundred and fifty-two compare favorably accuracy eight hundred and twenty-four obtain state art disambiguation method brill resnik ninety-four
derivation step graph interpolation grammar effect scan input token feature aim emulate incrementality natural parser restrict formal power gigs contrast fact derivation mechanism involve context sensitive device similar tree adjunction tag combine effect input drive derivation restrict context sensitiveness would conceivably unfortunate turn graph interpolation languages subsume context free languages partially context sensitive report set examine relations cfgs gigs show gils proper superclass cfls also bring strong equivalence cfgs gigs class cfls thus lay basis meaningfully investigate amount context sensitiveness support gigs leave investigation research
paper describe partial parser assign syntactic structure sequence part speech tag program use maximum entropy parameter estimation method allow flexible combination different knowledge source hierarchical structure part speech phrasal categories effect parser go beyond simple bracket recognise even fairly complex structure give accuracy figure different applications parser
describe stochastic approach partial parse ie recognition syntactic structure limit depth technique utilise markov model go beyond usual bracket approach since capable recognise boundaries also internal structure syntactic category simple well complex np pp ap adverbials compare tag accuracy different applications encode scheme
paper report development annotation scheme annotation tool unrestricted german text representation format base argument structure also permit extraction kinds representations discuss several methodological issue analysis phenomena additional focus tool develop project applications
paper explore commonalities differences dachs variant dependency grammar lexical functional grammar dachs base traditional linguistic insights modern mathematical tool aim integrate different knowledge systems syntax semantics via couple abstract syntactic primitive dependency relation knowledge systems correspond rather closely projections lfg investigate commonalities arise usage projection approach theories point differences due incompatible linguistic premise main difference lfg lie motivation status dimension information cod argue lfg confound different information one projection prevent achieve good separation alternatives call motivation projection question
method present automatically augment bilingual lexicon exist machine translation system extract bilingual entries align bilingual text propose method rely resources already available mt system base use bilingual lexical templates match terminal symbols parse align sentence
present several unsupervised statistical model prepositional phrase attachment task approach accuracy best supervise methods task unsupervised approach use heuristic base attachment proximity train raw text annotate part speech tag morphological base form oppose attachment information therefore less resource intensive portable previous corpus base algorithms propose task present result prepositional phrase attachment english spanish
paper present early work animate talk head commentary system call bf byrnefootnotedavid byrne lead singer talk head goal project develop system take output robocup soccer simulator generate appropriate affective speech facial expressions base character personality emotional state state play describe system take pre analyse simulator output input generate text mark use speech generator face animation system make heavy use inter system standards future versions byrne able take advantage advance technologies incorporate
paper examine differences model different data drive systems perform nlp task exploit yield higher accuracy best individual system mean experiment involve task morpho syntactic wordclass tag four well know tagger generators hide markov model memory base transformation rule maximum entropy train corpus data comparison output combine use several vote strategies second stage classifiers combination taggers outperform best component best combination show one hundred and ninety-one lower error rate best individual tagger
present empirical study applicability probabilistic lexicalize tree insertion grammars pltig lexicalize counterpart probabilistic context free grammars pcfg problems stochastic natural language process compare performance pltigs non hierarchical n gram model pcfgs show pltig combine best aspects language model capability comparable n grams improve parse performance non lexicalize counterpart furthermore train pltigs display faster convergence pcfgs
classical vector space model text retrieval show give better result twenty-nine better experiment wordnet synsets choose index space instead word form result obtain manually disambiguate test collection query document derive semcor semantic concordance sensitivity retrieval performance automatic disambiguation errors index document also measure finally observe query disambiguate index synsets perform best good standard word index
parallel corpora valuable resource machine translation present availability utility limit genre domain specificity license restrictions basic difficulty locate parallel texts dominant world languages parallel corpus resource yet explore world wide web host abundance page parallel translation offer potential solution problems unique opportunities paper present necessary first step exploration method automatically find parallel translate document web technique conceptually simple fully language independent scalable preliminary evaluation result indicate method may accurate enough apply without human intervention
recent observations theory verse empirical metrics suggest construct verse line involve pattern match search source text number find elements complete word total specify number syllables give divide total number word mean number syllables per word source text paper make latter point explicit mathematically course demonstration show word length frequency total english output distribute geometrically previous researchers report adjust poisson distribution sequential distribution random global level significant non randomness fine structure data corpus two million word syllable count lexicon seventy-one thousand word form report pattern match theory show internally coherent observe analytic techniques describe form satisfactory test regular isometric lineation text
present work progress abstract dialog managers domain order implement dialog manager development tool take among data domain description input deliver new dialog manager describe domain output thereby focus two topics firstly construction domain descriptions description logics secondly interpretation utterances give domain
paper build earlier observations theory regard word length frequency sequential distribution develop mathematical characterization language feature distinguish isometrically lineated text unlineated text word feature distinguish isometrical verse prose show frequency syllables make complete word produce flat distribution prose verse exhibit peak line length position subsequent multiples position data several verse author present include detail mathematical analysis dynamics underlie peak creation comment offer process author construct line note word length sequence prose random whereas lineation necessitate non random word length sequence probable consequence introduce degree randomness otherwise highly order grammatical sequence addition observe effect ameliorate reduction mean word length text confirm earlier observations verse tend use shorter word use line vary core isometrical set frequency variant line show coincident frequency polysyllables suggest use variant line motivate polysyllabic word placement restrictive effect different line lengths relationship metrical restriction poetic effect general character metrical rule also discuss
determine attachments prepositions subordinate conjunctions key problem parse natural language paper present trainable approach make attachments transformation sequence error drive learn approach broad coverage account roughly three time attachment case previously handle corpus base techniques addition approach base simplify model syntax consistent practice current state art language process systems paper sketch syntactic algorithmic detail present experimental result data set derive penn treebank obtain attachment accuracy seven hundred and fifty-four general case first corpus base result report restrict case previously study corpus base methods approach yield accuracy comparable current work eight hundred and thirty-one
relatively free word order languages grammatical function intricately relate case mark assume order representation predicate argument structure work propose combinatory categorial grammar formulation relate surface case cue categories type correctly place arguments predicate argument structure achieve assign case markers gf encode categories unlike cg formulations type shift proliferate spurious ambiguity categories argument encode grammatical function follow principle category assignment normal order evaluation combinatory form reveal predicate argument structure application method turkish show
paper present multidimensional dependency grammar dg decouple dependency tree word order surface order determine traverse dependency tree develop notion emphword order domain structure link structurally dissimilar syntactic dependency tree discuss implementation dg use construct unification base phrase structure approach namely lexical functional grammar lfg particular attention give analysis discontinuities dg term lfg functional uncertainty
paper present trainable methods generate letter sound rule give lexicon use pronounce vocabulary word method lexicon compression relationship string letter string phonemes represent pronunciation many languages trivial discuss two alignment procedures one fully automatic one hand seed produce reasonable alignments letter phone top induction tree model train align entries show combine phoneme stress prediction better separate prediction process still better include model last phonemes transcribe part speech information lexicons test model word accuracy include stress seventy-eight oald sixty-two cmu ninety-four brulex extremely high score train set allow substantial size reductions one twenty www site http tctsfpmsacbe synthesis mbrdico
argue learn first language baby use series small clue aid recognition comprehension one clue word length paper present statistical part speech tagger train solely number letter word sentence
paper propose decouple dependency tree word order surface order determine traverse dependency tree develop notion emphword order domain structure link structurally dissimilar syntactic dependency tree proposal result lexicalize declarative formally precise description word order feature lack previous proposals dependency grammars contrary lexicalize approach word order proposal require lexical ambiguities order alternatives
multiple default inheritance formalisms lexicons attract much interest recent years propose new efficient method access lexicons show two basic strategies lookup inheritance lexicons compromise develop combine large degree practical point view advantage strategies avoid disadvantage method kind line partial evaluation make subset inherit information explicit use lexicon identify part lexicon evaluate show partial evaluation work inheritance lexicons finally theoretical result confirm complete implementation speedups factor ten one hundred reach
radio speech corpus 9mn prosodically mark phonetician expert non expert listeners corpus large enough train test automatic boundary spot system namely time delay neural network feed f0 value vowels pseudo syllable durations result validate prosodic mark automatic spot prosodic events
find simple non recursive base noun phrase important subtask many natural language process applications previous empirical methods base np identification rather complex paper instead propose simple algorithm tailor relative simplicity task particular present corpus base approach find base nps match part speech tag sequence train phase algorithm base two successful techniques first base np grammar read treebank corpus grammar improve select rule high benefit score use simple algorithm naive heuristic match rule achieve surprise accuracy evaluation penn treebank wall street journal
paper provide account generate sentence coordination constructions clause size semantic representations algorithm develop generate sentence ellipsis gap right node raise non constituent coordination constructions various examples linguistic literature use demonstrate algorithm job well
earley algorithm widely use parse method natural language process applications introduce variant earley parse base delay recognition constituents allow us start recognition constituent case subconstituents find within input string particularly advantageous several case partial analysis constituent complete general case productions share suffix right hand side even different leave hand side nonterminals although two algorithms result asymptotic time space complexity practical perspective algorithm improve time space requirements original method show report experimental result
aim paper define dependency grammar framework linguistically motivate computationally parsable see demo http wwwconexorfi analysershtmltesting
purpose paper explore semantic problems relate use linguistic ontologies information systems suggest organize principles aim solve problems taxonomic structure current ontologies unfortunately quite complicate hard understand especially concern upper level focus problem isa overload believe main responsible difficulties purpose carefully analyze ontological nature categories use current upper level structure consider necessity split accord subtle distinctions opportunity exclude limit organizational role
paper present result use roget international thesaurus taxonomy semantic similarity measurement task four similarity metrics take literature apply roget experimental evaluation suggest traditional edge count approach surprisingly well correlation r088 benchmark set human similarity judgements upper bind r090 human subject perform task
internal need well commercial purpose cdc group produce several nlp base line contentware applications years development process applications subject numerous constraints quality service integration new advance nlp direct reactions users continuous versioning short delivery deadlines cost control follow industrial commercial experience malleability applications openness towards foreign components efficiency applications ease exploitation appear key point paper describe tallab powerful architecture line contentware fulfil requirements
disc project aim build depth understand state art speak language dialogue systems sldss components development evaluation purpose b develop first best practice methodology field methodology accompany c series development evaluation support tool limit extent possible within duration project draft versions methodology tool test slds developers industry research e package best suit need first year disc accomplish b c start proposal complete work propose add twelve months eighteen months present project submit esprit long term research march one thousand, nine hundred and ninety-eight
phrase structure tree hierarchical structure many subject notably taxonomy tree structure study use ultrametrics syntactical hierarchical phrase tree subject similar analysis much siompler branch structure readily discernible switch occurence hierarchical structure elsewhere linguistics mention phrase tree represent matrix elements matrix represent triangles height branch occur prescribe previous syntatic model use ultrametric matrix ambiguity branch height choose resolve postulate branch occur lowest height available ultrametric produce measure complexity sentence presumably complexity sentence increase language aquired test ultrametric triangles equilateral isocles show x structur imply equilateral triangles restrict attention simple syntax minium ultrametric distance lexical categories calculatex ultrametric distance show different matrix obtasined feaures show definition c commabnd replace equivalent ultrametric definition new definition invoke minimum distance nod aesthetically satisfing previouv varieties definitions new definition c command follow new definition government
large class machine learn problems natural language require characterization linguistic context two characteristic properties problems feature space high dimensionality target concepts refer small subset feature space condition multiplicative weight update algorithms winnow show exceptionally good theoretical properties present algorithm combine variants winnow weight majority vote apply problem aforementioned class context sensitive spell correction task fix spell errors happen result valid word substitute casual causal etc evaluate algorithm winspell compare bayspell statistics base method represent state art task find one run full unpruned set feature winspell achieve accuracies significantly higher bayspell able achieve either prune unpruned condition two compare systems literature winspell exhibit highest performance three primary reason winspell outperform bayspell winspell learn better linear separator four run test set draw different corpus train set draw winspell better able bayspell adapt use strategy present combine supervise learn train set unsupervised learn noisy test set
common method make theory understandable compare another theory better develop radical interpretation theory attempt explain communication mean radical interpretation treat another time dependent theory compare time dependent theory biological evolution main reason find nature time dependence produce analogs two theories necessary prerequisite bring many problems nature time dependence better know might allow underlie mechanism uncover several similarities differences uncover appear differences similarities
key problem text summarization find salience function determine information source include summary paper describe use machine learn train corpus document abstract discover salience function describe combination feature optimal give summarization task method address generic user focus summaries
analyze commonly use statistics base machine learn algorithms natural language disambiguation task observe cast learn linear separators feature space methods make priori assumptions employ give data search hypothesis nevertheless show search space rich space linear separators use build argument data drive approach merely search good linear separator feature space without assumptions domain specific problem present approach sparse network linear separators utilize winnow learn algorithm show use variety ambiguity resolution problems learn approach present attribute efficient therefore appropriate domains large number attribute particular present extensive experimental comparison approach methods several well study lexical disambiguation task context sensitive spell correction prepositional phrase attachment part speech tag case show approach either outperform methods try task perform comparably best
standard linguistic analysis syntax use model model require order structure structure lf representations movement alter order constituent word movement achieve use principles parameters syntactic theory psychological serial model accommodate model immediately new model call p model introduce argue lf representation replace variant frege three qualities f representation order elements necessarily lf suggest correct order f representation structure structure within framework movement originate outcome emphasis apply sentence
show language learn contrary receive wisdom keep exceptional train instance memory beneficial generalization accuracy investigate phenomenon empirically selection benchmark natural language process task grapheme phoneme conversion part speech tag prepositional phrase attachment base noun phrase chunk first series experiment combine memory base learn train set edit techniques instance edit base typicality class prediction strength result show edit exceptional instance low typicality low class prediction strength tend harm generalization accuracy second series experiment compare memory base learn decision tree learn methods selection task find decision tree learn often perform worse memory base learn moreover decrease performance link degree abstraction exceptions ie prune eagerness provide explanations result term properties natural language process task learn algorithms
many applications natural language process nlp necessary determine likelihood give word combination example speech recognizer may need determine two word combinations eat peach eat beach likely statistical nlp methods determine likelihood word combination frequency train corpus however nature language many word combinations infrequent occur give corpus work propose method estimate probability previously unseen word combinations use available information similar word describe probabilistic word association model base distributional word similarity apply two task language model pseudo word disambiguation language model task similarity base model use improve probability estimate unseen bigrams back language model similarity base method yield twenty perplexity improvement prediction unseen bigrams statistically significant reductions speech recognition error also compare four similarity base estimation methods back maximum likelihood estimation methods pseudo word sense disambiguation task control unigram bigram frequency avoid give much weight easy disambiguate high frequency configurations similarity base methods perform forty better particular task
argue colour name strategy object name strategy chunk strategy memory aspects general phenomena call stereotype point berlin kay universal partial order colour frequency traffic accidents classify colour surprisingly similar consequences existence name strategy philosophy language mathematics discuss argue real value quantities occur ab initio implication real value truth quantities bf continuum hypothesis pure mathematics side step existence name strategy show think sememes talk phonemes separate vindicate assumption think occur talk use psycholinguistic speech production model