paper outline approach build graph base reverse dictionaries use word definitions reverse dictionary take phrase input output list word semantically similar phrase solution tip tongue problem use distance base similarity measure compute graph assess similarity word input phrase compare performance approach onelook reverse dictionary distributional semantics method base word2vec show approach much better distributional semantics method good onelook 3k lexicon simple approach set new performance baseline reverse dictionaries
phrase base statistical machine translation smt systems previously use task grammatical error correction gec achieve state art accuracy superiority smt systems come ability learn text transformations erroneous correct text without explicitly model error type however phrase base smt systems suffer limitations discrete word representation linear map lack global context paper address limitations use two different yet complementary neural network model namely neural network global lexicon model neural network joint model neural network generalize better use continuous space representation word learn non linear mappings moreover leverage contextual information source sentence effectively add two components achieve statistically significant improvement accuracy grammatical error correction state art gec system
grammatical error correction gec task detect correct grammatical errors texts write second language learners statistical machine translation smt approach gec sentence write second language learners translate grammatically correct sentence achieve state art accuracy however smt approach unable utilize global context paper propose novel approach improve accuracy gec exploit n best hypotheses generate smt approach specifically build classifier score edit n best hypotheses classifier use select appropriate edit rank n best hypotheses apply methods state art gec system use smt approach experiment show methods achieve statistically significant improvements accuracy best publish result benchmark test dataset gec
syntactic parsers perform poorly prediction argument cluster coordination acc change ptb representation acc suitable learn statistical pcfg parser affect one hundred and twenty-five tree train set train modify tree yield slight improvement evalb score section twenty-two twenty-three main evaluation corpus 4th grade science exams acc structure prevalent corpus obtain impressive x27 improvement recover acc structure compare parser train original ptb tree
language model lms statistical model calculate probabilities sequence word discrete symbols currently two major paradigms language model exist count base n gram model advantage scalability test time speed neural lms often achieve superior model performance demonstrate varieties model unify single model framework define set probability distributions vocabulary word dynamically calculate mixture weight distributions formulation allow us create novel hybrid model combine desirable feature count base neural lms experiment demonstrate advantage approach
morphological reinflection task generate target form give source form source tag target tag propose new way model task neural encoder decoder model approach reduce amount require train data architecture achieve state art result make encoder decoder model applicable morphological reinflection even low resource languages present new automatic correction method output base edit tree
paper propose lexvec new method generate distribute word representations use low rank weight factorization positive point wise mutual information matrix via stochastic gradient descent employ weight scheme assign heavier penalties errors frequent co occurrences still account negative co occurrence evaluation word similarity analogy task show lexvec match often outperform state art methods many task
present method generate synthetic versions twitter data use neural generative model goal protect individuals source data stylometric identification attack still release data carry research value specifically generate tweet corpora maintain user level word distributions augment neural language model user specific components compare approach two standard text data protection methods redaction iterative translation evaluate three methods measure risk utility define risk follow stylometric model identification define utility base two general word distribution measure two common text analysis research task find neural model able significantly lower risk previous methods little cost utility also demonstrate neural model allow data providers actively control risk utility trade model tune parameters work present promise result new tool address problem privacy free text share social media data way respect privacy ethically responsible
various treebanks release dependency parse despite treebanks may belong different languages different annotation scheme contain syntactic knowledge potential benefit paper present universal framework exploit multi type treebanks improve parse deep multi task learn consider two kinds treebanks source multilingual universal treebanks monolingual heterogeneous treebanks multiple treebanks train jointly interact multi level parameter share experiment several benchmark datasets various languages demonstrate approach make effective use arbitrary source treebanks improve target parse model
paper take state art model distribute word representation explicitly factorize positive pointwise mutual information ppmi matrix use window sample negative sample address two shortcomings improve syntactic performance use positional contexts solve need store ppmi matrix memory work aggregate data external memory effectiveness modifications show use word similarity analogy task
long stand challenge coreference resolution incorporation entity level information feature define cluster mention instead mention pair present neural network base coreference system produce high dimensional vector representations pair coreference cluster use representations system learn combine cluster desirable train system learn search algorithm teach local decisions cluster merge lead high score final coreference partition system substantially outperform current state art english chinese portion conll two thousand and twelve share task dataset despite use hand engineer feature
work investigate several neural network architectures fine grain entity type classification particularly consider extensions recently propose attentive neural architecture make three key contributions previous work attentive neural architectures consider hand craft feature combine learn hand craft feature observe complement additionally quantitative analysis establish attention mechanism capable learn attend syntactic head phrase contain mention know strong hand craft feature task enable parameter share hierarchical label encode method low dimensional projections show clear cluster type hierarchy lastly despite use evaluation dataset literature frequently compare model train use different data establish choice train data drastic impact performance decrease much nine hundred and eighty-five loose micro f1 score previously propose method despite best model achieve state art result seven thousand, five hundred and thirty-six loose micro f1 score well establish figer gold dataset
submit two systems semeval two thousand and sixteen task twelve clinical tempeval challenge participate phase one identify text span time event expressions clinical note phase two predict relation event parent document creation time temporal entity extraction find joint inference base approach use structure prediction outperform vanilla recurrent neural network incorporate word embeddings train variety large clinical document set document creation time relations find combination date canonicalization distant supervision rule predict relations events time expressions improve classification though gain limit likely due small scale train data
recent neural model dialogue generation offer great promise generate responses conversational agents tend shortsighted predict utterances one time ignore influence future outcomes model future direction dialogue crucial generate coherent interest dialogues need lead traditional nlp model dialogue draw reinforcement learn paper show integrate goals apply deep reinforcement learn model future reward chatbot dialogue model simulate dialogues two virtual agents use policy gradient methods reward sequence display three useful conversational properties informativity non repetitive turn coherence ease answer relate forward look function evaluate model diversity length well human judge show propose algorithm generate interactive responses manage foster sustain conversation dialogue simulation work mark first step towards learn neural conversational model base long term success dialogues
discourse coherence strongly associate text quality make important natural language generation understand yet exist model coherence focus measure individual aspects coherence lexical overlap rhetorical structure entity center narrow domains paper describe domain independent neural model discourse coherence capable measure multiple aspects coherence exist sentence maintain coherence generate new sentence study discriminative model learn distinguish coherent incoherent discourse generative model produce coherent text include novel neural latent variable markovian generative model capture latent discourse dependencies sentence text work achieve state art performance multiple coherence evaluations mark initial step generate coherent texts give discourse contexts
exist approach zero pronoun resolution heavily rely annotate data often release share task organizers therefore lack annotate data become major obstacle progress zero pronoun resolution task also expensive spend manpower label data better performance alleviate problem paper propose simple novel approach automatically generate large scale pseudo train data zero pronoun resolution furthermore successfully transfer cloze style read comprehension neural network model zero pronoun resolution task propose two step train mechanism overcome gap pseudo train data real one experimental result show propose approach significantly outperform state art systems absolute improvements thirty-one f score ontonotes fifty data
recent years great success achieve sentiment classification english thank part availability copious annotate resources unfortunately languages enjoy abundance label data tackle sentiment classification problem low resource languages without adequate annotate data propose adversarial deep average network adan transfer knowledge learn label data resource rich source language low resource languages unlabeled data exist adan two discriminative branch sentiment classifier adversarial language discriminator branch take input share feature extractor learn hide representations simultaneously indicative classification task invariant across languages experiment chinese arabic sentiment classification demonstrate adan significantly outperform state art systems
introduce recurrent neural network language model rnn lm long short term memory lstm units utilize character level word level input model gate adaptively find optimal mixture character level word level input gate create final vector representation word combine two distinct representations word character level input convert vector representations word use bidirectional lstm word level input project another high dimensional space word lookup table final vector representations word use lstm language model predict next word give precede word model gate mechanism effectively utilize character level input rare vocabulary word outperform word level language model several english corpora
paper propose phrasenet neural machine translator phrase memory store phrase pair symbolic form mine corpus specify human experts give source sentence phrasenet scan phrase memory determine candidate phrase pair integrate tag information representation source sentence accordingly decoder utilize mixture word generate component phrase generate component specifically design strategy generate sequence multiple word phrasenet approach one step towards incorporate external knowledge neural machine translation also make effort extend word word generation mechanism recurrent neural network empirical study chinese english translation show carefully choose phrase table memory phrasenet yield three hundred and forty-five bleu improvement generic neural machine translator
propose simple neural architecture natural language inference approach use attention decompose problem subproblems solve separately thus make trivially parallelizable stanford natural language inference snli dataset obtain state art result almost order magnitude fewer parameters previous work without rely word order information add intra sentence attention take minimum amount order account yield improvements
infer implicit discourse relations natural language text difficult subtask discourse parse surface feature achieve good performance readily applicable languages without semantic lexicons previous neural model require parse surface feature small label set work well propose neural network model base feedforward long short term memory architecture without surface feature surprise best configure feedforward architecture outperform lstm base model case despite thorough tune various fine grain label set cross linguistic set feedforward model perform consistently better least well systems require hand craft surface feature model present first neural chinese discourse parser style chinese discourse treebank show result hold cross linguistically
enable computers automatically answer question like create character harry potter carefully build knowledge base provide rich source facts however remain challenge answer factoid question raise natural language due numerous expressions one question particular focus common question ones answer single fact knowledge base propose cfo conditional focus neural network base approach answer factoid question knowledge base approach first zoom question find probable candidate subject mention infer final answer unify conditional probabilistic framework power deep recurrent neural network neural embeddings propose cfo achieve accuracy seven hundred and fifty-seven dataset 108k question largest public one date outperform current state art absolute margin one hundred and eighteen
propose enhance rnn decoder neural machine translator nmt external memory natural powerful extension state decode rnn memory enhance rnn decoder call textscmemdec time decode textscmemdec read memory write memory content base address unlike unbounded memory previous workciternnsearch store representation source sentence memory textscmemdec matrix pre determine size design better capture information important decode process time step empirical study chinese english translation show improve forty-eight bleu upon groundhog fifty-three bleu upon moses yield best performance achieve train set
neural machine translation nmt often make mistake translate low frequency content word essential understand mean sentence propose method alleviate problem augment nmt systems discrete translation lexicons efficiently encode translations low frequency word describe method calculate lexicon probability next word translation candidate use attention vector nmt model select source word lexical probabilities model focus test two methods combine probability standard nmt probability one use bias two linear interpolation experiment two corpora show improvement twenty twenty-three bleu thirteen forty-four nist score faster convergence time
investigate potential attention base neural machine translation simultaneous translation introduce novel decode algorithm call simultaneous greedy decode allow exist neural machine translation model begin translate full source sentence receive approach unique previous work simultaneous translation segmentation translation do jointly maximize translation quality translate segment strongly condition previous segment paper present first step toward build full simultaneous translation system base neural machine translation
alignment link give english sentence abstract mean representation amr graph amr annotation automatic alignment become indispensable train amr parser previous study formalize string string problem solve unsupervised way suffer data sparseness due small size train data english amr alignment paper formalize syntax base alignment problem solve supervise manner base syntax tree address data sparseness problem generalize english amr tokens syntax tag experiment verify effectiveness propose method english amr alignment also amr parse
present epireader novel model machine comprehension text machine comprehension unstructured real world text major research goal natural language process current test machine comprehension pose question whose answer infer support text evaluate model response question epireader end end neural model comprise two components first component propose small set candidate answer compare question support text second component formulate hypotheses use propose candidates question reranks hypotheses base estimate concordance support text present experiment demonstrate epireader set new state art cnn children book test machine comprehension benchmarks outperform previous neural model significant margin
describe search algorithm optimize number latent state estimate latent variable pcfgs spectral methods result show contrary common belief number latent state nonterminal l pcfg decide isolation spectral methods parse result significantly improve number latent state nonterminal globally optimize take account interactions different nonterminals addition contribute empirical analysis spectral algorithms eight morphologically rich languages basque french german hebrew hungarian korean polish swedish result show estimation consistently perform better close coarse fine expectation maximization techniques languages
paper connect vector base composition model formal semantics dependency base compositional semantics dcs show theoretical evidence vector compositions model conform logic dcs experimentally show vector base composition bring strong ability calculate similar phrase similar vectors achieve near state art wide range phrase similarity task relation classification meanwhile dcs guide build vectors structure query directly execute evaluate utility sentence completion task report new state art
present defext easy use semi supervise definition extraction tool defext design extract target corpus textual fragment term explicitly mention together core feature ie definition work back conditional random field base sequential label algorithm bootstrapping approach bootstrapping enable model gradually become aware idiosyncrasies target corpus paper describe main components toolkit well experimental result stem automatic manual evaluation release defext open source along necessary file run unix machine also provide access train test data immediate use
coordination important common syntactic construction handle well state art parsers coordinations penn treebank miss internal structure many case include explicit mark conjuncts contain various errors inconsistencies work initiate manual annotation process solve issue identify different elements coordination phrase label element function add phrase boundaries miss unify inconsistencies fix errors outcome extension ptb include consistent detail structure coordinations make coordination annotation publicly available hope facilitate research coordination disambiguation
paper present joint model perform unsupervised morphological analysis word learn character level composition function morphemes word embeddings model split individual word segment weight segment accord ability predict context word morphological analysis comparable dedicate morphological analyzers task morpheme boundary recovery also perform better word base embed model task syntactic analogy answer finally show incorporate morphology explicitly character level model help produce embeddings unseen word correlate better human judgments
seek address lack label data high cost annotation textual entailment domains end first create experimental purpose entailment dataset clinical domain highly competitive supervise entailment system ent effective box two domains explore self train active learn strategies address lack label data self train successfully exploit unlabeled data improve ent fifteen f score newswire domain thirteen f score clinical data hand active learn experiment demonstrate match even beat ent use sixty-six train data clinical domain fifty-eight train data newswire domain
neural machine translation become major alternative widely use phrase base statistical machine translation notice however much research neural machine translation focus european languages despite language agnostic nature paper apply neural machine translation task arabic translation ar en compare standard phrase base translation system run extensive comparison use various configurations preprocessing arabic script show phrase base neural translation systems perform comparably proper preprocessing arabic script similar effect systems however observe neural machine translation significantly outperform phrase base system domain test set make attractive real world deployment
study problem generate abstractive summaries opinionated text propose attention base neural network model able absorb information multiple text units construct informative concise fluent summaries importance base sample method design allow encoder integrate information important subset input automatic evaluation indicate system outperform state art abstractive extractive summarization systems two newly collect datasets movie review arguments system summaries also rat informative grammatical human evaluation
word sentiment depend domain use computational social science research thus require sentiment lexicons specific domains study combine domain specific word embeddings label propagation framework induce accurate domain specific sentiment lexicons use small set seed word achieve state art performance competitive approach rely hand curated resources use framework perform two large scale empirical study quantify extent sentiment vary across time communities induce release historical sentiment lexicons one hundred and fifty years english community specific sentiment lexicons two hundred and fifty online communities social media forum reddit historical lexicons show five sentiment bear non neutral english word completely switch polarity last one hundred and fifty years community specific lexicons highlight sentiment vary drastically different communities
word shift mean many reason include cultural factor like new technologies regular linguistic process like subjectification understand evolution language culture require disentangle underlie cause show two different distributional measure use detect two different type semantic change first measure use many previous work analyze global shift word distributional semantics sensitive change due regular process linguistic drift semantic generalization promise promise promise excite second measure develop focus local change word nearest semantic neighbor sensitive cultural shift change mean cell prison cell cell phone compare measurements make two methods allow researchers determine whether change cultural linguistic nature distinction essential work digital humanities historical linguistics
participate wmt two thousand and sixteen share news translation task build neural translation systems four language pair train directions english czech english german english romanian english russian systems base attentional encoder decoder use bpe subword segmentation open vocabulary translation fix vocabulary experiment use automatic back translations monolingual news corpus additional train data pervasive dropout target bidirectional model report methods give substantial improvements see improvements forty-three one hundred and twelve bleu baseline systems human evaluation systems tie best constrain system seven eight translation directions participate
neural machine translation recently achieve impressive result use little way external linguistic information paper show strong learn capability neural mt model make linguistic feature redundant easily incorporate provide improvements performance generalize embed layer encoder attentional encoder decoder architecture support inclusion arbitrary feature addition baseline word feature add morphological feature part speech tag syntactic dependency label input feature english german english romanian neural machine translation systems experiment wmt16 train test set find linguistic input feature improve model quality accord three metrics perplexity bleu chrf3 open source implementation neural mt system available sample file configurations
directly read document able answer question unsolved challenge avoid inherent difficulty question answer qa direct towards use knowledge base kbs instead prove effective unfortunately kbs often suffer restrictive schema support certain type answer sparse eg wikipedia contain much information freebase work introduce new method key value memory network make read document viable utilize different encode address output stag memory read operation compare use kbs information extraction wikipedia document directly single framework construct analysis tool wikimovies qa dataset contain raw text alongside preprocessed kb domain movies method reduce gap three settings also achieve state art result exist wikiqa benchmark
paper explore problem document summarization persian language two distinct angle first approach modify popular widely cite persian document summarization framework see work realistic corpus news article human evaluation generate summaries show graph base methods perform better modify systems carry intuition forward second approach probe deeper nature graph base systems design several summarizers base centrality measure ad hoc evaluation use rouge score summarizers suggest small class centrality measure perform better three strong unsupervised baselines
psdvec python perl toolbox learn word embeddings ie map word natural language continuous vectors encode semantic syntactic regularities word psdvec implement word embed learn method base weight low rank positive semidefinite approximation scale learn process implement blockwise online learn algorithm learn embeddings incrementally strategy greatly reduce learn time word embeddings large vocabulary learn embeddings new word without learn whole vocabulary nine word similarity analogy benchmark set two natural language process nlp task psdvec produce embeddings best average performance among popular word embed tool psdvec provide new option nlp practitioners
work focus answer single relation factoid question freebase question acquire answer single fact form subject predicate object freebase task simple question answer simpleqa address via two step pipeline entity link fact selection fact selection match subject entity fact candidate entity mention question character level convolutional neural network char cnn match predicate fact question word level cnn word cnn work make two main contributions simple effective entity linker freebase propose entity linker outperform state art entity linker simpleqa task ii novel attentive maxpooling stack word cnn predicate representation match predicate focus question representation effectively experiment show system set new state art task
propose framework improve performance distantly supervise relation extraction jointly learn solve two relate task concept instance extraction relation extraction combine novel use document structure small well structure corpora section identify correspond relation arguments distantly label examples section tend good precision use seed extract additional relation examples apply label propagation graph compose noisy examples extract large unstructured test corpus combine soft constraint concept examples type second argument relation get significant improvements several state art approach distantly supervise relation extraction
model crisp logical regularities crucial semantic parse make difficult neural model task specific prior knowledge achieve good result paper introduce data recombination novel framework inject prior knowledge model train data induce high precision synchronous context free grammar capture important conditional independence properties commonly find semantic parse train sequence sequence recurrent network rnn model novel attention base copy mechanism datapoints sample grammar thereby teach model structural properties data recombination improve accuracy rnn model three semantic parse datasets lead new state art performance standard geoquery dataset model comparable supervision
natural language generation play critical role speak dialogue systems present new approach natural language generation task orient dialogue use recurrent neural network encoder decoder framework contrast previous work model use lexicalize delexicalized components ie slot value pair dialogue act slot correspond value align together allow model learn available data include slot value pair rather restrict delexicalized slot show help model generate natural sentence better grammar improve model performance transfer weight learn pretrained sentence auto encoder human evaluation best perform model indicate generate sentence users find appeal
morphosyntactic lexicons word vector representations prove useful improve accuracy statistical part speech taggers compare performances four systems datasets cover sixteen languages two systems feature base memms crfs two neural base bi lstms show average four approach perform similarly reach state art result yet better performances obtain feature base model lexically richer datasets eg morphologically rich languages whereas neural base result higher datasets less lexical variability eg english conclusions hold particular memm model rely system melt benefit newly design feature show certain condition feature base approach enrich morphosyntactic lexicons competitive respect neural methods
production color language essential ground language generation color descriptions many challenge properties vague compositionally complex denotationally rich present effective approach generate color descriptions use recurrent neural network fourier transform color representation model outperform previous work conditional language model task large corpus naturalistic color descriptions addition probe model output reveal accurately produce basic color term also descriptors non convex denotations greenish bare modifiers bright dull compositional phrase fade teal see train
paper propose novel finetuning algorithm recently introduce multi way mulitlingual neural machine translate enable zero resource machine translation use together novel many one translation strategies empirically show finetuning algorithm allow multi way multilingual model translate zero resource language pair one well single pair neural translation model train 1m direct parallel sentence language pair two better pivot base translation strategy keep one additional copy attention relate parameters
propose new active learn al method text classification convolutional neural network cnns al one select instance manually label aim maximize model performance minimal effort neural model capitalize word embeddings representations feature tune task hand argue al strategies multi layer neural model focus select instance affect embed space ie induce discriminative word representations contrast traditional al approach eg entropy base uncertainty sample specify higher level objectives propose simple approach sentence classification select instance contain word whose embeddings likely update greatest magnitude thereby rapidly learn discriminative task specific embeddings extend approach document classification jointly consider one expect change constituent word representations two model current overall uncertainty regard instance relative emphasis place criteria govern stochastic process favor select instance likely improve representations outset learn shift toward general uncertainty sample al progress empirical result show method outperform baseline al approach sentence document classification task also show expect method quickly learn discriminative word embeddings best knowledge first work al address neural model text classification
morphologically rich languages often lack annotate linguistic resources require develop accurate natural language process tool propose model suitable train morphological taggers rich tagsets low resource languages without use direct supervision approach extend exist approach project part speech tag across languages use bitext infer constraints possible tag give word type token propose tag model use wsabie discriminative embed base model rank base learn evaluation eleven languages average model perform par baseline weakly supervise hmm scalable multilingual experiment show method perform best project relate language pair despite inherently lossy projection show morphological tag predict model improve downstream performance parser six las average
previous approach chinese word segmentation formalize problem character base sequence label task contextual information within fix size local windows simple interactions adjacent tag capture paper propose novel neural framework thoroughly eliminate context windows utilize complete segmentation history model employ gate combination neural network character produce distribute representations word candidates give long short term memory lstm language score model experiment benchmark datasets show without help feature engineer exist approach model achieve competitive better performances previous state art methods
paper describe georgia tech team approach conll two thousand and sixteen supplementary evaluation discourse relation sense classification use long short term memories lstm induce distribute representations argument combine representations surface feature neural network architecture neural network determine bayesian hyperparameter search
end end neural machine translation nmt make remarkable progress recently nmt systems rely parallel corpora parameter estimation since parallel corpora usually limit quantity quality coverage especially low resource languages appeal exploit monolingual corpora improve nmt propose semi supervise approach train nmt model concatenation label parallel corpora unlabeled monolingual corpora data central idea reconstruct monolingual corpora use autoencoder source target target source translation model serve encoder decoder respectively approach exploit monolingual corpora target language also source language experiment chinese english dataset show approach achieve significant improvements state art smt nmt systems
introduce agreement base approach learn parallel lexicons phrase non parallel corpora basic idea encourage two asymmetric latent variable translation model ie source target target source agree identify latent phrase word alignments agreement define word phrase level develop viterbi algorithm jointly train two unidirectional model efficiently experiment chinese english dataset show agreement base learn significantly improve alignment translation performance
present siamese continuous bag word siamese cbow model neural network efficient estimation high quality sentence embeddings average embeddings word sentence prove surprisingly successful efficient way obtain sentence embeddings however word embeddings train methods currently available optimize task sentence representation thus likely suboptimal siamese cbow handle problem train word embeddings directly purpose average underlie neural network learn word embeddings predict sentence representation surround sentence show robustness siamese cbow model evaluate twenty datasets stem wide variety source
interlingua base machine translation mt aim encode multiple languages common linguistic representation decode sentence multiple target languages representation work explore idea context neural encoder decoder architectures albeit smaller scale without mt end goal specifically consider case three languages modalities x z wherein interest generate sequence start information available x however parallel train data available x train data available x z z often case many real world applications z thus act pivot bridge obvious solution perhaps less elegant work well practice train two stage model first convert x z z instead explore interlingua inspire solution jointly learn follow encode x z common representation ii decode common representation evaluate model two task bridge transliteration ii bridge caption report promise result applications believe right step towards truly interlingua inspire encoder decoder architectures
word embeddings play significant role many modern nlp systems since learn one representation per word problematic polysemous word homonymous word researchers propose use one embed per word sense approach mainly train word sense embeddings corpus paper propose use word sense definitions learn one embed per word sense experimental result word similarity task word sense disambiguation task show word sense embeddings produce approach high quality
paper propose investigate novel end end method automatically generate short email responses call smart reply generate semantically diverse suggestions use complete email responses one tap mobile system currently use inbox gmail responsible assist ten mobile responses design work high throughput process hundreds millions message daily system exploit state art large scale deep learn describe architecture system well challenge face build like response diversity scalability also introduce new method semantic cluster user generate content require modest amount explicitly label data
paper present university cambridge submission wmt16 motivate complementary nature syntactical machine translation neural machine translation nmt exploit synergies hiero nmt different combination scheme start simple neural lattice rescoring approach show hiero lattices often narrow nmt ensembles therefore instead hard restriction nmt search space lattice propose loosely couple nmt hiero composition modify version edit distance transducer loose combination outperform lattice rescoring especially use multiple nmt systems ensemble
first order factoid question answer assume question answer single fact knowledge base kb seem like challenge task many recent attempt apply either complex linguistic reason deep neural network achieve sixty-five seventy-six accuracy benchmark set approach formulate task two machine learn problems detect entities question classify question one relation type kb train recurrent neural network solve problem simplequestions dataset approach yield substantial improvements previously publish result even neural network base much complex architectures simplicity approach also practical advantage efficiency modularity valuable especially industry set fact present preliminary analysis performance model real query comcast x1 entertainment platform millions users every day
present stanford question answer dataset squad new read comprehension dataset consist one hundred thousand question pose crowdworkers set wikipedia article answer question segment text correspond read passage analyze dataset understand type reason require answer question lean heavily dependency constituency tree build strong logistic regression model achieve f1 score five hundred and ten significant improvement simple baseline twenty however human performance eight hundred and sixty-eight much higher indicate dataset present good challenge problem future research dataset freely available https stanford qacom
consider task learn context dependent map utterances denotations denotations train time must search combinatorially large space logical form even larger context dependent utterances cope challenge perform successive projections full model onto simpler model operate equivalence class logical form though less expressive find simpler model much faster surprisingly effective moreover use bootstrap full model finally collect three new context dependent semantic parse datasets develop new leave right parser
conventional word sense induction wsi methods usually represent instance discrete linguistic feature cooccurrence feature train model polysemous word individually work propose learn sense embeddings wsi task train stage method induce several sense centroids embed polysemous word test stage method represent instance contextual vector induce sense find nearest sense centroid embed space advantage method one distribute sense vectors take knowledge representations train discriminatively usually better performance traditional count base distributional model two general model whole vocabulary jointly train induce sense centroids mutlitask learn framework evaluate semeval two thousand and ten wsi dataset method outperform participants recent state art methods verify two advantage compare carefully design baselines
present natural language generator base sequence sequence approach train produce natural language string well deep syntax dependency tree input dialogue act use directly compare two step generation separate sentence plan surface realization stag joint one step approach able train setups successfully use little train data joint setup offer better performance surpass state art regard n gram base score provide relevant output
present novel unsupervised approach multilingual sentiment analysis drive compositional syntax base rule one hand exploit main advantage unsupervised algorithms one interpretability output contrast supervise model behave black box two robustness across different corpora domains hand introduce concept compositional operations exploit syntactic information form universal dependencies tackle one main drawbacks rigidity data structure differently depend language concern experiment show improvement exist unsupervised methods state art supervise model evaluate outside corpus origin experiment also show compositional operations share across languages system available http wwwgrupolysorg software uuusa
natural language understand often require deep semantic knowledge expand previous proposals suggest important aspects semantic knowledge model language model do appropriate level abstraction develop two distinct model capture semantic frame chain discourse information abstract specific mention predicate entities model investigate four implementations standard n gram language model three discriminatively train neural language model generate embeddings semantic frame quality semantic language model semlm evaluate intrinsically use perplexity narrative cloze test extrinsically show semlm help improve performance semantic natural language process task co reference resolution discourse parse
exist timeline generation systems complex events consider information traditional media ignore rich social context provide user generate content reveal representative public interest insightful opinions instead aim generate socially inform timelines contain news article summaries select user comment present optimization framework design balance topical cohesion article comment summaries along informativeness coverage event automatic evaluations real world datasets cover four complex events show system produce informative timelines state art systems human evaluation associate comment summaries furthermore rat insightful editor pick comment rank highly users
present submodular function base framework query focus opinion summarization within framework relevance order produce statistical ranker information coverage respect topic distribution diverse viewpoints encode submodular function dispersion function utilize minimize redundancy first evaluate different metrics text similarity submodularity base summarization methods experiment community qa blog summarization show system outperform state art approach automatic evaluation human evaluation human evaluation task conduct amazon mechanical turk scale show systems able generate summaries high overall quality information diversity
investigate novel task online dispute detection propose sentiment analysis solution problem aim identify sequence sentence level sentiments express discussion use feature classifier predict dispute non dispute label discussion whole evaluate dispute detection approach newly create corpus wikipedia talk page dispute find classifiers rely sentiment tag feature outperform best model achieve promise f1 score seventy-eight accuracy eighty
study problem agreement disagreement detection online discussions isotonic conditional random field isotonic crf base sequential model propose make predictions sentence segment level automatically construct socially tune lexicon bootstrapped exist general purpose sentiment lexicons improve performance evaluate agreement disagreement tag model two disparate online discussion corpora wikipedia talk page online debate model show outperform state art approach datasets example isotonic crf model achieve f1 score seventy-four sixty-seven agreement disagreement detection linear chain crf obtain fifty-eight fifty-six discussions wikipedia talk page
paper describe egyptian arabic english statistical machine translation smt system qcri columbia nyuad qcn group submit nist openmt two thousand and fifteen competition competition focus informal dialectal arabic use sms chat speech thus efforts focus process standardize arabic eg use tool 3arrib madamira train phrase base smt system use state art feature components operation sequence model class base language model sparse feature neural network joint model genre base hierarchically interpolate language model unsupervised transliteration mine phrase table merge hypothesis combination system rank second three genres
universal schema predict type entities relations knowledge base kb jointly embed union available schema type type multiple structure databases freebase wikipedia infoboxes also type express textual pattern raw text prediction typically model matrix completion problem one type per column either one two entities per row case entity type binary relation type respectively factorize sparsely observe matrix yield learn vector embed row column paper explore problem make predictions entities entity pair unseen train time hence without pre learn row embed propose approach per row parameters rather produce row vector fly use learn aggregation function vectors observe columns row experiment various aggregation function include neural network attention model approach understand natural language database question kb entities answer attend textual database evidence experiment predict relations entity type demonstrate despite order magnitude fewer parameters traditional universal schema match accuracy traditional model importantly make predictions unseen row nearly accuracy row available train time
recent progress neural learn demonstrate machine well regularize task eg game go however artistic activities poem generation still widely regard human special capability paper demonstrate simple neural model imitate human task art generation particularly focus traditional chinese poetry show machine well many contemporary poets weakly pass feigenbaum test variant turing test professional domains method base attention base recurrent neural network accept set keywords theme generate poems look keyword generation number techniques propose improve model include character vector initialization attention input hybrid style train compare exist poetry generation methods model generate much theme consistent semantic rich poems
recently bidirectional recurrent neural network brnn widely use question answer qa task promise performance however exist brnn model extract information question answer directly use pool operation generate representation loss similarity calculation hence exist model put supervision loss similarity calculation every time step lose useful information paper propose novel brnn model call full time supervision base brnn fts brnn put supervision every time step experiment factoid qa task show fts brnn outperform baselines achieve state art accuracy
state art speech recognition systems use data intensive context dependent phonemes acoustic units however approach translate well low resourced languages large amount train data available languages automatic discovery acoustic units critical paper demonstrate application nonparametric bayesian model acoustic unit discovery show discover units correlate phonemes therefore linguistically meaningful also present speak term detection std example query algorithm base automatically learn units show propose system produce pn six hundred and twelve ever one thousand, three hundred and ninety-five timit dataset improvement ever five pn slightly lower best report system literature
era big data deep learn common view machine learn approach way cope robust scalable information extraction summarization recently propose cnl approach could scale build concept embed cnl thus allow cnl base information extraction eg normative medical texts rather control nature still infringe boundaries cnl although arguable cnl exploit approach robust wide coverage semantic parse use case like media monitor potential become much obvious opposite direction generation story highlight summarize amr graph focus position paper
semantic roles play important role extract knowledge text current unsupervised approach utilize feature grammar structure induce semantic roles dependence grammars however make difficult adapt noisy new languages paper develop data drive approach identify semantic roles approach entirely unsupervised point rule need learn identify position semantic role occur specifically develop modify adios algorithm base adios solan et al two thousand and five learn grammar structure use grammar structure learn rule identify semantic roles base context grammar structure appear result obtain comparable current state art model inherently dependent human annotate data
recently neural network approach parse largely automate combination individual feature still rely often larger number atomic feature create human linguistic intuition potentially omit important global context reduce feature engineer bare minimum use bi directional lstm sentence representations model parser state three sentence position automatically identify important aspects entire sentence model achieve state art result among greedy dependency parsers english also introduce novel transition system constituency parse require binarization together architecture achieve state art result among greedy parsers english chinese
paper investigate neural character base morphological tag languages complex morphology large tag set systematically explore variety neural architectures dnn cnn cnnhighway lstm blstm obtain character base word vectors combine bidirectional lstms model across word context end end set explore supplementary use word base vectors train large amount unlabeled data experiment morphological tag suggest simple model configurations choice network architecture cnn vs cnnhighway vs lstm vs blstm augmentation pre train word embeddings important clearly impact accuracy increase model capacity add depth example carefully optimize neural network lead substantial improvements differences accuracy train time become much smaller even negligible overall best morphological taggers german czech outperform best result report literature large margin
introduce qvec cca intrinsic evaluation metric word vector representations base correlations learn vectors feature extract linguistic resources show qvec cca score effective proxy range extrinsic semantic syntactic task also show propose evaluation obtain higher consistent correlations downstream task compare exist approach intrinsic evaluation word vectors base word similarity
average uncertainty associate word information theoretic concept heart quantitative computational linguistics entropy establish measure average uncertainty also call average information content use parallel texts twenty-one languages establish number tokens word entropies converge stable value convergence point use select texts massively parallel corpus estimate word entropies across one thousand languages result help establish quantitative language comparisons understand performance multilingual translation systems normalize semantic similarity measure
situate question answer problem answer question environment image diagram problem require jointly interpret question environment use background knowledge select correct answer present parse probabilistic program p3 novel situate question answer model use background knowledge global feature question environment interpretation retain efficient approximate inference key insight treat semantic parse probabilistic program execute nondeterministically whose possible executions represent environmental uncertainty evaluate approach new publicly release data set five thousand science diagram question outperform several competitive classical neural baselines
neural sequence sequence learn recently become promise paradigm machine translation achieve competitive result statistical phrase base systems system description paper attempt utilize several recently publish methods use neural sequential learn order build systems wmt two thousand and sixteen share task automatic post edit multimodal machine translation
consider problem use sentence compression techniques facilitate query focus multi document summarization present sentence compression base framework task design series learn base compression model build parse tree innovative beam search decoder propose efficiently find highly probable compressions framework show integrate various indicative metrics linguistic motivation query relevance compression process derive novel formulation compression score function best model achieve statistically significant improvement state art systems several metrics eg eighty fifty-four improvements rouge two respectively duc two thousand and six two thousand and seven summarization task
word embed show remarkably effective lot natural language process task however exist model still couple limitations interpret dimension word vector paper provide new approach root affix modelraam interpret intrinsic structure natural language also use evaluation measure quality word embed introduce information entropy model divide dimension two categories like root affix lexical semantics consider category whole rather individually experiment english wikipedia corpus result show negative linear relation two attribute high positive correlation model downstream semantic evaluation task
offset method solve word analogies become standard evaluation tool vector space semantic model consider desirable space represent semantic relations consistent vector offset show method reliance cosine similarity conflate offset consistency largely irrelevant neighborhood structure propose simple baselines use improve utility method vector space evaluation
advance compute power natural language process digitization text make possible study culture evolution texts use big data lens ability communicate rely part upon share emotional experience stories often follow distinct emotional trajectories form pattern meaningful us classify emotional arc filter subset one thousand, three hundred and twenty-seven stories project gutenberg fiction collection find set six core emotional arc form essential build block complex emotional trajectories strengthen find separately apply matrix decomposition supervise learn unsupervised learn six core emotional arc examine closest characteristic stories publication today find particular emotional arc enjoy greater success measure download
investigate usage convolutional neural network cnns slot fill task speak language understand propose novel cnn architecture sequence label take account previous context word preserve order information pay special attention current word surround context moreover combine information past future word classification propose cnn architecture outperform even previously best ensembling recurrent neural network model achieve state art result f1 score nine thousand, five hundred and sixty-one atis benchmark dataset without use additional linguistic knowledge resources
present token level decision summarization framework utilize latent topic structure utterances identify summary worthy word concretely series unsupervised topic model explore experimental result show fine grain topic model discover topics utterance level rather document level better identify gist decision make process moreover propose token level summarization approach able remove redundancies within utterances outperform exist utterance rank base summarization methods finally context information also investigate add additional relevant information summary
present novel unsupervised framework focus meet summarization view problem instance relation extraction adapt exist domain relation learner chen et al two thousand and eleven exploit set task specific constraints feature evaluate approach decision summarization task show outperform unsupervised utterance level extractive summarization baselines well exist generic relation extraction base summarization method moreover approach produce summaries competitive generate supervise methods term standard rouge score
paper address problem corpus level entity type ie infer large corpus entity member class food artist application entity type interest knowledge base completion specifically learn class entity member propose figment tackle problem figment embed base combine global model score base aggregate contextual information entity ii context model first score individual occurrences entity aggregate score evaluation figment strongly outperform approach entity type rely relations obtain open information extraction system
introduce new methodology intrinsic evaluation word representations specifically identify four fundamental criteria base characteristics natural language pose difficulties nlp systems develop test directly show whether representations contain subspaces necessary satisfy criteria current intrinsic evaluations mostly base overall similarity full space similarity word thus view vector representations point show limit point base intrinsic evaluations apply evaluation methodology comparison count vector model several neural network model demonstrate important properties model
recent years concepts methods complex network employ tackle word sense disambiguation wsd task represent word nod connect semantically similar despite increasingly number study carry model use network represent data pattern recognition perform attribute space perform use traditional learn techniques word structural relationship word explicitly use pattern recognition process addition investigations probe suitability representations base bipartite network graph bigraphs problem many approach consider possible link word context assess relevance bipartite network model represent feature word ie word characterize context target ambiguous word solve ambiguities write texts focus semantical relationships two type word disregard relationships feature word special propose method serve represent texts graph also construct structure discrimination sense accomplish result reveal propose learn algorithm bipartite network provide excellent result mostly topical feature employ characterize context surprisingly method even outperform support vector machine algorithm particular case advantage robust even small train dataset available take together result obtain show propose representation classification method might useful improve semantical characterization write texts
paper address problem summarize decisions speak meet goal produce concise decision abstract meet decision explore compare token level dialogue act level automatic summarization methods use unsupervised supervise learn frameworks supervise summarization set give true cluster decision relate utterances find token level summaries employ discourse context approach upper bind decision abstract derive directly dialogue act unsupervised summarization settingwe find summaries base unsupervised partition decision relate utterances perform comparably base partition generate use supervise techniques twenty-two rouge f1 use lda base topic model vs twenty-three use svms
train statistical dialog model speak dialog systems sds require large amount annotate data lack scalable methods data mine annotation pose significant hurdle state art statistical dialog managers paper present approach directly leverage billions web search browse sessions overcome hurdle key insight task completion web search browse sessions predictable b generalize speak dialog task completion new method automatically mine behavioral search browse pattern web log translate speak dialog model experiment naturally occur speak dialogs large scale web log session base model outperform state art method entity extraction task sds also achieve better performance entity relation extraction web search query compare nontrivial baselines
biomedical information extraction bioie important many applications include clinical decision support integrative biology pharmacovigilance therefore active research unlike exist review cover holistic view bioie review focus mainly recent advance learn base approach systematically summarize different aspects methodological development addition dive open information extraction deep learn two emerge influential techniques envision next generation bioie
vector space model become popular distributional semantics despite challenge face capture various semantic phenomena propose novel probabilistic framework draw formal semantics recent advance machine learn particular separate predicate entities refer allow us perform bayesian inference base logical form describe implementation framework use combination restrict boltzmann machine feedforward neural network finally demonstrate feasibility approach train parse corpus evaluate establish similarity datasets
causal precedence biochemical interactions crucial biomedical domain transform collections individual interactions eg bind phosphorylations causal mechanisms need inform meaningful search inference analyze causal precedence biomedical domain distinct open domain temporal precedence first describe novel hand annotate text corpus causal precedence biomedical domain second use corpus investigate battery model precedence cover rule base feature base latent representation model highest perform individual model achieve micro f1 forty-three point approach best performers simpler temporal precedence task feature base latent representation model outperform rule base model performance complementary one another apply sieve base architecture capitalize lack overlap achieve micro f1 score forty-six point
exist corpora intrinsic evaluation target towards task informal domains twitter news comment forums want test whether representation informal word fulfill promise elide explicit text normalization preprocessing step one possible evaluation metric domains proximity spell variants propose metric might compute spell variant dataset collect use urbandictionary
consider incorporate topic information sequence sequence framework generate informative interest responses chatbots end propose topic aware sequence sequence ta seq2seq model model utilize topics simulate prior knowledge human guide form informative interest responses conversation leverage topic information generation joint attention mechanism bias generation probability joint attention mechanism summarize hide vectors input message context vectors message attention synthesize topic vectors topic attention topic word message obtain pre train lda model let vectors jointly affect generation word decode increase possibility topic word appear responses model modify generation probability topic word add extra probability item bias overall distribution empirical study automatic evaluation metrics human annotations show ta seq2seq generate informative interest responses significantly outperform state art response generation model
problem accurately predict relative read difficulty across set sentence arise number important natural language applications find curating effective usage examples intelligent language tutor systems yet significant research explore document passage level read difficulty special challenge involve assess aspects readability single sentence receive much less attention particularly consider role surround passages introduce evaluate novel approach estimate relative read difficulty set sentence without surround context use different set lexical grammatical feature explore model predict pairwise relative difficulty use logistic regression examine rank generate aggregate pairwise difficulty label use bayesian rat system form final rank also compare rank derive sentence assess without context find contextual feature help predict differences relative difficulty judgments across two condition
word2vec popular family algorithms unsupervised train dense vector representations word large text corpuses result vectors show capture semantic relationships among correspond word show promise reduce number natural language process nlp task mathematical operations vectors heretofore applications word2vec center around vocabularies million word wherein vocabulary set word vectors simultaneously train novel applications emerge areas outside nlp vocabularies comprise several one hundred million word exist word2vec train systems impractical train large vocabularies either require vectors vocabulary word store memory single server suffer unacceptable train latency due massive network data transfer paper present novel distribute parallel train system enable unprecedented practical train vectors vocabularies several one hundred million word share cluster commodity servers use far less network traffic exist solutions evaluate propose system benchmark dataset show quality vectors degrade relative non distribute train finally several quarter system deploy purpose match query ads gemini sponsor search advertise platform yahoo result significant improvement business metrics
paper present new selection base question answer dataset selqa dataset consist question generate crowdsourcing sentence length answer draw ten prevalent topics english wikipedia introduce corpus annotation scheme enhance generation large diverse challenge datasets explicitly aim reduce word co occurrences question answer annotation scheme compose series crowdsourcing task view effectively utilize crowdsourcing creation question answer datasets various domains several systems compare task answer sentence selection answer trigger provide strong baseline result future work improve upon
paper discuss model dialogue state track use recurrent neural network rnn present experiment standard dialogue state track dst dataset dstc2 one hand rnn model become state art model dst hand state art model turn base require dataset specific preprocessing eg dstc2 specific order achieve result implement two architectures use incremental settings require almost preprocessing compare performance benchmarks dstc2 discuss properties trivial preprocessing performance model close state art result
speech recognition especially name recognition widely use phone service company directory dialers stock quote providers location finders usually challenge due pronunciation variations paper propose efficient robust data drive technique automatically learn acceptable word pronunciations update pronunciation dictionary build better lexicon without affect recognition word similar target word generalize well datasets various size reduce error rate database thirteen thousand human name forty-two compare baseline regular dictionaries already cover canonical pronunciations ninety-seven word name plus well train spell pronunciation stp engine
recent years extract relevant information biomedical clinical texts research article discharge summaries electronic health record subject many research efforts share challenge relation extraction process detect classify semantic relation among entities give piece texts exist model task biomedical domain use either manually engineer feature kernel methods create feature vector feature feed classifier prediction correct class turn result methods highly dependent quality user design feature also suffer curse dimensionality work focus extract relations clinical discharge summaries main objective exploit power convolution neural network cnn learn feature automatically thus reduce dependency manual feature engineer evaluate performance propose model i2b2 two thousand and ten clinical relation extraction challenge dataset result indicate convolution neural network good model relation exaction clinical text without dependent expert knowledge define quality feature
hand craft feature base linguistic domain knowledge play crucial role determine performance disease name recognition systems methods limit scope feature word ability cover contexts word dependencies within sentence work focus reduce dependencies propose domain invariant framework disease name recognition task particular propose various end end recurrent neural network rnn model task disease name recognition classification four pre define categories also utilize convolution neural network cnn cascade rnn get character base embed feature employ word embed feature model compare model state art result two task ncbi disease dataset result disease mention recognition task indicate state art performance obtain without rely feature engineer propose model obtain improve performance classification task disease name
present simple neural network word alignment build source target word window representations compute alignment score sentence pair enable unsupervised train use aggregation operation summarize alignment score give target word soft margin objective increase score true target word decrease score target word present compare popular fast align model approach improve alignment accuracy seven aer english czech six aer romanian english seventeen aer english french alignment
machine translation quality estimation notoriously difficult task lessen usefulness real world translation environments scenarios improve quality predictions accompany measure uncertainty however model task traditionally evaluate term point estimate metrics take prediction uncertainty account investigate probabilistic methods quality estimation provide well calibrate uncertainty estimate evaluate term full posterior predictive distributions also show posterior information useful asymmetric risk scenario aim capture typical situations translation workflows
propose approach biomedical information extraction marry advantage machine learn model eg learn directly data benefit rule base approach eg interpretability approach start train feature base statistical model convert model rule base variant convert feature rule snap grid feature weight discrete vote proposal take advantage large body work machine learn produce interpretable model directly edit experts evaluate approach bionlp two thousand and nine event extraction task result show small performance penalty convert statistical model rule gain interpretability compensate minimal effort human experts improve model similar performance statistical model serve start point
statistical techniques analyze texts refer text analytics depart use simple word count statistics towards new paradigm text mine hinge sophisticate set methods include representations term complex network well establish word adjacency co occurrence methods successfully grasp syntactical feature write texts unable represent important aspects textual data topical structure ie sequence subject develop mesoscopic level along text aspects often overlook current methodologies order grasp mesoscopic characteristics semantical content write texts devise network model able analyze document multi scale fashion propose model limit amount adjacent paragraph represent nod connect whenever share minimum semantical content illustrate capabilities model present case example qualitative analysis alice adventure wonderland show mesoscopic structure document model network reveal many semantic traits texts approach pave way myriad semantic base applications addition approach illustrate machine learn context texts classify among real texts randomize instance
paper improve attention alignment accuracy neural machine translation utilize alignments train sentence pair simply compute distance machine attentions true alignments minimize cost train procedure experiment large scale chinese english task show model improve translation alignment qualities significantly large vocabulary neural machine translation system even beat state art traditional syntax base system
primary goal thesis identify better syntactic constraint bias language independent also efficiently exploitable sentence process focus particular syntactic construction call center embed well study psycholinguistics note particular difficulty comprehension since people use language tool communication one expect complex constructions avoid communication efficiency computational perspective center embed closely relevant leave corner parse algorithm capture degree center embed parse tree construct connection suggest leave corner methods tool exploit universal syntactic constraint people avoid generate center embed structure explore utilities center embed well leave corner methods extensively several theoretical empirical examinations primary task unsupervised grammar induction task input algorithm collection sentence model try extract salient pattern grammar particularly hard problem although expect universal constraint may help improve performance since effectively restrict possible search space model build model extend leave corner parse algorithm efficiently tabulate search space except involve center embed specific degree examine effectiveness approach many treebanks demonstrate often constraint lead better parse performance thus conclude leave corner methods particularly useful syntax orient systems exploit efficiently inherent universal constraints languages
recent advance corpus base natural language generation nlg hold promise easily portable across domains require costly train data consist mean representations mrs pair natural language nl utterances work propose novel framework crowdsourcing high quality nlg train data use automatic quality control measure evaluate different mrs elicit data show pictorial mrs result better nl data collect logic base mrs utterances elicit pictorial mrs judge significantly natural informative better phrase significant increase average quality rat around five point six point scale compare use logical mrs mr become complex benefit pictorial stimuli increase collect data release part submission
phonemic segmentation speech critical step speech recognition systems propose novel unsupervised algorithm base sequence prediction model markov chain recurrent neural network approach consist analyze error profile model train predict speech feature frame frame specifically try learn dynamics speech mfcc space hypothesize boundaries local maxima prediction error evaluate system timit dataset improvements similar methods
sequence label widely use method name entity recognition information extraction unstructured natural language data clinical domain one major application sequence label involve extraction medical entities medication indication side effect electronic health record narratives sequence label domain present set challenge objectives work experiment various crf base structure learn model recurrent neural network extend previously study lstm crf model explicit model pairwise potentials also propose approximate version skip chain crf inference rnn potentials use methodologies structure prediction order improve exact phrase detection various medical entities
word embed methods prove useful many task nlp natural language process much investigate word embeddings english word phrase little attention dedicate languages goal paper explore behavior state art word embed methods czech language characterize rich morphology introduce new corpus word analogy task inspect syntactic morphosyntactic semantic properties czech word phrase experiment word2vec glove algorithms discuss result corpus corpus available research community
represent semantics linguistic items machine interpretable form major goal natural language process since earliest days among range different linguistic items word attract research attention however word representations important limitation conflate different mean word single vector representations word sense potential overcome inherent limitation indeed representation individual word sense concepts recently gain popularity several experimental result show considerable performance improvement achieve across different nlp applications upon move word level deeper sense concept level another interest point regard representation concepts word sense model seamlessly apply linguistic items word phrase sentence
verbs play critical role mean sentence ubiquitous word receive little attention recent distributional semantics research introduce simverb three thousand, five hundred evaluation resource provide human rat similarity three thousand, five hundred verb pair simverb three thousand, five hundred cover normed verb type usf free association database provide least three examples every verbnet class broad coverage facilitate detail analyse syntactic semantic phenomena together influence human understand verb mean significantly larger development test set exist benchmarks simverb three thousand, five hundred enable robust evaluation representation learn architectures promote development methods tailor verbs hope simverb three thousand, five hundred enable richer understand diversity complexity verb semantics guide development systems effectively represent interpret mean
deep learn significantly advance state art speech recognition past years however compare conventional gaussian mixture acoustic model neural network model usually much larger therefore deployable embed devices previously investigate compact highway deep neural network hdnn acoustic model type depth gate feedforward neural network show hdnn base acoustic model achieve comparable recognition accuracy much smaller number model parameters compare plain deep neural network dnn acoustic model paper push boundary leverage knowledge distillation technique also know teacher student train ie train compact hdnn model supervision high accuracy cumbersome model furthermore also investigate sequence train adaptation context teacher student train experiment perform ami meet speech recognition corpus technique significantly improve recognition accuracy hdnn acoustic model less eight million parameters narrow gap model plain dnn thirty million parameters
discriminative segmental model offer way incorporate flexible feature function speech recognition however appeal limit computational requirements due large number possible segment consider multi pass cascade segmental model introduce feature increase complexity different pass pass segmental model rescores lattices produce previous simpler segmental model paper explore several ways make segmental cascade efficient practical reduce feature set first pass frame subsampling various prune approach experiment phonetic recognition find combination techniques possible maintain competitive performance greatly reduce decode prune train time
word embeddings allow natural language process systems share statistical information across relate word embeddings typically base distributional statistics make difficult generalize rare unseen word propose improve word embeddings incorporate morphological information capture share sub word feature unlike previous work construct word embeddings directly morphemes combine morphological distributional information unify probabilistic framework word embed latent variable morphological information provide prior distribution latent word embeddings turn condition likelihood function observe corpus approach yield improvements intrinsic word similarity evaluations also downstream task part speech tag
reorder pose major challenge machine translation mt two languages significant differences word order paper present novel reorder approach utilize sparse feature base dependency word pair instance feature capture whether two word relate dependency link source sentence dependency parse tree follow order swap translation output experiment chinese english translation show statistically significant improvement one hundred and twenty-one bleu point use approach compare state art statistical mt system incorporate prior reorder approach
paper present geometric approach problem model relationship word concepts focus particular analogical phenomena language cognition ground recent theories regard geometric conceptual space begin analysis exist static distributional semantic model move exploration dynamic approach use high dimensional space word mean project subspaces analogies potentially solve online contextualised way crucial element analysis position statistics geometric environment replete opportunities interpretation
paper give overview partial order space probability distributions carry notion information content serve generalisation bayesian order give coecke martin two thousand and eleven investigate constraints necessary order get unique notion information content partial order use give order word vector space model natural language mean relate contexts word use useful notion entailment word disambiguation construction use also point towards way create order space density operators allow fine grain study entailment partial order paper direct complete form domains sense domain theory
paper present novel approach automatically solve arithmetic word problems first algorithmic approach handle arithmetic problems multiple step operations without depend additional annotations predefined templates develop theory expression tree use represent evaluate target arithmetic expressions use uniquely decompose target arithmetic problem multiple classification problems compose expression tree combine world knowledge constrain inference framework classifiers gain use quantity schemas support better extraction feature experimental result show method outperform exist systems achieve state art performance benchmark datasets arithmetic word problems
paper describe system design nlpcc two thousand and sixteen share task word segmentation micro blog texts
cross language information retrieval clir become important problem solve recent years due growth content multiple languages web one standard methods use query translation source target language paper propose approach base word embeddings method capture contextual clue particular word source language give word translations occur similar context target language obtain word embeddings source target language pair learn projection source target word embeddings make use dictionary word translation pairswe propose various methods query translation aggregation advantage approach require corpora align difficult obtain resource scarce languages dictionary word translation pair enough train word vectors translation experiment forum information retrieval evaluation fire two thousand and eight two thousand and twelve datasets hindi english clir propose word embed base approach outperform basic dictionary base approach seventy word embeddings combine dictionary hybrid approach beat baseline dictionary base method seventy-seven outperform english monolingual baseline fifteen combine translations obtain google translate dictionary
vocabulary word account large proportion errors machine translation systems especially system use different domain one train order alleviate problem propose use log bilinear softmax base model vocabulary expansion give vocabulary source word model generate probabilistic list possible translations target language model use word embeddings train significantly large unlabelled monolingual corpora train fairly small word word bilingual dictionary input probabilistic list standard phrase base statistical machine translation system obtain consistent improvements translation quality english spanish language pair especially get improvement thirty-nine bleu point test domain test set
identification authorship dispute document still require human expertise unfeasible many task owe large volumes text author practical applications study introduce methodology base dynamics word co occurrence network represent write texts classify corpus eighty texts eight author texts divide section equal number linguistic tokens time series create twelve topological metrics series prove stationary p value005 permit use distribution moments learn attribute optimize supervise learn procedure use radial basis function network sixty-eight eighty texts correctly classify ie remarkable eighty-five author match success rate therefore fluctuations purely dynamic network metrics find characterize authorship thus open way description texts term small evolve network moreover approach introduce allow comparison texts diverse characteristics simple fast fashion
work present fine grain text chunk algorithm design task multiword expressions mwes segmentation lexical class mwes include wide variety idioms whose automatic identification necessity handle colloquial language algorithm core novelty use non word tokens ie boundaries bottom strategy leverage boundaries refine token level information forge high level performance relatively basic data generality model feature space allow application across languages domains experiment span nineteen different languages exhibit broadly applicable state art model evaluation recent share task data place text partition overall best perform mwe segmentation algorithm cover mwe class multiple english domains include user generate text performance couple non combinatorial fast run design produce ideal combination implementations scale facilitate release open source software
plethora vector space representations word currently available grow consist fix length vectors contain real value represent word result representation upon power many conventional information process data mine techniques bring bear long representations design forethought fit certain constraints paper detail desiderata design vector space representations word
paper investigate framework encoder decoder attention sequence label base speak language understand introduce bidirectional long short term memory long short term memory network blstm lstm encoder decoder model fully utilize power deep learn sequence label task input output sequence align word word attention mechanism provide exact alignment address limitation propose novel focus mechanism encoder decoder framework experiment standard atis dataset show blstm lstm focus mechanism define new state art outperform standard blstm attention base encoder decoder experiment also show propose model robust speech recognition errors
introduce hyperlex dataset evaluation resource quantify extent semantic category membership type relation also know hyponymy hypernymy lexical entailment le relation two thousand, six hundred and sixteen concept pair cognitive psychology research establish typicality category class membership compute human semantic memory gradual rather binary relation nevertheless nlp research exist large scale invetories concept category membership wordnet dbpedia etc treat category membership le binary address ask hundreds native english speakers indicate typicality strength category membership diverse range concept pair crowdsourcing platform result confirm category membership le indeed gradual binary compare human judgements predictions automatic systems reveal huge gap human performance state art le distributional representation learn model substantial differences model discuss pathway improve semantic model overcome discrepancy indicate future application areas improve grade le systems
language process mechanism humans generally robust computers cmabrigde uinervtisy cambridge university effect psycholinguistics literature demonstrate robust word process mechanism jumble word eg cmabrigde cambridge recognize little cost hand computational model word recognition eg spell checker perform poorly data noise inspire find cmabrigde uinervtisy effect propose word recognition model base semi character level recurrent neural network scrnn experiment demonstrate scrnn significantly robust performance word spell correction ie word recognition compare exist spell checker character base convolutional neural network furthermore demonstrate model cognitively plausible replicate psycholinguistics experiment human read difficulty use model
describe technique structure prediction base canonical correlation analysis learn algorithm find two projections input output space aim project give input correct output point close demonstrate technique language vision problem namely problem give textual description abstract scene
language provide simple ways communicate generalizable knowledge eg bird fly john hike fire make smoke though find every language emerge early development language generalization philosophically puzzle resist precise formalization propose first formal account generalizations convey language make quantitative predictions human understand test model three diverse domains generalizations categories generic language events habitual language cause causal language model explain gradience human endorsement interplay simple truth conditional semantic theory diverse beliefs properties formalize probabilistic model language understand work open door understand precisely abstract knowledge learn language
attention base neural machine translation nmt model suffer attention deficiency issue observe recent research propose novel mechanism address limitations improve nmt attention specifically approach memorize alignments temporally within sentence modulate attention accumulate temporal memory decoder generate candidate translation compare approach baseline nmt model two relate approach address issue either explicitly implicitly large scale experiment two language pair show approach achieve better robust gain baseline relate nmt approach model outperform strong smt baselines settings even without use ensembles
social media message brevity unconventional spell pose challenge language identification introduce hierarchical model learn character contextualized word level representations language identification method perform well strong base line also reveal code switch
natural language interfaces tool spellcheckers web search one language know useful ict mediate communication languages southern africa resourced however therefore would useful generic language specific nlp tool could reuse easily adapt across languages depend notion extent similarity languages assess angle orthography corpora twelve versions universal declaration human right udhr examine show cluster languages thus less amenable cross language adaptation nlp tool match guthrie zone examine generalisability result zoom isizulu quantitatively qualitatively four corpora texts different genres result show udhr typical text document orthographically result also provide insight usability typical measure lexical diversity genre statistic may mean different things different document nltk python could use basic analyse text similar nlp tool need considerable customization
automatically detect inappropriate content difficult nlp task require understand context innuendo identify specific keywords due large quantity online user generate content automatic detection become increasingly necessary take largely unsupervised approach use large corpus narratives community base self publish website small segment crowd source annotations explore topic model use latent dirichlet allocation variation use regress appropriateness rat effectively automate rat suitability result suggest certain topics infer may useful detect latent inappropriateness yield recall ninety-six low regression errors
present wikireading large scale natural language understand task publicly available dataset eighteen million instance task predict textual value structure knowledge base wikidata read text correspond wikipedia article task contain rich variety challenge classification extraction sub task make well suit end end model deep neural network dnns compare various state art dnn base architectures document classification information extraction question answer find model support rich answer space word character sequence perform best best perform model word level sequence sequence model mechanism copy vocabulary word obtain accuracy seven hundred and eighteen
languages employ different strategies transmit structural grammatical information example grammatical dependency relationships sentence mainly convey order word languages like mandarin chinese vietnamese word order much less restrict languages inupiatun quechua languages also use internal structure word eg inflectional morphology mark grammatical relationships sentence base quantitative analysis one thousand, five hundred unique translations different book bible one thousand, one hundred different languages speak native language approximately six billion people eighty world population present large scale evidence statistical trade amount information convey order word amount information convey internal word structure languages rely strongly word order information tend rely less word structure information vice versa addition find despite differences way information express also evidence trade different book biblical canon recur little variation across languages informative word order book less informative word structure vice versa argue might suggest one hand languages encode information different efficient ways hand content relate stylistic feature statistically encode similar ways
paper study word embeddings train british national corpus interact part speech boundaries work target universal pos tag set currently actively use annotation range languages experiment train classifiers predict pos tag word base embeddings result show information pos affiliation contain distributional vectors allow us discover group word distributional pattern differ word part speech data often reveal hide inconsistencies annotation process guidelines time support notion soft grade part speech affiliations finally show information pos distribute among dozens vector components limit one two feature
topic model typically represent top word list human interpretation corpus often pre process lemmatization stem representations undermine proliferation word similar mean little public work effect pre process recent work study effect stem topic model english texts find support evidence practice study effect lemmatization topic model russian wikipedia article find one configuration significantly improve interpretability accord word intrusion metric conclude lemmatization may benefit topic model morphologically rich languages investigation need
lot research interest encode variable length sentence fix length vectors way preserve sentence mean two common methods include representations base average word vectors representations base hide state recurrent neural network lstms sentence vectors use feature subsequent machine learn task pre train context deep learn however much know properties encode sentence representations language information capture propose framework facilitate better understand encode representations define prediction task around isolate aspects sentence structure namely sentence length word content word order score representations ability train classifier solve prediction task use representation input demonstrate potential contribution approach analyze different sentence representation mechanisms analysis shed light relative strengths different sentence embed methods respect low level prediction task effect encode vector dimensionality result representations
natural language process data analytics relate technology use widely many research areas artificial intelligence human language process translation present due explosive growth data many challenge natural language process hadoop one platforms process large amount data require natural language process koshik one natural language process architectures utilize hadoop contain language process components stanford corenlp opennlp study describe build koshik platform relevant tool provide step analyze wiki data finally evaluate discuss advantage disadvantage koshik architecture give recommendations improve process performance
efficient methods store query critical scale high order n gram language model large corpora propose language model base compress suffix tree representation highly compact easily hold memory support query need compute language model probabilities fly present several optimisations improve query runtimes 2500x despite incur modest increase construction time memory usage large corpora high markov order method highly competitive state art kenlm package impose much lower memory requirements often order magnitude runtimes either similar train comparable query
recurrent neural network train separately model language several document unknown author use measure similarity document able find clue common authorship even document short disparate topics easy make statistically significant predictions regard authorship difficult group document definite cluster high accuracy
within field statistical machine translation smt neural approach nmt recently emerge first technology able challenge long stand dominance phrase base approach pbmt particular iwslt two thousand and fifteen evaluation campaign nmt outperform well establish state art pbmt systems english german language pair know particularly hard morphology syntactic differences understand respect nmt provide better translation quality pbmt perform detail analysis neural versus phrase base smt output leverage high quality post edit perform professional translators iwslt data first time analysis provide useful insights linguistic phenomena best model neural model reorder verbs point aspects remain improve
lexical semantics continue play important role drive research directions nlp recognition understand context become increasingly important deliver successful outcomes nlp task besides traditional process areas word sense name entity disambiguation creation maintenance dictionaries annotate corpora resources become cornerstones lexical semantics research produce wealth contextual information nlp process exploit new efforts link construct scratch information link open data way formal tool come logic ontologies automate reason increase interoperability accessibility resources lexical computational semantics even languages previously limit lexsemlogics two thousand and sixteen combine 1st workshop lexical semantics lesser resources languages 3rd workshop logics ontologies accept paper program cover topics across two areas include encode plurals wordnets creation thesaurus multiple source base semantic similarity metrics use cross lingual treebanks annotations universal part speech tag also welcome talk two distinguish speakers portuguese lexical knowledge base different approach result application nlp task new strategies open information extraction capture verb base proposition massive text corpora
distant speech recognition challenge particularly due corruption speech signal reverberation cause large distance speaker microphone order cope wide range reverberations real world situations present novel approach acoustic model include ensemble deep neural network dnns ensemble jointly train dnns first multiple dnns establish correspond different reverberation time sixty rt60 setup step also model ensemble dnn acoustic model jointly train include feature map acoustic model feature map design dereverberation front end test phase two likely dnns choose dnn ensemble use maximum posteriori map probabilities compute online fashion use maximum likelihood ml base blind rt60 estimation posterior probability output two dnns combine use ml base weight simple average extensive experiment demonstrate propose approach lead substantial improvements speech recognition accuracy conventional dnn baseline systems diverse reverberant condition
recognize various semantic relations term beneficial many nlp task path base distributional information source consider complementary task superior result latter show recently suggest former contribution might become obsolete follow recent success integrate neural method hypernymy detection shwartz et al two thousand and sixteen extend recognize multiple relations empirical result show method effective multiclass set well show path base information source always contribute classification analyze case mostly complement distributional information
sentiment social media increasingly consider important resource customer segmentation market understand tackle socio economic issue however sentiment social media difficult measure since user generate content usually short informal although many traditional sentiment analysis methods propose identify slang sentiment word remain untackled one reason slang sentiment word available exist dictionaries sentiment lexicons end propose build first sentiment dictionary slang word aid sentiment analysis social media content laborious time consume collect label sentiment polarity comprehensive list slang word present approach leverage web resources construct extensive slang sentiment word dictionary slangsd easy maintain extend slangsd publicly available research purpose empirically show advantage use slangsd newly build slang sentiment word dictionary sentiment classification provide examples demonstrate ease use exist sentiment system
modal sense classification msc special wsd task depend mean proposition modal scope explore cnn architecture classify modal sense english german show cnns superior manually design feature base classifiers standard nn classifier analyze feature map learn cnn identify know previously unattested linguistic feature benchmark cnn standard wsd task compare favorably model use sense disambiguate target vectors
text speech synthesis indian languages see lot progress decade partly due annual blizzard challenge systems assume text write devanagari dravidian script nearly phonemic orthography script however common form computer interaction among indians ascii write transliterate text text generally noisy many variations spell word paper evaluate three approach synthesize speech noisy ascii text naive uni grapheme approach multi grapheme approach supervise grapheme phoneme g2p approach methods first convert ascii text phonetic script learn deep neural network synthesize speech train test model blizzard challenge datasets transliterate ascii use crowdsourcing experiment hindi tamil telugu demonstrate model generate speech competetive quality ascii text compare speech synthesize native script accompany transliterate datasets release public access
cross lingual word embeddings study extensively recent years qualitative differences different algorithms remain vague observe whether algorithm use particular feature set sentence ids account significant performance gap among algorithms feature set also use traditional alignment algorithms ibm model one demonstrate similar performance state art embed algorithms variety benchmarks overall observe different algorithmic approach utilize sentence id feature space result similar performance paper draw empirical theoretical parallel embed alignment literature suggest add additional source information go beyond traditional signal bilingual sentence align corpora may substantially improve cross lingual word embeddings future baselines least take feature account
construct new dataset two hundred thousand fill gap cloze multiple choice read comprehension problems construct ldc english gigaword newswire corpus wdw dataset variety novel feature first contrast cnn daily mail datasets hermann et al two thousand and fifteen avoid use article summaries question formation instead problem form two independent article article give passage read separate article events use form question second avoid anonymization choice person name entity third problems filter remove fraction easily solve simple baselines remain eighty-four solvable humans report performance benchmarks standard systems propose wdw dataset challenge task community
paper concern identify contexts useful train word representation model different word class adjectives verbs v nouns n introduce simple yet effective framework automatic selection class specific context configurations construct context configuration space base universal dependency relations word efficiently search space adapt beam search algorithm word similarity task word class show framework effective efficient particularly improve spearman rho correlation human score simlex nine hundred and ninety-nine best previously propose class specific contexts six six v five n rho point select context configurations train fourteen two hundred and sixty-two v three hundred and thirty-six n dependency base contexts result reduce train time result generalise show configurations algorithm learn one english train setup outperform previously propose context type another train setup english moreover base configuration space universal dependencies possible transfer learn configurations german italian also demonstrate improve per class result context type two languages
sequence sequence architecture widely use response generation neural machine translation model potential relationship two sentence typically consist two part encoder read source sentence decoder generate target sentence word word accord encoder output last generate word however face cold start problem generate first word previous word refer exist work mainly use special start symbol generate first word obvious drawback work learnable relationship word start symbol furthermore may lead error accumulation decode first word incorrectly generate paper propose novel approach learn generate first word sequence sequence architecture rather use start symbol experimental result task response generation short text conversation show propose approach outperform state art approach automatic manual evaluations
humans read text fixate word skip others however attempt explain skip behavior computational model exist work focus predict read time egusing surprisal paper propose novel approach model skip read use unsupervised architecture combine neural attention autoencoding train raw text use reinforcement learn model explain human read behavior tradeoff precision language understand encode input accurately economy attention fixate word possible evaluate model dundee eye track corpus show accurately predict skip behavior read time competitive surprisal capture know qualitative feature human read
paper report knowledge base method word sense disambiguation domains biomedical clinical text combine word representations create large corpora small number definitions umls create concept representations compare representations context ambiguous term use relational information obtain comparable performance previous approach msh wsd dataset well know dataset biomedical domain additionally method fast easy set extend domains supplementary materials include source code find https githubcom clip yarn
neural model recently use text summarization include headline generation model train use set document headline pair however model explicitly consider topical similarities differences document suggest categorize document various topics document within topic similar content share similar summarization pattern take advantage topic information document propose topic sensitive neural headline generation model model generate accurate summaries guide document topics test model lcsts dataset experiment show method outperform baselines topic achieve state art performance
study topmost weight matrix neural network language model show matrix constitute valid word embed train language model recommend tie input embed output embed analyze result update rule show tie embed evolve similar way output embed input embed untie model also offer new method regularize output embed methods lead significant reduction perplexity able show variety neural network language model finally show weight tie reduce size neural translation model less half original size without harm performance
neural machine translation nmt generation target word depend source target contexts find source contexts direct impact adequacy translation target contexts affect fluency intuitively generation content word rely source context generation functional word rely target context due lack effective control influence source target contexts conventional nmt tend yield fluent inadequate translations address problem propose context gate dynamically control ratios source target contexts contribute generation target word way enhance adequacy fluency nmt careful control information flow contexts experiment show approach significantly improve upon standard attention base nmt system twenty-three bleu point
mean representation amr semantic representation natural language embed annotations relate traditional task name entity recognition semantic role label word sense disambiguation co reference resolution describe transition base parser amr parse sentence leave right linear time propose test suite assess specific subtasks helpful compare amr parsers show parser competitive state art ldc2015e86 dataset outperform state art parsers recover name entities handle polarity
paper propose new approach duration model statistical parametric speech synthesis recurrent statistical model train output phone transition probability timestep acoustic frame unlike conventional approach duration model assume duration distributions particular form eg gaussian use mean distribution synthesis approach principle model distribution support non negative integers generation model perform many ways consider output generation base median predict duration median typical probable conventional mean duration robust train data irregularities enable incremental generation furthermore frame level approach duration prediction consistent longer term goal model durations acoustic feature together result indicate propose method competitive baseline approach approximate median duration hold natural speech
multimedia speak content present attractive information plain text content difficult display screen select user result access large collections former much difficult time consume latter humans highly attractive develop machine automatically understand speak content summarize key information humans browse endeavor propose new task machine comprehension speak content define initial goal listen comprehension test toefl challenge academic english examination english learners whose native language english propose attention base multi hop recurrent neural network amrnn architecture task achieve encourage result initial test initial result also show word level attention probably robust sentence level attention task asr errors
every field research consist multiple application areas various techniques routinely use solve problems wide range application areas exponential growth research volumes become difficult keep track ever grow number application areas well correspond problem solve techniques paper consider computational linguistics domain present novel information extraction system automatically construct pool application areas domain appropriately link correspond problem solve techniques categorize individual research article base application area techniques propose use article k gram base discount method along handwritten rule bootstrapped pattern learn employ extract application areas subsequently language model approach propose characterize article base application area similarly regular expressions high score noun phrase use extraction problem solve techniques propose greedy approach characterize article base techniques towards end present table represent frequent techniques adopt particular application area finally propose three use case present extensive temporal analysis usage techniques application areas
apply natural semantic metalanguage nsm approach goddard wierzbicka two thousand and fourteen lexical semantic analysis english evaluational adjectives compare result picture develop appraisal framework martin white two thousand and five analysis corpus assist examples mainly draw film book review support collocational statistical information wordbanks online propose nsm explications twenty-four evaluational adjectives argue fall five group correspond distinct semantic template group sketch follow first person think plus affect eg wonderful experiential eg entertain experiential bodily reaction eg grip last impact eg memorable cognitive evaluation eg complex excellent group semantic templates compare classifications appraisal framework system appreciation addition particularly interest sentiment analysis automatic identification evaluation subjectivity text discuss relevance two frameworks sentiment analysis language technology applications
link concepts name entities knowledge base become crucial natural language understand task respect recent work show key advantage exploit textual definitions various natural language process applications however date reliable large scale corpora sense annotate textual definitions available research community paper present large scale high quality corpus disambiguate gloss multiple languages comprise sense annotations concepts name entities unify sense inventory approach construction disambiguation corpus build upon structure large multilingual semantic network state art disambiguation system first gather complementary information equivalent definitions across different languages provide context disambiguation combine semantic similarity base refinement result obtain multilingual corpus textual definitions feature thirty-eight million definitions two hundred and sixty-three languages make freely available http lcluniroma1it disambiguate gloss experiment open information extraction sense cluster show two state art approach improve performance integrate disambiguate corpus pipeline
name entity recognition often fail idiosyncratic domains cause problem depend task entity link relation extraction propose generic robust approach high recall name entity recognition approach easy train offer strong generalization diverse domain specific language news document eg reuters biomedical text eg medline approach base deep contextual sequence learn utilize stack bidirectional lstm network model train hundred label sentence rely external knowledge report result f1 score range eighty-four ninety-four standard datasets
distributional model derive co occurrences corpus small proportion possible plausible co occurrences observe result sparse vector space require mechanism infer miss knowledge methods face challenge ways render result word representations uninterpretable consequence semantic composition become hard model paper explore alternative involve explicitly infer unobserved co occurrences use distributional neighbourhood show distributional inference improve sparse word representations several word similarity benchmarks demonstrate model competitive state art adjective noun noun noun verb object compositions fully interpretable
present novel natural language generation system speak dialogue systems capable entrain adapt users way speak provide contextually appropriate responses generator base recurrent neural network sequence sequence approach fully trainable data include precede context along responses generate show context aware generator yield significant improvements baseline automatic metrics human pairwise preference test
present new framework compositional distributional semantics distributional contexts lexemes express term anchor pack dependency tree show structure potential capture full sentential contexts lexeme provide uniform basis composition distributional knowledge way capture mutual disambiguation generalization
relation classification associate many potential applications artificial intelligence area recent approach usually leverage neural network base structure feature syntactic dependency feature solve problem however high cost structure feature make approach inconvenient directly use addition structure feature probably domain dependent therefore paper propose bi directional long short term memory recurrent neural network bi lstm rnn model base low cost sequence feature address relation classification model divide sentence text segment five part namely two target entities three contexts learn representations entities contexts use classify relations evaluate model two standard benchmark datasets different domains namely semeval two thousand and ten task eight bionlp st two thousand and sixteen task bb3 former dataset model achieve comparable performance compare model use sequence feature latter dataset model obtain third best result compare model official evaluation moreover find context two target entities play important role relation classification furthermore statistic experiment show context two target entities use approximate replacement shortest dependency path dependency parse use
distributional semantic model dsms vector cosine widely use estimate similarity word vectors although measure notice suffer several shortcomings recent literature propose methods attempt mitigate bias paper intend investigate apsyn measure compute extent intersection associate contexts two target word weight context relevance evaluate metric similarity estimation task several popular test set result show apsyn fact highly competitive even respect result report literature word embeddings top apsyn address weaknesses vector cosine perform well also genuine similarity estimation
multimedia speak content present attractive information plain text content former difficult display screen select user result access large collections former much difficult time consume latter humans therefore highly attractive develop machine automatically understand speak content summarize key information humans browse endeavor new task machine comprehension speak content propose recently initial goal define listen comprehension test toefl challenge academic english examination english learners whose native languages english attention base multi hop recurrent neural network amrnn architecture also propose task consider sequential relationship within speech utterances paper propose new hierarchical attention model ham construct multi hop attention mechanism tree structure rather sequential representations utterances improve comprehension performance robust respect asr errors obtain
real world data differ radically benchmark corpora use natural language process nlp soon apply technologies real world performance drop reason problem obvious nlp model train sample limit set canonical varieties consider standard prominently english newswire however many dimension eg socio demographics language genre sentence type etc texts differ standard solution obvious control factor clear best go beyond current practice train homogeneous data single domain language paper review notion canonicity shape community approach language argue leverage call fortuitous data ie non obvious data hitherto neglect hide plain sight raw data need refine embrace variety heterogeneous data combine proper algorithms produce robust model also enable adaptive language technology capable address natural language variation
present dictionary base approach racism detection dutch social media comment retrieve two public belgian social media sit likely attract racist reactions comment label racist non racist multiple annotators approach three discourse dictionaries create first create dictionary retrieve possibly racist neutral term train data augment general word remove bias second dictionary create automatic expansion use textttword2vec model train large corpus general dutch text finally third dictionary create manually filter incorrect expansions train multiple support vector machine use distribution word different categories dictionaries feature best perform model use manually clean dictionary obtain f score forty-six racist class test set consist unseen dutch comment retrieve sit use train set automate expansion dictionary slightly boost model performance increase performance statistically significant fact coverage expand dictionaries increase indicate word automatically add occur corpus able meaningfully impact performance dictionaries code procedure request corpus available https githubcom clip hades
though dialectal language increasingly abundant social media resources exist develop nlp tool handle language conduct case study dialectal language online conversational text investigate african american english aae twitter propose distantly supervise model identify aae like language demographics associate geo locate message verify language follow well know aae linguistic phenomena addition analyze quality exist language identification dependency parse tool aae like text demonstrate perform poorly text compare text associate white speakers also provide ensemble classifier language identification eliminate disparity release new corpus tweet contain aae like language
paper present challenge community give large corpus write text align normalize speak form train rnn learn correct normalization function present data set general text normalizations generate use exist text normalization component text speech system data set release open source near future also present experiment data set variety different rnn architectures architectures fact produce good result measure term overall accuracy errors produce problematic since would convey completely wrong message system deploy speech application hand show simple fst base filter mitigate errors achieve level accuracy achievable rnn alone though conclusions largely negative point actually argue text normalization problem intractable use pure rnn approach merely go something solve merely huge amount annotate text data feed general rnn model open source data provide novel data set sequence sequence model hop community find better solutions data use work release available https githubcom rwsproat text normalization data
exist work learn sentiment specific word representation improve twitter sentiment classification encode n gram distant supervise tweet sentiment information learn process assume word within tweet sentiment polarity whole tweet ignore word sentiment polarity address problem propose learn sentiment specific word embed exploit lexicon resource distant supervise information develop multi level sentiment enrich word embed learn method use parallel asymmetric neural network model n gram word level sentiment tweet level sentiment learn process experiment standard benchmarks show approach outperform state art methods
neural machine translation nmt make good progress past two years tens millions bilingual sentence pair need train however human label costly tackle train data bottleneck develop dual learn mechanism enable nmt system automatically learn unlabeled data dual learn game mechanism inspire follow observation machine translation task dual task eg english french translation primal versus french english translation dual primal dual task form close loop generate informative feedback signal train translation model even without involvement human labeler dual learn mechanism use one agent represent model primal task agent represent model dual task ask teach reinforcement learn process base feedback signal generate process eg language model likelihood output model reconstruction error original sentence primal dual translations iteratively update two model convergence eg use policy gradient methods call correspond approach neural machine translation emphdual nmt experiment show dual nmt work well englishleftrightarrowfrench translation especially learn monolingual data ten bilingual data warm start achieve comparable accuracy nmt train full bilingual data french english translation task
many natural language process nlp task document commonly model bag word use term frequency inverse document frequency tf idf vector one major shortcoming frequency base tf idf feature vector ignore word order carry syntactic semantic relationships among word document important nlp task genre classification paper propose novel distribute vector representation document simple recurrent neural network language model rnn lm long short term memory rnn language model lstm lm first create document task lm parameters adapt document adapt parameters vectorized represent document new document vectors label dv rnn dv lstm respectively believe new document vectors capture high level sequential information document current document representations fail capture new document vectors evaluate genre classification document three corpora brown corpus bnc baby corpus artificially create penn treebank dataset classification performances compare performance tf idf vector state art distribute memory model paragraph vector pv dm result show dv lstm significantly outperform tf idf pv dm case combinations propose document vectors tf idf pv dm may improve performance
common effective way train translation systems relate languages consider sub word level basic units however increase length sentence result increase decode time increase length also impact specific choice data format represent sentence subwords phrase base smt framework investigate different choices decoder parameters well data format impact decode time translation accuracy suggest best options settings significantly improve decode time little impact translation accuracy
sentiment analysis sa use code mix data social media several applications opinion mine range customer satisfaction social campaign analysis multilingual societies advance area impede lack suitable annotate dataset introduce hindi english hi en code mix dataset sentiment analysis perform empirical analysis compare suitability performance various state art sa methods social media paper introduce learn sub word level representations lstm subword lstm architecture instead character level word level representations linguistic prior architecture enable us learn information sentiment value important morphemes also seem work well highly noisy text contain misspell show experiment demonstrate morpheme level feature map learn model also hypothesize encode linguistic prior subword lstm architecture lead superior performance system attain accuracy four five greater traditional approach dataset also outperform available system sentiment analysis hi en code mix text eighteen
automatic response generation build chatbot systems draw lot attention recently limit understand need consider linguistic context input text generation process task challenge message conversational environment short informal evidence indicate message context dependent scarce study social conversation data crawl web observe characteristics estimate responses message discriminative identify context dependent message characteristics weak supervision propose use long short term memory lstm network learn classifier method carry text representation classifier learn unify framework experimental result show propose method significantly outperform baseline methods accuracy classification
humans capacity draw common sense inferences natural language various things likely certain hold base establish discourse rarely state explicitly propose evaluation automate common sense inference base extension recognize textual entailment predict ordinal human responses subjective likelihood inference hold give context describe framework extract common sense knowledge corpora use construct dataset ordinal entailment task train neural sequence sequence model dataset use score generate possible inferences annotate subsets previously establish datasets via ordinal annotation protocol order analyze distinctions construct
synonym polysemous word usually paraphrase one sense among many lexicons use improve vector space word representations paraphrase unreliable bring noise vector space prior work use coefficient adjust overall learn lexicons regard paraphrase equally paper propose novel approach regard paraphrase diversely alleviate adverse effect polysemy annotate paraphrase degree reliability paraphrase randomly eliminate accord degrees model learn word representations way approach drop unreliable paraphrase keep reliable paraphrase time experimental result show propose method improve word vectors approach attempt address polysemy problem keep one vector per word make approach easier use conventional methods estimate multiple vectors word approach also outperform prior work experiment
paper study novel approach name entity recognition ner mention detection natural language process instead treat ner sequence label problem propose new local detection approach rely recent fix size ordinally forget encode fofe method fully encode sentence fragment leave right contexts fix size representation afterwards simple feedforward neural network use reject predict entity label individual fragment propose method evaluate several popular ner mention detection task include conll two thousand and three ner task tac kbp2015 tac kbp2016 tri lingual entity discovery link edl task methods yield pretty strong performance examine task local detection approach show many advantage traditional sequence label methods
paper present empirical comparison different dependency parsers vietnamese unusual characteristics copula drop verb serialization experimental result show neural network base parsers perform significantly better traditional parsers report highest parse score publish date vietnamese label attachment score las seven thousand, three hundred and fifty-three unlabeled attachment score uas eight thousand and sixty-six
paper describe root eighteen classifier use score several unsupervised distributional measure feature discriminate semantically relate unrelated word classify relate pair accord semantic relation ie synonymy antonymy hypernymy part whole meronymy classifier participate cogalex v share task show solid performance first subtask poor performance second subtask low score report second subtask suggest distributional measure sufficient discriminate multiple semantic relations
recently le mikolov describe two log linear model call paragraph vector use learn state art distribute representations document inspire work present binary paragraph vector model simple neural network learn short binary cod fast information retrieval show binary paragraph vectors outperform autoencoder base binary cod despite use fewer bits also evaluate precision transfer learn settings binary cod infer document unrelated train corpus result experiment indicate binary paragraph vectors capture semantics relevant various domain specific document finally present model simultaneously learn short binary cod longer real value representations model use rapidly retrieve short list highly relevant document large document collection
recent work semantic parse question answer focus long complicate question many would seem unnatural ask normal conversation two humans effort explore conversational qa set present realistic task answer sequence simple inter relate question collect dataset six thousand and sixty-six question sequence inquire semi structure table wikipedia seventeen thousand, five hundred and fifty-three question answer pair total exist qa systems face two major problems evaluate dataset one handle question contain coreferences previous question answer two match word phrase question correspond entries associate table conclude propose strategies handle issue
success long short term memory lstm neural network language process typically attribute ability capture long distance statistical regularities linguistic regularities often sensitive syntactic structure dependencies capture lstms explicit structural representations begin address question use number agreement english subject verb dependencies probe architecture grammatical competence use train objectives explicit grammatical target number prediction grammaticality judgments use language model strongly supervise settings lstm achieve high overall accuracy less one errors errors increase sequential structural information conflict frequency errors rise sharply language model set conclude lstms capture non trivial amount grammatical structure give target supervision stronger architectures may require reduce errors furthermore language model signal insufficient capture syntax sensitive dependencies supplement direct supervision dependencies need capture
read comprehension task ask question give evidence document central problem natural language understand recent formulations task typically focus answer selection set candidates pre define manually use external nlp pipeline however rajpurkar et al two thousand and sixteen recently release squad dataset answer arbitrary string supply text paper focus answer extraction task present novel model architecture efficiently build fix length representations span evidence document recurrent network show score explicit span representations significantly improve performance approach factor prediction separate predictions word start end markers approach improve upon best publish result wang jiang two thousand and sixteen five decrease error rajpurkar et al baseline fifty
present neural model morphological inflection generation employ hard attention mechanism inspire nearly monotonic alignment commonly find character word character inflection evaluate model three previously study morphological inflection generation datasets show provide state art result various setups compare previous neural non neural approach finally present analysis continuous representations learn hard soft attention citebahdanaucb14 model task shed light feature model extract
machine comprehension mc answer query give context paragraph require model complex interactions context query recently attention mechanisms successfully extend mc typically methods use attention focus small portion context summarize fix size vector couple attentions temporally often form uni directional attention paper introduce bi directional attention flow bidaf network multi stage hierarchical process represent context different level granularity use bi directional attention flow mechanism obtain query aware context representation without early summarization experimental evaluations show model achieve state art result stanford question answer dataset squad cnn dailymail cloze test
propose general class language model treat reference explicit stochastic latent variable architecture allow model create mention entities attribute access external databases require eg dialogue generation recipe generation internal state require eg language model aware coreference facilitate incorporation information access predictable locations databases discourse context even target reference may rare word experiment three task show model variants base deterministic attention
present framework question answer efficiently scale longer document maintain even improve performance state art model successful approach read comprehension rely recurrent neural network rnns run long document prohibitively slow difficult parallelize sequence inspire people first skim document identify relevant part carefully read part produce answer combine coarse fast model select relevant sentence expensive rnn produce answer sentence treat sentence selection latent variable train jointly answer use reinforcement learn experiment demonstrate state art performance challenge subset wikireading new dataset speed model 35x 67x
automatic translation natural language descriptions program longstanding challenge problem work consider simple yet important sub problem translation textual descriptions program devise novel neural network architecture task train end end specifically introduce latent attention compute multiplicative weight word description two stage process goal better leverage natural language structure indicate relevant part predict program elements architecture reduce error rate two thousand, eight hundred and fifty-seven compare prior art also propose one shoot learn scenario program synthesis simulate exist dataset demonstrate variation train procedure scenario outperform original procedure significantly close gap model train data
although end end neural machine translation nmt achieve remarkable progress past two years suffer major drawback translations generate nmt systems often lack adequacy widely observe nmt tend repeatedly translate source word mistakenly ignore word alleviate problem propose novel encoder decoder reconstructor framework nmt reconstructor incorporate nmt model manage reconstruct input source sentence hide layer output target sentence ensure information source side transform target side much possible experiment show propose framework significantly improve adequacy nmt output achieve superior translation result state art nmt statistical mt systems
recently deeplearning model show capable make remarkable performance sentence document classification task work propose novel framework call ac blstm model sentence document combine asymmetric convolution neural network acnn bidirectional long short term memory network blstm experiment result demonstrate model achieve state art result five task include sentiment analysis question type classification subjectivity classification order improve performance ac blstm propose semi supervise learn framework call g ac blstm text classification combine generative model ac blstm
keyphrase annotation task identify textual units represent main content document keyphrase annotation either carry extract important phrase document keyphrase extraction assign entries control domain specific vocabulary keyphrase assignment assignment methods generally reliable provide better form keyphrases well keyphrases occur document often silent contrary extraction methods depend manually build resources paper propose new method perform keyphrase extraction keyphrase assignment integrate mutual reinforce manner experiment carry datasets cover different domains humanities social sciences show statistically significant improvements compare keyphrase extraction keyphrase assignment state art methods
timeline generation task summarise entity biography select stories represent key events large pool relevant document paper address lack standard dataset evaluative methodology problem present make publicly available new dataset eighteen thousand, seven hundred and ninety-three news article cover thirty-nine entities entity provide gold standard timeline set entity relate article propose rouge evaluation metric validate dataset show top google result outperform straw man baselines
report exploratory analysis emoji dick project leverage crowdsourcing translate melville moby dick emoji distinctive use emoji remove textual context lead vary translation quality paper use statistical word alignment part speech tag explore people use emoji despite simple methods observe differences token part speech distributions experiment also suggest semantics preserve translation repetition common emoji
objective build comprehensive corpus cover syntactic semantic annotations chinese clinical texts correspond annotation guidelines methods well develop tool train annotate corpus supply baselines research chinese texts clinical domain materials methods iterative annotation method propose train annotators develop annotation guidelines use annotation quality assurance measure comprehensive corpus build contain annotations part speech pos tag syntactic tag entities assertions relations inter annotator agreement iaa calculate evaluate annotation quality chinese clinical text process information extraction system cctpies develop base annotate corpus result syntactic corpus consist one hundred and thirty-eight chinese clinical document forty-seven thousand, four hundred and twenty-four tokens two thousand, five hundred and fifty-three full parse tree semantic corpus include nine hundred and ninety-two document annotate thirty-nine thousand, five hundred and eleven entities assertions seven thousand, six hundred and ninety-five relations iaa evaluation show comprehensive corpus good quality system modules effective discussion annotate corpus make considerable contribution natural language process nlp research chinese texts clinical domain however corpus number limitations additional type clinical text introduce improve corpus coverage active learn methods utilize promote annotation efficiency conclusions study several annotation guidelines annotation method chinese clinical texts propose comprehensive corpus nlp modules construct provide foundation study apply nlp techniques chinese texts clinical domain
prevalent approach neural machine translation rely bi directional lstms encode source sentence paper present faster simpler architecture base succession convolutional layer allow encode entire source sentence simultaneously compare recurrent network computation constrain temporal dependencies wmt sixteen english romanian translation achieve competitive accuracy state art outperform several recently publish result wmt fifteen english german task model obtain almost accuracy deep lstm setup wmt fourteen english french translation convolutional encoder speed cpu decode two time higher accuracy strong bi directional lstm baseline
crossword puzzle popular word game require large vocabulary also broad knowledge topics answer clue natural language task many clue contain nuances pun counter intuitive word definitions additionally extremely difficult ascertain definitive answer without constraints crossword grid task challenge humans computers describe new crossword solve system cruciform employ group natural language components return list candidate word score give clue list use conjunction fill intersections puzzle grid formulate constraint satisfaction problem manner similar one use dr fill system describe result several experiment system
goal sentence document model accurately represent mean sentence document various natural language process task work present dependency sensitive convolutional neural network dscnn general purpose classification system sentence document dscnn hierarchically build textual representations process pretrained word embeddings via long short term memory network subsequently extract feature convolution operators compare exist recursive neural model tree structure dscnn rely parsers expensive phrase label thus restrict sentence level task moreover unlike cnn base model analyze sentence locally slide windows system capture dependency information within sentence relationships across sentence document experiment result demonstrate approach achieve state art performance several task include sentiment analysis question type classification subjectivity classification
emerge various online video platforms like youtube youku letv online tv series review become important viewers producers customers rely heavily review select tv series producers use improve quality result automatically classify review accord different requirements evolve popular research topic essential daily life paper focus review hot tv series china successfully train generic classifiers base eight predefined categories experimental result show promise performance effectiveness generalization different tv series
acoustic word embeddings fix dimensional vector representations variable length speak word segment begin consider task speech recognition query example search embeddings learn discriminatively similar speech segment correspond word dissimilar segment correspond different word recent work find acoustic word embeddings outperform dynamic time warp query example search relate word discrimination task however space embed model train approach still relatively unexplored paper present new discriminative embed model base recurrent neural network rnns consider train losses successful prior work particular cross entropy loss word classification contrastive loss explicitly aim separate word different word pair siamese network train set find classifier base siamese rnn embeddings improve previously report result word discrimination task siamese rnns outperform classification model addition present analyse learn embeddings effect variables dimensionality network structure
utilization social media material journalistic workflows increase demand automate methods identification mis disinformation since textual contradiction across social media post signal rumorousness seek model claim twitter post textually contradict identify two different contexts contradiction emerge broader form observe across independently post tweet specific form thread conversations define two scenarios differ term central elements argumentation claim conversation structure design evaluate model two scenarios uniformly three way recognize textual entailment task order represent claim conversation structure implicitly generic inference model previous study use explicit representation properties address noisy text classifiers use simple similarity feature derive string part speech level corpus statistics reveal distribution differences feature contradictory oppose non contradictory tweet relations classifiers yield state art performance
present data drive method determine veracity set rumorous claim social media data tweet different source pertain rumor process three level first factuality value assign tweet base four textual cue categories relevant journalism use case amalgamate speaker support term polarity commitment term certainty speculation next proportion lexical cue utilize predictors tweet certainty generalize linear regression model subsequently lexical cue proportion predict certainty well time course characteristics use compute veracity rumor term identity rumor resolve tweet binary resolution value judgment system operate without access extralinguistic resources evaluate data portion hand label examples available achieve seventy-four f1 score identify rumor resolve tweet seventy-six f1 score predict rumor resolve true false
name entity recognition ner search classification tag name name like frequent informational elements texts become standard information extraction procedure textual data ner apply many type texts different type entities newspapers fiction historical record persons locations chemical compound protein families animals etc general ner system performance genre domain dependent also use entity categories vary nadeau sekine two thousand and seven general set name entities usually version three partite categorization locations persons organizations paper report first large scale trials evaluation ner data digitize finnish historical newspaper collection digi experiment result discussion research serve development web collection historical finnish newspapers digi collection contain one million, nine hundred and sixty thousand, nine hundred and twenty-one page newspaper material years one thousand, seven hundred and seventy-one one thousand, nine hundred and ten finnish swedish use material finnish document evaluation ocred newspaper collection lot ocr errors estimate word level correctness seventy seventy-five kettunen paakkonen two thousand and sixteen principal ner tagger rule base tagger finnish finer provide fin clarin consortium show also result limit category semantic tag tool semantic compute research group seco aalto university three tool also evaluate briefly research report first publish large scale result ner historical finnish ocred newspaper collection result research supplement ner result languages similar noisy data
word embeddings ubiquitous form word representation natural language process applications word embeddings monolingual word sense disambiguation wsd english comparisons do paper attempt bridge gap examine popular embeddings task monolingual english wsd simplify method lead comparable state art performance without expensive retrain cross lingual wsd word sense word source language e come separate target translation language f also assist language learn example provide translations target vocabulary learners thus also apply word embeddings novel task cross lingual wsd chinese provide public dataset benchmarking also experiment use word embeddings lstm network find surprisingly basic lstm network work well discuss ramifications outcome
exploit facebook reaction feature distant supervise fashion train support vector machine classifier emotion detection use several feature combinations combine different facebook page test model exist benchmarks emotion detection show employ information derive completely automatically thus without rely handcraft lexicon usually do achieve competitive result result also show large room improvement especially gear collection facebook page view target domain
bootstrap state art part speech tagger tag italian twitter data context evalita two thousand and sixteen postwita share task show train tagger native twitter data enrich little amount specifically select gold data additional silver label data scrap facebook yield better result use large amount manually annotate data mix genres
diachronic corpus italian build consecutive vector space time use compare term cosine similarity different time span assume drop similarity might relate emergence metaphorical sense give time similarity base observations match actual year figurative mean document reference dictionary manual inspection corpus occurrences
encoder decoder model widely use solve sequence sequence prediction task however current approach suffer two shortcomings first encoders compute representation word take account history word read far yield suboptimal representations second current decoders utilize large vocabularies order minimize problem unknown word result slow decode time paper address shortcomings towards goal first introduce simple mechanism first read input sequence commit representation word furthermore propose simple copy mechanism able exploit small vocabularies handle vocabulary word demonstrate effectiveness approach gigaword dataset duc competition outperform state art
aim syntactic track classify spatio temporal pattern target motion use natural language process model paper generalize earlier work consider constrain stochastic context free grammar cscfg model pattern confine roadmap constrain grammar facilitate model specific directions road name roadmap present novel particle filter algorithm exploit cscfg model estimate target pattern meta level algorithm operate conjunction base level track algorithm extensive numerical result use simulate grind move target indicator gmti radar measurements show substantial improvement target track accuracy
suggest new method create use gold standard datasets word similarity evaluation goal improve reliability evaluation redesign annotation task achieve higher inter rater agreement define performance measure take reliability annotation decision dataset account
ibm watson cognitive compute system capable question answer natural languages believe ibm watson understand large corpora answer relevant question effectively question answer system currently available unleash full power watson however need train instance large number well prepare question answer pair obviously manually generate pair large quantity prohibitively time consume significantly limit efficiency watson train recently large scale dataset thirty million question answer pair report assumption use automatically generate dataset could relieve burden manual question answer generation try use dataset train instance watson check train efficiency accuracy accord experiment use auto generate dataset effective train watson complement manually craft question answer pair best author knowledge work first attempt use large scale dataset automatically generate question answer pair train ibm watson anticipate insights lessons obtain experiment useful researchers want expedite watson train leverage automatically generate question answer pair
sentiment understand long term goal ai past decades paper deal sentence level sentiment classification though variety neural network model propose recently however previous model either depend expensive phrase level annotation whose performance drop substantially train sentence level annotation fully employ linguistic resources eg sentiment lexicons negation word intensity word thus able produce linguistically coherent representations paper propose simple model train sentence level annotation also attempt generate linguistically coherent representations employ regularizers model linguistic role sentiment lexicons negation word intensity word result show model effective capture sentiment shift effect sentiment negation intensity word still obtain competitive result without sacrifice model simplicity
language identification important aspect automatic speaker recognition many change new approach ameliorate performance last decade compare performance use audio spectrum log scale use polyphonic sound sequence raw audio sample train neural network classify speech either english spanish achieve use novel approach use convolutional recurrent neural network use long short term memory lstm gate recurrent unit gru forward propagation neural network hypothesis performance use polyphonic sound sequence feature lstm gru gate mechanisms neural network outperform traditional mfcc feature use unidirectional deep neural network
increase interpret need objective automatic measurement hold basic idea translate mean translate mean assessment interpretation quality compare mean interpret output source input translation unit chunk name frame come frame semantics components name frame elements fes come frame net propose explore match rate target source texts case study paper verify usability semi automatic grade semantic score measurement human simultaneous interpret show use frame fe match score experiment result show semantic score metrics significantly correlation coefficient human judgment
paper present approach classify document language english topical label space without text categorization train data approach cross lingual dataless document classification clddc rely map english label short category description wikipedia base semantic representation use target language wikipedia consequently performance could suffer wikipedia target language small paper focus languages small wikipedias small wikipedia languages swls use word level dictionary convert document swl large wikipedia language lwls perform clddc base lwl wikipedia approach apply thousands languages contrast machine translation supervision heavy approach do one hundred languages also develop rank algorithm make use language similarity metrics automatically select good lwl show significantly improve classification swls document perform comparably best bridge possible
joint representation learn text knowledge within unify semantic space enable us perform knowledge graph completion accurately work propose novel framework embed word entities relations continuous vector space model entity relation embeddings learn take knowledge graph plain text consideration experiment evaluate joint learn model three task include entity prediction relation prediction relation classification text experiment result show model significantly consistently improve performance three task compare baselines
present summarunner recurrent neural network rnn base sequence model extractive summarization document show achieve performance better comparable state art model additional advantage interpretable since allow visualization predictions break abstract feature information content salience novelty another novel contribution work abstractive train extractive model train human generate reference summaries alone eliminate need sentence level extractive label
conditional random field crf recurrent neural model achieve success structure prediction recently marriage crf recurrent neural model gain non linear dense feature globally normalize crf objective recurrent neural crf model mainly focus encode node feature crf undirected graph however edge feature prove important crf structure prediction work introduce new recurrent neural crf model learn non linear edge feature thus make non linear feature encode completely compare model different neural model well know structure prediction task experiment show model outperform state art methods np chunk shallow parse chinese word segmentation pos tag
focus name entity recognition ner chinese social media massive unlabeled text quite limit label corpus propose semi supervise learn model base b lstm neural network take advantage traditional methods ner crf combine transition probability deep learn model bridge gap label accuracy f score ner construct model directly train f score consider instability f score drive method meaningful information provide label accuracy propose integrate method train f score label accuracy integrate model yield seven hundred and forty-four improvement previous state art result
present two novel contrast recurrent neural network rnn base architectures extractive summarization document classifier base architecture sequentially accept reject sentence original document order membership final summary selector architecture hand free pick one sentence time arbitrary order piece together summary model architectures jointly capture notions salience redundancy sentence addition model advantage interpretable since allow visualization predictions break abstract feature information content salience redundancy show model reach outperform state art supervise model two different corpora also recommend condition one architecture superior base experimental evidence
topic model report beneficial aspect base sentiment analysis paper report simple topic model sarcasm detection first best knowledge design basis intuition sarcastic tweet likely mixture word sentiments tweet literal sentiment either positive negative hierarchical topic model discover sarcasm prevalent topics topic level sentiment use dataset tweet label use hashtags model estimate topic level sentiment level distributions evaluation show topics work gun laws weather sarcasm prevalent topics model also able discover mixture sentiment bear word exist text give sentiment relate label finally apply model predict sarcasm tweet outperform two prior work base statistical classifiers specific feature around twenty-five
article provide interest exploration character level convolutional neural network solve chinese corpus text classification problem construct large scale chinese language dataset result show character level convolutional neural network work better chinese corpus correspond pinyin format dataset first time character level convolutional neural network apply text classification problem
objective allow patients access electronic health record ehr note online patient portals potential improve patient center care however medical jargon abound ehr note show barrier patient ehr comprehension exist knowledge base link medical jargon lay term definitions play important role alleviate problem low coverage medical jargon ehrs develop data drive approach mine ehrs identify rank medical jargon base importance patients support build ehr centric lay language resources methods develop innovative adapt distant supervision ads model base support vector machine rank medical jargon ehrs distant supervision utilize open access collaborative consumer health vocabulary large publicly available resource link lay term medical jargon explore knowledge base feature unify medical language system distribute word representations learn unlabeled large corpora evaluate ads model use physician identify important medical term result ads model significantly surpass two state art automatic term recognition methods tfidf c value yield eight hundred and ten roc auc versus seven hundred and ten six hundred and sixty-seven respectively model identify 10k important medical jargon term rank 100k candidate term mine seven thousand, five hundred ehr narratives conclusion work important step towards enrich lexical resources link medical jargon lay term definitions support patient ehr comprehension identify medical jargon term rank available upon request
recent work begin explore neural acoustic word embeddings fix dimensional vector representations arbitrary length speech segment correspond word embeddings applicable speech retrieval recognition task reason whole word may make possible avoid ambiguous sub word representations main idea map acoustic sequence fix dimensional vectors examples word map similar vectors different word examples map different vectors work take multi view approach learn acoustic word embeddings jointly learn embed acoustic sequence correspond character sequence use deep bidirectional lstm embed model multi view contrastive losses study effect different loss variants include fix margin cost sensitive losses acoustic word embeddings improve previous approach task word discrimination also present result task enable multi view approach include cross view word discrimination word similarity
long text bring big challenge semantic match due complicate semantic syntactic structure tackle challenge consider use prior knowledge help identify useful information filter noise match long text end propose knowledge enhance hybrid neural network kehnn model fuse prior knowledge word representations knowledge gate establish three match channel word sequential structure sentence give gate recurrent units gru knowledge enhance representations three channel process convolutional neural network generate high level feature match feature synthesize match score multilayer perceptron model extend exist methods conduct match word local structure sentence global context sentence evaluation result extensive experiment public data set question answer conversation show kehnn significantly outperform state art match model particularly improve performance pair long text
work use recent advance representation learn propose neural architecture problem natural language inference approach align mimic human natural language inference process give two statements model use variants long short term memory lstm attention mechanism composable neural network carry task part model map clear functionality humans carry overall task natural language inference model end end differentiable enable train stochastic gradient descent stanford natural language inferencesnli dataset propose model achieve better accuracy number publish model literature
paper present first attempt build multilingual neural machine translation framework unify approach able employ attention base nmt many many multilingual translation task approach require special treatment network architecture allow us learn minimal number free parameters standard way train approach show effectiveness resourced translation scenario considerable improvements twenty-six bleu point addition approach achieve interest promise result apply translation task direct parallel corpus source target languages
document similarity problem estimate degree give pair document similar semantic content accurate document similarity measure improve several enterprise relevant task document cluster text mine question answer paper show document thematic flow often disregard bag word techniques pivotal estimate similarity end propose novel semantic document similarity framework call simdoc model document topic sequence topics represent latent generative cluster relate word use sequence alignment algorithm estimate semantic similarity conceptualize novel mechanism compute topic topic similarity fine tune system experiment show simdoc outperform many contemporary bag word techniques accurately compute document similarity practical applications document cluster
extract correct location information text data ie determine place event long goal automate text process approximate human like cod schema introduce supervise machine learn algorithm classify location word either correct incorrect use news article collect around world integrate crisis early warn system icews data open event data alliance oeda data test algorithm consist two stag feature selection stage extract contextual information texts namely n gram pattern location word frequency mention context sentence contain location word classification stage use three classifiers estimate model parameters train set predict whether location word test set news article place event validation result show algorithm improve accuracy rate current geolocation methods dictionary approach much twenty-five
recent neural machine translation approach deliver state art performance resource rich language pair suffer data scarcity problem resource scarce language pair although problem alleviate exploit pivot language bridge source target languages source pivot pivot target translation model usually independently train work introduce joint train algorithm pivot base neural machine translation propose three methods connect two model enable interact train experiment europarl wmt corpora show joint train source pivot pivot target model lead significant improvements independent train across various languages
sentence order one important task nlp previous work mainly focus improve performance use pair wise strategy however nontrivial pair wise model incorporate contextual sentence information addition error prorogation could introduce use pipeline strategy pair wise model paper propose end end neural approach address sentence order problem use pointer network ptr net alleviate error propagation problem utilize whole contextual information experimental result show effectiveness propose model source cod dataset paper available
paper describe centre development advance compute cdacm submission share task tool contest pos tag code mix indian social media facebook twitter whatsapp text collocate icon two thousand and sixteen share task predict part speech pos tag word level give text code mix text generate mostly social media multilingual users presence multilingual word transliterations spell variations make content linguistically complex paper propose approach pos tag code mix social media text use recurrent neural network language model rnn lm architecture submit result hindi english hi en bengali english bn en telugu english te en code mix data
national library finland digitize historical newspapers publish finland one thousand, seven hundred and seventy-one one thousand, nine hundred and ten collection contain approximately one hundred and ninety-five million page finnish swedish finnish part collection consist two hundred and forty billion word national library digital collections offer via digikansalliskirjastofi web service also know digi part newspaper material one thousand, seven hundred and seventy-one one thousand, eight hundred and seventy-four also available freely downloadable language bank finland provide finclarin consortium collection also access korp environment develop spraakbanken university gothenburg extend finclarin team university helsinki provide concordances text resources cranfield style information retrieval test collection also produce small part digi newspaper material university tampere quality ocred collections important topic digital humanities affect general usability searchability collections single available method assess quality large collections different methods use approximate quality paper discuss different corpus analysis style methods approximate overall lexical quality finnish part digi collection methods include usage parallel sample word error rat usage morphological analyzers frequency analysis word comparisons comparable edit lexical data aim quality analysis twofold firstly analyze present state lexical data secondly establish set assessment methods build compact procedure quality assessment eg new ocring post correction material discussion part paper shall synthesize result different analyse
summit work spanish golden age forefather call picaresque novel life lazarillo de tormes fortunes adversities still remain anonymous text although distinguish scholars try attribute different author base variety criteria consensus yet reach list candidates long enjoy support within scholarly community analyze work data drive perspective apply machine learn techniques style text fingerprint would light authorship lazarillo state art survey discuss methods use perform specific case accord methodology likely author seem juan arce de ot alora closely follow alfonso de vald es method state certain attribution make give corpus
recently neural network model natural language process task increasingly focus ability alleviate burden manual feature engineer however previous neural model extract complicate feature compositions traditional methods discrete feature work propose feature enrich neural model joint chinese word segmentation part speech tag task specifically simulate feature templates traditional discrete feature base model use different filter model complex compositional feature convolutional pool layer utilize long distance dependency information recurrent layer experimental result five different datasets show effectiveness propose model
recurrent neural network grammars rnng recently propose probabilistic generative model family natural language show state art language model parse performance investigate information learn linguistic perspective various ablations model data augment model attention mechanism ga rnng enable closer inspection find explicit model composition crucial achieve best performance attention mechanism find headedness play central role phrasal representation model latent attention largely agree predictions make hand craft head rule albeit important differences train grammars without nonterminal label find phrasal representations depend minimally nonterminals provide support endocentricity hypothesis
data representation fundamental task machine learn representation data affect performance whole machine learn system long history representation data do feature engineer researchers aim design better feature specific task recently rapid development deep learn representation learn bring new inspiration various domains natural language process widely use feature representation bag word model model data sparsity problem keep word order information feature part speech tag complex syntax feature fit specific task case thesis focus word representation document representation compare exist systems present new model first generate word embeddings make comprehensive comparisons among exist word embed model term theory figure relationship two important model ie skip gram glove experiment analyze three key point generate word embeddings include model construction train corpus parameter design evaluate word embeddings three type task argue cover exist use word embeddings theory practical experiment present guidelines generate good word embed second chinese character word representation introduce joint train chinese character word third document representation analyze exist document representation model include recursive nns recurrent nns convolutional nns point drawbacks model present new model recurrent convolutional neural network
large scale comparisons poetry tang song dynasties would light word collocations expressions use share among poets word use tang poetry song poetry could lead interest research linguistics frequent color different tang song poetry provide trace change social circumstances dynasties result current work link research topics lexicography semantics social transition discuss find present algorithms efficient comparisons among poems crucial complete billion time comparisons within acceptable time
paper propose pass phrase dependent background model pbms text dependent td speaker verification sv integrate pass phrase identification process conventional td sv system pbm derive text independent background model adaptation use utterances particular pass phrase train pass phrase specific target speaker model derive particular pbm use train data respective target model test best pbm first select test utterance maximum likelihood ml sense select pbm use log likelihood ratio llr calculation respect claimant model propose method incorporate pass phrase identification step llr calculation consider conventional standalone td sv systems performance propose method compare conventional text independent background model base td sv systems use either gaussian mixture model gmm universal background model ubm hide markov model hmm ubm vector paradigms addition consider two approach build pbms speaker independent speaker dependent show propose method significantly reduce error rat text dependent speaker verification non target type target wrong imposter wrong maintain comparable td sv performance imposters speak correct utterance respect conventional system experiment conduct reddots challenge rsr2015 databases consist short utterances
recurrent neural network rnn one popular architectures use natural language process nlp task recurrent structure suitable process variable length text rnn utilize distribute representations word first convert tokens comprise text vectors form matrix matrix include two dimension time step dimension feature vector dimension exist model usually utilize one dimensional 1d max pool operation attention base operation time step dimension obtain fix length vector however feature feature vector dimension mutually independent simply apply 1d pool operation time step dimension independently may destroy structure feature representation hand apply two dimensional 2d pool operation two dimension may sample meaningful feature sequence model task integrate feature dimension matrix paper explore apply 2d max pool operation obtain fix length representation text paper also utilize 2d convolution sample meaningful information matrix experiment conduct six text classification task include sentiment analysis question classification subjectivity classification newsgroup classification compare state art model propose model achieve excellent performance four six task specifically one propose model achieve highest accuracy stanford sentiment treebank binary classification fine grain classification task
transliterations play important role multilingual entity reference resolution proper name increasingly travel languages news social media previous work associate machine translation target transliteration single language pair focus specific class entities cities celebrities rely manual curation limit expression power transliteration multilingual environment contrast present unsupervised transliteration model cover sixty-nine major languages generate good transliterations arbitrary string language pair model yield top one twenty one hundred average three thousand, two hundred and eighty-five six thousand and forty-four eight thousand, three hundred and twenty match gold standard transliteration compare result recently publish system two thousand, six hundred and seventy-one five thousand and twenty-seven seven thousand, two hundred and seventy-nine also show quality model detect true false friends wikipedia high frequency lexicons method indicate strong signal pronunciation similarity boost probability find true friends sixty-eight sixty-nine languages
sequential lstm extend model tree structure give competitive result number task exist methods model constituent tree bottom combinations constituent nod make direct use input word information leaf nod different sequential lstms contain reference input word node paper propose method automatic head lexicalization tree structure lstms propagate head word leaf nod every constituent node addition enable head lexicalization build tree lstm top direction correspond bidirectional sequential lstm structurally experiment show extensions give better representations tree structure final model give best result standford sentiment treebank highly competitive result trec question type classification task
context natural language process representation learn emerge newly active research subject excellent performance many applications learn representations word pioneer study school research however paragraph sentence document embed learn suitable reasonable task sentiment classification document summarization nevertheless far aware relatively less work focus development unsupervised paragraph embed methods classic paragraph embed methods infer representation give paragraph consider word occur paragraph consequently stop function word occur frequently may mislead embed learn process produce misty paragraph representation motivate observations major contributions paper twofold first propose novel unsupervised paragraph embed method name essence vector ev model aim distil representative information paragraph also exclude general background information produce informative low dimensional vector representation paragraph second view increase importance speak content process extension ev model name denoising essence vector ev model propose ev model inherit advantage ev model also infer robust representation give speak paragraph imperfect speech recognition
large scale knowledge base currently reach impressive size however knowledge base still far complete addition exist methods knowledge base completion consider direct link entities ignore vital impact consistent semantics relation paths paper study problem better embed entities relations knowledge base different low dimensional space take full advantage additional semantics relation paths propose compositional learn model relation path embed rpe specifically correspond relation path projections rpe simultaneously embed entity two type latent space also propose type constraints could extend traditional relation specific constraints new propose path specific constraints result experiment show propose model achieve significant consistent improvements compare state art algorithms
automatically recognize terminology widely use various domain specific texts process task machine translation information retrieval sentiment analysis however still agreement methods best suit particular settings moreover reliable comparison already develop methods believe one main reason lack state art methods implementations usually non trivial recreate order address issue present atr4s open source software write scala comprise fifteen methods automatic terminology recognition atr implement whole pipeline text document preprocessing term candidates collection term candidates score finally term candidates rank highly scalable modular configurable tool support automatic cache also compare ten state art methods seven open datasets average precision process time experimental comparison reveal single method demonstrate best average precision datasets available tool atr contain best methods
significant number neural architectures read comprehension recently develop evaluate large cloze style datasets present experiment support emergence predication structure hide state vectors readers specifically provide evidence hide state vectors represent atomic formulas phic phi semantic property predicate c constant symbol entity identifier
spell errors introduce text either type user know correct phoneme grapheme language contain complex word like sandhi two morphemes join base rule spell check become tedious situations spell checker sandhi splitter alert user flag errors provide suggestions useful novel algorithm sandhi split propose paper sandhi splitter split seven thousand common sandhi word kannada language use test sample sandhi splitter integrate kannada spell checker mechanism generate suggestions add comprehensive platform independent standalone spell checker sandhi splitter application software thus develop test extensively efficiency correctness comparative analysis spell checker sandhi splitter make result conclude kannada spell checker sandhi splitter improve performance twice fast two hundred time space efficient ninety accurate case complex nouns fifty accurate complex verbs spell checker sandhi splitter foremost significance machine translation systems voice process etc first sandhi splitter kannada advantage novel algorithm extend indian languages
although attention base neural machine translation achieve great success attention mechanism capture entire mean source sentence attention mechanism generate target word depend heavily relevant part source sentence report earlier study introduce latent variable capture entire mean sentence achieve improvement attention base neural machine translation follow approach believe capture mean sentence benefit image information human be understand mean language textual information also perceptual information gain vision describe herein propose neural machine translation model introduce continuous latent variable contain underlie semantic extract texts image model train end end require image information train experiment conduct english german translation task show model outperform baseline
paper propose simple fast decode algorithm foster diversity neural generation algorithm modify standard beam search algorithm add inter sibling rank penalty favor choose hypotheses diverse parent evaluate propose model task dialogue response generation abstractive summarization machine translation find diverse decode help across task especially reranking need propose variation capable automatically adjust diversity decode rat different input use reinforcement learn rl observe performance boost rl technique paper include material unpublished script mutual information diverse decode improve neural machine translation li jurafsky two thousand and sixteen
recurrent neural network rnns achieve great success language model however since rnns fix size memory memory store information word see sentence thus useful long term information may ignore predict next word paper propose attention base memory selection recurrent network amsrn model review information store memory previous time step select relevant information help generate output amsrn attention mechanism find time step store relevant information memory memory selection determine dimension memory involve compute attention weight information extractedin experiment amsrn outperform long short term memory lstm base language model english chinese corpora moreover investigate use entropy regularizer attention weight visualize attention mechanism help language model
objective knowledge graph embed encode entities relations knowledge graph continuous low dimensional vector space previously work focus symbolic representation knowledge graph structure information handle new entities entities facts well paper propose novel deep architecture utilize structural textual information entities specifically introduce three neural model encode valuable information text description entity among attentive model select relate information need gate mechanism apply integrate representations structure text unify architecture experiment show model outperform baseline margin link prediction triplet classification task source cod paper available github
unsupervised model dependency parse typically require large amount clean unlabeled data plus gold standard part speech tag add indirect supervision eg language universals rule help show obtain small amount direct supervision partial dependency annotations provide strong balance zero full supervision adapt unsupervised convexmst dependency parser learn partial dependencies express graph fragment language less twenty-four hours total annotation obtain seven seventeen absolute improvement unlabeled dependency score english spanish respectively compare parser use universal grammar constraints
prepositions common ambiguous understand sense critical understand mean sentence supervise corpora preposition sense disambiguation task small suggest semi supervise approach task show signal unannotated multilingual data use improve supervise preposition sense disambiguation approach pre train lstm encoder predict translation preposition incorporate pre train encoder component supervise classification system fine tune task multilingual signal consistently improve result two preposition sense datasets
identify correct grammatical errors text write non native writers receive increase attention recent years although number annotate corpora establish facilitate data drive grammatical error detection correction approach still limit term quantity coverage human annotation labor intensive time consume expensive work propose utilize unlabeled data train neural network base grammatical error detection model basic idea cast error detection binary classification problem derive positive negative train examples unlabeled data introduce attention base neural network capture long distance dependencies influence word detect experiment show propose approach significantly outperform svms convolutional network fix size context window
cardiovascular disease cvd become lead death china case prevent control risk factor goal study build corpus cvd risk factor annotations base chinese electronic medical record cemrs corpus intend use develop risk factor information extraction system turn apply foundation study progress risk factor cvd design light annotation task capture cvd risk factor indicators temporal attribute assertions explicitly implicitly display record task include one prepare data two create guidelines capture annotations create help clinicians three propose annotation method include build guidelines draft train annotators update guidelines corpus construction risk factor annotate corpus base de identify discharge summaries progress note six hundred patients develop build help clinicians corpus inter annotator agreement iaa f1 measure nine hundred and sixty-eight indicate high reliability best knowledge first annotate corpus concern cvd risk factor cemrs guidelines capture cvd risk factor annotations cemrs propose obtain document level annotations apply future study monitor risk factor cvd long term
use reinforcement learn learn tree structure neural network compute representations natural language sentence contrast prior work tree structure model tree either provide input predict use supervision explicit treebank annotations tree structure work optimize improve performance downstream task experiment demonstrate benefit learn task specific composition order outperform sequential encoders recursive encoders base treebank annotations analyze induce tree show discover linguistically intuitive structure eg noun phrase simple verb phrase different conventional english syntactic structure
propose single neural network architecture two task line keyword spot voice activity detection develop novel inference algorithms end end recurrent neural network train connectionist temporal classification loss function allow model achieve high accuracy keyword spot voice activity detection without retrain contrast prior voice activity detection model architecture require align train data use parameters keyword spot model allow us deploy high quality voice activity detector additional memory maintenance requirements
paper propose simple test compositionality ie literal usage word phrase context specific way test computationally simple rely external resources use set train word vectors experiment show propose method competitive state art display high accuracy context specific compositionality detection variety natural language phenomena idiomaticity sarcasm metaphor different datasets multiple languages key insight connect compositionality curious geometric property word embeddings independent interest
paper study generate natural languages particular contexts situations propose two novel approach encode contexts continuous semantic representation decode semantic representation text sequence recurrent neural network decode context information attend gate mechanism address problem long range dependency cause lengthy sequence evaluate effectiveness propose approach user review data rich contexts available two informative contexts sentiments products select evaluation experiment show fake review generate approach natural result fake review detection human judge show fifty fake review misclassified real review ninety misclassified exist state art fake review detection algorithm
patent property right invention grant government inventor invention solution specific technological problem patent often high concentration scientific technical term rare everyday language chinese word segmentation model train currently available everyday language data set perform poorly effectively recognize scientific technical term paper describe pragmatic approach chinese word segmentation patent train character base semi supervise sequence label model extract feature manually segment corpus one hundred and forty-two patent enhance information extract chinese treebank experiment show accuracy model reach nine thousand, five hundred and eight f1 score hold test set nine thousand, six hundred and fifty-nine development set compare f1 score nine thousand, one hundred and forty-eight development set model train chinese treebank also experiment exist domain adaptation techniques result show amount target domain data select feature impact performance domain adaptation techniques
paper aim provide comprehensive model representation etymological data digital dictionaries purpose integrate one coherent framework digital representations legacy dictionaries also bear digital lexical databases construct manually semi automatically want propose systematic coherent set model principles variety etymological phenomena may contribute creation continuum exist future lexical construct anyone interest trace history word mean able seamlessly query lexical resourcesinstead design ad hoc model representation language digital etymological data focus identify possibilities offer tei guidelines representation lexical information
human evaluation machine translation normally use sentence level measure relative rank adequacy scale however provide insight possible errors scale well sentence length argue semantics base evaluation capture mean components retain mt output thus provide fine grain analysis translation quality enable construction tune semantics base mt present novel human semantic evaluation measure human ucca base mt evaluation hume build ucca semantic representation scheme hume cover wider range semantic phenomena previous methods rely semantic annotation potentially garble mt output experiment four language pair demonstrate hume broad applicability report good inter annotator agreement rat correlation human adequacy score
user simulation essential generate enough data train statistical speak dialogue system previous model user simulation suffer several drawbacks inability take dialogue history account need rigid structure ensure coherent user behaviour heavy dependence specific domain inability output several user intentions one dialogue turn requirement summarize action space tractability paper introduce data drive user simulator base encoder decoder recurrent neural network model take input sequence dialogue contexts output sequence dialogue act correspond user intentions dialogue contexts include information machine act status user goal show dialogue state track challenge two dstc2 dataset sequence sequence model outperform agenda base simulator n gram simulator accord f score furthermore show model use original action space thereby model user behaviour finer granularity
computer systems need able react stress order perform optimally task article describe tensistrength system detect strength stress relaxation express social media text message tensistrength use lexical approach set rule detect direct indirect expressions stress relaxation particularly context transportation slightly effective comparable sentiment analysis program although similar performances occur despite differences almost half tweet gather effectiveness tensistrength depend nature tweet classify tweet rich stress relate term particularly problematic although generic machine learn methods give better performance tensistrength overall exploit topic relate term way may undesirable practical applications may work well focus contexts conclusion tensistrength generic machine learn approach work well enough practical choices intelligent applications need take advantage stress information decision use depend nature texts analyse purpose task
state art approach name entity recognition rely hand craft feature annotate corpora recently neural network base model propose require handcraft feature still require annotate corpora however annotate corpora may available many languages paper propose neural network base model allow share decoder well word character level parameters two languages thereby allow resource fortunate language aid resource deprive language specifically focus case limit annotate corpora available one language l1 abundant annotate corpora available another language l2 share network architecture parameters l1 l2 lead improve performance l1 approach require hand craft feature instead directly learn meaningful feature representations train data experiment four language pair show indeed resource constrain setup lesser annotate corpora model jointly train data another language perform better model train limit corpora one language
word embeddings recently see strong increase interest result strong performance gain variety task however research also underline importance benchmark datasets difficulty construct variety language specific task still many datasets use task could prove fruitful linguistic resources allow unique observations language use variability paper demonstrate performance multiple type embeddings create count prediction base architectures variety corpora two language specific task relation evaluation dialect identification latter compare unsupervised methods traditional hand craft dictionary research provide embeddings relation evaluation task benchmark use research demonstrate benchmarked embeddings prove useful unsupervised linguistic resource effectively use downstream task
hide markov model hmm often regard dynamical model choice many field applications also heart state art speech recognition systems since seventy however gaussian mixture model hmms gmm hmm deep neural network hmms dnn hmm underlie markovian chain state art model change much leave right topology mostly always employ alternatives exist paper propose finely tune hmm topologies essential precise temporal model approach investigate state art hmm system propose proof concept framework learn efficient topologies prune complex generic model speech recognition experiment conduct indicate complex time dependencies better learn approach classical leave right model
paper describe technique compare large text source use word vector representations word2vec dimensionality reduction sne implement use python technique provide bird eye view text source eg text summaries source material enable users explore text source like geographical map word vector representations capture many linguistic properties gender tense plurality even semantic concepts like capital city use dimensionality reduction 2d map compute semantically similar word close technique use word2vec model gensim python library sne scikit learn
first observe potential weakness continuous vector representations symbols neural machine translation continuous vector representation word embed vector symbol encode multiple dimension similarity equivalent encode one mean word consequence encoder decoder recurrent network neural machine translation need spend substantial amount capacity disambiguate source target word base context define source sentence base observation paper propose contextualize word embed vectors use nonlinear bag word representation source sentence additionally propose represent special tokens number proper nouns acronyms type symbols facilitate translate word well suit translate via continuous vectors experiment en fr en de reveal propose approach contextualization symbolization improve translation quality neural machine translation systems significantly
work introduce temporal hierarchies sequence sequence seq2seq model tackle problem abstractive summarization scientific article propose multiple timescale model gate recurrent unit mtgru implement encoder decoder set better deal presence multiple compositionalities larger texts propose model compare conventional rnn encoder decoder result demonstrate model train faster show significant performance gain result also show temporal hierarchies help improve ability seq2seq model capture compositionalities better without presence highly complex architectural hierarchies
cross lingual projection linguistic annotation suffer many source bias noise lead unreliable annotations use directly paper introduce novel approach sequence tag learn correct errors cross lingual projection use explicit debiasing layer frame joint learn two corpora one tag gold standard project tag evaluate one thousand tokens tag gold standard tag along plentiful parallel data system equal exceed state art eight simulate low resource settings well two real low resource languages malagasy kinyarwanda
discriminative translation model utilize source context show help statistical machine translation performance propose novel extension work use target context information surprisingly show model efficiently integrate directly decode process approach scale large train data size result consistent improvements translation quality four language pair also provide analysis compare strengths baseline source context model extend source context target context model show extension allow us better capture morphological coherence work freely available part moses
goal combine rich multistep inference symbolic logical reason generalization capabilities neural network particularly interest complex reason entities relations text large scale knowledge base kbs neelakantan et al two thousand and fifteen use rnns compose distribute semantics multi hop paths kbs however multiple reason approach lack accuracy practicality paper propose three significant model advance one learn jointly reason relations entities entity type two use neural attention model incorporate multiple paths three learn share strength single rnn represent logical composition across relations largescale freebaseclueweb prediction task achieve twenty-five error reduction fifty-three error reduction sparse relations due share strength chain reason wordnet reduce error mean quantile eighty-four versus previous state art code data available https rajarshdgithubio chainsofreasoning
introduce first global recursive neural parse model optimality guarantee decode support global feature give dynamic program instead search directly space possible subtrees although space exponentially large sentence length show possible learn efficient parser augment exist parse model informative bound outside score global model loose bound need model non local phenomena global model train new objective encourage parser explore tiny fraction search space approach apply ccg parse improve state art accuracy four f1 parser find optimal parse nine hundred and ninety-nine hold sentence explore average one hundred and ninety subtrees
normative texts document base deontic notions obligation permission prohibition goal model texts use c diagram formalism make amenable formal analysis particular verify text satisfy properties concern causality action time constraints present experimental semi automatic aid bridge gap normative text formal representation approach use dependency tree combine rule heuristics extract relevant components result tabular data convert c diagram
paper explore simple efficient baseline text classification experiment show fast text classifier fasttext often par deep learn classifiers term accuracy many order magnitude faster train evaluation train fasttext one billion word less ten minutes use standard multicorecpu classify half million sentence among312k class less minute
order control computational complexity neural machine translation nmt systems convert rare word outside vocabulary single unk symbol previous solution luong et al two thousand and fifteen resort use multiple number unks learn correspondence source target rare word however test word unseen train corpus handle method also suffer noisy word alignment paper focus major type rare word name entity ne propose translate character level sequence sequence model ne translation model use derive high quality ne alignment bilingual train corpus integration ne translation alignment modules nmt system able surpass baseline system twenty-nine bleu point chinese english task
present charagram embeddings simple approach learn character base compositional model embed textual sequence word sentence represent use character n gram count vector follow single nonlinear transformation yield low dimensional embed use three task evaluation word similarity sentence similarity part speech tag demonstrate charagram embeddings outperform complex architectures base character level recurrent convolutional neural network achieve new state art performance several similarity task
paper identify several serious problems arise use syntactic data sswl database purpose computational phylogenetic reconstruction show naive approach fail produce reliable linguistic phylogenetic tree identify source observe problems discuss may least partly correct use additional information prior subdivision language families subfamilies better use information ancient languages also describe use phylogenetic algebraic geometry help estimate extent probability distribution leave phylogenetic tree obtain sswl data consider reliable test phylogenetic tree establish form linguistic analysis simple examples find restrict smaller language subfamilies consider sswl parameters fully map whole subfamily sswl data match extremely well reliable phylogenetic tree accord evaluation phylogenetic invariants promise sign use sswl data linguistic phylogenetics
study investigate use unsupervised word embeddings sequence feature sample representation active learn framework build extract clinical concepts clinical free text objective reduce manual annotation effort achieve higher effectiveness compare set baseline feature unsupervised feature derive skip gram word embeddings sequence representation approach comparative performance unsupervised feature baseline hand craft feature active learn framework investigate use wide range selection criteria include least confidence information diversity information density diversity domain knowledge informativeness two clinical datasets use evaluation i2b2 va two thousand and ten nlp challenge share clef two thousand and thirteen ehealth evaluation lab result demonstrate significant improvements term effectiveness well annotation effort save across datasets use unsupervised feature along baseline feature sample representation lead save nine ten token concept annotation rat respectively
traditional semantic parsers map language onto compositional executable query fix schema map allow effectively leverage information contain large formal knowledge base kbs eg freebase answer question also fundamentally limit semantic parsers assign mean language fall within kb manually produce schema recently propose methods open vocabulary semantic parse overcome limitation learn execution model arbitrary language essentially use text corpus kind knowledge base however prior approach open vocabulary semantic parse replace formal kb textual information make use kb model show combine disparate representations use two approach present first time semantic parser one produce compositional executable representations language two successfully leverage information contain formal kb large corpus three limit schema underlie kb demonstrate significantly improve performance state art baselines open domain natural language question answer task
several task argumentation mine debate question answer natural language inference involve classify sequence context another sequence refer bi sequence classification several single sequence classification task current state art approach base recurrent convolutional neural network hand bi sequence classification problems much understand best deep learn architecture paper attempt get understand category problems extensive empirical evaluation nineteen different deep learn architectures specifically different ways handle context various problems originate natural language process like debate textual entailment question answer follow empirical evaluation offer insights conclusions regard architectures consider also establish first deep learn baselines three argumentation mine task
paper present approach improve accuracy strong transition base dependency parser exploit dependency language model extract large parse corpus integrate small number feature base dependency language model parser demonstrate effectiveness propose approach evaluate parser standard english chinese data base parser could achieve competitive accuracy score enhance parser achieve state art accuracy chinese data competitive result english data gain large absolute improvement one point uas chinese five point english
study role second language bilingual word embeddings monolingual semantic evaluation task find strongly weakly positive correlations stream task performance second language similarity target language additionally show bilingual word embeddings employ task semantic language classification joint semantic space vary meaningful ways across second languages result support hypothesis semantic language similarity influence structural similarity well geography contact
cluster web document numerous applications aggregate news article meaningful events detect trend hot topics web preserve diversity search result etc time importance name entities particular ability recognize solve associate co reference resolution problem widely recognize key enable factor mine aggregate compare content web instead consider two problems separately propose paper method tackle jointly problem cluster news article events cross document co reference resolution name entities co occurrence name entities cluster use additional signal decide whether two referents merge one entity refine entities turn use enhance feature cluster document refine enter virtuous cycle improve simultaneously performances task implement prototype system report result use tdt5 collection news article demonstrate potential approach
recently le mikolov two thousand and fourteen propose doc2vec extension word2vec mikolov et al 2013a learn document level embeddings despite promise result original paper others struggle reproduce result paper present rigorous empirical evaluation doc2vec two task compare doc2vec two baselines two state art document embed methodologies find doc2vec perform robustly use model train large external corpora improve use pre train word embeddings also provide recommendations hyper parameter settings general purpose applications release source code induce document embeddings use train doc2vec model
identify language social media message important first step linguistic process exist model twitter focus content analysis successful dissimilar language pair propose label propagation approach take social graph tweet author account well content better tease apart similar languages result state art share task performance seven thousand, six hundred and sixty-three fourteen higher top system
paper propose methods analyze readability bengali language texts get exceptionally good result experiment
advance topic model yield effective methods characterize latent semantics textual data however apply standard topic model approach sentence level task introduce number challenge paper adapt approach latent dirichlet allocation include additional layer incorporate information sentence boundaries document show addition minimal information document structure improve perplexity result train model
present novel incremental learn approach unsupervised word segmentation combine feature probabilistic model model selection include super additive penalties address cognitive burden impose long word formation new model selection criteria base higher order generative assumptions approach fully unsupervised rely small number parameters permit flexible model mechanism automatically learn parameters data experimentation show intricate design lead top tier performance phonemic orthographic word segmentation
paper introduce variation skip gram model jointly learn distribute word vector representations way compose form phrase embeddings particular propose learn procedure incorporate phrase compositionality function capture want compose phrase vectors component word vectors experiment show improvement word phrase similarity task well syntactic task like dependency parse use propose joint model
sentiment analysis sa indeed fascinate area research steal attention researchers many facets importantly promise economic stake corporate governance sector sa stem text analytics establish separate identity domain research wide range result sa prove influence way critical decisions take hence become relevant thorough understand different dimension input output process approach sa
popularity distance education program increase fast pace en par development online communication fora social media review platforms students increase well exploit information support fellow students institutions require extract relevant opinions order automatically generate report provide overview pros con different distance education program report experiment involve distance education experts goal develop dataset review annotate relevant categories aspects category discuss specific review together indication sentiment base experiment present approach extract general categories specific aspects discussion review together sentiment frame task multi label hierarchical text classification problem empirically investigate performance different classification architectures couple prediction category prediction particular aspects category evaluate different architectures show hierarchical approach lead superior result comparison flat model make decisions independently
doctoral thesis apply premise cognitive linguistics terminological definitions present proposal call flexible terminological definition consist set definitions concept make general definition case one encompass entire environmental domain along additional definitions describe concept perspective subdomains relevant since context determine factor construction mean lexical units include term assume terminological definitions reflect effect context even though definitions traditionally treat expression mean void contextual effect main objective thesis analyze effect contextual variation specialize environmental concepts view representation terminological definitions specifically focus contextual variation base thematic restrictions accomplish objectives doctoral thesis conduct empirical study consist analysis set contextually variable concepts creation flexible definition two result first part empirical study divide notion domain dependent contextual variation three different phenomena modulation perspectivization subconceptualization phenomena additive concepts experience modulation concepts also undergo perspectivization finally small number concepts additionally subject subconceptualization second part apply notions terminological definitions present present guidelines build flexible definitions extraction knowledge actual write definition
introduce attentional mechanism neural network powerful concept achieve impressive result many natural language process task however exist model impose attentional distribution flat topology namely entire input representation sequence clearly well form sentence accompany syntactic tree structure much rich topology apply attention topology exploit underlie syntax also make attention interpretable paper explore direction context natural language inference result demonstrate efficacy also perform extensive qualitative analysis derive insights intuitions model work
paper present novel approach natural language understand utilize context free grammars cfgs conjunction sequence sequence seq2seq deep learn specifically take cfg author generate dialogue target application nlu videogame train long short term memory lstm recurrent neural network rnn map surface utterances produce trace grammatical expansions yield critically cfg author use tool develop support arbitrary annotation nonterminal symbols grammar already annotate symbols grammar semantic pragmatic considerations game dialogue manager operate use grammatical trace associate surface utterance infer information gameplay translate player utterances grammatical trace use rnn collect mark attribute symbols include trace pass information dialogue manager update conversation state accordingly offline evaluation task demonstrate train rnn translate surface utterances grammatical trace great accuracy knowledge first usage seq2seq learn conversational agents game character explicitly reason semantic pragmatic considerations
sentence order general critical task natural language generation applications previous work focus improve performance external downstream task multi document summarization give importance propose study isolate task collect large corpus academic texts derive data drive approach learn pairwise order sentence validate efficacy extensive experiment source cod dataset paper make publicly available
concepts methods complex network use analyse texts different complexity level examples natural language process nlp task study via topological analysis network keyword identification automatic extractive summarization authorship attribution even though myriad network measurements apply study authorship attribution problem use motifs text analysis restrict work goal paper apply concept motifs recurrent interconnection pattern authorship attribution task absolute frequencies thirteen direct motifs three nod extract co occurrence network use classification feature effectiveness feature verify four machine learn methods result show motifs able distinguish write style different author best scenario five hundred and seventy-five book correctly classify chance baseline problem one hundred and twenty-five addition find function word play important role recurrent pattern take together find suggest motifs explore relate linguistic task
paper introduce latent tree language model ltlm novel approach language model encode syntax semantics give sentence tree word roles learn phase iteratively update tree move nod accord gibbs sample introduce two algorithms infer tree give sentence first one base gibbs sample fast guarantee find probable tree second one base dynamic program slower guarantee find probable tree provide comparison algorithms combine ltlm four gram modify kneser ney language model via linear interpolation experiment english czech corpora show significant perplexity reductions forty-six english forty-nine czech compare standalone four gram modify kneser ney language model
paper present computational model process dynamic spatial relations occur embody robotic interaction setup complete system introduce allow autonomous robots produce interpret dynamic spatial phrase english give environment move object model unite two separate research strand computational cognitive semantics commonsense spatial representation reason model first time demonstrate integration different strand
paper discuss ground acquisition experiment increase complexity humanoid robots acquire english spatial lexicons robot tutor identify various spatial language systems projective absolute proximal learn propose learn mechanisms rely direct mean transfer direct access world model interlocutors finally show multiple systems acquire time
job search online match engines nowadays prominent beneficial job seekers employers solutions traditional engines without understand semantic mean different resume keep pace incredible change machine learn techniques compute capability solutions usually drive manual rule predefined weight keywords lead inefficient frustrate search experience end present machine learn solution rich feature deep learn methods solution include three configurable modules plug little restrictions namely unsupervised feature extraction base classifiers train ensemble method learn solution rather use manual rule machine learn methods automatically detect semantic similarity position propose four competitive shallow estimators deep estimators select finally ensemble methods bag estimators aggregate individual predictions form final prediction verify experimental result forty-seven thousand resume show solution significantly improve predication precision current position salary educational background company scale
generate synthetic languages aid test validation future computational linguistic model methods thesis extend beast2 phylogenetic framework add linguistic sequence generation multiple model new plugin use test effect phenomena word borrow inference process two widely use phylolinguistic model
exist methods bilingual word embed consider shallow context simple co occurrence information paper propose latent bilingual sense unit bilingual sense clique bsc derive maximum complete sub graph pointwise mutual information base graph bilingual corpus way treat source target word equally separate bilingual projection process use exist work necessary several dimension reduction methods evaluate summarize bsc word relationship propose method evaluate bilingual lexicon translation task empirical result show bilingual sense embed methods outperform exist bilingual word embed methods
although additional corpora available statistical machine translation smt ones belong similar domains original corpus indeed enhance smt performance directly exist adaptation methods focus sentence selection comparison phrase smaller fine grain unit data selection therefore propose straightforward efficient connect phrase base adaptation method apply bilingual phrase pair monolingual n gram adaptation propose method evaluate iwslt nist data set result show phrase base smt performance significantly improve sixteen comparison phrase base smt baseline system nine comparison exist methods
vanilla sequence sequence learn seq2seq read encode source sequence fix length vector suffer insufficiency model structural correspondence source target sequence instead handle insufficiency linearly weight attention mechanism paper propose use recurrent neural network rnn alternative cseq2seq decode cseq2seq cyclically feed previous decode state back encoder initial state rnn reencodes source representations produce context vectors surprisingly find introduce rnn succeed dynamically detect translationrelated source tokens accord partial target sequence base find hypothesize partial target sequence act feedback improve understand source sequence test hypothesis propose cyclic sequence sequence learn cseq2seq ii differ seq2seq reintroduction previous decode state encoder perform parameter share cseq2seq ii reduce parameter redundancy enhance regularization particular share weight encoder decoder two targetside word embeddings make cseq2seq ii equivalent single conditional rnn model thirty-one parameters prune even better performance cseq2seq ii preserve simplicity seq2seq also yield comparable promise result machine translation task experiment chinese english english german translation show cseq2seq achieve significant consistent improvements seq2seq competitive attention base seq2seq model
authorship attribution important problem many areas cluding information retrieval computational linguistics law journalism etc identify subject increasingly research interest cent years case author identification task pan clef two thousand and fifteen main focus give cross genre cross topic author verification task use several word base style base feature identify dif ferences know unknown problems one give set label unknown ones accordingly use random forest base classifier
subjective sentiment analysis gain considerable attention recently resources systems build far do english need design systems languages increase paper survey different ways use build systems subjective sentiment analysis languages english three different type systems use build systems first best one language specific systems second type systems involve reuse transfer sentiment resources english target language third type methods base use language independent methods paper present separate section devote arabic sentiment analysis
perplexity per word widely use metric evaluate language model despite dearth criticism metric criticisms center around lack correlation extrinsic metrics like word error rate wer dependence upon share vocabulary model comparison unsuitability unnormalized language model evaluation paper address last problem propose new discriminative entropy base intrinsic metric work traditional word level model unnormalized language model like sentence level model also propose discriminatively train sentence level interpretation recurrent neural network base language model rnn example unnormalized sentence level model demonstrate word level model contrastive entropy show strong correlation perplexity also observe train lower distortion level sentence level rnn considerably outperform traditional rnns new metric
distant label information extraction ie suffer noisy train data describe way reduce noise associate distant ie identify couple constraints potential instance label one example couple items list likely label second example couple come analysis document structure corpora section identify items section likely label section exist corpora show augment large corpus couple constraints even small well structure corpus improve performance substantially double f1 one task
build multi source machine translation model train maximize probability target english string give french german source use neural encoder decoder framework explore several combination methods report forty-eight bleu increase top strong attention base neural translation model
provide first extensive evaluation use different type context learn skip gram word embeddings affect performance wide range intrinsic extrinsic nlp task result suggest intrinsic task tend exhibit clear preference particular type contexts higher dimensionality careful tune require find optimal settings extrinsic task consider furthermore extrinsic task find benefit increase embed dimensionality mostly exhaust simple concatenation word embeddings learn different context type yield performance gain additional contribution propose new variant skip gram model learn word embeddings weight contexts substitute word
neural encoder decoder model machine translation achieve impressive result rival traditional translation model however model formulation overly simplistic omit several key inductive bias build traditional model paper extend attentional neural translation model include structural bias word base alignment model include positional bias markov condition fertility agreement translation directions show improvements baseline attentional model standard phrase base model several language pair evaluate difficult languages low resource set
paper discuss experiment carry us jadavpur university part participation icon two thousand and fifteen task pos tag code mix indian social media text tool develop task base trigram hide markov model utilize information dictionary well word level feature enhance observation probabilities know tokens well unknown tokens submit run bengali english hindi english tamil english language pair system train test datasets release icon two thousand and fifteen share task pos tag code mix indian social media text constrain mode system obtain average overall accuracy average three language pair seven thousand, five hundred and sixty close participate two systems seven thousand, six hundred and seventy-nine iiith seven thousand, five hundred and seventy-nine amritacen rank higher system unconstrained mode system obtain average overall accuracy seven thousand and sixty-five also close system seven thousand, two hundred and eighty-five amritacen obtain highest average overall accuracy
recurrent neural network rnn obtain excellent result many natural language process nlp task however understand interpret source success remain challenge paper propose recurrent memory network rmn novel rnn architecture amplify power rnn also facilitate understand internal function allow us discover underlie pattern data demonstrate power rmn language model sentence completion task language model rmn outperform long short term memory lstm network three large german italian english dataset additionally perform depth analysis various linguistic dimension rmn capture sentence completion challenge essential capture sentence coherence rmn obtain six hundred and ninety-two accuracy surpass previous state art large margin
semantic parse aim map natural language machine interpretable mean representations traditional approach rely high quality lexicons manually build templates linguistic feature either domain representation specific paper present general method base attention enhance encoder decoder model encode input utterances vector representations generate logical form condition output sequence tree encode vectors experimental result four datasets show approach perform competitively without use hand engineer feature easy adapt across domains mean representations
name entity disambiguation ned refer task resolve multiple name entity mention document correct reference knowledge base kb eg wikipedia paper propose novel embed method specifically design ned propose method jointly map word entities continuous vector space extend skip gram model use two model kb graph model learn relatedness entities use link structure kb whereas anchor context model aim align vectors similar word entities occur close one another vector space leverage kb anchor context word combine contexts base propose embed standard ned feature achieve state art accuracy nine hundred and thirty-one standard conll dataset eight hundred and fifty-two tac two thousand and ten dataset
recurrent neural network rnn one specific architectures long short term memory lstm widely use sequence label paper first enhance lstm base sequence label explicitly model label dependencies propose another enhancement incorporate global information span whole input sequence latter propose method encoder labeler lstm first encode whole input sequence fix length vector encoder lstm use encode vector initial state another lstm sequence label combine methods predict label sequence consider label dependencies information whole input sequence experiment slot fill task essential component natural language understand use standard atis corpus achieve state art f1 score nine thousand, five hundred and sixty-six
sequence model learn algorithms typically maximize log likelihood minus norm model minimize ham loss norm cross lingual part speech pos tag target language train data consist sequence sentence word word label project translations k languages label data via word alignments train data therefore noisy rademacher complexity high learn algorithms prone overfit norm base regularization assume constant width zero mean prior instead propose use k source language model estimate parameters gaussian prior learn new pos taggers lead significantly better performance multi source transfer set up also present drop version inject empirical gaussian noise online learn finally note use empirical gaussian priors lead much lower rademacher complexity superior optimally weight model interpolation
goal argumentation mine evolve research field computational linguistics design methods capable analyze people argumentation article go beyond state art several ways deal actual web data take challenge give variety register multiple domains unrestricted noisy user generate web discourse ii bridge gap normative argumentation theories argumentation phenomena encounter actual data adapt argumentation model test extensive annotation study iii create new gold standard corpus 90k tokens three hundred and forty document experiment several machine learn methods identify argument components offer data source cod annotation guidelines community free license find show argumentation mine user generate web discourse feasible challenge task
present corpus base analysis effect age gender region origin production netspeak chatspeak feature regional speech feature flemish dutch post collect belgian online social network platform present study show combine quantitative qualitative approach essential understand non standard linguistic variation cmc corpus also present methodology enable systematic study variation include non standard word corpus analyse result convince illustration adolescent peak principle addition approach reveal intrigue correlation use regional speech feature chatspeak feature
introduce trans gram simple computationally efficient method simultaneously learn align wordembeddings variety languages use monolingual data smaller set sentence align data use new method compute align wordembeddings twenty one languages use english pivot language show linguistic feature align across languages align data even though properties exist pivot language also achieve state art result standard cross lingual text classification word translation task
propose novel deep neural network architecture speech recognition explicitly employ knowledge background environmental noise within deep neural network acoustic model deep neural network use predict acoustic environment system use discriminative embed generate bottleneck layer network concatenate traditional acoustic feature input deep neural network acoustic model series experiment resource management chime three task aurora4 show propose approach significantly improve speech recognition accuracy noisy highly reverberant environments outperform multi condition train noise aware train vector framework multi task learn domain noise unseen noise
goal paper investigate connection performance gain obtain selftraining similarity corpora use approach self train semi supervise technique design increase performance machine learn algorithms automatically classify instance task add additional train material classifier context language process task train material mostly annotate corpus unfortunately self train always lead performance increase whether largely unpredictable show similarity corpora use identify setups self train beneficial consider research step process develop classifier able adapt new test corpus present
report present system generate political speeches desire political party furthermore system allow specify whether speech hold supportive oppose opinion system rely combination several state art nlp methods discuss report include n grams justeson katz pos tag filter recurrent neural network latent dirichlet allocation sequence word generate base probabilities obtain two underlie model language model take care grammatical correctness topic model aim textual consistency model train convote dataset contain transcripts us congressional floor debate furthermore present manual automate approach evaluate quality generate speeches experimental evaluation generate speeches show high quality term grammatical correctness sentence transition
neural machine translation show promise result lately nmt model follow encoder decoder framework make encoder decoder model flexible attention mechanism introduce machine translation also task like speech recognition image caption observe quality translation attention base encoder decoder significantly damage alignment incorrect attribute problems lack distortion fertility model aim resolve problems propose new variations attention base encoder decoder compare model machine translation propose method achieve improvement two bleu point original attention base encoder decoder
evograder free online demand formative assessment service design use undergraduate biology classrooms evograder web portal power amazon elastic cloud run lightside lab open source machine learn tool evograder web portal allow biology instructors upload response file csv contain unlimited number evolutionary explanations write response eighty-six different acorns assess contextual reason natural selection instrument items system automatically analyze responses provide detail information scientific naive concepts contain within student response well overall student sample reason model type graph visual model provide evograder summarize class level responses downloadable file raw score csv format also provide detail analyse although computational machinery evograder employ complex use system easy users need know use spreadsheets organize student responses upload file web use web browser series experiment use new sample two thousand, two hundred write evolutionary explanations demonstrate evograder score comparable train human raters although evograder score take ninety-nine less time free evograder interest biology instructors teach large class seek emphasize scientific practice generate scientific explanations teach crosscut ideas evolution natural selection software architecture evograder describe may serve template develop machine learn portals core concepts within biology across discipline
ibm model important word alignment model machine translation follow maximum likelihood estimation principle estimate parameters model easily overfit train data data sparse smooth popular solution language model still lack study smooth word alignment paper propose framework generalize notable work moore two thousand and four apply additive smooth word alignment model framework allow developers customize smooth amount pair word add amount scale appropriately common factor reflect much framework trust add strategy accord performance data also carefully examine various performance criteria propose smoothen version error count generally give best result
paper describe architecture implementation rule base grapheme phoneme converter turkish system accept surface form input output sampa map parallel pronounciations accord morphological analysis together stress position system implement python
present approach improve statistical machine translation image descriptions multimodal pivot define visual space key idea perform image retrieval database image caption target language use caption similar image crosslingual reranking translation output approach depend availability large amount domain parallel data rely available large datasets monolingually caption image state art convolutional neural network compute image similarities experimental evaluation show improvements one bleu point strong baselines
events various kinds mention discuss text document whether book news article blog microblog feed paper start give overview events treat linguistics philosophy follow discussion survey events associate information handle computationally particular look textual document mine extract events ancillary information days mostly application various machine learn techniques also discuss applications event detection extraction systems particularly summarization medical domain context twitter post end paper discussion challenge future directions
attention mechanism enhance state art neural machine translation nmt jointly learn align translate tend ignore past alignment information however often lead translation translation address problem propose coverage base nmt paper maintain coverage vector keep track attention history coverage vector feed attention model help adjust future attention let us nmt system consider untranslated source word experiment show propose approach significantly improve translation quality alignment quality standard attention base nmt
paper present new bayesian non parametric model extend usage hierarchical dirichlet allocation extract tree structure word cluster text data inference algorithm model collect word cluster share similar distribution document experiment observe meaningful hierarchical structure nip corpus radiology report collect public repositories
speech signal convey information different time scale short time scale segmental associate phonological phonetic information long time scale supra segmental associate syllabic prosodic information linguistic neurocognitive study recognize phonological class segmental level essential invariant representations use speech temporal organization context speech process deep neural network dnn effective computational method infer probability individual phonological class short segment speech signal vector phonological class probabilities refer phonological posterior class comprise short term speech signal hence phonological posterior sparse vector although phonological posteriors estimate segmental level claim convey supra segmental information specifically demonstrate phonological posteriors indicative syllabic prosodic events build find converge linguistic evidence gestural model articulatory phonology well neural basis speech perception hypothesize phonological posteriors convey properties linguistic class multiple time scale information embed support index active coefficients verify hypothesis obtain binary representation phonological posteriors segmental level refer first order sparsity structure high order structure obtain concatenation first order binary vectors confirm classification supra segmental linguistic events problem know linguistic parse achieve high accuracy use asimple binary pattern match first order high order structure
paper discuss syntagma rule base nlp system address tricky issue syntactic ambiguity reduction word sense disambiguation well provide innovative original solutions constituent generation constraints management provide insight operate system general architecture components well lexical syntactic semantic resources describe paper address mechanism perform selective parse interaction syntactic semantic information lead parser coherent accurate interpretation input text
one limitations semantic parse approach open domain question answer lexicosyntactic gap natural language question knowledge base entries many ways ask question answer paper propose bridge gap generate paraphrase input question goal least one correctly map knowledge base query introduce novel grammar model paraphrase generation require sentence align paraphrase corpus key idea leverage flexibility scalability latent variable probabilistic context free grammars sample paraphrase extrinsic evaluation paraphrase plug semantic parser freebase evaluation experiment webquestions benchmark dataset show performance semantic parser significantly improve strong baselines
quantify degree spatial dependence linguistic variables key task analyze dialectal variation however exist approach important drawbacks first base parametric model dependence limit power case underlie parametric assumptions violate second applicable type linguistic data approach apply frequencies others boolean indicators whether linguistic variable present present new method measure geographical language variation solve problems approach build reproduce kernel hilbert space rkhs representations nonparametric statistics take form test statistic compute pair individual geotagged observations without aggregation predefined geographical bin compare test prior work use synthetic data well diverse set real datasets corpus dutch tweet dutch syntactic atlas dataset letter editor north american newspapers propose test show support robust inferences across broad range scenarios type data
advancement web technology growth huge volume data present web internet users lot data generate internet become platform online learn exchange ideas share opinions social network sit like twitter facebook google rapidly gain popularity allow people share express view topicshave discussion different communities post message across world lot work field sentiment analysis twitter data survey focus mainly sentiment analysis twitter data helpful analyze information tweet opinions highly unstructured heterogeneous either positive negative neutral case paper provide survey comparative analyse exist techniques opinion mine like machine learn lexicon base approach together evaluation metrics use various machine learn algorithms like naive bay max entropy support vector machine provide research twitter data streamsgeneral challenge applications sentiment analysis twitter also discuss paper
last two years numerous paper look use deep neural network replace acoustic model traditional statistical parametric speech synthesis however far less attention pay approach like dnn base postfiltering dnns work conjunction traditional acoustic model paper investigate use recurrent neural network potential postfilter synthesis explore possibility replace exist postfilters well highlight ease arbitrary new feature add input postfilter also try novel approach jointly train classification regression tree postfilter rather traditional approach train independently
task text segmentation may undertake many level text analysis paragraph sentence word even letter focus relatively fine scale segmentation hypothesize accord stochastic model language generation smallest scale independent units mean produce goals letter include development methods segmentation minimal independent units produce feature representations texts align independence assumption bag term model commonly use prediction classification computational text analysis also propose measurement texts association respect realize segmentations model language generation find one segmentations phrase exhibit much better associations generation model word two texts well fit generally topically homogeneous generative model produce zipf law study suggest zipf law may consequence homogeneity language production
document classification task primarily tackle word level recent research work character level input show several benefit word level approach natural incorporation morphemes better handle rare word propose neural network architecture utilize convolution recurrent layer efficiently encode character input validate propose model eight large scale document classification task compare character level convolution model achieve comparable performances much less parameters
name suggest type logical grammars grammar formalism base logic type theory prespective grammar design type logical grammars develop syntactic semantic aspects linguistic phenomena hand hand let desire semantics expression inform syntactic type vice versa prototypical examples successful application type logical grammars syntax semantics interface include coordination quantifier scope extractionthis chapter describe grail theorem prover series tool design test grammars various modern type logical grammars function tool tool describe chapter freely available
train one multilingual model dependency parse use parse sentence several languages parse model use multilingual word cluster embeddings ii token level language information iii language specific feature fine grain pos tag input representation enable parser parse effectively multiple languages also generalize across languages base linguistic universals typological similarities make effective learn limit annotations parser performance compare favorably strong baselines range data scenarios include target language large treebank small treebank treebank train
introduce new methods estimate evaluate embeddings word fifty languages single share embed space estimation methods multicluster multicca use dictionaries monolingual data require parallel data new evaluation method multiqvec cca show correlate better previous ones two downstream task text categorization parse also describe web portal evaluation facilitate research area along open source release methods
article describe systems jointly submit institute infocomm i2r laboratoire informatique de l universit e du maine lium nanyang technology university ntu university eastern finland uef two thousand and fifteen nist language recognition evaluation lre submit system fusion nine sub systems base vectors extract different type feature give vectors several classifiers adopt language detection task include support vector machine svm multi class logistic regression mclr probabilistic linear discriminant analysis plda deep neural network dnn
present submatrix wise vector embed learner swivel method generate low dimensional feature embeddings feature co occurrence matrix swivel perform approximate factorization point wise mutual information matrix via stochastic gradient descent use piecewise loss special handle unobserved co occurrences thus make use information matrix require computation proportional size entire matrix make use vectorized multiplication process thousands row columns compute millions predict value furthermore partition matrix shards order parallelize computation across many nod approach result accurate embeddings achieve methods consider observe co occurrences scale much larger corpora handle sample methods
work explore recent advance recurrent neural network large scale language model task central language understand extend current model deal two key challenge present task corpora vocabulary size complex long term structure language perform exhaustive study techniques character convolutional neural network long short term memory one billion word benchmark best single model significantly improve state art perplexity five hundred and thirteen three hundred whilst reduce number parameters factor twenty ensemble model set new record improve perplexity four hundred and ten two hundred and thirty-seven also release model nlp ml community study improve upon
recent empirical model research focus semantic fluency task informative semantic memory interest interplay arise richness representations semantic memory complexity algorithms require process remain open question whether representations word relations learn language use enable simple search algorithm mimic observe behavior fluency task show plausible learn rich representations naturalistic data simple search algorithm random walk replicate human pattern suggest explicitly structure knowledge word semantic network play crucial role model human behavior memory search retrieval moreover case across range semantic information source
automatic sarcasm detection task predict sarcasm text crucial step sentiment analysis consider prevalence challenge sarcasm sentiment bear text begin approach use speech base feature sarcasm detection witness great interest sentiment analysis community paper first know compilation past work automatic sarcasm detection observe three milestones research far semi supervise pattern extraction identify implicit sentiment use hashtag base supervision use context beyond target text paper describe datasets approach trend issue sarcasm detection also discuss representative performance value share task pointers future work give prior work term resources could useful understand state art survey present several useful illustrations prominently table summarize past paper along different dimension feature annotation techniques data form etc
describe two new relate resources facilitate model general knowledge reason 4th grade science exams first collection curated facts form table second large set crowd source multiple choice question cover facts table setup crowd source annotation task obtain implicit alignment information question table envisage resources useful researchers work question answer also people investigate diverse range applications information extraction question parse answer type identification lexical semantic model
understand open domain text one primary challenge natural language process nlp machine comprehension benchmarks evaluate system ability understand text base text content work investigate machine comprehension mctest question answer qa benchmark prior work mainly base feature engineer approach come neural network framework name hierarchical attention base convolutional neural network habcnn address task without manually design feature specifically explore habcnn task two rout one traditional joint model passage question answer one textual entailment habcnn employ attention mechanism detect key phrase key sentence key snippets relevant answer question experiment show habcnn outperform prior deep learn approach big margin
wealth information financial systems embed document collections paper focus specialize text extraction task domain objective extract mention name financial institutions fi name financial prospectus document identify correspond real world entities eg match corpus entities task name entity recognition ner entity resolution er well study literature contribution develop rule base approach exploit list fi name task solution label dict base ner rank base er since fi name typically represent root suffix modify root use list fi name create specialize root suffix dictionaries evaluate effectiveness specialize solution extract fi name compare dict base ner general purpose rule base ner solution org ner evaluation highlight benefit limitations specialize versus general purpose approach present additional suggestions tune customization fi name extraction knowledge propose solutions dict base ner rank base er root suffix dictionaries first attempt exploit specialize knowledge ie list fi name rule base ner er
rapid crisis response require real time analysis message disaster happen volunteer attempt classify tweet determine need eg supply infrastructure damage etc give label data supervise machine learn help classify message scarcity label data cause poor performance machine train reuse old tweet train classifiers choose label tweet train specifically study usefulness label data past events label tweet different language help observe performance classifiers train use different combinations train set obtain past disasters perform extensive experimentation real crisis datasets show past label useful source target events type eg earthquakes similar languages eg italian spanish cross language domain adaptation useful however different languages eg italian english performance decrease
look structure natural language phrase word central notions consider problem identify meaningful subparts language length underlie composition principles completely corpus base language independent way without use kind prior linguistic knowledge unsupervised methods identify phrase mine subphrase structure find word fully automate way describe consider step towards automatically compute general dictionary grammar corpus hope long run variants approach turn useful kind sequence data well eg speech genom sequence music annotation even primarily interest immediate applications result obtain variety languages show methods interest many practical task text mine terminology extraction lexicography search engine technology relate field
people exhibit tendency generalize novel noun basic level hierarchical taxonomy cognitively salient category dog degree generalization depend number type exemplars recently change presentation time exemplars also show effect surprisingly reverse prior observe pattern basic level generalization explore precise mechanisms could lead behavior extend computational model word learn word generalization integrate cognitive process memory attention result show interaction forget attention novelty well sensitivity type token frequencies exemplars enable model replicate empirical result different presentation time result reinforce need incorporate general cognitive process within word learn model better understand range observe behaviors vocabulary acquisition
work model abstractive text summarization use attentional encoder decoder recurrent neural network show achieve state art performance two different corpora propose several novel model address critical problems summarization adequately model basic architecture model key word capture hierarchy sentence word structure emit word rare unseen train time work show many propose model contribute improvement performance also propose new dataset consist multi sentence summaries establish performance benchmarks research
propose train bi directional neural network language modelnnlm noise contrastive estimationnce experiment conduct rescore task ptb data set show nce train bi directional nnlm outperform one train conventional maximum likelihood train stillregretfully perform baseline uni directional nnlm
paper show one directly apply natural language process nlp methods classification problems cheminformatics connection seemingly separate field show consider standard textual representation compound smile problem activity prediction target protein consider crucial part computer aid drug design process conduct experiment show way one outrank state art result hand craft representations also get direct structural insights way decisions make
document exhibit sequential structure multiple level abstraction eg sentence paragraph section abstractions constitute natural hierarchy represent context infer mean word larger fragment text paper present clstm contextual lstm extension recurrent neural network lstm long short term memory model incorporate contextual feature eg topics model evaluate clstm three specific nlp task word prediction next sentence selection sentence topic prediction result experiment run two corpora english document wikipedia subset article recent snapshot english google news indicate use word topics feature improve performance clstm model baseline lstm model task example next sentence selection task get relative accuracy improvements twenty-one wikipedia dataset eighteen google news dataset clearly demonstrate significant benefit use context appropriately natural language nl task implications wide variety nl applications like question answer sentence completion paraphrase generation next utterance prediction dialog systems
work propose semi supervise method short text cluster represent texts distribute vectors neural network use small amount label data specify intention cluster design novel objective combine representation learn process k mean cluster process together optimize objective label data unlabeled data iteratively convergence three step one assign short text nearest centroid base representation current neural network two estimate cluster centroids base cluster assignments step one three update neural network accord objective keep centroids cluster assignments fix experimental result four datasets show method work significantly better several text cluster methods
conventional sentence similarity methods focus similar part two input sentence simply ignore dissimilar part usually give us clue semantic mean sentence work propose model take account similarities dissimilarities decompose compose lexical semantics sentence model represent word vector calculate semantic match vector word base word sentence word vector decompose similar component dissimilar component base semantic match vector two channel cnn model employ capture feature compose similar dissimilar components finally similarity score estimate compose feature vectors experimental result show model get state art performance answer sentence selection task achieve comparable result paraphrase identification task
petrarch two fourth generation series event data coders stem research phillip schrodt iteration bring new functionality usability exceptionpetrarch two take much power original petrarch dictionaries redirect faster smarter core logic earlier iterations handle sentence largely list word incorporate syntactic information petrarch two view sentence entirely syntactic level receive syntactic parse sentence stanford corenlp software store data tree structure link nod node phrase object prepositional noun verb phrase version phrase class deal logic particular kinds phrase since event coder core logic focus around verbs act act happen theory behind new structure logic found generative grammar information theory lambda calculus semantics
embeddings generic representations useful many nlp task paper introduce densifier method learn orthogonal transformation embed space focus information relevant task ultradense subspace dimensionality smaller factor one hundred original space show ultradense embeddings generate densifier reach state art lexicon creation task word annotate three type lexical information sentiment concreteness frequency semeval2015 10b sentiment analysis task show information lose ultradense subspace use train order magnitude efficient due compactness ultradense space
one key challenge natural language process nlp yield good performance across application domains languages work investigate robustness mention detection systems one fundamental task information extraction via recurrent neural network rnns advantage rnns traditional approach capacity capture long range context implicitly adapt word embeddings train large corpus task specific word representation still preserve original semantic generalization helpful across domains systematic evaluation rnn architectures demonstrate rnns outperform best report systems nine relative error reduction general set also achieve state art performance cross domain set english regard languages rnns significantly better traditional methods similar task name entity recognition dutch twenty-two relative error reduction
word completion word prediction two important phenomena type benefit users type use keyboard similar devices profound impact type disable people work base word prediction bangla sentence use stochastic ie n gram language model unigram bigram trigram delete interpolation backoff model auto complete sentence predict correct word sentence save time keystrokes type also reduce misspell use large data corpus bangla language different word type predict correct word accuracy much possible find promise result hope work impact baseline automate bangla type
software program generally use tlg thesaurus linguae graecae clclt cetedoc library christian latin texts cd roms well suit find quotations allusions quotationfinder use sophisticate criteria rank search result base closely match source text list search result literal quotations first loose verbal parallel last
propose method efficiently find parallel passages large corpus even passages quite identical due rephrase orthographic variation key ideas representation word corpus two infrequent letter find match pair string four five word differ one word identify cluster match pair use method four thousand, six hundred parallel pair passages identify babylonian talmud hebrew aramaic corpus eighteen million word thirty second empirical comparisons sample data indicate coverage obtain method essentially obtain use slow exhaustive methods
study successful language model develop evaluate mainly english close european languages french german etc important study applicability model languages use vector space model russian recently study multiple corpora wikipedia ruwac libru model evaluate word semantic similarity task knowledge twitter consider corpus task work fill gap result vectors train twitter corpus comparable accuracy single corpus train model although best performance currently achieve combination multiple corpora
paper describe quantitative criticism lab collaborative initiative classicists quantitative biologists computer scientists apply ideas methods draw sciences study literature core goal project use computational biology natural language process machine learn techniques investigate authorial style intertextuality relate phenomena literary significance case study approach review use sequence alignment common technique genomics computational linguistics detect intertextuality latin literature sequence alignment distinguish ability find inexact verbal similarities make ideal identify phonetic echo large corpora latin texts although especially suit latin sequence alignment principle extend many languages
much one hundred and thirty-one million us dollars help readers put number context propose new task automatically generate short descriptions know perspectives eg one hundred and thirty-one million cost employ everyone texas lunch period first collect dataset numeric mention news article mention label set rat perspectives propose system generate descriptions consist two step formula construction description generation construction compose formulae numeric facts knowledge base rank result formulas base familiarity numeric proximity semantic compatibility generation convert formula natural language use sequence sequence recurrent neural network system obtain one hundred and fifty-two f1 improvement non compositional baseline formula construction one hundred and twenty-five bleu point improvement baseline description generation
vector space methods measure semantic similarity relatedness often rely distributional information co occurrence frequencies statistical measure association weight importance particular co occurrences paper extend methods incorporate measure semantic similarity base human curated taxonomy second order vector representation result measure semantic relatedness combine contextual information available corpus base vector space representation semantic knowledge find biomedical ontology result show incorporate semantic similarity second order co occurrence matrices improve correlation human judgments similarity relatedness method compare favorably various different word embed methods recently evaluate reference standards use
recent work use artificial neural network base word distribute representation greatly boost performance various natural language learn task especially question answer though also carry along attendant problems corpus selection embed learn dictionary transformation different learn task etc paper propose straightforwardly model sentence mean character sequence utilize convolutional neural network integrate character embed learn together point wise answer selection train compare deep model pre train word embed strategy character sequential representation csr base method show much simpler procedure stable performance across different benchmarks extensive experiment two benchmark answer selection datasets exhibit competitive performance compare state art methods
describe efforts towards get better resources english arabic machine translation speak text particular look movie subtitle unique rich resource subtitle one language often get translate languages movie subtitle new resource explore previous research however create much larger bi text biggest date generate better quality alignment give subtitle movie different languages key problem align fragment level typically do use length base alignment movie subtitle also time information exploit information develop original algorithm outperform current best subtitle alignment tool subalign evaluation result show add bi text iwslt train bi text yield improvement two bleu point absolute
negative sample neg objective function use word2vec simplification noise contrastive estimation nce method neg find highly effective learn continuous word representations however unlike nce consider inapplicable purpose learn parameters language model study refute assertion provide principled derivation neg base language model found novel analysis low dimensional approximation matrix pointwise mutual information contexts predict word obtain language model closely relate nce language model base simplify objective function thus provide unify formulation two main language process task namely word embed language model base neg objective function experimental result two popular language model benchmarks show comparable perplexity result small advantage neg nce
attention base encoder decoder neural network model recently show promise result machine translation speech recognition work propose attention base neural network model joint intent detection slot fill critical step many speech understand dialog systems unlike machine translation speech recognition alignment explicit slot fill explore different strategies incorporate alignment information encoder decoder framework learn attention mechanism encoder decoder model propose introduce attention alignment base rnn model attentions provide additional information intent classification slot label prediction independent task model achieve state art intent detection error rate slot fill f1 score benchmark atis task joint train model obtain fifty-six absolute two hundred and thirty-eight relative error reduction intent detection twenty-three absolute gain slot fill independent task model
speaker intent detection semantic slot fill two critical task speak language understand slu dialogue systems paper describe recurrent neural network rnn model jointly perform intent detection slot fill language model neural network model keep update intent estimation word transcribe utterance arrive use contextual feature joint model evaluation language model online slu model make atis benchmarking data set language model task joint model achieve one hundred and eighteen relative reduction perplexity compare independent train language model slu task joint model outperform independent task train model two hundred and twenty-three intent detection error rate slight degradation slot fill f1 score joint model also show advantageous performance realistic asr settings noisy speech input
sentiment analysis review popular task natural language process work goal predict score food review scale one five two recurrent neural network carefully tune baseline train simple rnn classification extend baseline gru addition present two different methods deal highly skew data common problem review model evaluate use accuracies
robotic command natural language usually contain various spatial descriptions semantically similar syntactically different map syntactic variants semantic concepts understand robots challenge due high flexibility natural language expressions tackle problem collect robotic command navigation manipulation task use crowdsourcing define robot language use generative machine translation model translate robotic command natural language robot language main purpose paper simulate interaction process human robots use crowdsourcing platforms investigate possibility translate natural language robot language paraphrase
online harassment problem greater lesser extent since early days internet previous work apply anti spam techniques like machine learn base text classification reynolds two thousand and eleven detect harass message however exist public datasets limit size label vary quality hackharassment initiative alliance one tech company ngos devote fight bully internet begin address issue create new dataset superior predecssors term size quality hackharassment complete round label later iterations dataset increase available sample least order magnitude enable correspond improvements quality machine learn model harassment detection paper introduce first model build hackharassment dataset v10 new open dataset delight share interest researcherss benchmark future research
speak dialogue systems allow humans interact machine use natural speech many benefit use speech primary communication medium computer interface facilitate swift human like acquisition information recent years speech interfaces become ever popular evident rise personal assistants siri google cortana amazon alexa recently data drive machine learn methods apply dialogue model result achieve limit domain applications comparable outperform traditional approach methods base gaussian process particularly effective enable good model estimate limit train data furthermore provide explicit estimate uncertainty particularly useful reinforcement learn article explore additional step necessary extend methods model multiple dialogue domains show gaussian process reinforcement learn elegant framework naturally support range methods include prior knowledge bayesian committee machine multi agent learn facilitate extensible adaptable dialogue systems
arabic natural language process tool resources develop serve modern standard arabic msa official write language arab world dialectal arabic varieties notably egyptian arabic receive attention lately grow collection resources include annotate corpora morphological analyzers taggers gulf arabic however lag behind respect paper present gumar corpus large scale corpus gulf arabic consist one hundred and ten million word one thousand, two hundred forum novels annotate corpus sub dialect information document level also present result preliminary study morphological annotation gulf arabic include develop guidelines conventional orthography text corpus publicly browsable web interface develop
present computational analysis three language varieties native advance non native translation goal investigate similarities differences non native language productions translations contrast native language use collection computational methods establish three main result one three type texts easily distinguishable two non native language translations closer native language three characteristics depend source native language others reflect perhaps unify principles similarly affect translations non native language
translate texts distinctively different original ones extent supervise text classification methods distinguish high accuracy differences prove useful statistical machine translation however suggest accuracy translation detection deteriorate classifier evaluate outside domain train show indeed case variety evaluation scenarios show unsupervised classification highly accurate task suggest method determine correct label cluster outcomes use label vote improve accuracy even moreover suggest simple method cluster challenge case mix domain datasets spite dominance domain relate feature translation relate ones result effective fully unsupervised method distinguish original translate texts apply new domains reasonable accuracy
lack parallel data many language pair important challenge statistical machine translation smt one common solution pivot third language exist parallel corpora source target languages although pivot robust technique introduce low quality translations especially poor morphology language use pivot rich morphology languages paper examine use synchronous morphology constraint feature improve quality phrase pivot smt compare hand craft constraints learn limit parallel data source target languages learn morphology constraints base project align ments source target phrase pivot phrase table show positive result hebrew arabic smt pivot english get fifteen bleu point phrase pivot baseline eight bleu point system combination baseline direct model build parallel data
present dependency parser implement single deep neural network read orthographic representations word directly generate dependencies label unlike typical approach parse model require part speech pos tag sentence proper regularization additional supervision achieve multitask learn reach state art performance slavic languages universal dependencies treebank linguistic feature character parser accurate transition base system train perfect pos tag
attention mechanisim appeal neural machine translation since able dynam ically encode source sentence generate alignment target word source word unfortunately prove worse conventional alignment model aligment accuracy paper analyze explain issue point view order propose supervise attention learn guidance conventional alignment model experiment two chinese english translation task show super vised attention mechanism yield better alignments lead substantial gain standard attention base nmt
machine transliteration process automatically transform script word source language target language preserve pronunciation sequence sequence learn recently emerge new paradigm supervise learn paper character base encoder decoder model propose consist two recurrent neural network encoder bidirectional recurrent neural network encode sequence symbols fix length vector representation decoder generate target sequence use attention base recurrent neural network encoder decoder attention mechanism jointly train maximize conditional probability target sequence give source sequence experiment different datasets show propose encoder decoder model able achieve significantly higher transliteration quality traditional statistical model
introduce method transliteration generation produce transliterations every language previous result multilingual wikipedia show use train data wikipedia surrogate train language thus problem become one rank wikipedia languages order suitability respect target language introduce several task specific methods rank languages show approach comparable oracle ceiling even outperform case
present new approach neural machine translation nmt use morphological grammatical decomposition word factor output side neural network architecture address two main problems occur mt namely deal large target language vocabulary vocabulary oov word mean factor able handle larger vocabulary reduce train time systems equivalent target language vocabulary size addition produce new word vocabulary use morphological analyser get factor representation word lemmas part speech tag tense person gender number extend nmt approach attention mechanism order two different output one lemmas rest factor final translation build use textita priori linguistic information compare extension word base nmt system experiment perform iwslt fifteen dataset translate english french show performance always increase system manage much larger vocabulary consistently reduce oov rate observe two bleu point improvement simulate domain translation setup
work investigate style topic aspects language online communities look utility identifier community correlation community reception content style characterize use hybrid word part speech tag n gram language model topic represent use latent dirichlet allocation experiment several reddit forums show style better indicator community identity topic even communities organize around specific topics positive correlation community reception contribution style similarity community topic similarity
grow demand structure knowledge lead great interest relation extraction especially case limit supervision however exist distance supervision approach extract relations express single sentence general cross sentence relation extraction explore even supervise learn set paper propose first approach apply distant supervision cross sentence relation extraction core approach graph representation incorporate standard dependencies discourse relations thus provide unify way model relations within across sentence extract feature multiple paths graph increase accuracy robustness confront linguistic variation analysis error experiment important extraction task precision medicine show approach learn accurate cross sentence extractor use small exist knowledge base unlabeled text biomedical research article compare exist distant supervision paradigm approach extract twice many relations similar precision thus demonstrate prevalence cross sentence relations promise approach
automatic short answer grade asag techniques design automatically assess short answer question natural language length word sentence supervise asag techniques demonstrate effective suffer couple key practical limitations greatly reliant instructor provide model answer need label train data form grade student answer every assessment task overcome paper introduce asag technique two novel feature propose iterative technique ensemble text classifier student answer b classifier use numeric feature derive various similarity measure respect model answer second employ canonical correlation analysis base transfer learn common feature representation build classifier ensemble question label data propose technique handsomely beat win supervise entries scientsbank dataset student response analysis task semeval two thousand and thirteen additionally demonstrate generalizability benefit propose technique evaluation multiple asag datasets different subject topics standards
multilinear grammar provide framework integrate many different syntagmatic structure language coherent semiotically base rank interpretation architecture default linear grammars rank architecture define sui generis condition rank discourse utterance phrasal structure word sub rank morphology phonology rank unique structure semantic pragmatic prosodic phonetic interpretation model default computational model rank propose base procedural plausibility condition incremental process linear time finite work memory suggest rank interpretation architecture multilinear properties provide systematic design feature human languages contrast unordered list key properties single structural properties one rank recursion previously put forward language design feature framework provide realistic background gradual development complexity phylogeny ontogeny language clarify range challenge evaluation realistic linguistic theories applications empirical objective paper demonstrate unique multilinear properties rank thereby motivate multilinear grammar rank interpretation architecture framework coherent approach capture complexity human languages simplest possible way
paper describe arabic multi genre broadcast mgb two challenge slt two thousand and sixteen unlike last year english mgb challenge focus recognition diverse tv genres year challenge emphasis handle diversity dialect arabic speech audio data come nineteen distinct program aljazeera arabic tv channel march two thousand and five december two thousand and fifteen program split three group conversations interview report total one thousand, two hundred hours release lightly supervise transcriptions acoustic model language model make available 110m word crawl aljazeera arabic website aljazeeranet ten year duration two thousand two thousand and eleven two lexicons provide one phoneme base one grapheme base finally two task propose year challenge standard speech transcription word alignment paper describe task data evaluation process use mgb challenge summarise result obtain
work present new vector space model vsm speech utterances task speak dialect identification generally systems build use two set feature extract speech utterances acoustic phonetic acoustic phonetic feature use form vector representations speech utterances attempt encode information speak dialects phonotactic acoustic vsms thus form use task aim paper construct single vsm encode information speak dialects phonotactic acoustic vsms give two view data make use well know multi view dimensionality reduction technique know canonical correlation analysis cca form single vector representation speech utterance encode dialect specific discriminative information phonetic acoustic representations refer approach feature space combination approach show cca base feature vector representation perform better arabic task phonetic acoustic feature representations use alone also present feature space combination approach viable alternative model base combination approach two systems build use two vsms phonotactic acoustic final prediction score output score combination two systems
paper advance design ctc base neural end end speech recognizers propose novel symbol inventory novel iterate ctc method second system use transform noisy initial output cleaner version present number stabilization initialization methods find useful train network evaluate system commonly use nist two thousand conversational telephony test set significantly exceed previously publish performance similar systems without use external language model decode technology
reason inference central human artificial intelligence model inference human language challenge availability large annotate data bowman et al two thousand and fifteen recently become feasible train neural network base inference model show effective paper present new state art result achieve accuracy eight hundred and eighty-six stanford natural language inference dataset unlike previous top model use complicate network architectures first demonstrate carefully design sequential inference model base chain lstms outperform previous model base show explicitly consider recursive architectures local inference model inference composition achieve additional improvement particularly incorporate syntactic parse information contribute best result improve performance even add already strong model
paper address automatic quality assessment speak language translation slt relatively new task define formalize sequence label problem word slt hypothesis tag good bad accord large feature set propose several word confidence estimators wce base automatic evaluation transcription asr quality translation mt quality combine asrmt research work possible build specific corpus contain 67k utterances quintuplet contain asr output verbatim transcript text translation speech translation post edition translation build conclusion multiple experiment use joint asr mt feature wce mt feature remain influent asr feature bring interest complementary information robust quality estimators slt use score speech translation graph provide feedback user interactive speech translation computer assist speech text scenarios
deep neural network achieve remarkable result across many language process task however methods highly sensitive noise adversarial attack present regularization base method limit network sensitivity input inspire ideas computer vision thus learn model robust empirical evaluation range sentiment datasets convolutional neural network show compare baseline model dropout method method achieve superior performance noisy input domain data
paper present tint easy use set fast accurate extendable natural language process modules italian base stanford corenlp freely available standalone software library integrate exist project
automatically generate political event data important part social science data ecosystem approach generate data though remain largely two decades time field computational linguistics progress tremendously paper present overview political event data include methods ontologies set experiment determine applicability deep neural network extraction political events news text
neural machine translation nmt become new state art achieve promise translation result use simple encoder decoder neural network neural network train parallel corpus fix network use translate test sentence argue general fix network best fit specific test sentence paper propose dynamic nmt learn general network usual fine tune network test sentence fine tune work do small set bilingual train data obtain similarity search accord test sentence extensive experiment demonstrate method significantly improve translation performance especially highly similar sentence available
recent work unsupervised term discovery utd aim identify cluster repeat word like units audio alone systems promise low resource languages transcribe audio unavailable write form language exist however case may still feasible eg crowdsourcing obtain possibly noisy text translations audio information could use source side information improve utd present simple method rescoring output utd system use text translations test corpus spanish audio english translations show greatly improve average precision result wide range system configurations data preprocessing methods
new psychoactive substances nps drug lay grey area legislation since internationally officially ban possibly lead prosecutable trade exacerbation phenomenon nps easily sell buy online consider large corpora textual post publish online forums specialize drug discussions plus small set know substances associate effect call seed propose semi supervise approach knowledge extraction apply detection drug comprise nps effect corpora investigation base small set initial seed work highlight contrastive approach context deduction effective detect substances effect corpora promise result feature f1 score close nine pave way shorten detection time new psychoactive substances discuss advertise internet
speech applications text speech tts automatic speech recognition asr emphtext normalization refer task convert emphwritten representation representation text emphspoken real world speech applications text normalization engine develop large part hand example hand build grammar may use enumerate possible ways say give token give language statistical model use select appropriate pronunciation context study examine tradeoffs associate use less language specific domain knowledge text normalization engine data rich scenario access carefully construct hand build normalization grammar give token produce set possible verbalizations token also assume corpus align write speak utterances train rank model select appropriate verbalization give context substitute carefully construct grammar also consider scenario language universal normalization emphcovering grammar developer merely need provide set lexical items particular language substitute align corpus also consider scenario one speak side correspond write side hallucinate compose speak side invert normalization grammar investigate accuracy text normalization engine scenarios report result experiment english russian
recently increase interest end end speech recognition directly transcribe speech text without predefined alignments one approach attention base encoder decoder framework learn map variable length input output sequence one step use purely data drive method attention model often show improve performance another end end approach connectionist temporal classification ctc mainly explicitly use history target character without conditional independence assumptions however observe performance attention show poor result noisy condition hard learn initial train stage long input sequence attention model flexible predict proper alignments case due lack leave right constraints use ctc paper present novel method end end speech recognition improve robustness achieve fast convergence use joint ctc attention model within multi task learn framework thereby mitigate alignment issue experiment wsj chime four task demonstrate advantage ctc attention base encoder decoder baselines show fifty-four one hundred and forty-six relative improvements character error rate cer
summaries meet important convey essential content discussions concise form generally time consume read understand whole document therefore summaries play important role readers interest important context discussions work address task meet document summarization automatic summarization systems meet conversations develop far primarily extractive result unacceptable summaries hard read extract utterances contain disfluencies affect quality extractive summaries make summaries much readable propose approach generate abstractive summaries fuse important content several utterances first separate meet transcripts various topic segment identify important utterances segment use supervise learn approach important utterances combine together generate one sentence summary text generation step dependency parse utterances segment combine together create direct graph informative well form sub graph obtain integer linear program ilp select generate one sentence summary topic segment ilp formulation reduce disfluencies leverage grammatical relations prominent non conversational style text therefore generate summaries comparable human write abstractive summaries experimental result show method generate informative summaries baselines addition readability assessments human judge well log likelihood estimate obtain dependency parser show generate summaries significantly readable well form
abstractive summarization ideal form summarization since synthesize information multiple document create concise informative summaries work aim develop abstractive summarizer first propose approach identify important document multi document set sentence important document align sentence document generate cluster similar sentence second generate k shortest paths sentence cluster use word graph structure finally select sentence set shortest paths generate cluster employ novel integer linear program ilp model objective maximize information content readability final summary ilp model represent shortest paths binary variables consider length path information score linguistic quality score objective function experimental result duc two thousand and four two thousand and five multi document summarization datasets show propose approach outperform baselines state art extractive summarizers measure rouge score method also outperform recent abstractive summarization technique manual evaluation approach also achieve promise result informativeness readability
automatic summarization techniques meet conversations develop far primarily extractive result poor summaries improve propose approach generate abstractive summaries fuse important content several utterances meet generally comprise several discussion topic segment topic segment within meet conversation aim generate one sentence summary important utterances use integer linear program base sentence fusion approach experimental result show method generate informative summaries baselines
propose novel semantic tag task sem tag tailor purpose multilingual semantic parse present first tagger use deep residual network resnets tagger use word character representations include novel residual bypass architecture evaluate tagset intrinsically new task semantic tag well part speech pos tag system consist resnet auxiliary loss function predict semantic tag significantly outperform prior result english universal dependencies pos tag nine thousand, five hundred and seventy-one accuracy ud v12 nine thousand, five hundred and sixty-seven accuracy ud v13
textual information consider significant supplement knowledge representation learn krl two main challenge construct knowledge representations plain texts one take full advantage sequential contexts entities plain texts krl two dynamically select informative sentence correspond entities krl paper propose sequential text embody knowledge representation learn build knowledge representations multiple sentence give reference sentence entity first utilize recurrent neural network pool long short term memory network encode semantic information sentence respect entity design attention model measure informativeness sentence build text base representations entities evaluate method two task include triple classification link prediction experimental result demonstrate method outperform baselines task indicate method capable select informative sentence encode textual information well knowledge representations
propose new evaluation automatic solvers algebra word problems identify mistake exist evaluations overlook proposal evaluate solvers use derivations reflect equation system construct word problem accomplish develop algorithm check equivalence two derivations show derivation notations semi automatically add exist datasets make experiment comprehensive include derivation annotation draw 1k new dataset contain one thousand general algebra word problems experiment find annotate derivations enable accurate evaluation automatic solvers previously use metrics release derivation annotations two thousand, three hundred algebra word problems future evaluations
neural network base model achieve impressive result various specific task however previous work model learn separately base single task supervise objectives often suffer insufficient train data paper propose two deep architectures train jointly multiple relate task specifically augment neural model external memory share several task experiment two group text classification task show propose architectures improve performance task help relate task
task amr text generation generate grammatical text sustain semantic mean give amr graph tack task first partition amr graph smaller fragment generate translation fragment finally decide order solve asymmetric generalize travel salesman problem agtsp maximum entropy classifier train estimate travel cost tsp solver use find optimize solution final model report bleu score two thousand, two hundred and forty-four semeval two thousand and sixteen task8 dataset
distantly supervise relation extraction widely use find novel relational facts plain text predict relation pair two target entities exist methods solely rely direct sentence contain entities fact also many sentence contain one target entities provide rich useful information relation extraction address issue build inference chain two target entities via intermediate entities propose path base neural relation extraction model encode relational semantics direct sentence inference chain experimental result real world datasets show model make full use sentence contain one target entity achieve significant consistent improvements relation extraction compare baselines source code paper obtain https githubcom thunlp pathnre
introduce two first order graph base dependency parsers achieve new state art first consensus parser build ensemble independently train greedy lstm transition base parsers different random initializations cast approach minimum bay risk decode ham cost argue weaker consensus within ensemble useful signal difficulty ambiguity second parser distillation ensemble single model train distillation parser use structure hinge loss objective novel cost incorporate ensemble uncertainty estimate possible attachment thereby avoid intractable cross entropy computations require apply standard distillation objectives problems structure output first order distillation parser match surpass state art english chinese german
discriminate closely relate language varieties consider challenge important task paper describe submission dsl two thousand and sixteen share task include two sub task one discriminate similar languages one identify arabic dialects develop character level neural network task give sequence character model embed character vector space run sequence multiple convolutions different filter widths pool convolutional representations obtain hide vector representation text use predict language dialect primarily focus arabic dialect identification task obtain f1 score four thousand, eight hundred and thirty-four rank 6th eighteen participants also analyze errors make system arabic data detail point challenge approach face
drug name recognition dnr essential step pharmacovigilance pv pipeline dnr aim find drug name mention unstructured biomedical texts classify predefined categories state art dnr approach heavily rely hand craft feature domain specific resources difficult collect tune reason paper investigate effectiveness contemporary recurrent neural architectures elman jordan network bidirectional lstm crf decode perform dnr straight text experimental result achieve authoritative semeval two thousand and thirteen task ninety-one benchmarks show bidirectional lstm crf rank closely highly dedicate hand craft systems
sentence basic linguistic unit however little know information content distribute across different position sentence base authentic language data english present study calculate entropy entropy relate statistics different sentence position statistics indicate three step staircase shape distribution pattern entropy initial position lower medial position position initial final medial position lower final position medial position show significant difference result suggest one hypotheses constant entropy rate uniform information density hold sentence medial position two context word sentence simply define word precede sentence three contextual information content sentence accumulate incrementally follow pattern whole greater sum part
machine translation arabic hebrew far limit lack parallel corpora despite political cultural importance language pair previous work rely manually craft grammars pivot via english unsatisfactory build scalable accurate mt system work compare standard phrase base neural systems arabic hebrew translation experiment tokenization external tool sub word model character level neural model show methods lead improve translation performance small advantage neural model
neural machine translation nmt heavily rely word level model learn semantic representations input sentence however languages without natural word delimiters eg chinese input sentence tokenized first conventional nmt confront two issue one difficult find optimal tokenization granularity source sentence model two errors one best tokenizations may propagate encoder nmt handle issue propose word lattice base recurrent neural network rnn encoders nmt generalize standard rnn word lattice topology propose encoders take input word lattice compactly encode multiple tokenizations learn generate new hide state arbitrarily many input hide state precede time step word lattice base encoders alleviate negative impact tokenization errors also expressive flexible embed input sentence experiment result chinese english translation demonstrate superiorities propose encoders conventional encoder
present factorize compositional distributional semantics model representation transitive verb constructions model first produce subject verb verb object vector representations base similarity nouns construction nouns vocabulary tendency nouns take subject object roles verb vectors combine final subjectverbobject representation simple vector operations two establish task transitive verb construction model outperform recent previous work
non linear model recently receive lot attention people start discover power statistical embed feature however tree base model seldom study context structure learn despite recent success various classification rank task paper propose mart tree base structure learn framework base multiple additive regression tree mart especially suitable handle task dense feature use learn many different structure various loss function apply mart task tweet entity link core component tweet information extraction aim identify link name mention entities knowledge base novel inference algorithm propose handle special structure task experimental result show mart significantly outperform state art tweet entity link systems
entity link task identify mention entities text link entries knowledge base task especially difficult microblogs little additional text provide disambiguate context rather author rely implicit common grind share knowledge readers paper attempt capture implicit context exploit social network structure microblogs build theory homophily imply socially link individuals share interest therefore likely mention sort entities implement idea encode author mention entities continuous vector space construct socially connect author similar vector representations vectors incorporate neural structure prediction model capture structural constraints inherent entity link task together design decisions yield f1 improvements one five benchmark datasets compare previous state art
common model question answer qa good answer one closely relate question relatedness often determine use general purpose lexical model word embeddings argue better approach look answer relate question relevant way accord information need question may determine task specific embeddings causality use case implement insight three step first generate causal embeddings cost effectively bootstrapping effect pair extract free text use small set seed pattern second train dedicate embeddings data use task specific contexts ie context effect finally extend state art reranking approach qa incorporate causal embeddings evaluate causal embed model directly casual implication task indirectly downstream causal qa task use data yahoo answer show explicitly model causality improve performance task qa task best model achieve three hundred and seventy-three p1 significantly outperform strong baseline seventy-seven relative
many low resource languages speak language resources likely annotate translations transcriptions translate speech data potentially valuable document endanger languages train speech translation systems first step towards make use data would automatically align speak word translations present model combine dyer et al reparameterization ibm model two fast align k mean cluster use dynamic time warp distance metric two components train jointly use expectation maximization extremely low resource scenario model perform significantly better neural model strong baseline
align coordinate text stream multiple source multiple languages open many new research venues cross lingual knowledge discovery paper aim advance state art one extend coarse grain topic level knowledge mine fine grain information units entities events two follow novel data network knowledge d2n2k paradigm construct utilize network structure capture propagate reliable evidence introduce novel burst information network binet representation display important information illustrate connections among bursty entities events keywords corpus propose effective approach construct decipher binets incorporate novel criteria base multi dimensional clue pronunciation translation burst neighbor graph topological structure experimental result chinese english coordinate text stream show approach accurately decipher nod high confidence binets algorithm efficiently run parallel make possible apply huge amount stream data never end language information decipherment
paper investigate effect data size frequency range distributional semantic model compare performance number representative model several test settings data vary size test items various frequency result show neural network base model underperform data small reliable model data vary size frequency range invert factorize model
many current natural language process applications social media rely representation learn utilize pre train word embeddings currently exist several publicly available pre train set word embeddings contain emoji representations even emoji usage social media increase paper release emoji2vec pre train embeddings unicode emoji learn description unicode emoji standard result emoji embeddings readily use downstream social natural language process applications alongside word2vec demonstrate downstream task sentiment analysis emoji embeddings learn short descriptions outperform skip gram model train large collection tweet avoid need contexts emoji need appear frequently order estimate representation
present oc16 ce80 chinese english mixlingual speech database release main resource train development test chinese english mixlingual speech recognition mixasr chen challenge cocosda two thousand and sixteen database consist eighty hours speech signal record one thousand, four hundred speakers utterances chinese involve one several english word base database another two free data resources thchs30 cmu dictionary speech recognition asr baseline construct deep neural network hide markov model dnn hmm hybrid system report baseline result follow mixasr chen evaluation rule demonstrate oc16 ce80 reasonable data resource mixlingual research
coreference resolution systems typically train heuristic loss function require careful tune paper instead apply reinforcement learn directly optimize neural mention rank model coreference evaluation metrics experiment two approach reinforce policy gradient algorithm reward rescale max margin objective find latter effective result significant improvements current state art english chinese portion conll two thousand and twelve share task
present neural network architecture predict point color space sequence character color name use large scale color name pair obtain online color design forum evaluate model color turing test find give name color predict model prefer annotators color name create humans datasets demo system available online colorlabus
address problem integrate textual visual information vector space model word mean representation first present residual cca r cca method complement standard cca method represent modality difference original signal signal project share max correlation space show construct visual textual representations post process composition common model motifs pca cca r cca linear interpolation aka sequential model yield high quality model five standard semantic benchmarks sequential model outperform recent multimodal representation learn alternatives include ones rely joint representation learn two benchmarks r cca method part best configuration algorithm yield
identify mathematical relations express text essential understand broad range natural language text election report financial news sport commentaries mathematical word problems paper focus identify understand mathematical relations describe within single sentence introduce problem equation parse give sentence identify noun phrase represent variables generate mathematical equation express relation describe sentence introduce notion projective equation parse provide efficient algorithm parse text projective equations system make use high precision lexicon mathematical expressions pipeline structure predictors generate correct equations seventy case sixty time also identify correct noun phrase rightarrow variables map significantly outperform baselines also release new annotate dataset task evaluation
report system share task discriminate similar languages dsl two thousand and sixteen system use byte representations deep residual network resnet system name resident train data release task close train obtain eight thousand, four hundred and eighty-eight accuracy subtask six thousand, eight hundred and eighty accuracy subtask b1 six thousand, nine hundred and eighty accuracy subtask b2 large difference accuracy development data observe relatively minor change network architecture hyperparameters therefore expect fine tune parameters yield higher accuracies
natural language process techniques increasingly apply identify social trend predict behavior base large text collections exist methods typically rely surface lexical syntactic information yet research psychology show pattern human conceptualisation metaphorical frame reliable predictors human expectations decisions paper present method learn pattern metaphorical frame large text collections use statistical techniques apply method data three different languages evaluate identify pattern demonstrate psychological validity
recurrent neural network achieve state art result many problems nlp two popular rnn architectures tail model pool model paper hybrid architecture propose present first empirical study use lstms compare performance three rnn structure sentence classification task experimental result show max pool model hybrid max pool model achieve best performance datasets tail model outperform model
recently much progress make learn general purpose sentence representations use across domains however exist model typically treat word sentence equally contrast extensive study prove human read sentence efficiently make sequence fixation saccades motivate us improve sentence representations assign different weight vectors component word treat attention mechanism single sentence end propose two novel attention model attention weight derive use significant predictors human read time ie surprisal pos tag ccg supertags extensive experiment demonstrate propose methods significantly improve upon state art sentence representation model
work focus rapid development linguistic annotation tool resource poor languages experiment several cross lingual annotation projection methods use recurrent neural network rnn model distinctive feature approach multilingual word representation require parallel corpus source target language precisely method follow characteristics use word alignment information b assume knowledge foreign languages make applicable wide range resource poor languages c provide truly multilingual taggers investigate uni bi directional rnn model propose method include external information instance low level information pos rnn train higher level taggers instance super sense taggers demonstrate validity genericity model use parallel corpora obtain manual automatic translation experiment conduct induce cross lingual pos super sense taggers
neural encoder decoder model show great success many sequence generation task however previous work investigate situations would like control length encoder decoder output capability crucial applications text summarization generate concise summaries desire length paper propose methods control output sequence length neural encoder decoder model two decode base methods two learn base methods result show learn base methods capability control length without degrade summary quality summarization task
paper discuss lexicon word learn high dimensional mean space viewpoint referential uncertainty investigate various state art machine learn algorithms discuss impact scale representation mean space structure demonstrate current machine learn techniques successfully deal high dimensional mean space particular show exponentially increase dimension linearly impact learner performance referential uncertainty word sensitivity impact
paper present number experiment model change historical portuguese corpus compose literary texts purpose temporal text classification algorithms train classify texts respect publication date take account lexical variation represent word n grams morphosyntactic variation represent part speech pos distribution report result nine hundred and ninety-eight accuracy use word unigram feature support vector machine classifier predict publication date document time intervals one century half century feature analysis perform investigate informative feature task link language change
present analysis performance machine learn classifiers discriminate similar languages language varieties carry number experiment use result two editions discriminate similar languages dsl share task investigate progress make two task estimate upper bind possible performance use ensemble oracle combination provide learn curve help us understand languages challenge number difficult sentence identify investigate human annotation
classical translation model constrain space possible output select subset translation rule base input sentence recent work improve efficiency neural translation model adopt similar strategy restrict output vocabulary subset likely candidates give source paper experiment context embed base selection methods extend previous work examine speed accuracy trade off detail show decode time cpus reduce ninety train time twenty-five wmt15 english german wmt16 english romanian task negligible change accuracy bring time decode state art neural translation system one hundred and forty msec per sentence single cpu core english german
automate discourse analysis tool base natural language process nlp aim diagnosis language impair dementias generally extract several textual metrics narrative transcripts however absence sentence boundary segmentation transcripts prevent direct application nlp methods rely mark function properly taggers parsers present first step take towards automatic neuropsychological evaluation base narrative discourse analysis present new automatic sentence segmentation method impair speech model use recurrent convolutional neural network prosodic part speech pos feature word embeddings evaluate intrinsically impair spontaneous speech well normal prepare speech present better result healthy elderly ctl f1 seventy-four mild cognitive impairment mci patients f1 seventy conditional random field method f1 fifty-five fifty-three respectively use context study result suggest model robust impair speech use automate discourse analysis tool differentiate narratives produce mci ctl
paper describe extension optimization previous work deep convolutional neural network cnns effective recognition noisy speech aurora four task appropriate number convolutional layer size filter pool operations input feature map modify filter pool size reduce dimension input feature map extend allow add convolutional layer furthermore appropriate input pad input feature map selection strategies develop addition adaptation framework use joint train deep cnn auxiliary feature vector fmllr feature develop modifications give substantial word error rate reductions standard cnn use baseline finally deep cnn combine lstm rnn acoustic model show state level weight log likelihood score combination joint acoustic model decode scheme effective aurora four task deep cnn achieve wer eight hundred and eighty-one seven hundred and ninety-nine auxiliary feature joint train seven hundred and nine lstm rnn joint decode
assign binary ternary error correct cod data syntactic structure world languages study distribution code point space code parameters show cod populate lower region approximate superposition thomae function substantial presence cod gilbert varshamov bind even asymptotic bind plotkin bind investigate dynamics induce space code parameters spin glass model language change show presence entailment relations syntactic parameters dynamics sometimes improve code large set languages syntactic data one gain information spin glass dynamics induce dynamics space code parameters
introduce first generic text representation model completely nonsymbolic ie require availability segmentation tokenization method attempt identify word symbolic units text apply train parameters model train corpus well apply compute representation new text show model perform better prior work information extraction text denoising task
paper describe system generate three dimensional visual simulations natural language motion expressions use rich formal model events participants generate simulations satisfy minimal constraints entail associate utterance rely semantic knowledge physical object motion events paper outline technical considerations discuss implement aforementioned semantic model system
explore use orthographic syllable variable length consonant vowel sequence basic unit translation relate languages use abugida alphabetic script show orthographic syllable level translation significantly outperform model train basic units word morpheme character train small parallel corpora
lexical set contain word fill argument slot verb part determine selectional preferences purpose paper unravel properties lexical set distributional semantics investigate one whether lexical set behave prototypical categories centre periphery two whether polymorphic ie compose subcategories three whether distance lexical set different arguments explanatory verb properties particular case study lexical set causative inchoative verbs italian study several vector model find one base spatial distance centroid object fillers scatter uniformly across category whereas intransitive subject fillers lie edge two correlation exist amount verb sense cluster discover automatically especially intransitive subject three distance centroids object intransitive subject correlate properties verbs cross lingual tendency appear intransitive pattern rather transitive one paper noncommittal respect hypothesis connection underpin semantic reason namely spontaneity event denote verb
lot prior work event extraction exploit variety feature represent events methods several drawbacks one feature often specific particular domain generalize well two feature derive various linguistic analyse error prone three feature may expensive require domain expert paper develop chinese event extraction system use word embed vectors represent language deep neural network learn abstract feature representation order greatly reduce effort feature engineer addition framework leverage large amount unlabeled data address problem limit label corpus task experiment show propose method perform better compare system use rich language feature use unlabeled data benefit word embeddings study suggest potential dnn word embed event extraction task
alcohol abuse may lead unsociable behavior crime drink drive privacy leak introduce automatic drink texting prediction task identify whether text write influence alcohol experiment tweet label use hashtags distant supervision classifiers use set n gram stylistic feature detect drink tweet observations present first quantitative evidence text contain signal exploit detect drink texting
paper make simple increment state art sarcasm detection research exist approach unable capture subtle form context incongruity lie heart sarcasm explore prior work enhance use semantic similarity discordance word embeddings augment word embed base feature four feature set report past also experiment four type word embeddings observe improvement sarcasm detection irrespective word embed use original feature set feature augment example augmentation result improvement f score around four three four feature set minor degradation case fourth word2vec embeddings use finally comparison four embeddings show word2vec dependency weight base feature outperform lsa glove term benefit sarcasm detection
paper provide largest publish comparison translation quality phrase base smt neural machine translation across thirty translation directions ten directions also include hierarchical phrase base mt experiment perform recently publish unite nations parallel corpus v10 large six way sentence align subcorpus second part paper investigate aspects translation speed introduce amunmt efficient neural machine translation decoder demonstrate current neural machine translation could already use production systems compare word per second ratios
paper present approach combine lexico semantic resources distribute representations word apply evaluation machine translation mt study make enrichment well know mt evaluation metric meteor metric enable approximate match synonymy morphological similarity automatic reference translation experiment make framework metrics task wmt two thousand and fourteen show distribute representations good alternative lexico semantic resources mt evaluation even bring interest additional information augment versions meteor use vector representations make available github page
work propose tentative model calculation dimensionless distance phonemes sound describe binary distinctive feature distance show linear consistency term feature model use score function local global pairwise alignment phoneme sequence distance use prior probabilities bayesian analyse phylogenetic relationship languages particularly cognate identification case empirical prior probability available
present specification model language voxml encode semantic knowledge real world object represent three dimensional model events attribute relate enact object voxml intend overcome limitations exist 3d visual markup languages allow encode broad range semantic knowledge exploit variety systems platforms lead multimodal simulations real world scenarios use conceptual object represent semantic value
domain adaptation adapt model domains rich label train data domains poor data fundamental nlp challenge introduce neural network model marry together ideas two prominent strand research domain adaptation representation learn structural correspondence learn scl blitzer et al two thousand and six autoencoder neural network particularly model three layer neural network learn encode nonpivot feature input example low dimensional representation existence pivot feature feature prominent domains convey useful information nlp task example decode representation low dimensional representation employ learn algorithm task moreover show inject pre train word embeddings model order improve generalization across examples similar pivot feature task cross domain product sentiment classification blitzer et al two thousand and seven consist twelve domain pair model outperform scl marginalize stack denoising autoencoder msda chen et al two thousand and twelve methods three hundred and seventy-seven two hundred and seventeen respectively average across domain pair
paper describe computational model motion events natural language map linguistic expressions dynamic event interpretation three dimensional temporal simulations model start model pustejovsky moszkowicz two thousand and eleven analyze motion events use temporally trace label transition systems model distinction path manner motion operational semantics distinguish different type manner motion verbs term mereo topological relations hold throughout process movement representations generate minimal model realize three dimensional simulations software develop game engine unity generate simulations act conceptual debugger semantics different motion verbs test consistency informativeness model simulations expose presuppositions associate linguistic expressions compositions model generation component still incomplete paper focus implementation map directly linguistic interpretations unity code snippets create simulations
word embeddings demonstrate benefit nlp task impressively yet room improvement vector representations current word embeddings typically contain unnecessary information ie noise propose two novel model improve word embeddings unsupervised learn order yield word denoising embeddings word denoising embeddings obtain strengthen salient information weaken noise original word embeddings base deep fee forward neural network filter result benchmark task show filter word denoising embeddings outperform original word embeddings
understand expressions emotions support forums considerable value nlp methods key automate many approach understandably use subjective categories fine grain straightforward polarity base spectrum however definition categories non trivial fact argue need incorporate communicative elements even beyond subjectivity support position report experiment sentiment label corpus post take medical support forum argue fine grain approach text analysis important simultaneously recognise social function behind affective expressions enable accurate valuable level understand
machine translation discipline concern develop automate tool translate one human language another statistical machine translation smt dominant paradigm field smt translations generate mean statistical model whose parameters learn bilingual data scalability key concern smt one would like make use much data possible train better translation systems recent years mobile devices adequate compute power become widely available despite successful mobile applications rely nlp systems continue follow client server architecture limit use access internet often limit expensive goal dissertation show construct scalable machine translation system operate limit resources available mobile device main challenge port translation systems mobile devices memory usage amount memory available mobile device far less typically available server side client server application thesis investigate alternatives two components prevent standard translation systems work mobile devices due high memory usage show standard components replace propose alternatives obtain scalable translation system work device limit memory
current methods automatically evaluate grammatical error correction gec systems rely gold standard reference however methods suffer penalize grammatical edit correct gold standard show reference less grammaticality metrics correlate strongly human judgments competitive lead reference base evaluation metrics interpolate methods achieve state art correlation human judgments finally show gec metrics much reliable calculate sentence level instead corpus level set codalab site benchmarking gec output use common dataset different evaluation metrics
paper address challenge natural language process nlp non canonical multilingual data two languages mix refer code switch become popular daily life therefore obtain increase amount attention research community report experience cov ers core nlp task normalisation language identification language model part speech tag dependency parse also downstream ones machine translation automatic speech recognition highlight discuss key problems task support examples different language pair relevant previous work
work part large research project entitle eodule aim develop tool automatic speech recognition translation synthesis arabic language attention mainly focus attempt improve probabilistic model semantic decoder base achieve goal decide test influence pertinent context use contextual data integration different type effectiveness semantic decoder find quite satisfactory
link theory explain verbs semantic arguments map syntactic arguments inverse semantic role label task shallow semantic parse literature paper develop computational link theory framework method implement test link theories propose theoretical literature deploy framework assess two cross cut type link theory local v global model categorical v featural model investigate behavior model develop measurement model spirit previous work semantic role induction semantic proto role link model use model implement generalization dowty seminal proto role theory induce semantic proto roles compare dowty propose
present research towards bridge language gap migrant workers qatar medical staff particular present first step towards development real world hindi english machine translation system doctor patient communication low resource language pair especially speech medical domain initial focus gather suitable train data various source apply variety methods range fully automatic extraction web manual annotation test data moreover develop method automatically augment train data synthetically generate variants yield sizable improvement three bleu point absolute
combinatory category grammar ccg supertagging task assign lexical categories word sentence almost previous methods use fix context window size input feature however obvious different tag usually rely different context window size motivate us build supertagger dynamic window approach treat attention mechanism local contexts apply dropout dynamic filter see drop word directly superior regular dropout word embeddings use approach demonstrate state art ccg supertagging performance standard test set
describe attentive encoder combine tree structure recursive neural network sequential recurrent neural network model sentence pair since exist attentive model exert attention sequential structure propose way incorporate attention tree topology specially give pair sentence attentive encoder use representation one sentence generate via rnn guide structural encode sentence dependency parse tree evaluate propose attentive encoder three task semantic similarity paraphrase identification true false question selection experimental result show encoder outperform baselines achieve state art result two task
sequence sequence model show success end end speech recognition however model use shallow acoustic encoder network work successively train deep convolutional network add expressive power better generalization end end asr model apply network network principles batch normalization residual connections convolutional lstms build deep recurrent convolutional structure model exploit spectral structure feature space add computational depth without overfitting issue experiment wsj asr task achieve one hundred and five word error rate without dictionary language use fifteen layer deep network
paper propose novel neural approach paraphrase generation conventional para phrase generation methods either leverage hand write rule thesauri base alignments use statistical machine learn principles best knowledge work first explore deep learn model paraphrase generation primary contribution stack residual lstm network add residual connections lstm layer allow efficient train deep lstms evaluate model state art deep learn model three different datasets ppdb wikianswers mscoco evaluation result demonstrate model outperform sequence sequence attention base bi directional lstm model bleu meteor ter embed base sentence similarity metric
social norms share rule govern facilitate social interaction violate social norms via tease insult may serve upend power imbalances contrary reinforce solidarity rapport conversation rapport highly situate context dependent work investigate task automatically identify phenomena social norm violation discourse towards goal leverage power recurrent neural network multimodal information present interaction propose predictive model recognize social norm violation use long term temporal contextual information model achieve f1 score seven hundred and five implications work regard develop social aware agent discuss
paper empirically explore effect various kinds skip connections stack bidirectional lstms sequential tag investigate three kinds skip connections connect lstm cells skip connections gate b skip connections internal state c skip connections cell output present comprehensive experiment show skip connections cell output outperform remain two furthermore observe use gate identity function skip mappings work pretty well base novel skip connections successfully train deep stack bidirectional lstm model obtain state art result ccg supertagging comparable result pos tag
develop method start new instance nell various languages develop nell multilingualism base method experience nell portuguese nell french report explain method develop research perspectives
recently attempt make remove gaussian mixture model gmm train process deep neural network base hide markov model hmm dnn gmm free train hmm dnn hybrid solve two problems namely initial alignment frame level state label creation context dependent state although flat start train via iteratively realign retrain dnn use frame level error function viable quite cumbersome propose use sequence discriminative train criterion flat start sequence discriminative train routinely apply final phase model train show proper caution also suitable get alignment context independent dnn model construction tie state apply recently propose kl divergence base state cluster method hence whole train process gmm free experimental evaluation find sequence discriminative flat start train method significantly faster straightforward approach iterative retrain realignment word error rat attain slightly better well
keystroke dynamics extensively use psycholinguistic write research gain insights cognitive process keystroke log contain actual signal use learn better natural language process model postulate keystroke dynamics contain information syntactic structure inform shallow syntactic parse test hypothesis explore label derive keystroke log auxiliary task multi task bidirectional long short term memory bi lstm result show promise result two shallow syntactic parse task chunk ccg supertagging model simple advantage data come distinct source produce model significantly better model train text annotations alone
recent years linguistic typology classify world languages accord functional structural properties widely use support multilingual nlp grow importance typological information support multilingual task recognise systematic survey exist typological resources use nlp publish paper provide survey well discussion hope inform inspire future work area
distinguish property human intelligence ability flexibly use language order communicate complex ideas humans variety contexts research natural language dialogue focus design communicative agents integrate contexts productively collaborate humans abstract propose general situate language learn paradigm design bring robust language agents able cooperate productively humans
work implement train language model lm use recurrent neural network rnn glove word embeddings introduce pennigton et al one implementation follow general idea train rnns lm task present two rather use gate recurrent unit gru three memory cell commonly use lstm four
paper introduce task target aspect base sentiment analysis goal extract fine grain information respect entities mention user comment work extend aspect base sentiment analysis assume single entity per document target sentiment analysis assume single sentiment towards target entity particular identify sentiment towards aspect one entities testbed task introduce sentihood dataset extract question answer qa platform urban neighbourhoods discuss users context units text often mention several aspects one neighbourhoods first time generic social media platform case qa platform use fine grain opinion mine text come qa platforms far less constrain compare text review specific platforms current datasets base develop several strong baselines rely logistic regression state art recurrent neural network
question generation knowledge base kb task generate question relate domain input kb propose system generate fluent natural question kb significantly reduce human effort leverage massive web resources detail seed question set first generate apply small number hand craft templates input kb question retrieve iteratively form already obtain question search query standard search engine finally question select estimate fluency domain relevance evaluate human graders five hundred random select triple freebase question generate system judge fluent newciteserban etal2016p16 one human graders
propose neural network base model coordination boundary prediction network design incorporate two signal similarity conjuncts observation replace whole coordination phrase conjunct tend produce coherent sentence model make use several lstm network model train solely conjunction annotations treebank without use external resources show improvements predict coordination boundaries ptb compare two state art parsers well improvement previous coordination boundary prediction systems genia corpus
utilization statistical machine translation smt grow enormously last decade many use open source software develop nlp community commercial use increase need software optimize commercial requirements particular fast phrase base decode efficient utilization modern multicore servers paper examine major components phrase base decode decoder implementation particular emphasis speed scalability multicore machine result drop replacement moses decoder fifteen time faster scale monotonically number core
paper describe submission share task word phrase level quality estimation qe first conference statistical machine translation wmt16 objective share task predict give word phrase correct incorrect ok bad translation give sentence paper propose novel approach word level quality estimation use recurrent neural network language model rnn lm architecture rnn lms find effective different natural language process nlp applications rnn lm mainly use vector space language model different nlp problems task modify architecture rnn lm modify system predict label ok bad slot rather predict word input system word sequence similar standard rnn lm approach language independent require translate text qe estimate phrase level quality use output word level qe system
conventional attention base neural machine translation nmt conduct dynamic alignment generate target sentence repeatedly read representation source sentence keep fix generate encoder bahdanau et al two thousand and fifteen attention mechanism greatly enhance state art nmt paper propose new attention mechanism call interactive attention model interaction decoder representation source sentence translation read write operations interactive attention keep track interaction history therefore improve translation performance experiment nist chinese english translation task show interactive attention achieve significant improvements previous attention base nmt baseline state art variants attention base nmt ie coverage model tu et al two thousand and sixteen neural machine translator interactive attention outperform open source attention base nmt system groundhog four hundred and twenty-two bleu point open source phrase base system moses three hundred and ninety-four bleu point averagely multiple test set
neural machine translation nmt new approach machine translation make great progress recent years however recent study show nmt generally produce fluent inadequate translations tu et al 2016b tu et al 2016a et al two thousand and sixteen tu et al two thousand and seventeen contrast conventional statistical machine translation smt usually yield adequate non fluent translations natural therefore leverage advantage model better translations work propose incorporate smt model nmt framework specifically decode step smt offer additional recommendations generate word base decode information nmt eg generate partial translation attention history employ auxiliary classifier score smt recommendations gate function combine smt recommendations nmt generations jointly train within nmt architecture end end manner experimental result chinese english translation show propose approach achieve significant consistent improvements state art nmt smt systems multiple nist test set
recently development neural machine translation nmt significantly improve translation quality automatic machine translation sentence accurate fluent translations statistical machine translation smt base systems case nmt system produce translations completely different mean especially case rare word occur use statistical machine translation already show significant gain achieve simplify input preprocessing step commonly use example pre reorder approach work use phrase base machine translation pre translate input target language neural machine translation system generate final hypothesis use pre translation thereby use either output phrase base machine translation pbmt system combination pbmt output source sentence evaluate technique english german translation task use approach able outperform pbmt system well baseline neural mt system two bleu point analyze influence quality initial system final result
end end attention base model show competitive alternatives conventional dnn hmm model speech recognition systems paper extend exist end end attention base model apply distant speech recognition dsr task specifically propose end end attention base speech recognizer multichannel input perform sequence prediction directly character level gain better performance also incorporate highway long short term memory hlstm outperform previous model ami distant speech recognition task
language produce reflect personality various personal demographic characteristics detect natural language texts focus one particular personal trait author gender study manifest original texts translations show author gender powerful clear signal originals texts signal obfuscate human machine translation propose simple domain adaptation techniques help retain original gender traits translation without harm quality translation thereby create personalize machine translation systems
paper study impact different type feature apply learn rank question community question answer test model two datasets release semeval two thousand and sixteen task three community question answer task three target real life web fora english arabic model include bag word feature bow syntactic tree kernels tks rank feature embeddings machine translation evaluation feature best knowledge structural kernels barely apply question reranking task model paraphrase relations case english question rank task compare learn rank l2r algorithms strong baseline give google generate rank gr result show shallow structure use tks robust enough noisy data ii improve gr possible effective bow feature tks along accurate model gr feature use l2r algorithm require case arabic question rank task first time apply tree kernels syntactic tree arabic sentence approach task obtain second best result semeval two thousand and sixteen subtasks b english arabic
since first online demonstration neural machine translation nmt lisa nmt development recently move laboratory production systems demonstrate several entities announce roll nmt engines replace exist technologies nmt systems large number train configurations train process systems usually long often weeks role experimentation critical important share work present approach production ready systems simultaneously release online demonstrators cover large variety languages twelve languages thirty-two language pair explore different practical choices efficient evolutive open source framework data preparation network architecture additional implement feature tune production etc discuss evaluation methodology present first find finally outline work ultimate goal share expertise build competitive production systems generic translation aim contribute set collaborative framework speed adoption technology foster research efforts enable delivery adoption industry use case specific engines integrate real production workflows master technology would allow us build translation engines suit particular need outperform current simplest uniform systems
paper describe efficient approach improve accuracy name entity recognition system vietnamese approach combine regular expressions tokens bidirectional inference method sequence label model propose method achieve overall f1 score eight thousand, nine hundred and sixty-six test set evaluation campaign organize late two thousand and sixteen vietnamese language speech process vlsp community
extraction concepts present patient clinical record essential step clinical research two thousand and ten i2b2 va workshop natural language process challenge clinical record present concept extraction ce task aim identify concepts treatments test problems classify predefined categories state art ce approach heavily rely hand craft feature domain specific resources hard collect tune reason paper employ bidirectional lstm crf decode initialize general purpose shelf word embeddings ce experimental result achieve two thousand and ten i2b2 va reference standard corpora use bidirectional lstm crf rank closely top rank systems
paper introduce threshold free approach motivate chinese restaurant process purpose cognate cluster show approach yield similar result linguistically motivate cognate cluster system know lexstat chinese restaurant process system fast require threshold apply language family world
texts present coherent stories particular theme overall set example science fiction western paper present text generation method call rewrite edit exist human author narratives change theme without change underlie story apply approach math word problems might help students stay engage quickly transform homework assignments theme favorite movie without change math concepts teach rewrite method use two stage decode process propose new word target theme score result stories accord number factor define aspects syntactic semantic thematic coherence experiment demonstrate final stories typically represent new theme well still test original math concepts outperform number baselines also release new dataset human author rewrite math word problems several theme
describe simple effective method cross lingual syntactic transfer dependency parsers scenario large amount translation data available method make use three step one method derive cross lingual word cluster use multilingual parser two method transfer lexical information target language source language treebanks three method integrate step density drive annotation projection method rasooli collins two thousand and fifteen experiment show improvements state art several languages use previous work set source translation data bible considerably smaller corpus europarl corpus use previous work result use europarl corpus source translation data show additional improvements result rasooli collins two thousand and fifteen conclude result thirty-eight datasets universal dependencies corpora
advent word embeddings lexicons longer fully utilize sentiment analysis although still provide important feature traditional set paper introduce novel approach sentiment analysis integrate lexicon embeddings attention mechanism convolutional neural network approach perform separate convolutions word lexicon embeddings provide global view document use attention model experiment semeval sixteen task four dataset stanford sentiment treebank show comparative better result exist state art systems analysis show lexicon embeddings allow build high perform model much smaller word embeddings attention mechanism effectively dim noisy word sentiment analysis
authorship attribution problem considerable practical technical interest several methods design infer authorship dispute document multiple contexts traditional statistical methods base solely word count relate measurements provide simple yet effective solution particular case prone manipulation recently texts successfully model network word represent nod link accord textual similarity measurements model useful identify informative topological pattern authorship recognition task however consensus measurements use thus propose novel method characterize text network consider topological dynamical aspects network use concepts methods cellular automata theory devise strategy grasp informative spatio temporal pattern model experiment reveal outperformance traditional analysis rely topological measurements remarkably find dependence pre process step lemmatization obtain result feature mostly disregard relate work optimize result obtain pave way better characterization textual network
explore use segment learn use byte pair encode refer bpe units basic units statistical machine translation relate languages compare orthographic syllables currently best perform basic units translation task bpe identify frequent character sequence basic units orthographic syllables linguistically motivate pseudo syllables show bpe units modestly outperform orthographic syllables units translation show eleven increase bleu score orthographic syllables use languages whose write systems use vowel representations bpe write system independent show bpe outperform units non vowel write systems result support extensive experimentation span multiple language families write systems
year nara institute science technology naist carnegie mellon university cmu submission japanese english translation track two thousand and sixteen workshop asian translation base attentional neural machine translation nmt model addition standard nmt model make number improvements notably use discrete translation lexicons improve probability estimate use minimum risk train optimize mt system bleu score result system achieve highest translation evaluation score task
exist neural machine translation model use group character whole word unit input output propose model hierarchical char2word encoder take individual character input output first argue hierarchical representation character encoder reduce computational complexity show improve translation performance secondly qualitatively study attention plot decoder find model learn compress common word single embed whereas rare word name place represent character character
exist machine translation decode algorithms generate translations strictly monotonic fashion never revisit previous decisions result earlier mistake correct later stage paper present translation scheme start initial guess make iterative improvements may revisit previous decisions parameterize model convolutional neural network predict discrete substitutions exist translation base attention mechanism source sentence well current translation output make less one modification per sentence improve output phrase base translation system four bleu wmt15 german english translation
past work computational sarcasm deal primarily sarcasm detection paper introduce novel relate problem sarcasm target identification ie extract target ridicule sarcastic sentence present introductory approach sarcasm target identification approach employ two type extractors one base rule another consist statistical classifier compare approach use two baselines nai baseline another baseline base work sentiment target identification perform experiment book snippets tweet show hybrid approach perform better two baselines also comparison use two extractors individually introductory approach establish viability sarcasm target identification serve baseline future work
open domain human computer conversation attract much attention field nlp contrary rule template base domain specific dialog systems open domain conversation usually require data drive approach roughly divide two categories retrieval base generation base systems retrieval systems search user issue utterance call query large database return reply best match query generative approach typically base recurrent neural network rnns synthesize new reply suffer problem generate short meaningless utterances paper propose novel ensemble retrieval base generation base dialog systems open domain approach retrieve candidate addition original query feed rnn base reply generator neural model aware information generate reply feed back new candidate post reranking experimental result show ensemble outperform single part large margin
neural machine translation nmt become new state art several language pair however remain challenge problem integrate nmt bilingual dictionary mainly contain word rarely never see bilingual train data paper propose two methods bridge nmt bilingual dictionaries core idea behind design novel model transform bilingual dictionaries adequate sentence pair nmt distil latent bilingual mappings ample repetitive phenomena one method leverage mix word character model attempt synthesize parallel sentence guarantee massive occurrence translation lexicon extensive experiment demonstrate propose methods remarkably improve translation quality rare word test sentence obtain correct translations cover dictionary
paper discuss centre development advance compute mumbai cdacm submission nlp tool contest statistical machine translation indian languages ilsmt two thousand and fourteen collocate icon two thousand and fourteen objective contest explore effectiveness statistical machine translation smt indian language indian language english hindi machine translation paper propose suffix separation word split smt agglutinative languages hindi significantly improve baseline bl also show factor model reorder outperform phrase base smt english hindi enhi report work five pair languages namely bengali hindi bnhi marathi hindi mrhi tamil hindi tahi telugu hindi tehi enhi health tourism general domains
reorder preprocessing stage statistical machine translation smt system word source sentence reorder per syntax target language propose rich set rule better reorder idea facilitate train process better alignments parallel phrase extraction phrase base smt system reorder also help decode process hence improve machine translation quality observe significant improvements translation quality use approach baseline smt use bleu nist multi reference word error rate multi reference position independent error rate judge improvements exploit open source smt toolkit moses develop system
document briefly describe systems submit center robust speech systems crss university texas dallas utd two thousand and sixteen national institute standards technology nist speaker recognition evaluation sre develop several ubm dnn vector base speaker recognition systems different data set feature representations give emphasis nist sre two thousand and sixteen language mismatch train enrollment test data call domain mismatch system development focus one use unlabeled domain data centralize data alleviate domain mismatch problem two find best data set train lda plda three use newly propose dimension reduction technique incorporate unlabeled domain data plda train four unsupervised speaker cluster unlabeled data use alone previous sres plda train five score calibration use unlabeled data combination unlabeled development dev data separate experiment
emoji contemporary extremely popular way enhance electronic communication without rigid semantics attach emoji symbols take different mean base context message thus like word sense disambiguation task natural language process machine also need disambiguate mean sense emoji first step toward achieve goal paper present emojinet first machine readable sense inventory emoji emojinet resource enable systems link emoji context specific mean automatically construct integrate multiple emoji resources babelnet comprehensive multilingual sense inventory available date paper discuss construction evaluate automatic resource creation process present use case emojinet disambiguate emoji usage tweet emojinet available online use http emojinetknoesisorg
analyze performance encoder decoder neural model compare well know establish methods latter represent different class traditional approach apply monotone sequence sequence task ocr post correction spell correction grapheme phoneme conversion lemmatization task practical relevance various higher level research field include digital humanities automatic text correction speech recognition investigate well generic deep learn approach adapt task perform comparison establish specialize methods include adaptation prune crfs
semeval two thousand and ten benchmark dataset bring renew attention task automatic keyphrase extraction dataset make scientific article automatically convert pdf format plain text thus require careful preprocessing irrevelant span text negatively affect keyphrase extraction performance previous work wide range document preprocessing techniques describe impact overall performance keyphrase extraction model still unexplored assess performance several keyphrase extraction model measure robustness increasingly sophisticate level document preprocessing
natural language process historical document complicate abundance variant spell lack annotate data common approach normalize spell historical word modern form explore suitability deep neural network architecture task particularly deep bi lstm network apply character level model compare well previously establish normalization algorithms evaluate diverse set texts early new high german show multi task learn additional normalization data improve model performance
describe analyze simple effective algorithm sequence segmentation apply speech process task propose neural architecture compose two modules train jointly recurrent neural network rnn module structure prediction model rnn output consider feature function structure model overall model train structure loss function design give segmentation task demonstrate effectiveness method apply two simple task commonly use phonetic study word segmentation voice onset time segmentation result sug gest propose model superior previous methods ob taining state art result test datasets
paper present centre development advance compute mumbai cdacm submission nlp tool contest statistical machine translation indian languages ilsmt two thousand and fifteen collocate icon two thousand and fifteen aim contest collectively explore effectiveness statistical machine translation smt translate within indian languages english indian languages paper report work five language pair namely bengali hindi bnhi marathi hindi mrhi tamil hindi tahi telugu hindi tehi english hindi enhi health tourism general domains use suffix separation compound split preordering prior smt train test
data text systems powerful generate report data automatically thus simplify presentation complex data rather present data use visualisation techniques data text systems use natural human language common way human human communication addition data text systems adapt output content users preferences background interest therefore pleasant users interact content selection important part every data text system module determine available information convey user survey initially introduce field data text generation describe general data text system architecture review state art content selection methods finally provide recommendations choose approach discuss opportunities future research
progress text understand drive large datasets test particular capabilities like recent datasets read comprehension hermann et al two thousand and fifteen focus lambada dataset paperno et al two thousand and sixteen word prediction task require broader context immediate sentence view lambada read comprehension problem apply comprehension model base neural network though model constrain choose word context improve state art lambada seventy-three forty-nine analyze one hundred instance find neural network readers perform well case involve select name context base dialogue discourse cue struggle coreference resolution external knowledge need
distribute representation learn neural network recently show effective model natural languages fine granularities word phrase even sentence whether approach extend help model larger span text eg document intrigue investigation would still desirable paper aim enhance neural network model purpose typical problem document level model automatic summarization aim model document order generate summaries paper propose neural model train computers pay attention specific regions content input document attention model also distract traverse different content document better grasp overall mean summarization without engineer feature train model two large datasets model achieve state art performance significantly benefit distraction model particularly input document long
biomedical word sense disambiguation wsd important intermediate task many natural language process applications name entity recognition syntactic parse relation extraction paper employ knowledge base approach also exploit recent advance neural word concept embeddings improve state art biomedical wsd use msh wsd dataset test set methods involve weak supervision use hand label examples wsd build prediction model however employ exist well know name entity recognition concept map program metamap obtain concept vectors msh wsd dataset linear time term number sense word test instance method achieve accuracy nine thousand, two hundred and twenty-four absolute three improvement best know result obtain via unsupervised knowledge base mean expensive approach develop rely nearest neighbor framework achieve accuracy nine thousand, four hundred and thirty-four employ dense vector representations learn unlabeled free text show benefit many language process task recently efforts show biomedical wsd exception trend complex rapidly evolve domain biomedicine build label datasets larger set ambiguous term may impractical show weak supervision leverage recent advance representation learn rival supervise approach biomedical wsd however external knowledge base sense inventory play key role improvements achieve
present submission cogalex two thousand and sixteen share task corpus base identification semantic relations use lexnet shwartz dagan two thousand and sixteen integrate path base distributional method semantic relation classification report result share task bring submission third place subtask one word relatedness first place subtask two semantic relation classification demonstrate utility integrate complementary path base distributional information source recognize concrete semantic relations combine common similarity measure lexnet perform fairly good word relatedness task subtask one relatively low performance lexnet systems subtask two however confirm difficulty semantic relation classification task stress need develop additional methods task
sarcasm detection key task many natural language process task sentiment analysis example sarcasm flip polarity apparently positive sentence hence negatively affect polarity detection performance date approach sarcasm detection treat task primarily text categorization problem sarcasm however express subtle ways require deeper understand natural language standard text categorization techniques grasp work develop model base pre train convolutional neural network extract sentiment emotion personality feature sarcasm detection feature along network baseline feature allow propose model outperform state art benchmark datasets also address often ignore generalizability issue classify data see model learn phase
damage personal attack online discourse motivate many platforms try curb phenomenon however understand prevalence impact personal attack online platforms scale remain surprisingly difficult contribution paper develop illustrate method combine crowdsourcing machine learn analyze personal attack scale show evaluation method classifier term aggregate number crowd workers approximate apply methodology english wikipedia generate corpus 100k high quality human label comment 63m machine label ones classifier good aggregate three crowd workers measure area roc curve spearman correlation use corpus machine label score methodology allow us explore open question nature online personal attack reveal majority personal attack wikipedia result malicious users primarily consequence allow anonymous contributions unregistered users
focus problem learn distribute representations entity search query name entities short descriptions representation learn model entity search query name entity description represent low dimensional vectors goal develop simple effective model make distribute representations query relate entities similar query vector space hence propose three kinds learn strategies difference mainly lie deal relationship entity description analyze strengths weaknesses learn strategy validate methods public datasets contain four kinds name entities ie movies tv show restaurants celebrities experimental result indicate propose methods adapt different type entity search query outperform current state art methods base keyword match vanilla word2vec model besides propose methods train fast easily extend similar task
introduce word vectors construction domain vectors obtain run word2vec 11m word corpus create scratch leverage freely accessible online source construction relate text first explore embed space show vectors capture meaningful construction specific concepts evaluate performance vectors ones train 100b word corpus google news within framework injury report classification task without parameter tune embeddings give competitive result outperform google news vectors many case use keyword base compression report also lead significant speed limit loss performance release corpus data set create classification task publicly available hope use future study benchmarking build work
transliteration key component machine translation systems software internationalization paper demonstrate neural sequence sequence model obtain state art close state art result exist datasets effort make machine transliteration accessible open source new arabic english transliteration dataset train model
order extract event information text machine read model must learn accurately read interpret ways information express must also human reader must aggregate numerous individual value hypotheses single coherent global analysis apply global constraints reflect prior knowledge domain work focus task extract plane crash event information cluster relate news article whose label derive via distant supervision unlike previous machine read work assume target value occur frequently cluster may also miss incorrect introduce novel neural architecture explicitly model noisy nature data deal aforementioned learn issue model train end end achieve improvement one hundred and twenty-one f1 previous work despite use far less linguistic annotation apply factor graph constraints promote coherent event analyse belief propagation inference formulate within transition recurrent neural network show technique additionally improve maximum f1 twenty-eight point result relative improvement fifty previous state art
paper present centre development advance compute mumbai cdacm submission nlp tool contest part speech pos tag code mix indian social media text poscmismt two thousand and fifteen collocate icon two thousand and fifteen submit result hindi hi bengali bn telugu te languages mix english en paper describe approach pos tag techniques exploit task machine learn use pos tag mix language text pos tag distribute representations word vector space word2vec feature extraction log linear model try report work three languages hi bn te mix en
name entity recognition often large domain train corpus knowledge base adequate coverage train model directly paper propose method give train data relate domain similar identical name entity ne type small amount domain train data use transfer learn learn domain specific ne model novelty task setup assume domain mismatch also label mismatch
address novel problem automatically generate quiz style knowledge question knowledge graph dbpedia question kind ample applications instance educate users evaluate knowledge specific domain solve problem propose end end approach approach first select name entity knowledge graph answer generate structure triple pattern query yield answer sole result multiple choice question desire approach select alternative answer options finally approach use template base method verbalize structure query yield natural language question key challenge estimate difficult generate question human users make use historical data jeopardy quiz show semantically annotate web scale document collection engineer suitable feature train logistic regression classifier predict question difficulty experiment demonstrate viability overall approach
despite substantial progress make develop new sentiment lexicon generation slg methods english task transfer approach languages domains sound way still remain open paper contribute solution problem systematically compare semi automatic translations common english polarity list result original automatic slg algorithms apply directly german data evaluate lexicons corpus seven thousand, nine hundred and ninety-two manually annotate tweet addition also collate result dictionary corpus base slg methods order find paradigms better suit inherently noisy domain social media experiment show semi automatic translations notably outperform automatic systems reach macro average f1 score five hundred and eighty-nine dictionary base techniques produce much better polarity list compare corpus base approach whose best f1 score run four hundred and seventy-nine four hundred and nineteen respectively even non standard twitter genre
paper propose dynamic chunk reader dcr end end neural read comprehension rc model able extract rank set answer candidates give document answer question dcr able predict answer variable lengths whereas previous neural rc model primarily focus predict single tokens entities dcr encode document input question recurrent neural network apply word word attention mechanism acquire question aware representations document follow generation chunk representations rank module propose top rank chunk answer experimental result show dcr achieve state art exact match f1 score squad dataset
project aim provide semi supervise approach identify multiword expressions multilingual context consist english major indian languages multiword expressions group word refer conventional regional way say things literally translate one language another expression lose inherent mean automatically extract multiword expressions corpus extraction pipeline construct consist combination rule base statistical approach several type multiword expressions differ widely construction employ different methods detect different type multiword expressions give pos tag corpus english indian language system initially apply regular expression filter narrow search space certain pattern like reduplication partial reduplication compound nouns compound verbs conjunct verbs etc word sequence match require pattern subject series linguistic test include verb filter name entity filter hyphenation filter test exclude false positives candidates check semantic relationships among use wordnet order detect partial reduplication make use wordnet lexical database well tool lemmatising detect complex predicate investigate feature constituent word statistical methods apply detect collocations finally lexicographers examine list automatically extract candidates validate whether true multiword expressions add multiword dictionary accordingly
distribute representations word show capture lexical semantics demonstrate effectiveness word similarity analogical relation task task evaluate lexical semantics indirectly paper study whether possible utilize distribute representations generate dictionary definitions word direct transparent representation embeddings semantics introduce definition model task generate definition give word embed present several definition model architectures base recurrent neural network experiment model multiple data set result show model control dependencies word define definition word perform significantly better character level convolution layer design leverage morphology complement word level embeddings finally error analysis suggest errors make definition model may provide insight shortcomings word embeddings
present automatic mortality prediction scheme base unstructured textual content clinical note propose convolutional document embed approach empirical investigation use mimic iii intensive care database show significant performance gain compare previously employ methods latent topic distributions generic doc2vec embeddings improvements especially pronounce difficult problem post discharge mortality prediction
transition base model fast accurate constituent parse compare chart base model leverage richer feature extract history information parser stack span non local constituents hand incremental parse constituent information right hand side current word utilize relative weakness shift reduce parse address limitation leverage fast neural model extract lookahead feature particular build bidirectional lstm model leverage full sentence information predict hierarchy constituents word start end result pass strong transition base constituent parser lookahead feature result parser give thirteen absolute improvement wsj twenty-three ctb compare baseline give highest report accuracies fully supervise parse
though work improve distribute word representations use lexicons improper overfitting word multiple mean remain issue deteriorate learn lexicons use need solve alternative method allocate vector per sense instead vector per word however word representations estimate former way easy use latter one previous work use probabilistic method alleviate overfitting robust small corpus paper propose new neural network estimate distribute word representations use lexicon corpus add lexicon layer continuous bag word model threshold node output lexicon layer threshold reject unreliable output lexicon layer less likely input way alleviate overfitting polysemous word propose neural network train use negative sample maximize log probabilities target word give context word distinguish target word random noise compare propose neural network continuous bag word model work improve previous work estimate distribute word representations use lexicon corpus experimental result show propose neural network efficient balance semantic task syntactic task previous work robust size corpus
long short term memory lstm widely use speech recognition order achieve higher prediction accuracy machine learn scientists build larger larger model large model computation intensive memory intensive deploy bulky model result high power consumption lead high total cost ownership tco data center order speedup prediction make energy efficient first propose load balance aware prune method compress lstm model size 20x 10x prune 2x quantization negligible loss prediction accuracy prune model friendly parallel process next propose scheduler encode partition compress model pe parallelism schedule complicate lstm data flow finally design hardware architecture name efficient speech recognition engine ese work directly compress model implement xilinx xcku060 fpga run 200mhz ese performance two hundred and eighty-two gops work directly compress lstm network correspond two hundred and fifty-two top uncompress one process full lstm speech recognition power dissipation forty-one watts evaluate lstm speech recognition benchmark ese 43x 3x faster core i7 5930k cpu pascal titan x gpu implementations achieve 40x 115x higher energy efficiency compare cpu gpu respectively
automatic essay score aes refer process score free text responses give prompt consider human grader score gold standard write essay essential component many language aptitude exams hence aes become active establish area research many proprietary systems use real life applications today however much know specific linguistic feature useful prediction much consistent across datasets article address explore role various linguistic feature automatic essay score use two publicly available datasets non native english essay write test take scenarios linguistic properties model encode lexical syntactic discourse error type learner language feature set predictive model develop use feature datasets predictive feature compare result show feature set use result good predictive model datasets question predictive feature different answer dataset
generation political event data remain much since mid 1990s term data acquisition process cod text data since 1990s however significant improvements open source natural language process software availability digitize news content paper present new next generation event dataset name phoenix build advance dataset include improvements underlie news collection process event cod software along creation general process pipeline necessary produce daily update data paper provide face validity check briefly examine data conflict syria comparison phoenix integrate crisis early warn system data
math word problems provide natural abstraction range natural language understand problems involve reason quantities interpret election result news casualties financial section newspaper units associate quantities often provide information essential support reason paper propose principled way capture reason units show benefit arithmetic word problem solver paper present concept unit dependency graph udgs provide compact representation dependencies units number mention give problem induce udg alleviate brittleness unit extraction system allow natural way leverage domain knowledge unit compatibility word problem solve introduce decompose model induce udgs minimal additional annotations use augment expressions use arithmetic word problem solver roy roth two thousand and fifteen via constrain inference framework show introduction udgs reduce error solver ten surpass exist systems solve arithmetic word problems addition also make system robust adaptation new vocabulary equation form
product review contain lot useful information product feature customer opinions one important product feature complementary entity products may potentially work together review product know complementary entities review product important customers want buy compatible products avoid incompatible ones paper address problem complementary entity recognition cer since exist method solve problem first propose novel unsupervised method utilize syntactic dependency paths recognize complementary entities expand category level domain knowledge complementary entities use general seed verbs large amount unlabeled review domain knowledge help unsupervised method adapt different products greatly improve precision cer task advantage propose method require label data train conduct experiment seven popular products one thousand, two hundred review total demonstrate propose approach effective
paper present strategies map dialog act annotations lego corpus communicative function iso twenty-four thousand, six hundred and seventeen two standard use strategies obtain additional three hundred and forty-seven dialogs annotate accord standard particularly important give reduce amount exist data condition due recency standard furthermore dialogs widely explore corpus dialog relate task however dialog annotations neglect due high domain dependency render unuseful outside context corpus thus map process obtain data annotate accord recent standard provide useful dialog act annotations widely explore corpus context dialog research
study response selection multi turn conversation retrieval base chatbots exist work either concatenate utterances context match response highly abstract context vector finally may lose relationships among utterances important contextual information propose sequential match network smn address problems smn first match response utterance context multiple level granularity distill important match information pair vector convolution pool operations vectors accumulate chronological order recurrent neural network rnn model relationships among utterances final match score calculate hide state rnn empirical study two public data set show smn significantly outperform state art methods response selection multi turn conversation
paper propose first attempt build end end speech text translation system use source language transcription learn decode propose model direct speech text translation give promise result small french english synthetic corpus relax need source language transcription would drastically change data collection methodology speech translation especially resourced scenarios instance former project darpa transtac speech translation speak arabic dialects large effort devote collection speech transcripts prerequisite obtain transcripts often detail transcription guide languages little standardize spell end end approach speech text translation successful one might consider collect data ask bilingual speakers directly utter speech source language target language text utterances approach advantage applicable unwritten source language
diagnosis clinical condition challenge task often require significant medical investigation previous work relate diagnostic inferencing problems mostly consider multivariate observational data eg physiological signal lab test etc contrast explore problem use free text medical note record electronic health record ehr complex task like benefit structure knowledge base scalable instead exploit raw text wikipedia knowledge source memory network demonstrate effective task require comprehension free form text use final iteration learn representation predict probable class introduce condense memory neural network c memnns novel model iterative condensation memory representations preserve hierarchy feature memory experiment mimic iii dataset show propose model outperform variants memory network predict probable diagnose give complex clinical scenario
multitask learn apply successfully range task mostly morphosyntactic however little know mtl work whether data characteristics help determine success paper evaluate range semantic sequence label task mtl setup examine different auxiliary task amongst novel setup correlate impact data dependent condition result show mtl always effective significant improvements obtain one five task successful auxiliary task compact uniform label distributions preferable
word embeddings widely use natural language process mainly due success capture semantic information massive corpora however creation process allow different mean word automatically separate conflate single vector address issue propose new model learn word sense embeddings jointly model exploit large corpora knowledge semantic network order produce unify vector space word sense embeddings evaluate main feature approach qualitatively quantitatively variety task highlight advantage propose method comparison state art word sense base model
standard approach entity identification hard code boundary detection type prediction label eg john b per smith per perform viterbi two disadvantage one runtime complexity grow quadratically number type two natural segment level representation paper propose novel neural architecture address disadvantage frame problem multitasking separate boundary detection type prediction optimize jointly despite simplicity architecture perform competitively fully structure model bilstm crfs scale linearly number type furthermore construction model induce type disambiguate embeddings predict mention
investigate task infer conversational dependencies message one one online chat become one popular form customer service propose novel probabilistic classifier leverage conversational lexical semantic information approach evaluate empirically set customer service chat log chinese e commerce website outperform heuristic baselines
language generation task seek mimic human ability use language creatively difficult evaluate since one must consider creativity style non trivial aspects generate text goal paper develop evaluation methods one task ghostwrite rap lyric provide explicit quantifiable foundation goals future directions task ghostwrite must produce text similar style emulate artist yet distinct content develop novel evaluation methodology address several complementary aspects task illustrate evaluation use meaningfully analyze system performance provide corpus lyric thirteen rap artists annotate stylistic similarity allow us assess feasibility manual evaluation generate verse
work present new dataset computational humor specifically comparative humor rank attempt eschew ubiquitous binary approach humor detection dataset consist tweet humorous responses give hashtag describe motivation new dataset well collection process include description semi automate system data collection also present initial experiment dataset use unsupervised supervise approach best supervise system achieve six hundred and thirty-seven accuracy suggest task much difficult comparable humor detection task initial experiment indicate character level model suitable task token level model likely due large amount pun capture character level model
inspire recent research explore ways model highly morphological finnish language level character maintain performance word level model propose new character word character c2w2c compositional language model use character input output still internally process word level embeddings preliminary experiment use finnish europarl v7 corpus indicate c2w2c respond well challenge morphologically rich languages high vocabulary rat prediction novel word grow vocabulary size notably model able correctly score inflectional form present train data sample grammatically semantically correct finnish sentence character character
investigate distinguish report dream personal narratives continuity hypothesis stem psychological dream analysis work state dream refer person daily life personal concern similar personal narratives diary entries differences two texts may reveal linguistic markers dream text could basis new dream analysis work automatic detection dream descriptions use three text analytics methods text classification topic model text coherence analysis apply methods balance set texts represent dream diary entries personal stories observe dream texts could distinguish personal narratives nearly perfectly mostly base presence uncertainty markers descriptions scenes important markers non dream narratives specific time expressions conversational expressions dream texts also exhibit lower discourse coherence personal narratives
collection narrative spontaneous report irreplaceable source prompt detection suspect adverse drug reactions adrs qualify domain experts manually revise huge amount narrative descriptions encode texts accord meddra standard terminology manual annotation narrative document medical terminology subtle expensive task since number report grow day day magicoder natural language process algorithm propose automatic encode free text descriptions meddra term magicoder procedure efficient term computational complexity particular linear size narrative input terminology test large dataset four thousand, five hundred manually revise report perform automate comparison human magicoder revisions current base version magicoder measure short descriptions average recall eighty-six average precision eighty-eight medium long descriptions two hundred and fifty-five character average recall sixty-four average precision sixty-three practical point view magicoder reduce time require encode adr report pharmacologists simply review validate magicoder term propose application instead choose right term among 70k low level term meddra improvement efficiency pharmacologists work relevant impact also quality subsequent data analysis develop magicoder italian pharmacovigilance language however proposal base general approach depend consider language term dictionary
present novel scheme combine neural machine translation nmt traditional statistical machine translation smt approach borrow ideas linearise lattice minimum bay risk decode smt nmt score combine bay risk translation accord smt lattice make approach much flexible n best list lattice rescoring neural decoder restrict smt search space show efficient simple way integrate risk estimation nmt decoder suitable word level well subword unit level nmt test method english german japanese english report significant gain lattice rescoring several data set single ensembled nmt mbr decoder produce entirely new hypotheses far beyond simply rescoring smt search space fix unks nmt output
introduce new model recurrent entity network entnet equip dynamic long term memory allow maintain update representation state world receive new data language understand task reason fly read text require answer question respond case memory network sukhbaatar et al two thousand and fifteen like neural turing machine differentiable neural computer grave et al two thousand and fourteen two thousand and sixteen maintain fix size memory learn perform location content base read write operations however unlike model simple parallel architecture several memory locations update simultaneously entnet set new state art babi task first method solve task 10k train examples set also demonstrate solve reason task require large number support facts methods able solve generalize past train horizon also practically use large scale datasets children book test obtain competitive performance read story single pass
machine learn language improve supply specific knowledge source external information present new version link open data resource conceptnet particularly well suit use modern nlp techniques word embeddings conceptnet knowledge graph connect word phrase natural language label edge knowledge collect many source include expert create resources crowd source game purpose design represent general knowledge involve understand language improve natural language applications allow application better understand mean behind word people use conceptnet combine word embeddings acquire distributional semantics word2vec provide applications understand would acquire distributional semantics alone narrower resources wordnet dbpedia demonstrate state art result intrinsic evaluations word relatedness translate improvements applications word vectors include solve sit style analogies
paper describe methods evaluate automatic speech recognition asr systems comparison human perception result use measure derive linguistic distinctive feature error pattern term manner place voice present along examination confusion matrices via distinctive feature distance metric evaluation methods contrast conventional performance criteria focus phone word level intend provide detail profile asr system performanceas well mean direct comparison human perception result sub phonemic level
mismatch transcriptions propose mean acquire probabilistic transcriptions non native speakers languageprior work demonstrate value transcriptions successfully adapt cross lingual asr systems different tar get languages work describe two techniques refine probabilistic transcriptions noisy channel model non native phone misperception train use recurrent neural net work decode use minimally resourced language dependent pronunciation constraints innovations improve quality transcript innovations reduce phone error rate trainedasr seven nine respectively
parallel corpora drive great progress field text simplification however sentence alignment algorithms either offer limit range alignment type support simply ignore valuable clue present comparable document address problem introduce new set flexible vicinity drive paragraph sentence alignment algorithms one n n one n n long distance null alignments without need hard replicate supervise model
previous machine comprehension mc datasets either small train end end deep learn model difficult enough evaluate ability current mc techniques newly release squad dataset alleviate limitations give us chance develop realistic mc model base dataset propose multi perspective context match mpcm model end end system directly predict answer begin end point passage model first adjust word embed vector passage multiply relevancy weight compute question encode question weight passage use bi directional lstms point passage model match context point encode question multiple perspectives produce match vector give match vectors employ another bi directional lstm aggregate information predict begin end point experimental result test set squad show model achieve competitive result leaderboard
present dual contribution task machine read comprehension technique create large size machine comprehension mc datasets use paragraph vector model novel hybrid neural network architecture combine representation power recurrent neural network discriminative power fully connect multi layer network use mc dataset generation technique build dataset around two million examples empirically determine high ceiling human performance around ninety-one accuracy well performance variety computer model among model experiment hybrid neural network architecture achieve highest performance eight hundred and thirty-two accuracy remain gap human performance ceiling provide enough room future model improvements
fundamental role hypernymy nlp motivate development many methods automatic identification relation rely word distribution investigate extensive number unsupervised measure use several distributional semantic model differ context type feature weight analyze performance different methods base linguistic motivation comparison state art supervise methods show supervise methods generally outperform unsupervised ones former sensitive distribution train instance hurt reliability base general linguistic hypotheses independent train data unsupervised measure robust therefore still useful artillery hypernymy detection
product community question answer pcqa provide useful information products feature aspects may well address product descriptions review observe product compatibility issue products frequently discuss pcqa issue frequently address accessories ie via yes question mouse work windows ten paper address problem extract compatible incompatible products yes question pcqa problem naturally two stage framework first perform complementary entity product recognition cer yes question second identify polarities yes answer assign complementary entities compatibility label compatible incompatible unknown leverage exist unsupervised method first stage three class classifier combine distant pu learn method learn positive unlabeled examples together binary classifier second stage benefit use distant pu learn help expand implicit yes answer without use human annotate data conduct experiment four products show propose method effective
make one first attempt build work model intra sentential code switch base equivalence constraint poplack one thousand, nine hundred and eighty matrix language myers scotton one thousand, nine hundred and ninety-three theories conduct detail theoretical analysis small scale empirical study two model hindi english cs analyse show model neither sound complete take insights errors make model propose new model combine feature theories
emoji essential component dialogues broadly utilize almost social platforms could express delicate feel beyond plain texts thus smooth communications users make dialogue systems anthropomorphic vivid paper focus automatically recommend appropriate emojis give contextual information multi turn dialogue systems challenge locate understand whole conversations specifically propose hierarchical long short term memory model h lstm construct dialogue representations follow softmax classifier emoji classification evaluate model task emoji classification real world dataset explorations parameter sensitivity case study experimental result demonstrate method achieve best performances evaluation metrics indicate method could well capture contextual information emotion flow dialogues significant emoji recommendation
analyse translation quality regard specific linguistic phenomena historically difficult time consume neural machine translation attractive property produce score arbitrary translations propose novel method assess well nmt systems model specific linguistic phenomena agreement long distance production novel word faithful translation polarity core idea measure whether reference translation probable nmt model contrastive translation introduce specific type error present lingeval97 large scale data set ninety-seven thousand contrastive translation pair base wmt english german translation task errors automatically create simple rule report result number systems find recently introduce character level nmt systems perform better transliteration model byte pair encode bpe segmentation perform poorly morphosyntactic agreement translate discontiguous units mean
paper report domain cluster ambit adaptive mt architecture standard bottom hierarchical cluster algorithm instantiate five different distance compare mt benchmark build forty commercial domains term dendrograms intrinsic extrinsic evaluations main outcome expensive distance also one able allow mt engine guarantee good performance even highly populate cluster domains
present family neural network inspire model compute continuous word representations specifically design exploit monolingual multilingual text framework allow us perform unsupervised train embeddings exhibit higher accuracy syntactic semantic compositionality well multilingual semantic similarity compare previous model train unsupervised fashion also show multilingual embeddings optimize semantic similarity improve performance statistical machine translation respect handle word present parallel data
purpose copasul toolkit one automatic prosodic annotation two prosodic feature extraction syllable utterance level copasul stand contour base parametric superpositional intonation stylization framework intonation represent superposition global local contour describe parametrically term polynomial coefficients global level usually associate necessarily restrict intonation phrase stylization serve represent register term time vary f0 level range local level eg accent group local contour shape describe parameterization several feature relate prosodic boundaries prominence derive furthermore coefficient cluster prosodic contour class obtain bottom way next stylization base feature extraction also standard f0 energy measure eg mean variance well rhythmic aspects calculate current state automatic annotation comprise segmentation interpausal chunk syllable nucleus extraction unsupervised localization prosodic phrase boundaries prominent syllables f0 partly also energy feature set derive standard measurements median iqr register term f0 level range prosodic boundaries local contour shape bottom derive contour class gestalt accent group term deviation higher level prosodic units well rhythmic aspects quantify relation f0 energy contour prosodic event rat
paper present novel reranking model future reward reranking score action transition base parser use global scorer different conventional reranking parse model search best dependency tree feasible tree constrain sequence action get future reward sequence scorer base first order graph base parser bidirectional lstm catch different parse view compare transition base parser besides since context enhancement show substantial improvement arc stand transition base parse parse accuracy implement context enhancement arc eager transition base parser stack lstms dynamic oracle dropout support achieve improvement global scorer context enhancement result show uas parser increase much one hundred and twenty english one hundred and sixty-six chinese las increase much one hundred and thirty-two english one hundred and sixty-three chinese moreover get state art lass achieve eight thousand, seven hundred and fifty-eight chinese nine thousand, three hundred and thirty-seven english
create sentiment polarity lexicons labor intensive automatically translate resourceful languages require domain machine translation systems rely large quantities bi texts paper propose replace machine translation transfer word lexicon word embeddings align across languages simple linear transform approach lead degradation compare machine translation test sentiment polarity classification tweet four languages
social media websites electronic newspapers internet forums allow visitors leave comment others read interact exchange free participants malicious intentions troll others posit message intend provocative offensive menace goal facilitate computational model troll propose troll categorization novel sense allow comment base analysis troll responders perspectives characterize two perspectives use four aspects namely troll intention intention disclosure well responder interpretation troll intention response strategy use categorization annotate release dataset contain excerpt reddit conversations involve suspect troll interactions users finally identify difficult classify case corpus suggest potential solutions
topics generate topic model typically represent list term reduce cognitive overhead interpret topics end users propose label topic succinct phrase summarise theme idea use wikipedia document title label candidates compute neural embeddings document word select relevant label topics compare state art topic label system methodology simpler efficient find better topic label
propose new approach extract argument structure natural language texts contain underlie argument approach comprise two phase score assignment structure prediction score assignment phase train model classify relations argument units support attack neutral end different train strategies explore identify different linguistic lexical feature train classifiers ablation study observe novel use word embed feature effective task structure prediction phase make use score score assignment phase arrive optimal structure perform experiment three argumentation datasets namely araucariadb debatepedia wikipedia also propose two baselines observe propose approach outperform baseline systems final task structure prediction
address data selection problem statistical machine translation smt classification task new data selection method base neural network classifier present new method description empirical result prove data selection method provide better translation quality compare state art method ie cross entropy moreover empirical result report coherent across different language pair
explore task multi source morphological reinflection generalize standard single source version input consist target tag ii multiple pair source form source tag lemma motivation beneficial access one source form since different source form provide complementary information eg different stem present novel extension encoder decoder recurrent neural architecture consist multiple encoders better solve task show new architecture outperform single source reinflection model publish dataset multi source morphological reinflection facilitate future research
train efficiency one main problems neural machine translation nmt deep network need large data well many train iterations achieve state art performance result high computation cost slow research industrialisation paper propose alleviate problem several train methods base data boost bootstrap modifications neural network imitate learn process humans typically spend time learn difficult concepts easier ones experiment english french translation task show accuracy improvements one hundred and sixty-three bleu save twenty train time
text simplification aim reduce lexical grammatical structural complexity text keep mean context machine translation introduce idea simplify translations order boost learn ability deep neural translation model conduct preliminary experiment show translation complexity actually reduce translation source bi text compare target reference bi text use neural machine translation nmt system learn exact bi text base knowledge distillation idea train nmt system use simplify bi text show outperform initial system build reference data set performance boost reference automatic translations use learn network perform elementary analysis translate corpus report accuracy result propose approach english french english german translation task
machine translation systems sensitive domains train several domain adaptation techniques deeply study propose new technique neural machine translation nmt call domain control perform runtime use unique neural network cover multiple domains present approach show quality improvements compare dedicate domains translate cover domains even domain data addition model parameters need estimate domain make effective real use case evaluation carry english french translation two different test scenarios first consider case end user perform translations know domain secondly consider scenario domain know predict sentence level translate result show consistent accuracy improvements condition
domain adaptation key feature machine translation generally encompass terminology domain style adaptation especially human post edit workflows computer assist translation cat neural machine translation nmt introduce new notion domain adaptation call specialization show promise result learn speed adaptation accuracy paper propose explore approach several perspectives
parse accuracy use efficient greedy transition systems improve dramatically recent years thank neural network despite strike result dependency parse however neural model surpass state art approach constituency parse remedy introduce new shift reduce system whose stack contain merely sentence span represent bare minimum lstm feature also design first provably optimal dynamic oracle constituency parse run amortize of1 time compare ofn3 oracles standard dependency parse train oracle achieve best f1 score english french parser use reranking external data
neural network attention prove effective many natural language process task paper develop attention mechanisms uncertainty detection particular generalize standardly use attention mechanisms introduce external attention sequence preserve attention novel architectures differ standard approach use external resources compute attention weight preserve sequence information compare configurations along different dimension attention novel architectures set new state art wikipedia benchmark dataset perform similar state art model biomedical benchmark use large set linguistic feature
paper introduce new unsupervised approach dialogue act induction give sequence dialogue utterances task assign label represent function dialogue utterances represent real value vectors encode mean model dialogue hide markov model emission probabilities estimate gaussian mixtures use gibbs sample posterior inference present result standard switchboard damsl corpus algorithm achieve promise result compare strong supervise baselines outperform unsupervised algorithms
isizulu verb know morphological complexity subject go linguistics research well prospect computational use control natural language interfaces machine translation spellcheckers end seek answer question precise grammar rule isizulu complex verb extension bantu verb morphology end iteratively specify grammar context free grammar evaluate computationally grammar present paper cover subject object concord negation present tense aspect mood causative applicative stative reciprocal verbal extensions politeness wh question modifiers aspect double ensure correct order appear verbs grammar conform specification
purpose computational dialectology geographically bind text analysis task texts must annotate author location many texts locatable explicit label explicit annotation place paper describe series experiment determine positionally annotate microblog post use learn location indicate word use locate blog texts author gaussian distribution use model locational qualities word introduce notion placeness describe locational word find model word distributions account several locations thus several gaussian distributions per word define filter pick word high placeness base local distributional context aggregate locational information centroid text give useful result result apply data swedish language
people personality motivations manifest everyday language usage emergence social media ample examples usage procurable paper aim analyze vocabulary use close two hundred thousand blogger users yous purpose geographically portray various demographic linguistic psychological dimension state level give description web base tool view map depict various characteristics social media users derive large blog dataset two billion word
present feature vector formation technique document sparse composite document vector scdv overcome several shortcomings current distributional paragraph vector representations widely use text representation scdv word embed cluster capture multiple semantic contexts word occur chain together form document topic vectors express complex multi topic document extensive experiment multi class multi label classification task outperform previous state art method ntsg liu et al 2015a also show scdv embed perform well heterogeneous task like topic coherence context sensitive learn information retrieval moreover achieve significant reduction train prediction time compare representation methods scdv achieve best worlds better performance lower time space complexity
review score prediction text review recently gain lot attention recommendation systems major problem model review score prediction presence noise due user bias review score propose two simple statistical methods remove noise improve review score prediction compare methods use multiple classifiers one user model use single global classifier predict review score empirically evaluate methods two major categories textitelectronics textitmovies tv snap publish amazon e commerce review data set amazon textitfine food review data set obtain improve review score prediction three commonly use text feature representations
neural machine translation nmt new approach automatic translation text one human language another basic concept nmt train large neural network maximize translation performance give parallel corpus nmt gain popularity research community outperform traditional smt approach several translation task wmt evaluation task benchmarks least language pair however many enhancements smt years incorporate nmt framework paper focus one enhancement namely domain adaptation propose approach adapt nmt system new domain main idea behind domain adaptation availability large domain train data small domain train data report significant gain propose method automatic metrics human subjective evaluation metric two language pair adaptation method show large improvement new domain performance general domain degrade slightly addition approach fast enough adapt already train system new domain within hours without need retrain nmt model combine data usually take several days weeks depend volume data
paper propose carefully evaluate sequence label framework solely utilize sparse indicator feature derive dense distribute word representations propose model obtain near state art performance part speech tag name entity recognition variety languages model rely thousand sparse cod derive feature without apply modification word representations employ different task propose model favorable generalization properties retain eight hundred and ninety-eight average pos tag accuracy train twelve total available train data ie150 sentence per language
topic model successfully apply lexicon extraction however previous methods limit document align data paper try address two challenge apply topic model lexicon extraction non parallel data one hard model word relationship two noisy seed dictionary solve two challenge propose two new bilingual topic model better capture semantic information word discriminate multiple translations noisy seed dictionary extend scope topic model invert roles word document addition solve problem noise seed dictionary incorporate probability translation selection model moreover also propose effective measure evaluate similarity word different languages select optimal translation pair experimental result use real world data demonstrate utility efficacy propose model
exist model multilingual natural language process nlp treat language discrete category make predictions either one language contrast propose use continuous vector representations language show learn efficiently character base neural language model use improve inference language varieties see train experiment one thousand, three hundred and three bible translations nine hundred and ninety different languages empirically explore capacity multilingual language model also show language vectors capture genetic relationships languages
paper address two different type noise information extraction model noise distant supervision noise pipeline input feature target task entity type relation extraction first noise type introduce multi instance multi label learn algorithms use neural network model apply fine grain entity type first time give model comparable performance state art supervise approach use global embeddings entities second noise type propose ways improve integration noisy entity type predictions relation extraction experiment show probabilistic predictions robust discrete predictions joint train two task perform best
work describe conditional random field crf base system part speech pos tag code mix indian social media text part participation tool contest pos tag codemixed indian social media text hold conjunction two thousand and sixteen international conference natural language process iitbhu india participate constrain mode contest three language pair bengali english hindi english telegu english system achieve overall average f1 score seven thousand, nine hundred and ninety-nine highest overall average f1 score among sixteen systems participate constrain mode contest
pre dominant approach language model date base recurrent neural network success task often link ability capture unbounded context paper develop finite context approach stack convolutions efficient since allow parallelization sequential tokens propose novel simplify gate mechanism outperform oord et al two thousand and sixteen investigate impact key architectural decisions propose approach achieve state art wikitext one hundred and three benchmark even though feature long term dependencies well competitive result google billion word benchmark model reduce latency score sentence order magnitude compare recurrent baseline knowledge first time non recurrent approach competitive strong recurrent model large scale language task
work describe system detect paraphrase indian languages part participation share task detect paraphrase indian languages dpil organize forum information retrieval evaluation fire two thousand and sixteen paraphrase detection method use multinomial logistic regression model train variety feature basically lexical semantic level similarities two sentence pair performance system evaluate test set release fire two thousand and sixteen share task dpil system achieve highest f measure ninety-five task1 punjabi languagethe performance system task1 hindi language f measure ninety eleven team participate share task four team participate four languages hindi punjabi malayalam tamil remain seven team participate one four languages also participate task1 task2 four indian languages overall average performance system include task1 task2 overall four languages f1 score eighty-one second highest score among four systems participate four languages
neural network successfully apply many natural language process task come cost interpretability paper propose general methodology analyze interpret decisions neural model observe effect model erase various part representation input word vector dimension intermediate hide units input word present several approach analyze effect erasure compute relative difference evaluation metrics use reinforcement learn erase minimum set input word order flip neural model decision comprehensive analysis multiple nlp task include linguistic feature classification sentence level sentiment analysis document level sentiment aspect prediction show propose methodology offer clear explanations neural model decisions also provide way conduct error analysis neural model
develop model extract relevant feature automatic text summarization investigate performance different model duc two thousand and one dataset two different model develop one ridge regressor one multi layer perceptron hyperparameters vary performance note segregate summarization task two main step first sentence rank second step sentence selection first step give document sort sentence base importance second step order obtain non redundant sentence weed sentence high similarity previously select sentence
headline generation speak content important since speak content difficult show screen browse user special type abstractive summarization summaries generate word word scratch without use part original content many deep learn approach headline generation text document propose recently require huge quantities train data difficult speak document summarization paper propose asr error model approach learn underlie structure asr error pattern incorporate model attentive recurrent neural network arnn architecture way model abstractive headline generation speak content learn abundant text data asr data recognizers experiment show encourage result verify propose asr error model work well even input speak content recognize recognizer different one model learn
arabic widely speak language rich long history span fourteen centuries yet exist arabic corpora largely focus modern period lack sufficient diachronic information develop large scale historical corpus arabic one billion word diverse periods time clean corpus process morphological analyzer enhance detect parallel passages automatically date undated texts demonstrate utility select case study show application digital humanities
one major goals automate argumentation mine uncover argument structure present argumentative text order determine structure one must understand different individual components overall argument link general consensus field dictate argument components form hierarchy persuasion manifest tree structure work provide first neural network base approach argumentation mine focus two task extract link argument components classify type argument components order solve problem propose use joint model base pointer network architecture pointer network appeal task follow reason one take account sequential nature argument components two construction enforce certain properties tree structure present argument relations three hide representations apply auxiliary task order extend contribution original pointer network model construct joint model simultaneously attempt learn type argument component well continue predict link argument components propose joint model achieve state art result two separate evaluation corpora achieve far superior performance regular pointer network model result show optimize task add fully connect layer prior recurrent neural network input crucial high performance
paper present novel neural network algorithm conduct semi supervise learn sequence label task arrange linguistically motivate hierarchy relationship exploit regularise representations supervise task backpropagating error unsupervised task supervise task introduce neural network lower layer supervise junior downstream task final layer task auxiliary unsupervised task architecture show improvements two percentage point f1 chunk compare plausible baseline
suggest compositional vector representation parse tree rely recursive combination recurrent neural network encoders demonstrate effectiveness use representation backbone greedy bottom dependency parser achieve state art accuracies english chinese without rely external word embeddings parser implementation available download first author webpage
name entity recognition information extraction task frequently use linguistic feature part speech tag chunk languages word boundaries readily identify text word segmentation key first step generate feature ner system use word boundary tag feature helpful signal aid identify boundaries may provide richer information ner system new state art word segmentation systems use neural model learn representations predict word boundaries show representations jointly train ner system yield significant improvements ner chinese social media experiment jointly train ner word segmentation lstm crf model yield nearly five absolute improvement previously publish result
exist knowledge base question answer systems often rely small annotate train data shallow methods like relation extraction robust data scarcity less expressive deep mean representation methods like semantic parse thereby fail answer question involve multiple constraints alleviate problem empower relation extraction method additional evidence wikipedia first present neural network base relation extractor retrieve candidate answer freebase infer wikipedia validate answer experiment webquestions question answer dataset show method achieve f1 five hundred and thirty-three substantial improvement state art
introduce novel simple convolution neural network cnn architecture multi group norm constraint cnn mgnc cnn capitalize multiple set word embeddings sentence classification mgnc cnn extract feature input embed set independently join penultimate layer network form final feature vector adopt group regularization strategy differentially penalize weight associate subcomponents generate respective embed set model much simpler comparable alternative architectures require substantially less train time furthermore flexible require input word embeddings dimensionality show mgnc cnn consistently outperform baseline model
among zellig harris numerous contributions linguistics theory sublanguages science probably rank among underrate however theory lead exhaustive meaningful applications study grammar immunology language change time also illustrate nature mathematical relations chunk subsets grammar language whole become clear deal connection metalanguage language well reflect operators paper try justify claim sublanguages science stand particular algebraic relation rest language embed namely right ideals ring
move limit domain natural language generation nlg open domain difficult number semantic input combinations grow exponentially number domains therefore important leverage exist resources exploit similarities domains facilitate domain adaptation paper propose procedure train multi domain recurrent neural network base rnn language generators via multiple adaptation step procedure model first train counterfeit data synthesise domain dataset fine tune small set domain utterances discriminative objective function corpus base evaluation result show propose procedure achieve competitive performance term bleu score slot error rate significantly reduce data need train generators new unseen domains subjective test human judge confirm procedure greatly improve generator performance small amount data available domain
automatic event schema induction aesi mean extract meta event raw text word find type templates event may exist raw text roles slot may exist event type paper propose joint entity drive model learn templates slot simultaneously base constraints templates slot sentence addition entities semantic information also consider inner connectivity entities borrow normalize cut criteria image segmentation divide entities accurate template cluster slot cluster experiment show model gain relatively higher result previous work
state art name entity recognition systems rely heavily hand craft feature domain specific knowledge order learn effectively small supervise train corpora available paper introduce two new neural architectures one base bidirectional lstms conditional random field construct label segment use transition base approach inspire shift reduce parsers model rely two source information word character base word representations learn supervise corpus unsupervised word representations learn unannotated corpora model obtain state art performance ner four languages without resort language specific knowledge resources gazetteers
propose bayesian model unsupervised semantic role induction multiple languages use explore usefulness parallel corpora task joint bayesian model consist individual model language plus additional latent variables capture alignments roles across languages generative bayesian model evaluations variety scenarios vary inference procedure without change model thereby compare scenarios directly compare use monolingual data use parallel corpus use parallel corpus annotations language use small amount annotation target language find biggest impact add parallel corpus train actually increase mono lingual data alignments another language result small improvements even label data language
article develop algorithm detect parallel texts masoretic text hebrew bible result present online chapters hebrew bible contain parallel passages inspect synoptically differences parallel passages highlight similar way mt isaiah present synoptically 1qisaa also investigate one investigate degree similarity parallel passages help case study two kings nineteen twenty-five parallel isaiah jeremiah two chronicle
several large cloze style context question answer datasets introduce recently cnn daily mail news data children book test thank size datasets associate text comprehension task well suit deep learn techniques currently seem outperform alternative approach present new simple model use attention directly pick answer context oppose compute answer use blend representation word document usual similar model make model particularly suitable question answer problems answer single word document ensemble model set new state art evaluate datasets
semantic nlp applications often rely dependency tree recognize major elements proposition structure sentence yet much semantic structure indeed express syntax many phenomena easily read dependency tree often lead ad hoc heuristic post process information loss directly address need semantic applications present prop output representation design explicitly uniformly express much proposition structure imply syntax associate tool extract dependency tree
goal research extract large list table name entities relations specific domain small set handful instance relations require input user system exploit summaries google search engine source text instance use extract pattern output set new entities relations result four experiment show precision recall vary accord relation type precision range sixty-one seventy-five recall range seventy-one eighty-three best result obtain player club relationship seventy-two eighty-three precision recall respectively
large organisation public private monitor media information keep abreast developments field interest usually also become aware positive negative opinions express towards least write media computer program become efficient help human analysts significantly monitor task gather media report analyse detect trend case even issue early warn make predictions likely future developments present trend recognition relate functionality europe media monitor emm system develop european commission joint research centre jrc public administrations european union eu beyond emm perform large scale media analysis seventy languages recognise various type trend combine information news article write different languages social media post emm also let us users explore huge amount multilingual media data interactive map graph allow examine data various view point accord multiple criteria lot emm functionality accessibly freely internet via apps hand hold devices
settings unlabelled speech data available speech technology need develop without transcriptions pronunciation dictionaries language model text similar problem face model infant language acquisition case categorical linguistic structure need discover directly speech audio present novel unsupervised bayesian model segment unlabelled speech cluster segment hypothesize word group result complete unsupervised tokenization input speech term discover word type approach potential word segment arbitrary length embed fix dimensional acoustic vector space model implement gibbs sampler build whole word acoustic model space jointly perform segmentation report word error rat small vocabulary connect digit recognition task map unsupervised decode output grind truth transcriptions model achieve around twenty error rate outperform previous hmm base system ten absolute moreover contrast baseline model require pre specify vocabulary size
study analyze corpus eight million word academic literature computational lingustics academic literature lexical bundle corpus categorize base structure function
describe challenge advantage unique coreference resolution biomedical domain sieve base architecture leverage domain knowledge entity event coreference resolution domain general coreference resolution algorithms perform poorly biomedical document cue rely gender largely absent domain encode domain specific knowledge number type participants require chemical reactions moreover difficult directly encode knowledge coreference resolution algorithms rule base rule base architecture use sequentially apply hand design sieve output sieve inform constrain subsequent sieve architecture provide thirty-two increase throughput reach event extraction system precision parallel stricter system rely solely syntactic pattern extraction
adapt greedy stack lstm dependency parser dyer et al two thousand and fifteen support train exploration procedure use dynamic oraclesgoldberg nivre two thousand and thirteen instead cross entropy minimization form train account model predictions train time rather assume error free action history improve parse accuracies english chinese obtain strong result languages discuss modifications need order get train exploration work well probabilistic neural network
humans comprehend mean relations discourse heavily rely semantic memory encode general knowledge concepts facts inspire propose neural recognizer implicit discourse relation analysis build upon semantic memory store knowledge distribute fashion refer recognizer semder start word embeddings discourse arguments semder employ shallow encoder generate distribute surface representation discourse semantic encoder attention semantic memory matrix establish surface representations able retrieve deep semantic mean representation discourse memory use surface semantic representations input semder finally predict implicit discourse relations via neural recognizer experiment benchmark data set show semder benefit semantic memory achieve substantial improvements two hundred and fifty-six average current state art baselines term f1 score
implicit discourse relation recognition crucial component automatic discourselevel analysis nature language understand previous study exploit discriminative model build either powerful manual feature deep discourse representations paper instead explore generative model propose variational neural discourse relation recognizer refer model varndrr varndrr establish direct probabilistic model latent continuous variable generate discourse relation two arguments discourse order perform efficient inference learn introduce neural discourse relation model approximate prior posterior distributions latent variable employ approximate distributions optimize reparameterized variational lower bind allow varndrr train standard stochastic gradient methods experiment benchmark data set show varndrr achieve comparable result stateof art baselines without use manual feature
contribution special issue computer aid process intertextuality ancient texts illustrate use digital tool interact hebrew bible offer new promise perspectives visualize texts perform task education research contribution explore corpus hebrew bible create maintain eep talstra centre bible computer support new methods modern knowledge workers within field digital humanities theology apply ancient texts envision new field digital intertextuality article first describe corpus use develop bible online learner persuasive technology enhance language learn around database act engine drive interactive task learners intertextuality case matter active exploration ongoing practice furthermore interactive corpus technology important bear task textual criticism specialize area research depend increasingly availability digital resources commercial solutions develop software company like logos accordance offer market base intertextuality define production advance digital resources scholars students useful alternatives often inaccessible expensive print versions reasonable expect future interactive corpus technology allow scholars innovative academic task textual criticism interpretation already see emergence promise tool text categorization analysis translation shift interpretation broadly speak interactive tool task within three areas language learn textual criticism biblical study illustrate new kind intertextuality emerge within digital humanities
present simple effective scheme dependency parse base bidirectional lstms bilstms sentence token associate bilstm vector represent token sentential context feature vectors construct concatenate bilstm vectors bilstm train jointly parser objective result effective feature extractors parse demonstrate effectiveness approach apply greedy transition base parser well globally optimize graph base parser result parsers simple architectures match surpass state art accuracies english chinese
propose mvcnn convolution neural network cnn architecture sentence classification combine diverse versions pretrained word embeddings ii extract feature multigranular phrase variable size convolution filter also show pretraining mvcnn critical good performance mvcnn achieve state art performance four task small scale binary small scale multi class largescale twitter sentiment prediction subjectivity classification
propose new algorithm topic model vec2topic identify main topics corpus use semantic information capture via high dimensional distribute word embeddings technique unsupervised generate list topics rank respect importance find work better exist topic model techniques latent dirichlet allocation identify key topics user generate content email chat etc topics diffuse across corpus also find vec2topic work equally well non user generate content paper report etc small corpora single document
name entity disambiguation ned task link name entity mention instance knowledge base typically wikipedia task closely relate word sense disambiguation wsd supervise word expert approach prevail work present result word expert approach ned one classifier build target entity mention string resources necessary build system dictionary set train instance automatically derive wikipedia provide empirical evidence value approach well study differences wsd ned include ambiguity synonymy statistics
paper present novel approach recurrent neural network rnn regularization differently widely adopt dropout method apply textitforward connections fee forward architectures rnns propose drop neurons directly textitrecurrent connections way loss long term memory approach easy implement apply regular fee forward dropout demonstrate effectiveness long short term memory network popular type rnn cells experiment nlp benchmarks show consistent improvements even combine conventional fee forward dropout
address relation classification context slot fill task find evaluate fillers like steve job slot x x found apple propose convolutional neural network split input sentence three part accord relation arguments compare state art traditional approach relation classification finally combine different methods show combination better individual approach also analyze effect genre differences performance
several major proposals treat donkey anaphora discourse representation theory like e type theories like every one work well set specific examples use demonstrate validity approach show paper however generalisable account essentially problem remedy manifest examples propose another logical approach develoop logic extend recent propositional gradual logic show treat donkey anaphora generally also identify address problem around modern convention existential import furthermore show aristotle syllogisms conversion realisable logic
readability define read level speech grade one grade twelve result use reap readability analysis vocabulary collins thompson callan two thousand and four syntax heilman et al two thousand and six two thousand and seven use lexical content grammatical structure sentence document predict read level analysis result group average readability candidate evolution candidate speeches readability time standard deviation much candidate vary speech one venue another comparison one speech four past presidents gettysburg address also analyze
propose new method evaluate readability simplify sentence pair wise rank validity method establish corpus cross corpus evaluation experiment approach correctly identify rank simplify unsimplified sentence term read level accuracy eighty significantly outperform previous result gain qualitative insights nature simplification sentence level study impact specific linguistic feature empirically confirm word level syntactic feature play role compare degree simplification authentic data carry research create new sentence align corpus professionally simplify news article new corpus resource enrich empirical basis sentence level simplification research far rely single resource importantly facilitate cross corpus evaluation simplification key step towards generalizable result
tree structure neural network exploit valuable syntactic parse information interpret mean sentence however suffer two key technical problems make slow unwieldy large scale nlp task usually operate parse sentence directly support batch computation address issue introduce stack augment parser interpreter neural network spinn combine parse interpretation within single tree sequence hybrid model integrate tree structure sentence interpretation linear sequential structure shift reduce parser model support batch computation speedup twenty-five time tree structure model integrate parser operate unparsed data little loss accuracy evaluate stanford nli entailment task show significantly outperform sentence encode model
present novel method jointly learn compositional non compositional phrase embeddings adaptively weight type embeddings use compositionality score function score function use quantify level compositionality phrase parameters function jointly optimize objective learn phrase embeddings experiment apply adaptive joint learn method task learn embeddings transitive verb phrase show compositionality score strong correlation human rat verb object compositionality substantially outperform previous state art moreover embeddings improve upon previous best model transitive verb disambiguation task also show simple ensemble technique improve result task
exist neural machine translation nmt model focus conversion sequential data directly use syntactic information propose novel end end syntactic nmt model extend sequence sequence model source side phrase structure model attention mechanism enable decoder generate translate word softly align phrase well word source sentence experimental result wat fifteen english japanese dataset demonstrate propose model considerably outperform sequence sequence attentional nmt model compare favorably state art tree string smt system
detect hypernymy relations key task nlp address literature use two complementary approach distributional methods whose supervise variants current best performers path base methods receive less research attention suggest improve path base algorithm dependency paths encode use recurrent neural network achieve result comparable distributional methods extend approach integrate path base distributional signal significantly improve upon state art task
present persona base model handle issue speaker consistency neural response generation speaker model encode personas distribute embeddings capture individual characteristics background information speak style dyadic speaker addressee model capture properties interactions two interlocutors model yield qualitative performance improvements perplexity bleu score baseline sequence sequence model similar gain speaker consistency measure human judge
study use greedy feature selection methods morphosyntactic tag number different condition compare static order feature dynamic order base mutual information statistics apply techniques standalone taggers well joint systems tag parse experiment five languages show feature selection result compact model well higher accuracy condition also dynamic order work better static order joint systems benefit standalone taggers also show techniques use select morphosyntactic categories predict order maximize syntactic accuracy joint system final result represent substantial improvement state art several languages time reduce number feature run time eighty case
traditional syntax model typically leverage part speech pos information construct feature hand tune templates demonstrate better approach utilize pos tag regularizer learn representations propose simple method learn stack pipeline model call stack propagation apply dependency parse tag use hide layer tagger network representation input tokens parser test time parser require predict pos tag nineteen languages universal dependencies method thirteen absolute accurate state art graph base approach twenty-seven accurate comparable greedy model
determine intend sense word text word sense disambiguation wsd long stand problem natural language process recently researchers show promise result use word vectors extract neural network language model feature wsd algorithms however simple average concatenation word vectors word text lose sequential syntactic information text paper study wsd sequence learn neural net lstm better capture sequential syntactic pattern text alleviate lack train data word wsd employ lstm semi supervise label propagation classifier demonstrate state art result especially verbs
traditional approach extractive summarization rely heavily human engineer feature work propose data drive approach base neural network continuous sentence feature develop general framework single document summarization compose hierarchical document encoder attention base extractor architecture allow us develop different class summarization model extract sentence word train model large scale corpora contain hundreds thousands document summary pair experimental result two summarization datasets demonstrate model obtain result comparable state art without access linguistic annotation
semantic textual similarity sts systems design encode evaluate semantic similarity word phrase sentence document one method assess quality authenticity semantic information encode systems comparison human judgments data set evaluate semantic model develop consist seven hundred and seventy-five english word sentence pair annotate semantic relatedness human raters engage maximum difference scale mds task well faster alternative task sample application relatedness data behavior base relatedness compare relatedness compute via four shelf sts model n gram latent semantic analysis lsa word2vec umbc ebiquity sts model capture much variance human judgments collect sensitive implicatures entailments process consider participants text stimuli judgment data make freely available
recent work exhibit distribute word representations good capture linguistic regularities language allow vector orient reason base simple linear algebra word since many different methods propose learn document representations natural ask whether also linear structure learn representations allow similar reason document level answer question design new document analogy task test semantic regularities document representations conduct empirical evaluations several state art document representation model result reveal neural embed base document representations work better analogy task conventional methods provide preliminary explanations observations
work examine impact cross linguistic transfer grammatical errors english second language esl texts use computational framework formalize theory contrastive analysis ca demonstrate language specific error distributions esl write predict typological properties native language relation typology english typology drive model enable obtain accurate estimate distributions without access esl data target languages furthermore present strategy adjust method low resource languages lack typological documentation use bootstrapping approach approximate native language typology esl texts finally show framework instrumental linguistic inquiry seek identify first language factor contribute wide range difficulties second language acquisition
paper propose model learn word embeddings weight contexts base part speech pos relevance weight pos fundamental element natural language however state art word embed model fail consider paper propose use position dependent pos relevance weight matrices model inherent syntactic relationship among word within context window utilize pos relevance weight model word context pair word embed train process model propose paper paper jointly optimize word vectors pos relevance matrices experiment conduct popular word analogy word similarity task demonstrate effectiveness propose method
paper introduce neural model concept text generation scale large rich domains experiment new dataset biographies wikipedia order magnitude larger exist resources 700k sample dataset also vastly diverse 400k vocabulary compare hundred word weathergov robocup model build upon recent work conditional neural language model text generation deal large vocabulary extend model mix fix vocabulary copy action transfer sample specific word input database generate output sentence neural model significantly perform classical kneser ney language model adapt task nearly fifteen bleu
successful information extraction systems operate access large collection document work explore task acquire incorporate external evidence improve extraction accuracy domains amount train data scarce process entail issue search query extraction new source reconciliation extract value repeat sufficient evidence collect approach problem use reinforcement learn framework model learn select optimal action base contextual information employ deep q network train optimize reward function reflect extraction accuracy penalize extra effort experiment two databases shoot incidents food adulteration case demonstrate system significantly outperform traditional extractors competitive meta classifier baseline
paper present comparison classification methods linguistic typology purpose expand extensive sparse language resource world atlas language structure wals dryer haspelmath two thousand and thirteen experiment variety regression nearest neighbor methods use classification set three hundred and twenty-five languages six syntactic rule draw wals classify rule consider typological feature five rule linguistic feature extract word align bible language genealogical feature genus family language general find propagate majority label among languages genus achieve best accuracy label pre diction follow logistic regression model combine typological linguistic feature offer next best performance interestingly model actually outperform majority label among languages family
paper attempt solve problem prepositional phrase pp attachments english motivation work come nlp applications like machine translation get correct attachment prepositions crucial idea correct pp attachments sentence help alignments parallel data another language novelty work lie formulation problem dual decomposition base algorithm enforce agreement parse tree two languages constraint experiment perform english hindi language pair performance improve ten baseline baseline attachment predict mstparser model train english
paper claim vector cosine generally consider one efficient unsupervised measure identify word similarity vector space model outperform completely unsupervised measure evaluate extent intersection among associate contexts two target word weight intersection accord rank share contexts dependency rank list claim come hypothesis similar word simply occur similar contexts share larger portion relevant contexts compare relate word prove describe evaluate apsyn variant average precision independently adopt parameters outperform vector cosine co occurrence esl toefl test set best set apsyn reach seventy-three accuracy esl dataset seventy accuracy toefl dataset beat therefore non english us college applicants whose average report literature six thousand, four hundred and fifty several state art approach
root9 supervise system classification hypernyms co hyponyms random word derive already introduce root13 santus et al two thousand and sixteen rely random forest algorithm nine unsupervised corpus base feature evaluate ten fold cross validation nine thousand, six hundred pair equally distribute among three class involve several part speech ie adjectives nouns verbs class present root9 achieve f1 score nine hundred and seven baseline five hundred and seventy-two vector cosine classification binary root9 achieve follow result baseline hypernyms co hyponyms nine hundred and fifty-seven vs six hundred and ninety-eight hypernyms random nine hundred and eighteen vs six hundred and forty-one co hyponyms random nine hundred and seventy-eight vs seven hundred and ninety-four order compare performance state art also evaluate root9 subsets weed et al two thousand and fourteen datasets prove fact competitive finally investigate whether system learn semantic relation simply learn prototypical hypernyms claim levy et al two thousand and fifteen second possibility seem likely even though root9 train negative examples ie switch hypernyms drastically reduce bias
paper describe root13 supervise system classification hypernyms co hyponyms random word system rely random forest algorithm thirteen unsupervised corpus base feature evaluate ten fold cross validation nine thousand, six hundred pair equally distribute among three class involve several part speech ie adjectives nouns verbs class present root13 achieve f1 score eight hundred and eighty-three baseline five hundred and seventy-six vector cosine classification binary root13 achieve follow result hypernyms co hyponyms nine hundred and thirty-four vs six hundred and two hypernymsrandom nine hundred and twenty-three vs six hundred and fifty-five co hyponyms random nine hundred and seventy-three vs eight hundred and fifteen result competitive stateof art model
article implementation compilation technique use raw feldspar complete rewrite feldspar embed domain specific language edsl axelsson et al two thousand and ten feldspar high level functional language generate efficient c code run embed target gist technique present post follow rather write back end convert pure feldspar expressions directly c translate low level monadic edsl low level edsl c code generate approach several advantage one translation simpler write complete c back end two translation two type edsls rule many potential errors three low level edsl reusable share several high level edsls although article contain lot code fact reusable mention discussion write implementation less fifty line code use generic libraries develop support feldspar
corpora web texts become rich language learn resource mean assess whether linguistically appropriate learners give proficiency level paper aim address issue present first approach predict linguistic complexity swedish second language learn material five point scale show traditional swedish readability measure lasbarhetsindex lix suitable task propose supervise machine learn model base range linguistic feature reliably classify texts accord difficulty level model obtain accuracy eight hundred and thirteen f score eight comparable state art english considerably higher previously report result languages study utility feature single sentence instead full texts since sentence common linguistic unit language learn exercise train separate model sentence level data five class yield six hundred and thirty-four accuracy although lower document level performance achieve adjacent accuracy ninety-two furthermore find use combination different feature compare use lexical feature alone result seven improvement classification accuracy sentence level whereas document level lexical feature dominant model intend use freely accessible web base language learn platform automatic generation exercise
understand unstructured text major goal within natural language process comprehension test pose question base short text passages evaluate understand work investigate machine comprehension challenge mctest benchmark partly limit size prior work mctest focus mainly engineer better feature tackle dataset neural approach harness simple neural network arrange parallel hierarchy parallel hierarchy enable model compare passage question answer variety trainable perspectives oppose use manually design rigid feature set perspectives range word level sentence fragment sequence sentence network operate word embed representations text train methodology design help cope limit train data parallel hierarchical model set new state art mctest outperform previous feature engineer approach slightly previous neural approach significant margin fifteen absolute
present discriminative model single document summarization integrally combine compression anaphoricity constraints model select textual units include summary base rich set sparse feature whose weight learn large corpus allow deletion content within sentence deletion license compression rule framework implement dependencies subsentential units text anaphoricity constraints improve cross sentence coherence guarantee pronoun include summary pronoun antecedent include well pronoun rewrite full mention train end end final system outperform prior work rouge well human judgments linguistic quality
paper claim vector cosine generally consider among efficient unsupervised measure identify word similarity vector space model outperform unsupervised measure calculate extent intersection among mutually dependent contexts target word prove describe evaluate apsyn variant average precision without optimization outperform vector cosine co occurrence standard esl test set improvement range nine hundred one thousand, seven hundred and ninety-eight depend number choose top contexts
paper present conversational model incorporate context participant role two party conversations different architectures explore integrate participant role context information long short term memory lstm language model conversational model function language model language generation model experiment ubuntu dialog corpus show model capture multiple turn interaction participants propose method outperform traditional lstm model measure language model perplexity response rank generate responses show characteristic differences two participant roles
recurrent neural network architectures combine attention mechanism neural attention model show promise performance recently task include speech recognition image caption generation visual question answer machine translation paper neural attention model apply two sequence classification task dialogue act detection key term extraction sequence label task model input sequence output label input sequence major difficulty sequence label input sequence long include many noisy irrelevant part information whole sequence treat equally noisy irrelevant part may degrade classification performance attention mechanism helpful sequence classification task capable highlight important part among entire sequence classification task experimental result show attention mechanism discernible improvements achieve sequence label task consider roles attention mechanism task analyze visualize paper
traditional language model treat language finite state automaton probability space word strong assumption model something inherently complex language paper challenge show linear chain assumption inherent previous work translate sequential composition tree propose new model marginalize possible composition tree thereby remove underlie structural assumptions partition function new model intractable use recently propose sentence level evaluation metric contrastive entropy evaluate model give new evaluation metric report one hundred improvement across distortion level current state art recurrent neural network base language model
goal paper use multi task learn efficiently scale slot fill model natural language understand handle multiple target task domains key scalability reduce amount train data need learn model new task propose multi task model deliver better performance less data leverage pattern learn task approach support open vocabulary allow model generalize unseen word particularly important little train data use newly collect crowd source data set cover four different domains use demonstrate effectiveness domain adaptation open vocabulary techniques
evaluation text summarization approach mostly base metrics measure similarities system generate summaries set human write gold standard summaries widely use metric summarization evaluation rouge family rouge solely rely lexical overlap term phrase sentence therefore case terminology variations paraphrase rouge effective scientific article summarization one case different general domain summarization eg newswire data provide extensive analysis rouge effectiveness evaluation metric scientific summarization show contrary common belief rouge much reliable evaluate scientific summaries furthermore show different variants rouge result different correlations manual pyramid score finally propose alternative metric summarization evaluation base content relevance system generate summary correspond human write summaries call metric sera summarization evaluation relevance analysis unlike rouge sera consistently achieve high correlations manual score show effectiveness evaluation scientific article summarization
despite interest use cross lingual knowledge learn word embeddings various task systematic comparison possible approach lack literature perform extensive evaluation four popular approach induce cross lingual embeddings require different form supervision four typographically different language pair evaluation setup span four different task include intrinsic evaluation mono lingual cross lingual similarity extrinsic evaluation downstream semantic syntactic applications show model require expensive cross lingual knowledge almost always perform better cheaply supervise model often prove competitive certain task
propose online unsupervised domain adaptation da perform incrementally data come applicable batch da possible part speech pos tag evaluation find online unsupervised da perform well batch da
work concern paraphrase identification task one hand contribute expand deep learn embeddings include continuous discontinuous linguistic phrase hand come new scheme tf kld knn learn discriminative weight word phrase specific paraphrase task weight sum embeddings represent sentence effectively base two innovations get competitive state art performance paraphrase identification
key challenge entity link make effective use contextual information disambiguate mention might refer different entities different contexts present model use convolutional neural network capture semantic correspondence mention context propose target entity convolutional network operate multiple granularities exploit various kinds topic information rich parameterization give capacity learn n grams characterize different topics combine network sparse linear model achieve state art performance multiple entity link datasets outperform prior systems durrett klein two thousand and fourteen nguyen et al two thousand and fourteen
paper present precursory yet novel approach question answer task use structural decomposition system first generate linguistic structure syntactic semantic tree text decompose multiple field index term field question decompose question multiple field measure relevance score field index ones rank document relevance score weight associate field weight learn statistical model final model give absolute improvement forty baseline approach use simple search detect document contain answer
paper propose convolutional neural network learn optimal representation question answer sentence main aspect use relational information give match word two members pair match encode embeddings additional parameters dimension tune network allow better capture interactions question answer result significant boost accuracy test model two widely use answer sentence selection benchmarks result clearly show effectiveness relational information allow relatively simple network approach state art
paper step outside comfort zone traditional nlp task like automatic speech recognition asr machine translation mt address two novel problems arise automate multilingual news monitor segmentation tv radio program asr transcripts individual stories cluster individual stories come various source languages storylines storyline cluster stories cover events essential task inquisitorial media monitor address two problems jointly engage low dimensional semantic representation capabilities sequence sequence neural translation model enable joint multi task learn multilingual neural translation morphologically rich languages replace attention mechanism slide window mechanism operate sequence sequence neural translation model character level rather word level story segmentation storyline cluster problem tackle examine low dimensional vectors produce side product neural translation process result paper describe novel approach automatic story segmentation storyline cluster problem
tree adjoin grammar tag specifically suit morph rich agglutinate languages like tamil due psycho linguistic feature parse time dependency morph resolution though tag ltag formalisms know three decades efforts design tag syntax tamil entirely successful due complexity specification rich morphology tamil language paper present minimalistic tag tamil without much morphological considerations also introduce parser implementation obvious variations xtag system
two extensions amr smatch score script present first extension com bin smatch score script c60 rule base classifier produce human readable report error pattern frequency observe score amr graph first extension result four gain state art camr baseline parser add manually craft wrapper fix identify camr parser errors second extension combine per sentence smatch en semble method select best amr graph among set amr graph sentence second modification au tomatically yield four gain ap ply output two nondeterministic amr parsers camrwrapper parser novel character level neural translation amr parser amr parse task character level neural translation attain surprise seven gain carefully optimize word level neural translation overall achieve smatch f162 semeval two thousand and sixteen official scor ing set f167 ldc2015e86 test set
currently successful approach computational semantics represent word embeddings machine learn vector space present ensemble method combine embeddings produce glove pennington et al two thousand and fourteen word2vec mikolov et al two thousand and thirteen structure knowledge semantic network conceptnet speer havasi two thousand and twelve ppdb ganitkevitch et al two thousand and thirteen merge information common representation large multilingual vocabulary embeddings produce achieve state art performance many word similarity evaluations score rho five hundred and ninety-six evaluation rare word luong et al two thousand and thirteen sixteen higher previous best know system
recently neural model propose headline generation learn map document headline recurrent neural network nevertheless traditional neural network utilize maximum likelihood estimation parameter optimization essentially constrain expect train objective within word level rather sentence level moreover performance model prediction significantly rely train data distribution overcome drawbacks employ minimum risk train strategy paper directly optimize model parameters sentence level respect evaluation metrics lead significant improvements headline generation experiment result show model outperform state art systems english chinese headline generation task
encoder decoder framework neural machine translation nmt show effective large data scenarios much less effective low resource languages present transfer learn method significantly improve bleu score across range low resource languages key idea first train high resource language pair parent model transfer learn parameters low resource pair child model initialize constrain train use transfer learn method improve baseline nmt model average fifty-six bleu four low resource language pair ensembling unknown word replacement add another two bleu bring nmt performance low resource machine translation close strong syntax base machine translation sbmt system exceed performance one language pair additionally use transfer learn model score improve sbmt system average thirteen bleu improve state art low resource machine translation
paper present novel approach perform sentiment analysis news videos base fusion audio textual visual clue extract content propose approach aim contribute semiodiscoursive study regard construction ethos identity media universe become central part modern day live millions people achieve goal apply state art computational methods one automatic emotion recognition facial expressions two extraction modulations participants speeches three sentiment analysis close caption associate videos interest specifically compute feature visual intensities recognize emotions field size participants voice probability sound loudness speech fundamental frequencies sentiment score polarities text sentence close caption experimental result dataset contain five hundred and twenty annotate news videos three brazilian one american popular tv newscasts show approach achieve accuracy eighty-four sentiments tension level classification task thus demonstrate high potential use media analysts several applications especially journalistic domain
person knowledge extraction foundation tibetan knowledge graph construction provide support tibetan question answer system information retrieval information extraction research promote national unity social stability paper propose svm template base approach tibetan person knowledge extraction construct train corpus build templates base shallow parse analysis tibetan syntactic semantic feature verbs use train corpus design hierarchical svm classifier realize entity knowledge extraction finally experimental result prove method greater improvement tibetan person knowledge extraction
small grow body research statistical script model event sequence allow probabilistic inference implicit events document systems operate structure verb argument events produce nlp pipeline compare systems recent recurrent neural net model directly operate raw tokens predict sentence find latter roughly comparable former term predict miss events document
compel evidence coreference prediction would benefit model global information entity cluster yet state art performance achieve systems treat mention prediction independently attribute inherent difficulty craft informative cluster level feature instead propose use recurrent neural network rnns learn latent global representations entity cluster directly mention show representations especially useful prediction pronominal mention incorporate end end coreference system outperform state art without require additional search
study problem shallow parse hindi english code mix social media text csmt address annotate data develop language identifier normalizer part speech tagger shallow parser best knowledge first attempt shallow parse csmt pipeline develop make available research community goal enable better text analysis hindi english csmt pipeline accessible http bitly csmt parser api
introduce new approach disfluency detection use bidirectional long short term memory neural network blstm addition word sequence model take input pattern match feature develop reduce sensitivity vocabulary size train lead improve performance word sequence alone blstm take advantage explicit repair state addition standard reparandum state final output leverage integer linear program incorporate constraints disfluency structure experiment switchboard corpus model achieve state art performance standard disfluency detection task correction detection task analysis show model better detection non repetition disfluencies tend much harder detect
show eye track corpora use improve sentence compression model present novel multi task learn algorithm base multi layer lstms obtain performance competitive better state art approach
demonstrate attention base encoder decoder model use sentence level grammatical error identification automate evaluation scientific write aesw share task two thousand and sixteen attention base encoder decoder model use generation corrections addition error identification interest certain end user applications show character base encoder decoder model particularly effective outperform result aesw share task show gain word base counterpart final model combination three character base encoder decoder model one word base encoder decoder model sentence level cnn highest perform system aesw two thousand and sixteen binary prediction share task
paper describe experimental approach detection minimal semantic units mean dimsum explore within framework semeval two thousand and sixteen task ten approach primarily base combination word embeddings parserbased feature employ unidirectional incremental computation compositional embeddings multiword expressions
address problem automatically find parameters statistical machine translation system maximize bleu score ensure decode speed exceed minimum value propose use bayesian optimization efficiently tune speed relate decode parameters easily incorporate speed noisy constraint function obtain parameter value guarantee satisfy speed constraint associate confidence margin across three language pair two speed constraint value report overall optimization time reduction compare grid random search also show bayesian optimization decouple speed bleu measurements result reduction overall optimization time speed measure small subset sentence
present experience apply distributional semantics neural word embeddings problem represent cluster document bilingual comparable corpus data collection russian ukrainian academic texts topics academic field order build language independent semantic representations document train neural distributional model monolingual corpora learn optimal linear transformation vectors one language another result vectors use produce semantic fingerprint document serve input cluster algorithm present method compare several baselines include orthographic translation levenshtein edit distance outperform large margin also show language independent semantic fingerprint superior multi lingual cluster algorithms propose previous work time require less linguistic resources
many natural language process nlp task generalize segmentation problem paper combine semi crf neural network solve nlp segmentation task model represent segment compose input units embed entire segment thoroughly study different composition function different segment embeddings conduct extensive experiment two typical segmentation task name entity recognition ner chinese word segmentation cws experimental result show neural semi crf model benefit represent entire segment achieve state art performance cws benchmark dataset competitive result conll03 dataset
recent work use artificial neural network base distribute word representation greatly boost performance various natural language process task especially answer selection problem nevertheless previous work use deep learn methods like lstm rnn cnn etc capture semantic representation sentence separately without consider interdependence paper propose novel end end learn framework constitute deep convolutional neural network base multi modal similarity metric learn m2s net pairwise tokens propose model demonstrate performance surpass previous state art systems answer selection benchmark ie trec qa dataset map mrr metrics
work propose novel attention base neural network model task fine grain entity type classification unlike previously propose model recursively compose representations entity mention contexts model achieve state art performance seven thousand, four hundred and ninety-four loose micro f1 score well establish figer dataset relative improvement two hundred and fifty-nine also investigate behavior attention mechanism model observe learn contextual linguistic expressions indicate fine grain category memberships entity
bidirectional long short term memory bi lstm network recently prove successful various nlp sequence model task little know reliance input representations target languages data set size label noise address issue evaluate bi lstms word character unicode byte embeddings pos tag compare bi lstms traditional pos taggers across languages data size also present novel bi lstm model combine pos tag loss function auxiliary loss function account rare word model obtain state art performance across twenty-two languages work especially well morphologically complex languages analysis suggest bi lstms less sensitive train data size label corruptions small noise level previously assume
show efficient popular method calculate bigram frequencies unsuitable body short texts offer simple alternative method computational complexity old method offer exact count instead approximation
corpus pattern analysis cpa topic semeval two thousand and fifteen task fifteen aim produce system aid lexicographers efforts build dictionary mean english verbs use cpa annotation process cpa parse one subtasks annotation process make focus report supervise machine learn approach implement syntactic feature derive parse tree semantic feature derive wordnet word embeddings use show approach perform well even data sparsity issue characterize dataset obtain better result system margin four f score
exist approach chinese zero pronoun resolution overlook semantic information zero pronouns descriptive information result difficulty explicitly capture semantic similarities antecedents moreover deal candidate antecedents traditional systems simply take advantage local information single candidate antecedent fail consider underlie information provide candidates global perspective address weaknesses propose novel zero pronoun specific neural network capable represent zero pronouns utilize contextual information semantic level addition deal candidate antecedents two level candidate encoder employ explicitly capture local global information candidate antecedents conduct experiment chinese portion ontonotes fifty corpus experimental result show approach substantially outperform state art method various experimental settings
long term goal machine learn research build intelligent dialog agent research natural language understand focus learn fix train set label data supervision either word level tag parse task sentence level question answer machine translation kind supervision realistic humans learn language learn use communication work study dialog base language learn supervision give naturally implicitly response dialog partner conversation study setup two domains babi dataset weston et al two thousand and fifteen large scale question answer dodge et al two thousand and fifteen evaluate set baseline learn strategies task show novel model incorporate predictive lookahead promise approach learn teacher response particular surprise result learn answer question correctly without reward base supervision
speaker cluster base speaker adaptive train sit method deep neural network hide markov model dnn hmm framework present paper train speakers acoustically adjacent hierarchically cluster use vector base distance metric dnns speaker dependent layer adaptively train cluster speakers decode start unseen speaker test set match closest speaker cluster compare vector base distance previously train dnn match speaker cluster use decode utterances test speaker performance propose method large vocabulary spontaneous speech recognition task evaluate train set one thousand, five hundred hours speech test set twenty-four speakers one thousand, seven hundred and seventy-four utterances compare speaker independent dnn baseline word error rate one hundred and sixteen relative sixty-eight reduction word error rate observe propose method
learn generate chinese poems charm yet challenge task traditional approach involve various language model machine translation techniques however perform well generate poems complex pattern constraints example song iambics famous type poems involve variable length sentence strict rhythmic pattern paper apply attention base sequence sequence model generate chinese song iambics specifically encode cue sentence bi directional long short term memory lstm model predict entire iambic information provide encoder form attention base lstm regularize generation process fine structure input cue several techniques investigate improve model include global context integration hybrid style train character vector initialization adaptation automatic subjective evaluation result show model indeed learn complex structural rhythmic pattern song iambics generation rather successful
drop pronouns dp pronouns frequently drop source language retain target language challenge machine translation response problem propose semi supervise approach recall possibly miss pronouns translation firstly build train data dp generation dps automatically label accord alignment information parallel corpus secondly build deep learn base dp generator input sentence decode correspond reference exist specifically generation two phase one dp position detection model sequential label task recurrent neural network two dp prediction employ multilayer perceptron rich feature finally integrate output translation system recall miss pronouns extract rule dp label train data translate dp generate input sentence experimental result show approach achieve significant improvement one hundred and fifty-eight bleu point translation performance sixty-six f score dp generation accuracy
universal schema jointly embed knowledge base textual pattern reason entities relations automatic knowledge base construction information extraction past entity pair relations represent learn vectors compatibility determine score function limit generalization unseen text pattern entities recently column less versions universal schema use compositional pattern encoders generalize text pattern work take next step propose row less model universal schema remove explicit entity pair representations instead learn vector representations entity pair train set treat entity pair function relation type experimental result fb15k two hundred and thirty-seven benchmark demonstrate match performance comparable model explicit entity pair representations use model attention relation type demonstrate model per form nearly accuracy entity pair never see train
present new resource swedish swell corpus swedish learner essay link learners performance accord common european framework reference cefr swell consist three subcorpora spin sw1203 tisus collect three different educational establishments common metadata subcorpora include age gender native languages time residence sweden type write task depend subcorpus learner texts may contain additional information text genres topics grade five six cefr level represent corpus a1 a2 b1 b2 c1 comprise total three hundred and thirty-nine essay c2 level include since course c2 level offer work flow consist collection essay permit essay digitization registration meta data annotation automatic linguistic annotation inter rater agreement present basis sw1203 subcorpus work swell still ongoing one hundred essay wait pipeline article describe resource behind compilation swell
problem aggression internet communities rampant anonymous forums usually call imageboards notorious aggressive deviant behaviour even comparison internet communities study aim study ways automatic detection verbal expression aggression popular american 4chanorg russian 2chhk imageboards set one million, eight hundred and two thousand, seven hundred and eighty-nine message use study machine learn algorithm word2vec apply detect state aggression decent result obtain english eighty-eight result russian yet improve
article study verbal expression aggression detection use machine learn neural network methods test result use corpora message anonymous imageboards also compare random forest classifier convolutional neural network movie review one sentence per review corpus
work study comparatively two typical sentence pair classification task textual entailment te answer selection observe phrase alignments different intensities contribute differently task address problems identify phrase alignments flexible granularity pool alignments different intensities task examples flexible granularity alignments two single word single word phrase short phrase long phrase intensity roughly mean degree match range identity surface form co occurrence rephrase semantic relatedness unrelated word lot parenthesis text prior work limitations phrase generation representation ii conduct alignment word phrase level handcraft feature iii utilize single attention mechanism alignment intensities without consider characteristics specific task limit system effectiveness across task propose architecture base gate recurrent unit support representation learn phrase arbitrary granularity ii task specific focus phrase alignments two sentence attention pool experimental result te match observation state art
article present novel approach parse argumentation structure identify argument components use sequence label token level apply new joint model detect argumentation structure propose model globally optimize argument component type argumentative relations use integer linear program show model considerably improve performance base classifiers significantly outperform challenge heuristic baselines moreover introduce novel corpus persuasive essay annotate argumentation structure show annotation scheme annotation guidelines successfully guide human annotators substantial agreement corpus annotation guidelines freely available ensure reproducibility encourage future research computational argumentation
order create corpus exploration method provide topics easier interpret standard lda topic model propose combine two techniques call entity link label lda method identify ontology series descriptive label document corpus generate specific topic label direct relation topics label make interpretation easier use ontology background knowledge limit label ambiguity topics describe limit number clear cut label promote interpretability may help quantitative evaluation illustrate potential approach apply order define relevant topics address party european parliament fifth mandate one thousand, nine hundred and ninety-nine two thousand and four
structure information result temporal information process crucial variety natural language process task instance generate timeline summarization events news document answer temporal causal relate question events thesis present framework integrate temporal causal relation extraction system first develop robust extraction component type relations ie temporal order causality combine two extraction components integrate relation extraction system catena causal temporal relation extraction natural language texts utilize presumption event precedence causality cause events must happen result events several resources techniques improve relation extraction systems also discuss include word embeddings train data expansion finally report adaptation efforts temporal information process languages english namely italian indonesian
describe collection acoustic language model techniques lower word error rate english conversational telephone lvcsr system record sixty-six switchboard subset hub5 two thousand evaluation testset acoustic side use score fusion three strong model recurrent net maxout activations deep convolutional net 3x3 kernels bidirectional long short term memory net operate fmllr vector feature language model side use update model hierarchical neural network lms
introduce new measure distance languages base word embed call word embed language divergence weld weld define divergence unify similarity distribution word languages use measure perform language comparison fifty natural languages twelve genetic languages natural language dataset collection sentence align parallel corpora bible translations fifty languages span variety language families although use parallel corpora guarantee content languages interestingly many case languages within family cluster together addition natural languages perform language comparison cod regions genomes twelve different organisms four plant six animals two human subject result confirm significant high level difference genetic language model humans animals versus plant propose method step toward define quantitative measure similarity languages applications languages classification genre identification dialect identification evaluation translations
recent work word order argue syntactic structure important even require effectively recover order sentence find fact n gram language model simple heuristic give strong result task furthermore show long short term memory lstm language model even effective recover order basic model outperform state art syntactic model one hundred and fifteen bleu point additional data larger beam yield gain expense train search time
aspect phrase group important task aspect level sentiment analysis challenge problem due polysemy context dependency propose attention base deep distance metric learn addml method consider aspect phrase representation well context representation first leverage characteristics review text automatically generate aspect phrase sample pair distant supervision second fee word embeddings aspect phrase contexts attention base neural network learn feature representation contexts aspect phrase embed context embed use learn deep feature subspace measure distance aspect phrase k mean cluster experiment four review datasets show propose method outperform state art strong baseline methods
consider incorporate topic information message response match boost responses rich content retrieval base chatbots end propose topic aware convolutional neural tensor network tacntn tacntn match message response conduct message vector response vector generate convolutional neural network also leverage extra topic information encode two topic vectors two topic vectors linear combinations topic word message response respectively topic word obtain pre train lda model weight determine well message vector response vector message vector response vector two topic vectors feed neural tensors calculate match score empirical study public data set human annotate data set show tacntn significantly outperform state art methods message response match
paper describe hierarchical composition recurrent network hcrn consist three level hierarchy compositional model character word sentence model design overcome two problems represent sentence basis constituent word sequence first data sparsity problem word embed usage inter sentence dependency hcrn word representations build character thus resolve data sparsity problem inter sentence dependency embed sentence representation level sentence composition adopt hierarchy wise learn scheme order alleviate optimization difficulties learn deep hierarchical recurrent network end end fashion hcrn quantitatively qualitatively evaluate dialogue act classification task especially sentence representations inter sentence dependency able capture implicit explicit semantics sentence significantly improve performance end hcrn achieve state art performance test error rate two hundred and twenty-seven dialogue act classification swbd damsl database
often detect person utterances whether favor give target entity stance towards target however person may express stance towards target use negative positive language first time present dataset tweet target pair annotate stance sentiment target may may refer tweet may may target opinion tweet partition dataset use train test set semeval two thousand and sixteen share task competition propose simple stance detection system outperform submissions nineteen team participate share task additionally access stance sentiment annotations allow us explore several research question show know sentiment express tweet beneficial stance classification alone sufficient finally use additional unlabeled data distant supervision techniques word embeddings improve stance classification
shelf natural language process software perform poorly parse patent claim owe use irregular language relative corpora build news article web typically utilize train software stop short extensive expensive process accumulate large enough dataset completely retrain parsers patent claim method adapt exist natural language process software towards patent claim via force part speech tag correction propose amazon mechanical turk collection campaign organize generate public corpus train improve claim parse system discuss identify lessons learn campaign use future nlp dataset collection campaign amt experiment utilize corpus patent claim set measure parse performance improvement garner via claim parse system finally utility improve claim parse system within patent process applications demonstrate via experiment show improve automate patent subject classification new claim parse system utilize generate feature
explore factor influence dependence single sentence larger textual context order automatically identify candidate sentence language learn exercise corpora presentable isolation depth investigation question previously carry understand aspect contribute efficient selection candidate sentence besides reduce time require item write also ensure higher degree variability authenticity present set relevant aspects collect base qualitative analysis smaller set context dependent corpus example sentence furthermore implement rule base algorithm use criteria achieve average precision seventy-six identification different issue relate context dependence method also evaluate empirically eighty sentence system detect context dependent elements also consider context independent human raters
distribute dense word vectors show effective capture token level semantic syntactic regularities language topic model form interpretable representations document work describe lda2vec model learn dense word vectors jointly dirichlet distribute latent document level mixtures topic vectors contrast continuous dense document representations formulation produce sparse interpretable document mixtures non negative simplex constraint method simple incorporate exist automatic differentiation frameworks allow unsupervised document representations gear use scientists simultaneously learn word vectors linear relationships
drop pronouns dps ubiquitous pro drop languages like chinese japanese etc previous work mainly focus painstakingly explore empirical feature dps recovery paper propose neural recovery machine nrm model recover dps chinese avoid non trivial feature engineer process experimental result show propose nrm significantly outperform state art approach two heterogeneous datasets experiment result chinese zero pronoun zp resolution show performance zp resolution also improve recover zps dps
multi sentence compression msc great value many real world applications guide microblog summarization opinion summarization newswire summarization recently word graph base approach propose become popular msc key assumption redundancy among set relate sentence provide reliable way generate informative grammatical sentence paper propose effective approach enhance word graph base msc tackle issue state art msc approach confront ie improve informativity grammaticality time approach consist three main components one merge method base multiword expressions mwe two map strategy base synonymy word three rank step identify best compression candidates generate use pos base language model pos lm demonstrate effectiveness novel approach use dataset make cluster english newswire sentence observe improvements informativity grammaticality generate compressions show approach superior state art msc methods
present first corpus annotate preposition supersenses unlexicalized categories semantic function mark english prepositions schneider et al two thousand and fifteen scheme improve upon predecessors better facilitate comprehensive manual annotation moreover unlike previous scheme preposition supersenses organize hierarchically data publicly release web upon publication
lack standardize extrinsic evaluation methods vector representations word nlp community rely heavily word similarity task proxy intrinsic evaluation word vectors word similarity evaluation correlate distance vectors human judgments semantic similarity attractive computationally inexpensive fast paper present several problems associate evaluation word vectors word similarity datasets summarize exist solutions study suggest use word similarity task evaluation word vectors sustainable call research evaluation methods
rare texts entire book write control natural language cnl become popular exactly happen book publish last year randall munroe thing explainer use one zero often use word english language together draw picture explain complicate things nuclear reactors jet engines solar system dishwashers restrict language interest new case cnl community describe place context exist approach control natural languages provide first analysis scientific perspective cover word production rule word distributions
gleu metric propose evaluate grammatical error corrections use n gram overlap set reference sentence oppose precision recall specific annotate errors napoles et al two thousand and fifteen paper describe improvements make gleu metric address problems arise use increase number reference set unlike originally present metric modify metric require tune recommend version use instead original version
paper enhance attention base neural machine translation nmt add explicit coverage embed model alleviate issue repeat drop translations nmt source word model start full coverage embed vector track coverage status keep update neural network translation go experiment large scale chinese english task show enhance model improve translation quality significantly various test set strong large vocabulary nmt system
order capture rich language phenomena neural machine translation model use large vocabulary size require high compute time large memory usage paper alleviate issue introduce sentence level batch level vocabulary small sub set full output vocabulary sentence batch predict target word sentence level batch level vocabulary thus reduce compute time memory usage method simply take account translation options word phrase source sentence pick small target vocabulary sentence base word word translation model bilingual phrase library learn traditional machine translation model experimental result large scale english french task show method achieve better translation performance one bleu point large vocabulary neural machine translation system jean et al two thousand and fifteen
machine comprehension play essential role nlp widely explore dataset like mctest however dataset simple small learn true reason abilities citehermann2015teaching therefore release large scale news article dataset propose deep lstm reader system machine comprehension however train process expensive therefore try feature engineer approach semantics new dataset see traditional machine learn technique semantics help machine comprehension meanwhile propose l2r reader system achieve good performance efficiency less train data
present system base sequential decision make online summarization massive document stream find web give event interest eg boston marathon bomb system able filter stream relevance produce series short text update describe event unfold time unlike previous work approach able jointly model relevance comprehensiveness novelty timeliness require time sensitive query demonstrate two hundred and eighty-three improvement summary f1 four hundred and thirty-eight improvement time sensitive f1 metrics
introduce polyglot language model recurrent neural network model train predict symbol sequence many different languages use share representations symbols condition typological information language predict apply problem model phone sequence domain universal symbol inventory cross linguistically share feature representations natural fit intrinsic evaluation hold perplexity qualitative analysis learn representations extrinsic evaluation two downstream applications make use phonetic feature show polyglot model better generalize hold data comparable monolingual model ii polyglot phonetic feature representations higher quality learn monolingually
use bayesian optimization learn curricula word representation learn optimize performance downstream task depend learn representations feature curricula model linear rank function scalar product learn weight vector engineer feature vector characterize different aspects complexity instance train corpus show learn curriculum improve performance variety downstream task random order comparison natural corpus order
due lack structure knowledge apply learn distribute representation categories exist work incorporate category hierarchies entity informationwe propose framework embed entities categories semantic space integrate structure knowledge taxonomy hierarchy large knowledge base framework allow compute meaningful semantic relatedness entities categoriescompared previous state art framework handle single word concepts multiple word concepts superior performance concept categorization semantic relatedness
word embeddings converge learn similar things different initializations repeatable experiment word embeddings word embed techniques equally reliable paper propose evaluate methods learn word representations consistency across initializations propose measure quantify similarity learn word representations set subject different random initializations preliminary result illustrate metric measure intrinsic property word embed methods also correlate well evaluation metrics downstream task believe methods useful characterize robustness important property consider develop new word embed methods
propose novel framework analysis learn algorithms allow us say algorithms generalize certain pattern train data test data particular focus situations rule must learn concern two components stimulus identical call basis discrimination identity base rule identity base rule prove difficult impossible certain type learn algorithms acquire limit datasets contrast human behaviour similar task provide framework rigorously establish learn algorithms fail generalize identity base rule novel stimuli use framework show algorithms unable generalize identity base rule novel input unless train virtually possible input demonstrate result computationally multilayer feedforward neural network
natural language consider tool produce large databases consist texts write discursive tool description turn require large databases dictionaries grammars etc nowadays notion database associate computer process computer memory however natural language reside also human brain function human communication interpersonal intergenerational one discuss survey research paper mathematical particular geometric constructions help bridge two worlds particular paper consider vector space model semantics base frequency matrices use natural language process investigate underlie geometries formulate term grassmannians projective space flag varieties formulate relation vector space model semantic space base semic ax term projectability subvarieties grassmannians projective space interpret latent semantics geometric flow grassmannians also discuss formulate gardenfors notion meet mind geometric set
introduce treebank learner english tle first publicly available syntactic treebank english second language esl tle provide manually annotate pos tag universal dependency ud tree five thousand, one hundred and twenty-four sentence cambridge first certificate english fce corpus ud annotations tie pre exist error annotation fce whereby full syntactic analyse provide original error correct versions sentence delineate esl annotation guidelines allow consistent syntactic treatment ungrammatical english finally benchmark pos tag dependency parse performance tle dataset measure effect grammatical errors parse accuracy envision treebank support wide range linguistic computational research second language acquisition well automatic process ungrammatical language treebank available universaldependenciesorg annotation manual use project graphical query engine available esltreebankorg
problem collect reliable estimate occurrence entities open web form premise report model learn tag entities expect perform well deploy web owe severe mismatch distributions entities web relatively diminutive train data report build case maximum mean discrepancy estimation occurrence statistics entities web take review name entity disambiguation techniques relate concepts along way
present new convolutional neural network cnn model text classification jointly exploit label document component sentence specifically consider scenarios annotators explicitly mark sentence snippets support overall document categorization ie provide rationales model exploit supervision via hierarchical approach document represent linear combination vector representations component sentence propose sentence level convolutional model estimate probability give sentence rationale scale contribution sentence aggregate document representation proportion estimate experiment five classification datasets document label associate rationales demonstrate approach consistently outperform strong baselines moreover model naturally provide explanations predictions
obtain syntactic parse crucial part many nlp pipelines however world languages large amount syntactically annotate corpora available build parsers syntactic projection techniques attempt address issue use parallel corpora consist resource poor resource rich language pair take advantage parser resource rich language word alignment languages project parse onto data resource poor language projection methods suffer however two languages divergent paper investigate possibility use small parallel annotate corpora automatically detect divergent structural pattern two languages pattern use improve structural projection algorithms allow better perform nlp tool resource poor languages particular may large amount annotate data necessary traditional fully supervise methods detection process exhaustive demonstrate common pattern divergence identify automatically without prior knowledge give language pair pattern use improve performance projection algorithms
present study two key characteristics human syntactic annotations anchor agreement anchor well know cognitive bias human decision make judgments draw towards pre exist value study influence anchor standard approach creation syntactic resources syntactic annotations obtain via human edit tagger parser output experiment demonstrate clear anchor effect reveal unwanted consequences include overestimation parse performance lower quality annotations comparison human base annotations use sentence penn treebank wsj also report systematically obtain inter annotator agreement estimate english dependency parse agreement result control parser bias consequential par state art parse performance english newswire discuss impact find strategies future annotation efforts parser evaluations
introduce machine translation mt evaluation survey contain manual automatic evaluation methods traditional human evaluation criteria mainly include intelligibility fidelity fluency adequacy comprehension informativeness advance human assessments include task orient measure post edit segment rank extend criteriea etc classify automatic evaluation methods two categories include lexical similarity scenario linguistic feature application lexical similarity methods contain edit distance precision recall f measure word order linguistic feature divide syntactic feature semantic feature respectively syntactic feature include part speech tag phrase type sentence structure semantic feature include name entity synonyms textual entailment paraphrase semantic roles language model deep learn model evaluation newly propose subsequently also introduce evaluation methods mt evaluation include different correlation score recent quality estimation qe task mt paper differ exist work citegaleprogram2009euromatrixproject2007 several aspects introduce recent development mt evaluation measure different classifications manual automatic evaluation measure introduction recent qe task mt concise construction content hope work helpful mt researchers easily pick metrics best suitable specific mt model development help mt evaluation researchers get general clue mt evaluation research develop furthermore hopefully work also shine light evaluation task except translation nlp field
similarity core notion use psychology two branch linguistics theoretical computational similarity datasets come two field differ design psychological datasets focus around certain topic fruit name linguistic datasets contain word various categories later make humans assign low similarity score word nothing common word contrast mean make similarity score ambiguous work discuss similarity collection procedure multi category dataset avoid score ambiguity suggest change evaluation procedure reflect insights psychological literature word phrase sentence similarity suggest ask humans provide list commonalities differences instead numerical similarity score employ structure human judgements beyond pairwise similarity model evaluation believe propose approach give rise datasets test mean representation model thoroughly respect human treatment similarity
investigate use hierarchical phrase base smt lattices end end neural machine translation nmt weight push transform hiero score complete translation hypotheses full translation grammar score full n gram language model score posteriors compatible nmt predictive probabilities slightly modify nmt beam search decoder find gain hiero nmt decode alone practical advantage extend nmt large input output vocabularies
paper describe submission amu adam mickiewicz university team automatic post edit ape task wmt two thousand and sixteen explore application neural translation model ape problem achieve good result treat different model components log linear model allow multiple input mt output source decode target language post edit translations simple string match penalty integrate within log linear model use control higher faithfulness regard raw machine translation output overcome problem little train data generate large amount artificial data submission improve uncorrected baseline unseen test set thirty-two ter fifty-five bleu outperform system submit share task large margin
paper describe amu uedin submissions wmt two thousand and sixteen share task news translation explore methods decode time integration attention base neural translation model phrase base statistical machine translation efficient batch algorithms gpu query propose implement english russian system stay behind state art pure neural model term bleu among restrict systems manual evaluation place first cluster tie pure neural model russian english task submission achieve top bleu result outperform best pure neural system eleven bleu point phrase base baseline sixteen bleu manual evaluation system best restrict system cluster follow experiment improve result additional eight bleu
neural network base methods obtain great progress variety natural language process task however previous work model learn base single task supervise objectives often suffer insufficient train data paper use multi task learn framework jointly learn across multiple relate task base recurrent neural network propose three different mechanisms share information model text task specific share layer entire network train jointly task experiment four benchmark text classification task show propose model improve performance task help relate task
model human conversations essence build satisfy chat bots multi turn dialog ability conversation model notably benefit domain knowledge since relationships sentence clarify due semantic hint introduce knowledge paper deep neural network propose incorporate background knowledge conversation model specially design recall gate domain knowledge transform extra global memory long short term memory lstm enhance lstm cooperate local memory capture implicit semantic relevance sentence within conversations addition paper introduce loose structure domain knowledge base build slight amount manual work easily adopt recall gate model evaluate context orient response select task experimental result two datasets show approach promise model human conversations build key components automatic chat systems
paper explore use convolutional network convnets purpose cognate identification compare architecture binary classifiers base string similarity measure different language families experiment show convolutional network achieve competitive result across concepts across language families task cognate identification
recent work learn vector space embeddings multi relational data focus combine relational information derive knowledge base distributional information derive large text corpora propose simple approach leverage descriptions entities phrase available lexical resources conjunction distributional semantics order derive better initialization train relational model apply initialization transe model result significant new state art performances wordnet dataset decrease mean rank previous best two hundred and twelve fifty-one also result faster convergence entity representations find trade improve mean rank hits10 approach illustrate much remain understand regard performance improvements relational model
recently rise interest model interactions two sentence deep neural network however exist methods encode two sequence separate encoders sentence encode little information sentence paper propose deep architecture model strong interaction sentence pair two couple lstms specifically introduce two couple ways model interdependences two lstms couple local contextualized interactions two sentence aggregate interactions use dynamic pool select informative feature experiment two large datasets demonstrate efficacy propose architecture superiority state art methods
describe machine learn base method identify incorrect entries translation memories extend previous work barbu two thousand and fifteen incorporate recall base machine translation part speech tag feature system rank first binary classification ii task two three language pair english italian english spanish
work study parameter tune towards m2 metric standard metric automatic grammar error correction gec task implement m2 scorer moses tune framework investigate interactions dense sparse feature different optimizers tune strategies conll two thousand and fourteen share task notice erratic behavior optimize sparse feature weight m2 offer partial solutions find bare bone phrase base smt setup task specific parameter tune outperform previously publish result conll two thousand and fourteen test set large margin four thousand, six hundred and thirty-seven m2 previously four thousand, one hundred and seventy-five smt system neural feature train publicly available data newly introduce dense sparse feature widen gap improve state art four thousand, nine hundred and forty-nine m2
paper novel approach propose automatically construct parallel discourse corpus dialogue machine translation firstly parallel subtitle data correspond monolingual movie script data crawl collect internet tag speaker discourse boundary script data project subtitle data via information retrieval approach order map monolingual discourse bilingual texts evaluate map result also integrate speaker information translation experiment show propose method achieve eight thousand, one hundred and seventy-nine nine thousand, eight hundred and sixty-four accuracy speaker dialogue boundary annotation speaker base language model adaptation obtain around five bleu point improvement translation qualities finally publicly release around 100k parallel discourse data manual speaker dialogue boundary annotation
paper investigate two different neural architectures task relation classification convolutional neural network recurrent neural network model demonstrate effect different architectural choices present new context representation convolutional neural network relation classification extend middle context furthermore propose connectionist bi directional recurrent neural network introduce rank loss optimization finally show combine convolutional recurrent neural network use simple vote scheme accurate enough improve result neural model achieve state art result semeval two thousand and ten relation classification task
paper present ongoing effort lexical semantic analysis annotation modern standard arabic msa text semi automatic annotation tool concern morphologic syntactic semantic level description
natural language generation systems typically two part strategic say tactical say present experiment build unsupervised corpus drive template base tactical nlg system consider templates sequence word contain gap idea base observation templates grammatical locally within textual span posit construction sentence highly restrict sequence templates work attempt explore result search space use genetic algorithms arrive acceptable solutions present baseline implementation approach output gap text
paper introduce novel model semantic role label make use neural sequence model techniques approach motivate observation complex syntactic structure relate phenomena nest subordinations nominal predicate handle well exist model model treat instance sub sequence lexicalize dependency paths learn suitable embed representations experimentally demonstrate embeddings improve result previous state art semantic role labelers showcase qualitative improvements obtain method
traditional dialog systems use goal orient applications require lot domain specific handcraft hinder scale new domains end end dialog systems components train dialogs escape limitation encourage success recently obtain chit chat dialog may carry goal orient settings paper propose testbed break strengths shortcomings end end dialog systems goal orient applications set context restaurant reservation task require manipulate sentence symbols properly conduct conversations issue api call use output call show end end dialog system base memory network reach promise yet imperfect performance learn perform non trivial operations confirm result compare system hand craft slot fill baseline data second dialog state track challenge henderson et al 2014a show similar result pattern data extract online concierge service
propose novel vector representation integrate lexical contrast distributional vectors strengthen salient feature determine degrees word similarity improve vectors significantly outperform standard model distinguish antonyms synonyms average precision sixty-six seventy-six across word class adjectives nouns verbs moreover integrate lexical contrast vectors objective function skip gram model novel embed outperform state art model predict word similarities simlex nine hundred and ninety-nine distinguish antonyms synonyms
paper develop novel approach aspect term extraction base unsupervised learn distribute representations word dependency paths basic idea connect two word w1 w2 dependency path r embed space specifically method optimize objective w1 w2 low dimensional space multi hop dependency paths treat sequence grammatical relations model recurrent neural network design embed feature consider linear context dependency context information conditional random field crf base aspect term extraction experimental result semeval datasets show one embed feature achieve state art result two embed method incorporate syntactic information among word yield better performance representative ones aspect term extraction
model neural machine translation often discriminative family encoderdecoders learn conditional distribution target sentence give source sentence paper propose variational model learn conditional distribution neural machine translation variational encoderdecoder model train end end different vanilla encoder decoder model generate target translations hide representations source sentence alone variational model introduce continuous latent variable explicitly model underlie semantics source sentence guide generation target translations order perform efficient posterior inference large scale train build neural posterior approximator condition source target side equip reparameterization technique estimate variational lower bind experiment chinese english english german translation task show propose variational neural machine translation achieve significant improvements vanilla neural machine translation baselines
paper propose bidimensional attention base recursive autoencoder battrae integrate clue sourcetarget interactions multiple level granularity bilingual phrase representations employ recursive autoencoders generate tree structure phrase embeddings different level granularity eg word sub phrase phrase embeddings source target side introduce bidimensional attention network learn interactions encode bidimensional attention matrix extract two soft attention weight distributions simultaneously weight distributions enable battrae generate compositive phrase representations via convolution base learn phrase representations use bilinear neural model train via max margin method measure bilingual semantic similarity evaluate effectiveness battrae incorporate semantic similarity additional feature state art smt system extensive experiment nist chinese english test set show model achieve substantial improvement one hundred and sixty-three bleu point average baseline
paper open domain factoid question answer system polish rafael present system go beyond find answer sentence also extract single string correspond require entity herein focus place different approach entity recognition essential retrieve information match question constraints apart traditional approach include name entity recognition ner solutions novel technique call deep entity recognition deeper introduce implement allow comprehensive search form entity reference match give wordnet synset eg impressionist base previously assemble entity library create analyse first sentence encyclopaedia entries disambiguation redirect page deeper also provide automatic evaluation make possible numerous experiment include thousand question quiz tv show answer ground polish wikipedia final result manual evaluation separate question set show strength deeper approach lie ability answer question demand answer beyond traditional categories name entities
evaluation nlp methods require test previously vet gold standard test set report standard metrics accuracy precision recall f1 current assumption items give test set equal regard difficulty discriminate power propose item response theory irt psychometrics alternative mean gold standard test set generation nlp system evaluation irt able describe characteristics individual items difficulty discriminate power account characteristics estimation human intelligence ability nlp task paper demonstrate irt generate gold standard test set recognize textual entailment collect large number human responses fit irt model show irt model compare nlp systems performance human population able provide insight system performance standard evaluation metrics show high accuracy score always imply high irt score depend item characteristics response pattern
introduce deep memory network aspect level sentiment classification unlike feature base svm sequential neural model lstm approach explicitly capture importance context word infer sentiment polarity aspect importance degree text representation calculate multiple computational layer neural attention model external memory experiment laptop restaurant datasets demonstrate approach perform comparable state art feature base svm system substantially better lstm attention base lstm architectures datasets show multiple computational layer could improve performance moreover approach also fast deep memory network nine layer fifteen time faster lstm cpu implementation
paper propose sentence encode base model recognize text entailment approach encode sentence two stage process firstly average pool use word level bidirectional lstm bilstm generate first stage sentence representation secondly attention mechanism employ replace average pool sentence better representations instead use target sentence attend word source sentence utilize sentence first stage representation attend word appear call inner attention paper experiment conduct stanford natural language inference snli corpus prove effectiveness inner attention mechanism less number parameters model outperform exist best sentence encode base approach large margin
understand word change mean time key model language cultural evolution historical data mean scarce make theories hard develop test word embeddings show promise diachronic tool carefully evaluate develop robust methodology quantify semantic change evaluate word embeddings ppmi svd word2vec know historical change use methodology reveal statistical laws semantic evolution use six historical corpora span four languages two centuries propose two quantitative laws semantic change law conformity rate semantic change scale inverse power law word frequency ii law innovation independent frequency word polysemous higher rat semantic change
build part speech pos taggers code mix indian languages particularly challenge problem computational linguistics due dearth accurately annotate train corpora icon part nlp tool contest organize challenge share task second consecutive year improve state art paper describe pos tagger build surukam predict coarse grain fine grain pos tag three language pair bengali english telugu english hindi english text span three popular social media platforms facebook whatsapp twitter employ conditional random field sequence tag algorithm use library call sklearn crfsuite thin wrapper around crfsuite train model among feature use include character n grams language information pattern emoji number punctuation web address submissions constrain environmentie without make use monolingual pos taggers like obtain overall average f1 score seven thousand, six hundred and forty-five comparable two thousand and fifteen win score seven thousand, six hundred and seventy-nine
recent approach sentiment lexicon induction capitalize pre train word embeddings capture latent semantic properties however embeddings obtain optimize performance give task eg predict contextual word sub optimal applications paper address problem exploit task specific representations induce via embed sub space projection allow us expand lexicons describe multiple semantic properties property model jointly learn suitable representations concomitant predictor experiment conduct multiple subjective lexicons show model outperform previous work baselines even low train data regimes furthermore lexicon base sentiment classifiers build top lexicons outperform similar resources yield performances comparable supervise model
argumentation mine social media content attract increase attention task challenge reward informal nature user generate content make task dauntingly difficult hand insights could gain large scale analysis social media argumentation make worthwhile task position paper discuss motivation social media argumentation mine well task challenge involve
number recent work propose attention model visual question answer vqa generate spatial map highlight image regions relevant answer question paper argue addition model look visual attention equally important model word listen question attention present novel co attention model vqa jointly reason image question attention addition model reason question consequently image via co attention mechanism hierarchical fashion via novel one dimensional convolution neural network cnn model improve state art vqa dataset six hundred and three six hundred and five six hundred and sixteen six hundred and thirty-three coco qa dataset use resnet performance improve six hundred and twenty-one vqa six hundred and fifty-four coco qa
investigate task model open domain multi turn unstructured multi participant conversational dialogue specifically study effect incorporate different elements conversation unlike previous efforts focus model message responses extend model long context participant history system rely handwritten rule engineer feature instead train deep neural network large conversational dataset particular exploit structure reddit comment post extract twenty-one billion message one hundred and thirty-three million conversations evaluate model task predict next response conversation find model context participants improve prediction accuracy
conventional graph base dependency parsers guarantee tree structure train inference instead formalize dependency parse problem independently select head word sentence model call textscdense shorthand bf dependency bf neural bf selection produce distribution possible head word use feature obtain bidirectional recurrent neural network without enforce structural constraints train textscdense generate inference time tree overwhelm majority sentence non tree output adjust maximum span tree algorithm evaluate textscdense four languages english chinese czech german vary degrees non projectivity despite simplicity approach parsers par state art
paper propose neural conversation model conduct dialogues demonstrate use model generate help desk responses users ask question pc applications model distinguish two characteristics first model intention across turn recurrent network incorporate attention model condition representation intention secondly avoid generate non specific responses incorporate idf term objective function model evaluate pure generation model help desk response generate scratch retrieval model performance measure use recall rat correct response experimental result indicate model outperform previously propose neural conversation architectures use specificity objective function significantly improve performances generation retrieval
paper study problem answer cloze style question document model gate attention ga reader integrate multi hop architecture novel attention mechanism base multiplicative interactions query embed intermediate state recurrent neural network document reader enable reader build query specific representations tokens document accurate answer selection ga reader obtain state art result three benchmarks task cnn daily mail news stories dataset effectiveness multiplicative interaction demonstrate ablation study compare alternative compositional operators implement gate attention code available https githubcom bdhingra ga reader
present proof net calculus displacement calculus show correctness first proof net calculus model displacement calculus directly sort translation another formalism proof net calculus open new possibilities parse proof search displacement calculus
propose novel neural attention architecture tackle machine comprehension task answer cloze style query respect document unlike previous model collapse query single vector instead deploy iterative alternate attention mechanism allow fine grain exploration query document model outperform state art baselines standard machine comprehension benchmarks cnn news article children book test cbt dataset
introduce new language learn set relevant build adaptive natural language interfaces inspire wittgenstein language game human wish accomplish task eg achieve certain configuration block communicate computer perform actual action eg remove red block computer initially know nothing language therefore must learn scratch interaction human adapt computer capabilities create game block world collect interactions one hundred people play first analyze humans strategies show use compositionality avoid synonyms correlate positively task performance second compare computer strategies show quickly learn semantic parse model scratch model pragmatics accelerate learn successful players
paper describe new speak dialog portal connect systems produce speak dialog academic research community give access real users introduce distribute multi modal multi agent prototype dialog framework afford easy integration various remote resources range end end dialog systems external knowledge apis date dialport portal successfully connect multi domain speak dialog system cambridge university noaa national oceanic atmospheric administration weather api yelp api
describe two step approach dialogue management task orient speak dialogue systems unify neural network framework propose enable system first learn supervision set dialogue data continuously improve behaviour via reinforcement learn use gradient base algorithms one single model experiment demonstrate supervise model effectiveness corpus base evaluation user simulation pay human subject use reinforcement learn improve model performance interactive settings especially higher noise condition
enable computer understand document answer comprehension question central yet unsolved goal nlp key factor impede solution machine learn systems limit availability human annotate data hermann et al two thousand and fifteen seek solve problem create million train examples pair cnn daily mail news article summarize bullet point show neural network train give good performance task paper conduct thorough examination new read comprehension task primary aim understand depth language understand require well task approach one side careful hand analysis small subset problems show simple carefully design systems obtain accuracies seven hundred and thirty-six seven hundred and sixty-six two datasets exceed current state art result seven ten approach believe ceiling performance task
large increase volume textual data automate methods identify significant topics classify textual document receive grow interest many efforts make direction still remain real challenge moreover issue even complex full texts always freely available use partial information annotate document promise remain ambitious issue methodswe propose two classification methods k nearest neighbour knn base approach explicit semantic analysis esa base approach although knn base approach widely use text classification need improve perform well specific classification problem deal partial information compare exist knn base methods method use classical machine learn ml algorithms rank label additional feature also investigate order improve classifiers performance addition combination several learn algorithms various techniques fix number relevant topics perform hand esa seem promise classification task yield interest result relate issue semantic relatedness computation texts text classification unlike exist work use esa enrich bag word approach additional knowledge base feature esa base method build standalone classifier furthermore investigate result method could useful complementary feature knn base approachresultsexperimental evaluations perform large standard annotate datasets provide bioasq organizers show knn base method random forest learn algorithm achieve good performances compare current state art methods reach competitive f measure fifty-five esa base approach surprisingly yield reserve resultsconclusionswe propose simple classification methods suitable annotate textual document use partial information therefore adequate large multi label classification particularly biomedical domain thus work contribute extraction relevant information unstructured document order facilitate automate process consequently could use various purpose include document index information retrieval etc
paper propose use deep policy network train advantage actor critic method statistically optimise dialogue systems first show summary state action space deep reinforcement learn rl outperform gaussian process methods summary state action space lead good performance require pre engineer effort rl knowledge domain expertise order remove need define summary space show deep rl also train efficiently original state action space dialogue systems base partially observable markov decision process know require many dialogues train make unappealing practical deployment show deep rl method base actor critic architecture exploit small amount data efficiently indeed hundred dialogues collect handcraft policy actor critic deep learner considerably bootstrapped combination supervise batch rl addition convergence optimal policy significantly speed compare deep rl methods initialize data batch rl experiment perform restaurant domain derive dialogue state track challenge two dstc2 dataset
unsupervised text embeddings extraction crucial text understand machine learn word2vec variants receive substantial success map word similar syntactic semantic mean vectors close however extract context aware word sequence embed remain challenge task train large corpus difficult label difficult get importantly challenge pre train model obtain word sequence embeddings universally good downstream task new datasets propose two phase convdicdeconvdec framework solve problem combine word sequence dictionary learn model word sequence embed decode model propose convolutional tensor decomposition mechanism learn good word sequence phrase dictionary learn phase prove accurate much efficient popular alternate minimization method decode phase introduce deconvolution framework immune problem vary sentence lengths word sequence embeddings extract use convdicdeconvdec universally good downstream task test framework require neither pre train prior outside information
decision make often dependent uncertain data eg data associate confidence score probabilities present comparison different information presentations uncertain data first time measure effect human decision make show use natural language generation nlg improve decision make uncertainty compare state art graphical base representation methods task base study four hundred and forty-two adults find presentations use nlg lead twenty-four better decision make average graphical presentations forty-four better decision make nlg combine graphics also show women achieve significantly better result present nlg output eighty-seven increase average compare graphical presentations
encoder decoder network popular model sequence probabilistically many applications model use power long short term memory lstm architecture capture full dependence among variables unlike earlier model like crfs typically assume conditional independence among non adjacent variables however practice encoder decoder model exhibit bias towards short sequence surprisingly get worse increase beam size paper show phenomenon due discrepancy full sequence margin per element margin enforce locally condition train objective encoder decoder model discrepancy adversely impact long sequence explain bias towards predict short sequence case predict sequence come close set show globally condition model alleviate problems encoder decoder model practical point view propose model also eliminate need beam search inference reduce efficient dot product base search vector space
conduct large scale study human attention visual question answer vqa understand humans choose look answer question image design test multiple game inspire novel attention annotation interfaces require subject sharpen regions blur image answer question thus introduce vqa hat human attention dataset evaluate attention map generate state art vqa model human attention qualitatively via visualizations quantitatively via rank order correlation overall experiment show current attention model vqa seem look regions humans
paper present clean yet effective model word sense disambiguation approach leverage bidirectional long short term memory network share word enable model share statistical strength scale well vocabulary size model train end end directly raw text sense label make effective use word order evaluate approach two standard datasets use identical hyperparameter settings turn tune third set hold data employ external resources eg knowledge graph part speech tag etc language specific feature hand craft rule still achieve statistically equivalent result best state art systems employ limitations
describe mitre submission semeval two thousand and sixteen task six detect stance tweet effort achieve top score task supervise stance detection produce average f1 score six hundred and seventy-eight assess whether tweet author favor topic employ recurrent neural network initialize feature learn via distant supervision two large unlabeled datasets train embeddings word phrase word2vec skip gram method use feature learn sentence representations via hashtag prediction auxiliary task sentence vectors fine tune stance detection several hundred label examples result high perform system use transfer learn maximize value available train data
prediction without justification limit applicability remedy learn extract piece input text justifications rationales tailor short coherent yet sufficient make prediction approach combine two modular components generator encoder train operate well together generator specify distribution text fragment candidate rationales pass encoder prediction rationales never give train instead model regularize desiderata rationales evaluate approach multi aspect sentiment analysis manually annotate test case approach outperform attention base baseline significant margin also successfully illustrate method question retrieval task
neural machine translation nmt aim solve machine translation mt problems use neural network exhibit promise result recent years however exist nmt model shallow still performance gap single nmt model best conventional mt system work introduce new type linear connections name fast forward connections base deep long short term memory lstm network interleave bi directional architecture stack lstm layer fast forward connections play essential role propagate gradients build deep topology depth sixteen wmt fourteen english french task achieve bleu377 single attention model outperform correspond single shallow model sixty-two bleu point first time single nmt model achieve state art performance outperform best conventional model seven bleu point still achieve bleu363 even without use attention mechanism special handle unknown word model ensembling obtain best score report date task bleu404 model also validate difficult wmt fourteen english german task
deal complex word form morphologically rich languages open problem language process particularly important translation contrast modern neural systems translation discard identity rare word paper propose several architectures learn word representations character morpheme level word decompositions incorporate representations novel machine translation model jointly learn word alignments translations via hard attention mechanism evaluate translate several morphologically rich languages english show consistent improvements strong baseline methods one fifteen bleu point
selection suitable document representation approach play crucial role performance document cluster task able pick representative word within document lead substantial improvements document cluster case web document html markup define layout content provide additional structural information exploit identify representative word paper introduce fuzzy term weigh approach make html structure document cluster set forth build hypothesis good representation take advantage humans skim document extract representative word author web page make use html tag convey important message web page page elements attract readers attention page title emphasize elements define set criteria exploit information provide page elements introduce fuzzy combination criteria evaluate within context web page cluster task propose approach call abstract fuzzy combination criteria afcc adapt datasets whose feature distribute differently achieve good result compare similar fuzzy logic base approach tf idf across different datasets
paper study problem question answer reason multiple facts require propose query reduction network qrn variant recurrent neural network rnn effectively handle short term local long term global sequential dependencies reason multiple facts qrn consider context sentence sequence state change trigger reduce original query inform query observe trigger context sentence time experiment show qrn produce state art result babi qa dialog task real goal orient dialog dataset addition qrn formulation allow parallelization rnn time axis save order magnitude time complexity train inference
video caption attract broad research attention multimedia community however exist approach either ignore temporal information among video frame employ local contextual temporal knowledge work propose novel video caption framework term emphbidirectional long short term memory bilstm deeply capture bidirectional global temporal structure video specifically first devise joint visual model approach encode video data combine forward lstm pass backward lstm pass together visual feature convolutional neural network cnns inject derive video representation subsequent language model initialization benefit two fold one comprehensively preserve sequential visual information two adaptively learn dense visual feature sparse semantic representations videos sentence respectively verify effectiveness propose video caption framework commonly use benchmark ie microsoft video description msvd corpus experimental result demonstrate superiority propose approach compare several state art methods
investigate share language yous supreme court majority opinions interest group correspond amicus brief specifically evaluate whether language originate amicus brief acquire legal precedent status cite court opinion use plagiarism detection software automate query large legal database manual analysis establish seven instance interest group amici able formulate constitutional case law set bind legal precedent discuss several instance implications supreme court creation case law
present evaluate new model natural language generation nlg speak dialogue systems base statistical plan give noisy feedback current generation context eg user surface realiser study use standard nlg problem present information case set search result users give complex trade off utterance length amount information convey cognitive load set trade off analyse exist match data train nlg pol icy use reinforcement learn rl adapt behaviour noisy fee back current generation context policy compare several base line derive previous work area learn policy significantly perform prior approach
task dialog management commonly decompose two sequential subtasks dialog state track dialog policy learn end end dialog system aim dialog state track accurately estimate true dialog state noisy observations produce speech recognition natural language understand modules state track task primarily mean support dialog policy probabilistic perspective achieve maintain posterior distribution hide dialog state compose set context dependent variables dialog policy learn strive select optimal dialog act give estimate dialog state define reward function paper introduce novel method dialog state track base bilinear algebric decomposition model provide efficient inference schema collective matrix factorization evaluate propose approach second dialog state track challenge dstc two dataset show propose tracker give encourage result compare state art trackers participate standard benchmark finally show prediction schema computationally efficient comparison previous approach
much attention give task gender inference twitter users although name strong gender indicators name twitter users rarely use feature probably due high number ill form name find name dictionary instead rely solely name database propose novel name classifier approach extract characteristics user name use order assign name gender enable us classify international first name well ill form name
recruiters usually spend less minute look r esum e decide whether worth continue recruitment process candidate recruiters focus keywords almost impossible guarantee fair process candidate selection main scope paper tackle issue introduce data drive approach show process r esum es automatically give recruiters time examine promise candidates furthermore show leverage machine learn natural language process order extract require information r esum es information extract rank score calculate score describe well candidates fit base education work experience skills later paper illustrate prototype application show novel approach increase productivity recruiters application enable filter rank candidates base predefined job descriptions guide rank recruiters get deeper insights candidate profile validate application rank application show improve hire process give unbiased hire decision support
paper describe approach detect stance tweet task semeval two thousand and sixteen task six utilize recent advance short text categorization use deep learn create word level character level model choice word level character level model particular case inform validation performance final system combination classifiers use word level character level model also employ novel data augmentation techniques expand diversify train dataset thus make system robust system achieve macro average precision recall f1 score sixty-seven sixty-one six hundred and thirty-five respectively
acoustic model base long short term memory recurrent neural network lstm rnns apply statistical parametric speech synthesis spss show significant improvements naturalness latency base hide markov model hmms paper describe optimizations lstm rnn base spss deployment mobile devices weight quantization multi frame inference robust inference use epsilon contaminate gaussian loss function experimental result subjective listen test show optimizations make lstm rnn base spss comparable hmm base spss runtime speed maintain naturalness evaluations lstm rnn base spss hmm drive unit selection speech synthesis also present
word embed specially recent developments promise quantification similarity term however clear extent similarity value genuinely meaningful useful subsequent task explore similarity score obtain model really indicative term relatedness first observe quantify uncertainty factor word embed model regard similarity value base factor introduce general threshold various dimension effectively filter highly relate term evaluation four information retrieval collections support effectiveness approach result introduce threshold significantly better baseline equal statistically indistinguishable optimal result
compositional model theoretic semantics researchers assemble truth condition kinds denotations use lambda calculus previously observe lambda term denotations study tend follow pattern instance monad paper present extension simply type lambda calculus exploit uniformity use recently discover technique effect handlers prove calculus exhibit key formal properties lambda calculus use construct modular semantics small fragment involve multiple distinct semantic phenomena
provide qualitative analysis descriptions contain negations n nobody etc flickr30k corpus categorization negation use base analysis provide set requirements image description system order generate negation sentence pilot experiment use categorization manually annotate sentence contain negations flickr30k corpus agreement score k067 paper hope open broader discussion subjective language image descriptions
people share opinions stories review online video share websites every day study sentiment subjectivity opinion videos experience grow attention academia industry sentiment analysis successful text understudy research question videos multimedia content biggest setbacks study direction lack proper dataset methodology baselines statistical analysis information different modality source relate paper introduce scientific community first opinion level annotate corpus sentiment subjectivity analysis online videos call multimodal opinion level sentiment intensity dataset mosi dataset rigorously annotate label subjectivity sentiment intensity per frame per opinion annotate visual feature per milliseconds annotate audio feature furthermore present baselines future study direction well new multimodal fusion approach jointly model speak word visual gesture
knowledge base useful resources many natural language process task however far complete paper define novel entity representation mixture neighborhood knowledge base apply technique transe well know embed model knowledge base completion experimental result show neighborhood information significantly help improve result transe model lead better performance obtain state art embed model three benchmark datasets triple classification entity prediction relation prediction task
investigate integration word embeddings classification feature set large scale text classification representations use plethora task however application classification scenarios thousands class extensively research partially due hardware limitations work examine efficient composition function obtain document level word level embeddings subsequently investigate combination traditional one hot encode representations present empirical evidence large multi class multi label classification problems demonstrate efficiency performance benefit combination
show mutual information two symbols function number symbols two decay exponentially probabilistic regular grammar decay like power law context free grammar result formal languages closely relate well know result classical statistical mechanics phase transition dimension fewer two also relate emergence power law correlations turbulence cosmological inflation recursive generative process elucidate physics connections comment potential applications result machine learn task like train artificial recurrent neural network along way introduce useful quantity dub rational mutual information discuss generalizations claim involve complicate bayesian network
core problem learn semantic parsers denotations pick consistent logical form yield correct denotation combinatorially large space control search space previous work rely restrict set rule limit expressivity paper consider much expressive class logical form show use dynamic program efficiently represent complete set consistent logical form expressivity also introduce many spurious logical form consistent correct denotation represent mean utterance address generate fictitious worlds use crowdsourced denotations worlds filter spurious logical form wikitablequestions dataset increase coverage answerable question five hundred and thirty-five seventy-six additional crowdsourced supervision let us us rule nine hundred and twenty-one spurious logical form
recently rapid development word embed neural network bring new inspiration various nlp ir task paper describe stag hybrid model combine recurrent convolutional neural network rcnn highway layer highway network module incorporate middle take output bi directional recurrent neural network bi rnn module first stage provide convolutional neural network cnn module last stage input experiment show model outperform common neural network model cnn rnn bi rnn sentiment analysis task besides analysis sequence length influence rcnn highway layer show model could learn good representation long text
zero resource speech technology grow research area aim develop methods speech process absence transcriptions lexicons language model text early term discovery systems focus identify isolate recur pattern corpus recent full coverage systems attempt completely segment cluster audio word like units effectively perform unsupervised speech recognition article present first attempt aware apply system large vocabulary multi speaker data system use bayesian model framework segmental word representations word segment represent fix dimensional acoustic embed obtain map sequence feature frame single embed vector compare system english xitsonga datasets state art baselines use variety measure include word error rate obtain map unsupervised output grind truth transcriptions high word error rat report order seventy eighty speaker dependent eighty ninety-five speaker independent systems highlight difficulty task nevertheless term cluster quality word segmentation metrics show impose consistent top segmentation also use bottom knowledge detect syllable boundaries single speaker multi speaker versions system outperform purely bottom single speaker syllable base approach also show discover cluster make less speaker gender specific use unsupervised autoencoder like feature extractor learn better frame level feature prior embed system discover cluster still less pure unsupervised term discovery systems provide far greater coverage
paper present preliminary work use word embed word2vec query expansion context personalize information retrieval traditionally word embeddings learn general corpus like wikipedia work try personalize word embeddings learn achieve learn user profile word embeddings context user interest proposal evaluate clef social book search two thousand and sixteen collection result obtain show efforts make way apply word embed context personalize information retrieval
word embeddings convolutional neural network cnn attract extensive attention various classification task twitter eg sentiment classification however effect configuration use train generate word embeddings classification performance study exist literature paper use twitter election classification task aim detect election relate tweet investigate impact background dataset use train embed model context window size dimensionality word embeddings classification performance compare classification result two word embed model train use different background corpora eg wikipedia article twitter microposts show background data type align twitter classification dataset achieve better performance moreover evaluate result word embeddings model train use various context window size dimensionalities find large context window dimension size preferable improve performance experimental result also show use word embeddings cnn lead statistically significant improvements various baselines random svm tf idf svm word embeddings
community question answer cqa forums become popular medium solicit direct answer specific question users experts experience users give topic however give question users sometimes sift large number low quality irrelevant answer find answer satisfy information need alleviate problem answer quality prediction aqp aim predict quality answer post response forum question current aqp systems either learn model use various hand craft feature hcf b use deep learn dl techniques automatically learn require feature representations paper propose novel approach aqp know deep feature fusion network dffn leverage advantage hand craft feature deep learn base systems give question answer pair along metadata dffn independently learn deep feature use convolutional neural network cnn b compute hand craft feature use various external resources combine use deep neural network train predict final answer quality dffn achieve state art performance standard semeval two thousand and fifteen semeval two thousand and sixteen benchmark datasets outperform baseline approach individually employ either hcf dl base techniques alone
paper tackle problem semantic gap document query within ad hoc information retrieval task context knowledge base kbs already acknowledge valuable mean since allow representation explicit relations entities however necessarily represent implicit relations could hide corpora latter issue tackle recent work deal deep representation learn ing texts mind argue embed kbs within deep neural architectures support documentquery match would give rise fine grain latent representations word semantic relations paper review main approach neural base document rank well approach latent representation entities relations via kbs propose avenues incorporate kbs deep neural approach document rank particularly paper advocate kbs use either support enhance latent representations query document base distributional relational semantics serve semantic translator latent distributional representations
present nn grams novel hybrid language model integrate n grams neural network nn speech recognition model take input word histories well n gram count thus combine memorization capacity scalability n gram model generalization ability neural network report experiment model train 26b word nn grams efficient run time since include output soft max layer model train use noise contrastive estimation nce approach transform estimation problem neural network one binary classification data sample noise sample present result noise sample derive either n gram distribution speech recognition lattices nn grams outperform n gram model italian speech recognition dictation task
text classification dictionaries use define human comprehensible feature propose improvement dictionary feature call smooth dictionary feature feature recognize document contexts instead n grams describe principled methodology solicit dictionary feature teacher present result show model build use human comprehensible feature competitive model train bag word feature
recent work information retrieval ir use deep learn model yield state art result variety ir task deep neural network dnn capable learn ideal representations data train process remove need independently extract feature however structure dnns often tailor perform specific datasets addition ir task deal text vary level granularity single factoids document contain thousands word paper examine role granularity performance common state art dnn structure ir
recent caption model limit ability scale describe concepts unseen pair image text corpora propose novel object captioner noc deep visual semantic caption model describe large number object categories present exist image caption datasets model take advantage external source label image object recognition datasets semantic knowledge extract unannotated text propose minimize joint objective learn diverse data source leverage distributional semantic embeddings enable model generalize describe novel object outside image caption datasets demonstrate model exploit semantic information generate caption hundreds object categories imagenet object recognition dataset observe mscoco image caption train data well many categories observe rarely automatic evaluations human judgements show model considerably outperform prior work able describe many categories object
since introduction word2vec variants widely use learn semantics preserve representations word entities embed space use produce state art result various natural language process task exist implementations aim learn efficiently run multiple thread parallel operate single model share memory ignore incidental memory update collisions show collisions degrade efficiency parallel learn propose straightforward cache strategy improve efficiency factor four
many practical perception systems exist within larger process include interactions users additional components capable evaluate quality predict solutions contexts beneficial provide oracle mechanisms multiple highly likely hypotheses rather single prediction work pose task produce multiple output learn problem ensemble deep network introduce novel stochastic gradient descent base approach minimize loss respect oracle method simple implement agnostic architecture loss function parameter free approach achieve lower oracle error compare exist methods wide range task deep architectures also show qualitatively diverse solutions produce often provide interpretable representations task ambiguity
software system simulate classical collaborative japanese poetry form renga make link haikus use nlp methods wrap web service experiment partial success since result fail satisfy classical constraints gather ideas future work examine relate research semiotics linguistics compute
knowledge base real world facts entities relationships useful resources variety natural language process task however knowledge base typically incomplete useful able perform link prediction knowledge base completion ie predict whether relationship knowledge base likely true paper combine insights several previous link prediction model new embed model stranse represent entity low dimensional vector relation two matrices translation vector stranse simple combination se transe model obtain better link prediction performance two benchmark datasets previous embed model thus stranse serve new baseline complex model link prediction task
consider problem learn distribute representations document data stream document represent low dimensional vectors jointly learn distribute vector representations word tokens use hierarchical framework two embed neural language model particular exploit context document stream use one language model model document sequence model word sequence within model learn continuous vector representations word tokens document semantically similar document word close common vector space discuss extensions model apply personalize recommendation social relationship mine add user layer hierarchy thus learn user specific vectors represent individual preferences validate learn representations public movie rat data set movielens well large scale yahoo news data comprise three months user activity log collect yahoo servers result indicate propose model learn useful representations document word tokens outperform current state art large margin
present transition base parser jointly produce syntactic semantic dependencies learn representation entire algorithm state use stack long short term memories greedy inference algorithm linear time include feature extraction conll two thousand and eight nine english share task obtain best publish parse performance among model jointly learn syntax semantics
present paper show distributional information particularly important consider concept availability implicit language learn condition base result different behavioural experiment argue implicit learnability semantic regularities depend degree relevant concept reflect language use simulations train vector space model either english chinese corpus fee result representations fee forward neural network task neural network find map word representations novel word use datasets four behavioural experiment use different semantic manipulations able obtain learn pattern similar obtain humans
crosslingual word embeddings represent lexical items different languages vector space enable transfer nlp tool however previous attempt expensive resource requirements difficulty incorporate monolingual data unable handle polysemy address drawbacks method take advantage high coverage dictionary style train algorithm monolingual corpora two languages model achieve state art performance bilingual lexicon induction task exceed model use large bilingual corpora competitive result monolingual word similarity cross lingual document classification task
humans refer object environments time especially dialogue people explore generate comprehend natural language refer expressions object image particular focus incorporate better measure visual context refer expression model find visual comparison object within image help improve performance significantly also develop methods tie language generation process together generate expressions object particular category jointly evaluation three recent datasets refcoco refcoco refcocog show advantage methods refer expression generation comprehension
current language model significant limitation ability encode decode factual knowledge mainly acquire knowledge statistical co occurrences although knowledge word rarely observe paper propose neural knowledge language model nklm combine symbolic knowledge provide knowledge graph rnn language model predict whether word generate underlie fact model generate knowledge relate word copy description predict fact experiment show nklm significantly improve performance generate much smaller number unknown word
topics generate topic model usually represent list term alternatively use short phrase image current state art work label topics use image select image rank small set candidates give topic paper present generic method estimate degree association arbitrary pair unseen topic image use deep neural network method better runtime performance ofn compare ofn2 current state art method also significantly accurate
languages genes transmit generation generation opportunity differential reproduction survivorship form apply rigorous inference framework draw population genetics distinguish two broad mechanisms language change drift selection drift change result stochasticity transmission may occur absence intrinsic difference linguistic form whereas selection truly evolutionary force arise intrinsic differences example one form prefer members population use large corpora parse texts span 12th century 21st century analyze three examples grammatical change english regularization past tense verbs rise periphrastic syntactic variation verbal negation show reject stochastic drift favor selective force drive language change others strength drift depend word frequency drift provide alternative explanation word prone change others result suggest important role stochasticity language change provide null model selective theories language evolution must compare
brown cluster hard hierarchical bottom cluster word vocabulary word assign cluster base usage pattern give corpus result cluster hierarchical structure use construct class base language model generate feature use nlp task high computational cost use version brown cluster greedy algorithm use window restrict search space like cluster algorithms brown cluster find sub optimal nonetheless effective map word cluster ability produce high quality human understandable cluster brown cluster see high uptake nlp research community use preprocessing feature generation step little research do towards improve quality brown cluster despite greedy heuristic nature algorithm approach try far focus study effect initialisation similar algorithm tune parameters use define desire number cluster behaviour algorithm include separate parameter differentiate window desire number cluster however approach yield significant improvements cluster quality thesis close analysis brown algorithm provide reveal important specifications weaknesses original algorithm serious effect cluster quality reproducibility research use brown cluster second part thesis two modifications propose finally thorough evaluation perform consider optimization criterion brown cluster performance result class base language model
paper present novel query cluster approach capture broad interest areas users query search engines make use recent advance nlp word2vec extend get query2vec vector representations query base query contexts obtain top search result query use highly scalable divide merge cluster algorithm top query vectors get cluster try approach variety segment include retail travel health phone find cluster effective discover user interest areas high monetization potential
sequence sequence model soft attention significant success machine translation speech recognition question answer though capable easy use require entirety input sequence available begin inference assumption valid instantaneous translation speech recognition address problem present new method solve sequence sequence problems use hard online alignments instead soft offline alignments online alignments model able start produce output without need first process entire input sequence highly accurate online sequence sequence model useful use build accurate voice base instantaneous translator model use hard binary stochastic decisions select timesteps output produce model train produce stochastic decisions use standard policy gradient method experiment show model achieve encourage performance timit wall street journal wsj speech recognition datasets
propose new application quantum compute field natural language process ongoing work field attempt incorporate grammatical structure algorithms compute mean coecke sadrzadeh clark two thousand and ten author introduce model csc model base tensor product composition algorithm many advantage implementation hamper large classical computational resources require work show computational shortcomings csc approach could resolve use quantum computation possibly addition exist techniques dimension reduction address value quantum ram giovannetti2008 model extend algorithm wiebe braun lloyd two thousand and twelve quantum algorithm categorize sentence csc new algorithm demonstrate quadratic speedup classical methods certain condition
winograd schema pair sentence differ single word contain ambiguous pronoun whose referent different two sentence require use commonsense knowledge world knowledge disambiguate paper discuss winograd schemas sentence pair could use challenge machine translation use distinctions pronouns gender appear target language source
one major deficiency semantic representation techniques usually model word type single point semantic space hence conflate mean word address issue learn distinct representations individual mean word subject several research study past years however generate sense representations either link sense inventory unreliable infrequent word sense propose technique tackle problems de conflate representations word base deep knowledge derive semantic network approach provide multiple advantage comparison past work include high coverage ability generate accurate representations even infrequent word sense carry evaluations six datasets across two semantic similarity task report state art result
main approach traditional information retrieval ir examine many word query appear document drawback approach however may fail detect relevant document word query find semantic analysis methods lsa latent semantic analysis lda latent dirichlet allocation propose address issue performance superior compare common ir approach present query document similarity measure motivate word mover distance unlike similarity measure propose method rely neural word embeddings compute distance word process help identify relate word direct match find query document method efficient straightforward implement experimental result trec genomics data show approach outperform bm25 rank function average twelve mean average precision furthermore real world dataset collect pubmed search log combine semantic measure bm25 use learn rank method lead improve rank score twenty-five experiment demonstrate propose approach bm25 nicely complement together produce superior performance
many domains medicine train data short supply case external knowledge often helpful build predictive model propose novel method incorporate publicly available domain expertise build accurate model specifically use word2vec model train domain specific corpus estimate relevance feature text description prediction problem use relevance estimate rescale feature cause important feature experience weaker regularization apply method predict onset five chronic diseases next five years two genders two age group rescale approach improve accuracy model particularly positive examples furthermore method select sixty fewer feature ease interpretation physicians method applicable domains feature outcome descriptions available
article describe result case study apply neural network base optical character recognition ocr scan image book print one thousand, four hundred and eighty-seven one thousand, eight hundred and seventy train ocr engine ocropus breuel2013high ridge herbal text corpus odebrechtetalsubmitted train specific ocr model possible necessary grind truth available error correct diplomatic transcriptions ocr result evaluate accuracy grind truth unseen test set character word accuracies percentage correctly recognize items result machine readable texts individual document range ninety-four ninety-nine character level seventy-six ninety-seven word level include earliest print book think inaccessible ocr methods recently furthermore ocr model train one part corpus consist book different print date different typeset mix model test predictive power book part contain yet fonts mostly yield character accuracies well ninety therefore seem possible construct generalize model train range fonts apply wide variety historical print still give good result moderate postcorrection effort page enable train individual model even better accuracies use method diachronic corpora include early print construct much faster cheaper manual transcription ocr methods report open possibility transform print textual cultural heritage electronic text largely automatic mean prerequisite mass conversion scan book
every day media generate large amount text unbiased view media report require understand political bias media content assistive technology estimate political bias texts helpful context study propose simple statistical learn approach predict political bias text standard text feature extract speeches manifestos political party use predict political bias term political party affiliation term political view result indicate political bias predict chance accuracy mistake model interpret respect change policies political actors two approach present make result interpretable discriminative text feature relate political orientation party b sentiment feature texts correlate measure political power political power appear strongly correlate positive sentiment text highlight potential use case web application show model use texts political bias clear news article
lambek calculus consider version non commutative intuitionistic linear logic one interest feature lambek calculus call lambek restriction antecedent provable sequent non empty paper discuss ways extend lambek calculus linear logic exponential modality keep lambek restriction interestingly enough show system equip reasonable exponential modality follow hold system enjoy cut elimination substitution full extent system necessarily violate lambek restriction nevertheless show two three condition implement namely design system lambek restriction cut elimination another system lambek restriction substitution calculi prove undecidable even take one two divisions provide lambek calculus system cut elimination substitution without lambek restriction folklore know undecidable
recent years identity vector vector base speaker verification sv systems become successful nevertheless environmental noise speech duration variability still significant effect degrade performance systems many real life applications duration record short result extract vectors reliably represent attribute speaker investigate effect speech duration performance three state art speaker recognition systems addition use variety available score fusion methods investigate effect score fusion speaker verification techniques benefit performance difference different methods different enrollment test speech duration condition technique perform significantly better baseline score fusion methods
tremendous growth social media content internet inspire development text analytics understand solve real life problems leverage statistical topic model help researchers practitioners better comprehension textual content well provide useful information analysis statistical topic model become especially important work large volumes dynamic text eg facebook twitter datasets study summarize message content four data set twitter message relate challenge social events kenya use latent dirichlet allocation lda topic model analyze content study use two evaluation measure normalize mutual information nmi topic coherence analysis select best lda model obtain lda result show tool effectively use extract discussion topics summarize manual analysis
many domain adaptation approach rely learn cross domain share representations transfer knowledge learn one domain domains traditional domain adaptation consider adapt one task paper explore multi task representation learn domain adaptation scenario propose neural network framework support domain adaptation multiple task simultaneously learn share representations better generalize domain adaptation apply propose framework domain adaptation sequence tag problems consider two task chinese word segmentation name entity recognition experiment show multi task domain adaptation work better disjoint domain adaptation task achieve state art result task social media domain
describe tweetime temporal tagger recognize normalize time expressions twitter previous work social media analysis rely temporal resolvers design well edit text therefore suffer reduce performance due domain mismatch present minimally supervise method learn large quantities unlabeled data require hand engineer rule hand annotate train corpora tweetime achieve sixty-eight f1 score end end task resolve date expressions outperform broad range state art systems
paper explore task translate natural language query regular expressions embody mean contrast prior work propose neural model utilize domain specific craft learn translate directly parallel corpus fully explore potential neural model propose methodology collect large corpus regular expression natural language pair result model achieve performance gain one hundred and ninety-six previous state art model
discover underlie structure present large real world graph fundamental scientific problem paper show graph clique tree use extract hyperedge replacement grammar store order extraction process extract graph grammar guarantee generate isomorphic copy original graph stochastic application graph grammar rule use quickly create random graph experiment large real world network show random graph generate extract graph grammars exhibit wide range properties similar original graph addition graph properties like degree eigenvector centrality graph look like ultimately depend small detail local graph substructures difficult define global level show generative graph model able preserve local substructures generate new graph perform well new difficult test model robustness
paper describe open source software system automatic conversion nlp event representations system biology structure data interchange format sbml biopax part larger effort make result nlp community available system biology pathway modelers
paper evaluate difference human pathway curation current nlp systems propose graph analysis methods quantify gap human curated pathway map output state art automatic nlp systems evaluation perform popular mtor pathway base analyze current systems perform well fail identify possible avenues progress
lambek calculus well know logical formalism model natural language syntax original calculus cover substantial number intricate natural language phenomena restrict context free set order address subtle linguistic issue lambek calculus extend various ways particular morrill valentin two thousand and fifteen introduce extension call exponential bracket modalities extension base non standard contraction rule exponential interact bracket structure intricate way standard contraction rule admissible calculus paper prove undecidability derivability problem calculus also investigate restrict decidable fragment consider morrill valentin show fragment belong np class
semantic error detection correction important task applications fact check speech text grammatical error correction current approach generally focus relatively shallow semantics account numeric quantities approach use language model ground number within text ground easily achieve recurrent neural language model architectures condition incomplete background knowledge base evaluation clinical report show numerical ground improve perplexity thirty-three f1 semantic error correction five point compare ungrounded approach condition knowledge base yield improvements
paper present name entity extraction system detect attribute product title ecommerce retailers like walmart absence syntactic structure short piece text make extract attribute value challenge problem find combine sequence label algorithms conditional random field structure perceptron curated normalization scheme produce effective system task extract product attribute value title keep discussion concrete illustrate mechanics system point view particular attribute brand also discuss importance attribute extraction system context retail websites large product catalog compare approach potential approach problem end paper discussion performance system extract attribute
neural machine translation aim build single large neural network train maximize translation performance encoder decoder architecture attention mechanism achieve translation performance comparable exist state art phrase base systems task english french translation however use large vocabulary become bottleneck train improve performance paper propose efficient architecture train deep character level neural machine translation introduce decimator interpolator decimator use sample source sequence encode interpolator use resample decode deep model two major advantage avoid large vocabulary issue radically time much faster memory efficient train conventional character base model interestingly model able translate misspell word like human be
many social media platforms offer mechanism readers react comment positively negatively aggregate think community endorsement paper address problem predict community endorsement online discussions leverage participant response structure text comment different type feature integrate neural network use novel architecture learn latent modes discussion structure perform well deep neural network interpretable addition latent modes use weight text feature thereby improve prediction accuracy
picture worth thousand word recently however notice success stories understand visual scenes model able detect name object describe attribute recognize relationships interactions paper propose phrase base hierarchical long short term memory phi lstm model generate image description propose model encode sentence sequence combination phrase word instead sequence word alone conventional solutions two level model dedicate learn generate image relevant noun phrase ii produce appropriate image description phrase word corpus adopt convolutional neural network learn image feature lstm learn word sequence sentence propose model show better competitive result comparison state art model flickr8k flickr30k datasets
word embeddings currently predominant natural language process exist model learn solely contexts however context base word embeddings limit since word mean learn base context moreover also difficult learn representation rare word due data sparsity problem work address issue learn representations word integrate intrinsic descriptive extrinsic contextual information prove effectiveness model evaluate four task include word similarity reverse dictionarieswiki link prediction document classification experiment result show model powerful word document model
political scientists often find track amendments political texts different actors weigh texts change draft redrafted reflect political preferences power study provide novel solution prob lem detect amendments political text base upon minimum edit distance demonstrate usefulness two language insensitive transparent efficient minimum edit distance algorithms suit task algorithms capable provide account type insertions deletions substitutions trans position substantive amount amendments make version texts illustrate usefulness efficiency approach replicate two exist stud ies field legislative study result demonstrate minimum edit distance methods produce superior measure text amendments hand cod efforts fraction time resource cost
automatically test web applications crawl base techniques usually adopt mine behavior model explore state space detect violate invariants applications however exist crawlers rule identify topics input text field login ids passwords email date phone number manually configure moreover rule one application often suitable another addition several rule conflict match input text field one topics difficult determine rule suggest better match paper present natural language approach automatically identify topics encounter input field crawl semantically compare similarities input field label corpus evaluation one hundred real world form propose approach demonstrate comparable performance rule base one experiment also show accuracy rule base approach improve nineteen integrate approach
lexical query model lead paradigm session search paper analyze trec session query log compare performance different lexical match approach session search naive methods base term frequency weigh perform par specialize session model addition investigate viability lexical query model set session search give important insights potential limitations lexical query model session search propose future directions field session search
paper introduce new measure call termclass relevance compute relevancy term classify document particular class propose measure estimate degree relevance give term place unlabeled document member know class product classterm weight classterm density classterm weight ratio number document class contain term total number document contain term classterm density relative density occurrence term class total occurrence term entire population unlike exist term weight scheme tf idf variants propose relevance measure take account degree relative participation term across document class entire population demonstrate significance propose measure experimentation conduct twenty newsgroups dataset superiority novel measure bring comparative analysis
machine comprehension text important problem natural language process recently release dataset stanford question answer dataset squad offer large number real question answer create humans crowdsourcing squad provide challenge testbed evaluate machine comprehension algorithms partly compare previous datasets squad answer come small set candidate answer variable lengths propose end end neural architecture task architecture base match lstm model propose previously textual entailment pointer net sequence sequence model propose vinyals et al2015 constrain output tokens input sequence propose two ways use pointer net task experiment show two model substantially outperform best result obtain rajpurkar et al2016 use logistic regression manually craft feature
thesis study problem recognize video sequence fingerspell letter american sign language asl fingerspell comprise significant relatively understudy part asl recognize challenge number reason involve quick small motion often highly coarticulated exhibit significant variation signers dearth continuous fingerspell data collect work propose several type recognition approach explore signer variation problem best perform model segmental semi markov conditional random field use deep neural network base feature signer dependent set recognizers achieve eight letter error rat signer independent set much challenge neural network adaptation achieve seventeen letter error rat
constant growth world wide web number document different languages accordingly need reliable language detection tool increase well platforms twitter predominantly short texts become important information resources additionally impose need short texts language detection algorithms paper show incorporate personalize user specific information language detection algorithm lead important improvement detection result choose best algorithm language detection short text message investigate several machine learn approach approach include use well know classifiers svm logistic regression dictionary base approach probabilistic model base modify kneser ney smooth furthermore extension probabilistic model include additional user specific information evidence accumulation per user user interface language explore goal improve classification performance propose approach evaluate randomly collect twitter data contain latin well non latin alphabet languages quality obtain result compare follow selection best perform algorithm algorithm evaluate two already exist general language detection tool chromium compact language detector two cld2 langid method significantly outperform result achieve mention methods additionally preview benefit possible applications reliable language detection algorithm give
instead study properties social relationship objective view paper focus individuals subjective asymmetric opinions interrelationships inspire theories sociolinguistics investigate two individuals opinions interrelationship interactive language feature eliminate difference personal language style clarify asymmetry interactive language feature value indicate individuals asymmetric opinions interrelationship also discuss degree opinions asymmetry relate individuals personality traits furthermore measure individuals asymmetric opinions interrelationship concretely develop novel model synthetizing interactive language social network feature experimental result enron email dataset provide multiple evidence asymmetric opinions interrelationship also verify effectiveness propose model measure degree opinions asymmetry
though current research often study properties online social relationship objective view also need understand individuals subjective opinions interrelationships social compute study inspire theories sociolinguistics latest work indicate interactive language reveal individuals asymmetric opinions interrelationship work order explain opinions asymmetry interrelationship latent factor extend investigation single relationship structural context online social network analyze correlation interactive language feature structural context interrelationships structural context vertex edge triangles social network consider statistical analysis enron email dataset find individuals opinions measure interactive language feature interrelationship relate important structural context social network result help us understand measure individuals opinions interrelationship intrinsic information
propose language agnostic way automatically generate set semantically similar cluster entities along set outlier elements may use perform intrinsic evaluation word embeddings outlier detection task use methodology create gold standard dataset call wikisem500 evaluate multiple state art embeddings result show correlation performance dataset performance sentiment analysis
transfer multi task learn traditionally focus either single source target pair similar task ideally linguistic level morphology syntax semantics would benefit train single model introduce joint many task model together strategy successively grow depth solve increasingly complex task higher layer include shortcut connections lower level task predictions reflect linguistic hierarchies use simple regularization term allow optimize model weight improve one task loss without exhibit catastrophic interference task single end end model obtain state art competitive result five different task tag parse relatedness entailment task
several deep learn model propose question answer however due single pass nature way recover local maxima correspond incorrect answer address problem introduce dynamic coattention network dcn question answer dcn first fuse co dependent representations question document order focus relevant part dynamic point decoder iterate potential answer span iterative procedure enable model recover initial local maxima correspond incorrect answer stanford question answer dataset single dcn model improve previous state art seven hundred and ten f1 seven hundred and fifty-nine dcn ensemble obtain eight hundred and four f1
paper present technique train neural network model small amount data current methods train neural network small amount rich data typically rely strategies fine tune pre train neural network use domain specific hand engineer feature take approach treat network layer entire network modules combine pre train modules untrained modules learn shift distributions data set central impact use modular approach come add new representations network oppose replace representations via fine tune use technique able surpass result use standard fine tune transfer learn approach also able significantly increase performance approach use smaller amount data
previous work combine word level character level representations use concatenation scalar weight suboptimal high level task like read comprehension present fine grain gate mechanism dynamically combine word level character level representations base properties word also extend idea fine grain gate model interaction question paragraph read comprehension experiment show approach improve performance read comprehension task achieve new state art result children book test dataset demonstrate generality gate mechanism also show improve result social media tag prediction task
paper build recent work kiperwasser goldberg two thousand and sixteen use neural attention simple graph base dependency parser use larger thoroughly regularize parser recent bilstm base approach biaffine classifiers predict arc label parser get state art near state art performance standard treebanks six different languages achieve nine hundred and fifty-seven uas nine hundred and forty-one las popular english ptb dataset make highest perform graph base parser benchmark outperform kiperwasser goldberg two thousand and sixteen eighteen twenty-two comparable highest perform transition base parser kuncoro et al two thousand and sixteen achieve nine hundred and fifty-eight uas nine hundred and forty-six las also show hyperparameter choices significant effect parse accuracy allow us achieve large gain graph base approach
many nlp task include machine comprehension answer selection text entailment require comparison sequence match important units sequence key solve problems paper present general compare aggregate framework perform word level match follow aggregation use convolutional neural network particularly focus different comparison function use match two vectors use four different datasets evaluate model find simple comparison function base element wise operations work better standard neural network neural tensor network
paper present domain adaptation technique formant estimation use deep network first train deep learn network small read speech dataset freeze parameters train network use several different datasets train adaptation layer make obtain network universal sense work well variety speakers speech domains different characteristics evaluate adapt network three datasets different speaker characteristics speech style performance method compare favorably alternative methods formant estimation
truth discovery resolve conflict find truth multiple source statements conventional methods mostly research base mutual effect reliability source credibility statements however pay attention mutual effect among credibility statements object propose memory network base model incorporate two ideas truth discovery use feedforward memory network feedback memory network learn representation credibility statements object specially adopt memory mechanism learn source reliability use truth prediction learn model use multiple type data categorical data continuous data assign different weight automatically loss function base effect truth discovery prediction experiment result show memory network base model much outperform state art method baseline methods
automatic speech recognition asr allow natural intuitive interface robotic educational applications children however number challenge overcome allow interface operate robustly realistic settings include intrinsic difficulties recognise child speech high level background noise often present classrooms part eu easel project provide several contributions address challenge implement asr module use robotics applications use latest deep neural network algorithms provide leap performance traditional gmm approach apply data augmentation methods improve robustness noise speaker variation provide close integration asr module rest dialogue system allow asr receive real time language model relevant current section dialogue greatly improve accuracy integrate asr module interactive multimodal system use small humanoid robot help children learn exercise energy system instal public museum event part research study three hundred and twenty children age three fourteen interact robot asr achieve ninety accuracy fluent near fluent speech
article automate system propose essay score arabic language online exams base stem techniques levenshtein edit operations online exam develop propose mechanisms exploit capabilities light heavy stem implement online grade system show efficient tool automate score essay question
manuscript present experiment implementation machine translation system mapreduce model empirical evaluation do use fully implement translation systems embed mapreduce program model two machine translation paradigms study shallow transfer rule base machine translation statistical machine translation result show mapreduce model successfully use increase throughput machine translation system furthermore method enhance throughput machine translation system without decrease quality translation output thus present manuscript also represent contribution seminal work natural language process specifically machine translation first point toward importance definition metric throughput translation system second applicability machine translation task mapreduce paradigm
vocabulary mismatch problem long stand problem information retrieval semantic match hold promise solve problem recent advance language technology give rise unsupervised neural model learn representations word well bigger textual units representations enable powerful semantic match methods survey mean introduction use neural model semantic match remain focus limit web search detail require background terminology taxonomy group rapidly grow body work area survey work neural model semantic match context three task query suggestion ad retrieval document retrieval include section resources best practice believe help readers new area conclude assessment state art suggestions future work
paper test hypothesis distinctive feature classifiers anchor phonetic landmarks transfer cross lingually without loss accuracy three consonant voice classifiers develop one manually select acoustic feature anchor phonetic landmark two mfccs either average across segment anchor landmark and3 acoustic feature compute use convolutional neural network cnn detectors train english data timitand test english turkish spanish performance measure use f1 accuracy experiment demonstrate manual feature outperform mfcc classifiers cnnfeatures outperform mfcc base classifiers suffer f1reduction sixteen absolute generalize english languages manual feature suffer five f1 reductionand cnn feature actually perform better turkish span ish train language demonstrate feature capable represent long term spectral dynamics cnn landmark base feature able generalize cross lingually little loss accuracy
show generalize gibbs shannon entropies provide new insights statistical properties texts universal distribution word frequencies zipf law imply generalize entropies compute word level dominate word specific range frequencies show case generalize entropies also generalize jensen shannon divergences use compute similarity different texts find allow us identify contribution specific word word frequencies different generalize entropies also estimate size databases need obtain reliable estimation divergences test result large databases book google n gram database scientific paper index web science
many recent work demonstrate benefit knowledge graph embeddings complete monolingual knowledge graph inasmuch relate knowledge base build several different languages achieve cross lingual knowledge alignment help people construct coherent knowledge base assist machine deal different expressions entity relationships across diverse human languages unfortunately achieve highly desirable crosslingual alignment human labor costly errorprone thus propose mtranse translation base model multilingual knowledge graph embeddings provide simple automate solution encode entities relations language separate embed space mtranse provide transition embed vector cross lingual counterparts space preserve functionalities monolingual embeddings deploy three different techniques represent cross lingual transition namely axis calibration translation vectors linear transformations derive five variants mtranse use different loss function model train partially align graph small portion triple align cross lingual counterparts experiment cross lingual entity match triple wise alignment verification show promise result variants consistently outperform others different task also explore mtranse preserve key properties monolingual counterpart transe
propose simple solution use single neural machine translation nmt model translate multiple languages solution require change model architecture base system instead introduce artificial token begin input sentence specify require target language rest model include encoder decoder attention remain unchanged share across languages use share wordpiece vocabulary approach enable multilingual nmt use single model without increase parameters significantly simpler previous proposals multilingual nmt method often improve translation quality involve language pair even keep total number model parameters constant wmt fourteen benchmarks single multilingual model achieve comparable performance englishrightarrowfrench surpass state art result englishrightarrowgerman similarly single multilingual model surpass state art result frenchrightarrowenglish germanrightarrowenglish wmt fourteen wmt fifteen benchmarks respectively production corpora multilingual model twelve language pair allow better translation many individual pair addition improve translation quality language pair model train model also learn perform implicit bridge language pair never see explicitly train show transfer learn zero shoot translation possible neural translation finally show analyse hint universal interlingua representation model show interest examples mix languages
study entropy chinese english texts base character case chinese texts base word languages significant differences find languages different personal style debate partner entropy analysis point direction lower entropy higher complexity text analysis would apply individuals different style single individual different age well different group population
topological argument present concering structure semantic space base negative correlation polysemy word length result graph structure apply model free recall experiment result predictions comparative value recall probabilities associative recall find favor longer word whereas sequential recall find favor shorter word data peer experiment lohnas et al two thousand and fifteen healey kahana two thousand and sixteen confirm predictons correlation coefficients rseq seventeen rass seventeen argument apply predict global properties list recall lead novel explanation word length effect base optimization retrieval strategies
research social media analysis experience recent surge large number work apply representation learn model solve high level syntactico semantic task sentiment analysis semantic textual similarity computation hashtag prediction although performance representation learn model better traditional baselines task little know core properties tweet encode within representations understand core properties would empower us make generalizable conclusions quality representations work present constitute first step open black box vector embed social media post emphasis tweet particular order understand core properties encode tweet representation evaluate representations estimate extent model properties tweet length presence word hashtags mention capitalization do help multiple classifiers take representation input essentially classifier evaluate one syntactic social properties arguably salient tweet also first holistic study extensively analyse ability encode properties wide variety tweet representation model include traditional unsupervised methods bow lda unsupervised representation learn methods siamese cbow tweet2vec well supervise methods cnn blstm
lstms become basic build block many deep nlp model recent years many improvements variations propose deep sequence model general lstms particular propose analyze series augmentations modifications lstm network result improve performance text classification datasets observe compound improvements traditional lstms use monte carlo test time model average average pool residual connections along four suggest modifications analysis provide simple reliable high quality baseline model
visual narrative often combination explicit information judicious omissions rely viewer supply miss detail comics movements time space hide gutter panel follow story readers logically connect panel together infer unseen action process call closure computers describe explicitly depict natural image paper examine whether understand closure drive narratives convey stylize artwork dialogue comic book panel construct dataset comics consist twelve million panel one hundred and twenty gb pair automatic textbox transcriptions depth analysis comics demonstrate neither text image alone tell comic book story computer must understand modalities keep plot introduce three cloze style task ask model predict narrative character centric aspects panel give n precede panel context various deep neural architectures underperform human baselines task suggest comics contain fundamental challenge vision language
neural network base model powerful tool create word embeddings objective model group similar word together embeddings use feature improve result various applications document classification name entity recognition etc neural language model able learn word representations use capture semantic shift across time geography objective paper first identify visualize word change mean different text corpus train neural language model texts diverse set discipline philosophy religion fiction etc text alter embeddings word represent mean word inside text present computational technique detect word exhibit significant linguistic shift mean usage use enhance scatterplots storyline visualization visualize linguistic shift
paper introduce key value memory network multimodal set novel key address mechanism deal sequence sequence model propose model naturally decompose problem video caption vision language segment deal key value pair specifically learn semantic embed v correspond frame k video thereby create k v memory slot propose find next step attention weight condition previous attention distributions key value memory slot memory address schema exploit flexibility framework additionally capture spatial dependencies map visual semantic embed experiment do youtube2text dataset demonstrate usefulness recurrent key address achieve competitive score bleu4 meteor metrics state art model
recent progress image caption make possible generate novel sentence describe image natural language compress image single sentence describe visual content coarse detail one new caption approach dense caption potentially describe image finer level detail caption many regions within image turn unable produce coherent story image paper overcome limitations generate entire paragraph describe image tell detail unify stories develop model decompose image paragraph constituent part detect semantic regions image use hierarchical recurrent neural network reason language linguistic analysis confirm complexity paragraph generation task thorough experiment new dataset image paragraph pair demonstrate effectiveness approach
work address issue generic automate disease incidence monitor twitter employ ontology disease relate concepts use obtain conceptual representation tweet unlike previous key word base systems topic model approach ontological approach allow us apply stringent criteria determine message relevant spatial temporal characteristics whilst give stronger guarantee result model perform well new data may lexically divergent achieve train learners concepts rather individual word train use dataset contain mention influenza listeria use learn model classify datasets contain mention arbitrary selection diseases show ontological approach achieve good performance task use variety natural language process techniques also show word vectors learn directly concepts achieve even better result
model coherent conversation continuation via rnn base dialogue model equip dynamic attention mechanism attention rnn language model dynamically increase scope attention history conversation continue oppose standard attention alignment model fix input scope sequence sequence model allow generate word associate relevant word correspond conversation history evaluate model two popular dialogue datasets open domain movietriples dataset close domain ubuntu troubleshoot dataset achieve significant improvements state art baselines several metrics include complementary diversity base metrics human evaluation qualitative visualizations also show vanilla rnn dynamic attention outperform complex memory model eg lstm gru allow flexible long distance memory promote coherence via topic model base reranking
deep learn approach widely apply sequence model problems term automatic speech recognition asr performance significantly improve increase large speech corpus deeper neural network especially recurrent neural network deep convolutional neural network apply asr successfully give arise problem train speed build novel deep recurrent convolutional network acoustic model apply deep residual learn experiment show faster convergence speed better recognition accuracy traditional deep convolutional recurrent network experiment compare convergence speed novel deep recurrent convolutional network traditional deep convolutional recurrent network faster convergence speed novel deep recurrent convolutional network reach comparable performance show apply deep residual learn boost convergence speed novel deep recurret convolutional network finally evaluate experimental network phoneme error rate per propose bidirectional statistical n gram language model evaluation result show newly propose deep recurrent convolutional network apply deep residual learn reach best per one thousand, seven hundred and thirty-three fastest convergence speed timit database outstanding performance novel deep recurrent convolutional neural network deep residual learn indicate potentially adopt sequential problems
previous model video caption often use output specific layer convolutional neural network cnn video feature however variable context dependent semantics video may make appropriate adaptively select feature multiple cnn layer propose new approach generate adaptive spatiotemporal representations videos caption task novel attention mechanism develop adaptively sequentially focus different layer cnn feature level feature abstraction well local spatiotemporal regions feature map layer propose approach evaluate three benchmark datasets youtube2text vad msr vtt along visualize result model work experiment quantitatively demonstrate effectiveness propose adaptive spatiotemporal feature abstraction translate videos sentence rich semantics
propose new encoder decoder approach learn distribute sentence representations applicable multiple purpose model learn use convolutional neural network encoder map input sentence continuous vector use long short term memory recurrent neural network decoder several task consider include sentence reconstruction future sentence prediction hierarchical encoder decoder model propose encode sentence predict multiple future sentence train model large collection novels obtain highly generic convolutional sentence encoder perform well practice experimental result several benchmark datasets across broad range applications demonstrate superiority propose model compete methods
recurrent neural network rnns show promise performance language model however traditional train rnns use back propagation time often suffer overfitting one reason stochastic optimization use large train set provide good estimate model uncertainty paper leverage recent advance stochastic gradient markov chain monte carlo also appropriate large train set learn weight uncertainty rnns yield principled bayesian learn algorithm add gradient noise train enhance exploration model parameter space model average test extensive experiment various rnn model across broad range applications demonstrate superiority propose approach stochastic optimization
community base question answer service arise popular knowledge share pattern netizens abundant interactions among users individuals capable obtain satisfactory information however effective users attain answer within minutes users check progress time satisfy answer submit address problem user personalize satisfaction prediction task exist methods usually exploit manual feature selection desirable require careful design labor intensive paper settle issue develop new multiple instance deep learn framework specifically settings question follow weakly supervise learn multiple instance learn assumption obtain answer regard instance set define question resolve least one satisfactory answer thus design efficient framework exploit multiple instance learn property deep learn model question answer pair extensive experiment large scale datasets stack exchange demonstrate feasibility propose framework predict askers personalize satisfaction framework extend numerous applications ui satisfaction prediction multi arm bandit problem expert find
community base question answer platforms attract substantial users share knowledge learn rapid enlargement cqa platforms quantities overlap question emerge make users confound select proper reference urgent us take effective automate algorithms reuse historical question correspond answer paper focus problem question retrieval aim match historical question relevant semantically equivalent resolve one query directly challenge task lexical gap question word ambiguity word mismatch problem furthermore limit word query sentence sparsity word feature alleviate challenge propose novel framework name hnil encode question content also askers social interactions enhance question embed performance specifically apply random walk base learn method recurrent neural network match similarities askers question historical question propose users extensive experiment large scale dataset real world cqa site show employ heterogeneous social network information outperform state art solutions task
study polysemy potential learn bias vocabulary learn children word low polysemy could prefer reduce disambiguation effort listener however preference could side effect another bias preference children nouns combination lower polysemy nouns respect part speech categories result show mean polysemy children increase time two phase ie fast growth till 31st month follow slower tendency towards adult speech contrast evolution find adults interact children suggest children preference non polysemous word early stag vocabulary acquisition interestingly evolutionary pattern describe weaken control syntactic category noun verb adjective adverb disappear completely suggest could result acombination standalone bias low polysemy preference nouns
stochastic model short term verbal memory propose psychological state subject encode instantaneous position particle diffuse semantic graph probabilistic structure model particularly suitable study dependence free recall observables semantic properties word recall besides predict well know experimental feature contiguity effect forward asymmetry word length effect novel prediction obtain relationship contiguity effect syllabic length word shorter word way wider semantic range predict characterize stronger forward contiguity fresh analysis archival data allow confirm prediction
statistical properties letter frequencies european literature texts investigate determination logarithmic dependence letter sequence one language two language texts examine pare languages suggest voynich manuscript internal structure manuscript consider spectral portraits two letter distribution construct
many natural language generation task abstractive summarization text simplification paraphrase orientate task copy rewrite two main write modes previous sequence sequence seq2seq model use single decoder neglect fact paper develop novel seq2seq model fuse copy decoder restrict generative decoder copy decoder find position copy base typical attention model generative decoder produce word limit source specific vocabulary combine two decoders determine final output develop predictor predict mode copy rewrite predictor guide actual write mode train data conduct extensive experiment two different paraphrase datasets result show model outperform state art approach term informativeness language quality
develop far multi document summarization reach bottleneck due lack sufficient train data diverse categories document text classification make deficiencies paper propose novel summarization system call tcsum leverage plentiful text classification data improve performance multi document summarization tcsum project document onto distribute representations act bridge text classification summarization also utilize classification result produce summaries different style extensive experiment duc generic multi document summarization datasets show tcsum achieve state art performance without use hand craft feature capability catch variations summary style respect different text categories
introduce large scale machine read comprehension dataset name ms marco dataset comprise one million, ten thousand, nine hundred and sixteen anonymized question sample bing search query log human generate answer one hundred and eighty-two thousand, six hundred and sixty-nine completely human rewrite generate answer addition dataset contain eight million, eight hundred and forty-one thousand, eight hundred and twenty-three passages extract three million, five hundred and sixty-three thousand, five hundred and thirty-five web document retrieve bing provide information necessary curating natural language answer question ms marco dataset may multiple answer answer use dataset propose three different task vary level difficulty predict question answerable give set context passages extract synthesize answer human would ii generate well form answer possible base context passages understand question passage context finally iii rank set retrieve passages give question size dataset fact question derive real user search query distinguish ms marco well know publicly available datasets machine read comprehension question answer believe scale real world nature dataset make attractive benchmarking machine read comprehension question answer model
analyse sentiment tweet important help determine users opinion know people opinion crucial several purpose start gather knowledge customer base e governance campaign many report aim develop system detect sentiment tweet employ several linguistic feature along external source information detect sentiment tweet show augment one hundred and forty character long tweet information harvest external urls share tweet well social media feature enhance sentiment prediction accuracy significantly
classify products categories precisely efficiently major challenge modern e commerce high traffic new products upload daily dynamic nature categories raise need machine learn model reduce cost time human editors paper propose decision level fusion approach multi modal product classification use text image input train input specific state art deep neural network input source show potential forge together multi modal architecture train novel policy network learn choose finally demonstrate multi modal network improve top one accuracy network real world large scale product classification dataset collect fromwalmartcom focus image text fusion characterize e commerce domains algorithms easily apply modalities audio video physical sensors etc
study methods automate parse informal mathematical expressions formal ones main prerequisite deep computer understand informal mathematical texts propose context base parse approach combine efficient statistical learn deep parse tree semantic prune type check large theory automate theorem prove show methods significantly improve previous result parse theorems flyspeck corpus
important aspect develop conversational agents give bot ability improve communicate humans learn mistake make research focus learn fix train set label data rather interact dialogue partner online fashion paper explore direction reinforcement learn set bot improve question answer ability feedback teacher give follow generate responses build simulator test various aspects learn synthetic environment introduce model work regime finally real experiment mechanical turk validate approach
present newsqa challenge machine comprehension dataset one hundred thousand human generate question answer pair crowdworkers supply question answer base set ten thousand news article cnn answer consist span text correspond article collect dataset four stage process design solicit exploratory question require reason thorough analysis confirm newsqa demand abilities beyond simple word match recognize textual entailment measure human performance dataset compare several strong neural model performance gap humans machine one hundred and ninety-eight f1 indicate significant progress make newsqa future research dataset freely available https datasetsmaluubacom newsqa
prof robert berwick abstract forthcoming invite talk acl2016 workshop cognitive aspects computational language learn revive ancient debate entitle take chance berwick seem refer implicitly chomsky critique statistical approach harris well currently dominant paradigms conll berwick avoid chomsky use innate state debate existence sophisticate mental grammars settle chomsky logical structure linguistic theory one thousand, nine hundred and fifty-seven one thousand, nine hundred and seventy-five acknowledge debate often revive paper agree view debate long since settle opposite outcome give embers yet die away question remain fundamental perhaps appropriate refuel debate would like join bob throw fuel fire review evidence chomskian position
short text message tweet noisy sparse use vocabulary traditional textual representations tf idf difficulty grasp semantic mean texts important applications event detection opinion mine news recommendation etc construct method base semantic word embeddings frequency information arrive low dimensional representations short texts design capture semantic similarity purpose design weight base model learn procedure base novel median base loss function paper discuss detail model optimization methods together experimental result wikipedia twitter data find method outperform baseline approach experiment generalize well different word embeddings without retrain method therefore capable retain semantic information text applicable box
use neural network generate reply human computer dialogue systems attract increase attention past years however performance satisfactory neural network tend generate safe universally relevant reply carry little mean paper propose content introduce approach neural network base generative dialogue systems first use pointwise mutual information pmi predict noun keyword reflect main gist reply propose seq2bf sequence backward forward sequence model generate reply contain give keyword experimental result show approach significantly outperform traditional sequence sequence model term human evaluation entropy measure predict keyword appear appropriate position reply
introduce deep neural network automate sarcasm detection recent work emphasize need model capitalize contextual feature beyond lexical syntactic cue present utterances example different speakers tend employ sarcasm regard different subject thus sarcasm detection model ought encode speaker information current methods achieve way laborious feature engineer contrast propose automatically learn exploit user embeddings use concert lexical signal recognize sarcasm approach require elaborate feature engineer concomitant data scrap fit user embeddings require text previous post experimental result show model outperform state art approach leverage extensive set carefully craft feature
entity resolution problem identify underlie entity reference find data research many decades many communities common theme research importance incorporate relational feature resolution process relational entity resolution particularly important knowledge graph kgs regular structure capture entities interrelationships identify three major problems kg entity resolution one intra kg reference ambiguity two inter kg reference ambiguity three ambiguity extend kgs new facts implement framework generalize across three settings exploit regular structure kgs framework many advantage custom solutions widely deploy industry include collective inference scalability interpretability apply framework two real world kg entity resolution problems ambiguity nell merge data freebase musicbrainz demonstrate importance relational feature
ontologies one core foundations semantic web participate semantic web project domain experts need able understand ontologies involve visual notations provide overview ontology help users understand connections among entities however users first need learn visual notation interpret correctly control natural language representation would readable right away might prefer case complex axioms however structure ontology would remain less apparent propose combine ontology visualizations contextual ontology verbalizations select ontology diagram elements display control natural language cnl explanations owl axioms correspond select visual notation elements thus domain experts benefit high level overview provide graphical notation detail textual explanations particular elements diagram
paper propose effective way bias attention mechanism sequence sequence neural machine translation nmt model towards well study statistical word alignment model show novel guide alignment train approach improve translation quality real life e commerce texts consist product title descriptions overcome problems pose many unknown word large type token ratio also show meta data associate input texts topic category information significantly improve translation quality use additional signal decoder part network novel feature bleu score nmt system product title set improve one hundred and eighty-six two hundred and thirteen even larger mt quality gain obtain domain adaptation general domain nmt system e commerce data develop nmt system also perform well iwslt speech translation task ensemble four variant systems outperform phrase base baseline twenty-one bleu absolute
public administration produce translation data provider useful reusable data meet translation need ones public organizations private company work texts domain data also crucial produce domain tune machine translation systems organization management translation process characteristics archive generate resources infrastructure available support determine efficiency effectiveness materials produce convert reusable data however utmost importance organizations first become aware goods produce second adapt internal process become optimal providers article propose maturity model help organizations achieve identify different stag management translation data determine path aforementioned goal
several study sentence process suggest mental lexicon keep track mutual expectations word current dsms however represent context word separate feature thereby loose important information word expectations word interrelations paper present dsm address issue define verb contexts joint syntactic dependencies test representation verb similarity task two datasets show joint contexts achieve performances comparable single dependencies even better moreover able overcome data sparsity problem joint feature space spite limit size train corpus
read comprehension embrace boom recent nlp research several institute release cloze style read comprehension data greatly accelerate research machine comprehension work firstly present chinese read comprehension datasets consist people daily news dataset children fairy tale cft dataset also propose consensus attention base neural network architecture tackle cloze style read comprehension problem aim induce consensus attention every word query experimental result show propose neural network significantly outperform state art baselines several public datasets furthermore setup baseline chinese read comprehension task hopefully would speed process future research
type base compositional distributional semantic model present interest line research functional representations linguistic mean one drawbacks model however lack train data require train word type combination paper address introduce train methods share parameters similar word show methods enable zero shoot learn word train data well enable construction high quality tensors train examples per word
rapid increase internet users along grow power online review sit social media give birth sentiment analysis opinion mine aim determine people think comment sentiments opinions contain public generate content products service policies politics people usually interest seek positive negative opinions contain like dislike share users feature particular product service paper propose sentence level lexical base domain independent sentiment classification method different type data review blog propose method base general lexicons ie wordnet sentiwordnet user define lexical dictionaries semantic orientation relations gloss dictionaries provide solution domain portability problem method perform better word text level corpus base machine learn methods semantic orientation result show propose method perform better show precision eighty-seven and83 document sentence level respectively online comment
work apply word embeddings neural network long short term memory lstm text classification problems classification criteria decide context application examine two applications particular first actionability build model classify social media message customers service providers actionable non actionable build model thirty different languages actionability model achieve accuracy around eighty-five reach ninety accuracy also show use lstm neural network word embeddings vastly outperform traditional techniques second explore classification message respect political lean social media message classify democratic republican model able classify message high accuracy eight thousand, seven hundred and fifty-seven part experiment vary different hyperparameters neural network report effect variation accuracy actionability model deploy production help company agents provide customer support prioritize message respond model political lean open make available wider use
annotate datasets commonly use train evaluation task involve natural language vision image description generation action recognition visual question answer however many exist datasets reflect problems emerge process data selection annotation point difficulties problems one confront create validate annotate vision language datasets
open information extraction open ie systems aim obtain relation tuples highly scalable extraction portable across domain identify variety relation phrase arguments arbitrary sentence first generation open ie learn linear chain model base unlexicalized feature part speech pos shallow tag label intermediate word pair potential arguments identify extractable relations open ie currently develop second generation able extract instance frequently observe relation type verb noun prep verb prep infinitive deep linguistic analysis expose simple yet principled ways verbs express relationships linguistics verb phrase base extraction clause base extraction obtain significantly higher performance previous systems first generation paper describe overview two open ie generations include strengths weaknesses application areas
study analyze political agenda european parliament ep plenary evolve time manner members european parliament meps react external internal stimuli make plenary speeches unveil plenary agenda detect latent theme legislative speeches time mep speech content analyze use new dynamic topic model method base two layer non negative matrix factorization nmf method apply new corpus english language legislative speeches ep plenary period one thousand, nine hundred and ninety-nine two thousand and fourteen find suggest two layer nmf valuable alternative exist dynamic topic model approach find literature unveil niche topics associate vocabularies capture exist methods substantively find suggest political agenda ep evolve significantly time react exogenous events eu treaty referenda emergence euro crisis mep contributions plenary agenda also find impact upon vote behaviour committee structure parliament
present novel neural architecture answer query design optimally leverage explicit support form query answer memories model able refine update give query separately accumulate evidence predict answer architecture reflect separation dedicate embed matrices loosely connect information pathways modules update query accumulate evidence separation responsibilities effectively decouple search query relate support prediction answer recent benchmark datasets read comprehension model achieve state art result qualitative analysis reveal model effectively accumulate weight evidence query multiple support retrieval cycle result robust answer prediction
remember event affect personal emotional status examine psychological status personal factor depression center epidemiological study depression radloff one thousand, nine hundred and seventy-seven present affective positive affective negative affective schedule watson et al one thousand, nine hundred and eighty-eight life orient life orient test scheier carver one thousand, nine hundred and eighty-five self awareness core self evaluation scale judge et al two thousand and three social factor social support sarason et al one thousand, nine hundred and eighty-three undergraduate students n64 get summaries story chronicle death foretell gabriel garcia marquez one thousand, nine hundred and eighty-one implement sentimental analysis model base convolutional neural network lecun bengio one thousand, nine hundred and ninety-five evaluate summary vein use transfer learn pan yang two thousand and ten collect thirty-eight thousand, two hundred and sixty-five movie review data train model use score summaries student result ces panas show relationship emotion memory retrieval follow depress people show tendency represent story negatively seem less expressive people full emotion high panas retrieve memory expressively others use negative word others contributions study summarize follow first lighten relationship emotion effect time store retrieve memory second suggest objective methods evaluate intensity emotion natural language format use sentimental analysis model
recently sound recognition use identify sound car river however sound nuances may better describe adjective noun pair slow car verb noun pair fly insects explore therefore work investigate relation audio content adjective noun pair verb noun pair due lack datasets kinds annotations collect process audiopairbank corpus consist combine total one thousand, one hundred and twenty-three pair thirty-three thousand audio file one contribution previously unavailable documentation challenge implications collect audio record type label second contribution show degree correlation audio content label sound recognition experiment yield result seventy accuracy hence also provide performance benchmark result study paper encourage exploration nuances audio mean complement similar research perform image text multimedia analysis
distributional semantics create vector space representations capture many form semantic similarity relation semantic entailment less clear propose vector space model provide formal foundation distributional semantics entailment use mean field approximation develop approximate inference procedures entailment operators vectors probabilities feature know versus unknown use framework reinterpret exist distributional semantic model word2vec approximate entailment base model distributions word contexts thereby predict lexical entailment relations unsupervised semi supervise experiment hyponymy detection get substantial improvements previous result
gender bias increasingly important issue sport journalism work propose language model base approach quantify differences question pose female vs male athletes apply tennis post match interview find journalists ask male players question generally focus game compare question ask female counterparts also provide fine grain analysis extent salience bias depend various factor question type game outcome player rank
recently neural network prove extremely effective many natural language process task sentiment analysis question answer machine translation aim exploit advantage ontology learn process technical report present detail description recurrent neural network base system use pursue goal
cloze style query representative problems read comprehension past months see much progress utilize neural network approach solve cloze style question paper present novel model call attention attention reader cloze style read comprehension task model aim place another attention mechanism document level attention induce attend attention final predictions unlike previous work neural network model require less pre define hyper parameters use elegant architecture model experimental result show propose attention attention model significantly outperform various state art systems large margin public datasets cnn children book test datasets
deep neural network show recent promise many language relate task model conversations extend rnn base sequence sequence model capture long range discourse across many turn conversation perform sensitivity analysis much additional context affect performance provide quantitative qualitative evidence model able capture discourse relationships across multiple utterances result quantify add additional rnn layer model discourse improve quality output utterances provide previous conversation input also improve performance search generate output specific discourse markers show neural discourse model exhibit increase coherence cohesion conversations
continuous word representations train large unlabeled corpora useful many natural language process task popular model learn representations ignore morphology word assign distinct vector word limitation especially languages large vocabularies many rare word paper propose new approach base skipgram model word represent bag character n grams vector representation associate character n gram word represent sum representations method fast allow train model large corpora quickly allow us compute word representations word appear train data evaluate word representations nine different languages word similarity analogy task compare recently propose morphological word representations show vectors achieve state art performance task
rapidly expand corpus medical research literature present major challenge understand previous work extraction maximum information collect data identification promise research directions present case use advance machine learn techniques aide task introduce novel methodology show capable extract meaningful information large longitudinal corpora track complex temporal change within
paper present simple computationally efficient quantization scheme enable us reduce resolution parameters neural network thirty-two bite float point value eight bite integer value propose quantization scheme lead significant memory save enable use optimize hardware instructions integer arithmetic thus significantly reduce cost inference finally propose quantization aware train process apply propose scheme network train find allow us recover loss accuracy introduce quantization validate propose techniques apply long short term memory base acoustic model open end large vocabulary speech recognition task
know word attend previous time step generate translation rich source information predict word attend future improve upon attention model bahdanau et al two thousand and fourteen explicitly model relationship previous subsequent attention level word use one recurrent network per input word architecture easily capture informative feature fertility regularities relative distortion experiment show parameterization attention improve translation quality
information content ic base measure find semantic similarity gain preferences day day semantics concepts highly characterize information theory conventional way calculate ic base probability appearance concepts corpora due data sparseness corpora dependency issue conventional approach new corpora independent intrinsic ic calculation measure evolve paper mainly focus intrinsic ic model several topological aspects underlie ontology accuracy intrinsic ic calculation semantic similarity measure rely aspects deeply base analysis propose information theoretic framework comprise intrinsic ic calculator semantic similarity model approach compare state art semantic similarity measure base corpora dependent ic calculation well intrinsic ic base methods use several benchmark data set also compare model relate edge base feature base distributional approach experimental result show intrinsic ic model give high correlation value apply different semantic similarity model propose semantic similarity model also achieve significant result embed state art ic model include
authorship attribution long stand problem natural language process several statistical computational methods use find solution problem paper propose methods deal authorship attribution problem bengali
robust far field speech recognition critical enable true hand free communication far field condition signal attenuate due distance improve robustness loudness variation introduce novel frontend call per channel energy normalization pcen key ingredient pcen use automatic gain control base dynamic compression replace widely use static log root compression evaluate pcen keyword spot task large rerecorded noisy far field eval set show pcen significantly improve recognition performance furthermore model pcen neural network layer optimize high dimensional pcen parameters jointly keyword spot acoustic model train pcen frontend demonstrate significant improvements without increase model complexity inference time cost
neural conversational model tend produce generic safe responses different contexts eg reply textitof course narrative statements textiti know question paper propose end end approach avoid problem neural generative model additional memory mechanisms introduce standard sequence sequence seq2seq model context consider generate sentence three seq2seq model memorize fix size contextual vector hide input hide input output gate contextual attention structure respectively train test dataset label question answer pair chinese model contextual attention outperform others include state art seq2seq model perplexity test novel contextual model generate diverse robust responses able carry conversations wide range topics appropriately
present system generate understand dynamic static spatial relations robotic interaction setups robots describe environment move block use english phrase include spatial relations across front evaluate system robot robot interactions show system robustly deal visual perception errors language omissions ungrammatical utterances
paper present first experiment use neural network model task error detection learner write perform systematic comparison alternative compositional architectures propose framework error detection base bidirectional lstms experiment conll fourteen share task dataset show model able outperform participants detect errors learner write finally model integrate publicly deploy self assessment system lead performance comparable human annotators
background grow research interest automate answer question generation summary free form text news article order implement task computer able identify sequence events duration events time event occur relationship type event pair time pair event time pair specific problem important accurately identify relationship type combinations event time temporal order events define machine learn approach take mani et al two thousand and six provide accuracy six hundred and twenty-five baseline data timebank researchers use maximum entropy classifier methodology timeml use tlink annotation tag relationship type events time time complexity quadratic come tag document tlink use human annotation research propose use decision tree parse improve relationship type tag research attempt solve gap human annotation automate task relationship type tag attempt improve accuracy event time relationship annotate document scope information document domain news use tag perform within document across document relationship type identify pair event time chain events research focus document tag use timeml specification contain tag event tlink timex tag attribute identifier relation pos time etc
paper provide global vision scientific publications relate systemic lupus erythematosus sle take start point abstract article time abstract evolve towards higher complexity use terminology make necessary use sophisticate statistical methods answer question include vocabulary evolve time ones influential article one article introduce new term vocabulary answer analyze dataset compose five hundred and six abstract download one hundred and fifteen different journals cover eighteen year period
due lack structure knowledge apply learn distribute representation cate gories exist work incorporate category hierarchies entity information propose framework embed entities categories semantic space integrate structure knowledge taxonomy hierarchy large knowledge base framework allow com pute meaningful semantic relatedness entities categories framework han dle single word concepts multiple word concepts superior performance concept categorization yield state art result dataless hierarchical classification
medical domain continuous stream scientific research contain contradictory result support arguments counter arguments medical expertise occur different level part human agents difficulties face huge amount study also understand reason piece evidence claim proponents opponents debate topic better understand support arguments new find relate current state art medical domain need tool able identify arguments scientific paper work aim fill technological gap quite aware difficulty task embark road rely well know interleave domain knowledge natural language process formalise exist medical knowledge rely ontologies structure argumentation model use also expressivity reason capabilities description logics perform argumentation mine formalise various linguistic pattern rule base language test solution corpus scientific paper relate breast cancer run experiment show f measure seventy-one eighty-six identify conclusions argument sixty-five eighty-six identify premise argument
selectional restrictions semantic constraints form certain complex type natural language paper give overview model selectional restrictions relational type system morphological syntactic type discuss foundations system ways formalize selectional restrictions keywords type theory selectional restrictions syntax morphology
considerable interest task automatically generate image caption however evaluation challenge exist automatic evaluation metrics primarily sensitive n gram overlap neither necessary sufficient task simulate human judgment hypothesize semantic propositional content important component human caption evaluation propose new automate caption evaluation metric define scene graph coin spice extensive evaluations across range model datasets indicate spice capture human judgments model generate caption better automatic metrics eg system level correlation eighty-eight human judgments ms coco dataset versus forty-three cider fifty-three meteor furthermore spice answer question caption generator best understand color caption generators count
paper describe approach query word label attempt share task mix script information retrieval forum information retrieval evaluation fire two thousand and fifteen query write roman script word english transliterate indian regional languages total eight indian languages present addition english also identify name entities special symbols part task crf base machine learn framework use label individual word correspond language label use dictionary base approach language identification also take account context word identify language system demonstrate overall accuracy seven hundred and fifty-five token level language identification strict f measure score identification token level language label bengali english hindi seven thousand, four hundred and eighty-six eight hundred and ninety-two seven thousand, nine hundred and seventy-two respectively overall weight f measure system seven thousand, four hundred and ninety-eight
paper present study personalize emphasis frame use tailor content message enhance appeal different individuals framework directly model content selection decisions base set psychologically motivate domain independent personal traits include personality eg extraversion conscientiousness basic human value eg self transcendence hedonism also demonstrate analysis result use automate personalize content selection persuasive message generation
sequence sequence neural translation model learn semantic syntactic relations sentence pair optimize likelihood target give source ie pyx objective ignore potentially useful source information introduce alternative objective function neural mt maximize mutual information source target sentence model bi directional dependency source target implement model simple rank method also introduce decode algorithm increase diversity n best list produce first pass apply wmt german english french english task propose model offer consistent performance boost standard lstm attention base neural mt architectures
present novel end end neural model extract entities relations recurrent neural network base model capture word sequence dependency tree substructure information stack bidirectional tree structure lstm rnns bidirectional sequential lstm rnns allow model jointly represent entities relations share parameters single model encourage detection entities train use entity information relation extraction via entity pretraining schedule sample model improve state art feature base model end end relation extraction achieve one hundred and twenty-one fifty-seven relative error reductions f1 score ace2005 ace2004 respectively also show lstm rnn base model compare favorably state art cnn base model f1 score nominal relation classification semeval two thousand and ten task eight finally present extensive ablation analysis several model components
semantic parse methods use capture represent semantic mean text mean representation capture concepts text may always available may sufficiently complete ontologies provide structure reason capable way model content collection texts work present novel approach joint learn ontology semantic parser text method base semi automatic induction context free grammar semantically annotate text grammar parse text semantic tree grammar semantic tree use learn ontology several level class instance taxonomic non taxonomic relations approach evaluate first sentence wikipedia page describe people
propose multi way multilingual neural machine translation propose approach enable single neural translation model translate multiple languages number parameters grow linearly number languages make possible single attention mechanism share across language pair train propose multi way multilingual model ten language pair wmt fifteen simultaneously observe clear performance improvements model train one language pair particular observe propose model significantly improve translation quality low resource language pair
number scientific paper grow exponentially many discipline share online available paper grow well time period time paper loose chance cite anymore shorten decay cite rate show similarity ultradiffusional process online content social network distribution paper per author show similarity distribution post per user social network rate uncited paper online available paper grow paper go viral term cite summarize practice scientific publish move towards domain social network goal project create text engineer tool semi automatically categorize paper accord type contribution extract relationships ontological database semi automatic categorization mean mistake make automatic pre categorization relationship extraction correct wikipedia like front end volunteer general public tool help researchers general public find relevant supplementary material peer faster also provide information research fund agencies
recently recurrent neural network rnns powerful sequence model emerge potential acoustic model statistical parametric speech synthesis spss long short term memory lstm architecture particularly attractive address vanish gradient problem standard rnns make easier train although recent study demonstrate lstms achieve significantly better performance spss deep fee forward neural network little know attempt answer two question lstms work well sequence model spss b component eg input gate output gate forget gate important present visual analysis alongside series experiment result proposal simplify architecture simplify architecture significantly fewer parameters lstm thus reduce generation complexity considerably without degrade quality
nowadays neural network play important role task relation classification design different neural architectures researchers improve performance large extent comparison traditional methods however exist neural network relation classification usually shallow architectures eg one layer convolutional neural network recurrent network may fail explore potential representation space different abstraction level paper propose deep recurrent neural network drnns relation classification tackle challenge propose data augmentation method leverage directionality relations evaluate drnns semeval two thousand and ten task8 achieve f1 score eight hundred and sixty-one outperform previous state art record result
automatic description generation natural image challenge problem recently receive large amount interest computer vision natural language process communities survey classify exist approach base conceptualize problem viz model cast description either generation problem retrieval problem visual multimodal representational space provide detail review exist model highlight advantage disadvantage moreover give overview benchmark image datasets evaluation measure develop assess quality machine generate image descriptions finally extrapolate future directions area automatic image description generation
present approach structure prediction bandit feedback call bandit structure prediction value task loss function single predict point instead correct structure observe learn present application discriminative reranking statistical machine translation smt learn algorithm access one bleu loss evaluation predict translation instead obtain gold standard reference translation experiment bandit feedback obtain evaluate bleu reference translations without reveal algorithm think simulation interactive machine translation smt system personalize user provide single point feedback predict translations experiment show approach improve translation quality comparable approach employ informative feedback learn
news events social media compose evolve storylines capture public attention limit period time identify storylines require integrate temporal linguistic information prior work take largely heuristic approach present novel online non parametric bayesian framework storyline detection use distance dependent chinese restaurant process dd crp ensure efficient linear time inference employ fix lag gibbs sample procedure novel dd crp evaluate trec twitter timeline generation ttg obtain encourage result despite use weak baseline retrieval model dd crp story cluster method competitive best entries two thousand and fourteen ttg task
extractive summarization aim select set indicative sentence source document summary express major theme document general consensus extractive summarization relevance coverage critical issue address exist methods design model coverage characterize either reduce redundancy increase diversity summary maximal margin relevance mmr widely cite method since take relevance redundancy account generate summary give document addition mmr dearth research concentrate reduce redundancy increase diversity speak document summarization task far aware motivate observations two major contributions present paper first contrast mmr consider coverage reduce redundancy propose two novel coverage base methods directly increase diversity propose methods set representative sentence relevant give document also cover important sub theme document select automatically second make step forward plug several document sentence representation methods propose framework enhance summarization performance series empirical evaluations demonstrate effectiveness propose methods
vector space representations word capture many aspects word similarity methods tend make vector space antonyms well synonyms close present new sign spectral normalize graph cut algorithm sign cluster overlay exist thesauri upon distributionally derive vector representations word antonym relationships word pair represent negative weight sign cluster algorithm produce cluster word simultaneously capture distributional synonym relations evaluate cluster simlex nine hundred and ninety-nine dataset hill et al2014 human judgments word pair similarities also show benefit use cluster predict sentiment give text
use phonological speech vocoding propose platform explore relations phonology speech process broader term explore relations abstract physical structure speech signal goal make step towards bridge phonology speech process contribute program laboratory phonology show three application examples laboratory phonology compositional phonological speech model comparison phonological systems experimental phonological parametric text speech tts system featural representations follow three phonological systems consider work government phonology gp ii sound pattern english spe iii extend spe espe compare gp espe base vocoded speech conclude latter achieve slightly better result former however gp compact phonological speech representation perform comparably systems higher number phonological feature parametric tts base phonological speech representation train unlabelled audiobook unsupervised manner achieve intelligibility eighty-five state art parametric speech synthesis envision present approach pave way researchers field form meaningful hypotheses explicitly testable use concepts develop exemplify paper one hand laboratory phonologists might test apply concepts theoretical model hand speech process community may utilize concepts develop theoretical phonological model improvements current state art applications
morrill valentin paper computational coverage tlg nonlinearity consider extension lambek calculus enrich call exponential modality modality behave relevant style allow contraction permutation weaken morrill valentin state open problem whether system decidable show undecidability result remain valid consider fragment division operations one direction also show derivability problem restrict case modality apply variables primitive type decidable belong np class
paper address question render sequence level network better handle structure input propose machine read simulator process text incrementally leave right perform shallow reason memory attention reader extend long short term memory architecture memory network place single memory cell enable adaptive memory usage recurrence neural attention offer way weakly induce relations among tokens system initially design process single sequence also demonstrate integrate encoder decoder architecture experiment language model sentiment analysis natural language inference show model match outperform state art
introduce model linguistic hedge quite within label semantics framework combine prototype conceptual space theories concepts propose model emerge naturally representational framework use clear semantic ground give generalisations hedge model show compose function go examine behaviour limit composition
paper aim introduce new algorithm automatic speech text summarization base statistical divergences probabilities graph input text speech conversations noise output compact text summary result pilot task cccs multiling two thousand and fifteen french corpus encourage
voynich manuscript medieval book write unknown script paper study distribution similarly spell word voynich manuscript show distribution word within manuscript compatible natural languages
lipread ie speech recognition visual record speaker face achieve process pipeline base solely neural network yield significantly better accuracy conventional methods fee forward recurrent neural network layer namely long short term memory lstm stack form single structure train back propagate error gradients layer performance stack network experimentally evaluate compare standard support vector machine classifier use conventional computer vision feature eigenlips histograms orient gradients evaluation perform data nineteen speakers publicly available grid corpus fifty-one different word classify report best word accuracy hold evaluation speakers seven hundred and ninety-six use end end neural network base solution one hundred and sixteen improvement best feature base solution evaluate
name disambiguation become one main theme semantic web agenda semantic web extension current web information give well define mean also many purpose contain ambiguous naturally lot thing come overlap mainly deal persons name therefore develop approach extract keywords web snippet utilize overlap principle concept understand things ambiguous whereby feature person generate deal variety web web steadily gain grind semantic research
language social media mostly drive new word spell constantly enter lexicon thereby pollute result high deviation formal write version primary entities language vocabulary oov word paper study various sociolinguistic properties oov word propose classification model categorize least six categories achieve eight thousand, one hundred and twenty-six accuracy high precision recall observe content feature discriminative ones follow lexical context feature
work aim discover high quality speech feature linguistic units directly unlabeled speech data zero resource scenario result evaluate use metrics corpora propose zero resource speech challenge organize interspeech two thousand and fifteen multi layer acoustic tokenizer mat propose automatic discovery multiple set acoustic tokens give corpus acoustic token set specify set hyperparameters describe model configuration set acoustic tokens carry different characteristics fof give corpus language behind thus mutually reinforce multiple set token label use target multi target deep neural network mdnn train low level acoustic feature bottleneck feature extract mdnn use feedback input mat mdnn next iteration call iterative deep learn framework multi layer acoustic tokenizing deep neural network mat dnn generate high quality speech feature track one challenge acoustic tokens track two challenge addition perform extra experiment corpora application query example speak term detection experimental result show iterative deep learn framework mat dnn improve detection performance due better underlie speech feature acoustic tokens
people produce write material anytime history increase high professionals various field able cope amount publications text mine tool offer tool help one tool aid information retrieval information extraction semantic text annotation report present marvin text annotator write java use command line tool java library marvin able annotate text use multiple source include wordnet metamap dbpedia thesauri represent skos
develop draw topics toolkit provide fast way incorporate social scientists interest standard topic model instead use raw corpus primitive process input algorithm base vector space model conditional entropy use connect social scientists willingness unsupervised topic model output space users adjustment specific corpus interest also accommodate demonstrate toolkit use diachronic people daily corpus chinese
statistical language model central many applications use semantics recurrent neural network rnn know produce state art result language model outperform traditional n gram counterparts many case generate probability distribution across vocabulary model require softmax output layer linearly increase size size vocabulary large vocabularies need commensurately large softmax layer train typical laptops pcs require significant time machine resources paper present new technique implement rnn base large vocabulary language model substantially speed computation optimally use limit memory resources technique build notion factorize output layer multiple output layer improve earlier work substantially optimize individual output layer size also eliminate need multistep prediction process
internet increase amount information available however read understand information costly task scenario natural language process nlp applications enable important solutions highlight automatic text summarization ats produce summary one source texts automatically summarize one texts however complex task difficulties inherent analysis generation summary master thesis describe main techniques methodologies nlp heuristics generate summaries also address propose heuristics base graph similarity matrix measure relevance judgments generate summaries extract sentence use multiple languages english french spanish cstnews brazilian portuguese rpm french decoda french corpus evaluate developped systems result obtain quite interest
model cognition develop smolensky legendre two thousand and six seek unify two level description cognitive process connectionist symbolic theory develop bring together two level integrate connectionist symbolic cognitive architecture ics clark pulman two thousand and seven draw parallel semantics mean may model distributional symbolic level develop coecke et al two thousand and ten distributional compositional disco model mean current work revisit smolensky legendre sandl model describe disco framework summarise key ideas sandl architecture describe description harmony grade measure grammaticality may apply disco model
software review text fragment considerably valuable information users experience include huge set properties include software quality opinion mine sentiment analysis concern analyze textual user judgments application sentiment analysis software review find quantitative value represent software quality although many software quality methods propose consider difficult customize many limit article investigate application opinion mine approach extract software quality properties find major issue software review mine use sentiment analysis due software lifecycle diverse users team
paper describe data collection effort part project sprekend nederland netherlands talk discuss potential use automatic accent location define automatic accent location task describe accent speaker term location speaker history discuss possible ways describe accent location consequence task automatic accent location potential evaluation metrics
unsupervised methods learn distribute representations word ubiquitous today nlp research far less know best ways learn distribute phrase sentence representations unlabelled data paper systematic comparison model learn representations find optimal approach depend critically intend application deeper complex model preferable representations use supervise systems shallow log linear model work best build representation space decode simple spatial distance metrics also propose two new unsupervised representation learn objectives design optimise trade train time domain portability performance
identify relationships concepts key aspect scientific knowledge synthesis find link often require researcher laboriously search scien tific paper databases size resources grow ever larger paper describe distributional semantics use unify structure knowledge graph unstructured text predict new relationships medical concepts use probabilistic generative model approach also design ameliorate data sparsity scarcity issue medical domain make language model challenge specifically integrate medical relational database semmeddb text electronic health record ehrs perform knowledge graph completion demonstrate ability model predict relationships tokens appear relational database
article present new alternatives similarity function textrank algorithm automatic summarization texts describe generalities algorithm different function propose variants achieve significative improvement use metrics dataset original publication
work propose attentive pool ap two way attention mechanism discriminative model train context pair wise rank classification neural network ap enable pool layer aware current input pair way information two input items directly influence computation representations along representations pair input ap jointly learn similarity measure project segment eg trigrams pair subsequently derive correspond attention vector input guide pool two way attention mechanism general framework independent underlie representation learn apply convolutional neural network cnns recurrent neural network rnns study empirical result three different benchmark task question answer answer selection demonstrate propose model outperform variety strong baselines achieve state art performance benchmarks
complex organization syntax hierarchical structure one core design feature human language duality pattern refer instance organization meaningful elements language two distinct level combinatorial level meaningless form combine meaningful form compositional level meaningful form compose larger lexical units question remain wide open regard structure could emerge furthermore clear mathematical framework quantify phenomenon still lack aim paper address two aspects self consistent way first introduce suitable measure quantify level combinatoriality compositionality language present framework estimate observables human natural languages second show theoretical predictions multi agents model scheme namely blend game surprisingly good agreement empirical data blend game population individuals play language game aim success communication remarkable two side duality pattern emerge simultaneously consequence pure cultural dynamics simulate environment contain meaningful relations provide simple constraint message transmission fidelity also consider
relationship read write rrw one major theme learn science one obstacles difficult define measure latent background knowledge individual however academic research set scholars require explicitly list background knowledge citation section manuscripts unique opportunity take advantage observe rrw especially publish academic commentary scenario rrw visualize propose topic process model use state art version latent dirichlet allocation lda empirical study show academic commentary modulate target paper author background knowledge although conclusion obtain unique environment suggest implications also would light similar interest areas dialog conversation group discussion social media
chapter give overview application complex network theory quantify properties language study base two fables ukrainian mykyta fox abu kasym slippers consist two part analysis frequency rank distributions word application complex network theory first part show text size sufficiently large observe statistical properties support selection analysis typical properties language network second part chapter describe language complex network word usually associate nod variability choice link different representations result different network examine number representations language network perform comparative analysis characteristics result suggest irrespective link representation ukrainian language network use select fables strongly correlate scale free small world discuss empirical approach may help form useful basis theoretical description language evolution may use analyse textual narratives
recurrent neural networkrnn broadly apply natural language processingnlp problems kind neural network design model sequential data testify quite efficient sequential tag task paper propose use bi directional rnn long short term memorylstm units chinese word segmentation crucial preprocess task model chinese sentence article classical methods focus design combine hand craft feature context whereas bi directional lstm networkblstm need prior knowledge pre design expert keep contextual information directions experiment result show approach get state art performance word segmentation traditional chinese datasets simplify chinese datasets
practice train language model individual author often expensive limit data resources case neural network language model nnlms generally outperform traditional non parametric n gram model investigate performance fee forward nnlm authorship attribution problem moderate author set size relatively limit data also consider text topics impact performance compare well construct n gram baseline method kneser ney smooth propose method achieve nearly twenty-five reduction perplexity increase author classification accuracy three hundred and forty-three average give five test sentence performance competitive state art term accuracy demand test data source code preprocessed datasets detail description methodology result available https githubcom zge authorship attribution
current systems fine grain entity type use distant supervision conjunction exist knowledge base assign categories type label entity mention however type label obtain knowledge base often noisy ie incorrect entity mention local context define new task label noise reduction entity type lnr automatic identification correct type label type paths train examples give set candidate type label obtain distant supervision give type hierarchy unknown type label individual entity mention semantic similarity entity type pose unique challenge solve lnr task propose general framework call ple jointly embed entity mention text feature entity type low dimensional space space object whose type semantically close similar representations estimate type path train example top manner use learn embeddings formulate global objective learn embeddings text corpora knowledge base adopt novel margin base loss robust noisy label faithfully model type correlation derive knowledge base experiment three public type datasets demonstrate effectiveness robustness ple average twenty-five improvement accuracy compare next best method
create linguistic annotations require reliable annotation scheme annotation complex endeavour potentially involve many people stag tool chapter outline process create end end linguistic annotations identify specific task researchers often perform tool support central achieve high quality reusable annotations low cost focus identify capabilities necessary useful annotation tool well common problems tool present reduce utility although examples specific tool provide many case chapter concentrate abstract capabilities problems new tool appear continuously old tool disappear disuse disrepair two core capabilities tool must support choose annotation scheme ability work language study additional capabilities organize three categories widely provide often useful find tool yet little available tool support
conceptual space geometric representations conceptual knowledge entities correspond point natural properties correspond convex regions dimension space correspond salient feature conceptual space enable elegant model various cognitive phenomena lack automate methods construct representations far limit application artificial intelligence address issue propose method learn vector space embed entities wikipedia constrain embed entities semantic type locate lower dimensional subspace experimentally demonstrate usefulness subspaces approximate conceptual space representations show among others important feature model directions natural properties tend correspond convex regions
traditional convolutional layer extract feature patch data apply non linearity affine function input propose model enhance feature extraction process case sequential data feed patch data recurrent neural network use output hide state recurrent units compute extract feature exploit fact window contain frame sequential data sequence additional structure might encapsulate valuable information addition allow step computation feature extraction process potentially beneficial affine function follow non linearity result simple feature use convolutional recurrent layer obtain improvement performance two audio classification task compare traditional convolutional layer tensorflow code convolutional recurrent layer publicly available https githubcom cruvadom convolutional rnn
match two texts fundamental problem many natural language process task effective way extract meaningful match pattern word phrase sentence produce match score inspire success convolutional neural network image recognition neurons capture many complicate pattern base extract elementary visual pattern orient edge corner propose model text match problem image recognition firstly match matrix whose entries represent similarities word construct view image convolutional neural network utilize capture rich match pattern layer layer way show resemble compositional hierarchies pattern image recognition model successfully identify salient signal n gram n term match experimental result demonstrate superiority baselines
human language color broad range topics exist text analysis tool focus small number present empath tool generate validate new lexical categories demand small set seed term like bleed punch generate category violence empath draw connotations word phrase deep learn neural embed across eighteen billion word modern fiction give small set seed word characterize category empath use neural embed discover new relate term validate category crowd power filter empath also analyze text across two hundred build pre validate categories generate common topics web dataset like neglect government social media show empath data drive human validate categories highly correlate r0906 similar categories liwc
study temporal network character literature focus alice adventure wonderland one thousand, eight hundred and sixty-five lewis carroll anonymous la chanson de roland around one thousand, one hundred former one influential piece nonsense literature ever write describe adventure alice fantasy world logic play intersperse along narrative latter song heroic deeds depict battle roncevaux seven hundred and seventy-eight ad charlemagne campaign iberian peninsula apply methods recently develop taylor coworkers citetaylor2015 find time average eigenvector centralities freeman indices vitalities character show temporal network appropriate static ones study stories capture feature time independent approach fail yield
research show accent classification improve integrate semantic information pure acoustic approach work combine phonetic knowledge vowels enhance acoustic feature build improve accent classification system classifier base gaussian mixture model universal background model gmm ubm normalize perceptual linear predictive plp feature feature optimize principle component analysis pca hetroscedastic linear discriminant analysis hlda use seven major type accent speech foreign accent english fae corpus system achieve classification accuracy fifty-four input test data short twenty second competitive state art field
limit automate twitter sentiment classification analyze large set manually label tweet different languages use train data construct automate classification model turn quality classification model depend much quality size train data type model train experimental result indicate statistically significant difference performance top classification model quantify quality train data apply various annotator agreement measure identify weakest point different datasets show model performance approach inter annotator agreement size train set sufficiently large however crucial regularly monitor self inter annotator agreements since improve train datasets consequently model performance finally show strong evidence humans perceive sentiment class negative neutral positive order
paper argue paradigmatic shift reductionism togetherness particular show interaction systems quantum theory naturally carry model word mean interact natural language since mean natural language depend subject domain encompass discussions within scientific discipline obtain template theories social interaction animal behaviour many others
introduce recurrent neural network grammars probabilistic model sentence explicit phrase structure explain efficient inference procedures allow application parse language model experiment show provide better parse english single previously publish supervise generative model better language model state art sequential rnns english chinese
present novel algorithm optimize order chinese character learn one incorporate benefit learn order usage frequency order hierarchal structural relationships show work outperform previously publish order algorithms algorithm applicable schedule task nod intrinsic differences importance must visit topological order
present novel methods analyze activation pattern rnns linguistic point view explore type linguistic structure learn case study use multi task gate recurrent network architecture consist two parallel pathways share word embeddings train predict representations visual scene correspond input sentence predict next word sentence base propose method estimate amount contribution individual tokens input final prediction network show image prediction pathway sensitive information structure sentence b pay selective attention lexical categories grammatical function carry semantic information c learn treat input token differently depend grammatical function sentence contrast language model comparatively sensitive word syntactic function furthermore propose methods ex plore function individual hide units rnns show two pathways architecture case study contain specialize units tune pattern informative task carry activations later time step encode long term dependencies
research accomplishment usually measure consider citations equal importance thus ignore wide variety purpose article cite posit measure intensity reference crucial perceive better understand research endeavor also improve quality citation base applications end collect rich annotate dataset reference label intensity propose novel graph base semi supervise model gralap label intensity reference experiment aan datasets show significant improvement compare baselines achieve true label reference forty-six better correlation finally provide four applications demonstrate knowledge reference intensity lead design better real world applications
explore linguistic behavioral feature dogmatism social media construct statistical model identify dogmatic comment model base corpus reddit post collect across diverse set conversational topics annotate via pay crowdsourcing operationalize key aspects dogmatism describe exist psychology theories confidence find predictive power also find evidence new signal dogmatism tendency dogmatic post refrain signal cognitive process use predictive model analyze millions reddit post find evidence suggest dogmatism deeper personality trait present dogmatic users across many different domains users engage dogmatic comment tend show increase dogmatic post
citations important indicator state scientific field reflect author frame work influence uptake future scholars however understand citation behavior limit small scale manual citation analysis perform largest behavioral study citations date analyze citations frame take scholars one entire field natural language process introduce new dataset nearly two thousand citations annotate function centrality use develop state art classifier label entire acl reference corpus study citations frame author use paper online trace track citations follow readers demonstrate author sensitive discourse structure publication venue cite online readers follow temporal link previous future work rather methodological link paper cite relate work predictive citation count finally use change citation roles show field nlp undergo significant increase consensus
relation extraction key process obtain good detectors find relevant sentence describe target relation minimize necessity label data refine detectors previous work successfully make use babelnet semantic graph structure express relationships synsets side information prior knowledge goal paper enhance use graph structure framework random walk adjustable parameters actually straightforward application random walk degrade performance even parameter optimization insight unsuccessful trial propose synsetrank adjust initial probability high degree nod influence neighbor strong low degree nod experiment thirteen relations fb15k two hundred and thirty-seven dataset synsetrank significantly outperform baselines plain random walk approach
paper propose kb infobot multi turn dialogue agent help users search knowledge base kbs without compose complicate query goal orient dialogue agents typically need interact external database access real world knowledge previous systems achieve issue symbolic query kb retrieve entries base attribute however symbolic operations break differentiability system prevent end end train neural dialogue agents paper address limitation replace symbolic query induce soft posterior distribution kb indicate entities user interest integrate soft retrieval process reinforcement learner lead higher task success rate reward simulations real users also present fully neural end end agent train entirely user feedback discuss application towards personalize dialogue agents source code available https githubcom miulab kb infobot
context competition legal information extraction entailment coliee propose method comprise necessary step find relevant document legal question decide textual entailment evidence provide correct answer propose method base combination several lexical morphological characteristics build language model set feature machine learn algorithms provide detail study propose method performance failure case indicate competitive state art approach legal information retrieval question answer need extensive train data depend expert produce knowledge propose method achieve significant result competition indicate substantial level adequacy task address
clinicians expect date broad knowledge disease treatment options patient online health knowledge resources contain wealth information however time investment need disseminate rank pertinent information need summarize information concise format aim study provide clinicians concise overview popular treatments give disease use information automatically compute medline abstract analyze treatments two disorder atrial fibrillation congestive heart failure calculate precision recall f score two rank methods measure accuracy result atrial fibrillation disorder maximum f score new treatments weigh method six hundred and eleven occur sixty treatments congestive heart failure disorder maximum f score new treatments weigh method five hundred and three occur eighty treatments
paper propose two different approach rule base approach machine learn base approach identify active heart failure case automatically analyze electronic health record ehr rule base approach extract cardiovascular data elements clinical note match patients different color accord heart failure condition use rule provide experts heart failure achieve six hundred and ninety-four accuracy seven hundred and twenty-nine f1 score machine learn approach bigram clinical note feature try four different model svm linear kernel achieve best performance eight hundred and seventy-five accuracy eighty-six f1 score also classification comparison four different model believe linear model fit better problem combine machine learn rule base algorithms enable hospital wide surveillance active heart failure increase accuracy interpretability output
rare diseases difficult identify among large number possible diagnose better availability patient data improvement machine learn algorithms empower us tackle problem computationally paper target one rare disease cardiac amyloidosis aim automate process identify potential cardiac amyloidosis patients help machine learn algorithms also learn predictive factor help experience cardiologists prepare gold standard seventy-three positive cardiac amyloidosis one hundred and ninety-seven negative instance achieve high average cross validation f1 score ninety-eight use ensemble machine learn classifier predictive variables age diagnosis cardiac arrest chest pain congestive heart failure hypertension prim open angle glaucoma shoulder arthritis study need validate accuracy system across entire health system generalizability diseases
background clinical guidelines recommendations drive wheel evidence base medicine ebm paradigm available primarily unstructured text generally highly heterogeneous nature significantly reduce dissemination automatic application recommendations point care comprehensive structure representation recommendations highly beneficial regard objective objective paper present clinical recommendation type system crts common type system effectively represent clinical recommendation structure form methods crts build analyze one hundred and twenty-five recommendations one hundred and ninety-five research article correspond six different diseases available uptodate publicly available clinical knowledge system national guideline clearinghouse public resource evidence base clinical practice guidelines result show crts cover recommendations also flexible extend represent information primary literature also describe develop type system apply clinical decision support medical knowledge summarization citation retrieval conclusion show propose type system precise comprehensive represent large sample recommendations available various disorder crts use build interoperable information extraction systems automatically extract clinical recommendations relate data elements clinical evidence resources guidelines systematic review primary publications keywords guidelines recommendations type system clinical decision support evidence base medicine information storage retrieval
reduce large amount time spend screen identify recruit patients clinical trials need prescreening systems able automate data extraction decision make task typically relegate clinical research study coordinators however major obstacle vast amount patient data available unstructured free form text electronic health record propose information extraction base approach first automatically convert unstructured text structure form structure data compare list eligibility criteria use rule base system determine patients qualify enrollment heart failure clinical trial show achieve highly accurate result recall precision value ninety-five eighty-six respectively system allow us significantly reduce time need prescreening patients weeks minutes open source information extraction modules available researchers could test validate cardiovascular trials approach one demonstrate may decrease cost expedite clinical trials could enhance reproducibility trials across institutions populations
novel information retrieval methods identify citations relevant clinical topic overcome knowledge gap exist primary literature medline online clinical knowledge resources uptodate search medline database directly query expansion methods return large number citations relevant query current study present citation retrieval system retrieve citations evidence base clinical knowledge summarization approach combine query expansion concept base screen algorithm concept base vector similarity also propose information extraction framework automate concept population intervention comparison disease extraction evaluate propose system topics query available uptodate two diseases heart failure hf atrial fibrillation afib system achieve overall f score four hundred and twelve hf topics four hundred and twenty-four afib topics gold standard citations available uptodate significantly high compare query expansion base baseline f score thirteen hf twenty-two afib system use query expansion disease hyponyms journal name concept base screen term base vector similarity system f score three hundred and seventy-five hf three hundred and ninety-five afib evaluate system top k relevant citations k number citations gold standard achieve much higher overall f score six hundred and ninety-nine hf topics seven hundred and fifty-one afib topics addition system retrieve eighteen new relevant citations per topic test ten hf six afib clinical topics
manual correction speech transcription involve selection plausible transcriptions recent work show feasibility employ mismatch crowd speech transcription however yet establish whether mismatch worker sufficiently fine granular speech perception choose among phonetically proximate options likely generate trellis asru hence consider five languages arabic german hindi russian spanish generate synthetic phonetically proximate options emulate post edit scenarios vary difficulty consistently observe non trivial crowd ability choose among fine granular options
online review increasingly become important resource consumers make purchase though become difficult people make well inform buy decisions without deceive fake review prior work opinion spam problem mostly consider classify fake review use behavioral user pattern focus prolific users write couple review discard one time reviewers number singleton reviewers however expect high many review websites behavioral pattern effective deal elite users one time reviewers review text need exploit paper tackle problem detect fake review write person use multiple name post review different name propose two methods detect similar review show result generally outperform vectorial similarity measure use prior work first method extend semantic similarity word review level second method base topic model exploit similarity review topic distributions use two model bag word bag opinion phrase experiment conduct review three different datasets yelp 57k review trustpilot 9k review ott dataset eight hundred review
opinion mine customer review become pervasive recent years sentence review however usually classify independently even though form part review argumentative structure intuitively sentence review build elaborate upon knowledge review structure sentential context thus inform classification sentence demonstrate hypothesis task aspect base sentiment analysis model interdependencies sentence review hierarchical bidirectional lstm show hierarchical model outperform two non hierarchical baselines obtain result competitive state art outperform state art five multilingual multi domain datasets without hand engineer feature external resources
paper describe deep learn base approach sentiment analysis twitter part semeval two thousand and sixteen task four use convolutional neural network determine sentiment participate subtasks ie two point three point five point scale sentiment classification two point five point scale sentiment quantification achieve competitive result two point scale sentiment classification quantification rank fifth close fourth third second alternative metrics respectively despite use pre train embeddings contain sentiment information achieve good performance three point scale sentiment classification rank eighth thirty-five perform poorly five point scale sentiment classification quantification error analysis reveal due low expressiveness model capture negative sentiment well inability take account ordinal information propose improvements order address issue
paper describe deep learn base approach multilingual aspect base sentiment analysis part semeval two thousand and sixteen task five use convolutional neural network cnn aspect extraction aspect base sentiment analysis cast aspect extraction multi label classification problem output probabilities aspects parameterized threshold determine sentiment towards aspect concatenate aspect vector every word embed apply convolution constrain system unconstrained english achieve competitive result across languages domains place first second five seven eleven language domain pair aspect category detection slot one sentiment polarity slot three respectively thereby demonstrate viability deep learn base approach multilingual aspect base sentiment analysis
computer science divide conquer dandc algorithm design paradigm base multi branch recursion dandc algorithm work recursively monotonically break problem sub problems relate type become simple enough solve directly solutions sub problems combine give solution original problem present work identify dandc algorithms assume within contemporary syntactic theory discuss limit applicability realms syntax semantics syntax morphophonology interfaces propose dandc algorithms valid process fall short flexibility give mix approach structure linguistic phrase markers arguments favour computationally mix approach linguistic structure present alternative offer advantage uniform dandc approach
natural language understand nlu core component speak dialogue system recently recurrent neural network rnn obtain strong result nlu due superior ability preserve sequential information time traditionally nlu module tag semantic slot utterances consider flat structure underlie rnn structure linear chain however natural language exhibit linguistic properties provide rich structure information better understand paper introduce novel model knowledge guide structural attention network k san generalization rnn additionally incorporate non flat network topologies guide prior knowledge two characteristics one important substructures capture small train data allow model generalize previously unseen test data two model automatically figure salient substructures essential predict semantic tag give sentence understand performance improve experiment benchmark air travel information system atis data show propose k san architecture effectively extract salient knowledge substructures attention mechanism outperform performance state art neural network base frameworks
creativity complex multi faceted concept encompass variety relate aspects abilities properties behaviours wish study creativity scientifically tractable well articulate model creativity require model would great value researchers investigate nature creativity particular concern evaluation creative practice paper describe unique approach develop suitable model creative behaviour emerge base word people use describe concept use techniques field statistical natural language process identify collection fourteen key components creativity analysis corpus academic paper topic word identify appear significantly often connection discussions concept use measure lexical similarity help cluster word number distinct theme emerge collectively contribute comprehensive multi perspective model creativity components provide ontology creativity set build block use model creative practice variety domains components employ two case study evaluate creativity computational systems prove useful articulate achievements work directions research
describe microsoft conversational speech recognition system combine recent developments neural network base acoustic language model advance state art switchboard recognition task inspire machine learn ensemble techniques system use range convolutional recurrent neural network vector model lattice free mmi train provide significant gain acoustic model architectures language model rescoring multiple forward backward run rnnlms word posterior base system combination provide twenty boost best single system use resnet architecture acoustic model rnnlm rescoring achieve word error rate sixty-nine nist two thousand switchboard task combine system error rate sixty-two represent improvement previously report result benchmark task
events entities closely relate entities often actors participants events events without entities uncommon interpretation events entities highly contextually dependent exist work information extraction typically model events separately entities perform inference sentence level ignore rest document paper propose novel approach model dependencies among variables events entities relations perform joint inference variables across document goal enable access document level contextual information facilitate context aware predictions demonstrate approach substantially outperform state art methods event extraction well strong baseline entity extraction
text simplification ts aim reduce lexical structural complexity text still retain semantic mean current automatic ts techniques limit either lexical level applications manually define large amount rule since deep neural network powerful model achieve excellent performance many difficult task paper propose use long short term memory lstm encoder decoder model sentence level ts make minimal assumptions word sequence conduct preliminary experiment find model able learn operation rule reverse sort replace sequence pair show model may potentially discover apply rule modify sentence structure substitute word remove word ts
attention mechanism important part neural machine translation nmt report produce richer source representation compare fix length encode sequence sequence model recently effectiveness attention also explore context image caption work assess feasibility multimodal attention mechanism simultaneously focus image natural language description generate description another language train several variants propose attention mechanism multi30k multilingual image caption dataset show dedicate attention modality achieve sixteen point bleu meteor compare textual nmt baseline
propose approximate strategy efficiently train neural network base language model large vocabularies approach call adaptive softmax circumvent linear dependency vocabulary size exploit unbalance word distribution form cluster explicitly minimize expectation computation time approach reduce computational time exploit specificities modern architectures matrix matrix vector operations make particularly suit graphical process units experiment carry standard benchmarks europarl one billion word show approach bring large gain efficiency standard approximations achieve accuracy close full softmax code method available https githubcom facebookresearch adaptive softmax
compare automatic speech recognition asr human auditory system adept handle noise adverse situations include environmental noise channel distortion mimic adeptness auditory model widely incorporate asr systems improve robustness paper propose novel auditory model incorporate psychoacoustics otoacoustic emissions oaes asr particular successfully implement frequency dependent property psychoacoustic model effectively improve result system performance also present novel double transform spectrum analysis technique qualitatively predict asr performance different noise type detail theoretical analysis provide show effectiveness propose algorithm experiment carry aurora2 database show word recognition rate use propose feature extraction method significantly increase baseline give model train clean speech propose method achieve eight thousand, five hundred and thirty-nine word recognition accuracy noisy data
use know speaker intrinsic normalization procedure formant data scale reciprocal geometric mean first three formant frequencies reduce influence talker result distort vowel space propose speaker extrinsic procedure scale normalize value mean formant value vowels test formant data vowels publish peterson barney combine approach lead well separate cluster reduce spread due talkers propose procedure perform better two top rank normalization procedures base accuracy vowel classification objective measure
language students engage read texts appropriate difficulty level however exist methods evaluate text difficulty focus mainly vocabulary prioritize grammatical feature hence work well language learners limit knowledge grammar paper introduce grammatical templates expert identify units grammar students learn class important feature text difficulty evaluation experimental classification result show grammatical template feature significantly improve text difficulty prediction accuracy baseline readability feature seventy-four moreover build simple human understandable text difficulty evaluation approach eight hundred and seventy-seven accuracy use five grammatical template feature
user machine interaction important speak content retrieval text content retrieval user easily scan select list retrieve item impossible speak content retrieval retrieve items difficult show screen besides due high degree uncertainty speech recognition retrieval result noisy one way counter difficulties user machine interaction machine take different action interact user obtain better retrieval result show user suitable action depend retrieval status example request extra information user return list topics user select etc previous work hand craft state estimate present retrieval result use determine proper action paper propose use deep q learn techniques instead determine machine action interactive speak content retrieval deep q learn bypass need estimation hand craft state directly determine best action base present retrieval status even without human knowledge show achieve significantly better performance compare previous hand craft state
multimodal sentiment analysis draw increase amount attention days enable mine opinions video review available aplenty online platforms however multimodal sentiment analysis high quality data set annotate train machine learn algorithms limit resources restrict generalizability model example unique characteristics speakers eg wear glass may become confound factor sentiment classification task paper propose select additive learn sal procedure improve generalizability train neural network multimodal sentiment analysis experiment show sal approach improve prediction accuracy significantly three modalities verbal acoustic visual well fusion result show sal even train one dataset achieve good generalization across two new test datasets
due wide use personal importantly professional contexts email represent valuable source information harvest understand reengineering repurposing undocumented business process company institutions towards aim researchers investigate problem extract process orient information email log order take benefit many available process mine techniques tool paper go direction propose new method mine process model email log leverage unsupervised machine learn techniques little human involvement moreover method allow semi automatically label email activity name use activity recognition new incoming email use case demonstrate usefulness propose solution use modest size yet real world dataset contain email belong two different process model
automatic accurate classification items enable numerous downstream applications many domains applications range faceted browse items product recommendations big data analytics online recruitment domain refer classify job ads pre define custom occupation categories job title classification large scale job title classification system power various downstream applications semantic search job recommendations labor market analytics paper discuss experiment conduct improve house job title classification system classification component system compose two stage coarse fine level classifier cascade classify input text job title job ads one thousands job title taxonomy improve classification accuracy effectiveness experiment various semantic representation strategies average w2v vectors document similarity measure word movers distance wmd initial result show overall improvement accuracy carotene1
recognize implicit discourse relations challenge important task field natural language process complex text process task different previous study argue necessary repeatedly read arguments dynamically exploit efficient feature useful recognize discourse relations mimic repeat read strategy propose neural network multi level attention nnma combine attention mechanism external memories gradually fix attention specific word helpful judge discourse relations experiment pdtb dataset show propose method achieve state art result visualization attention weight also illustrate progress model observe arguments level progressively locate important word
language recognition task reject differentiate closely space versus acoustically far space languages remain major challenge confusable closely space languages system need longer input test duration material obtain sufficient information distinguish languages alternatively languages distinct acoustically linguistically similar others duration sufficient remedy solution propose explore duration distribution analysis near far languages base language recognition vector machine learn challenge two thousand and fifteen lrimlc15 database use knowledge propose likelihood ratio base fusion approach leverage score duration information experimental result show use duration score fusion improve language recognition performance five relative lrimlc15 cost
visual question answer vqa task showcased new stage interaction language vision two pivotal components artificial intelligence however mostly focus generate short repetitive answer mostly single word fall short rich linguistic capabilities humans introduce full sentence visual question answer fsvqa dataset consist nearly one million pair question full sentence answer image build apply number rule base natural language process techniques original vqa dataset caption ms coco dataset pose many additional complexities conventional vqa task provide baseline approach evaluate task top invite research community build improvements
convolutional neural network cnns demonstrate superior capability extract information raw signal computer vision recently character level multi channel cnns exhibit excellent performance sentence classification task apply cnns large scale authorship attribution aim determine unknown text author among many candidate author motivate ability process character level signal differentiate large number class make fast predictions comparison state art approach extensively evaluate cnn base approach leverage word character channel compare state art methods large range author number shed new light traditional approach show character level cnns outperform state art four five datasets different domains additionally present first application authorship attribution reddit
entity image could provide significant visual information knowledge representation learn conventional methods learn knowledge representations merely structure triple ignore rich visual information extract entity image paper propose novel image embody knowledge representation learn model ikrl knowledge representations learn triple facts image specifically first construct representations image entity neural image encoder image representations integrate aggregate image base representation via attention base method evaluate ikrl model knowledge graph completion triple classification experimental result demonstrate model outperform baselines task indicate significance visual information knowledge representations capability model learn knowledge representations image
paper new statistic feature discrete short time amplitude spectrum discover experiment signal unvoiced pronunciation random vary short time spectrum feature reveal relationship amplitude average standard every frequency component hand association amplitude distributions different frequency components also study new model represent association inspire normalize histogram amplitude mathematical analysis new statistic feature discover prove necessary evidence support propose model also direct evidence widely use hypothesis identical distribution amplitude frequencies
work explore deep generative model text latent representation document draw discrete language model distribution formulate variational auto encoder inference model apply task compress sentence application generative model first draw latent summary sentence background language model subsequently draw observe sentence condition latent summary empirical evaluation show generative formulations abstractive extractive compression yield state art result train large amount supervise data explore semi supervise compression scenarios show possible achieve performance competitive previously propose supervise model train fraction supervise data
paper present result speaker recognition sr children speech use ogi kid corpus gmm ubm gmm svm sr systems regions spectrum contain important speaker information children identify conduct sr experiment twenty-one frequency band adults spectrum split four regions first contain primary vocal tract resonance information third correspond high frequency speech sound useful sr however frequencies regions occur eleven thirty-eight higher children also note subband sr rat lower younger children finally result present sr experiment identify child class thirty children similar age school two hundred and eighty-eight children vary age class performance depend age accuracy vary ninety young children ninety-nine older children identification rate achieve child school eighty-one
power law ubiquitous natural social phenomena consider universal relationship frequency rank diverse social systems however general model still lack interpret seemingly unrelated systems share great similarity detail analysis natural language texts simulation experiment base propose hierarchical selection model find existence hierarchies human pursuit top hierarchy lead power law power law statistical emergent performance hierarchies universality hierarchies contribute ubiquity power law
recent neural network sequence model softmax classifiers achieve best language model performance large hide state large vocabularies even struggle predict rare unseen word even context make prediction unambiguous introduce pointer sentinel mixture architecture neural sequence model ability either reproduce word recent context produce word standard softmax classifier pointer sentinel lstm model achieve state art language model performance penn treebank seven hundred and nine perplexity use far fewer parameters standard softmax lstm order evaluate well language model exploit longer contexts deal realistic vocabularies larger corpora also introduce freely available wikitext corpus
study problem recognize video sequence fingerspell letter american sign language asl fingerspell comprise significant relatively understudy part asl recognize fingerspell challenge number reason involve quick small motion often highly coarticulated exhibit significant variation signers dearth continuous fingerspell data collect work collect annotate new data set continuous fingerspell videos compare several type recognizers explore problem signer variation best perform model segmental semi markov conditional random field use deep neural network base feature signer dependent set recognizers achieve ninety-two letter accuracy multi signer set much challenge neural network adaptation achieve eighty-three letter accuracies set
multilingual question answer either question need translate document language vice versa addition direction multiple methods perform translation four explore paper word base ten best context base grammar base build feature combination translation direction method train model learn optimal feature weight large forum dataset consist post english arabic chinese novel learn translate approach effective strong baseline p005 translate text english train classifier base english original translate text
describe course hackathon dedicate development linguistic tool tibetan buddhist study period five days group seventeen scholars scientists students develop compare algorithms intertextual alignment text classification along basic language tool include stemmer word segmenter
motivate need automate medical information extraction free text radiological report present bi directional long short term memory bilstm neural network architecture model radiological language model use address two nlp task medical name entity recognition ner negation detection investigate whether learn several type word embeddings improve bilstm performance task use large dataset chest x ray report compare propose model baseline dictionary base ner system negation detection system leverage hand craft rule negex algorithm grammatical relations obtain stanford dependency parser compare traditional rule base systems argue bilstm offer strong alternative task
plda popular normalization approach vector model deliver state art performance speaker verification however plda train require large amount label development data highly expensive case possible approach mitigate problem various unsupervised adaptation methods use unlabeled data adapt plda scatter matrices target domain paper present new local train approach utilize inaccurate much cheaper local label train plda model local label discriminate speakers within single conversion much easier obtain compare normal global label experiment show propose approach deliver significant performance improvement particularly limit globally label data
paper present unify model perform language speaker recognition simultaneously altogether model base multi task recurrent neural network output one task feed input lead collaborative learn framework improve language speaker recognition borrow information experiment demonstrate multi task model outperform task specific model task
present ap16 ol7 database release train test data oriental language recognition olr challenge apsipa two thousand and sixteen base database baseline system construct basis vector model report baseline result evaluate various metrics define ap16 olr evaluation plan demonstrate ap16 ol7 reasonable data resource multilingual research
paper describe system dub ws4a web service participate fourth edition bioasq challenge two thousand and sixteen use ws4a perform question answer qa task 4b consist retrieval relevant concepts document snippets rdf triple exact answer ideal answer give question novelty approach consist maximum exploitation exist web service step ws4a annotation text retrieval metadata annotation information retrieve include concept identifiers ontologies ancestors importantly pubmed identifiers paper describe ws4a pipeline also present precision recall f measure value obtain task 4b system achieve two second place two subtasks one five batch
yous highest rate firearm relate deaths compare industrialize countries violence particularly affect low income urban neighborhoods cities like chicago saw forty increase firearm violence two thousand and fourteen two thousand and fifteen three thousand shoot victims recent study find urban gang involve individuals curate unique complex communication style within social media platforms organizations focus reduce gang violence struggle keep grow complexity social media platforms sheer volume data present paper describe digital urban violence analysis approach duvva collaborative qualitative analysis method use collaboration data scientists social work researchers develop suite systems decode high stress language urban gang involve youth approach leverage principles ground theory analyze approximately eight hundred tweet post chicago gang members participation youth chicago neighborhoods create language resource natural language process nlp methods uncover unique language communication style develop automate tool potential detect aggressive language social media aid individuals group perform violence prevention interruption
work present first result neuralizing unsupervised hide markov model evaluate approach tag duction approach outperform exist generative model competitive state art though simpler model easily extend include additional context
rumour stance classification task determine tweet collection discuss rumour support deny question simply comment rumour attract substantial interest introduce novel approach make use sequence transition observe tree structure conversation thread twitter conversation thread form harvest users reply one another result nest tree like structure previous work address stance classification task treat tweet separate unit analyse tweet virtue position sequence test two sequential classifiers linear chain crf tree crf make different assumptions conversational structure experiment eight twitter datasets collect break news show exploit sequential structure twitter conversations achieve significant improvements non sequential methods work first model twitter conversations tree structure manner introduce novel way tackle nlp task twitter conversations
recently surge study obtain partially annotate data model supervision however still lack systematic study train statistical model partial annotation pa take dependency parse case study paper describe compare two straightforward approach three mainstream dependency parsers first approach previously propose directly train log linear graph base parser llgpar pa base forest base objective work first time propose second approach directly train linear graph base parse lgpar linear transition base parser ltpar pa base idea constrain decode conduct extensive experiment penn treebank three different settings simulate pa ie random dependencies uncertain dependencies dependencies divergent output three parsers result show llgpar effective learn pa ltpar lag behind graph base counterparts large margin moreover lgpar ltpar achieve best performance use llgpar complete pa full annotation fa
compare effectiveness four different syntactic ccg parsers semantic slot fill task explore much syntactic supervision require downstream semantic analysis extrinsic task base evaluation provide unique window explore strengths weaknesses semantics capture unsupervised grammar induction systems release new freebase semantic parse dataset call spade semantic parse declarative sentence contain 93k cloze style question pair answer evaluate model dataset code data available https githubcom sivareddyg graph parser
translate real time aka simultaneous translation output translation word input sentence end challenge problem conventional machine translation methods propose neural machine translation nmt framework simultaneous translation agent learn make decisions translate interaction pre train nmt environment trade quality delay extensively explore various target delay design method beam search applicable simultaneous mt set experiment state art baselines two language pair demonstrate efficacy propose framework quantitatively qualitatively
describe arabic hebrew parallel corpus ted talk build upon wit3 web inventory repurposes original content ted website way convenient mt researchers benchmark consist two thousand talk whose subtitle arabic hebrew accurately align rearrange sentence total 35m tokens per language talk partition train development test set similarly respect mt task iwslt two thousand and sixteen evaluation campaign addition describe benchmark list problems encounter prepare novel methods design solve baseline mt result measure sentence length provide extrinsic evaluation quality benchmark
evolution internet create abundance unstructured data web significant part textual task author profile seek find demographics people solely linguistic content base feature text ability describe traits author clearly applications field security forensics well market instead see age classification problem also frame age regression one use ensemble chain method incorporate power classification regression learn author exact age
paper introduce event capture annotation tool ecat user friendly open source interface tool annotate events participants video capable extract 3d position orientations object video capture microsoft kinectr hardware model language voxml pustejovsky krishnaswamy two thousand and sixteen underlie ecat object program attribute representations although ecat use spec explicit label motion instance demonstration show tool workflow options available capture event participant relations browse visual data map ecat output voxml also address
pascal challenge entitle monaural multi talker speech recognition develop target problem robust automatic speech recognition speech like noise significantly degrade performance automatic speech recognition systems challenge two compete speakers say simple command simultaneously objective recognize speech target speaker surprisingly challenge team ibm research could achieve performance better human listeners task propose method ibm team consist intermediate speech separation single talker speech recognition paper reconsider task challenge base gain adapt factorial speech process model develop joint token pass algorithm direct utterance decode target masker speakers simultaneously compare challenge winner use maximum uncertainty decode use past two phase method provide detail derivation inference model base general inference procedures probabilistic graphical model another improvement use deep neural network joint speaker identification gain estimation make two step easier produce competitive result step propose method work outperform past super human result even result achieve recently microsoft research use deep neural network achieve fifty-five absolute task performance improvement compare first super human system twenty-seven absolute task performance improvement compare recent competitor
besides speak word speech signal also carry information speaker gender age emotional state use variety speech analysis applications paper divide conquer strategy ensemble classification propose recognize emotions speech intrinsic hierarchy emotions utilize construct emotions tree assist break emotion recognition task smaller sub task propose framework generate predictions three phase firstly emotions detect input speech signal classify neutral emotional speech classify emotional second phase classify positive negative class finally individual positive negative emotions identify base outcomes previous stag several experiment perform widely use benchmark dataset propose method able achieve improve recognition rat compare several approach
word embeddings extensively study large text datasets however study analyze semantic representations small corpora particularly relevant single person text production study present paper compare skip gram lsa capabilities scenario test techniques extract relevant semantic pattern single series dream report lsa show better performance skip gram small size train corpus two semantic test study case show lsa capture relevant word associations dream report series even case small number dream low frequency word propose lsa use explore word associations dream report could bring new insight classic research area psychology
use microblogging platforms twitter crises become widespread importantly information disseminate affect people contain useful information like report miss find people request urgent need etc rapid crisis response humanitarian organizations look situational awareness information understand assess severity crisis paper present novel framework generate abstractive summaries useful situational awareness ii capture sub topics present short informative summary topics summary generate use two stage framework first extract set important tweet whole set information integer linear program ilp base optimization technique follow word graph concept event base abstractive summarization technique produce final summary high accuracies obtain task show effectiveness propose framework
nowadays lot data collect online forums one key task determine social structure online group example identification subgroups within larger group approach group individual classification problem classifier base fuzzy logic input classifier linguistic feature degree relationships among individuals output classifiers group individuals also incorporate method rank members detect subgroup identify hierarchies subgroup data hbo television show wire use analyze efficacy usefulness fuzzy logic base methods alternative methods classical statistical methods usually use problems propose methodology could detect automatically influential members organization wire ninety accuracy
emerge challenge online classification social media data stream keep categories use classification date paper propose innovative framework base expert machine crowd emc triad help categorize items continuously identify novel concepts heterogeneous data stream often riddle outliers unify constrain cluster outlier detection formulate novel optimization problem cod mean design algorithm solve cod mean problem show cod mean help detect novel categories also seamlessly discover human annotation errors improve overall quality categorization process experiment diverse real data set demonstrate approach effective efficient
morphology unbalance languages remain big challenge context machine translation paper propose de couple machine translation morphology generation order better deal problem investigate morphology simplification reasonable trade expect gain generation complexity chinese spanish task optimum morphological simplification gender number purpose design new classification architecture compare standard machine learn techniques obtain best result propose neural base architecture consist several layer embed convolutional follow recurrent neural network finally end sigmoid softmax layer obtain classification result ninety-eight accuracy gender classification ninety-three number classification overall translation improvement seven meteor
adverse reactions cause drug follow release market among lead cause death many countries rapid growth electronically available health relate information ability process large volumes automatically use natural language process nlp machine learn algorithms open new opportunities pharmacovigilance survey find seventy us internet users consult internet require medical information recent years research area address adverse drug reaction adr pharmacovigilance use social media mainly twitter medical forums websites paper show information collect variety internet data source search engines mainly google trend google correlate consider case study two popular major depressive disorder mdd drug duloxetine venlafaxine provide comparative analysis reactions use publicly available alternative data source
present interpretable neural network approach predict understand politeness natural language request model base simple convolutional neural network directly raw text avoid manual identification complex sentiment syntactic feature perform better feature base model previous work importantly use challenge task politeness prediction testbed next present much need understand successful network actually learn present several network visualizations base activation cluster first derivative saliency embed space transformations help us automatically identify several subtle linguistics markers politeness theories analysis reveal multiple novel high score politeness strategies add back new feature reduce accuracy gap original featurized system neural model thus provide clear quantitative interpretation success neural network
linguistic laws constitute one quantitative cornerstones modern cognitive sciences routinely investigate write corpora equivalent transcription oral corpora mean inferences statistical pattern language acoustics bias arbitrary language dependent segmentation signal virtually preclude possibility make comparative study human voice animal communication systems bridge gap propose method allow measure pattern acoustic signal arbitrary origin without need access language corpus underneath method apply six different human languages recover successfully well know laws human communication timescales even phoneme find yet another link complexity criticality biological system methods pave way new comparative study animal communication analysis signal unknown code
imprecise information process play indispensable role intelligent systems especially anthropomorphic intelligent systems intelligent robots new theoretical technological system imprecise information process found principles imprecise information process new theoretical technological system1 different fuzzy technology system clear hierarchy rigorous structure result formation principle imprecise information solid mathematical logical base many advantage beyond fuzzy technology system provide technological platform relevant applications lay theoretical foundation research
speaker verification systems vulnerable spoof attack present major problem real life deployment date propose synthetic speech detectors ssds weight importance different segment speech equally however different attack methods different strengths weaknesses trace leave may short long term acoustic artifacts moreover may occur particular phonemes sound propose three algorithms weigh likelihood ratio score individual frame phonemes sound class depend importance ssd significant improvement baseline system obtain know attack methods use train ssds however improvement unknown attack type substantial thus type distortions cause unknown systems different could capture better propose ssd compare baseline ssd
exist machine translation systems operate level word rely explicit segmentation extract tokens introduce neural machine translation nmt model map source character sequence target character sequence without segmentation employ character level convolutional network max pool encoder reduce length source representation allow model train speed comparable subword level model capture local regularities character character model outperform recently propose baseline subword level encoder wmt fifteen de en cs en give comparable performance fi en ru en demonstrate possible share single character level encoder across multiple languages train model many one translation task multilingual set character level encoder significantly outperform subword level encoder language pair observe cs en fi en ru en quality multilingual character level translation even surpass model specifically train language pair alone term bleu score human judgment
classic supervise classification algorithms efficient time consume complicate interpretable make difficult analyze result limit possibility improve base real observations paper propose new simple classifier predict sentiment label short text model keep capacity human interpret ability extend integrate nlp techniques interpretable way model base correlation metric measure degree association sentiment label word ten correlation metrics propose evaluate intrinsically classifier base metric propose evaluate compare classic classification algorithms prove performance many study model outperform algorithms several correlation metrics
long short term memory lstm recurrent neural network rnns show give state art performance many speech recognition task able provide learn dynamically change contextual window sequence history hand convolutional neural network cnns bring significant improvements deep fee forward neural network ffnns able better reduce spectral variation input signal paper network architecture call convolutional recurrent neural network crnn propose combine cnn lstm rnn propose crnns speech frame without adjacent context frame organize number local feature patch along frequency axis lstm network perform feature patch along time axis train compare ffnns lstm rnns propose lstm crnns various number configurations experimental result show lstm crnns exceed state art speech recognition performance
present model visually ground language learn base stack gate recurrent neural network learn predict visual feature give image description form sequence phonemes learn task resemble face human language learners need discover structure mean noisy ambiguous data across modalities show model indeed learn predict feature visual context give phonetically transcribe image descriptions show represent linguistic information hierarchy level lower layer stack comparatively sensitive form whereas higher layer sensitive mean
paper explore new evaluation perspectives image caption introduce noun translation task achieve comparative image caption generation performance translate set nouns caption imply image caption word categories nouns evoke powerful language model without sacrifice performance n gram precision paper also investigate lower upper bound much individual word categories caption contribute final bleu score large possible improvement exist nouns verbs prepositions
first objective towards effective use microblogging service twitter situational awareness emerge disasters discovery disaster relate post give wide range possible disasters use pre select set disaster relate keywords discovery suboptimal alternative focus work train classifier use small set label post become available disaster emerge hypothesis utilize large quantities historical microblogs could improve quality classification compare train classifier label data propose use unlabeled microblogs cluster word limit number cluster use word cluster feature classification evaluate propose semi supervise approach use twitter data six different disasters result indicate number label tweet one hundred less propose approach superior standard classification base bag word feature representation result also reveal choice unlabeled corpus choice word cluster algorithm choice hyperparameters significant impact classification accuracy
neural network among state art techniques language model exist neural language model typically map discrete word distribute dense vector representations information process precede context word hide layer output layer estimate probability next word approach time memory intensive large number parameters word embeddings output layer paper propose compress neural language model sparse word representations experiment number parameters model increase slowly growth vocabulary size almost imperceptible moreover approach reduce parameter space large extent also improve performance term perplexity measure
human computer conversation systems context user issue utterance particularly important provide useful background information conversation however unwise track previous utterances current session equally important paper address problem session segmentation propose embed enhance texttiling approach inspire observation conversation utterances highly noisy word embeddings provide robust way capture semantics experimental result show approach achieve better performance texttiling mmd approach
machine read use differentiable reason model recently show remarkable progress context end end trainable memory network memn2n demonstrate promise performance simple natural language base reason task factual reason basic deduction however task namely multi fact question answer positional reason dialog relate task remain challenge particularly due necessity complex interactions memory controller modules compose family model paper introduce novel end end memory access regulation mechanism inspire current progress connection short cut principle field computer vision concretely develop gate end end trainable memory network architecture gmemn2n machine learn perspective new capability learn end end fashion without use additional supervision signal far knowledge go first kind experiment show significant improvements challenge task twenty babi dataset without use domain knowledge show improvements dialog babi task include real human bot conversion base dialog state track challenge dstc two dataset two datasets model set new state art
many methods use recognize author personality traits text typically combine linguistic feature engineer shallow learn model eg linear regression support vector machine work use deep learn base model atomic feature text character build hierarchical vectorial word sentence representations trait inference method apply corpus tweet show state art performance across five traits three languages english spanish italian compare prior work author profile result support preliminary visualisation work encourage ability detect complex human traits
accord distributional inclusion hypothesis entailment word measure via feature inclusions distributional vectors recent work show hypothesis extend word phrase sentence set compositional distributional semantics paper focus inclusion properties tensors main contribution theoretical experimental analysis feature inclusion work different concrete model verb tensors present result relational frobenius projective holistic methods compare simple vector addition multiplication min max model degrees entailment thus obtain evaluate via variety exist word base measure weed clarke kl divergence apinc balapinc two previously propose metrics phrase sentence level perform experiment three entailment datasets investigate version tensor base composition achieve highest performance combine sentence level measure
sentence similarity consider basis many natural language task information retrieval question answer text summarization semantic mean compare text fragment base word semantic feature relationships article review set word sentence similarity measure compare benchmark datasets study datasets result show hybrid semantic measure perform better knowledge corpus base measure
article deal issue modification metric classification algorithms particular study algorithm k nearest neighbour application sequential data method generalization metric classification algorithms propose part develop algorithm solve problem classification label sequential data advantage develop algorithm classification comparison exist one also discuss article comparison effectiveness propose algorithm algorithm crf task chunk open data set conll2000
paper simple text categorization method use term class relevance measure propose initially text document process extract significant term present every term extract document compute importance preserve content class novel term weight scheme know termclass relevance tcr measure propose guru suhil two thousand and fifteen one way every term relevance class present corpus compute store knowledgebase test term present test document extract term class relevance term obtain store knowledgebase achieve quick search term weight btree index data structure adapt finally class receive maximum support term term class relevance decide class give test document propose method work logarithmic complexity test time simple implement compare text categorization techniques available literature experiment conduct various benchmarking datasets reveal performance propose method satisfactory encourage
recently neural network achieve great success sentiment classification due ability alleviate feature engineer however one remain challenge model long texts document level sentiment classification recurrent architecture deficiency memory unit address problem present cache long short term memory neural network clstm capture overall semantic information long texts clstm introduce cache mechanism divide memory several group different forget rat thus enable network keep sentiment information better within recurrent unit propose clstm outperform state art model three publicly available document level sentiment analysis datasets
conversational speech recognition serve flagship speech recognition task since release switchboard corpus 1990s paper measure human error rate widely use nist two thousand test set find latest automate system reach human parity error rate professional transcribers fifty-nine switchboard portion data newly acquaint pair people discuss assign topic one hundred and thirteen callhome portion friends family members open end conversations case automate system establish new state art edge past human benchmark achieve error rat fifty-eight one hundred and ten respectively key system performance use various convolutional lstm acoustic model architectures combine novel spatial smooth method lattice free mmi acoustic train multiple recurrent neural network language model approach systematic use system combination
vocalizations less often gesture object linguistic research decades however development general theory communication human language particular case require clear understand organization communication mean infochemicals chemical compound carry information employ small organisms emit acoustic signal optimal frequency achieve successful communication distribution infochemicals across species investigate rank degree number species associate produce sensitive quality fit different function dependency degree rank evaluate penalty number parameters function surprisingly double zipf zipf distribution two regimes different exponent model yield best fit although function largest number parameters suggest world wide repertoire infochemicals contain chemical nucleus share many species reminiscent core vocabularies find human language dictionaries large corpora
function word adjacency network wan use study authorship play early modern english period network nod function word direct edge two nod represent relative frequency direct co appearance two word every analyze play wan construct aggregate generate author profile network first study similarity write style early english playwrights compare profile wan accuracy use wan authorship attribution demonstrate attribute know play among six popular playwrights moreover wan method show outperform frequency base methods attribute early english play addition wan show reliable classifiers even attribute collaborative play several play dispute co authorship deeper analysis perform attribute every act scene separately corroborate exist breakdowns provide evidence new assignments
state art speech recognition systems typically employ neural network acoustic model however compare gaussian mixture model deep neural network dnn base acoustic model often many model parameters make challenge deploy resource constrain platforms mobile devices paper study application recently propose highway deep neural network hdnn train small footprint acoustic model hdnns depth gate feedforward neural network include two type gate function facilitate information flow different layer study demonstrate hdnns compact regular dnns acoustic model ie achieve comparable recognition accuracy many fewer model parameters furthermore hdnns controllable dnns gate function hdnn control behavior whole network use small number model parameters finally show hdnns adaptable dnns example simply update gate function use adaptation data result considerable gain accuracy demonstrate aspects experiment use publicly available ami corpus around eighty hours train data
propose attention enable encoder decoder model problem grapheme phoneme conversion previous work tackle problem via joint sequence model require explicit alignments train contrast attention enable encoder decoder model allow jointly learn align convert character phonemes explore different type attention model include global local attention best model achieve state art result three standard data set cmudict pronlex nettalk
machine learn big success story ai resurgence one particular stand success relate unsupervised learn massive amount data albeit much relate one modality type data time spite early assertions unreasonable effectiveness data increase recognition utilize knowledge whenever available create purposefully paper focus discuss indispensable role knowledge deeper understand complex text multimodal data situations large amount train data label unlabeled available labor intensive create ii object particularly text recognize complex ie beyond simple entity person location organization name implicit entities highly subjective content iii applications need use complementary relate data multiple modalities media bring us cusp rapid progress ability create knowledge vary comprehensive cross domain domain application specific b carefully exploit knowledge empower extend applications ml nlp techniques use early result several diverse situations data type applications seek foretell unprecedented progress ability deeper understand exploitation multimodal data
vector representation sentence important many text process task involve cluster classify rank sentence recently distribute representation sentence learn neural model unlabeled data show outperform traditional bag word representation however learn methods consider content sentence disregard relations among sentence discourse large paper propose series novel model learn latent representations sentence sen2vec consider content sentence well inter sentence relations first represent inter sentence relations language network use network induce contextual information content base sen2vec model two different approach introduce exploit information network first approach retrofit already train sen2vec vectors respect network two different ways one use adjacency relations node two use stochastic sample method flexible sample neighbor node second approach use regularizer encode information network exist sen2vec model experimental result show propose model outperform exist methods three fundamental information system task demonstrate effectiveness approach model leverage computational power multi core cpus achieve fine grain computational efficiency make code publicly available upon acceptance
product review websites provide incredible lens wide variety opinions experience different people play critical role help users discover products match personal need preferences help address question easily answer read others review review websites also allow users pose question community via question answer qa system one would expect opinions diverge among different reviewers answer question may also subjective opinionated divergent mean answer question automatically quite different traditional qa task assume single correct answer available recent work introduce idea question answer use product review account two aspects consider paper one question multiple often divergent answer full spectrum answer somehow use train system two make good answer depend asker answerer factor incorporate order system personalize build new qa dataset eight hundred thousand question thirty-one million answer show explicitly account personalization ambiguity lead quantitatively better answer also nuanced view range support subjective opinions
paper two part first part discuss word embeddings discuss need methods create interest properties also compare image embeddings see word embed image embed combine perform different task second part implement convolutional neural network train top pre train word vectors network use several sentence level classification task achieve state art comparable result demonstrate great power pre trainted word embeddings random ones
several mechanisms focus attention neural network select part input memory use successfully deep learn model recent years attention improve image classification image caption speech recognition generative model learn algorithmic task probably largest impact neural machine translation recently similar improvements obtain use alternative mechanisms focus single part memory operate parallel uniform way mechanism call active memory improve attention algorithmic task image process generative model far however active memory improve attention natural language process task particular machine translation analyze shortcoming paper propose extend model active memory match exist attention model neural machine translation generalize better longer sentence investigate model explain previous active memory model succeed finally discuss active memory bring benefit attention better choice
extract entities relations type interest text important understand massive text corpora traditionally systems entity relation extraction rely human annotate corpora train adopt incremental pipeline systems require additional human expertise port new domain vulnerable errors cascade pipeline paper investigate joint extraction type entities relations label data heuristically obtain knowledge base ie distant supervision algorithm type label via distant supervision context agnostic noisy train data pose unique challenge task propose novel domain independent framework call cotype run data drive text segmentation algorithm extract entity mention jointly embed entity mention relation mention text feature type label two low dimensional space entity relation mention respectively space object whose type close also similar representations cotype use learn embeddings estimate type test unlinkable mention formulate joint optimization problem learn embeddings text corpora knowledge base adopt novel partial label loss function noisy label data introduce object translation function capture cross constraints entities relations experiment three public datasets demonstrate effectiveness cotype across different domains eg news biomedical average twenty-five improvement f1 score compare next best method
humans continuously adapt style language variety domains however reliable definition domain elude researchers thus far additionally notion discrete domains stand contrast multiplicity heterogeneous domains humans navigate many overlap order better understand change variation human language draw research domain adaptation extend notion discrete domains continuous spectrum propose representation learn base model adapt continuous domains detail use investigate variation language end propose use dialogue model test bed due proximity language model social component
paper examine benefit perform name entity recognition ner co reference resolution english greek corpus use text segmentation aim examine whether combination text segmentation information extraction beneficial identification various topics appear document ner perform manually english corpus compare output produce publicly available annotation tool already exist tool use greek corpus produce annotations corpora manually correct enrich cover four type name entities co reference resolution ie substitution every reference instance name entity identifier subsequently perform evaluation use five text segmentation algorithms english corpus four greek corpus lead conclusion benefit highly depend segment topic number name entity instance appear well segment length
paper describe end end neural model name entity recognition ner base bi directional rnn lstm almost ner systems hindi use language specific feature handcraft rule gazetteers model language independent use domain specific feature handcraft rule model rely semantic information form word vectors learn unsupervised learn algorithm unannotated corpus model attain state art performance english hindi without use morphological analysis without use gazetteers sort
chinese poetry generation challenge task natural language process paper propose novel two stage poetry generate method first plan sub topics poem accord user write intent generate line poem sequentially use modify recurrent neural network encoder decoder framework propose plan base method ensure generate poem coherent semantically consistent user intent comprehensive evaluation human judgments demonstrate propose approach outperform state art poetry generate methods poem quality somehow comparable human poets
recurrent neural network rnns achieve state art performances many natural language process task language model machine translation however vocabulary large rnn model become big eg possibly beyond memory capacity gpu device train become inefficient work propose novel technique tackle challenge key idea use two component 2c share embed word representations allocate every word vocabulary table row associate vector column associate another vector depend position table word jointly represent two components row vector column vector since word row share row vector word column share column vector need two sqrtv vectors represent vocabulary v unique word far less v vectors require exist approach base two component share embed design new rnn algorithm evaluate use language model task several benchmark datasets result show algorithm significantly reduce model size speed train process without sacrifice accuracy achieve similar better perplexity compare state art language model remarkably one billion word benchmark dataset algorithm achieve comparable perplexity previous language model whilst reduce model size factor forty one hundred speed train process factor two name propose algorithm emphlightrnn reflect small model size high train speed
propose rule base technique generate redundancy free nl descriptions owl entitiesthe exist approach address problem verbalize owl ontologies generate nl text segment close counterpart owl statementssome approach also perform group aggregate nl text segment generate fluent comprehensive form contentrestricting attention description individuals concepts find approach currently follow available tool determine set logical condition satisfy give individual concept name translate condition verbatim correspond nl descriptionshuman understandability descriptions affect presence repetitions redundancies high fidelity owl representationin literature efforts take remove redundancies repetitions logical level generate nl descriptions entities find main reason lack readability generate textherein propose technique call semantic refinementsr generate meaningful easily understandable descriptions individuals concepts give owlontologywe identify combinations owl dl construct lead repetitive redundant descriptions propose series refinement rule rewrite condition satisfy individual concept mean preserve mannerthe reduce set condition employ generate nl descriptionsour experiment show sr lead significantly improve descriptions ontology entitieswe also test effectiveness usefulness generate descriptions purpose validate ontologies find propose technique indeed helpful context
advent web twenty lead increase amount sentimental content available web content often find social media web sit form movie product review user comment testimonials message discussion forums etc timely discovery sentimental opinionated web content number advantage important monetization understand sentiments human mass towards different entities products enable better service contextual advertisements recommendation systems analysis market trend focus project sentiment focus web crawl framework facilitate quick discovery sentimental content movie review hotel review analysis use statistical methods capture elements subjective style sentence polarity paper elaborately discuss two supervise machine learn algorithms k nearest neighbourk nn naive bay compare overall accuracy precisions well recall value see case movie review naive bay give far better result k nn hotel review algorithms give lesser almost accuracies
present novel neural network process sequence bytenet one dimensional convolutional neural network compose two part one encode source sequence decode target sequence two network part connect stack decoder top encoder preserve temporal resolution sequence address differ lengths source target introduce efficient mechanism decoder dynamically unfold representation encoder bytenet use dilation convolutional layer increase receptive field result network two core properties run time linear length sequence sidestep need excessive memorization bytenet decoder attain state art performance character level language model outperform previous best result obtain recurrent network bytenet also achieve state art performance character character machine translation english german wmt translation task surpass comparable neural translation model base recurrent network attentional pool run quadratic time find latent alignment structure contain representations reflect expect alignment tokens
content internet heterogeneous arise various domains like news entertainment finance technology understand content require identify name entities persons place organizations one key step traditionally name entity recognition ner systems build use available annotate datasets like conll muc demonstrate excellent performance however model fail generalize onto domains like sport finance conventions language use differ significantly furthermore several domains large amount annotate label data train robust name entity recognition model key step towards challenge adapt model learn domains large amount annotate train data available domains scarce annotate data paper propose methods effectively adapt model learn one domain onto domains use distribute word representations first analyze linguistic variation present across domains identify key linguistic insights boost performance across domains propose methods capture domain specific semantics word usage addition global semantics demonstrate effectively use domain specific knowledge learn ner model outperform previous baselines domain adaptation set
systems automatic extraction semantic information events large textual resources available tool capable generate rdf datasets text extract events knowledge use reason recognize events hand text base task event recognition example event coreference ie recognize whether two textual descriptions refer event take account ontological information extract events process paper propose method derive event coreference text extract event data use semantic base rule reason demonstrate method consider limit yet representative set event type introduce formal analysis ontological properties base define set coreference criteria implement criteria rdf base reason rule apply text extract event data evaluate effectiveness approach standard coreference benchmark dataset
current image caption methods usually train via penalize maximum likelihood estimation however log likelihood score caption correlate well human assessments quality standard syntactic evaluation metrics bleu meteor rouge also well correlate newer spice cider metrics better correlate traditionally hard optimize paper show use policy gradient pg method directly optimize linear combination spice cider combination call spider spice score ensure caption semantically faithful image cider score ensure caption syntactically fluent pg method propose improve prior mixer approach use monte carlo rollouts instead mix mle train pg show empirically algorithm lead easier optimization improve result compare mixer finally show use pg method optimize metrics include propose spider metric result image caption strongly prefer human raters compare caption generate model train optimize mle coco metrics
typical techniques sequence classification design well segment sequence edit remove noisy irrelevant part therefore methods easily apply noisy sequence expect real world applications paper present temporal attention gate model tagm integrate ideas attention model gate recurrent network better deal noisy unsegmented sequence specifically extend concept attention model measure relevance observation time step sequence use novel gate recurrent network learn hide representation final prediction important advantage approach interpretability since temporal attention weight provide meaningful value salience time step sequence demonstrate merit tagm approach prediction accuracy interpretability three different task speak digit recognition text base sentiment analysis visual event recognition
natural language understand dialogue policy learn essential conversational systems predict next system action response current user utterance conventional approach aggregate separate model natural language understand nlu system action prediction sap pipeline sensitive noisy output error prone nlu address issue propose end end deep recurrent neural network limit contextual dialogue memory jointly train nlu sap dstc4 multi domain human human dialogues experiment show propose model significantly outperform state art pipeline model nlu sap indicate joint model capable mitigate affect noisy nlu output nlu model refine error flow backpropagating extra supervise signal system action
online content publishers often use catchy headline article order attract users websites headline popularly know clickbaits exploit user curiosity gap lure click link often disappoint exist methods automatically detect clickbaits rely heavy feature engineer domain knowledge introduce neural network architecture base recurrent neural network detect clickbaits model rely distribute word representations learn large unannotated corpora character embeddings learn via convolutional neural network experimental result dataset news headline show model outperform exist techniques clickbait detection accuracy ninety-eight f1 score ninety-eight roc auc ninety-nine
ontologies different natural languages often differ quality term richness schema richness internal link difference markedly visible compare rich english language ontology non english language counterpart discover alignment useful endeavor serve start point bridge disparity particular work motivate absence inter language link predicate localise versions dbpedia paper propose demonstrate ad hoc system find possible owlequivalentproperty link predicate ontologies different natural languages seek achieve map use pre exist inter language link resources connect give predicate thus methodology stress semantic similarity rather lexical moreover evaluation show system capable outperform baseline system similar one use recent oaei campaign
ever increase scientific literature need natural language interface bibliographic information retrieval systems retrieve relate information effectively paper propose natural language interface nli gibir graph base bibliographic information retrieval system design nli gibir develop novel framework applicable graph base bibliographic information retrieval systems framework integrate algorithms heuristics interpret analyze natural language bibliographic query nli gibir allow users search variety bibliographic data natural language series text linguistic base techniques use analyze answer natural language query include tokenization name entity recognition syntactic analysis find framework effectively represent address complex bibliographic information need thus contributions paper follow first knowledge first attempt propose natural language interface graph base bibliographic information retrieval second propose novel customize natural language process framework integrate original algorithms heuristics interpret analyze natural language bibliographic query third show propose framework natural language interface provide practical solution build real world natural language interface base bibliographic information retrieval systems experimental result show present system correctly answer thirty-nine forty example natural language query vary lengths complexities
understand analyze big data firmly recognize powerful strategic priority deeper interpretation better intelligence big data important transform raw data unstructured semi structure structure data source eg text video image data set curated data contextualized data knowledge maintain make available use end users applications particular data curation act glue raw data analytics provide abstraction layer relieve users time consume tedious error prone curation task context data curation process become vital analytics asset increase add value insights paper identify implement set curation apis make available github researchers developers assist transform raw data curated data curation apis enable developers easily add feature extract keyword part speech name entities persons locations organizations company products diseases drug etc provide synonyms stem extract information items leverage lexical knowledge base english language wordnet link extract entities external knowledge base google knowledge graph wikidata discover similarity among extract information items calculate similarity string number date time data classify sort categorize data various type form distinct class index structure unstructured data applications
paper introduce novel neural network model question answer emphentity base memory network enhance neural network ability represent calculate information long period keep record entities contain text core component memory pool comprise entities state entities state continuously update accord input text question regard input text use search memory pool relate entities answer predict base state retrieve entities compare previous memory network model propose model capable handle fine grain information sophisticate relations base entities formulate several different task question answer problems test propose model experiment report satisfy result
recent research show performance search personalization depend richness user profile normally represent user topical interest paper propose new embed approach learn user profile users embed topical interest space directly utilize user profile search personalization experiment query log major commercial web search engine demonstrate embed approach improve performance search engine also achieve better search performance strong baselines
paper address problem visual question answer propose novel model call vibiknet model base integrate kernelized convolutional neural network long short term memory units generate answer give question image prove vibiknet optimal trade accuracy computational load term memory time consumption validate method vqa challenge dataset compare top perform methods order illustrate performance speed
consider problem produce compact architectures text classification full model fit limit amount memory consider different solutions inspire hash literature propose method build upon product quantization store word embeddings original technique lead loss accuracy adapt method circumvent quantization artefacts experiment carry several benchmarks show approach typically require two order magnitude less memory fasttext slightly inferior respect accuracy result outperform state art good margin term compromise memory usage accuracy
traditional sentiment analysis often use sentiment dictionary extract sentiment information text classify document however emerge informal word phrase user generate content call analysis aware context usually special mean particular context great performance represent inter word relation use sentiment word vectors identify special word base distribute language model word2vec paper represent novel method sentiment representation word particular context detail identify word abnormal sentiment polarity long answer result show improve model show better performance represent word special mean keep well represent special idiomatic pattern finally discuss mean vectors represent field sentiment may different general object base condition
user give tag label valuable resources semantic understand visual media image videos recently new type label mechanism know hash tag become increasingly popular social media sit paper study problem generate relevant useful hash tag short video clip traditional data drive approach tag enrichment recommendation use direct visual similarity label transfer propagation attempt learn direct low cost map video hash tag use two step train process first employ natural language process nlp technique skip gram model neural network train learn low dimensional vector representation hash tag tag2vec use corpus ten million hash tag train embed function map video feature low dimensional tag2vec space learn embed twenty-nine categories short video clip hash tag query video without tag information directly map vector space tag use learn embed relevant tag find perform simple nearest neighbor retrieval tag2vec space validate relevance tag suggest system qualitatively quantitatively user study
propose extension neural network language model adapt prediction recent history model simplify version memory augment network store past hide activations memory access dot product current hide activation mechanism efficient scale large memory size also draw link use external memory neural network cache model use count base language model demonstrate several language model datasets approach perform significantly better recent memory augment network
paper present work apply recurrent deep stack network rdsns robust automatic speech recognition asr task paper also propose efficient yet comparable substitute rdsn bi pass stack network bpsn main idea two model add phoneme level information acoustic model transform acoustic model combination acoustic model phoneme level n gram model experiment show rdsn bpsn substantially improve performances conventional dnns
good dialogue agent ability interact users respond question ask question importantly learn type interaction work explore direction design simulator set synthetic task movie domain allow interactions learner teacher investigate learner benefit ask question offline online reinforcement learn settings demonstrate learner improve ask question finally real experiment mechanical turk validate approach work represent first step develop end end learn interactive dialogue agents
along prosperity recurrent neural network model sequential data power attention mechanism automatically identify salient information image caption aka image description remarkably advance recent years nonetheless exist paradigms may suffer deficiency invariance image different scale rotation etc effective integration standalone attention form holistic end end system paper propose novel image caption architecture term recurrent image captioner textbfric allow visual encoder language decoder coherently cooperate recurrent manner specifically first equip cnn base visual encoder differentiable layer enable spatially invariant transformation visual signal moreover deploy attention filter module differentiable encoder decoder dynamically determine salient visual part also employ bidirectional lstm preprocess sentence generate better textual representations besides propose exploit variational inference optimize whole architecture extensive experimental result three benchmark datasets ie flickr8k flickr30k ms coco demonstrate superiority propose architecture compare state art methods
paper describe construction teknowbase knowledge base technical concepts computer science main information source technical websites webopedia techtarget well wikipedia online textbooks divide knowledge base construction problem two part acquisition entities extraction relationships among entities knowledge base consist approximately one hundred thousand triple conduct evaluation sample triple report accuracy little ninety additionally conduct classification experiment stackoverflow data feature teknowbase achieve improve classification accuracy
intelligent systems capable automatically understand natural language text important many artificial intelligence applications include mobile phone voice assistants computer vision robotics understand language often constitute fit new information previously acquire view world however many machine read systems rely text alone infer mean paper pursue different approach machine read methods make use background knowledge facilitate language understand end develop two methods first method address prepositional phrase attachment ambiguity use background knowledge within semi supervise machine learn algorithm learn label unlabeled data approach yield state art result two datasets strong baselines second method extract relationships compound nouns knowledge aware method compound noun analysis accurately extract relationships significantly outperform baseline make use background knowledge
recently attention mechanism play key role achieve high performance neural machine translation model however compute score function encoder state position decode step attention model greatly increase computational complexity paper investigate adequate vision span attention model context machine translation propose novel attention framework capable reduce redundant score computation dynamically term vision span mean window encoder state consider attention model one step experiment find average window size vision span reduce fifty modest loss accuracy english japanese german english translation task result indicate conventional attention mechanism perform significant amount redundant computation
work propose novel representation learn model compute semantic representations tweet accurately model systematically exploit chronologically adjacent tweet context users twitter timelines task make model user aware well model target tweet exploit rich knowledge user way user write post also summarize topics user write empirically demonstrate propose model outperform state art model predict user profile attribute like spouse education job one thousand, nine hundred and sixty-six two hundred and twenty-seven two hundred and twenty-two respectively
paper propose first model able generate visually ground question diverse type single image visual question generation emerge topic aim ask question natural language base visual input best knowledge lack automatic methods generate meaningful question various type visual input circumvent problem propose model automatically generate visually ground question vary type model take input image caption generate dense caption model sample probable question type generate question sequel experimental result two real world datasets show model outperform strongest baseline term correctness diversity wide margin
currently grow number health consumers ask health relate question online time anywhere effectively lower cost health care common approach use online health expert question answer hqa service health consumers will trust answer professional physicians however answer vary quality depend circumstance addition available hqa service grow predict answer quality hqa service via machine learn become increasingly important challenge hqa service answer normally short texts severely affect data sparsity problem furthermore hqa service lack community feature best answer user vote therefore wisdom crowd available rate answer quality address problems paper prediction hqa answer quality define classification task first base characteristics hqa service feedback medical experts standard hqa service answer quality evaluation define next base characteristics hqa service several novel non textual feature propose include surface linguistic feature social feature finally deep belief network dbn base hqa answer quality prediction framework propose predict quality answer learn high level hide semantic representation physicians answer result prove propose framework overcome problem overly sparse textual feature short text answer effectively identify high quality answer
neural network base sequence sequence model encoder decoder framework successfully apply solve question answer qa problems predict answer statements question however almost previous model fail consider detail context information unknown state systems enough information answer give question scenarios incomplete ambiguous information common set interactive question answer iqa address challenge develop novel model employ context dependent word level attention accurate statement representations question guide sentence level attention better context model also generate unique iqa datasets test model make publicly available employ attention mechanisms model accurately understand output answer require generate supplementary question additional input depend different contexts available user feedback encode directly apply update sentence level attention infer answer extensive experiment qa iqa datasets quantitatively demonstrate effectiveness model significant improvement state art conventional qa model
task generate natural language descriptions image receive lot attention recent years consequently become increasingly important evaluate image caption approach automatic manner paper provide depth evaluation exist image caption metrics series carefully design experiment moreover explore utilization recently propose word mover distance wmd document metric purpose image caption find outline differences similarities metrics relative robustness mean extensive correlation accuracy distraction base evaluations result also demonstrate wmd provide strong advantage metrics
connections relations relation extraction call class tie common distantly supervise scenario one entity tuple may multiple relation facts exploit class tie relations one entity tuple promise distantly supervise relation extraction however previous model effective ignore model property work effectively leverage class tie propose make joint relation extraction unify model integrate convolutional neural network cnn general pairwise rank framework three novel rank loss function introduce additionally effective method present relieve severe class imbalance problem nr relation model train experiment widely use dataset show leverage class tie enhance extraction demonstrate effectiveness model learn class tie model outperform baselines significantly achieve state art performance
introduce new multi modal task computer systems pose combine vision language comprehension challenge identify suitable text describe scene give several similar options accomplish task entail demonstrate comprehension beyond recognize keywords key phrase correspond visual concepts instead require alignment representations two modalities achieve visually ground understand various linguistic elements dependencies new task also admit easy compute well study metric accuracy detect true target among decoy paper make several contributions effective extensible mechanism generate decoy human create image caption instance apply mechanism yield large scale machine comprehension dataset base coco image caption make publicly available human evaluation result dataset inform performance upper bind several baseline competitive learn approach illustrate utility propose task dataset advance image language comprehension also show multi task learn set performance propose task positively correlate end end task image caption
one key task sentiment analysis product review extract product aspects feature users express opinions work focus use supervise sequence label base approach perform task although several extraction methods use sequence label methods conditional random field crf hide markov model hmm propose show supervise approach significantly improve exploit idea concept share across multiple domains example screen aspect iphone iphone screen many electronic devices screen screen appear review new domain product likely aspect know information enable us much better extraction new domain paper propose novel extraction method exploit idea context supervise sequence label experimental result show produce markedly better result without use past information
paper describe methodology use result obtain us complete task give share task consumer health information search chis collocate forum information retrieval evaluation fire two thousand and sixteen isi kolkata share task consist two sub task one task1 give query document set document associate query task classify sentence document relevant query two task two relevant sentence need classify support claim make query oppose claim make query participate sub task percentage accuracy obtain develop system task1 seven thousand, three hundred and thirty-nine third highest among nine team participate share task
automatic profile social media users important task support multitude downstream applications number study use social media content extract study collective social attribute lack substantial research address detection user industry frame task classification use feature engineer ensemble learn industry detection system use post content profile information detect user industry six hundred and forty-three accuracy significantly outperform majority baseline taxonomy fourteen industry class qualitative analysis suggest person industry affect word use perceive mean also number type emotions express
paper extend usual techniques classification result large scale data mine network approach new technology particular design suitable big data use construct open consolidate database raw data four million patent take us patent office one thousand, nine hundred and seventy-six onward build pattern network look patent title also examine full abstract extract relevant keywords accordingly refer classification semantic approach contrast common technological approach consist take topology consider us patent office technological class moreover document approach highly different topological measure strong statistical evidence feature different model suggest method useful tool extract endogenous information
article devote verification empirical heap law european languages use google book ngram corpus data connection word distribution frequency expect dependence individual word number text size analyse term simple probability model text generation show heap exponent vary significantly within characteristic time intervals sixty one hundred years
question answer system qas use information retrieval natural language process nlp reduce human effort numerous qas base user document present today limit provide objective answer process simple question complex question answer exist qas require interpretation current old data well question ask user limitations overcome use deep case neural network hence propose modify qas create deep artificial neural network associative memory text document modify qas process content text document provide find answer even complex question document
paper deal entity extraction task name entity recognition text mine process aim unveil non trivial semantic structure relationships interaction entities communities paper present simple efficient name entity extraction algorithm method name pampo pattern match pos tag base algorithm ner rely flexible pattern match part speech tag lexical base rule develop process texts write portuguese however potentially applicable languages well compare approach current alternatives support name entity recognition ner content write portuguese alchemy zemanta rembrandt evaluation efficacy entity extraction method several texts write portuguese indicate considerable improvement recall f1 measure
article present question seek answer phd research posit analyze natural language text help semantic annotations mine important events navigate large text corpora semantic annotations name entities geographic locations temporal expressions help us mine events give corpora events thus provide us useful mean discover lock knowledge pose three problems help unlock knowledge vault semantically annotate text corpora identify important events ii semantic search iii event analytics
work present novel counter fit method inject antonymy synonymy constraints vector space representations order improve vectors capability judge semantic similarity apply method publicly available pre train word vectors lead new state art performance simlex nine hundred and ninety-nine dataset also show method use tailor word vector space downstream task dialogue state track result robust improvements across different dialogue domains
paper discuss semantic annotations use introduce mathematical algorithmic information underlie imperative code enable compilers produce code transformations enable better performance use approach good performance achieve also better programmability maintainability portability across different hardware architectures exemplify use polynomial equations different degrees
far different study tackle sentiment analysis several domains restaurant movie review problem study scholarly book review different term review style size paper propose combine different feature order present supervise classifiers extract opinion target expressions detect polarities scholarly book review construct label corpus train evaluate methods french book review also evaluate english restaurant review order measure robustness across domains languages evaluation show methods enough robust english restaurant review french book review
extend revise form tim buckwalter arabic lexical morphological resource aramorph extend revise aramorph henceforth xram present address number weaknesses inconsistencies original model allow wider coverage real world classical contemporary formal informal arabic texts build upon previous research xram enhancements include flag selectable usage markers ii probabilistic mildly context sensitive pos tag filter disambiguation rank alternative morphological analyse iii semi automatic increment lexical coverage extraction lexical morphological information exist lexical resources test xram front end python module show remarkable success level
automatic quality evaluation web information task many field applications great relevance especially critical domains like medical one move intuition quality content medical web document affect feature relate specific domain first usage specific vocabulary domain informativeness adoption specific cod like use infoboxes wikipedia article type document eg historical technical ones paper propose leverage specific domain feature improve result evaluation wikipedia medical article particular evaluate article adopt actionable model whose feature relate content article model also directly suggest strategies improve give article quality rely natural language process nlp dictionaries base techniques order extract bio medical concepts text prove effectiveness approach classify medical article wikipedia medicine portal previously manually label wiki project team result experiment confirm consider domain orient feature possible obtain sensible improvements respect exist solutions mainly article approach less correctly classify interest result call research area domain specific feature suitable web data quality assessment
although semi supervise variational autoencoder semivae work image classification task fail text classification task use vanilla lstm decoder perspective reinforcement learn verify decoder capability distinguish different categorical label essential therefore semi supervise sequential variational autoencoder ssvae propose increase capability feed label decoder rnn time step two specific decoder structure investigate verify effective besides order reduce computational complexity train novel optimization method propose estimate gradient unlabeled objective function sample along two variance reduction techniques experimental result large movie review dataset imdb ag news corpus show propose approach significantly improve classification accuracy compare pure supervise classifiers achieve competitive performance previous advance methods state art result obtain integrate pretraining base methods
first step towards agents learn communicate visual environment propose system give visual representations referent cat context sofa identify discriminative attribute ie properties distinguish hastail moreover despite lack direct supervision attribute level model learn assign plausible attribute object sofa hascushion finally present preliminary experiment confirm referential success predict discriminative attribute
recent research show great progress fine grain entity type exist methods require pre define set type train multi class classifier large label data set base multi level linguistic feature thus limit certain domains genres languages paper propose novel unsupervised entity type framework combine symbolic distributional semantics start learn general embeddings entity mention compose embeddings specific contexts use linguistic structure link mention knowledge base learn relate knowledge representations develop novel joint hierarchical cluster link algorithm type mention use representations framework rely annotate data predefined type schema hand craft feature therefore quickly adapt new domain genre language furthermore great flexibility incorporate linguistic structure eg abstract mean representation amr dependency relations improve specific context representation experiment genres news discussion forum show comparable performance state art supervise type systems train large amount label data result various languages english chinese japanese hausa yoruba domains general biomedical demonstrate portability framework
historical texts digitize interest apply natural language process tool archive however performance tool often unsatisfactory due language change genre differences spell normalization heuristics dominant solution deal historical texts approach fail account change usage vocabulary empirical paper assess capability domain adaptation techniques cope historical texts focus classic benchmark task part speech tag evaluate several domain adaptation methods task tag early modern english modern british english texts penn corpora historical english demonstrate feature embed method unsupervised domain adaptation outperform word embeddings brown cluster show importance embed entire feature space rather individual word feature embeddings also give better performance spell normalization combination two methods better still yield five raw improvement tag accuracy early modern english texts
zipf law predict power law relationship word rank frequency language communication systems widely report texts yet remain enigmatic origins computer simulations show language communication systems emerge abrupt phase transition fidelity mappings symbols object since phase transition approximate heaviside step function show zipfian scale emerge asymptotically high rank base laplace transform thereby demonstrate zipf law gradually emerge moment phase transition communicative systems show power law scale behavior explain emergence natural languages phase transition find emergence zipf law language communication suggest use rare word lexicon critical construction effective communicative system phase transition
present new proof o2 multiple context free language contrast recent proof salvati two thousand and fifteen avoidance concepts seem specific two dimensional geometry complex exponential function simple proof create realistic prospect widen result higher dimension find central importance relation extreme free word order class grammars use describe syntax natural language
coreference resolution one first stag deep language understand importance well recognize natural language process community paper propose generative unsupervised rank model entity coreference resolution introduce resolution mode variables unsupervised system achieve five thousand, eight hundred and forty-four f1 score conll metric english data conll two thousand and twelve share task pradhan et al two thousand and twelve outperform stanford deterministic system lee et al two thousand and thirteen three hundred and one
traditionally formation vocabularies study agent base model specially name game random pair agents negotiate word mean associations discrete time step paper propose first approximation novel question extent negotiation word mean associations influence order individuals interact automata network provide adequate mathematical framework explore question computer simulations suggest two dimensional lattices typical feature formation word mean associations recover random scheme update small fraction population time
work develop computational model automata network phonological similarity effect involve formation word mean associations artificial populations speakers classical study show recall experiment memory performance impair phonologically similar word versus dissimilar ones individuals confound phonologically similar word accord predefined parameter main hypothesis critical range parameter work memory mechanisms imply drastic change final consensus entire population theoretical result present proof convergence particular case model within worst case complexity framework computer simulations describe evolution energy function measure amount local agreement individuals main find appearance sudden change energy function critical parameters
informatics around public health increasingly shift professional public spheres work apply linguistic analytics restaurant review yelp order automatically predict official health inspection report consider two type feature set ie keyword detection topic model feature use several classification methods empirical analysis show extract feature predict public health inspection report ninety accuracy use simple support vector machine
present approach base fee forward neural network learn distribution textual document approach inspire neural autoregressive distribution estimatornade model show good estimator distribution discrete value igh dimensional vectors paper present nade successfully adapt case textual data retain nade property sample compute probability observations do exactly efficiently approach also use learn deep representations document competitive learn alternative topic model approach finally describe approach combine regular neural network n gram model substantially improve performance make learn representation sensitive larger document specific context
exist machine translation systems whether phrase base neural rely almost exclusively word level model explicit segmentation paper ask fundamental question neural machine translation generate character sequence without explicit segmentation answer question evaluate attention base encoder decoder subword level encoder character level decoder four language pair en cs en de en ru en fi use parallel corpora wmt fifteen experiment show model character level decoder outperform ones subword level decoder four language pair furthermore ensembles neural model character level decoder outperform state art non neural machine translation systems en cs en de en fi perform comparably en ru
present deep hierarchical recurrent neural network sequence tag give sequence word model employ deep gate recurrent units character word level encode morphology context information apply conditional random field layer predict tag model task independent language independent feature engineer free extend model multi task cross lingual joint train share architecture parameters model achieve state art result multiple languages several benchmark task include pos tag chunk ner also demonstrate multi task cross lingual joint train improve performance various case
recently several work domain natural language process present successful methods word embed among skip gram negative sample know also word2vec advance state art various linguistics task paper propose scalable bayesian neural word embed algorithm algorithm rely variational bay solution skip gram objective detail step step description provide present experimental result demonstrate performance propose algorithm word analogy similarity task six different datasets show competitive original skip gram method
build question answer systems natural language interfaces semantic parse emerge important powerful paradigm semantic parsers map natural language logical form classic representation many important linguistic phenomena modern twist interest learn semantic parsers data introduce new layer statistical computational issue article lay components statistical semantic parser highlight key challenge see semantic parse rich fusion logical statistical world fusion play integral role future natural language understand systems
many language generation task require production text condition structure unstructured input present novel neural network architecture generate output sequence condition arbitrary number input function crucially approach allow choice condition context granularity generation example character tokens marginalise thus permit scalable effective train use framework address problem generate program code mix natural language structure specification create two new data set paradigm derive collectible trade card game magic gather hearthstone third preexist corpus demonstrate marginalise multiple predictors allow model outperform strong benchmarks
parallel texts relatively rare language resource however constitute useful research material wide range applications study present analyse new methodologies develop obtain data previously build comparable corpora methodologies automatic unsupervised make good large scale research task highly practical non parallel multilingual data occur much frequently parallel corpora access easy although parallel sentence considerably useful resource study propose method automatic web crawl order build topic align comparable corpora eg base wikipedia euronewscom also develop new methods obtain parallel sentence comparable data propose methods filtration corpora capable select inconsistent partially equivalent translations methods easily scalable languages evaluation quality create corpora perform analyse impact use statistical machine translation systems experiment present basis polish english language pair texts different domains ie lecture phrasebooks film dialogues european parliament proceed texts contain medicine leaflets also test second method create parallel corpora base data comparable corpora allow automatically expand exist corpus sentence give domain basis analogies find require therefore past parallel resources order train classifier
apply distribute language embed methods natural language process assign vector database entity associate token example token may word occur table row name column vectors typical dimension two hundred capture mean tokens base contexts tokens appear together form vectors apply learn method token sequence derive database describe various techniques extract token sequence database techniques differ complexity token sequence output database information use eg foreign key vectors use algebraically quantify semantic relationships tokens similarities analogies vectors enable dual view data relational meaningful rather purely syntactical text introduce explore new class query call cognitive intelligence ci query extract information database base part relationships encode vectors implement prototype system top spark exhibit power ci query ci query realize via sql udfs power go far beyond text extensions relational systems due information encode vectors also consider various extensions basic scheme include use collection view derive database focus domain interest utilize vectors text external source maintain vectors database evolve explore database without utilize schema latter consider minimal extensions sql vastly improve query expressiveness
article journals customary methods measure book academic impact mainly involve citations easy limit interrogate traditional citation databases scholarly book review researchers attempt use metrics google book libcitation publisher prestige however approach lack content level information determine citation intentions users meanwhile abundant online review resources concern academic book use mine deeper information content utilize altmetric perspectives study measure impact academic book multi granularity mine online review identify factor affect book impact first online review sample academic book amazoncn crawl process multi granularity review mine conduct identify review sentiment polarities aspects sentiment value lastly number positive review negative review aspect sentiment value star value information regard helpfulness integrate via entropy method lead calculation final book impact score result correlation analysis book impact score obtain via method versus traditional book citations show although substantial differences subject areas online book review tend reflect academic impact thus infer online review represent promise source mine book impact within altmetric perspective multi granularity content level moreover propose method might also mean measure book besides academic publications
invariant refinement method self adaptation irm sa design method target development smart cyber physical systems scps allow systematic translation system requirements system architecture express ensemble base component system ebcs however since requirements capture use natural language exist danger misinterpretation due natural language requirements ambiguity could eventually lead design errors thus automation validation design process desirable paper analyze translation process natural language requirements irm sa model ii identify individual step automate validate use natural language process techniques iii propose suitable methods
imagine princess asleep castle wait prince slay dragon rescue tales like famous sleep beauty clearly divide gender roles modern stories bear generation increasingly aware social construct like sexism racism stories tend reinforce gender stereotype counter paper present technique combine natural language process crowdsourced lexicon stereotype capture gender bias fiction apply technique across eighteen billion word fiction wattpad online write community investigate gender representation stories male female character behave describe author use gender stereotype associate community rat find male representation traditional gender stereotype eg dominant men submissive women common throughout nearly every genre corpus however stereotype like sexual violent men associate highly rat stories finally despite women often target negative stereotype female author equally likely write stereotype men
introduce new task visual sense disambiguation verbs give image verb assign correct sense verb ie one describe action depict image textual word sense disambiguation useful wide range nlp task visual sense disambiguation useful multimodal task image retrieval image description text illustration introduce verse new dataset augment exist multimodal datasets coco tuhoi sense label propose unsupervised algorithm base lesk perform visual sense disambiguation use textual visual multimodal embeddings find textual embeddings perform well gold standard textual annotations object label image descriptions available multimodal embeddings perform well unannotated image also verify find use textual multimodal embeddings feature supervise set analyse performance visual sense disambiguation task verse make publicly available download https githubcom spandanagella verse
text independent short utterance speaker recognition susr performance often degrade dramatically paper present combination approach susr task two phonetic aware systems one dnn base vector system recently propose subregion base gmm ubm system former employ phone posteriors construct vector model share statistics offer stronger robustness limit test data latter establish phone dependent gmm ubm system represent speaker characteristics detail score level fusion implement integrate respective advantage two systems experimental result show text independent susr task dnn base vector system subregion base gmm ubm system outperform respective baselines score level system combination deliver performance improvement
present deep neural network dnn acoustic model include parametrised differentiable pool operators unsupervised acoustic model adaptation cast problem update decision boundaries implement pool operator particular experiment two type pool parametrisations learn lp norm pool weight gaussian pool weight operators treat speaker dependent perform investigations use three different large vocabulary speech recognition corpora ami meet ted talk switchboard conversational telephone speech demonstrate differentiable pool operators provide robust relatively low dimensional way adapt acoustic model relative word error rat reductions range five twenty respect unadapted systems better baseline fully connect dnn base acoustic model also investigate propose techniques work various adaptation condition include quality adaptation data complementarity feature model space adaptation methods well provide analysis characteristics propose approach
paper present dataset collect natural dialogs enable test ability dialog systems learn new facts user utterances throughout dialog interactive learn help one prevail problems open domain dialog system sparsity facts dialog system reason propose dataset consist one thousand, nine hundred collect dialogs allow simulation interactive gain denotations question explanations users use interactive learn
natural language correction potential help language learners improve write skills approach separate classifiers different error type high precision flexibly handle errors redundancy non idiomatic phrase hand word phrase base machine translation methods design cope orthographic errors recently outpace neural model motivate issue present neural network base approach language correction core component method encoder decoder recurrent neural network attention mechanism operate character level network avoid problem vocabulary word illustrate flexibility approach dataset noisy user generate text collect english learner forum combine language model method achieve state art f05 score conll two thousand and fourteen share task demonstrate train network additional data synthesize errors improve performance
query relevance rank sentence saliency rank two main task extractive query focus summarization previous supervise summarization systems often perform two task isolation however since reference summaries trade relevance saliency use supervision neither two rankers could train well paper propose novel summarization system call attsum tackle two task jointly automatically learn distribute representations sentence well document cluster meanwhile apply attention mechanism simulate attentive read human behavior query give extensive experiment conduct duc query focus summarization benchmark datasets without use hand craft feature attsum achieve competitive performance also observe sentence recognize focus query indeed meet query need
motivate application fact level image understand present automatic method data collection structure visual facts image caption example structure facts include attribute object eg action eg interactions eg positional information eg collect annotations form fact image pair eg image region contain fact language approach propose method able collect hundreds thousands visual fact annotations accuracy eighty-three accord human judgment method automatically collect three hundred and eighty thousand visual fact annotations one hundred and ten thousand unique visual facts image caption localize image less one day process time standard cpu platforms
present model pragmatically describe scenes contrastive behavior result combination inference drive pragmatics learn semantics like previous learn approach language generation model use simple feature drive architecture pair neural listener speaker model grind language world like inference drive approach pragmatics model actively reason listener behavior select utterances train approach require ordinary caption annotate without demonstration pragmatic behavior model ultimately exhibit human evaluations refer expression game approach succeed eighty-one time compare sixty-nine success rate use exist techniques
nearly previous work neural machine translation nmt use quite restrict vocabularies perhaps subsequent method patch unknown word paper present novel word character solution achieve open vocabulary nmt build hybrid systems translate mostly word level consult character components rare word character level recurrent neural network compute source word representations recover unknown target word need twofold advantage hybrid approach much faster easier train character base ones time never produce unknown word case word base model wmt fifteen english czech translation task hybrid approach offer addition boost twenty-one one hundred and fourteen bleu point model already handle unknown word best system achieve new state art result two hundred and seven bleu score demonstrate character model successfully learn generate well form word czech highly inflect language complex vocabulary also build correct representations english source word
grammar point view role punctuation mark sentence formally define well understand semantic analysis punctuation play also crucial role method avoid ambiguity mean different situation observe statistical analyse language sample decision whether punctuation mark consider neglect see rather arbitrary present belong researcher preference objective work would light onto problem provide us answer question whether punctuation mark may treat ordinary word whether include analysis word co occurences already know previous study sdrozdz et al inf sci three hundred and thirty-one two thousand and sixteen thirty-two forty-four full stop determine length sentence main carrier long range correlations extend study analyze statistical properties common punctuation mark indo european languages investigate frequencies locate accordingly zipf rank frequency plot well study role word adjacency network show statistical viewpoint punctuation mark reveal properties qualitatively similar properties frequent word like article conjunctions pronouns prepositions refer zipfian analysis network analysis add punctuation mark zipf plot also show plot normally describe zipf mandelbrot distribution largely restore power law zipfian behaviour frequent items
present ensemble approach categorize search query entities recruitment domain understand type entities express search query company skill job title etc enable intelligent information retrieval base upon entities compare traditional keyword base search search query typically short leverage traditional bag word model identify entity type would inappropriate due lack contextual information approach instead combine clue different source vary complexity order collect real world knowledge query entities employ distributional semantic representations query entities two model one contextual vectors generate encyclopedic corpora like wikipedia two high dimensional word embed vectors generate millions job post use word2vec additionally approach utilize entity linguistic properties obtain wordnet ontological properties extract dbpedia evaluate approach data set create careerbuilder largest job board us data set contain entities extract millions job seekers recruiters search query job post resume document construct distributional vectors search entities use supervise machine learn infer search entity type empirical result show approach outperform state art word2vec distributional semantics model train wikipedia moreover achieve micro average f one score ninety-seven use propose distributional representations ensemble
take generation chinese classical poem line sequence sequence learn problem build novel system base rnn encoder decoder structure generate quatrains jueju chinese topic word input system jointly learn semantic mean within single line semantic relevance among line poem use structural rhythmical tonal pattern without utilize constraint templates experimental result show system outperform competitive systems also find attention mechanism capture word associations chinese classical poetry invert target line train improve performance
representation learn commonsense knowledge one foundational problems quest enable deep language understand issue particularly challenge understand casual correlational relationships events topic receive lot interest nlp community research hinder lack proper evaluation framework paper attempt address problem new framework evaluate story understand script learn story cloze test test require system choose correct end four sentence story create new corpus 50k five sentence commonsense stories rocstories enable evaluation corpus unique two ways one capture rich set causal temporal commonsense relations daily events two high quality collection everyday life stories also use story generation experimental evaluation show host baselines state art model base shallow language understand struggle achieve high score story cloze test discuss implications script story learn offer suggestions deeper language understand
paper investigate linguistic knowledge mine large text corpora aid generation natural language descriptions videos specifically integrate neural language model distributional semantics train large text corpora recent lstm base architecture video description evaluate approach collection youtube videos well two large movie description datasets show significant improvements grammaticality modestly improve descriptive quality
word sense disambiguation help identify proper sense ambiguous word text large terminologies umls metathesaurus ambiguities appear highly effective disambiguation methods require supervise learn algorithm methods use one approach perform disambiguation feature extract context ambiguous word use identify proper sense word type feature impact machine learn methods thus affect disambiguation performance work evaluate several type feature derive context ambiguous word explore well global feature derive medline use word embeddings result show word embeddings improve performance traditional feature allow well use recurrent neural network classifiers base long short term memory lstm nod combination unigrams word embeddings svm set new state art performance macro accuracy nine thousand, five hundred and ninety-seven msh wsd data set
impressive progress make field computer vision natural language process however remain challenge find best point interaction different modalities chapter discuss attribute allow us exchange information two modalities way lead interaction semantic level specifically discuss attribute allow use knowledge mine language resources recognize novel visual categories generate sentence description image video grind natural language visual content finally answer natural language question image
holy quran holy book muslims contain information many domains often people search particular concepts holy quran base relations among concepts ontological model holy quran useful scenario paper model nature relate concepts holy quran use owl web ontology language rdf resource description framework methodology involve identify nature relate concepts mention holy quran identify relations among concepts concepts relations represent class instance properties owl ontology later result section show use ontological model sparql query retrieve verse concepts interest thus model help semantic search query holy quran work use english translation holy quran sahih international protege owl editor query use sparql
current low bite rate vlbr speech cod systems use hide markov model hmm base speech recognition synthesis techniques allow transmission information phonemes segment segment decrease bite rate however encoder base phoneme speech recognition may create burst segmental errors segmental errors propagate optional suprasegmental syllable information cod together errors voice detection pitch parametrization hmm base speech cod create speech discontinuities unnatural speech sound artefacts paper propose novel vlbr speech cod framework base neural network nns end end speech analysis synthesis without hmms speech cod framework rely phonological sub phonetic representation speech design composition deep spike nns bank phonological analysers transmitter phonological synthesizer receiver realise deep nns spike nn incremental robust encoder syllable boundaries cod continuous fundamental frequency f0 combination phonological feature define much sound pattern phonetic feature define hmm base speech coders finer analysis synthesis code contribute smoother encode speech listeners significantly prefer nn base approach due fewer discontinuities speech artefacts encode speech single forward pass require speech encode decode propose vlbr speech cod operate bite rate approximately three hundred and sixty bits
present result combine supervise unsupervised methods ensemble multiple systems two popular knowledge base population kbp task cold start slot fill cssf tri lingual entity discovery link tedl demonstrate combine system along auxiliary feature outperform best perform system task two thousand and fifteen competition several ensembling baselines well state art stack approach ensembling kbp systems success technique two different challenge problems demonstrate power generality combine approach ensembling
knowledge representation important long history topic ai large amount work knowledge graph embed project symbolic entities relations low dimensional real value vector space however embed methods merely concentrate data fit ignore explicit semantic expression lead uninterpretable representations thus traditional embed methods limit potentials many applications question answer entity classification end paper propose semantic representation method knowledge graph textbfksr impose two level hierarchical generative process globally extract many aspects locally assign specific category aspect every triple since aspects categories semantics relevant collection categories aspect treat semantic representation triple extensive experiment justify model outperform state art baselines substantially
entity disambiguation map phrase canonical representation knowledge base fundamental step many natural language process applications exist techniques base global rank model fail capture individual peculiarities word hence either struggle meet accuracy requirements many real world applications complex satisfy real time constraints applications paper propose new disambiguation system learn specialize feature model disambiguate ambiguous phrase english language train validate hundreds thousands learn model purpose use wikipedia hyperlink dataset one hundred and seventy million label annotations provide extensive experimental evaluation show accuracy approach compare favourably respect many state art disambiguation systems train require approach easily distribute cluster furthermore update system new entities calibrate special ones computationally fast process affect disambiguation entities
answer science question pose natural language important ai challenge answer question often require non trivial inference knowledge go beyond factoid retrieval yet systems task base relatively shallow information retrieval ir statistical correlation techniques operate large unstructured corpora propose structure inference system task formulate integer linear program ilp answer natural language question use semi structure knowledge base derive text include question require multi step inference combination multiple facts dataset real unseen science question system significantly outperform fourteen best previous attempt structure reason task use markov logic network mlns also improve upon previous ilp formulation one hundred and seventy-seven combine unstructured inference methods ilp system significantly boost overall performance ten finally show approach substantially robust simple answer perturbation compare statistical correlation methods
paper explore use learn classifier post ocr text correction experiment arabic language show approach integrate weight confusion matrix shallow language model improve vast majority segmentation recognition errors frequent type error dataset
start description lacan work take analytics methodology first investigation lacan motivate template poe story fit data segmentation storyline use order map diachrony base show synchronous aspects potentially relate lacanian register seek demonstrate effectiveness approach base model template storyline narrative second comprehensive investigation develop approach reveal uncover lacanian register relationships objectives work include wide general application methodology methodology strongly base let data speak correspondence analysis analytics platform jean paul benz ecri also geometric data analysis qualitative quantitative analytics develop pierre bourdieu
previous accent classification research focus mainly detect accent pure acoustic information without recognize accent speech work combine phonetic knowledge vowels acoustic information build guassian mixture model gmm classifier perceptual linear predictive plp feature optimize hetroscedastic linear discriminant analysis hlda input twenty second accent speech system achieve classification rate fifty-one seven way classification system focus major type accent english competitive state art result field
spammer detection social network challenge problem rigid anti spam rule result emergence smart spammers resemble legitimate users difficult identify paper present novel spammer classification approach base latent dirichlet allocationlda topic model approach extract local global information topic distribution pattern capture essence spamming test one benchmark dataset one self collect dataset propose method outperform state art methods term average f1 score
natural language whether speak attend humans process generate computers require network structure reflect creative process semantic syntactic phonetic linguistic social emotional cultural modules able produce novel useful behavior follow repeat practice get root artificial intelligence human language paper investigate modalities involve language like applications computers programmers engage aim fine tune question ask better account context self awareness embodiment
introduce multi30k dataset stimulate multilingual multimodal research recent advance image description demonstrate english language datasets almost exclusively image description limit english dataset extend flickr30k dataset german translations create professional translators subset english descriptions ii descriptions crowdsourced independently original english descriptions outline data use multilingual image description multimodal machine translation anticipate data useful broader range task
paper describe win entry imageclef two thousand and fifteen image sentence generation task improve google cnn lstm model introduce concept base sentence reranking data drive approach exploit large amount concept level annotations flickr different previous usage concept detection tailor specific image caption model propose approach reranks predict sentence term match detect concepts essentially treat underlie model black box property make approach applicable number exist solutions also experiment fine tune deep language model improve performance score meteor one thousand, eight hundred and seventy-five imageclef two thousand and fifteen test set system outperform runner meteor one thousand, six hundred and eighty-seven clear margin
present new tool train neural network language model nnlms score sentence generate text tool write use python library theano allow researcher easily extend tune aspect train process regardless flexibility theano able generate extremely fast native code utilize gpu multiple cpu core order parallelize heavy numerical computations tool evaluate difficult finnish english conversational speech recognition task significant improvement obtain best back n gram model result obtain finnish task compare exist rnnlm rwthlm toolkits find good better train time order magnitude shorter
interpretable semantic textual similarity ists task add crucial explanatory layer pairwise sentence similarity address various components task chunk level semantic alignment along assignment similarity type score align chunk novel system present paper propose algorithm imatch alignment multiple non contiguous chunk base integer linear program ilp similarity type score assignment pair chunk do use supervise multiclass classification technique base random forrest classifier result show algorithm imatch low execution time outperform participate systems term alignment score three datasets top rank answer students dataset term overall score top alignment score headline dataset gold chunk track
introduce lstm base method dynamically integrate several word prediction experts obtain conditional language model good simultaneously several subtasks illustrate general approach application dialogue integrate neural chat model good conversational aspects neural question answer model good retrieve precise information knowledge base show integration combine strengths independent components hope focus contribution attract attention benefit use mixtures experts nlp
short review similarities dolphins humans help quantitative linguistics information theory
pattern base methods relation extraction rely heavily call hearst pattern ways express instance enumerations class natural language lexico syntactic pattern prove quite useful may capture taxonomical relations express text therefore paper describe novel method relation extraction pattern use morpho syntactical annotations along grammatical case noun phrase constitute entities participate relation also describe method increase number extract relations call pseudo subclass boost potential application pattern base relation extraction method experiment conduct corpus five billion web document polish language
description annotation guidelines yahoo webscope release query treebank version ten may two thousand and sixteen
automatic text summarization tool help users biomedical domain acquire intend information various textual resources efficiently biomedical text summarization systems put basis sentence selection approach frequency concepts extract input text however seem explore measure rather frequency identify valuable content input document consider correlations exist concepts may useful type summarization paper describe bayesian summarizer biomedical text document bayesian summarizer initially map input text unify medical language system umls concepts select important ones use classification feature introduce different feature selection approach identify important concepts text select informative content accord distribution concepts show use appropriate feature selection approach bayesian biomedical summarizer improve performance summarization perform extensive evaluations corpus scientific paper biomedical domain result show bayesian summarizer outperform biomedical summarizers rely frequency concepts domain independent baseline methods base recall orient understudy gisting evaluation rouge metrics moreover result suggest use meaningfulness measure consider correlations concepts feature selection step lead significant increase performance summarization
paper present neurorobotics cognitive model explain understand generalisation nouns verbs combinations vocal command consist verb noun sentence provide humanoid robot generalisation process do via ground process different object interact associate different motor behaviours follow learn approach inspire developmental language acquisition infants cognitive model base multiple time scale recurrent neural network mtrnnwith data obtain object manipulation task humanoid robot platform robotic agent implement model grind primitive embody structure verbs train verb noun combination sample moreover show functional hierarchical architecture base mtrnn able generalise produce novel combinations noun verb sentence analyse learn network dynamics representations also demonstrate generalisation possible via exploitation functional hierarchical recurrent network
text social media provide set challenge traditional nlp approach fail informal language spell errors abbreviations special character commonplace post lead prohibitively large vocabulary size word level approach propose character composition model tweet2vec find vector space representations whole tweet learn complex non local dependencies character sequence propose model outperform word level baseline predict user annotate hashtags associate post significantly better input contain many vocabulary word unusual character sequence tweet2vec encoder publicly available
audio description ad provide linguistic descriptions movies allow visually impair people follow movie along peer descriptions design mainly visual thus naturally form interest data source computer vision computational linguistics work propose novel dataset contain transcribe ads temporally align full length movies addition also collect align movie script use prior work compare two source descriptions total large scale movie description challenge lsmdc contain parallel corpus one hundred and eighteen thousand, one hundred and fourteen sentence video clip two hundred and two movies first characterize dataset benchmarking different approach generate video descriptions compare ads script find ads indeed visual describe precisely show rather happen accord script create prior movie production furthermore present compare result several team participate challenge organize context workshop describe understand video large scale movie description challenge lsmdc iccv two thousand and fifteen
show correspondence analysis ca equivalent define gini index appropriately scale one hot encode use relation introduce nonlinear kernel extension ca extend ca give know analysis natural language via specialize kernels use appropriate contingency table propose semi supervise ca special case kernel extension ca ca require excessive memory apply numerous categories ca use natural language process address problem introduce delay evaluation randomize singular value decomposition memory efficient ca apply word vector representation task propose tail cut kernel extension skip gram within kernel extension ca tail cut kernel outperform exist word vector representation methods
speech act way conceptualize speech action hold true communication platform include social media platforms twitter paper explore speech act recognition twitter treat multi class classification problem create taxonomy six speech act twitter propose set semantic syntactic feature train test logistic regression classifier use data set manually label tweet method achieve state art performance average f1 score seventy also explore classifiers three different granularities twitter wide type specific topic specific order find right balance generalization overfitting task
explore implications use fuzzy techniques mainly commonly use linguistic description summarization data discipline natural language generation perspective provide extensive discussion general convergence point exploration relationship different task involve standard nlg system pipeline architecture common fuzzy approach use linguistic summarization description data fuzzy quantify statements evaluation criteria aggregation operators individual discussion illustrate relate use case recent work make context cross fertilization research field also reference paper encompass general ideas emerge part phd thesis application fuzzy set data text systems present specific application formal approach rather discuss current high level issue potential usages fuzzy set focus linguistic summarization data natural language generation
open challenge construct dialogue systems develop methods automatically learn dialogue strategies large amount unlabelled data recent work propose next utterance classification nuc surrogate task build dialogue systems text data paper investigate performance humans task validate relevance nuc method evaluation result show three main find one humans able correctly classify responses rate much better chance thus confirm task feasible two human performance level vary across task domains consider three datasets expertise level novice vs experts thus show range performance possible type task three automate dialogue systems build use state art machine learn methods similar performance human novices worse experts thus confirm utility class task drive research automate dialogue systems
consider task predict lexical entailment use distributional vectors perform novel qualitative analysis one exist model previously show measure prototypicality word pair find model strongly learn identify hypernyms use hearst pattern well know predictive lexical relations present novel model exploit behavior method feature extraction iterative procedure similar principal component analysis model combine extract feature strengths propose model literature match outperform prior work multiple data set
untested assumption behind crowdsourced descriptions image flickr30k dataset young et al two thousand and fourteen focus information obtain image alone hodosh et al two thousand and thirteen p eight hundred and fifty-nine paper present evidence assumption provide list bias unwarranted inferences find flickr30k dataset finally consider methods find examples discuss deal stereotype drive descriptions future applications
similes natural language expressions use compare unlikely things comparison take literally often use everyday communication important part cultural heritage date corpus similes challenge constantly coin adapt contemporary time paper present methodology semi automate collection similes world wide web use text mine techniques expand exist corpus traditional similes contain three hundred and thirty-three similes collect four hundred and forty-six additional expressions also explore crowdsourcing use extract curate new similes
face face communication communication online environments convey information beyond actual verbal message traditional face face conversation paralanguage ancillary mean emotion lade aspects speech actual verbal prose give contextual information allow interactors appropriately understand message convey paper conceptualize textual paralanguage tpl define write manifestations nonverbal audible tactile visual elements supplement replace write language express word symbols image punctuation demarcations combination elements develop typology textual paralanguage use data twitter facebook instagram present conceptual framework antecedents consequences brand use textual paralanguage implications theory practice discuss
computer support learn monitor engage group learners complex task teachers especially learners work collaboratively students motivate kind progress make intervene communication didactic design adapt students hypothesis analysis natural language interactions students students teachers provide valuable information could use produce qualitative indicators help teachers decisions develop automatic approach three step one explore discursive function message cscl platform two classify message automatically three evaluate correlations discursive attitudes variables link learn activity result tend show type discourse correlate notion progress learn activities importance emotive participation teacher
ability compute accurate reward function essential optimise dialogue policy via reinforcement learn real world applications use explicit user feedback reward signal often unreliable costly collect problem mitigate user intent know advance data available pre train task success predictor line practice neither apply real world applications propose line learn framework whereby dialogue policy jointly train alongside reward model via active learn gaussian process model gaussian process operate continuous space dialogue representation generate unsupervised fashion use recurrent neural network encoder decoder experimental result demonstrate propose framework able significantly reduce data annotation cost mitigate noisy user feedback dialogue policy learn
multiple attempt resolve various inflection match problems information retrieval stem common approach end among many techniques stem statistical stem show effective number languages particularly highly inflect languages paper propose method find affix different position word common statistical techniques heavily rely string similarity term prefix suffix match since infix common irregular informal inflections morphologically complex texts require find infix stem paper propose method whose aim find statistical inflectional rule base minimum edit distance table word pair likelihoods rule language rule use statistically stem word use different text mine task experimental result clef two thousand and eight clef two thousand and nine english persian clir task indicate propose method significantly outperform baselines term map
continuous space word embeddings receive great deal attention natural language process machine learn communities ability model term similarity relationships study use term relatedness context query expansion ad hoc information retrieval demonstrate word embeddings word2vec glove train globally underperform corpus query specific embeddings retrieval task result suggest task benefit global embeddings may also benefit local embeddings
probabilistic topic model generative model describe content document discover latent topics underlie however structure textual input instance group word coherent text span sentence contain much information generally lose model paper propose sentencelda extension lda whose goal overcome limitation incorporate structure text generative inference process illustrate advantage sentencelda compare lda use intrinsic perplexity extrinsic text classification evaluation task different text collections
retrospective assessments internet news report show capture early report unknown infectious disease transmission prior official laboratory confirmation general media interest report peak wan course outbreak study quantify extent media interest infectious disease outbreaks indicative trend report incidence introduce approach use supervise temporal topic model transform large corpora news article temporal topic trend key advantage approach include applicability wide range diseases ability capture disease dynamics include seasonality abrupt peak troughs evaluate method use data multiple infectious disease outbreaks report unite state america yous china india note temporal topic trend extract disease relate news report successfully capture dynamics multiple outbreaks whoop cough yous two thousand and twelve dengue outbreaks india two thousand and thirteen china two thousand and fourteen observations also suggest efficient model temporal topic trend use time series regression techniques estimate disease case count increase precision official report health organizations
popular approach topic model involve extract co occur n grams corpus semantic theme set n grams theme represent underlie topic topic model approach able label set word single n gram label useful topic identification summarization systems paper introduce novel approach label group n grams comprise individual topic approach take complement exist topic distributions word know distribution base predefined set topics do integrate exist label knowledge source represent know potential topics probabilistic topic model knowledge source translate distribution use set hyperparameters dirichlet generate distribution word inference modify distributions guide convergence latent topics conform complementary distributions approach ensure topic inference process consistent exist knowledge label assignment complementary knowledge source transfer latent topics corpus result show accurate label assignment topics well improve topic generation obtain use various label approach base latent dirichlet allocation lda
stochastic structure prediction bandit feedback follow learn protocol sequence iterations learner receive input predict output structure receive partial feedback form task loss evaluation predict structure present applications learn scenario convex non convex objectives structure prediction analyze stochastic first order methods present experimental evaluation problems natural language process exponential output space compare convergence speed across different objectives practical criterion optimal task performance development data optimization theoretic criterion minimal square gradient norm best result criteria obtain non convex objective pairwise preference learn bandit feedback
rapid growth knowledge base kbs web take full advantage become increasingly important knowledge base base question answer kb qa one promise approach access substantial knowledge meantime neural network base nn base methods develop nn base kb qa already achieve impressive result however previous work put emphasis question representation question convert fix vector regardless candidate answer simple representation strategy unable express proper information question hence present neural attention base model represent question dynamically accord different focus various candidate answer aspects addition leverage global knowledge inside underlie kb aim integrate rich kb information representation answer also alleviate vocabulary oov problem help attention model represent question precisely experimental result webquestions demonstrate effectiveness propose approach
authorship analysis aa study unveil hide properties author body exponentially explode textual data extract author identity sociolinguistic characteristics base reflect write style text essential process various areas cybercrime investigation psycholinguistics political socialization etc however previous techniques critically depend manual feature engineer process consequently choice feature set show scenario dataset dependent paper mimic human sentence composition process use neural network approach propose incorporate different categories linguistic feature distribute representation word order learn simultaneously write style representations base unlabeled texts authorship analysis particular propose model allow topical lexical syntactical character level feature vectors document extract stylometrics evaluate performance approach problems authorship characterization authorship verification twitter novel essay datasets experiment suggest propose text representation outperform bag lexical n grams latent dirichlet allocation latent semantic analysis pvdm pvdbow word2vec representations
paper present model end end learn task orient dialog systems main component model recurrent neural network lstm map raw dialog history directly distribution system action lstm automatically infer representation dialog history relieve system developer much manual feature engineer dialog state addition developer provide software express business rule provide access programmatic apis enable lstm take action real world behalf user lstm optimize use supervise learn sl domain expert provide example dialogs lstm imitate use reinforcement learn rl system improve interact directly end users experiment show sl rl complementary sl alone derive reasonable initial policy small number train dialogs start rl optimization policy train sl substantially accelerate learn rate rl
propose zoneout novel method regularize rnns timestep zoneout stochastically force hide units maintain previous value like dropout zoneout use random noise train pseudo ensemble improve generalization preserve instead drop hide units gradient information state information readily propagate time feedforward stochastic depth network perform empirical investigation various rnn regularizers find zoneout give significant performance improvements across task achieve competitive result relatively simple model character word level language model penn treebank text8 datasets combine recurrent batch normalization yield state art result permute sequential mnist
ability reason natural language fundamental prerequisite many nlp task information extraction machine translation question answer quantify ability systems commonly test whether recognize textual entailment ie whether one sentence infer another one however nlp applications single source sentence instead sentence pair available hence propose new task measure well model generate entail sentence source sentence take entailment pair stanford natural language inference corpus train lstm attention manually annotate test set find eighty-two generate sentence correct improvement one hundred and three lstm baseline qualitative analysis show model capable shorten input sentence also infer new statements via paraphrase phrase entailment apply model recursively input output pair thereby generate natural language inference chain use automatically construct entailment graph source sentence finally swap source target sentence also train model give input sentence invent additional information generate new sentence
open problem categorical compositional distributional semantics representation word consider semantically vacuous distributional perspective determiners prepositions relative pronouns coordinators paper deal topic coordination identical syntactic type account majority coordination case language exploit compact close structure underlie category frobenius operators canonically induce fix basis finite dimensional vector space provide morphism representation coordinator tensor show lift atomic type compound type linguistic intuitions provide importance frobenius operators addition compact close set regard language discuss
dominant approach many nlp task recurrent neural network particular lstms convolutional neural network however architectures rather shallow comparison deep convolutional network push state art computer vision present new architecture vdcnn text process operate directly character level use small convolutions pool operations able show performance model increase depth use twenty-nine convolutional layer report improvements state art several public text classification task best knowledge first time deep convolutional net apply text process
model textual visual information vector representations train large language visual datasets successfully explore recent years however task visual question answer require combine vector representations approach multimodal pool include element wise product sum well concatenation visual textual representations hypothesize methods expressive outer product visual textual vectors outer product typically infeasible due high dimensionality instead propose utilize multimodal compact bilinear pool mcb efficiently expressively combine multimodal feature extensively evaluate mcb visual question answer ground task consistently show benefit mcb ablations without mcb visual question answer present architecture use mcb twice predict attention spatial feature combine attend representation question representation model outperform state art visual7w dataset vqa challenge
impact culture visual emotion perception recently capture attention multimedia research study pro vide powerful computational linguistics tool explore retrieve browse dataset 16k multilingual affective visual concepts 73m flickr image first design effective crowdsourc ing experiment collect human judgements sentiment connect visual concepts use word embeddings repre send concepts low dimensional vector space allow us expand mean around concepts thus enable insight commonalities differences among different languages compare variety concept representations novel evaluation task base notion visual semantic relatedness base representations design cluster scheme group multilingual visual concepts evaluate novel metrics base crowdsourced sentiment annotations well visual semantic relatedness propose cluster framework enable us analyze full multilingual dataset depth also show application facial data subset explore cultural sight portrait relate affective visual concepts
current research lifelog data pay enough attention analysis cognitive activities comparison physical activities argue look future wearable devices go cheaper prevalent textual data play significant role data capture lifelogging devices increasingly include speech text potentially useful analysis intellectual activities analyze person hear read see able measure extent cognitive activity devote certain topic subject learner test base lifelog record benefit semantic analysis tool develop natural language process show semantic analysis text data achieve use taxonomic subject facets facets might useful quantify cognitive activity devote various topics person day currently develop method automatically create taxonomic topic vocabularies apply detection intellectual activity
paper study different type recurrent neural network rnn sequence label task propose two new variants rnns integrate improvements sequence label compare traditional elman jordan rnns compare model either traditional new four distinct task sequence label two speak language understand atis media two pos tag french treebank ftb penn treebank ptb corpora result show new variants rnns always effective others
paper present end end framework task orient dialog systems use variant deep recurrent q network drqn model able interface relational database jointly learn policies language understand dialog strategy moreover propose hybrid algorithm combine strength reinforcement learn supervise learn achieve faster learn speed evaluate propose model twenty question game conversational game simulator result show propose method outperform modular base baseline learn distribute representation latent dialog state
sequence sequence seq2seq model rapidly become important general purpose nlp tool prove effective many text generation sequence label task seq2seq build deep neural language model inherit remarkable accuracy estimate local next word distributions work introduce model beam search train scheme base work daume iii marcu two thousand and five extend seq2seq learn global sequence score structure approach avoid classical bias associate local train unify train loss test time usage preserve prove model architecture seq2seq efficient train approach show system outperform highly optimize attention base seq2seq system baselines three different sequence sequence task word order parse machine translation
recurrent neural network gru lstm find wide adoption natural language process achieve state art result many task model characterize memory state write read apply gate composition operations current input previous state however cover small subset potentially useful compositions propose multi function recurrent units mufurus allow arbitrary differentiable function composition operations furthermore mufurus allow input state dependent choice composition operations learn experiment demonstrate additional functionality help different sequence model task include evaluation propositional logic formulae language model sentiment analysis
investigate task assess sentence level prompt relevance learner essay various systems use word overlap neural embeddings neural compositional model evaluate two datasets learner write propose new method sentence level similarity calculation learn adjust weight pre train word embeddings specific task achieve substantially higher accuracy compare relevant baselines
convolutional neural network cnns convolutional pool operations along frequency axis propose attain invariance frequency shift feature however inappropriate regard fact acoustic feature vary frequency paper contend convolution along time axis effective also propose addition intermap pool imp layer deep cnns layer filter group extract common spectrally variant feature layer pool feature map group result propose imp cnn achieve insensitivity spectral variations characteristic different speakers utterances effectiveness imp cnn architecture demonstrate several lvcsr task even without speaker adaptation techniques architecture achieve wer one hundred and twenty-seven swb part hub5 two thousand evaluation test set competitive state art methods
huge amount digital videos produce broadcast every day lead giant media archive effective techniques need make data accessible automatic meta data label broadcast media essential task multimedia index standard use multi modal input purpose paper describe novel method automatic detection media genre show identities use acoustic feature textual feature combination thereof furthermore inclusion available meta data time broadcast show lead high performance latent dirichlet allocation use model acoustics text yield fix dimensional representations media record use support vector machine base classification experiment conduct one thousand, two hundred hours tv broadcast british broadcast corporation bbc task categorise broadcast eight genres one hundred and thirty-three show identities two hundred hour test set accuracies nine hundred and eighty-six eight hundred and fifty-seven achieve genre show identification respectively use combination acoustic textual feature meta data
complex nature big data resources demand new methods structure especially textual content wordnet good knowledge source comprehensive abstraction natural language good implementations exist many languages since wordnet embed natural language form complex network transformation mechanism wordnet2vec propose paper create vectors word wordnet vectors encapsulate general position role give word towards word natural language list set vectors contain knowledge context component within whole language word representation easily apply many analytic task like classification cluster usefulness wordnet2vec method demonstrate sentiment analysis ie classification transfer learn real amazon opinion textual dataset
recently variety lstm base conditional language model lm apply across range language generation task work study various model architectures different ways represent aggregate source information end end neural dialogue system framework method call snapshot learn also propose facilitate learn supervise sequential signal apply companion cross entropy objective function condition vector experimental analytical result demonstrate firstly competition occur condition vector lm differ architectures provide different trade off two secondly discriminative power transparency condition vector key provide model interpretability better performance thirdly snapshot learn lead consistent performance improvements independent architecture use
objective patient note electronic health record ehrs may contain critical information medical investigations however vast majority medical investigators access de identify note order protect confidentiality patients unite state health insurance portability accountability act hipaa define eighteen type protect health information phi need remove de identify patient note manual de identification impractical give size ehr databases limit number researchers access non de identify note frequent mistake human annotators reliable automate de identification system would consequently high value materials methods introduce first de identification system base artificial neural network anns require handcraft feature rule unlike exist systems compare performance system state art systems two datasets i2b2 two thousand and fourteen de identification challenge dataset largest publicly available de identification dataset mimic de identification dataset assemble twice large i2b2 two thousand and fourteen dataset result ann model outperform state art systems yield f1 score nine thousand, seven hundred and eighty-five i2b2 two thousand and fourteen dataset recall nine thousand, seven hundred and thirty-eight precision nine thousand, seven hundred and thirty-two f1 score nine thousand, nine hundred and twenty-three mimic de identification dataset recall nine thousand, nine hundred and twenty-five precision nine thousand, nine hundred and six conclusion find support use anns de identification patient note show better performance previously publish systems require feature engineer
introduce online popularity prediction track task benchmark task reinforcement learn combinatorial natural language action space specify number discussion thread predict popular recommend choose fix window recent comment track novel deep reinforcement learn architectures study effective model value function associate action comprise interdependent sub action propose model represent dependence sub action bi directional lstm give best performance across different experimental configurations domains also generalize well vary number recommendation request
one core components modern speak dialogue systems belief tracker estimate user goal every step dialogue however current approach difficulty scale larger complex dialogue domains due dependency either speak language understand model require large amount annotate train data b hand craft lexicons capture linguistic variation users language propose novel neural belief track nbt framework overcome problems build recent advance representation learn nbt model reason pre train word vectors learn compose distribute representations user utterances dialogue context evaluation two datasets show approach surpass past limitations match performance state art model rely hand craft semantic lexicons outperform lexicons provide
present herein novel model similar question rank within collaborative question answer platforms present approach integrate regression stage relate topics derive question derive question answer pair help avoid problems cause differences vocabulary use within question answer tendency question shorter answer performance model show outperform translation methods topic model without regression several real world datasets
many important nlp problems pose dual sequence sequence sequence model task recent advance build end end neural architectures highly successful solve task work propose new architecture dual sequence model base associative memory derive rnns recurrent associative memory augment generic recurrent neural network rnn architecture extend dual rnn operate two ams model achieve competitive result textual entailment qualitative analysis demonstrate long range dependencies source target sequence bridge effectively use dual rnns however initial experiment auto encode reveal benefit exploit system learn solve sequence sequence task indicate additional supervision regularization need
end end dialog system aim dialog state track accurately estimate compact representation current dialog status sequence noisy observations produce speech recognition natural language understand modules paper introduce novel method dialog state track base general paradigm machine read propose solve use end end memory network memn2n memory enhance neural network architecture evaluate propose approach second dialog state track challenge dstc two dataset corpus convert occasion order frame hide state variable inference question answer task base sequence utterances extract dialog show propose tracker give encourage result propose extend dstc two dataset specific reason capabilities requirement like count list maintenance yes question answer indefinite knowledge management finally present encourage result use propose memn2n base track model
paper propose graph community detection approach identify cross document relationships topic segment level give set relate document automatically find relationships cluster segment similar content topics context study different weight mechanisms influence discovery word communities relate different topics find document finally test different map function assign topic segment word communities determine topic segment consider equivalent perform task possible enable efficient multi document browse since user find relevant content one document provide access similar topics document deploy approach two different scenarios one educational scenario equivalence relationships learn materials need find consist series dialogs social context students discuss commonplace topics result show propose approach better discover equivalence relationships learn material document obtain close result social speech domain best perform approach cluster technique
automate text score ats provide cost effective consistent alternative human mark however order achieve good performance predictive feature system need manually engineer human experts introduce model form word representations learn extent specific word contribute text score use long short term memory network represent mean texts demonstrate fully automate framework able achieve excellent result similar approach attempt make result interpretable inspire recent advance visualize neural network introduce novel method identify regions text model find discriminative
paper describe participation team twise semeval two thousand and sixteen challenge specifically participate task four namely sentiment analysis twitter implement sentiment classification systems subtasks b c approach consist two step first step generate validate diverse feature set twitter sentiment evaluation inspire work participants previous editions challenge second step focus optimization evaluation measure different subtasks end examine different learn strategies validate data provide task organisers final submissions use ensemble learn approach stack generalization subtask single linear model rest subtasks official leaderboard rank nine thirty-five eight nineteen one eleven two fourteen subtasks b c respectivelyfootnotewe make code available research purpose urlhttps githubcom balikasg semeval2016 twittersentimentevaluation
online social network users tend select information adhere system beliefs form polarize group like mind people polarization well effect online social interactions extensively investigate still relation group formation personality traits remain unclear better understand cognitive psychological determinants online social dynamics might help design efficient communication strategies challenge digital misinformation threat work focus users comment post publish us facebook page support scientific conspiracy like narratives classify personality traits users accord online behavior show different conflict communities populate users show similar psychological profile dominant personality model scientific conspiracy echo chamber moreover observe permanence within echo chamber slightly shape users psychological profile result suggest presence specific personality traits individuals lead considerable involvement support narratives inside virtual echo chamber
phonemic phonetic sub word units commonly use atomic elements represent speech signal modern asrs however optimal choice due several reason large amount effort require handcraft pronunciation dictionary pronunciation variations human mistake resourced dialects languages propose data drive pronunciation estimation acoustic model method take orthographic transcription jointly estimate set sub word units reliable dictionary experimental result show propose method base semi supervise train deep neural network largely outperform phoneme base continuous speech recognition timit dataset
deep neural network continue revolutionize various application domains increase interest make powerful model understandable interpretable narrow cause good bad predictions focus recurrent neural network rnns state art model speech recognition translation approach increase interpretability combine rnn hide markov model hmm simpler transparent model explore various combinations rnns hmms hmm train lstm state hybrid model hmm train first small lstm give hmm state distributions train fill gap hmm performance jointly train hybrid model find lstm hmm learn complementary information feature text
stance detection task classify attitude express text towards target hillary clinton positive negative neutral previous work assume either target mention text train data every target give paper consider challenge version task target always mention train data available test target experiment conditional lstm encode build representation tweet dependent target demonstrate outperform encode tweet target independently performance improve conditional model augment bidirectional encode evaluate approach semeval two thousand and sixteen task six twitter stance detection corpus achieve performance second best system train semi automatically label tweet test target weak supervision add approach achieve state art result
paper present novel approach spam filter demonstrate applicability respect sms message approach require minimum feature engineer small set la bell data sample feature extract use topic model base latent dirichlet allocation comprehensive data model create use stack denoising autoencoder sda topic model summarise data provide ease use high interpretability visualise topics use word cloud give sms message regard either spam unwanted ham want sda able model message accurately discriminate two class without need pre label train set result compare state art spam detection algorithms propose approach achieve ninety-seven accuracy compare favourably best report algorithms present literature
deep neural network dnns successfully apply wide variety acoustic model task recent years include applications dnns either discriminative feature extraction hybrid acoustic model scenario despite rapid progress area number challenge remain train dnns paper present effective way train dnns use manifold learn base regularization framework framework parameters network optimize preserve underlie manifold base relationships speech feature vectors minimize measure loss network output target achieve incorporate manifold base locality constraints objective criterion dnns empirical evidence provide demonstrate train network manifold constraints preserve structural compactness hide layer network manifold regularization apply train bottleneck dnns feature extraction hide markov model hmm base speech recognition experiment work conduct aurora two speak digits aurora four read news large vocabulary continuous speech recognition task performance measure term word error rate wer task show manifold regularize dnns result thirty-seven reduction wer relative standard dnns
introduce lambada dataset evaluate capabilities computational model text understand mean word prediction task lambada collection narrative passages share characteristic human subject able guess last word expose whole passage see last sentence precede target word succeed lambada computational model simply rely local context must able keep track information broader discourse show lambada exemplify wide range linguistic phenomena none several state art language model reach accuracy one novel benchmark thus propose lambada challenge test set mean encourage development new model capable genuine understand broad context natural language text
product classification task automatically predict taxonomy path product predefined taxonomy hierarchy give textual product description title efficient product classification require suitable representation document textual description product feature vector efficient fast algorithms prediction address challenge propose new distributional semantics representation document vector formation also develop new two level ensemble approach utilize respect taxonomy tree path wise node wise depth wise classifiers error reduction final product classification experiment show effectiveness distributional representation ensemble approach data set lead e commerce platform achieve better result various evaluation metrics compare earlier approach
machine learn algorithms optimize model statistical properties train data input data reflect stereotype bias broader society output learn algorithm also capture stereotype paper initiate study gender stereotype word embed popular framework represent text data use become increasingly common applications inadvertently amplify unwanted stereotype show across multiple datasets embeddings contain significant gender stereotype especially regard professions create novel gender analogy task combine crowdsourcing systematically quantify gender bias give embed develop efficient algorithm reduce gender stereotype use handful train examples preserve useful geometric properties embed evaluate algorithm several metrics focus male female stereotype framework may applicable type embed bias
describe method proactive information retrieval target retrieve relevant information write task method current task need user estimate potential next step unobtrusively predict base user past action focus task write user coalesce previously collect information text proactive system automatically recommend user relevant background information propose system incorporate text input prediction use long short term memory lstm network present simulations show system able reach higher precision value exploratory search set compare baseline comparison system
tag line content informative keywords widespread phenomenon scientific article repositories blog line news portals case tag give item free word choose author independently therefore relations among keywords collection news items unknown however case topics concepts describe keywords form latent hierarchy general topics categories top specialise ones bottom apply recent cooccurrence base tag hierarchy extraction method set keywords obtain four different line news portals result hierarchies show substantial differences topics render important top hierarchy less interest categorise low hierarchy also underlie network structure reveal discrepancies plausible keyword association frameworks study news portals
explore two techniques use color make sense statistical text model one method use text annotations illustrate model view particular tokens particular document another use high level word pixels graphic display entire corpus together methods offer zoom zoom perspectives model understand text show interconnect methods help diagnose classifier poor performance twitter slang make sense topic model historical political texts
present framework couple syntax semantics natural language sentence generative model order develop semantic parser jointly infer syntactic morphological semantic representations give sentence guidance background knowledge generate sentence framework semantic statement first sample prior set beliefs knowledge base give semantic statement grammar probabilistically generate output sentence joint semantic syntactic parser derive return k best semantic syntactic parse give sentence semantic prior flexible use incorporate background knowledge parse ways unlike previous semantic parse approach example semantic statements correspond beliefs knowledge base give higher prior probability type correct statements give somewhat lower probability beliefs outside knowledge base give lower probability construction grammar invoke novel application hierarchical dirichlet process hdps turn require novel efficient inference approach present experimental result show simple grammar parser outperform state art ccg semantic parser scale knowledge base millions beliefs
train system new input either say know make prediction guarantee correct answer question affirmative provide model family well specify specifically introduce unanimity principle predict model consistent train data predict output operationalize principle semantic parse task map utterances logical form develop simple efficient method reason infinite set consistent model check two model prove method obtain one hundred precision even modest amount train data possibly adversarial distribution empirically demonstrate effectiveness approach standard geoquery dataset
systematic review identify collate various clinical study compare data elements result order provide evidence base answer particular clinical question process manual involve lot time tool automate process lack aim work develop framework use natural language process machine learn build information extraction algorithms identify data elements new primary publication without go expensive task manual annotation build gold standards data element type system develop two stag initially use information contain exist systematic review identify sentence pdf file include reference contain specific data elements interest use modify jaccard similarity measure sentence treat label dataa support vector machine svm classifier train label data extract data elements interest new article conduct experiment cochrane database systematic review relate congestive heart failure use inclusion criteria example data element empirical result show propose system automatically identify sentence contain data element interest high recall nine thousand, three hundred and seventy-five reasonable precision two thousand, seven hundred and five mean reviewers read thirty-seven sentence average empirical result suggest tool retrieve valuable information reference article even time consume identify manually thus hope tool useful automatic data extraction biomedical research publications future scope work generalize information framework type systematic review
visual question answer vqa task answer natural language question image introduce novel problem determine relevance question image vqa current vqa model reason whether question even relate give image eg capital argentina require information external resources answer correctly break continuity dialogue human machine interaction approach determine relevance compose two stag give image question one first determine whether question visual two visual determine whether question relevant give image approach base lstm rnns vqa model uncertainty caption question similarity able outperform strong baselines relevance task also present human study show vqa model augment question relevance reason perceive intelligent reasonable human like
since shoot black teenager michael brown white police officer darren wilson ferguson missouri protest hashtag blacklivesmatter amplify critique extrajudicial kill black americans response blacklivesmatter twitter users adopt alllivesmatter counter protest hashtag whose content argue equal attention give live regardless race multi level analysis eight hundred and sixty thousand tweet study protest counter protest diverge quantify aspects discourse find alllivesmatter facilitate opposition blacklivesmatter hashtags policelivesmatter bluelivesmatter way historically echo tension black protesters law enforcement addition show significant portion alllivesmatter use stem hijack blacklivesmatter advocate beyond simply inject alllivesmatter blacklivesmatter content hijackers use hashtag directly confront counter protest notion live matter find suggest black live matter movement able grow exhibit diverse conversations avoid derailment social media make discussion counter protest opinions central topic alllivesmatter rather movement
performance automatic speech recognition systems noisy environments still leave room improvement speech enhancement feature enhancement techniques increase noise robustness systems usually add components recognition system need careful optimization work propose use relatively simple curriculum train strategy call accordion anneal accan use multi stage train schedule sample signal noise ratio snr value low 0db first add sample increase higher snr value gradually add snr value 50db also use method call per epoch noise mix pem generate noisy train sample online train thus enable dynamically change snr train data accan pem methods evaluate end end speech recognition pipeline wall street journal corpus accan decrease average word error rate wer 20db 10db snr range three hundred and fourteen compare conventional multi condition train method
present comprehensive study deep bidirectional long short term memory lstm recurrent neural network rnn base acoustic model automatic speech recognition asr study effect size depth train model eight layer investigate train aspect study different variants optimization methods batch truncate backpropagation different regularization techniques dropout l2 regularization different gradient clip variants major part experimental analysis perform quaero corpus additional experiment also perform switchboard corpus best lstm model relative improvement word error rate fourteen compare best fee forward neural network ffnn baseline quaero task task get best result eight layer bidirectional lstm show pretraining scheme layer wise construction help deep lstms finally compare train calculation time many present experiment relation recognition performance experiment do returnn rwth extensible train framework universal recurrent neural network combination rasr rwth asr toolkit
many predictive task diagnose patient base medical chart ultimately define decisions human experts unfortunately encode experts knowledge often time consume expensive propose simple way use fuzzy informal knowledge experts guide discovery interpretable latent topics text underlie intuition approach latent factor informative correlations data set relevance variables specify expert mathematically approach combination information bottleneck total correlation explanation corex give preliminary evaluation anchor corex show produce coherent interpretable topics two distinct corpora
conversational agents bots begin widely use conversational interfaces design system capable emulate human like interactions conversational layer serve fabric chat like interaction agent need paper introduce model employ information retrieval utilize convolutional deep structure semantic neural network base feature ranker present human like responses ongoing conversation user conversations account context critical retrieval model show context sensitive approach use convolutional deep structure semantic model cdssm character trigrams significantly outperform several conventional baselines term relevance responses retrieve
present simple approach automatically extract number subject involve randomise control trials rct approach first apply set rule base techniques extract candidate study size abstract article supervise classification perform candidates support vector machine use small set lexical structural contextual feature small annotate train set two hundred and one rcts obtain accuracy eighty-eight believe system aid complex medical text process task summarisation question answer
one lead platforms creative content tumblr offer advertisers unique way create brand identity advertisers tell story image animation text music video promote content sponsor appear advertisement stream tumblr users paper present framework enable one key target advertise components tumblr specifically gender interest target describe main challenge involve development framework include create grind truth train gender prediction model well map tumblr content interest taxonomy purpose infer user interest propose novel semi supervise neural language model categorization tumblr content ie post tag post keywords model train large scale data set consist sixty-eight billion user post limit amount categorize keywords show superior performance bag word model successfully deploy gender interest target capability yahoo production systems deliver inference users cover ninety daily activities tumblr online performance result indicate advantage propose approach observe twenty lift user engagement sponsor post compare untargeted campaign
paper tackle problem image search query short textual description image user look choose implement actual search process similarity search visual feature space learn translate textual query visual representation search visual feature space advantage update translation model require reprocess typically huge image collection search perform propose text2vis neural network generate visual representation visual feature space fc6 fc7 layer imagenet short descriptive text text2vis optimize two loss function use stochastic loss selection method visual focus loss aim learn actual text visual feature map text focus loss aim model higher level semantic concepts express language counter overfit non relevant visual components visual loss report preliminary result ms coco dataset
recently number deep learn base model propose task visual question answer vqa performance model cluster around sixty seventy paper propose systematic methods analyze behavior model first step towards recognize strengths weaknesses identify fruitful directions progress analyze two model one two major class vqa model attention without attention show similarities differences behavior model also analyze win entry vqa challenge two thousand and sixteen behavior analysis reveal despite recent progress today vqa model myopic tend fail sufficiently novel instance often jump conclusions converge predict answer listen half question stubborn change answer across image
recurrent neural network particular long short term memory lstm network remarkably effective tool sequence model learn dense black box hide representation sequential input researchers interest better understand model study change hide state representations time notice interpretable pattern also significant noise work present lstmvis visual analysis tool recurrent neural network focus understand hide state dynamics tool allow users select hypothesis input range focus local state change match state change similar pattern large data set align result structural annotations domain show several use case tool analyze specific hide state properties dataset contain nest phrase structure chord progressions demonstrate tool use isolate pattern statistical analysis characterize domain different stakeholders goals task
temporal common sense applications ai task qa multi document summarization human ai communication propose task sequence give jumble set align image caption pair belong story task sort output sequence form coherent story present multiple approach via unary position pairwise order predictions ensemble base combinations achieve strong result task use text base image base feature depict complementary improvements use qualitative examples demonstrate model learn interest aspects temporal common sense
although several rdf knowledge base available lod initiative ontology schema link datasets rich particular lack object properties problem find new object properties instance two give class investigate detail context link data paper present dart detect arbitrary relations enrich box link data unsupervised solution enrich lod cloud new object properties two give class dart exploit contextual similarity identify text pattern web corpus potentially represent relations individuals text pattern cluster mean paraphrase detection capture object properties two give lod class dart also perform fully automate map discover relations properties link dataset serve many purpose identification completely new relations elimination irrelevant relations generation prospective property axioms empirically evaluate approach several pair class find system indeed use enrich link datasets new object properties instance compare dart newontext system offshoot nell never end language learn effort experiment reveal dart give better result newontext respect correctness well number relations
paper present new model word sense disambiguation formulate term evolutionary game theory word disambiguate represent node graph whose edge represent word relations sense represent class word simultaneously update class membership preferences accord sense neighbor word likely choose use distributional information weigh influence word decisions others semantic similarity information measure strength compatibility among choices information formulate word sense disambiguation problem constraint satisfaction problem solve use tool derive game theory maintain textual coherence model base two ideas similar word assign similar class mean word depend word text paper provide depth motivation idea model word sense disambiguation problem term game theory illustrate example conclusion present extensive analysis combination similarity measure use framework comparison state art systems result show model outperform state art algorithms apply different task different scenarios
neural machine translation nmt offer novel alternative formulation translation potentially simpler statistical approach however reach competitive performance nmt model need exceedingly large paper consider apply knowledge distillation approach bucila et al two thousand and six hinton et al two thousand and fifteen prove successful reduce size neural model domains problem nmt demonstrate standard knowledge distillation apply word level prediction effective nmt also introduce two novel sequence level versions knowledge distillation improve performance somewhat surprisingly seem eliminate need beam search even apply original teacher model best student model run ten time faster state art teacher little loss performance also significantly better baseline model train without knowledge distillation forty-two seventeen bleu greedy decode beam search apply weight prune top knowledge distillation result student model thirteen time fewer parameters original teacher model decrease four bleu
sequence label extraction medical events attribute unstructured text electronic health record ehr note key step towards semantic understand ehrs important applications health informatics include pharmacovigilance drug surveillance state art supervise machine learn model domain base conditional random field crfs feature calculate fix context windows application explore various recurrent neural network frameworks show significantly outperform crf model
methods base representation learn currently hold state art many natural language process knowledge base inference task yet major challenge efficiently incorporate commonsense knowledge model recent approach regularize relation entity representations propositionalization first order logic rule however propositionalization scale beyond domains entities rule paper present highly efficient method incorporate implication rule distribute representations automate knowledge base construction map entity tuple embeddings approximately boolean space encourage partial order relation embeddings base implication rule mine wordnet surprisingly find strong restriction entity tuple embed space hurt expressiveness model even act regularizer improve generalization incorporate commonsense rule achieve increase two percentage point mean average precision matrix factorization baseline observe negligible increase runtime
one basic function language refer object share scene model reference continuous representations challenge require individuation ie track distinguish arbitrary number referents introduce neural network model give definite description set object represent natural image point intend object expression unique referent indicate failure model directly train reference act competitive pipeline manually engineer perform task referents purely visual characterize combination visual linguistic properties
present systematic analysis performance phonetic recogniser window input feature symmetric respect current frame recogniser base context dependent deep neural network cd dnns hide markov model hmms objective reduce latency system reduce number future feature frame require estimate current output test perform timit database show performance degrade input window shift five frame past compare common practice future frame correspond improve latency fifty ms settings test also show best result obtain symmetric window commonly employ asymmetric window eight past two future context frame although observation confirm data set reduction latency suggest result critical specific applications real time lip synchronisation tele presence may also beneficial general applications improve lag human machine speak interaction
add emotions use prosody manipulation method indonesian text speech system text speech tts system convert text one language speech accordance read text language use focus research natural sound concept make humanize pronunciation voice synthesis system text speech humans emotions intonation may affect sound produce main requirement system use text speech research espeak database mbrola use id1 human speech corpus database website summarize word highest frequency common word use country three type emotional intonation design base happy angry sad emotion method develop emotional filter manipulate relevant feature prosody especially pitch duration value use predetermine rate factor establish analyze differences standard output text speech voice record emotional prosody particular intonation test result perception test human speech corpus happy emotion ninety-five nine thousand, six hundred and twenty-five angry emotion nine thousand, eight hundred and seventy-five sad emotions perception test system carry intelligibility naturalness test intelligibility test accuracy sound original sentence nine hundred and thirty-three clarity rate sentence six hundred and twenty-eight naturalness accuracy emotional election amount seven hundred and fifty-six happy emotion seven hundred and thirty-three angry emotion sixty sad emotions text speech tts merupakan suatu sistem yang dapat mengonversi teks dalam format suatu bahasa menjadi ucapan sesuai dengan pembacaan teks dalam bahasa yang digunakan
study problem automatically build hypernym taxonomies textual visual data previous work taxonomy induction generally ignore increasingly prominent visual data encode important perceptual semantics instead propose probabilistic model taxonomy induction jointly leverage text image avoid hand craft feature engineer design end end feature base distribute representations image word model discriminatively train give small set exist ontologies capable build full taxonomies scratch collection unseen conceptual label items associate image evaluate model feature wordnet hierarchies system outperform previous approach large gap
neural machine translation nmt like many deep learn domains typically suffer parameterization result large storage size paper examine three simple magnitude base prune scheme compress nmt model namely class blind class uniform class distribution differ term prune thresholds compute different class weight nmt architecture demonstrate efficacy weight prune compression technique state art nmt system show nmt model two hundred million parameters prune forty little performance loss measure wmt fourteen english german translation task shed light distribution redundancy nmt architecture main result retrain recover even surpass original performance eighty prune model
one key obstacles make learn protocols realistic applications need supervise costly process often require hire domain experts consider framework use world knowledge indirect supervision world knowledge general purpose knowledge design specific domain key challenge adapt world knowledge domains represent learn paper provide example use world knowledge domain dependent document cluster provide three ways specify world knowledge domains resolve ambiguity entities type represent data world knowledge heterogeneous information network propose cluster algorithm cluster multiple type incorporate sub type information constraints experiment use two exist knowledge base source world knowledge one freebase collaboratively collect knowledge entities organizations yago2 knowledge base automatically extract wikipedia map knowledge linguistic knowledge base wordnet experimental result two text benchmark datasets 20newsgroups rcv1 show incorporate world knowledge indirect supervision significantly outperform state art cluster algorithms well cluster algorithms enhance world knowledge feature
classical scope assignment strategies multi quantifier sentence involve quantifier phrase qp movement recent continuation base approach provide compel alternative interpret qp situ without resort logical form structure beyond overt syntax continuation base strategies divide two group locate source scope ambiguity rule semantic composition attribute lexical entries quantifier word paper focus former operation base approach nature semantic operations involve specifically discuss three possible operation base strategies multi quantifier sentence together relative merit cost
keyphrases efficiently summarize document content use various document process retrieval task several unsupervised techniques classifiers exist extract keyphrases text document methods operate phrase level rely part speech pos filter candidate phrase generation addition directly handle keyphrases vary lengths overcome model shortcomings address keyphrase extraction sequential label task paper explore basic set feature commonly use nlp task well predictions various unsupervised methods train taggers addition natural model keyphrase extraction problem show tag model yield significant performance benefit exist state art extraction methods
state art cnn model give good performance sentence classification task purpose work empirically study desirable properties semantic coherence attention mechanism reusability cnns task semantically coherent kernels preferable lot interpretable explain decision learn cnn model observe learn kernels semantic coherence motivate observation propose learn kernels semantic coherence use cluster scheme combine word2vec representation domain knowledge sentiwordnet suggest technique visualize attention mechanism cnns decision explanation purpose reusable property enable kernels learn one problem use another problem help efficient learn additional domain specific filter may learn demonstrate efficacy core ideas learn semantically coherent kernels leverage reusable kernels efficient learn several benchmark datasets experimental result show usefulness approach achieve performance close state art methods semantic reusable properties
work release extensible easily configurable neural network train software provide rich set functional layer particular focus efficient train recurrent neural network topologies multiple gpus source software package public freely available academic research purpose use framework standalone tool support flexible configuration software allow train state art deep bidirectional long short term memory lstm model one dimensional data like speech two dimensional data like handwritten text use develop successful submission systems several evaluation campaign
volume contain proceed two thousand and sixteen workshop semantic space intersection nlp physics cognitive science slpcs two thousand and sixteen hold 11th june university strathclyde glasgow co locate quantum physics logic qpl two thousand and sixteen exploit common grind provide concept vector space workshop bring together researchers work intersection natural language process nlp cognitive science physics offer appropriate forum present uniquely motivate work ideas interplay three discipline inspire theoretically motivate approach understand word mean interact sentence discourse diagrammatic reason depict simplify interaction language model determine input world word sentence mean interact logically first edition workshop consist three invite talk distinguish speakers hans briegel peter gardenfors dominic widdows eight presentations select contribute paper submission referee least three members programme committee deliver detail insightful comment suggestions
accessibility test digital preservation time experience drift localize label content statistical model evolve semantics represent vector field articulate need detect measure interpret model outcomes knowledge dynamics end employ high performance machine learn algorithm train extremely large emergent self organize map exploratory data analysis work hypothesis present dynamics semantic drift model relax version newtonian mechanics call social mechanics use term distance measure semantic relatedness vs pagerank value indicate social importance apply variable term mass gravitation metaphor express change semantic content vector field lend new perspective experimentation term gravitation time one compute generate potential whose fluctuations manifest modifications pairwise term similarity vs social importance thereby update osgood semantic differential dataset examine public catalog metadata tate galleries london
density operators allow represent ambiguity vector representation quantum theory distributional natural language mean formally equivalently allow discard part description composite system consider discard part context introduce dual density operators allow two independent notions context demonstrate use dual density operators within grammatical compositional distributional framework natural language mean show dual density operators use simultaneously represent ambiguity word mean eg queen person vs queen band ii lexical entailment eg tiger mammal provide proof concept example
propose apply categorical compositional scheme six conceptual space model cognition order introduce category convex relations new set categorical compositional semantics emphasize convex structure important conceptual space applications show conceptual space composite type adjectives verbs construct illustrate new model detail examples
previous work j hedge formalise generalise quantifiers theory natural language categorical compositional distributional semantics help bialgebras paper show quantifier scope ambiguity represent set representation generalise branch quantifiers
develop novel bi directional attention model dependency parse learn agree headword predictions forward backward parse directions parse procedure direction formulate sequentially query memory component store continuous headword embeddings propose parser make use soft headword embeddings allow model implicitly capture high order parse history without dramatically increase computational complexity conduct experiment english chinese twelve languages conll two thousand and six share task show propose model achieve state art unlabeled attachment score six languages
sarcasm peculiar form sentiment expression surface sentiment differ imply sentiment detection sarcasm social media platforms apply past mainly textual utterances lexical indicators interjections intensifiers linguistic markers contextual information user profile past conversations use detect sarcastic tone however modern social media platforms allow create multimodal message audiovisual content integrate text make analysis mode isolation partial work first study relationship textual visual aspects multimodal post three major social media platforms ie instagram tumblr twitter run crowdsourcing task quantify extent image perceive necessary human annotators moreover propose two different computational frameworks detect sarcasm integrate textual visual modalities first approach exploit visual semantics train external dataset concatenate semantics feature state art textual feature second method adapt visual neural network initialize parameters train imagenet multimodal sarcastic post result show positive effect combine modalities detection sarcasm across platforms methods
present mean box pool novel visual representation pool cnn representations large number highly overlap object proposals show representation together ncca successful multimodal embed technique achieve state art performance visual madlibs task moreover inspire ncca objective function extend classical cnnlstm approach train network directly maximize similarity internal representation deep learn architecture candidate answer approach achieve significant improvement prior work also use cnnlstm approach visual madlibs
present self contain system construct natural language model use text compression system improve upon previous neural network base model utilize recent advance syntactic parse google syntaxnet augment character level recurrent neural network rnns prove exceptional model sequence data text architecture allow model long term contextual information
current approach learn vector representations text compatible different languages usually require amount parallel text align word sentence least document level hypothesize however different natural languages share enough semantic structure possible principle learn compatible vector representations analyze monolingual distribution word order evaluate hypothesis propose scheme map word vectors train source language vectors semantically compatible word vectors train target language use adversarial autoencoder present preliminary qualitative result discuss possible future developments technique applications cross lingual sentence representations
accommodate integrate connectionist symbolic architecture ics thirty-two within categorical compositional semantics catco thirteen form model categorical compositional cognition catcog resolve intrinsic problems ics fact representations inhabit unbounded space sentence differ tree structure directly compare way make grammatical structure available contrast strategies like circular convolution use catco model also allow us make use tool develop catco representation ambiguity logical reason via density matrices structural mean word relative pronouns address extension present cognitive process moreover catcog framework sufficiently flexible allow entirely different representations mean conceptual space interestingly since catco model largely inspire categorical quantum mechanics catcog
role social media particular microblogging platforms twitter conduit actionable tactical information disasters increasingly acknowledge however time critical analysis big crisis data social media stream bring challenge machine learn techniques especially ones use supervise learn scarcity label data particularly early hours crisis delay machine learn process current state art classification methods require significant amount label data specific particular event train plus lot feature engineer achieve best result work introduce neural network base classification methods binary multi class tweet classification task show neural network base model require feature engineer perform better state art methods early hours disaster label data available propose method make best use event data achieve good result
health utilities measure patient preferences perfect health compare specific unhealthy state asthma fracture hip colon cancer integrate time estimations call quality adjust life years qalys characterize health utilities hus require detail patient interview write survey reliable specific data remain costly due efforts locate enlist coordinate participants thus scope context temporality diseases examine remain limit billion people use social media propose novel strategy use natural language process analyze public online conversations signal severity medical condition correlate know hus use machine learn work filter dataset originally contain two billion tweet relevant content sixty diseases use data algorithm successfully distinguish mild severe diseases previously categorize traditional techniques represent progress towards two relate applications first predict hus information nonexistent second rich hu data already exist estimate temporal geographic pattern disease severity data mine
multiple side every story statistical topic model highly successful topically summarize stories corpora text document explicitly address issue learn different side viewpoints express document paper show viewpoints learn completely unsupervised represent human interpretable form use novel approach apply corrlda2 purpose learn topic viewpoint relations use form group topics group represent viewpoint corpus document israeli palestinian conflict use demonstrate palestinian israeli viewpoint learn leverage magnitudes sign feature weight linear svm introduce principled method evaluate associations topics viewpoints demonstrate quantitatively qualitatively learn topic group contextually coherent form consistently correct topic viewpoint associations
descriptions often provide along recommendations help users discovery recommend automatically generate music playlists eg personalise playlists introduce problem generate descriptions paper propose method generate music playlist descriptions call music caption propose method audio content analysis natural language process adopt utilise information track
study cohesion within coalitions political group eighth european parliament two thousand and fourteen two thousand and nineteen analyze two entirely different aspects behavior members european parliament meps policy make process one hand analyze co vote pattern retweeting behavior make use two diverse datasets analysis first one roll call vote dataset cohesion regard tendency co vote within group coalition form members several group exhibit high degree co vote agreement subject second dataset come twitter capture retweeting ie endorse behavior meps imply cohesion retweets within group coalitions retweets group completely different perspective employ two different methodologies analyze cohesion coalitions first one base krippendorff alpha reliability use measure agreement raters data analysis scenarios second one base exponential random graph model often use social network analysis give general insights cohesion political group european parliament explore whether coalitions form way different policy areas examine degree retweeting behavior meps correspond co vote pattern novel interest aspect work relationship co vote retweeting pattern
introduce unsupervised discriminative model task retrieve experts online document collections exclusively employ textual evidence avoid explicit feature engineer learn distribute word representations unsupervised way compare model state art unsupervised statistical vector space probabilistic generative approach propose log linear model achieve retrieval performance level state art document centric methods low inference cost call profile centric approach yield statistically significant improve rank vector space generative model case match performance supervise methods various benchmarks use solely text well methods work external evidence relevance feedback contrastive analysis rank produce discriminative generative approach show complementary strengths due ability unsupervised discriminative model perform semantic match
artificial intelligence machine learn period astound growth however concern technologies may use either without intention perpetuate prejudice unfairness unfortunately characterize many human institutions show first time human like semantic bias result application standard machine learn ordinary language sort language humans expose every day replicate spectrum standard human bias expose implicit association test well know psychological study replicate use widely use purely statistical machine learn model namely glove word embed train corpus text web result indicate language contain recoverable accurate imprint historic bias whether morally neutral towards insects flower problematic towards race gender even simply veridical reflect status quo distribution gender respect career first name regularities capture machine learn along rest semantics addition empirical find concern language also contribute new methods evaluate bias text word embed association test weat word embed factual association test wefat result implications ai machine learn also field psychology sociology human ethics since raise possibility mere exposure everyday language account bias replicate
introduce novel latent vector space model jointly learn latent representations word e commerce products map two without need explicit annotations power model lie ability directly model discriminative relation products particular word compare method exist latent vector space model lsi lda word2vec evaluate feature learn rank set latent vector space model achieve enhance performance learn better product representations furthermore map word products representations word benefit directly errors propagate back product representations parameter estimation provide depth analysis performance model analyze structure learn representations
recurrent neural network recently use learn describe image use natural language however observe model generalize poorly scenes observe train possibly depend strongly statistics text train data propose describe image use short structure representations aim capture crux description structure representations allow us tease evaluate separately two type generalization standard generalization new image similar scenes generalization new combinations know entities compare two learn approach ms coco dataset state art recurrent network base lstm show attend tell simple structure prediction model top deep network find structure model generalize new compositions substantially better lstm seven time accuracy predict structure representations provide concrete method quantify generalization unseen combinations argue structure representations compositional split useful benchmark image caption advocate compositional model capture linguistic visual structure
tang six hundred and eighteen nine hundred and seven ad song nine hundred and sixty one thousand, two hundred and seventy-nine dynasties two important periods development chinese literary influential form poetry tang song shi ci respectively tang shi song ci establish crucial foundations chinese literature influence literary work daily live chinese communities last today analyze compare complete tang shi complete song ci various viewpoints presentation report find differences vocabularies interest new word start appear song ci continue use modern chinese identify color important ingredient imagery poetry discuss frequent color word appear tang shi song ci
context topic model find human readable structure unstructured textual data widely use topic modeler latent dirichlet allocation run different datasets lda suffer order effect ie different topics generate order train data shuffle order effect introduce systematic error study error relate mislead resultsspecifically inaccurate topic descriptions reduction efficacy text mine classification result objective provide method distributions generate lda stable use analysis method use ldade search base software engineer tool tune lda parameters use de differential evolution ldade evaluate data programmer information exchange site stackoverflow title abstract text thousands ofsoftware engineer se paper software defect report nasa result collect across different implementations lda pythonscikit learn scalaspark across different platforms linux macintosh different kinds ldas vemor use gibbs sample result score via topic stability text mine classification accuracy result treatments standard lda exhibit large topic instability ii ldade tune dramatically reduce cluster instability iii ldade also lead improve performances supervise well unsupervised learn conclusion due topic instability use standard lda shelf settings depreciate also future require se paper use lda test need mitigate lda topic instability finally ldade candidate technology effectively efficiently reduce instability
visual question answer vqa systems emerge desire empower users ask natural language question visual content receive valid answer response however close examination vqa problem reveal unavoidable entangle problem multiple humans may may always agree single answer visual question train model automatically predict visual question whether crowd would agree single answer propose exploit system novel application efficiently allocate human effort collect answer visual question specifically propose crowdsourcing system automatically solicit fewer human responses answer agreement expect human responses answer disagreement expect system improve upon exist crowdsourcing systems typically eliminate least twenty human effort loss information collect crowd
machine become intelligent renew interest methods measure intelligence common approach propose task human excel one machine find difficult however ideal task also easy evaluate easily gameable begin case study explore recently popular task image caption limitations task measure machine intelligence alternative promise task visual question answer test machine ability reason language vision describe dataset unprecedented size create task contain seven hundred and sixty thousand human generate question image use around ten million human generate answer machine may easily evaluate
paper propose application feature hash create word embeddings natural language process feature hash use successfully create document vectors relate task like document classification work show feature hash apply obtain word embeddings linear time size data result show algorithm need train able capture semantic mean word compare result glove show similar far know first application feature hash word embeddings problem result indicate scalable technique practical result nlp applications
opinions two thousand and sixteen yous presidential candidates express millions tweet challenge analyze automatically crowdsourcing analysis political tweet effectively also difficult due large inter rater disagreements sarcasm involve tweet typically analyze fix number workers majority vote propose crowdsourcing framework instead use dynamic allocation number workers explore two dynamic allocation methods one number workers query label tweet compute offline base predict difficulty discern sentiment particular tweet two number crowd workers determine online iterative crowd source process base inter rater agreements labelswe apply approach one thousand twitter message four yous presidential candidates clinton cruz sanders trump collect february two thousand and sixteen implement two propose methods use decision tree allocate crowd efforts tweet predict sarcastic show framework outperform traditional static allocation scheme collect opinion label crowd much lower cost maintain label accuracy
deep neural network show strike progress obtain state art result many ai research field recent years however often unsatisfying know predict paper address problem interpret visual question answer vqa model specifically interest find part input pixels image word question vqa model focus answer question tackle problem use two visualization techniques guide backpropagation occlusion find important word question important regions image present qualitative quantitative analyse importance map find even without explicit attention mechanisms vqa model may sometimes implicitly attend relevant regions image often appropriate word question
harness statistical power neural network perform language understand symbolic reason difficult require execute efficient discrete operations large knowledge base work introduce neural symbolic machine contain neural programmer ie sequence sequence model map language utterances program utilize key variable memory handle compositionality b symbolic computer ie lisp interpreter perform program execution help find good program prune search space apply reinforce directly optimize task reward structure prediction problem train weak supervision improve stability reinforce augment iterative maximum likelihood train process nsm outperform state art webquestionssp dataset train question answer pair without require feature engineer domain specific knowledge
sentiment prediction contemporary music wide range applications modern society instance select music public institutions hospitals restaurants potentially improve emotional well personnel patients customers respectively project music recommendation system build upon naive bay classifier train predict sentiment songs base song lyric alone experimental result show music correspond happy mood detect high precision base text feature obtain song lyric
whether officials trust protect national security information become matter great public controversy reignite long stand debate scope nature official secrecy declassification millions electronic record make possible analyze issue greater rigor precision use machine learn methods examine nearly million state department cable 1970s identify feature record likely classify international negotiations military operations high level communications even incomplete data algorithms use feature identify ninety classify cable eleven false positives result also show longstanding problems identification sensitive information error analysis reveal many examples overclassification underclassification indicate need research inter coder reliability among officials constitute classify material opportunity develop recommender systems better manage classification declassification
recommender systems research algorithms often characterize either collaborative filter cf content base cb cf algorithms train use dataset user preferences cb algorithms typically base item profile approach harness different data source therefore result recommend items generally different paper present cb2cf deep neural multiview model serve bridge items content cf representations cb2cf real world algorithm design microsoft store service handle around billion users worldwide cb2cf demonstrate movies apps recommendations show outperform alternative cb model completely cold items
paper describe naive bayesian predictive model two thousand and sixteen yous presidential election base twitter data use thirty-three thousand, seven hundred and eight tweet gather since december sixteen two thousand and fifteen february twenty-nine two thousand and sixteen introduce simpler data preprocessing method label data train model model achieve nine hundred and fifty-eight accuracy ten fold cross validation predict ted cruz bernie sanders republican democratic nominee respectively achieve comparable result competitor methods
paper present intelligent voice iv system submit nist two thousand and sixteen speaker recognition evaluation sre primary emphasis sre year develop speaker recognition technology robust novel languages much heterogeneous use current state art use significantly less train data contain meta data languages system base state art vector plda develop fix train condition result report protocol define development set challenge
recently significant activity develop algorithms provable guarantee topic model standard topic model topic sport business politics view probability distribution vec ai word document generate first select mixture vec w topics generate word iid associate mixture avec w give large collection document goal recover topic vectors correctly classify new document accord topic mixture work consider broad generalization framework word longer assume draw iid instead topic complex distribution sequence paragraph since one could hope even represent distribution general even paragraph give use natural feature representation aim instead directly learn document classifier aim learn predictor give new document accurately predict topic mixture without learn distributions explicitly present several natural condition one efficiently discuss issue noise tolerance sample complexity model generally model view generalization multi view co train set machine learn
recurrent neural network successful predict sequence word task language model however model base conventional classification framework model train one hot target word represent input output isolation cause inefficiencies learn term utilize information term number parameters need train introduce novel theoretical framework facilitate better learn language model show framework lead tie together input embed output projection matrices greatly reduce number trainable variables framework lead state art performance penn treebank variety network model
recurrent neural network powerful tool model sequential data dependence timestep computation previous timestep output limit parallelism make rnns unwieldy long sequence introduce quasi recurrent neural network qrnns approach neural sequence model alternate convolutional layer apply parallel across timesteps minimalist recurrent pool function apply parallel across channel despite lack trainable recurrent layer stack qrnns better predictive accuracy stack lstms hide size due increase parallelism sixteen time faster train test time experiment language model sentiment classification character level neural machine translation demonstrate advantage underline viability qrnns basic build block variety sequence task
lipread task decode text movement speaker mouth traditional approach separate problem two stag design learn visual feature prediction recent deep lipread approach end end trainable wand et al two thousand and sixteen chung zisserman 2016a however exist work model train end end perform word classification rather sentence level sequence prediction study show human lipread performance increase longer word easton basala one thousand, nine hundred and eighty-two indicate importance feature capture temporal context ambiguous communication channel motivate observation present lipnet model map variable length sequence video frame text make use spatiotemporal convolutions recurrent network connectionist temporal classification loss train entirely end end best knowledge lipnet first end end sentence level lipread model simultaneously learn spatiotemporal visual feature sequence model grid corpus lipnet achieve nine hundred and fifty-two accuracy sentence level overlap speaker split task outperform experience human lipreaders previous eight hundred and sixty-four word level state art accuracy gergen et al two thousand and sixteen
paper propose topicrnn recurrent neural network rnn base language model design directly capture global semantic mean relate word document via latent topics sequential nature rnns good capture local structure word sequence semantic syntactic might face difficulty remember long range dependencies intuitively long range dependencies semantic nature contrast latent topic model able capture global underlie semantic structure document account word order propose topicrnn model integrate merit rnns latent topic model capture local syntactic dependencies use rnn global semantic dependencies use latent topics unlike previous work contextual rnn language model model learn end end empirical result word prediction show topicrnn outperform exist contextual rnn baselines addition topicrnn use unsupervised feature extractor document sentiment analysis imdb movie review dataset report error rate six hundred and twenty-eight comparable state art five hundred and ninety-one result semi supervise approach finally topicrnn also yield sensible topics make useful alternative document model latent dirichlet allocation
question answer qa subject resurgence past years say resurgence lead multitude question answer qa systems develop company research facilities components qa systems get reuse across implementations systems leverage full potential component reuse hence development qa systems currently still tedious time consume process address challenge accelerate creation novel tailor qa systems present concept self wire approach compose qa systems approach allow reuse exist web base qa systems modules develop new qa platforms end rely qa modules describe use web ontology language base descriptions approach able automatically compose qa systems use data drive approach automatically
propose gaussian attention model content base neural memory access propose attention model neural network additional degree freedom control focus attention laser sharp attention broad attention applicable whenever assume distance latent space reflect notion semantics use propose attention model score function embed knowledge base continuous vector space train model perform question answer entities knowledge base propose attention model handle propagation uncertainty follow series relations also conjunction condition natural way dataset soccer players participate fifa world cup two thousand and fourteen demonstrate model handle path query conjunctive query well
twitter social network contain large amount information generate users information compose opinions comment may reflect trend social behavior talk trend possible identify opinions comment gear towards share lot people direction determine two write opinions share address techniques natural language process nlp use paper propose methodology predict reflect twitter use sentiment analysis function nlp base social behaviors case study select two thousand and fifteen presidential argentina software architecture big data compose vertica data base component call pulse use analysis possible detect trend vote intentions regard presidential candidates achieve greater accuracy predict achieve traditional systems survey
formulate sequence sequence transduction noisy channel decode problem use recurrent neural network parameterise source channel model unlike direct model suffer explain away effect train noisy channel model must produce output explain input component model train pair train sample also unpaired sample marginal output distribution use latent variable control much condition sequence channel model need read order generate subsequent symbol obtain tractable effective beam search decoder experimental result abstractive sentence summarisation morphological inflection machine translation show noisy channel model outperform direct model significantly benefit increase amount unpaired output data direct model easily use
model structure coherent texts key nlp problem task coherently organize give set sentence commonly use build evaluate model understand structure propose end end unsupervised deep learn approach base set sequence framework address problem model strongly outperform prior methods order discrimination task novel task order abstract scientific article furthermore work show useful text representations obtain learn order sentence visualize learn sentence representations show model capture high level logical structure paragraph representations perform comparably state art pre train methods sentence similarity paraphrase detection task
work present general unsupervised learn method improve accuracy sequence sequence seq2seq model method weight encoder decoder seq2seq model initialize pretrained weight two language model fine tune label data apply method challenge benchmarks machine translation abstractive summarization find significantly improve subsequent supervise model main result pretraining improve generalization seq2seq model achieve state art result wmt englishrightarrowgerman task surpass range methods use phrase base machine translation neural machine translation method achieve significant improvement thirteen bleu previous best model wmt fourteen wmt fifteen englishrightarrowgerman also conduct human evaluations abstractive summarization find method outperform purely supervise learn baseline statistically significant manner
work propose train algorithm audio visual automatic speech recognition av asr system use deep recurrent neural network rnnfirst train deep rnn acoustic model connectionist temporal classification ctc objective function frame label obtain acoustic model use perform non linear dimensionality reduction visual feature use deep bottleneck network audio visual feature fuse use train fusion rnn use bottleneck feature visual modality help model converge properly train system evaluate grid corpus result show presence visual modality give significant improvement character error rate cer various level noise even model train without noisy data also provide comparison two fusion methods feature fusion decision fusion
acquire first language incredible feat easily duplicate learn communicate use nothing pictureless book corpus would likely impossible even humans nevertheless dominate approach natural language process today alternative propose use situate interactions agents drive force communication framework deep recurrent q network evolve share language ground provide environment task agents interactive image search form game guess image game provide non trivial environment agents discuss natural ground concepts decide encode communication experiment show agents learn encode physical concepts word ie ground also agents learn hold multi step dialogue remember state dialogue step step
paper describe ustcnelslip systems submit trilingual entity detection link edl track two thousand and sixteen tac knowledge base population kbp contest build two systems entity discovery mention detection md one use conditional rnnlm one use attention base encoder decoder framework entity link el system consist two modules rule base candidate generation neural network probability rank model moreover simple string match rule use nil cluster end best system achieve f1 score six hundred and twenty-four end end type mention ceaf plus metric
neural network model document classification social media focus text infor mation neglect information platforms paper classify post stance social media channel develop utcnn neural network model incorporate user taste topic taste user comment post utcnn work social media texts also analyze texts forums message board experiment perform chinese facebook data english online debate forum data show utcnn achieve seven hundred and fifty-five macro average f score supportive neutral unsupportive stance class facebook data significantly better model either user topic comment information withhold model design greatly mitigate lack data minor class without use oversampling addition utcnn yield eight hundred and forty-two accuracy english online debate forum data also significantly outperform result previous work well deep learn model show utcnn perform well regardless language platform
study attempt build contemporary linguistic corpus arabic language corpus produce text corpus include five million newspaper article contain billion half word total three million unique word data collect newspaper article ten major news source eight arabic countries period fourteen years corpus encode two type encode namely utf eight windows cp one thousand, two hundred and fifty-six also mark two mark languages namely sgml xml
sequence label architectures use word embeddings capture similarity suffer handle previously unseen rare word investigate character level extensions model propose novel architecture combine alternative word representations use attention mechanism model able dynamically decide much information use word character level component evaluate different architectures range sequence label datasets character level extensions find improve performance every benchmark addition propose attention base architecture deliver best result even smaller number trainable parameters
propose approach build neural machine translation system supervise resources ie parallel corpora use multimodal embed representation texts image base assumption text document often likely describe multimedia information eg image somewhat relate content try indirectly estimate relevance two languages use multimedia pivot project modalities one common hide space sample belong similar semantic concepts come close whatever observe space sample modality agnostic representation key bridge gap different modalities put decoder top network flexibly draw output input modality notably test phase need source language texts input translation experiment test method two benchmarks show achieve reasonable translation performance compare investigate several possible implementations find end end model simultaneously optimize rank loss multimodal encoders cross entropy loss decoders perform best
since large knowledge base typically incomplete miss facts need infer observe facts task call knowledge base completion successful approach task typically explore explicit paths sequence triple approach usually resort human design sample procedures since large knowledge graph produce prohibitively large number possible paths uninformative alternative approach propose perform single short sequence interactive lookup operations embed knowledge graph train end end backpropagation optimize compress version initial knowledge base propose model call embed knowledge graph network ekgn achieve new state art result popular knowledge base completion benchmarks
topic model many algorithms guarantee identifiability topics develop premise exist anchor word ie word appear positive probability one topic follow work resort three higher order statistics data corpus relax anchor word assumption reliable estimate higher order statistics hard obtain however identification topics model hinge uncorrelatedness topics unrealistic paper revisit topic model base second order moments propose anchor free topic mine framework propose approach guarantee identification topics much milder condition compare anchor word assumption thereby exhibit much better robustness practice associate algorithm involve one eigen decomposition small linear program make easy implement scale large problem instance experiment use tdt2 reuters twenty-one thousand, five hundred and seventy-eight corpus demonstrate propose anchor free approach exhibit favorable performance measure use coherence similarity count cluster accuracy metrics compare prior art
recent years witness increase interest potential benefit intelligent autonomous machine robots honda asimo humanoid robot irobot roomba robot vacuum cleaner google driverless cars fire imagination general public social media buzz speculation utopian world helpful robot assistants come robot apocalypse however long way go autonomous systems reach level capabilities require even simplest task involve human robot interaction especially involve communicative behaviour speech language course field artificial intelligence ai make great stride areas move abstract high level rule base paradigms embody architectures whose operations ground real physical environments still miss however overarch theory intelligent communicative behaviour inform system level design decisions order provide coherent approach system integration chapter introduce beginnings framework inspire principles perceptual control theory pct particular observe pct hitherto tend view perceptual process relatively straightforward series transformations sensation perception overlook potential powerful generative model base solutions emerge practical field visual auditory scene analysis start first principles sequence arguments present show ideas might integrate pct also extend pct towards remarkably symmetric architecture need drive communicative agent conclude behaviour control perception perception simulation behaviour
examine effect group lasso glasso regularizer select salient nod deep neural network dnn hide layer apply dnn hmm hybrid speech recognizer ted talk speech data test two type glasso regularization one outgo weight vectors another incoming weight vectors well two size dnns two thousand and forty-eight hide layer nod four thousand and ninety-six nod furthermore compare glasso l2 regularizers experiment result demonstrate dnn train glasso regularizer embed successfully select hide layer nod necessary sufficient achieve high classification power
part appeal visual question answer vqa promise answer new question previously unseen image current methods demand train question illustrate every possible concept therefore never achieve capability since volume require train data would prohibitive answer general question image require methods capable zero shoot vqa methods able answer question beyond scope train question propose new evaluation protocol vqa methods measure ability perform zero shoot vqa highlight significant practical deficiencies current approach mask bias current datasets propose evaluate several strategies achieve zero shoot vqa include methods base pretrained word embeddings object classifiers semantic embeddings test time retrieval example image extensive experiment intend serve baselines zero shoot vqa also achieve state art performance standard vqa evaluation set
recurrent neural network rnns use extensively increase success model various type sequential data much progress achieve devise recurrent units architectures flexibility capture complex statistics data long range dependency localize attention phenomena however many sequential data video speech language highly variable information flow recurrent model still consume input feature constant rate perform constant number computations per time step detrimental speed model capacity paper explore modification exist recurrent units allow learn vary amount computation perform step without prior knowledge sequence time structure show experimentally model require fewer operations also lead better performance overall evaluation task
curriculum learn emphasize order train instance computational learn setup core hypothesis simpler instance learn early build block learn complex ones despite usefulness still unknown exactly internal representation model affect curriculum learn paper study effect curriculum learn long short term memory lstm network show strong competency many natural language process nlp problems experiment sentiment analysis task synthetic task similar sequence prediction task nlp show curriculum learn positive effect lstm internal state bias model towards build constructive representations ie internal representation previous timesteps use build block final prediction also find smaller model significantly improve train curriculum learn lastly show curriculum learn help amount train data limit
researchers recently start investigate deep neural network dialogue applications particular generative sequence sequence seq2seq model show promise result unstructured task word level dialogue response generation hope model able leverage massive amount data learn meaningful natural language representations response generation strategies require minimum amount domain knowledge hand craft important challenge develop model effectively incorporate dialogue context generate meaningful diverse responses support goal review recently propose model base generative encoder decoder neural network architectures show model better ability incorporate long term dialogue history model uncertainty ambiguity dialogue generate responses high level compositional structure
rumour detection hard accurate systems operate retrospectively recognize rumour collect repeat signal rumour might already spread cause harm introduce new category feature base novelty tailor detect rumour early compensate absence repeat signal make use news wire additional data source unconfirmed novel information respect news article consider indication rumour additionally introduce pseudo feedback assume document similar previous rumour likely also rumour comparison real time approach show novelty base feature conjunction pseudo feedback perform significantly better detect rumour instantly publication
online social media game increasingly replace offline social activities social media indispensable mode communication online game genuine social activity also popular spectator sport support anonymity larger audiences online interaction shrink social geographical barriers despite benefit social disparities gender inequality persist online social media particular online game communities criticize persistent gender disparities objectification game evolve social platform persistence gender disparity press question yet large scale systematic study gender inequality objectification social game platforms analyze one billion chat message twitch social game stream platform study gender streamers associate nature conversation use combination computational text analysis methods show gendered conversation objectification prevalent chat female streamers receive significantly objectify comment male streamers receive game relate comment difference pronounce popular streamers also exist large number users post female male stream employ neural vector space embed paragraph vector method analyze gendered chat message create prediction model identify gender streamers base message post channel ii identify gender viewer prefer watch base chat message find suggest disparities social game stream platforms nuanced phenomenon involve gender streamers well produce gendered game relate conversation
critical advance manufacture machine autonomously execute task follow end user natural language nl instructions however nl instructions usually ambiguous abstract machine may misunderstand incorrectly execute task address nl base human machine communication problem enable machine appropriately execute task follow end user nl instructions develop machine executable plan generation exeplan method exeplan method conduct task center semantic analysis extract task relate information ambiguous nl instructions addition method specify machine execution parameters generate machine executable plan interpret abstract nl instructions evaluate exeplan method industrial robot baxter instruct nl perform three type industrial task drill hole clean spot install screw experiment result prove exeplan method effective generate machine executable plan end user nl instructions method promise endow machine ability nl instruct task execution
lexicon base classification document assign label compare number word appear two oppose lexicons positive negative sentiment create word list often easier label instance debug non experts classification performance unsatisfactory however little analysis justification classification heuristic paper describe set assumptions use derive probabilistic justification lexicon base classification well analysis expect accuracy one key assumption behind lexicon base classification word lexicon equally predictive rarely true practice lexicon base approach usually outperform supervise classifiers learn distinct weight word label instance paper show possible learn weight without label data leverage co occurrence statistics across lexicons offer best worlds light supervision form lexicons data drive classification higher accuracy traditional word count heuristics
accuracy optical character recognition ocr crucial success subsequent applications use text analyze pipeline recent model ocr post process significantly improve quality ocr generate text still prone suggest correction candidates limit observations insufficiently account characteristics ocr errors paper show enlarge candidate suggestion space use external corpus integrate ocr specific feature regression approach correct ocr generate errors evaluation result show model correct six hundred and fifteen ocr errors consider top one suggestion seven hundred and fifteen ocr errors consider top three suggestions case theoretical correction upper bind seventy-eight
speech one effective ways communication among humans even though audio common way transmit speech important information find modalities vision vision particularly useful acoustic signal corrupt multi modal speech recognition however yet find wide spread use mostly temporal alignment fusion different information source challenge paper present end end audiovisual speech recognizer avsr base recurrent neural network rnn connectionist temporal classification ctc loss function ctc create sparse peaky output activations analyze differences alignments output target phonemes visemes audio video audio visual feature representations present first experiment large vocabulary ibm viavoice database outperform previously publish approach phone accuracy clean noisy condition
currently personal assistant systems run smartphones use natural language interfaces however systems rely mostly web find information mobile wearable devices collect enormous amount contextual personal data sleep physical activities information object applications know quantify self mobile health personal informatics use provide deeper insight behavior knowledge exist personal assistant systems support type quantify self query response undertake user study analyze set textual question query users use search quantify self mobile health data analyze question construct light weight natural language base query interface include text parser algorithm user interface process users query use search quantify self information query interface design operate small devices ie smartwatches well augment personal assistant systems allow process end users natural language query quantify self data
semantic compositional network scn develop image caption semantic concepts ie tag detect image probability tag use compose parameters long short term memory lstm network scn extend weight matrix lstm ensemble tag dependent weight matrices degree member ensemble use generate image caption tie image dependent probability correspond tag addition caption image also extend scn generate caption video clip qualitatively analyze semantic composition scns quantitatively evaluate algorithm three benchmark datasets coco flickr30k youtube2text experimental result show propose method significantly outperform prior state art approach across multiple evaluation metrics
enhance developer productivity modern integrate development environments ides include code suggestion functionality propose likely next tokens cursor current ides work well statically type languages reliance type annotations mean provide level support dynamic program languages statically type languages moreover suggestion engines modern ides propose expressions multi statement idiomatic code recent work show language model improve code suggestion systems learn software repositories paper introduce neural language model sparse pointer network aim capture long range dependencies release large scale code suggestion corpus 41m line python code crawl github corpus find standard neural language model perform well suggest local phenomena struggle refer identifiers introduce many tokens past augment neural language model pointer network specialize refer predefined class identifiers obtain much lower perplexity five percentage point increase accuracy code suggestion compare lstm baseline fact increase code suggestion accuracy due thirteen time accurate prediction identifiers furthermore qualitative analysis show model indeed capture interest long range dependencies like refer class member define sixty tokens past
paper focus train evaluate effective word embeddings text visual information specifically introduce large scale dataset three hundred million sentence describe forty million image crawl download publicly available pin ie image sentence descriptions upload users pinterest dataset two hundred time larger ms coco standard large scale image dataset sentence descriptions addition construct evaluation dataset directly assess effectiveness word embeddings term find semantically similar relate word phrase word phrase pair evaluation dataset collect click data millions users image search system thus contain rich semantic relationships base datasets propose compare several recurrent neural network rnns base multimodal text image model experiment show model benefit incorporate visual information word embeddings weight share strategy crucial learn multimodal embeddings project page http wwwstatuclaedu junhuamao multimodalembeddinghtml
automate extraction concepts patient clinical record essential facilitator clinical research reason two thousand and ten i2b2 va natural language process challenge clinical record introduce concept extraction task aim identify classify concepts predefined categories ie treatments test problems state art concept extraction approach heavily rely handcraft feature domain specific resources hard collect define reason paper propose alternative streamline approach recurrent neural network bidirectional lstm crf decode initialize general purpose shelf word embeddings experimental result achieve two thousand and ten i2b2 va reference corpora use propose framework outperform recent methods rank closely best submission original two thousand and ten i2b2 va challenge
introduce task visual dialog require ai agent hold meaningful dialog humans natural conversational language visual content specifically give image dialog history question image agent grind question image infer context history answer question accurately visual dialog disentangle enough specific downstream task serve general test machine intelligence ground vision enough allow objective evaluation individual responses benchmark progress develop novel two person chat data collection protocol curate large scale visual dialog dataset visdial visdial v09 release contain one dialog ten question answer pair 120k image coco total 12m dialog question answer pair introduce family neural encoder decoder model visual dialog three encoders late fusion hierarchical recurrent encoder memory network two decoders generative discriminative outperform number sophisticate baselines propose retrieval base evaluation protocol visual dialog ai agent ask sort set candidate answer evaluate metrics mean reciprocal rank human response quantify gap machine human performance visual dialog task via human study put together demonstrate first visual chatbot dataset code train model visual chatbot available https visualdialogorg
standard deep reinforcement learn methods deep q network dqn multiple task domains face scalability problems propose method multi domain dialogue policy learn term ndqn apply information seek speak dialogue system domains restaurants hotels experimental result compare dqn baseline versus ndqn propose use simulations report propose method exhibit better scalability promise optimise behaviour multi domain dialogue systems
structural correspondence learn scl effective method cross lingual sentiment classification approach use unlabeled document along word translation oracle automatically induce task specific cross lingual correspondences transfer knowledge identify important feature ie pivot feature simplicity however assume word translation oracle map pivot feature source language exactly one word target language one one map word different languages strict also context consider paper propose cross lingual scl base distribute representation word learn meaningful one many mappings pivot word use large amount monolingual data small dictionary conduct experiment nlpandcc two thousand and thirteen cross lingual sentiment analysis dataset employ english source language chinese target language method rely parallel corpora experimental result show approach competitive state art methods cross lingual sentiment classification
learn natural language interface database table challenge task involve deep language understand multi step reason task often approach map natural language query logical form program provide desire response execute database knowledge paper present first weakly supervise end end neural network model induce program real world dataset enhance objective function neural programmer neural network build discrete operations apply wikitablequestions natural language question answer dataset model train end end weak supervision question answer pair require domain specific grammars rule annotations key elements previous approach program induction main experimental result paper single neural programmer model achieve three hundred and forty-two accuracy use ten thousand examples weak supervision ensemble fifteen model trivial combination technique achieve three hundred and seventy-seven accuracy competitive current state art accuracy three hundred and seventy-one obtain traditional natural language semantic parser
regard computational representation literary plot paper look use sentiment analysis happy end detection german novels focus lie investigation previously propose sentiment feature order gain insight relevance specific feature one hand implications performance hand therefore study various partition novels consider highly variable concept end also show approach even though still rather simple potentially lead substantial find relevant literary study
developers text speech synthesizers tts often make use human raters assess quality synthesize speech demonstrate model human raters mean opinion score mos synthesize speech use deep recurrent neural network whose input consist solely raw waveform best model provide utterance level estimate mos moderately inferior sample human rat show pearson spearman correlations multiple utterances score average scenario common synthesizer quality assessment automos achieve correlations approach human raters automos model number applications ability explore parameter space speech synthesizer without require human loop
computer vision pixelwise dense prediction task predict label pixel image convolutional neural network achieve good performance task computationally efficient paper carry ideas problem assign sequence label set speech frame task commonly know framewise classification show dense prediction view framewise classification offer several advantage insights include computational efficiency ability apply batch normalization dense prediction pay specific attention stride pool time introduce asymmetric dilate convolution call time dilate convolution allow efficient elegant implementation pool time show result use time dilate convolutions deep vgg style cnn batch normalization hub5 switchboard two thousand benchmark task big n gram language model achieve seventy-seven wer best single model single pass performance report far
exist many problem domains interpretability neural network model essential deployment introduce recurrent architecture compose input switch affine transformations word rnn without explicit nonlinearities input dependent recurrent weight simple form allow rnn analyze via straightforward linear methods exactly characterize linear contribution input model predictions use change basis disentangle input output computational hide unit subspaces fully reverse engineer architecture solution simple task despite ease interpretation input switch affine network achieve reasonable performance text model task allow greater computational efficiency network standard nonlinearities
advent semantic web various tool techniques introduce present organize knowledge concept hierarchies one technique gain significant attention due usefulness create domain ontologies consider integral part semantic web automate concept hierarchy learn algorithms focus extract relevant concepts unstructured text corpus connect together identify potential relations exist paper propose novel approach identify relevant concepts plain text learn hierarchy concepts exploit subsumption relation start model topics use probabilistic topic model make use lightweight linguistic process extract semantically rich concepts connect concepts identify relationship pair concepts propose method completely unsupervised need domain specific train corpus concept extraction learn experiment large real world text corpora bbc news dataset reuters news corpus show propose method outperform exist methods concept extraction efficient concept hierarchy learn possible overall task guide probabilistic topic model algorithm
exist word embed approach distinguish word different contexts therefore ignore contextual mean result learn embeddings word usually mixture multiple mean paper acknowledge multiple identities word different contexts learn textbfidentity sensitive word embeddings base identity label text corpora heterogeneous network word word identities construct model different level word co occurrences heterogeneous network embed low dimensional space principled network embed approach able obtain embeddings word embeddings word identities study three different type word identities include topics sentiments categories experimental result real world data set show identity sensitive word embeddings learn approach indeed capture different mean word outperform competitive methods task include text classification word similarity computation
statistical topic model efficiently facilitate exploration large scale data set many model develop broadly use summarize semantic structure news science social media digital humanities however common practical objective data exploration task enumerate exist topics quickly extract representative ones broadly cover content corpus ie topics serve good summary data exist topic model fit exactly number topics user specify impose unnecessary burden users limit prior knowledge instead propose new model able learn fewer representative topics purpose data summarization propose reinforce random walk allow prominent topics absorb tokens similar smaller topics thus enhance diversity among top topics extract reinforce random walk general process embed classical topic model obtain textitdiverse topic model able extract prominent diverse topics data inference procedures diverse topic model remain simple efficient classical model experimental result demonstrate diverse topic model discover topics better summarize data also require minimal prior knowledge users
social media users tend mention entities react news events main purpose work create entity centric aggregations tweet daily basis apply topic model sentiment analysis create data visualization insights current events people reactions events entity centric perspective
propose novel deep learn model support permutation invariant train pit speaker independent multi talker speech separation commonly know cocktail party problem different prior arts treat speech separation multi class regression problem deep cluster technique consider segmentation cluster problem model optimize separation regression error ignore order mix source strategy cleverly solve long last label permutation problem prevent progress deep learn base techniques speech separation experiment equal energy mix setup danish corpus confirm effectiveness pit believe improvements build upon pit eventually solve cocktail party problem enable real world adoption eg automatic meet transcription multi party human computer interaction overlap speech common
propose simple domain adaptation method neural network supervise set supervise domain adaptation way improve generalization performance target domain use source domain dataset assume datasets label recently recurrent neural network show successful variety nlp task caption generation however exist domain adaptation techniques limit one tune model parameters target dataset train source dataset two design network dual output one source domain target domain reformulate idea domain adaptation technique propose daume two thousand and seven propose simple domain adaptation method apply neural network train cross entropy loss caption datasets show performance improvements domain adaptation methods
consider task kbp slot fill extract relation information newswire document knowledge base construction present pipeline employ relational dependency network rdns learn linguistic pattern relation extraction additionally demonstrate several components weak supervision word2vec feature joint learn use human advice incorporate relational framework evaluate different components benchmark kbp two thousand and fifteen task show rdns effectively model diverse set feature perform competitively current state art relation extraction
consider problem model temporal textual data take endogenous exogenous process account text document arise real world applications include job advertisements economic news article influence fluctuations general economy propose hierarchical bayesian topic model impose group correlate hierarchical structure evolution topics time incorporate process show model estimate markov chain monte carlo sample methods demonstrate model capture intrinsic relationships topic distribution time dependent factor compare performance latent dirichlet allocation lda two relate model model apply two collections document illustrate empirical performance online job advertisements directemployers association journalists post businessinsidercom
sponsor search represent major source revenue web search engines popular advertise model bring unique possibility advertisers target users immediate intent communicate search query usually display ads alongside organic search result query deem relevant products service however due large number unique query challenge advertisers identify relevant query reason search engines often provide service advance match automatically find additional relevant query advertisers bid present novel advance match approach base idea semantic embeddings query ads embeddings learn use large data set user search sessions consist search query click ads search link utilize contextual information dwell time skip ads address large scale nature problem term data vocabulary size propose novel distribute algorithm train embeddings finally present approach overcome cold start problem associate new ads query report result editorial evaluation online test actual search traffic result show approach significantly outperform baselines term relevance coverage incremental revenue lastly open source learn query embeddings use researchers computational advertise relate field
efficient market hypothesis popular theory stock prediction failure much research carry area prediction stock project take non quantifiable data financial news article company predict future stock trend news sentiment classification assume news article impact stock market attempt study relationship news stock trend show create three different classification model depict polarity news article positive negative observations show rf svm perform well type test nai bay give good result compare two experiment conduct evaluate various aspects propose model encourage result obtain experiment accuracy prediction model eighty comparison news random label fifty accuracy model increase accuracy thirty
highway deep neural network hdnn type depth gate feedforward neural network show easier train hide layer also generalise better compare conventional plain deep neural network dnns previously investigate structure hdnn architecture speech recognition two gate function tie across hide layer able train much smaller model without sacrifice recognition accuracy paper carry study architecture sequence discriminative train criterion speaker adaptation techniques ami meet speech recognition corpus show two techniques improve speech recognition accuracy top model train cross entropy criterion furthermore demonstrate two gate function tie across hide layer able control information flow whole network achieve considerable improvements update gate function sequence train adaptation experiment
nearly seventy thousand bill introduce yous congress two thousand and one two thousand and fifteen two thousand, five hundred and thirteen enact develop machine learn approach forecast probability bill become law start two thousand and one 107th congress train model data previous congresses predict bill current congress repeat 113th congress serve test prediction score sentence bill language model embed legislative vocabulary high dimensional semantic lade vector space language representation enable investigation word increase probability enactment topic test relative importance text context compare text model context model use variables whether bill sponsor majority party test effect change bill introduction ability predict final outcome compare use bill text meta data available time introduction use recent data time introduction context predictions outperform text newest data text outperform context combine text context always perform best conduct global sensitivity analysis combine model determine important variables predict enactment
work propose game theoretic model document cluster document cluster represent player cluster strategy players receive reward interact players try maximize choose best strategies geometry data model weight graph encode pairwise similarity among document similar players constrain choose similar strategies update strategy preferences iteration game use different approach find prototypical elements cluster information divide players two disjoint set one collect players definite strategy one collect players try learn others correct strategy play latter set players consider new data point cluster accord previous information representation useful scenarios data stream continuously evaluation system conduct thirteen document datasets use different settings show propose method perform well compare different document cluster algorithms
introduce rnns log linear rnns extension recurrent neural network replace softmax output layer log linear output layer softmax special case conceptually simple move two main advantage first allow learner combat train data sparsity allow model word generally output symbols complex combinations attribute without require combination directly observe train data softmax second permit inclusion flexible prior knowledge form priori specify modular feature neural network component learn dynamically control weight log linear distribution exploit feature conduct experiment domain language model french exploit morphological prior knowledge show important decrease perplexity relative baseline rnn provide motivate iillustrations finally argue log linear neural network components contribute complementary strengths rnn aspect allow model incorporate rich prior knowledge nn aspect accord representation learn paradigm allow model discover novel combination characteristics
sentiment analysis polarities opinions express object feature determine assess sentiment sentence document whether positive negative neutral naturally object feature noun representation refer product component product let us say lens camera opinions emanate capture adjectives verbs adverbs noun word apart word meta information diverse effective feature also go play important role influence sentiment polarity contribute significantly performance system paper associate information meta data explore investigate sentiment text base analysis result present scope assessment utilization meta information feature text categorization rank text document identification spam document polarity classification problems
word embeddings show useful across state art systems many natural language process task range question answer systems dependency parse herbelot vecchi two thousand and fifteen explore word embeddings utility model language semantics particular present approach automatically map standard distributional semantic space onto set theoretic model use partial least square regression show paper simple baseline achieve fifty-one relative improvement compare model one two datasets use yield competitive result second dataset
many sequential process task require complex nonlinear transition function one step next however recurrent neural network deep transition function remain difficult train even use long short term memory lstm network introduce novel theoretical analysis recurrent network base gersgorin circle theorem illuminate several model optimization issue improve understand lstm cell base analysis propose recurrent highway network extend lstm architecture allow step step transition depths larger one several language model experiment demonstrate propose architecture result powerful efficient model penn treebank corpus solely increase transition depth one ten improve word level perplexity nine hundred and six six hundred and fifty-four use number parameters larger wikipedia datasets character prediction text8 enwik8 rhns outperform previous result achieve entropy one hundred and twenty-seven bits per character
link human motion natural language great interest generation semantic representations human activities well generation robot activities base natural language input however years research area standardize openly available dataset exist support development evaluation systems therefore propose kit motion language dataset large open extensible aggregate data multiple motion capture databases include dataset use unify representation independent capture system marker set make easy work data regardless origin obtain motion annotations natural language apply crowd source approach web base tool specifically build purpose motion annotation tool thoroughly document annotation process discuss gamification methods use keep annotators motivate propose novel method perplexity base selection systematically select motion annotation either represent dataset erroneous annotations show method mitigate two aforementioned problems ensure systematic annotation process provide depth analysis structure content result dataset october ten two thousand and sixteen contain three thousand, nine hundred and eleven motion total duration one thousand, one hundred and twenty-three hours six thousand, two hundred and seventy-eight annotations natural language contain fifty-two thousand, nine hundred and three word believe make dataset excellent choice enable transparent comparable research important area
present memory augment neural network natural language understand neural semantic encoders nse equip novel memory update rule variable size encode memory evolve time maintain understand input sequence read compose write operations nse also access multiple share memories paper demonstrate effectiveness flexibility nse five different natural language task natural language inference question answer sentence classification document sentiment analysis machine translation nse achieve state art performance evaluate publically available benchmarks example share memory model show encourage result neural machine translation improve attention base baseline approximately ten bleu
recurrent neural network rnns process input text sequentially model conditional transition word tokens contrast advantage recursive network include explicitly model compositionality recursive structure natural language however current recursive architecture limit dependence syntactic tree paper introduce robust syntactic parse independent tree structure model neural tree indexers nti provide middle grind sequential rnns syntactic treebased recursive model nti construct full n ary tree process input text node function bottom fashion attention mechanism apply structure node function implement evaluate binarytree model nti show model achieve state art performance three different nlp task natural language inference answer sentence selection sentence classification outperform state art recurrent recursive neural network
recent years see significant market penetration voice base personal assistants apple siri however despite success user take frustratingly low position paper argue habitability gap cause inevitable mismatch capabilities expectations human users feature benefit provide contemporary technology suggestions make problems might mitigate worrisome question emerge speak language nothing answer base contemporary view special nature speak language may indeed fundamental limit interaction take place mismatch interlocutors humans machine however conclude interactions native non native speakers adults children even humans dog might provide critical inspiration design future speech base human machine interaction
present novel view unify two frameworks aim solve sequential prediction problems learn search l2s recurrent neural network rnn point equivalences elements two frameworks complement miss one framework compare introduce advance imitation learn framework one hand augment l2s notion search space hand enhance rnns train procedure robust compound errors arise train highly correlate examples
natural language inference important task natural language understand concern classify logical relation two sentence paper propose several text generative neural network generate text hypothesis allow construction new natural language inference datasets evaluate model propose new metric accuracy classifier train generate dataset accuracy obtain best generative model twenty-seven lower accuracy classifier train original human craft dataset furthermore best generate dataset combine original dataset achieve highest accuracy best model learn map embed train example compare various metrics show datasets obtain higher rouge meteor score necessarily yield higher classification accuracies also provide analysis characteristics good dataset include distinguishability generate datasets original one
recent years cross modal retrieval draw much attention due rapid growth multimodal data take one type data query retrieve relevant data another type example user use text retrieve relevant picture videos since query retrieve result different modalities measure content similarity different modalities data remain challenge various methods propose deal problem paper first review number representative methods cross modal retrieval classify two main group one real value representation learn two binary representation learn real value representation learn methods aim learn real value common representations different modalities data speed cross modal retrieval number binary representation learn methods propose map different modalities data common ham space introduce several multimodal datasets community show experimental result two commonly use multimodal datasets comparison reveal characteristic different kinds cross modal retrieval methods expect benefit practical applications future research finally discuss open problems future research directions
question answer qa neural network ie neural qa achieve promise result recent years lack large scale real word qa dataset still challenge develop evaluate neural qa system alleviate problem propose large scale human annotate real world qa dataset webqa 42k question 556k evidence exist neural qa methods resolve qa either sequence generation classification rank problem face challenge expensive softmax computation unseen answer handle separate candidate answer generation component work cast neural qa sequence label problem propose end end sequence label model overcome challenge experimental result webqa show model outperform baselines significantly f1 score seven thousand, four hundred and sixty-nine word base input performance drop three hundred and seventy-two f1 point challenge character base input
blind application machine learn run risk amplify bias present data danger face us word embed popular framework represent text data vectors use many machine learn natural language process task show even word embeddings train google news article exhibit female male gender stereotype disturb extent raise concern widespread use describe often tend amplify bias geometrically gender bias first show capture direction word embed second gender neutral word show linearly separable gender definition word word embed use properties provide methodology modify embed remove gender stereotype association word receptionist female maintain desire associations word queen female define metrics quantify direct indirect gender bias embeddings develop algorithms debias embed use crowd worker evaluation well standard benchmarks empirically demonstrate algorithms significantly reduce gender bias embeddings preserve useful properties ability cluster relate concepts solve analogy task result embeddings use applications without amplify gender bias
word embed methods revolve around learn continuous distribute vector representations word neural network capture semantic syntactic cue turn use induce similarity measure among word sentence document context celebrate methods categorize prediction base count base methods accord train objectives model architectures pros con extensively analyze evaluate recent study relatively less work continue line research develop enhance learn method bring together advantage two model families addition interpretation learn word representations still remain somewhat opaque motivate observations consider press need paper present novel method learn word representations inherit advantage classic word embed methods also offer clearer rigorous interpretation learn word representations build upon propose word embed method formulate translation base language model framework extractive speech summarization task series empirical evaluations demonstrate effectiveness propose word representation learn language model techniques extractive speech summarization
action may proceed plan may interrupt resume override challenge handle natural language understand system describe extensions exist implementation control autonomous systems natural language enable systems handle incoming language request regard action language communication autonomous systems lcas extend support x net parameterized executable schemas represent action x net enable system control action desire level granularity provide mechanism language request process asynchronously standard semantics support include request stop continue override exist action specific domain demonstrate control motion simulate robot approach general could apply domains
present tweet2vec novel method generate general purpose vector representation tweet model learn tweet embeddings use character level cnn lstm encoder decoder train model three million randomly select english language tweet model evaluate use two methods tweet semantic similarity tweet sentiment categorization outperform previous state art task evaluations demonstrate power tweet embeddings generate model various tweet categorization task vector representations generate model generic hence apply variety task though model present paper train english language tweet method present use learn tweet embeddings different languages
application development happen context complex apis reference documentation apis grow tremendously variety complexity volume difficult navigate grow need develop well organize ways access knowledge latent documentation several research efforts deal organization ontology api relate knowledge extensive knowledge engineer work support rigorous qualitative analysis maalej robillard three identify useful taxonomy api knowledge base taxonomy introduce domain independent technique extract knowledge type give api reference documentation system ontocat introduce total nine different feature semantic statistical combinations classify different knowledge type test ontocat python api reference documentation experimental result show effectiveness system open scope probably relate research areas ie user behavior documentation quality etc
paper explore suitability use automatically discover topics mooc discussion forums model students academic abilities rasch model psychometrics popular generative probabilistic model relate latent student skill latent item difficulty observe student item responses within principled unify framework accord scholarly educational theory discover topics regard appropriate measurement items one students participation across discover topics well fit rasch model two topics interpretable subject matter experts educationally meaningful rasch scale topics associate difficulty level could potential benefit curriculum refinement student assessment personalise feedback technical challenge remain discover meaningful topics simultaneously achieve good statistical fit rasch model address challenge combine rasch model non negative matrix factorisation base topic model jointly fit model demonstrate suitability approach quantitative experiment data three coursera moocs qualitative survey result topic interpretability discrete optimisation mooc
first years life infants learn languages environment amaze speed despite large cross cultural variations amount complexity available language input understand simple fact still escape current cognitive linguistic theories recently spectacular progress engineer science notably machine learn wearable technology offer promise revolutionize study cognitive development machine learn offer powerful learn algorithms achieve human like performance many linguistic task wearable sensors capture vast amount data enable reconstruction sensory experience infants natural environment project reverse engineer language development ie build effective system mimic infant achievements appear therefore within reach analyze condition project contribute scientific understand early language development argue instead define sub problem simplify data computational model address full complexity learn situation take input raw sensory signal available infants imply one accessible privacy preserve repositories home data setup widely share two model evaluate different linguistic level benchmark psycholinguist test pass machine humans alike three linguistically psychologically plausible learn architectures scale real data use probabilistic optimization principles machine learn discuss feasibility approach present preliminary result
dlvhex system implement hex semantics integrate answer set program asp arbitrary external source since first release ten years ago significant advancements achieve importantly exploitation properties external source lead efficiency improvements flexibility enhancements language technical improvements system side increase user convenience paper present current status system point important recent enhancements early versions exist literature focus theoretical aspects specific components bird eye view overall system miss order promote system real world applications present applications already successfully realize top dlvhex paper consideration acceptance theory practice logic program
article discuss open scientific challenge understand development evolution speech form commentary moulin frier et al moulin frier et al two thousand and fifteen base analysis mathematical model origins speech form focus assumptions study fundamental question speech form non speech developmental evolutionary scale particular emphasize importance embody self organization well role mechanisms motivation active curiosity drive exploration speech formation finally discuss evolutionary developmental perspective origins speech
social network platforms use data produce users serve better one service platforms provide recommendation service recommendation systems predict future preferences users use past preferences recommendation systems literature various techniques neighborhood base methods machine learn base methods matrix factorization base methods work set well know methods natural language process domain namely word2vec apply recommendation systems domain unlike previous work use word2vec recommendation work use non textual feature check ins recommend venues visit check target users experiment foursquare check dataset use result show use continuous vector space representations items model techniques word2vec promise make recommendations
describe question answer model apply image structure knowledge base model use natural language string automatically assemble neural network collection composable modules parameters modules learn jointly network assembly parameters via reinforcement learn world question answer triple supervision approach term dynamic neural model network achieve state art result benchmark datasets visual structure domains
speech base solutions take center stage growth service industry need cater large number people strata society natural language speech interfaces talk research community yet practice menu base speech solutions thrive typically menu base speech solution user require respond speak close set word prompt system sequence human speech response ivr prompt result completion transaction transaction deem successful speech solution correctly recognize speak utterances user whenever prompt system usual mechanism evaluate performance speech solution extensive test system put actual people use evaluate performance analyze log successful transactions kind evaluation could lead dissatisfy test users especially performance system result poor transaction completion rate negate wizard oz approach adopt evaluation speech system overall kind evaluations expensive proposition term time cost paper propose method evaluate performance speech solution without actually put people use first describe methodology show experimentally use identify performance bottleneck speech solution even system actually use thus save evaluation time expense
speak mechanism obtain high quality subtitle use live broadcast public events rely humans perform actual speak task estimate quality result non trivial organisations rely humans perform actual quality assessment purely automatic methods develop similar problems like machine translation paper try compare several methods bleu ebleu nist meteor meteor pl ter rib match human derive ner metric commonly use speak
work present broad study adaptation neural network acoustic model mean learn hide unit contributions lhuc method linearly combine hide units speaker environment dependent manner use small amount unsupervised adaptation data also extend lhuc speaker adaptive train sit framework lead adaptable dnn acoustic model work speaker dependent speaker independent manner without requirements maintain auxiliary speaker dependent feature extractors introduce significant speaker dependent change dnn structure series experiment four different speech recognition benchmarks ted talk switchboard ami meet aurora4 comprise two hundred and seventy test speakers show lhuc test sit variants result consistent word error rate reductions range five twenty-three relative depend task degree mismatch train test data addition investigate effect amount adaptation data per speaker quality unsupervised adaptation target complementarity adaptation techniques one shoot adaptation extension adapt dnns train sequence discriminative manner
structure sentence represent network vertices word edge indicate syntactic dependencies interestingly cross syntactic dependencies observe infrequent human languages lead question whether scarcity cross languages arise independent specific constraint cross provide statistical evidence suggest case proportion dependency cross sentence wide range languages accurately estimate simple predictor base null hypothesis local probability two dependencies cross give lengths relative error predictor never exceed five average whereas error baseline predictor assume random order word sentence least six time greater result suggest low frequency cross natural languages neither originate hide knowledge language undesirability cross per se mere side effect principle dependency length minimization
word embeddings ubiquitous nlp information retrieval unclear represent word polysemous show multiple word sense reside linear superposition within word embed simple sparse cod recover vectors approximately capture sense success approach apply several embed methods mathematically explain use variant random walk discourse model arora et al two thousand and sixteen novel aspect technique extract word sense accompany one two thousand discourse atoms give succinct description word co occur word sense discourse atoms independent interest make method potentially useful empirical test use verify support theory
turbotax answerxchange social qanda system support users work federal state tax return use two thousand and fifteen data demonstrate content popularity number view per answerxchange question predict reasonable accuracy base attribute question alone also employ probabilistic topic analysis uplift model identify question feature highest impact popularity demonstrate content popularity drive behavioral attribute answerxchange users depend complex interactions search rank algorithms psycholinguistic factor question write style find provide rationale employ popularity predictions guide users formulate better question edit exist ones example start question title question word add detail question increase number view per question similar approach apply promote answerxchange content index google drive organic traffic turbotax
location tag also know geotagging geolocation process assign geographical coordinate input data paper present algorithm location tag textual document approach make use previous work natural language process use state art part speech tagger name entity recognizer find block text may refer locations knowledge base openstreatmap use find list possible locations block finally one location choose block assign distance base score location repeatedly select location block best score test geolocation algorithm wikipedia article topics well define geographical location geotagged article author classification approach achieve median errors low eleven km attainable accuracy limit class size approach achieve 10th percentile error four hundred and ninety metres median error fifty-four kilometres wikipedia dataset use consider five location tag greatest score fifty article assign least one tag within eighty-five kilometres article author assign true location also test approach twitter message tag location message send twitter texts challenge short unstructured often contain word refer location send obtain potentially useful result explain use spark framework data analytics collect process test data general classification base approach location tag may reach upper accuracy limit precision focus approach high accuracy texts show significant potential improvement overall
propose model acoustic space deep neural network dnn class conditional posterior probabilities union low dimensional subspaces end train posteriors use dictionary learn sparse cod sparse representation test posteriors use dictionary enable projection space train data rely fact intrinsic dimension posterior subspaces indeed small matrix posteriors belong class low rank demonstrate low dimensional structure enable enhancement posteriors rectify spurious errors due mismatch condition enhance acoustic model method lead improvements continuous speech recognition task use hybrid dnn hmm hide markov model framework clean noisy condition upto one hundred and fifty-four relative reduction word error rate wer achieve
urban legends genre modern folklore consist stories rare exceptional events plausible enough believe tend propagate inexorably across communities view urban legends represent form sticky deceptive text mark tension credible incredible credible like news article incredible like fairy tale go viral particular focus idea urban legends mimic detail news credible emotional readable like fairy tale catchy memorable use nlp tool provide quantitative analysis prototypical characteristics also lay machine learn experiment show possible recognize urban legend use simple feature
real time speech recognition applications latency important issue develop character level incremental speech recognition isr system respond quickly even speech hypotheses gradually improve speak proceed algorithm employ speech character unidirectional recurrent neural network rnn end end train connectionist temporal classification ctc rnn base character level language model lm output value ctc train rnn character level probabilities process beam search decode rnn lm augment decode provide long term dependency information propose tree base online beam search additional depth prune enable system process infinitely long input speech low latency system respond quickly speech also dictate vocabulary oov word accord pronunciation propose model achieve word error rate wer eight hundred and ninety wall street journal wsj nov ninety-two 20k evaluation set train wsj si two hundred and eighty-four train set
thesis investigate generation new concepts combinations exist concepts language evolve give method combine concepts investigate utility composite concepts language evolution thence utility concept generation
investigate emergence share concepts community language users use multi agent simulation extend result show negate assertions use develop share categories include assertions modify linguistic hedge result show use hedge assertions positively affect emergence share categories two distinct ways firstly use contraction hedge like give better convergence time secondly use expansion hedge quite reduce concept overlap however improvements come cost slower speed development
investigate generation new concepts combinations properties artificial language develop develop new framework conjunctive concept combination framework give semantic ground weight sum approach concept combination see literature implement framework multi agent simulation language evolution show share combination weight emerge expect value variance weight across agents may predict distribution elements conceptual space determine underlie environment together rate agents adopt others concepts rate smaller agents able converge weight lower variance however time take converge steady state distribution weight longer
change someone opinion arguably one important challenge social interaction underlie process prove difficult study hard know someone opinions form whether someone view shift fortunately changemyview active community reddit provide platform users present opinions reason invite others contest acknowledge ensue discussions change original view work study interactions understand mechanisms behind persuasion find persuasive arguments characterize interest pattern interaction dynamics participant entry order degree back forth exchange furthermore compare similar counterarguments opinion show language factor play essential role particular interplay language opinion holder counterargument provide highly predictive cue persuasiveness finally since even favorable set people may persuade investigate problem determine whether someone opinion susceptible change difficult task show stylistic choices opinion express carry predictive power
paper propose novel unsupervised learn method lexical acquisition word relate place visit robots human continuous speech signal address problem learn novel word robot prior knowledge word except primitive acoustic model propose method allow robot effectively use learn word mean self localization task propose method nonparametric bayesian spatial concept acquisition method spcoa integrate generative model self localization unsupervised word segmentation utter sentence via latent variables relate spatial concept implement propose method spcoa sigverse simulation environment turtlebot2 mobile robot real environment conduct experiment evaluate performance spcoa experimental result show spcoa enable robot acquire name place speech sentence also reveal robot could effectively utilize acquire spatial concepts reduce uncertainty self localization
sentiment analysis opinion mine twitter data attract much attention recently one system key feature immediacy communication users easy user friendly fast way consequently people tend express feel freely make twitter ideal source accumulate vast amount opinions towards wide diversity topics amount information offer huge potential harness receive sentiment tendency towards topics however since none invest infinite amount time read tweet automate decision make approach necessary nevertheless exist solutions limit centralize environments thus process thousand tweet sample representative define sentiment polarity towards topic due massive number tweet publish daily paper go one step develop novel method sentiment learn mapreduce framework algorithm exploit hashtags emoticons inside tweet sentiment label proceed classification procedure diverse sentiment type parallel distribute manner moreover utilize bloom filter compact storage size intermediate data boost performance algorithm extensive experimental evaluation prove solution efficient robust scalable confirm quality sentiment identification
categorical compositional distributional semantics model natural language combine statistical vector space model word compositional model grammar formalise model generalise quantifier theory natural language due barwise cooper underlie set compact close category bialgebras start generative grammar formalisation develop abstract categorical compositional semantics instantiate abstract set set relations finite dimensional vector space linear map prove equivalence relational instantiation truth theoretic semantics generalise quantifiers vector space instantiation formalise statistical usages word enable us first time reason quantify phrase sentence compositionally distributional semantics
generate natural language descriptions image challenge task traditional way use convolutional neural network cnn extract image feature follow recurrent neural network rnn generate sentence paper present new model add memory cells gate feed image feature deep neural network intuition enable model memorize much information image feed stage rnn experiment flickr8k flickr30k datasets show model outperform state art model higher bleu score
propose sparsemax new activation function similar traditional softmax able output sparse probabilities derive properties show jacobian efficiently compute enable use network train backpropagation propose new smooth convex loss function sparsemax analogue logistic loss reveal unexpected connection new loss huber classification loss obtain promise empirical result multi label classification problems attention base neural network natural language inference latter achieve similar performance traditional softmax selective compact attention focus
information age bring deluge data much text form insurmountable scope humans incomprehensible structure computers text mine expand field research seek utilize information contain vast document collections general data mine methods base machine learn face challenge scale text data pose need scalable text mine methods thesis propose solution scalable text mine generative model combine sparse computation unify formalization generative text model define bring together research traditions use formally equivalent model ignore parallel developments framework allow use methods develop different process task retrieval classification yield effective solutions across different text mine task sparse computation use invert indices propose inference probabilistic model reduce computational complexity common text mine operations accord sparsity yield probabilistic model scalability modern search engines propose combination provide sparse generative model solution text mine general effective scalable extensive experimentation text classification rank retrieval datasets conduct show propose solution match outperform lead task specific methods effectiveness order magnitude decrease classification time wikipedia article categorization million class develop methods apply two two thousand and fourteen kaggle data mine prize competitions hundred compete team earn first second place
one hot cnn convolutional neural network show effective text categorization johnson zhang two thousand and fifteen view special case general framework jointly train linear model non linear feature generator consist text region embed pool framework explore sophisticate region embed method use long short term memory lstm lstm embed text regions variable possibly large size whereas region size need fix cnn seek effective efficient use lstm purpose supervise semi supervise settings best result obtain combine region embeddings form lstm convolution layer train unlabeled data result indicate task embeddings text regions convey complex concepts useful embeddings single word isolation report performances exceed previous best result four benchmark datasets
individuals social network experience call friendship paradox less popular friends average effect may explain recent find widespread social network media use lead reduce happiness however relation popularity happiness poorly understand friendship paradox necessarily imply happiness paradox individuals less happy friends report first direct observation significant happiness paradox large scale online social network thirty-nine thousand, one hundred and ten twitter users result reveal popular individuals indeed happier majority individuals experience significant happiness paradox magnitude latter effect shape complex interactions individual popularity happiness fact users cluster assortatively level happiness result indicate topology online social network distribution happiness populations widespread psycho social effect affect well billions individuals
automate feature selection important text categorization reduce feature size speed learn process classifiers paper present novel efficient feature selection framework base information theory aim rank feature discriminative capacity classification first revisit two information measure kullback leibler divergence jeffreys divergence binary hypothesis test analyze asymptotic properties relate type type ii errors bayesian classifier introduce new divergence measure call jeffreys multi hypothesis jmh divergence measure multi distribution divergence multi class classification base jmh divergence develop two efficient feature selection methods term maximum discrimination md md chi2 methods text categorization promise result extensive experiment demonstrate effectiveness propose approach
attention mechanisms neural network prove useful problems input output fix dimension often exist feature locally translation invariant would valuable direct model attention previous attentional architectures construct learn feature specifically introduce attentional neural network employ convolution input tokens detect local time invariant long range topical attention feature context dependent way apply architecture problem extreme summarization source code snippets short descriptive function name like summaries use feature model sequentially generate summary marginalize two attention mechanisms one predict next summary token base attention weight input tokens another able copy code token directly summary demonstrate convolutional attention neural network performance ten popular java project show achieve better performance compare previous attentional mechanisms
study problem recognition fingerspell letter sequence american sign language signer independent set fingerspell sequence challenge important recognize use many content word proper nouns technical term previous work show possible achieve almost ninety accuracies fingerspell recognition signer dependent set however realistic signer independent set present challenge due significant variations among signers couple dearth available train data investigate problem approach inspire automatic speech recognition start best perform approach prior work base tandem model segmental conditional random field scrfs feature base deep neural network dnn classifiers letter phonological feature use dnn adaptation find possible bridge large part gap signer dependent signer independent performance use one hundred and fifteen transcribe word adaptation target signer obtain letter accuracies eight hundred and twenty-seven frame level adaptation label six hundred and ninety-seven word label
provide solution elementary science test use instructional materials posit hide structure explain correctness answer give question instructional materials present unify max margin framework learn find hide structure give corpus question answer pair instructional materials use learn answer novel elementary science question evaluation show framework outperform several strong baselines
exploration social conversations address patient need important analytical task many scholarly publications contribute fill knowledge gap area main difficulty remain inability turn contributions pragmatic process pharmaceutical industry leverage order generate insight social media data consider one challenge source information available today due sheer volume noise study base work scott spangler jeffrey kreulen apply identify structure social media extraction topical taxonomy able capture latent knowledge social conversations health relate sit mechanism automatically identify generate taxonomy social conversations develop pressure test use public data media sit focus need cancer patients families moreover novel method generate category label determination optimal number categories present extend scott jeffrey research meaningful way assume reader familiar taxonomies use
graph form vertices weight edge generalize minimum dominate set mds vertex set smallest cardinality sum weight edge outside vertex vertices set equal larger certain threshold value generalize mds problem reduce conventional mds problem limit case edge weight equal threshold value treat generalize mds problem present paper replica symmetric spin glass theory derive set belief propagation equations practical application consider problem extract set sentence best summarize give input text document carry preliminary test statistical physics inspire method automatic text summarization problem
supervise topic model simultaneously model latent topic structure large collections document response variable associate document exist inference methods base variational approximation monte carlo sample often suffer local minimum defect spectral methods apply learn unsupervised topic model latent dirichlet allocation lda provable guarantee paper investigate possibility apply spectral methods recover parameters supervise lda slda first present two stage spectral method recover parameters lda follow power update method recover regression model parameters present single phase spectral algorithm jointly recover topic distribution matrix well regression weight spectral algorithms provably correct computationally efficient prove sample complexity bind algorithm subsequently derive sufficient condition identifiability slda thorough experiment synthetic real world datasets verify theory demonstrate practical effectiveness spectral algorithms fact result large scale review rat dataset demonstrate single phase spectral algorithm alone get comparable even better performance state art methods previous work spectral methods rarely report promise performance
propose two novel techniques stack bottleneck feature minimum generation error train criterion improve performance deep neural network dnn base speech synthesis techniques address relate issue frame frame independence ignorance relationship static dynamic feature within current typical dnn base synthesis frameworks stack bottleneck feature acoustically inform linguistic representation provide efficient way include detail linguistic context input minimum generation error train criterion minimise overall output trajectory error across utterance rather minimise error per frame independently thus take account interaction static dynamic feature two techniques easily combine improve performance present objective subjective result demonstrate effectiveness propose techniques subjective result show combine two techniques lead significantly natural synthetic speech conventional dnn long short term memory lstm recurrent neural network rnn systems
probabilistic linear discriminant analysis plda become state art method model vector space speaker recognition task however performance degradation observe enrollment data size differ one speaker another paper present solution problem introduce new plda score normalization technique normalization parameters derive blind way unlike traditional textitzt norm extra development data require moreover propose method show optimal term detection cost function experiment conduct nist sre two thousand and fourteen database demonstrate improve accuracy mix enrollment number condition
paper describe recent advancements make ibm vector speaker recognition system conversational speech particular identify key techniques contribute significant improvements performance system quantify contributions techniques include one nearest neighbor discriminant analysis nda approach formulate alleviate limitations associate conventional linear discriminant analysis lda assume gaussian class conditional distributions two application speaker channel adapt feature derive automatic speech recognition asr system speaker recognition three use deep neural network dnn acoustic model large number output units 10k senones compute frame level soft alignments require vector estimation process evaluate techniques nist two thousand and ten speaker recognition evaluation sre extend core condition involve telephone microphone trials experimental result indicate one nda effective thirty-five relative improvement term ever traditional parametric lda speaker recognition two compare raw acoustic feature eg mfccs asr speaker adapt feature provide gain speaker recognition performance three increase number output units dnn acoustic model ie increase senone set size 2k 10k provide consistent improvements performance example thirty-seven fifty-seven relative ever gain baseline gmm vector system knowledge result report paper represent best performances publish date nist sre two thousand and ten extend core task
authorship attribution refer task automatically determine author base give sample text problem long history wide range application build author profile use language model one successful methods automate task new language model methods base neural network alleviate curse dimensionality usually outperform conventional n gram methods however much research apply authorship attribution paper present novel setup neural network language model nnlm apply database text sample different author investigate nnlm perform task moderate author set size relatively limit train test data topics text sample affect accuracy nnlm achieve nearly twenty-five reduction perplexity measurement fitness train language model test data give five random test sentence also increase author classification accuracy three hundred and forty-three average compare n gram methods use srilm tool open source implementation methodology freely available https githubcom zge authorship attribution
many important form data store digitally xml format errors occur textual content data field xml fix errors manually time consume expensive especially large amount data increase interest research development use automate techniques assist data clean electronic dictionaries important form data frequently store xml format frequently errors introduce mixture manual typographical entry errors optical character recognition errors paper describe methods flag statistical anomalies likely errors electronic dictionaries store xml format describe six systems base different source information systems detect errors use various signal data include uncommon character text length character base language model word base language model tie field length ratios tie field transliteration model four systems detect errors base expectations automatically infer content within elements single field type call single field systems two systems detect errors base correspondence expectations automatically infer content within elements multiple relate field type call tie field systems system provide intuitive analysis type error successful detect finally describe two larger scale evaluations use crowdsourcing amazon mechanical turk platform use annotations domain expert evaluations consistently show systems useful improve efficiency errors xml electronic dictionaries detect
paper present method detect mispronunciations aim improve computer assist language learn call tool use foreign language learners algorithm base principle component analysis pca hierarchical successive step refine estimate classify test word either mispronounce correct preprocessing detection like normalization time scale modification implement guarantee uniformity feature vectors input detection system performance use various feature include spectrograms mel frequency cepstral coefficients mfccs compare evaluate best result obtain use mfccs achieve ninety-nine accuracy word verification ninety-three native non native classification compare hide markov model hmms use pervasively recognition application particular approach computational efficient effective train data limit
study problem structure prediction test time budget constraints propose novel approach applicable wide range structure prediction problems computer vision natural language process approach seek adaptively generate computationally costly feature test time order reduce computational cost prediction maintain prediction performance show train adaptive feature generation system reduce series structure learn problems result efficient train use exist structure learn algorithms framework provide theoretical justification several exist heuristic approach find literature evaluate propose adaptive system two structure prediction task optical character recognition ocr dependency parse show strong performance reduction feature cost without degrade accuracy
paper describe new kind knowledge representation mine system call semantic knowledge graph heart semantic knowledge graph leverage invert index along complementary uninverted index represent nod term edge document within intersect post list multiple term nod provide layer indirection pair nod correspond edge enable edge materialize dynamically underlie corpus statistics result combination nod edge nod materialize score reveal latent relationships nod provide numerous benefit knowledge graph build automatically real world corpus data new nod along combine edge instantly materialize arbitrary combination preexist nod use set operations full model semantic relationships entities within domain represent dynamically traverse use highly compact representation graph system widespread applications areas diverse knowledge model reason natural language process anomaly detection data cleanse semantic search analytics data classification root analysis recommendations systems main contribution paper introduction novel system semantic knowledge graph able dynamically discover score interest relationships arbitrary combination entities word phrase extract concepts dynamically materialize nod edge compact graphical representation build automatically corpus data representative knowledge domain
hierarchy common effective way organize data represent relationships different level abstraction however hierarchical data dependencies difficulties estimation separable model distinguish entities hierarchy extract separable model hierarchical entities require us take relative position account consider different type dependencies hierarchy paper present investigation effect separability text base entity classification argue hierarchical classification separation property establish entities layer also different layer main find follow first analyse importance separability data representation task classification base introduce strong separation principle optimize expect effectiveness classifiers decision base separation property second present hierarchical significant word language model hswlm capture essential feature hierarchical entities accord relative position hierarchy result horizontally vertically separable model third validate claim real world data demonstrate hswlm improve accuracy classification provide transferable model time although discussions paper focus classification problem model applicable information access task data map hierarchical structure
paper report performances shallow word level convolutional neural network cnn earlier work two thousand and fifteen eight datasets relatively large train data use test deep character level cnn conneau et al two thousand and sixteen find follow shallow word level cnns achieve better error rat error rat report conneau et al though result interpret consideration due unique pre process conneau et al shallow word level cnn use parameters therefore require storage deep character level cnn however shallow word level cnn compute much faster
social media tend rife rumour new report release piecemeal break news interestingly one mine multiple reactions express social media users situations explore stance towards rumour ultimately enable flag highly dispute rumour potentially false work set develop automate supervise classifier use multi task learn classify stance express individual tweet rumourous conversation either support deny question rumour use classifier base gaussian process explore effectiveness two datasets different characteristics vary distributions stances show approach consistently outperform competitive baseline classifiers classifier especially effective estimate distribution different type stance associate give rumour set forth desire characteristic rumour track system warn ordinary users twitter professional news practitioners rumour rebut
language change complex social phenomenon reveal pathways communication sociocultural influence language change long topic study sociolinguistics traditional linguistic research methods rely circumstantial evidence estimate direction change differences older younger speakers paper use data set several million twitter users track language change progress first show language change view form social influence observe complex contagion phonetic spell netspeak abbreviations eg lol older dialect markers speak language next test whether specific type social network connections influential others use parametric hawk process model find tie strength play important role densely embed social tie significantly better conduits linguistic influence geographic locality appear play limit role find relatively little evidence support hypothesis individuals influence geographically local social tie even usage geographical dialect markers
paper advance recently propose uncertainty decode scheme dnn hmm deep neural network hide markov model hybrid systems numerical sample concept average dnn output produce finite set feature sample draw probabilistic distortion model approximate posterior likelihoods context dependent hmm state main innovation propose weight dnn output average base minimum classification error criterion apply probabilistic distortion model spatial diffuseness feature experimental evaluation perform eight channel reverb challenge task use dnn hmm hybrid system multichannel front end signal enhancement show recognition accuracy dnn hmm hybrid system improve incorporate uncertainty decode base random sample propose weight dnn output average reduce word error rate score
variety application domains content recommend users associate text include research paper movies associate plot summaries news article blog post etc recommendation approach base latent factor model extend naturally leverage text employ explicit map text factor enable recommendations new unseen content may generalize better since factor items produce compactly parametrized model previous work use topic model average word embeddings map paper present method leverage deep recurrent neural network encode text sequence latent vector specifically gate recurrent units grus train end end collaborative filter task task scientific paper recommendation yield model significantly higher accuracy cold start scenarios beat previous state art ignore word order performance improve multi task learn text encoder network train combination content recommendation item metadata prediction regularize collaborative filter model ameliorate problem sparsity observe rat matrix
paper present simple end end model speech recognition combine convolutional network base acoustic model graph decode train output letter transcribe speech without need force alignment phonemes introduce automatic segmentation criterion train sequence annotation without alignment par ctc simpler show competitive result word error rate librispeech corpus mfcc feature promise result raw waveform
network model language provide way link cognitive process structure connectivity language however one shortcoming current approach focus one type linguistic relationship time miss complex multi relational nature language work overcome limitation model mental lexicon english speak toddlers multiplex lexical network ie multi layer network n529 word nod connect accord four type relationships free associations ii feature share iii co occurrence iv phonological similarity provide analysis topology result multiplex proceed evaluate single layer well full multiplex structure ability predict empirically observe age acquisition data english speak toddlers find emerge multiplex network topology important proxy cognitive process acquisition capable capture emergent lexicon structure fact show multiplex topology fundamentally powerful individual layer predict order word acquire furthermore multiplex analysis allow quantification distinct phase lexical acquisition early learners initially multiplex layer contribute word learn month twenty-three free associations take lead drive word acquisition
recurrent neural network rnn base character level language model clms extremely useful model vocabulary word nature however performance generally much worse word level language model wlms since clms need consider longer history tokens properly predict next one address problem propose hierarchical rnn architectures consist multiple modules different timescales despite multi timescale structure input output layer operate character level clock allow exist rnn clm train approach directly applicable without modifications clm model show better perplexity kneser ney kn five gram wlms one billion word benchmark two parameters also present real time character level end end speech recognition examples wall street journal wsj corpus replace traditional mono clock rnn clms propose model result better recognition accuracies even though number parameters reduce thirty
article propose method refine cluster result obtain nonnegative matrix factorization nmf technique impose consistency constraints final label data research community focus effort initialization optimization part method without pay attention final cluster assignments propose game theoretic framework object cluster represent player choose cluster membership information obtain nmf use initialize strategy space players weight graph use model interactions among players interactions allow players choose cluster coherent cluster choose similar players property guarantee nmf since produce soft cluster data result common benchmarks show model able improve performances many nmf formulations
analyse text corpora time reveal trend beliefs interest sentiment topic focus view express artificial intelligence ai new york time thirty year period general interest awareness discussion ai wax wan since field found one thousand, nine hundred and fifty-six present set measure capture level engagement measure pessimism optimism prevalence specific hop concern topics link discussions ai decades find discussion ai increase sharply since two thousand and nine discussions consistently optimistic pessimistic however examine specific concern find worry loss control ai ethical concern ai negative impact ai work grow recent years also find hop ai healthcare education increase time
present neural encoder decoder model convert image presentational markup base scalable coarse fine attention mechanism method evaluate context image latex generation introduce new dataset real world render mathematical expressions pair latex markup show unlike neural ocr techniques use ctc base model attention base approach tackle non standard ocr task approach outperform classical mathematical ocr systems large margin domain render data pretraining also perform well domain handwritten data reduce inference complexity associate attention base approach introduce new coarse fine attention layer select support region apply attention
paper propose improve visual question answer vqa structure representations scene content question key challenge vqa require joint reason visual text domains predominant cnn lstm base approach vqa limit monolithic vector representations largely ignore structure scene form question cnn feature vectors effectively capture situations simple multiple object instance lstms process question series word reflect true complexity language structure instead propose build graph scene object question word describe deep neural network exploit structure representations show significant benefit sequential process lstms overall efficacy approach demonstrate significant improvements state art seven hundred and twelve seven hundred and forty-four accuracy abstract scenes multiple choice benchmark three hundred and forty-seven three hundred and ninety-one accuracy pair balance scenes ie image fine grain differences opposite yes answer question
aspect base opinion mine widely apply review data aggregate summarize opinions product current state art achieve latent dirichlet allocation lda base model although social media data like tweet lade opinions dirty nature natural language discourage researchers apply lda base opinion model product review mine tweet often informal unstructured lack label data categories rat make challenge product opinion mine paper propose lda base opinion model name twitter opinion topic model totm opinion mine sentiment analysis totm leverage hashtags mention emoticons strong sentiment word present tweet discovery process improve opinion prediction model target opinion interaction directly thus discover target specific opinion word neglect exist approach moreover propose new formulation incorporate sentiment prior information topic model utilize exist public sentiment lexicon novel learn update data conduct experiment nine million tweet electronic products demonstrate improve performance totm quantitative evaluations qualitative analysis show aspect base opinion analysis massive volume tweet provide useful opinions products
compare policy differences across institutions embed representations entire legal corpus institution vocabulary share across corpora continuous vector space apply method gov2vec supreme court opinions presidential action official summaries congressional bill model discern meaningful differences government branch also learn representations fine grain word source individual presidents two year congresses similarities learn representations congresses time sit presidents negatively correlate bill veto rate temporal order presidents congresses implicitly learn text result vectors answer question obama 113th house differ address climate change vary environmental economic perspectives work illustrate vector arithmetic base investigations complex relationships word source base texts extend create comprehensive legal semantic map
dirichlet process extension pitman yor process stochastic process take probability distributions parameter process stack form hierarchical nonparametric bayesian model article present efficient methods use process hierarchical context apply latent variable model text analytics particular propose general framework design bayesian model call topic model computer science community propose specific nonparametric bayesian topic model model text social media focus tweet post twitter article due ease access find nonparametric model perform better exist parametric model goodness fit real world applications
twitter data extremely noisy tweet short unstructured informal language challenge current topic model hand tweet accompany extra information authorship hashtags user follower network exploit additional information propose twitter network tn topic model jointly model text social network full bayesian nonparametric way tn topic model employ hierarchical poisson dirichlet process pdp text model gaussian process random function model social network model show tn topic model significantly outperform several exist nonparametric model due flexibility moreover tn topic model enable additional informative inference author interest hashtag analysis well lead applications author recommendation automatic topic label hashtag suggestion note general inference framework readily apply topic model embed pdp nod
neural machine translation nmt end end learn approach automate translation potential overcome many weaknesses conventional phrase base translation systems unfortunately nmt systems know computationally expensive train translation inference also nmt systems difficulty rare word issue hinder nmt use practical deployments service accuracy speed essential work present gnmt google neural machine translation system attempt address many issue model consist deep lstm network eight encoder eight decoder layer use attention residual connections improve parallelism therefore decrease train time attention mechanism connect bottom layer decoder top layer encoder accelerate final translation speed employ low precision arithmetic inference computations improve handle rare word divide word limit set common sub word units wordpieces input output method provide good balance flexibility character delimit model efficiency word delimit model naturally handle translation rare word ultimately improve overall accuracy system beam search technique employ length normalization procedure use coverage penalty encourage generation output sentence likely cover word source sentence wmt fourteen english french english german benchmarks gnmt achieve competitive result state art use human side side evaluation set isolate simple sentence reduce translation errors average sixty compare google phrase base production system
introduce online neural sequence sequence model learn alternate encode decode segment input read independently track encode decode representations algorithm permit exact polynomial marginalization latent segmentation train decode beam search employ find best alignment path together predict output sequence model tackle bottleneck vanilla encoder decoders read memorize entire input sequence fix length hide state produce output different previous attentive model instead treat attention weight output deterministic function model assign attention weight sequential latent variable marginalize permit online generation experiment abstractive sentence summarization morphological inflection show significant performance gain baseline encoder decoders
research multilingual speech recognition remain attractive yet challenge recent study focus learn share structure multi task paradigm particular feature share structure approach find effective improve performance individual language however approach useful deploy system support one language true multilingual scenario multiple languages allow performance significantly reduce due competition among languages decode space paper present multi task recurrent model involve multilingual speech recognition asr component language recognition lr component asr component inform language information lr component lead language aware recognition test approach english chinese bilingual recognition task result show propose multi task recurrent model improve performance multilingual recognition systems
plda popular normalization approach vector model deliver state art performance speaker verification however plda train require large amount label development data highly expensive case present cheap plda train approach assume speakers session easily separate speakers different sessions simply different result weak label fully accurate cheap lead weak plda train experimental result real life large scale telephony customer service achieve demonstrate weak train offer good performance human label data limit interestingly weak train employ discriminative adaptation approach efficient prevail unsupervised method human label data insufficient
infer topics overwhelm amount short texts become critical challenge task many content analysis task content character user interest profile emerge topic detect exist methods probabilistic latent semantic analysis plsa latent dirichlet allocation lda solve prob lem well since limit word co occurrence information available short texts paper study incorporate external word correlation knowledge short texts improve coherence topic model base recent result word embeddings learn se mantically representations word large corpus introduce novel method embed base topic model etm learn latent topics short texts etm solve problem limit word co occurrence information aggregate short texts long pseudo texts also utilize markov random field regularize model give correlate word better chance put topic experiment real world datasets validate effectiveness model compare state art model
systems base artificial neural network anns achieve state art result many natural language process task although anns require manually engineer feature anns many hyperparameters optimize choice hyperparameters significantly impact model performances however ann hyperparameters typically choose manual grid random search either require expert experience computationally expensive recent approach base bayesian optimization use gaussian process gps systematic way automatically pinpoint optimal near optimal machine learn hyperparameters use previously publish ann model yield state art result dialog act classification demonstrate optimize hyperparameters use gp improve result reduce computational time factor four compare random search therefore useful technique tune ann model yield best performances natural language process task
recurrent neural network rnns show clear superiority sequence model particularly ones gate units long short term memory lstm gate recurrent unit gru however dynamic properties behind remarkable performance remain unclear many applications eg automatic speech recognition asr paper employ visualization techniques study behavior lstm gru perform speech recognition task experiment show interest pattern gate memory inspire simple yet effective modifications network structure report two modifications one lazy cell update lstm two shortcut connections residual learn modifications lead comprehensible powerful network
recently end end memory network show promise result question answer task encode past facts explicit memory perform reason ability make multiple computational step memory however memory network conduct reason sentence level memory output coarse semantic vectors take attention mechanism focus word may lead model lose detail information especially answer rare unknown word paper propose novel hierarchical memory network dub hmn first encode past facts sentence level memory word level memory respectively k max pool exploit follow reason module sentence level memory sample k relevant sentence question fee sentence attention mechanism word level memory focus word select sentence finally prediction jointly learn output sentence level reason module word level attention mechanism experimental result demonstrate approach successfully conduct answer selection unknown word achieve better performance memory network
academic researchers often need face large collection research paper literature problem may even worse postgraduate students new field may know start address problem develop online catalog research paper paper automatically categorize topic model catalog contain seven thousand, seven hundred and nineteen paper proceed two artificial intelligence conferences two thousand two thousand and fifteen rather commonly use latent dirichlet allocation use recently propose method call hierarchical latent tree analysis topic model result topic model contain hierarchy topics users browse topics top level bottom level topic model contain manageable number general topics top level allow thousands fine grain topics bottom level also detect topics emerge recently
present novel semi supervise approach sequence transduction apply semantic parse unsupervised component base generative model latent sentence generate unpaired logical form apply method number semantic parse task focus domains limit access label train data extend datasets synthetically generate logical form
text network refer data type vertex associate text document relationship document represent edge proliferation text network hyperlinked webpages academic citation network lead increase demand quickly develop general sense new text network namely text network exploration paper address problem text network exploration construct heterogeneous web topics allow people investigate text network associate word level document level achieve probabilistic generative model text link propose three different relationships heterogeneous topic web quantify also develop prototype demo system name topicatlas exhibit heterogeneous topic web demonstrate system facilitate task text network exploration extensive qualitative analyse include verify effectiveness heterogeneous topic web besides validate model real life text network show preserve good performance objective evaluation metrics
sentiment analysis sa action research area digital age rapid constant growth online social media sit service increase amount textual data statuses comment review etc available application automatic sa rise however research work sa natural language process nlp base english language despite sixth widely speak language world bangla still large standard dataset recent research work bangla fail produce result comparable work do others reusable step stone future researchers progress field therefore first try provide textual dataset include bangla romanize bangla texts well substantial post process multiple validate ready use sa experiment test dataset deep recurrent model specifically long short term memory lstm use two type loss function binary crossentropy categorical crossentropy also experimental pre train use data one validation pre train vice versa lastly document result along analysis promise
propose application semi supervise learn method improve performance acoustic model automatic speech recognition base deep neural net work oppose unsupervised initialisation follow supervise fine tune method take advantage unlabelled label data simultaneously mini batch stochastic gradient descent test method vary proportion label vs unlabelled observations frame base phoneme classification timit database experiment show method outperform standard supervise train equal amount label data provide competitive error rat compare state art graph base semi supervise learn techniques
paper neural network base real time speech recognition sr system develop use fpga low power operation implement system employ two recurrent neural network rnns one speech character rnn acoustic model character level language model lm system also employ statistical word level lm improve recognition accuracy result character level lm word level lm combine use fairly simple n best search algorithm instead hide markov model hmm base network rnns implement use massively parallel process elements pes low latency high throughput weight quantize six bits store chip memory fpga propose algorithm implement xilinx xc7z045 system operate much faster real time
practically unlimited amount natural language data available still recent work text comprehension focus datasets small relative current compute possibilities article make case community move larger data step direction propose booktest new dataset similar popular children book test cbt however sixty time larger show train new data improve accuracy attention sum reader model original cbt test data much larger margin many recent attempt improve model architecture one version dataset ensemble even exceed human baseline provide facebook show human study still space improvement
natural man make disasters humanitarian response organizations look useful information support decision make process social media platforms twitter consider vital source useful information disaster response management despite advance natural language process techniques process short informal twitter message challenge task paper propose use deep neural network dnn address two type information need response organizations one identify informative tweet two classify topical class dnns use distribute representation word learn representation well higher level feature automatically classification task propose new online algorithm base stochastic gradient descent train dnns online fashion disaster situations test model use crisis relate real world twitter dataset
visual question answer vqa recent problem computer vision natural language process garner large amount interest deep learn computer vision natural language process communities vqa algorithm need answer text base question image since release first vqa dataset two thousand and fourteen additional datasets release many algorithms propose review critically examine current state vqa term problem formulation exist datasets evaluation metrics algorithms particular discuss limitations current datasets regard ability properly train assess vqa algorithms exhaustively review exist algorithms vqa finally discuss possible future directions vqa image understand research
demonstrate conversational products recommendation agent system show combine research personalize recommendation systems research dialogue systems build virtual sales agent base new deep learn technologies develop virtual agent capable learn interact users answer user question next question ask recommend chat human user normally descent conversational agent particular domain require tens thousands hand label conversational data hand write rule major barrier launch conversation agent new domain explore demonstrate effectiveness learn solution even hand write rule hand label train data
one essential task information extraction medical corpus drug name recognition compare text source come domains medical text special unique characteristics addition medical text mine pose challenge eg unstructured text fast grow new term addition wide range name variation drug mine even challenge due lack label dataset source external knowledge well multiple token representations single drug name common real application set although many approach propose overwhelm task problems remain poor f score performance less seventy-five paper present new treatment data representation techniques overcome challenge propose three data representation techniques base characteristics word distribution word similarities result word embed train first technique evaluate standard nn model ie mlp multi layer perceptrons second technique involve two deep network classifiers ie dbn deep belief network sae stack denoising encoders third technique represent sentence sequence evaluate recurrent nn model ie lstm long short term memory extract drug name entities third technique give best f score performance compare state art average f score eight thousand, six hundred and forty-five
neural sequence model widely use model time series data equally ubiquitous usage beam search bs approximate inference algorithm decode output sequence model bs explore search space greedy leave right fashion retain top b candidates result sequence differ slightly produce list nearly identical sequence computationally wasteful also typically fail capture inherent ambiguity complex ai task overcome problem propose diverse beam search dbs alternative bs decode list diverse output optimize diversity augment objective observe method find better top one solutions control exploration exploitation search space imply dbs better search algorithm moreover gain achieve minimal computational memory head compare beam search demonstrate broad applicability method present result image caption machine translation visual question generation use standard quantitative metrics qualitative human study study role diversity image ground language generation task complexity image change observe method consistently outperform bs previously propose techniques diverse decode neural sequence model
thesis report study methods solve visual question answer vqa task deep learn framework preliminary step explore long short term memory lstm network use natural language process nlp tackle question answer text base modify previous model accept image input addition question purpose explore vgg sixteen k cnn convolutional neural network extract visual feature image merge word embed sentence embed question predict answer work successfully submit visual question answer challenge two thousand and sixteen achieve five thousand, three hundred and sixty-two accuracy test dataset develop software follow best program practice python code style provide consistent baseline keras different configurations
difficult train personalize task orient dialogue system data collect individual often insufficient personalize dialogue systems train small dataset overfit make difficult adapt different user need one way solve problem consider collection multiple users data source domain individual user data target domain perform transfer learn source target domain follow idea propose petalpersonalized task orient dialogue transfer learn framework base pomdp learn personalize dialogue system system first learn common dialogue knowledge source domain adapt knowledge target user framework avoid negative transfer problem consider differences source target users policy personalize pomdp learn choose different action appropriately different users experimental result real world coffee shop data simulation data show personalize dialogue system choose different optimal action different users thus effectively improve dialogue quality personalize set
paper investigate problem network embed aim learn low dimensional vector representation nod network exist network embed methods rely solely network structure ie linkage relationships nod ignore rich content information associate common real world network beneficial describe characteristics node paper propose content enhance network embed cene capable jointly leverage network structure content information approach integrate text model structure model general framework treat content information special kind node experiment several real world net work application node classification show model outperform exist network embed methods demonstrate merit content information joint learn
present latent sequence decompositions lsd framework lsd decompose sequence variable lengthed output units function input sequence output sequence present train algorithm sample valid extensions approximate decode algorithm experiment wall street journal speech recognition task lsd model achieve one hundred and twenty-nine wer compare character baseline one hundred and forty-eight wer combine convolutional network encoder achieve ninety-six wer
term weight metrics assign weight term order discriminate important term less crucial ones due characteristic metrics attract grow attention text classification recently sentiment analysis use weight give metrics could lead accurate document representation may improve performance classification previous study focus propose compare different weight metrics two class document level sentiment analysis study propose analyse result give metric order find characteristics good bad weight metrics therefore present empirical study fifteen global supervise weight metrics four local weight metrics adopt information retrieval also give analysis understand behavior metric observe analyse metric distribute term deduce characteristics may distinguish good bad metrics evaluation do use support vector machine three different datasets twitter restaurant laptop review
modern robotics applications involve human robot interaction require robots able communicate humans seamlessly effectively natural language provide flexible efficient medium robots exchange information human partner significant advancements make develop robots capable interpret free form instructions less attention devote endow robots ability generate natural language propose navigational guide model enable robots generate natural language instructions allow humans navigate priori unknown environments first decide information share user accord preferences use policy train human demonstrations via inverse reinforcement learn translate information natural language instruction use neural sequence sequence model learn generate free form instructions natural language corpora evaluate method benchmark route instruction dataset achieve bleu score seven thousand, two hundred and eighteen compare human generate reference instructions additionally conduct navigation experiment human participants demonstrate method generate instructions people follow accurately easily produce humans
several research group show correlate fmri responses mean present stimuli paper present new methods natural language annotation available description stimulus study fmri data gather subject watch episode bbcs sherlock one learn bidirectional mappings fmri responses natural language representations show leverage data multiple subject watch movie improve accuracy mappings allow us succeed scene classification task seventy-two accuracy random guess would give four scene rank task average rank top four random guess would give fifty key ingredients use share response model srm variant srm ica two three aggregate fmri data multiple subject show superior standard pca produce low dimensional representations task paper b sentence embed technique adapt natural language process nlp literature four produce semantic vector representation annotations c use previous timestep information featurization predictor data
speech translation always give source text audio input wait system give translate output desire form paper present acoustic dialect decoder add voice voice ear piece translation device introduce survey recent advance make field speech engineer employ add particularly focus three major process step recognition translation synthesis tackle problem machine understand natural language design recognition unit source audio text translation unit source language text target language text synthesis unit target language text target language speech speech surround record recognition unit present ear piece translation start soon one sentence successfully read way hope give translate output input read recognition unit use hide markov model hmms base tool kit htk hybrid rnn systems gate memory cells synthesis unit hmm base speech synthesis system hts system initially build english tamil translation device
paper present deep learn architecture semantic decoder component statistical speak dialogue system slot fill dialogue semantic decoder predict dialogue act set slot value pair set n best hypotheses return automatic speech recognition current model speak language understand assume word align semantic annotations sequence taggers ii delexicalisation map input word domain specific concepts use heuristics try capture morphological variation scale domains language variation eg morphology synonyms paraphrase work semantic decoder train use unaligned semantic annotations use distribute semantic representation learn overcome limitations explicit delexicalisation propose architecture use convolutional neural network sentence representation long short term memory network context representation result present publicly available dstc2 corpus car corpus similar dstc2 significantly higher word error rate wer
present civique system emergency detection urban areas monitor micro blog like tweet system detect emergency relate events classify appropriate categories like fire accident earthquake etc demonstrate ideas classify twitter post real time visualize ongoing event map interface alert users options contact relevant authorities online offline evaluate classifiers step ie emergency detection categorization obtain f score exceed seventy ninety respectively demonstrate civique use web interface android application realtime show use tweet detection visualization
consider multi class classification predictor hierarchical structure allow large number label train test time predictive power model heavily depend structure tree although past work show learn tree structure expect feature vectors remain static provide novel algorithm simultaneously perform representation learn input data learn hierarchi cal predictor approach optimize objec tive function favor balance easily separable multi way node partition theoret ically analyze objective show give rise boost style property bind classification error next show extend algorithm conditional density estimation empirically validate variants al gorithm text classification language mod eling respectively show compare favorably common baselines term accu racy run time
conventional deep neural network dnn speech acoustic model rely gaussian mixture model gmm hide markov model hmm obtain binary class label target dnn train subword class speech recognition systems correspond context dependent tie state senones present work address limitations gmm hmm senone alignments dnn train hypothesize senone probabilities obtain dnn train binary label provide accurate target learn better acoustic model however dnn output bear inaccuracies exhibit high dimensional unstructured noise whereas informative components structure low dimensional exploit principle component analysis pca sparse cod characterize senone subspaces enhance probabilities obtain low rank sparse reconstructions use soft target dnn acoustic model also enable train untranscribed data experiment conduct ami corpus show forty-six relative reduction word error rate
work bayesian approach speaker normalization propose compensate degradation performance speaker independent speech recognition system speaker normalization method propose herein use technique vocal tract length normalization vtln vtln parameters estimate use novel bayesian approach utilize gibbs sampler special type markov chain monte carlo method additionally hyperparameters estimate use maximum likelihood approach model use assume human vocal tract model tube uniform cross section capture variation length vocal tract different speakers effectively linear model use literature work also investigate different methods like minimization mean square error mse mean absolute error mae estimation vtln parameters single pass two pass approach use build vtln base speech recognizer experimental result recognition vowels hindi phrase medium vocabulary indicate bayesian method improve performance considerable margin
assist text input techniques save time effort improve text quality paper investigate ground conditional extensions standard neural language model bring improvements task word prediction completion extensions incorporate structure knowledge base numerical value text context use predict next word automate evaluation clinical dataset show extend model significantly outperform standard model best system use condition ground orthogonal benefit word prediction list five suggestions improve recall two thousand, five hundred and three seven thousand, one hundred and twenty-eight word completion improve keystroke save three thousand, four hundred and thirty-five four thousand, four hundred and eighty-one theoretical bind dataset five thousand, eight hundred and seventy-eight also perform qualitative investigation model lower perplexity occasionally fare better task find test time number influence document level individual word probabilities
hypothesis test important cognitive process support human reason paper introduce computational hypothesis test approach base memory augment neural network approach involve hypothesis test loop reconsider progressively refine previously form hypothesis order generate new hypotheses test apply propose approach language comprehension task use neural semantic encoders nse nse model achieve state art result show absolute improvement twelve twenty-six accuracy previous result obtain single ensemble systems standard machine comprehension benchmarks children book test cbt wdw news article datasets
answer open end question essential capability intelligent agent one interest recent open end question answer challenge visual question answer vqa attempt evaluate system visual understand answer natural language question image exist many approach vqa majority exhibit deeper semantic understand candidate answer produce study importance generate plausible answer give question introduce novel task answer proposal give open end question system generate rank list candidate answer inform semantics question experiment various model include neural generative model well semantic graph match one provide intrinsic extrinsic evaluations task answer proposal show best model learn propose plausible answer high recall perform competitively solutions vqa
recent work discriminative segmental model show achieve competitive speech recognition performance use feature base deep neural frame classifiers however segmental model challenge train standard frame base approach segmental model successfully train end end lack understand train different settings different losses investigate model class base recent successful approach consist linear model combine segmental feature base lstm frame classifier similarly hybrid hmm neural network model segmental model class train two stag frame classifier train follow linear segmental model weight train end end joint train frame classifier linear weight end end fine tune two stage train study segmental model train end end hinge loss log loss latent hinge loss marginal log loss consider several losses case train alignments available well find general marginal log loss provide consistent strong performance without require grind truth alignments also find train dropout important obtain good performance end end train finally best result typically obtain combination two stage train fine tune
recent years traditional cybersecurity safeguard prove ineffective insider threats famous case sensitive information leak cause insiders include wikileaks release diplomatic cable edward snowden incident greatly harm yous government relationship governments citizens data leak prevention dlp solution detect prevent information leak within organization network however state art dlp detection model able detect limit type sensitive information research field hinder due lack available sensitive texts many researchers focus document base detection artificially label confidential document security label assign entire document reality portion document sensitive type whole document base security label increase chance prevent authorize users access non sensitive information within sensitive document paper introduce automate classification enable security similarity ace new innovative detection model penetrate complexity big text security classification detection analyze ace system construct novel dataset contain formerly classify paragraph diplomatic cable make public wikileaks organization knowledge paper first analyze dataset contain actual formerly sensitive information annotate paragraph granularity
break news lead situations fast pace report social media produce kinds update relate news stories albeit caveat early update tend rumour ie information unverified status time post flag information unverified helpful avoid spread information may turn false detection rumour also fee rumour track system ultimately determine veracity paper introduce novel approach rumour detection learn sequential dynamics report break news social media detect rumour new stories use twitter datasets collect five break news stories experiment conditional random field sequential classifier leverage context learn event rumour detection compare state art rumour detection system well baselines contrast exist work classifier need observe tweet query piece information deem rumour instead detect rumour tweet alone exploit context learn event classifier achieve competitive performance beat state art classifier rely query tweet improve precision recall well outperform best baseline nearly forty improvement term f1 score scale diversity experiment reinforce generalisability classifier
special issue dedicate get better picture relationships computational linguistics cognitive science specifically raise two question potential contribution computational language model cognitive science conversely influence cognitive science contemporary computational linguistics
mean call holy grail variety scientific discipline range linguistics philosophy psychology neurosciences field artifical intelligence ai much part list development sophisticate natural language semantics sine qua non achieve level intelligence comparable humans embodiment theories cognitive science hold human semantic representation depend sensori motor experience abundant evidence human mean representation ground perception physical reality lead conclusion mean must depend fusion multiple perceptual modalities despite ai research general subdisciplines computational linguistics computer vision particular focus primarily task involve single modality propose virtual embodiment alternative long term strategy ai research multi modal nature allow kind scalability require develop field coherently incrementally ethically responsible fashion
vector representations word herald transformational approach classical problems nlp popular example word2vec however single vector suffice model polysemous nature many frequent word ie word multiple mean paper propose three fold approach unsupervised polysemy model context representations b sense induction disambiguation c lexeme word sense pair representations key feature work find sentence contain target word well represent low rank subspace instead point vector space show subspaces associate particular sense target word tend intersect line one dimensional subspace use disambiguate sense use cluster algorithm harness grassmannian geometry representations disambiguation algorithm call k grassmeans lead procedure label different sense target word corpus yield lexeme vector representations unsupervised manner start large wikipedia corpus english apart several prototypical target wordsense examples host empirical study intuit justify various geometric representations validate algorithms standard sense induction disambiguation datasets present new state art result
multi hop inference necessary machine learn systems successfully solve task recognise textual entailment machine read work demonstrate effectiveness adaptive computation learn number inference step require examples different complexity learn correct number inference step difficult introduce first model involve adaptive computation time provide small performance benefit top similar model without adaptive component well enable considerable insight reason process model
gang affiliate join mass use social media share thoughts action publicly interestingly use public medium express recent illegal action intimidate others share outrageous image statements agencies able unearth profile may thus able anticipate stop hasten investigation gang relate crimes paper investigate use word embeddings help identify gang members twitter build previous work generate word embeddings translate twitter users post profile descriptions tweet profile image link youtube content real vector format amenable machine learn classification experimental result show pre train word embeddings boost accuracy supervise learn algorithms train gang members social media post
predict stock market movements well know problem interest days social media perfectly represent public sentiment opinion current events especially twitter attract lot attention researchers study public sentiments stock market prediction basis public sentiments express twitter intrigue field research previous study conclude aggregate public mood collect twitter may well correlate dow jones industrial average index djia thesis work observe well change stock price company rise fall correlate public opinions express tweet company understand author opinion piece text objective sentiment analysis present paper employ two different textual representations word2vec n gram analyze public sentiments tweet paper apply sentiment analysis supervise machine learn principles tweet extract twitter analyze correlation stock market movements company sentiments tweet elaborate way positive news tweet social media company would definitely encourage people invest stock company result stock price company would increase end paper show strong correlation exist rise fall stock price public sentiments tweet
street gang members use twitter intimidate others present outrageous image statements world share recent illegal activities tweet may thus useful law enforcement agencies discover clue recent crimes anticipate ones may occur find post however require method discover gang member twitter profile challenge task since gang members represent small population three hundred and twenty million twitter users paper study problem automatically find gang members twitter outline process curate one largest set verifiable gang member profile ever study review profile establish differences language image youtube link emojis gang members use compare rest twitter population feature review use train series supervise classifiers classifier achieve promise f1 score low false positive rate
patient note contain wealth information potentially great interest medical investigators however protect patients privacy protect health information phi must remove patient note legally release process know patient note de identification main objective de identification system highest possible recall recently first neural network base de identification system propose yield state art result unlike systems rely human engineer feature allow quickly deploy leverage knowledge human experts electronic health record ehrs work explore method incorporate human engineer feature well feature derive ehrs neural network base de identification system result show addition feature especially ehr derive feature improve state art patient note de identification include sensitive phi type patient name since real life set patient note typically come ehrs recommend developers de identification systems leverage information ehrs contain
present result show possible build competitive greatly simplify large vocabulary continuous speech recognition system whole word acoustic units model output vocabulary one hundred thousand word directly use deep bi directional lstm rnns ctc loss model train one hundred and twenty-five thousand hours semi supervise acoustic train data enable us alleviate data sparsity problem word model show ctc word model work well end end neural speech recognition model without use traditional context dependent sub word phone units require pronunciation lexicon without language model remove need decode demonstrate ctc word model perform better strong complex state art baseline sub word units
present method induce new dialogue systems small amount unannotated dialogue data show word level exploration use reinforcement learn rl combine incremental semantic grammar dynamic syntax ds allow systems discover generate understand many new dialogue variants method avoid use expensive time consume dialogue act annotations support natural incremental dialogues turn base systems language generation dialogue management treat joint decision optimisation problem mdp model rl construct automatically implement system show method enable wide range dialogue variations automatically capture even system train single dialogue variants include question answer pair answer self corrections clarification interaction split utterances ellipsis generalisation property result structural knowledge constraints present within ds grammar highlight limitations recent systems build use machine learn techniques
advance neural variational inference facilitate learn powerful direct graphical model continuous latent variables variational autoencoders hope model learn represent rich multi modal latent factor real world data natural language text however current model often assume simplistic priors latent variables uni modal gaussian distribution incapable represent complex latent factor efficiently overcome restriction propose simple highly flexible piecewise constant distribution distribution capacity represent exponential number modes latent target distribution remain mathematically tractable result demonstrate incorporate new latent distribution different model yield substantial improvements natural language process task document model natural language generation dialogue
problems intersection vision language significant importance challenge research question rich set applications enable however inherent structure world bias language tend simpler signal learn visual modalities result model ignore visual information lead inflate sense capability propose counter language priors task visual question answer vqa make vision v vqa matter specifically balance popular vqa dataset collect complementary image every question balance dataset associate single image rather pair similar image result two different answer question dataset construction balance original vqa dataset approximately twice number image question pair complete balance dataset publicly available wwwvisualqaorg part 2nd iteration visual question answer dataset challenge vqa v20 benchmark number state art vqa model balance dataset model perform significantly worse balance dataset suggest model indeed learn exploit language priors find provide first concrete empirical evidence seem qualitative sense among practitioners finally data collection protocol identify complementary image enable us develop novel interpretable model addition provide answer give image question pair also provide counter example base explanation specifically identify image similar original image believe different answer question help build trust machine among users
tackle prediction instructor intervention student post discussion forums massive open online course moocs key find use automatically obtain discourse relations improve prediction instructors intervene student discussions compare state art feature rich baseline supervise classifier make use automatic discourse parser output penn discourse treebank pdtb tag represent post discourse feature show pdtb relation base feature increase robustness classifier complement baseline feature recall diverse instructor intervention pattern comprehensive experiment fourteen mooc offer several discipline pdtb discourse feature improve performance average resultant model less dependent domain specific vocabulary allow better generalize new course
extend success deep neural network natural language understand symbolic reason require complex operations external memory recent neural program induction approach attempt address problem typically limit differentiable memory consequently scale beyond small synthetic task work propose manager programmer computer framework integrate neural network non differentiable memory support abstract scalable precise operations friendly neural computer interface specifically introduce neural symbolic machine contain sequence sequence neural programmer non differentiable computer lisp interpreter code assist successfully apply reinforce train augment approximate gold program find iterative maximum likelihood train process nsm able learn semantic parser weak supervision large knowledge base achieve new state art performance webquestionssp challenge semantic parse dataset weak supervision compare previous approach nsm end end therefore rely feature engineer domain specific knowledge
sentiment analysis one fastest grow research areas computer science make challenge keep track activities area present computer assist literature review utilize text mine qualitative cod analyze six thousand, nine hundred and ninety-six paper scopus find root sentiment analysis study public opinion analysis begin 20th century text subjectivity analysis perform computational linguistics community one thousand, nine hundred and ninety however outbreak computer base sentiment analysis occur availability subjective texts web consequently ninety-nine paper publish two thousand and four sentiment analysis paper scatter multiple publication venues combine number paper top fifteen venues represent ca thirty paper total present top twenty cite paper google scholar scopus taxonomy research topics recent years sentiment analysis shift analyze online product review social media texts twitter facebook many topics beyond product review like stock market elections disasters medicine software engineer cyberbullying extend utilization sentiment analysis
advent attention mechanism neural machine translation model improve performance machine translation systems enable selective lookup source sentence paper efficiencies translation use bidirectional encoder attention decoder model study respect translation involve morphologically rich languages english tamil language pair select analysis first use word2vec embed english tamil word improve translation result seventy-three bleu point baseline rnnsearch model four hundred and eighty-four bleu score use morphological segmentation word vectorization split morphologically rich tamil word respective morphemes translation cause reduction target vocabulary size factor eight also model rnnmorph improve performance neural machine translation seven hundred and five bleu point rnnsearch model use corpus since bleu evaluation rnnmorph model might unreliable due increase number match tokens per sentence performances translations also compare mean human evaluation metrics adequacy fluency relative rank use morphological segmentation also improve efficacy attention mechanism
recently propose sequence sequence seq2seq framework advocate replace complex data process pipelines entire automatic speech recognition system single neural network train end end fashion contribution analyse attention base seq2seq speech recognition system directly transcribe record character observe two shortcomings overconfidence predictions tendency produce incomplete transcriptions language model use propose practical solutions problems achieve competitive speaker independent word error rat wall street journal dataset without separate language model reach one hundred and six wer together trigram language model reach sixty-seven wer
train speech recognition systems label audio clip expensive data equally valuable active learn aim label informative sample reduce cost speech recognition confidence score likelihood base active learn methods show effective gradient base active learn methods however still well understand work investigate expect gradient length egl approach active learn end end speech recognition justify egl variance reduction perspective observe egl measure informativeness pick novel sample uncorrelated confidence score experimentally show egl reduce word errors eleven alternatively reduce number sample label fifty compare random sample
provide brief technical description online platform disease monitor title flu detector fludetectorcsuclacuk flu detector current version v05 use either twitter google search data conjunction statistical natural language process model estimate rate influenza like illness population england back end live service collect online data utilise modern technologies large scale text process finally apply statistical inference model train offline front end visualise various disease rate estimate notably model base google data achieve high level accuracy respect recent four flu season england two thousand and twelve thirteen two thousand and fifteen sixteen highlight flu detector great potential become complementary source domestic traditional flu surveillance scheme
propose online end end neural generative conversational model open domain dialogue train use unique combination offline two phase supervise learn online human loop active learn exist research propose offline supervision hand craft reward function online reinforcement devise novel interactive learn mechanism base ham diverse beam search response generation one character user feedback step experiment show model inherently promote generation semantically relevant interest responses use train agents customize personas moods conversational style
present architecture information extraction text augment exist parser character level neural network network train use measure consistency extract data exist databases form noisy supervision architecture combine ability constraint base information extraction systems easily incorporate domain knowledge constraints ability deep neural network leverage large amount data learn complex feature boost exist parser precision system lead large improvements mature highly tune constraint base production information extraction system use bloomberg financial language text
research interference provide evidence formation dependencies non adjacent word rely cue base retrieval mechanism two different model account one main predictions interference ie slowdown retrieval site several items share feature associate retrieval cue lewis vasishth two thousand and five activation base model mcelree two thousand direct access model even though two model use almost interchangeably base different assumptions predict differences relationship read time response accuracy activation base model follow assumptions act r retrieval process behave lognormal race accumulators evidence single variance model accuracy retrieval determine winner race retrieval time rate accumulation contrast direct access model assume model memory probability retrieval vary items model differences latencies product possibility repair incorrect retrievals implement model bayesian hierarchical framework order evaluate compare show aspects data better fit direct access model activation base model suggest find rule possibility retrieval may behave race model assumptions follow less closely ones act r framework show introduce modification activation model ie assume accumulation evidence retrieval incorrect items slower noisier ie different variances correct incorrect items model provide fit good one direct access model
article describe data drive method derive relationship personality media preferences qunatifiable representation relationship leverage use recommendation systems ameliorate cold start problem data comprise original collection one thousand, three hundred and sixteen okcupid date profile profile eight hundred label one sixteen possible myers briggs type indicators mbti personality specific topic model describe person favorite book movies show music food generate use latent dirichlet allocation lda several significant find example intuitive think type prefer sci fi fantasy entertainment extraversion correlate positively upbeat dance music jazz folk international cuisine correlate positively characterize openness experience many correlations confirm previous find describe relationship among personality write style personal preferences complete word personality type assocations see appendix
paper propose class novel deep recurrent neural network incorporate language level information acoustic model simplicity name network recurrent deep language network rdlns multiple variants rdlns consider include two kinds context information two methods process context two methods incorporate language level information rdlns provide possible methods fine tune whole automatic speech recognition asr system acoustic model process
deep model defacto standard visual decision model due impressive performance wide array visual task however frequently see opaque unable explain decisions contrast humans justify decisions natural language point evidence visual world lead decisions postulate deep model well propose point justification pj x model justify decision sentence point evidence introspect decision explanation process use attention mechanism unfortunately dataset available reference explanations visual decision make thus collect two datasets two domains interest challenge explain decisions first extend visual question answer task provide answer also natural language explanation answer second focus explain human activities traditionally challenge object classification extensively evaluate pj x model justification point task compare prior model ablations use automatic human evaluations
user acceptance artificial intelligence agents might depend ability explain reason require add interpretability layer fa cilitates users understand behavior paper focus add terpretable layer top semantic textual similarity sts measure degree semantic equivalence two sentence interpretability layer formalize alignment pair segment across two sentence relation segment label relation type similarity score present publicly available dataset sentence pair annotate follow formalization develop system train dataset give sentence pair explain similar different form grade type segment alignments evaluate dataset system perform better inform baseline show dataset task well define feasible importantly two user study show system output use automatically produce explanations natural language users perform better access explanations pro viding preliminary evidence dataset method automatically produce explanations useful real applications
exist model base artificial neural network anns sentence classification often incorporate context sentence appear classify sentence individually however traditional sentence classification approach show greatly benefit jointly classify subsequent sentence conditional random field work present ann architecture combine effectiveness typical ann model classify sentence isolation strength structure prediction model achieve state art result two different datasets sequential sentence classification medical abstract
recently sentiment analysis receive lot attention due interest mine opinions social media users sentiment analysis consist determine polarity give text ie degree positiveness negativeness traditionally sentiment analysis algorithms tailor specific language give complexity number lexical variations errors introduce people generate content contribution aim provide simple implement easy use multilingual framework serve baseline sentiment analysis contest start point build new sentiment analysis systems compare approach eight different languages three important international contest namely semeval english tass spanish sentipolc italian within competitions approach reach medium high position rank whereas remain languages approach outperform report result
despite widespread interest reinforcement learn task orient dialogue systems several obstacles frustrate research development progress first reinforcement learners typically require interaction environment conventional dialogue corpora use directly second task present specific challenge require separate corpus task specific annotate data third collect annotate human machine human human conversations task orient dialogues require extensive domain knowledge build appropriate dataset financially costly time consume one popular approach build user simulator base upon corpus example dialogues one train reinforcement learn agents online fashion interact simulator dialogue agents train simulators serve effective start point agents master simulator may deploy real environment interact humans continue train online ease empirical algorithmic comparisons dialogues paper introduce new publicly available simulation framework simulator design movie book domain leverage rule collect data simulator support two task movie ticket book movie seek finally demonstrate several agents detail procedure add test agent propose framework
project propose new approach emotion recognition use web base similarity eg confidence pmi pming aim extract basic emotions short sentence emotional content eg news title tweet caption perform web base quantitative evaluation semantic proximity word analyze sentence emotion psychological model eg plutchik ekman lovheim phase extraction include text preprocessing tokenization stop word filter search engine automate query html parse result ie scrap estimation semantic proximity rank emotions accord proximity measure main idea since possible generalize semantic similarity assumption similar concepts co occur document index search engines therefore also emotions generalize way tag term express particular language rank emotions train result compare human evaluation additional comparative test result perform global rank correlation eg kendall spearman pearson evaluation emotion link single word different sentiment analysis approach work deeper level abstraction aim recognize specific emotions positive negative sentiment order predict emotions semantic data
introduce exceptionally simple gate recurrent neural network rnn achieve performance comparable well know gate architectures lstms grus word level language model task prove model simple predicable non chaotic dynamics stand stark contrast standard gate architectures whose underlie dynamical systems exhibit chaotic behavior
meet important decisions get make items receive attention may influence outcome examine different type rhetorical de emphasis include hedge superlatives contrastive conjunctions correlate get revisit later control item frequency speaker data consist transcripts recur meet federal reserve open market committee fomc important aspects yous monetary policy decide surprisingly find word appear context hedge usually consider way express uncertainty likely repeat subsequent meet strong emphasis indicate superlatives slightly negative effect word recurrence subsequent meet also observe interest pattern effect vary depend social factor status gender speaker instance positive effect hedge pronounce female speakers male speakers
build artificial intelligence systems reason answer question visual data need diagnostic test analyze progress discover shortcomings exist benchmarks visual question answer help strong bias model exploit correctly answer question without reason also conflate multiple source error make hard pinpoint model weaknesses present diagnostic dataset test range visual reason abilities contain minimal bias detail annotations describe kind reason question require use dataset analyze variety modern visual reason systems provide novel insights abilities limitations
text document describe number abstract concepts semantic category write style sentiment machine learn ml model train automatically map document abstract concepts allow annotate large text collections could process human lifetime besides predict text category accurately also highly desirable understand categorization process take place paper demonstrate understand achieve trace classification decision back individual word use layer wise relevance propagation lrp recently develop technique explain predictions complex non linear classifiers train two word base ml model convolutional neural network cnn bag word svm classifier topic categorization task adapt lrp method decompose predictions model onto word result score indicate much individual word contribute overall classification decision enable one distill relevant information text document without explicit semantic information extraction step use word wise relevance score generate novel vector base document representations capture semantic information base document vectors introduce measure model explanatory power show although svm cnn model perform similarly term classification accuracy latter exhibit higher level explainability make comprehensible humans potentially useful applications
present novel method image text multi modal representation learn knowledge work first approach apply adversarial learn concept multi modal learn exploit image text pair information learn multi modal feature use category information contrast previous methods use image text pair information multi modal embed paper show multi modal feature achieve without image text pair information method make similar distribution image text multi modal feature space methods use image text pair information show multi modal feature universal semantic information even though train category prediction model end end backpropagation intuitive easily extend multi modal learn work
discourse vary age education psychiatric state historical epoch ontogenetic cultural dynamics discourse structure remain quantitatively characterize end investigate word graph obtain verbal report two hundred subject age two fifty-eight six hundred and seventy-six literary texts span five thousand years healthy subject lexical diversity graph size long range recurrence depart initial near random level monotonic asymptotic increase across age short range recurrence show correspond decrease change explain education suggest hierarchical development discourse structure short range recurrence lexical diversity stabilize elementary school graph size long range recurrence stabilize high school gradual maturation blur psychotic subject maintain adulthood near random structure literature monotonic asymptotic change time remarkable lexical diversity long range recurrence graph size increase away near randomness short range recurrence decline random level bronze age texts structurally similar childish psychotic discourse subsequent texts converge abruptly healthy adult pattern around onset axial age eight hundred two hundred bc period pivotal cultural change thus individually well historically discourse maturation increase range word recurrence away randomness
refer expressions natural language constructions use identify particular object within scene paper propose unify framework task refer expression comprehension generation model compose three modules speaker listener reinforcer speaker generate refer expressions listener comprehend refer expressions reinforcer introduce reward function guide sample discriminative expressions listener speaker modules train jointly end end learn framework allow modules aware one another learn also benefit discriminative reinforcer feedback demonstrate unify framework train achieve state art result comprehension generation three refer expression datasets project demo page https visioncsuncedu refer
folksodriven framework make possible data scientists define ontology environment search bury pattern kind predictive power build predictive model effectively accomplish abstractions isolate parameters predictive model process search pattern design feature set reflect evolve knowledge paper consider ontologies base folksonomies accord new concept structure call folksodriven represent folksonomies study transformational regulation folksodriven tag regard important adaptive folksonomies classifications evolve environment use intelligent systems represent knowledge share folksodriven tag use categorize salient data point feed machine learn system featurizing data
traditional disease surveillance augment wide variety real time source news social media however source general unstructured construction surveillance tool taxonomical correlations trace map involve considerable human supervision paper motivate disease vocabulary drive word2vec model dis2vec model diseases constituent attribute word embeddings healthmap news corpus use word embeddings automatically create disease taxonomies evaluate model correspond human annotate taxonomies compare model accuracies several state art word2vec methods result demonstrate dis2vec outperform traditional distribute vector representations ability faithfully capture taxonomical attribute across different class diseases endemic emerge rare
study segmental recurrent neural network end end acoustic model model connect segmental conditional random field crf recurrent neural network rnn use feature extraction compare previous crf base acoustic model rely external system provide feature segmentation boundaries instead model marginalise possible segmentations feature extract rnn train together segmental crf essence model self contain train end end paper discuss practical train decode issue well method speed train context speech recognition perform experiment timit dataset achieve one hundred and seventy-three phone error rate per first pass decode best report result use crfs despite fact use zeroth order crf without use language model
recursive neural network rnn recently propose extension recursive long short term memory network rlstm model compute representations sentence recursively combine word embeddings accord externally provide parse tree model thus unlike recurrent network explicitly make use hierarchical structure sentence paper demonstrate rnns nevertheless suffer vanish gradient long distance dependency problem rlstms greatly improve rnn problems present artificial learn task allow us quantify severity problems model show ratio gradients root node focal leaf node highly indicative success backpropagation optimize relevant weight low tree paper thus provide explanation exist superior result rlstms task sentiment analysis suggest benefit include hierarchical structure include lstm style gate complementary
neural machine translation mt reach state art result however one main challenge neural mt still face deal large vocabularies morphologically rich languages paper propose neural mt system use character base embeddings combination convolutional highway layer replace standard lookup base word representations result unlimited vocabulary affix aware source word embeddings test state art neural mt base attention base bidirectional recurrent neural network propose mt scheme provide improve result even source language morphologically rich improvements three bleu point obtain german english wmt task
state art sequence label systems traditionally require large amount task specific knowledge form hand craft feature data pre process paper introduce novel neutral network architecture benefit word character level representations automatically use combination bidirectional lstm cnn crf system truly end end require feature engineer data pre process thus make applicable wide range sequence label task evaluate system two data set two sequence label task penn treebank wsj corpus part speech pos tag conll two thousand and three corpus name entity recognition ner obtain state art performance two data nine thousand, seven hundred and fifty-five accuracy pos tag nine thousand, one hundred and twenty-one f1 ner
neural network architectures memory attention mechanisms exhibit certain reason capabilities require question answer one architecture dynamic memory network dmn obtain high accuracy variety language task however show whether architecture achieve strong result question answer support facts mark train whether could apply modalities image base analysis dmn propose several improvements memory input modules together change introduce novel input module image order able answer visual question new dmn model improve state art visual question answer dataset babi 10k text question answer dataset without support fact supervision
paper consider two sequence tag task medieval latin part speech tag lemmatization basic yet foundational preprocessing step applications text use detection nevertheless generally complicate considerable orthographic variation typical medieval latin digital classics task traditionally solve cascade ii lexicon dependent fashion example lexicon use generate potential lemma tag pair token next context aware pos tagger use select appropriate tag lemma pair apart problems lexicon items error percolation major downside approach paper explore possibility elegantly solve task use single integrate approach make use layer neural network architecture field deep representation learn
paper present novel latent variable recurrent neural network architecture jointly model sequence word possibly latent discourse relations adjacent sentence recurrent neural network generate individual word thus reap benefit discriminatively train vector representations discourse relations represent latent variable predict marginalize depend task result model therefore employ train objective include discourse relation classification also word prediction result outperform state art alternatives two task implicit discourse relation classification penn discourse treebank dialog act classification switchboard corpus furthermore marginalize latent discourse relations test time obtain discourse inform language model improve strong lstm baseline
without discourse connectives classify implicit discourse relations challenge task bottleneck build practical discourse parser previous research usually make use one kind discourse framework pdtb rst improve classification performance discourse relations actually different discourse annotation frameworks exist multiple corpora internal connections exploit combination different discourse corpora design relate discourse classification task specific corpus propose novel convolutional neural network embed multi task learn system synthesize task learn unique share representations task experimental result pdtb implicit discourse relation classification task demonstrate model achieve significant gain baseline systems
paper provide update concern set european dariah infrastructure series strong action line relate development data centre strategy humanities come years particular tackle various aspect data management data host set dariah seal approval establishment charter cultural heritage institutions scholars finally specific view certification mechanisms data
describe large vocabulary speech recognition system accurate low latency yet small enough memory computational footprint run faster real time nexus five android smartphone employ quantize long short term memory lstm acoustic model train connectionist temporal classification ctc directly predict phoneme target reduce memory footprint use svd base compression scheme additionally minimize memory footprint use single language model dictation voice command domains construct use bayesian interpolation finally order properly handle device specific information proper name context dependent information inject vocabulary items decoder graph bias language model fly system achieve one hundred and thirty-five word error rate open end dictation task run median speed seven time faster real time
describe strategy acquisition train data necessary build social media drive early detection system individuals risk preventable type two diabetes mellitus t2dm strategy use game like quiz data question acquire semi automatically twitter question design inspire participant engagement collect relevant data train public health model apply individuals prior systems design use social media twitter predict obesity risk factor t2dm operate entire communities state counties cities base statistics gather government agencies considerable variation among individuals within group train data individual level would effective data difficult acquire approach propose aim address issue strategy two step first train random forest classifier data gather public twitter statuses state level statistics state art accuracy convert classifier twenty question style quiz make available online achieve high engagement individuals take quiz also build train set voluntarily supply individual level data future classification
introduce globally normalize transition base neural network model achieve state art part speech tag dependency parse sentence compression result model simple fee forward neural network operate task specific transition system yet achieve comparable better accuracies recurrent model discuss importance global oppose local normalization key insight label bias problem imply globally normalize model strictly expressive locally normalize model
explosion work vision language community past years image caption video transcription answer question image task focus literal descriptions image move beyond literal choose explore question image often direct commonsense inference abstract events evoke object image paper introduce novel task visual question generation vqg system task ask natural engage question show image provide three datasets cover variety image object centric event centric considerably abstract train data provide state art caption systems thus far train test several generative retrieval model tackle task vqg evaluation result show model ask reasonable question variety image still wide gap human performance motivate work connect image commonsense knowledge pragmatics propose task offer new challenge community hope further interest explore deeper connections vision language
transfer learn aim make use valuable knowledge source domain help model performance target domain particularly important neural network likely overfitting field like image process many study show effectiveness neural network base transfer learn neural nlp however exist study casually apply transfer learn conclusions inconsistent paper conduct systematic case study provide illuminate picture transferability neural network nlp
review task sentence pair score popular literature various form view answer sentence selection semantic text score next utterance rank recognize textual entailment paraphrase eg component memory network argue task similar model perspective propose new baselines compare performance common ir metrics popular convolutional recurrent attention base neural model across many sentence pair score task datasets discuss problem evaluate randomize model propose statistically ground methodology attempt improve comparisons release new datasets much harder currently use well explore benchmarks introduce unify open source software framework easily pluggable model task enable us experiment multi task reusability train sentence model set new state art performance ubuntu dialogue dataset
combine deep neural network structure logic rule desirable harness flexibility reduce uninterpretability neural model propose general framework capable enhance various type neural network eg cnns rnns declarative first order logic rule specifically develop iterative distillation method transfer structure information logic rule weight neural network deploy framework cnn sentiment analysis rnn name entity recognition highly intuitive rule obtain substantial improvements achieve state art comparable result previous best perform systems
address important problem sequence sequence seq2seq learn refer copy certain segment input sequence selectively replicate output sequence similar phenomenon observable human language communication example humans tend repeat entity name even long phrase conversation challenge regard copy seq2seq new machinery need decide perform operation paper incorporate copy neural network base seq2seq learn propose new model call copynet encoder decoder structure copynet nicely integrate regular way word generation decoder new copy mechanism choose sub sequence input sequence put proper place output sequence empirical study synthetic data set real world data set demonstrate efficacy copynet example copynet outperform regular rnn base model remarkable margins text summarization task
paper present system create visualize probabilistic semantic link concepts thesaurus class classification system create link build polylingual label topic model pll tm pll tm identify probable thesaurus descriptors class classification system use information natural language text document assign thesaurus descriptors designate class link present users system interactive visualization provide automatically generate overview relations thesaurus classification system
aspect base sentiment analysis extract aspect term along opinions express user generate content one important subtasks previous study show exploit connections aspect opinion term promise task paper propose novel joint model integrate recursive neural network conditional random field unify framework explicit aspect opinion term co extraction propose model learn high level discriminative feature double propagate information aspect opinion term simultaneously moreover flexible incorporate hand craft feature propose model boost information extraction performance experimental result semeval challenge two thousand and fourteen dataset show superiority propose model several baseline methods well win systems challenge
past decade large scale supervise learn corpora enable machine learn researchers make substantial advance however date large scale question answer corpora available paper present 30m factoid question answer corpus enormous question answer pair corpus produce apply novel neural network architecture knowledge base freebase transduce facts natural language question produce question answer pair evaluate human evaluators use automatic evaluation metrics include well establish machine translation sentence similarity metrics across evaluation criteria question generation model outperform compete template base baseline furthermore present human evaluators generate question appear comparable quality real human generate question
apply general recurrent neural network rnn encoder framework community question answer cqa task approach rely linguistic process apply different languages domains improvements observe extend rnn encoders neural attention mechanism encourage reason entire sequence deal practical issue data sparsity imbalanced label apply various techniques transfer learn multitask learn experiment semeval two thousand and sixteen cqa task show ten improvement map score compare information retrieval base approach achieve comparable performance strong handcraft feature base method
samtla search mine tool linguistic analysis digital humanities system design collaboration historians linguists assist research work quantify content textual corpora approximate phrase search document comparison retrieval engine use character base n gram language model rather conventional word base one achieve great flexibility language agnostic query process index implement space optimise character base suffix tree accompany database document content metadata number text mine tool integrate system allow researchers discover textual pattern perform comparative analysis find currently popular research community herein describe system architecture user interface model algorithms data storage samtla system also present several case study usage practice together evaluation systems rank performance crowdsourcing
conditor software tool work textual document contain historical information purpose work two fold firstly show validity develop engine correctly identify label entities universe discourse label combine xtm dita model secondly explain improvements achieve information retrieval process thank use object orient database jpox well integration lucene type database search process accomplish accurate search also help future development recommender system finish brief demo 3d graph result aforementioned search
increase number people use online social network service snss significant amount information relate experience consumption share new media form text mine emerge technique mine useful information web aim discover particular tweet semantic pattern consumers discussions social media specifically purpose study twofold one find similarity dissimilarity two set textual document include consumers sentiment polarities two form positive vs negative opinions two drive actual content textual data semantic trend consider tweet include consumers opinions us retail company eg amazon walmart cosine similarity k mean cluster methods use achieve former goal latent dirichlet allocation lda popular topic model algorithm use latter purpose first study discover semantic properties textual data consumption context beyond sentiment analysis addition major find apply lda latent dirichlet allocations data draw latent topics represent consumers positive opinions negative opinions social media
consider problem learn distribute representations tag associate content task tag recommendation consider tag information usually sparse effective learn content tag association crucial challenge task recently various neural representation learn model wsabie variants show promise performance mainly due compact feature representations learn semantic space however capacity limit linear compositional approach represent tag sum equal part hurt performance work propose neural feedback relevance model learn tag representations weight feature representations experiment two widely use datasets show significant improvement quality recommendations various baselines
investigate evaluation metrics dialogue response generation systems supervise label task completion available recent work response generation adopt metrics machine translation compare model generate response single target response show metrics correlate weakly human judgements non technical twitter domain technical ubuntu domain provide quantitative qualitative result highlight specific weaknesses exist metrics provide recommendations future development better automatic evaluation metrics dialogue systems
study problem compress recurrent neural network rnns particular focus compression rnn acoustic model motivate goal build compact accurate speech recognition systems run efficiently mobile devices work present technique general recurrent model compression jointly compress recurrent non recurrent inter layer weight matrices find propose technique allow us reduce size long short term memory lstm acoustic model third original size negligible loss accuracy
thesis focus gain linguistic insights textual discussions word level special interest distinguish message constructively contribute discussion detrimental thereby want determine whether message indicators either two discussion style message nowadays often use guidelines successful communication although effect successfully evaluate multiple time large scale analysis never conduct thus use wikipedia article deletion short afd discussions together record block users develop fully automate creation annotate data set data set message label either constructive disruptive apply binary classifiers data determine characteristic word discussion style thereby also investigate whether function word like pronouns conjunctions play important role distinguish two find message strong indicator disruptive message match attribute effect communication however find message indicative disruptive message well contrary attribute effect importance function word could neither confirm refute characteristic word either communication style find yet result suggest different model might represent disruptive constructive message textual discussions better
understand language go hand hand ability integrate complex contextual information obtain via perception work present novel task ground language understand disambiguate sentence give visual scene depict one possible interpretations sentence end introduce new multimodal corpus contain ambiguous sentence represent wide range syntactic semantic discourse ambiguities couple videos visualize different interpretations sentence address task extend vision model determine sentence depict video demonstrate model adjust recognize different interpretations underlie sentence allow disambiguate sentence unify fashion across different ambiguity type
development e commerce many products sell worldwide manufacturers eager obtain better understand customer behavior various regions achieve goal previous efforts focus mainly questionnaires time consume costly tremendous volume product review e commerce websites see new trend emerge whereby manufacturers attempt understand user preferences analyze online review follow trend paper address problem study customer behavior exploit recently develop opinion mine techniques work novel three reason first questionnaire base investigation automatically enable employ algorithms template base question generation opinion mine base answer extraction use system manufacturers able obtain report customer behavior feature much larger sample size direct information higher degree automation lower cost second international customer behavior study make easier integrate tool multilingual opinion mine third first time automatic questionnaire investigation conduct compare customer behavior china america product review write read chinese english respectively study digital cameras smartphones tablet computers yield three find first chinese customers follow doctrine mean often use euphemistic expressions american customers express opinions directly second chinese customers care general feel american customers pay attention product detail third chinese customers focus external feature american customers care internal feature products
problem rare unknown word important issue potentially influence performance many nlp systems include traditional count base deep learn model propose novel way deal rare unseen word neural network model use attention model use two softmax layer order predict next word conditional language model one predict location word source sentence predict word shortlist vocabulary time step decision softmax layer use choose adaptively make mlp condition contextwe motivate work psychological evidence humans naturally tendency point towards object context environment name object knownwe observe improvements two task neural machine translation europarl english french parallel corpora text summarization gigaword dataset use propose model
paper focus two key problems audio visual emotion recognition video one audio visual stream temporal alignment feature level fusion one locate weight perception attentions whole audio visual stream better recognition long short term memory recurrent neural network lstm rnn employ main classification architecture firstly soft attention mechanism align audio visual stream secondly seven emotion embed vectors correspond classification emotion type add locate perception attentions locate weight process also base soft attention mechanism experiment result emotiw2015 dataset qualitative analysis show efficiency propose two techniques
identify topics discussions online health communities ohc critical various applications difficult topics ohc content usually heterogeneous domain dependent paper provide multi class schema annotate dataset supervise classifiers base convolutional neural network cnn model task classify discussion topics apply cnn classifier popular breast cancer online community carry longitudinal analysis show topic distributions topic change throughout members participation experimental result suggest cnn outperform classifiers task topic classification certain trajectories detect respect topic change
paper introduce visually inform embed word view continuous vector representation word extract deep neural model train use microsoft coco data set forecast spatial arrangements visual object give textual description model compose deep multilayer perceptron mlp stack top long short term memory lstm network latter precede embed layer view apply transfer multimodal background knowledge spatial role label sprl algorithms recognize spatial relations object mention text work also contribute new method select complementary feature fine tune method mlp improve f1 measure classify word spatial roles view evaluate task three semeval two thousand and thirteen benchmark data set spaceeval
clearly explain rationale classification decision end user important decision exist approach deep visual recognition generally opaque output justification text contemporary vision language model describe image content fail take account class discriminative image aspects justify visual predictions propose new model focus discriminate properties visible object jointly predict class label explain predict label appropriate image propose novel loss function base sample reinforcement learn learn generate sentence realize global sentence property class specificity result fine grain bird species classification dataset show model able generate explanations consistent image also discriminative descriptions produce exist caption methods
present approach learn multi sense word embeddings rely monolingual bilingual information model consist encoder use monolingual bilingual context ie parallel sentence choose sense give word decoder predict context word base choose sense two components estimate jointly observe word representations induce bilingual data outperform monolingual counterparts across range evaluation task even though crosslingual information available test time
dominant language model lms n gram neural network nn model represent sentence probabilities term conditionals contrast new trans dimensional random field trf lm recently introduce show superior performances whole sentence model random field paper examine trf model interpolate nn model obtain one hundred and twenty-one one hundred and seventy-nine relative error rate reductions six gram lms english chinese speech recognition respectively log linear combination
report implementation clinical information extraction tool leverage deep neural network annotate event span attribute raw clinical note pathology report approach use context word part speech tag shape information feature hire temporal 1d convolutional neural network learn hide feature representations finally use multilayer perceptron mlp predict event span empirical evaluation demonstrate approach significantly outperform baselines
neural network base approach sentence relation model automatically generate hide match feature raw sentence pair however quality match feature representation may satisfy due complex semantic relations entailment contradiction address challenge propose new deep neural network architecture jointly leverage pre train word embed auxiliary character embed learn sentence mean two kinds word sequence representations input multi layer bidirectional lstm learn enhance sentence representation construct match feature follow another temporal cnn learn high level hide match feature representations experimental result demonstrate approach consistently outperform exist methods standard evaluation datasets
deep learn dramatically improve performance speech recognition systems learn hierarchies feature optimize task hand however true end end learn feature learn directly waveforms recently reach performance hand tailor representations base fourier transform paper detail approach use convolutional filter push past inherent tradeoff temporal frequency resolution exist spectral representations increase computational cost show increase temporal resolution via reduce stride increase frequency resolution via additional filter deliver significant performance improvements find efficient representations simultaneously learn multiple scale lead overall decrease word error rate difficult internal speech test set two hundred and seven relative network number parameters train spectrograms
although highly correlate speech speaker recognition regard two independent task study two communities certainly way people behave decipher speech content speaker traits time paper present unify model perform speech speaker recognition simultaneously altogether model base unify neural network output one task feed input lead multi task recurrent network experiment show joint model outperform task specific model two task
web discussion forums use millions people worldwide share information belong variety domains automotive vehicles pet sport etc typically contain post fall different categories problem solution feedback spam etc automatic identification categories aid information retrieval tailor specific user requirements previously number supervise methods attempt solve problem however depend availability abundant train data exist unsupervised semi supervise approach either focus identify single category report category specific performance contrast work propose unsupervised semi supervise methods require minimal train data achieve objective without compromise performance fine grain analysis also carry discuss limitations propose methods base sequence model specifically hide markov model model language category use word part speech probability distributions manually specify feature empirical evaluations across domains demonstrate propose methods better suit task exist ones
traditional topic model account semantic regularities language recent distributional representations word exhibit semantic consistency directional metrics cosine similarity however neither categorical gaussian observational distributions use exist topic model appropriate leverage correlations paper propose use von mises fisher distribution model density word unit sphere representation well suit directional data use hierarchical dirichlet process base topic model propose efficient inference algorithm base stochastic variational inference model enable us naturally exploit semantic structure word embeddings flexibly discover number topics experiment demonstrate method outperform competitive approach term topic coherence two different text corpora offer efficient inference
study address problem train neuralnetwork language identification use label unlabeled speech sample form vectors propose neural network architecture also handle set languages utilize modify version recently propose ladder network semisupervised train procedure optimize reconstruction cost stack denoising autoencoders show approach successfully apply case train dataset compose label unlabeled acoustic data result show enhance language identification nist two thousand and fifteen language identification dataset
modern nlp model rely heavily engineer feature often combine word contextual information complex lexical feature combination result large number feature lead fit present new model represent complex lexical feature comprise part word contextual information label tensor capture conjunction information among part apply low rank tensor approximations correspond parameter tensors reduce parameter space improve prediction speed furthermore investigate two methods handle feature include n grams mix lengths model achieve state art result task relation extraction pp attachment preposition disambiguation
show character level encoder decoder framework successfully apply question answer structure knowledge base use model single relation question answer demonstrate effectiveness approach simplequestions dataset bordes et al two thousand and fifteen improve state art accuracy six hundred and thirty-nine seven hundred and nine without use ensembles importantly character level model 16x fewer parameters equivalent word level model learn significantly less data compare previous work rely data augmentation robust new entities test
work present end end trainable deep bidirectional lstm long short term memory model image caption model build deep convolutional neural network cnn two separate lstm network capable learn long term visual language interactions make use history future context information high level semantic space two novel deep bidirectional variant model increase depth nonlinearity transition different way propose learn hierarchical visual language embeddings data augmentation techniques multi crop multi scale vertical mirror propose prevent overfitting train deep model visualize evolution bidirectional lstm internal state time qualitatively analyze model translate image sentence propose model evaluate caption generation image sentence retrieval task three benchmark datasets flickr8k flickr30k mscoco datasets demonstrate bidirectional lstm model achieve highly competitive performance state art result caption generation even without integrate additional mechanism eg object detection attention model etc significantly outperform recent methods retrieval task
work extend previous analyse linguistic network adopt multi layer network framework model human mental lexicon ie abstract mental repository word concepts store together linguistic pattern across three layer linguistic multiplex model english word nod connect accord phonological similarities ii synonym relationships iii free word associations main aim exploit multi layer structure explore influence phonological semantic relationships lexicon assembly time propose model lexicon growth drive phonological layer word suggest accord different order insertion eg shorter word length highest frequency semantic multiplex feature accept reject subject constraints measure time network assembly compare empirical data age acquisition word agreement empirical study psycholinguistics result provide quantitative evidence hypothesis word acquisition drive feature multiple level organisation within language
visual question answer vqa problems attract increase interest multiple research discipline solve vqa problems require techniques computer vision understand visual content present image video well ones natural language process understand semantics question generate answer regard visual content model exist vqa methods adopt strategy extract global feature image video inevitably fail capture fine grain information spatial configuration multiple object extract feature auto generate regions region base image recognition methods essentially address problem may introduce overwhelm irrelevant feature question work propose novel focus dynamic attention fda model provide better align image content representation propose question aware key word question fda employ shelf object detector identify important regions fuse information regions global feature via lstm unit question drive representations combine question representation feed reason unit generate answer extensive evaluation large scale benchmark dataset vqa clearly demonstrate superior performance fda well establish baselines
deep cnns small 3x3 kernels recently show achieve strong performance acoustic model hybrid nn hmm speech recognition systems paper investigate efficiently scale model larger datasets specifically address design choice pool pad along time dimension render convolutional evaluation sequence highly inefficient propose new cnn design without timepadding without timepooling slightly suboptimal accuracy two significant advantage enable sequence train deployment allow efficient convolutional evaluation full utterances allow batch normalization straightforwardly adopt cnns sequence data batch normalization recover lose peformance remove time pool keep benefit efficient convolutional evaluation demonstrate performance model larger scale data sequence train deep cnn model sequence train 2000h switchboard dataset obtain ninety-four word error rate hub5 test set match single model performance two thousand and fifteen ibm system combination previous best publish result
topic model emerge fundamental tool unsupervised machine learn modern topic model algorithms take probabilistic view derive inference algorithms base latent dirichlet allocation lda variants contrast study topic model combinatorial optimization problem propose new objective function derive lda pass small variance limit minimize derive objective use ideas combinatorial optimization result new fast high quality topic model algorithm particular show result competitive popular lda base topic model approach also discuss dissimilarities approach probabilistic counterparts
propose sentence level recurrent topic model slrtm new topic model assume generation word within sentence depend topic sentence whole history precede word sentence different conventional topic model largely ignore sequential order word topic coherence slrtm give full characterization use recurrent neural network rnn base framework experimental result show slrtm outperform several strong baselines various task furthermore slrtm automatically generate sentence give topic ie topics sentence key technology real world applications personalize short text conversation
present approach simultaneously perform semantic segmentation prepositional phrase attachment resolution caption image ambiguities language resolve without simultaneously reason associate image consider sentence shoot elephant pajamas look language alone use common sense unclear person elephant wear pajamas approach produce diverse set plausible hypotheses semantic segmentation prepositional phrase attachment resolution jointly reranked select consistent pair show semantic segmentation prepositional phrase attachment resolution modules complementary strengths joint reason produce accurate result module operate isolation multiple hypotheses also show crucial improve multiple module reason vision language approach significantly outperform stanford parser de marneffe et al two thousand and six one thousand, seven hundred and ninety-one two thousand, eight hundred and sixty-nine relative one thousand, two hundred and eighty-three two thousand, five hundred and twenty-eight relative two different experiment also make small improvements deeplab crf chen et al two thousand and fifteen
recurrent neural network rnns include long short term memory lstm rnns produce state art result variety speech recognition task however model often large size deployment mobile devices memory latency constraints work study mechanisms learn compact rnns lstms via low rank factorizations parameter share scheme goal investigate redundancies recurrent architectures compression admit without lose performance hybrid strategy use structure matrices bottom layer share low rank factor top layer find particularly effective reduce parameters standard lstm seventy-five small cost three increase wer two thousand hr english voice search task
human communication often execute form narrative account connect events compose character action settings coherent narrative structure therefore requisite well formulate narrative fictional nonfictional informative effective communication open possibility deeper understand narrative study structural properties paper present network base framework model analyze structure narrative expand incorporate methods computational linguistics utilize narrative text model narrative dynamically unfold system characterize progression via growth pattern character network use sentiment analysis topic model represent actual content narrative form interaction map character associate sentiment value keywords network framework advance beyond simple occurrence base one often use allow one utilize unique characteristics give narrative high degree give ubiquity importance narratives advance network base representation analysis framework may lead systematic model understand narratives social interactions expression human sentiments communication
although traditionally use machine translation field encoder decoder framework recently apply generation video image descriptions combination convolutional recurrent neural network model prove outperform previous state art obtain accurate video descriptions work propose push model introduce two contributions encode stage first produce richer image representations combine object location information convolutional neural network second introduce bidirectional recurrent neural network capture forward backward temporal relationships input frame
social botnets become important phenomenon social media many ways social bots disrupt influence online discourse spam hashtags scam twitter users astroturfing paper consider one specific social botnet twitter understand grow time content tweet social botnet differ regular users dataset lastly social botnet may influence relevant discussions analysis base qualitative cod approximately three thousand tweet arabic english syrian social bot active thirty-five weeks twitter shutdown find growth behavior content particular botnet specifically align common conceptions botnets identify interest aspects botnet distinguish regular users
introduce first dataset sequential vision language explore data may use task visual storytelling first release dataset sind v1 include eighty-one thousand, seven hundred and forty-three unique photos twenty thousand, two hundred and eleven sequence align descriptive caption story language establish several strong baselines storytelling task motivate automatic metric benchmark progress model concrete description well figurative social language provide dataset storytelling task potential move artificial intelligence basic understand typical visual scenes towards human like understand ground event structure subjective expression
exist open domain human computer conversation systems typically passive either synthesize retrieve reply provide human issue utterance generally presume humans take role lead conversation introduce new content stalemate occur computer need respond paper propose stalematebreaker conversation system proactively introduce new content appropriate design pipeline determine introduce new content human computer conversation propose novel reranking algorithm bi pagerank hit enable rich interaction conversation context candidate reply experiment show content introduce approach reranking algorithm effective full stalematebreaker model outperform state practice conversation system one hundred and forty-four p1 stalemate occur
semantic match aim determine match degree two texts fundamental problem many nlp applications recently deep learn approach apply problem significant improvements achieve paper propose view generation global interaction two texts recursive process ie interaction two texts position composition interactions prefix well word level interaction current position base idea propose novel deep architecture namely match srnn model recursive match structure firstly tensor construct capture word level interactions spatial rnn apply integrate local interactions recursively importance determine four type gate finally match score calculate base global interaction show degenerate exact match scenario match srnn approximate dynamic program process longest common subsequence thus exist clear interpretation match srnn experiment two semantic match task show effectiveness match srnn ability visualize learn match structure
teach machine accomplish task converse naturally humans challenge currently develop task orient dialogue systems require create multiple components typically involve either large amount handcraft acquire costly label datasets solve statistical learn problem component work introduce neural network base text text end end trainable goal orient dialogue system along new way collect dialogue data base novel pipe line wizard oz framework approach allow us develop dialogue systems easily without make many assumptions task hand result show model converse human subject naturally whilst help accomplish task restaurant search domain
word2vec widely use algorithm extract low dimensional vector representations word generate considerable excitement machine learn natural language process nlp communities recently due exceptional performance many nlp applications name entity recognition sentiment analysis machine translation question answer state art algorithms include mikolov et al parallelize multi core cpu architectures base vector vector operations memory bandwidth intensive efficiently use computational resources paper improve reuse various data structure algorithm use minibatching hence allow us express problem use matrix multiply operations also explore different techniques distribute word2vec computation across nod compute cluster demonstrate good strong scalability thirty-two nod combination techniques allow us scale computation near linearly across core nod process hundreds millions word per second fastest word2vec implementation best knowledge
online user review describe various products service abundant web information convey review texts rat easily comprehensible wealth hide information immediately obvious study unlock hide value behind user review understand various dimension along users rate products learn set users represent dimension use rat predict product rat specifically work restaurant review identify users whose rat influence dimension like service atmosphere etc order predict restaurant rat understand variation rat behaviour across different cuisines previous approach obtain product rat require either large number user rat review texts show possible predict rat user rat review text experiment show approach outperform conventional methods sixteen twenty-seven term rmse
identify communicate relationships cause effect important understand world affect language structure cognitive emotional bias properties communication medium despite increase importance social media much remain unknown causal statements make online study real world causal attribution extract large scale corpus causal statements make twitter social network platform well comparable random control corpus compare causal control statements use statistical language sentiment analysis tool find causal statements number significant lexical grammatical differences compare control tend negative sentiment control causal statements make online tend focus news current events medicine health interpersonal relationships show topic model quantify feature potential bias causality communication study improve understand accuracy information opinions find online
embed base knowledge base completion model far mostly combine distribute representations individual entities relations compute truth score miss link facts however also represent use pairwise embeddings ie embeddings pair entities relations paper explore bigram embeddings flexible factorization machine model several ablations investigate relevance various bigram type fb15k237 dataset find relative improvements compare compositional model
propose transition base dependency parser use recurrent neural network long short term memory lstm units extend feedforward neural network parser chen man two thousand and fourteen enable model entire sequence shift reduce transition decisions google web treebank lstm parser competitive best feedforward parser overall accuracy notably achieve three improvement long range dependencies prove difficult previous transition base parsers due error propagation limit context information find additionally suggest dropout regularisation embed layer crucial improve lstm generalisation
recently long short term memory neural network lstm attract wide interest due success many task lstm architecture consist memory cell three gate look similar neuronal network brain however still lack evidence cognitive plausibility lstm architecture well work mechanism paper study cognitive plausibility lstm align internal architecture brain activity observe via fmri subject read story experiment result show artificial memory vector lstm accurately predict observe sequential brain activities indicate correlation lstm architecture cognitive process story read
develop natural language interface human robot interaction implement reason deep semantics natural language realize require deep analysis employ methods cognitive linguistics namely modular compositional framework embody construction grammar ecg feldman two thousand and nine use ecg robots able solve fine grain reference resolution problems issue relate deep semantics compositionality natural language also include verbal interaction humans clarify command query ambiguous execute safely implement nlu framework ros package present proof concept scenarios different robots well survey state art
contrast much previous work focus location classification tweet restrict specific country undertake task broader context classify global tweet country level far unexplored real time scenario analyse extent tweet country origin determine make use eight tweet inherent feature classification furthermore use two datasets collect year apart analyse extent model train historical tweet still leverage classification new tweet classification experiment two hundred and seventeen countries datasets well top twenty-five countries offer insights best use tweet inherent feature accurate country level classification tweet find use single feature use tweet content alone widely use feature previous work leave much desire choose appropriate combination tweet content metadata actually lead substantial improvements twenty fifty observe tweet content user self report location user real name inherent tweet available real time scenario particularly useful determine country origin also experiment applicability model train historical tweet classify new tweet find choice particular combination feature whose utility fade time actually lead comparable performance avoid need retrain however difficulty achieve accurate classification increase slightly countries multiple commonalities especially english spanish speak countries
novelty detection news events long difficult problem number model perform well specific data stream certain issue far solve particularly large data stream www unpredictability new term require adaptation vector space model present novel event detection system base incremental term frequency inverse document frequency tf idf weight incorporate locality sensitive hash lsh system could efficiently effectively adapt change within data stream new term continual update vector space model regard miss probability propose novelty detection framework outperform recognise baseline system approximately sixteen evaluate benchmark dataset google news
large scale automate meta analysis neuroimaging data recently establish important tool advance understand human brain function research pioneer neurosynth database collect brain activation coordinate associate text across large cohort neuroimaging research paper one fundamental aspects meta analysis text mine date word count sophisticate methods latent dirichlet allocation propose work present unsupervised study neurosynth text corpus use deep boltzmann machine dbms use dbms yield several advantage aforementioned methods principal among fact yield word document embeddings high dimensional vector space embeddings serve facilitate use traditional machine learn techniques text corpus propose dbm model show learn embeddings clear semantic structure
sketch new derivation zipf law word frequencies base optimal cod structure derivation reminiscent mandelbrot random type model multiple advantage random type one start realistic cognitive pressure two require fine tune parameters three shed light origins statistical laws language thus lead compact theory linguistic laws find suggest recurrence zipf law human languages could originate pressure easy fast communication
sentiment analysis social media data tweet weibo become important challenge task due intrinsic properties data tweet short noisy divergent topics sentiment classification data require model various contexts retweet reply history tweet social context author relationships prior study approach issue model contexts tweet paper propose use hierarchical lstm model rich contexts tweet particularly long range context experimental result show contexts help us perform sentiment classification remarkably better
present recent advance along error analysis ibm speaker recognition system conversational speech key advancements contribute system include nearest neighbor discriminant analysis nda approach oppose lda intersession variability compensation vector space application speaker channel adapt feature derive automatic speech recognition asr system speaker recognition use dnn acoustic model large number output units 10k senones compute frame level soft alignments require vector estimation process evaluate techniques nist two thousand and ten sre extend core condition c1 c9 well 10sec 10sec condition knowledge result achieve system represent best performances publish date condition example extend tel tel condition c5 system achieve ever fifty-nine garner understand remain errors c5 examine record associate low score target trials various issue identify problematic record trials interestingly observe correct pathological record improve score target trials also nontarget trials
number user review tourist attractions restaurants mobile apps etc increase languages yet research lack review multiple languages aggregate display speakers different languages may consistently different experience eg different information available different languages tourist attractions different user experience software due internationalization localization choices paper assess similarity rat give speakers different languages london tourist attractions tripadvisor correlations different languages generally high language pair correlate others result question common practice compute average rat review many languages
dialog state track challenge four dstc four propose several pilot task paper focus speak language understand pilot task consist tag give utterance speech act semantic slot compare different classifiers best system obtain fifty-two sixty-seven f1 score test set speech act recognition tourist guide respectively fifty-two f1 score semantic tag guide tourist
dialog state track challenge four dstc four differentiate previous three editions follow number slot value pair present ontology much larger speak language understand output give utterances label subdialog level paper describe novel dialog state track method design work robustly condition use elaborate string match coreference resolution tailor dialogs improvements method correctly identify many value explicitly present utterance final evaluation method come first among seven compete team twenty-four entries f1 score achieve method nine seven percentage point higher runner utterance level evaluation subdialog level evaluation respectively
computerize evaluation english essay perform use machine learn techniques like latent semantic analysis lsa generalize lsa bilingual evaluation understudy maximum entropy ontology concept map domain knowledge enhance performance techniques use ontology make evaluation process holistic presence keywords synonyms right word combination coverage concepts check paper mention techniques implement without ontology test common input data consist technical answer computer science domain ontology computer graphics design develop software use implementation include java program language tool matlab prot eg e etc ten question computer graphics sixty answer question use test result analyze conclude result accurate use ontology
address question answer task real world image set visual turing test combine latest advance image representation natural language process propose ask neurons scalable jointly train end end formulation problem contrast previous efforts face multi modal problem language output answer condition visual natural language input image question provide additional insights problem analyze much information contain language part provide new human baseline study human consensus relate ambiguities inherent challenge task propose two novel metrics collect additional answer extend original daquar dataset daquar consensus moreover also extend analysis vqa large scale question answer image dataset investigate particular design choices show importance stronger visual model time achieve strong performance model still use global image representation finally base analysis refine ask neurons daquar also lead better performance challenge task
recent advance conditional recurrent language model mainly focus network architectures eg attention mechanism learn algorithms eg schedule sample sequence level train novel applications eg image video description generation speech recognition etc hand notice decode algorithms strategies investigate much become standard use greedy beam search paper propose novel decode strategy motivate earlier observation nonlinear hide layer deep neural network stretch data manifold propose strategy embarrassingly parallelizable without communication overhead improve exist decode algorithm extensively evaluate attention base neural machine translation task en cz translation
categorical compositional distributional discocat model mean rigorously connect distributional semantics pregroup grammars find variety applications computational linguistics abstract standpoint discocat paradigm predicate construction map syntax categorical semantics work present concrete construction one map toy model syntax corpora annotate constituent structure tree categorical semantics take place category free r semimodules involutive commutative semiring r
since late 1990s speech company begin provide customer service software market people get use speak machine people interact often voice gesture control machine expect machine recognize different emotions understand high level communication feature humor sarcasm intention order make communication possible machine need empathy module extract emotions human speech behavior decide correct response robot although research empathetic robots still early stage describe approach use signal process techniques sentiment analysis machine learn algorithms make robots understand human emotion propose zara supergirl prototype system empathetic robots software base virtual android animate cartoon character present screen get smarter empathetic deep learn algorithms gather data learn paper present work far areas deep learn emotion sentiment recognition well humor recognition hope explore future direction android development help improve people live
paper reflexion computability natural language semantics contain new model new result formal semantics natural language rather computational analysis logical model algorithms currently use natural language semantics define map statement logical formulas formulas statement ambiguous argue long possible world semantics leave one compute semantic representations give statement include aspects lexical mean also discuss algorithmic complexity process
give set document specific domain eg medical research journals automatically build knowledge graph kg domain automatic identification relations schemas ie type signature arguments relations eg undergopatient surgery important first step towards goal refer problem relation schema induction rsi paper propose schema induction use couple tensor factorization sictf novel tensor factorization method relation schema induction sictf factorize open information extraction openie triple extract domain corpus along additional side information principled way induce relation schemas best knowledge first application tensor factorization rsi problem extensive experiment multiple real world datasets find sictf accurate state art baselines also significantly faster 14x faster
mental illness one press public health issue time counsel psychotherapy effective treatments knowledge conduct successful counsel conversations limit due lack large scale data label outcomes conversations paper present large scale quantitative study discourse text message base counsel conversations develop set novel computational discourse analysis methods measure various linguistic aspects conversations correlate conversation outcomes apply techniques sequence base conversation model language model comparisons message cluster psycholinguistics inspire word frequency analyse discover actionable conversation strategies associate better conversation outcomes
consider problem recognize textual entailment within information retrieval context must simultaneously determine relevancy well degree entailment individual piece evidence determine yes answer binary natural language question compare several variants neural network sentence embeddings set decision make base evidence vary relevance propose basic model integrate evidence entailment show joint train sentence embeddings model relevance entailment feasible even explicit per evidence supervision show importance evaluate strong baselines also demonstrate benefit carry text comprehension model train unrelated task small datasets research motivate primarily new open dataset introduce consist binary question news base evidence snippets also apply propose relevance entailment model similar task rank multiple choice test answer evaluate preliminary dataset school test question well standard mctest dataset improve neural model state art
twitter become one main source news many people real world events emergencies unfold twitter abuzz hundreds thousands stories events stories harmless others could potentially life save source malicious rumor thus critically important able efficiently track stories spread twitter events paper present novel semi automatic tool enable users efficiently identify track stories real world events twitter run user study twenty-five participants demonstrate compare conventional methods tool increase speed accuracy users track stories real world events
rise popularity public social media micro blogging service notably twitter people find venue hear hear peer without intermediary consequence aid public nature twitter political scientists potentially mean analyse understand narratives organically form spread decline among public political campaign however volume diversity conversation twitter combine noisy idiosyncratic nature make hard task thus advance data mine language process techniques require process analyse data paper present evaluate technical framework base recent advance deep neural network identify analyse election relate conversation twitter continuous longitudinal basis model detect election relate tweet f score ninety-two categorize tweet twenty-two topics f score ninety
ever grow number users account multiple social media network sit consequently increase interest match user account profile across different social network order create aggregate profile users paper present model digital stylometry method match users stylometry inspire techniques experiment linguistic temporal combine temporal linguistic model match user account use standard novel techniques use publicly available data best model combine temporal linguistic one able correctly match account thirty-one five thousand, six hundred and twelve distinct users across twitter facebook
rise popularity ubiquity twitter make sentiment analysis tweet important well cover area research however one hundred and forty character limit impose tweet make hard use standard linguistic methods sentiment classification hand tweet lack structure make sheer volume rich metadata metadata include geolocation temporal author information hypothesize sentiment dependent contextual factor different locations time author different emotional valences paper explore hypothesis utilize distant supervision collect millions label tweet different locations time author use data analyse variation tweet sentiments across different author time locations explore understand relationship variables sentiment use bayesian approach combine variables standard linguistic feature n grams create twitter sentiment classifier combine classifier outperform purely linguistic classifier show integrate rich contextual information available twitter sentiment classification promise direction research
review websites tripadvisor yelp allow users post online review various businesses products service recently show significant influence consumer shop behaviour online review typically consist free form text star rat five problem predict user star rat product give user text review product call review rat prediction lately become popular albeit hard problem machine learn paper treat review rat prediction multi class classification problem build sixteen different prediction model combine four feature extraction methods unigrams ii bigrams iii trigrams iv latent semantic index four machine learn algorithms logistic regression ii naive bay classification iii perceptrons iv linear support vector classification analyse performance sixteen model come best model predict rat review use dataset provide yelp train test model
microblogging platforms twitter provide active communication channel mass convergence emergency events earthquakes typhoons sudden onset crisis situation affect people post useful information twitter use situational awareness humanitarian disaster response efforts process timely effectively process social media information pose multiple challenge parse noisy brief informal message learn information categories incoming stream message classify different class among others one basic necessities many task availability data particular human annotate data paper present human annotate twitter corpora collect nineteen different crises take place two thousand and thirteen two thousand and fifteen demonstrate utility annotations train machine learn classifiers moreover publish first largest word2vec word embeddings train fifty-two million crisis relate tweet deal tweet language issue present human annotate normalize lexical resources different lexical variations
sequential data often possess hierarchical structure complex dependencies subsequences find utterances dialogue effort model kind generative process propose neural network base generative architecture latent stochastic variables span variable number time step apply propose model task dialogue response generation compare recent neural network architectures evaluate model performance automatic evaluation metrics carry human evaluation experiment demonstrate model improve upon recently propose model latent variables facilitate generation long output maintain context
community structure essential social communications individuals belong community much actively interact communicate different communities within human society name game hand social communication model simulate process learn name object within community humans individuals generally reach global consensus asymptotically iterative pair wise conversations underlie network indicate relationships among individuals paper three typical topologies namely random graph small world scale free network employ embed multi local world community structure study name game simulations show one convergence process global consensus get slower community structure become prominent eventually might fail two inter community connections sufficiently dense neither number size communities affect convergence process three different topologies average node degree local cluster individuals obstruct prohibit global consensus take place result reveal role local communities global name game social network study
present novel method hierarchical topic detection topics obtain cluster document multiple ways specifically model document collections use class graphical model call hierarchical latent tree model hltms variables bottom level hltm observe binary variables represent presence absence word document variables level binary latent variables lowest latent level represent word co occurrence pattern higher level represent co occurrence pattern level latent variable give soft partition document document cluster partition interpret topics latent variables high level hierarchy capture long range word co occurrence pattern hence give thematically general topics low level hierarchy capture short range word co occurrence pattern give thematically specific topics unlike lda base topic model hltms refer document generation process use word variables instead token variables use tree structure model relationships topics word conducive discovery meaningful topics topic hierarchies
introduce openxbow open source toolkit generation bag word bow representations multimodal input bow principle word histograms first use feature document classification idea easily adapt eg acoustic visual low level descriptors introduce prior step vector quantisation openxbow toolkit support arbitrary numeric input feature text input concatenate compute subbags final bag provide variety extensions options knowledge openxbow first publicly available toolkit generation crossmodal bag word capabilities tool exemplify two sample scenarios time continuous speech base emotion recognition sentiment analysis tweet improve result feature representation form observe
propose interactive multimodal framework language learn instead passively expose large amount natural text learners implement fee forward neural network engage cooperative referential game start tabula rasa setup thus develop language need communicate order succeed game preliminary experiment provide promise result also suggest important ensure agents train way develop adhoc communication code effective game play
memory network neural network explicit memory component read write network memory often address soft way use softmax function make end end train backpropagation possible however computationally scalable applications require network read extremely large memories hand well know hard attention mechanisms base reinforcement learn challenge train successfully paper explore form hierarchical memory network consider hybrid hard soft attention memory network memory organize hierarchical structure read do less computation soft attention flat memory also easier train hard attention flat memory specifically propose incorporate maximum inner product search mips train inference procedures hierarchical memory network explore use various state art approximate mips techniques report result simplequestions challenge large scale factoid question answer task
report present general model architecture information systems speech recognition children present model speech data stream work result study present vein architectural model show research need focus acoustic phonetic model order improve quality children speech recognition sustainability systems noise change transmission environment another important aspect development accurate algorithms model spontaneous child speech
report present process plan design development database speak children speech whose native language bulgarian propose model design children age four six without speech disorder reflect specific capabilities age children read sustain concentration emotional etc aim unite media information accompany record process speak speech thereby facilitate work researchers field speech recognition database use development systems children speech recognition children speech synthesis systems game allow voice control etc result propose model prototype system speech recognition present
use top rank document response query show effective approach improve quality query translation dictionary base cross language information retrieval paper propose new method dictionary base query translation base dimension projection embed vectors pseudo relevant document source language equivalents target language end first learn low dimensional vectors word pseudo relevant collections separately aim find query dependent transformation matrix vectors translation pair appear collections next step representation query term project target language use softmax function query dependent translation model build finally model use query translation experiment four clef collections french spanish german italian demonstrate propose method outperform word embed baseline base bilingual shuffle number competitive baselines propose method reach eighty-seven performance machine translation mt short query considerable improvements verbose query
automatic extraction effect relationships natural language texts challenge open problem artificial intelligence early attempt solution use manually construct linguistic syntactic rule small domain specific data set however advent big data availability affordable compute power recent popularization machine learn paradigm tackle problem slowly shift machine expect learn generic causal extraction rule label data minimal supervision domain independent manner paper provide comprehensive survey causal relation extraction techniques paradigms analyse relative strengths weaknesses recommendations future work
propose novel extension encoder decoder framework call review network review network generic enhance exist encoder decoder model paper consider rnn decoders cnn rnn encoders review network perform number review step attention mechanism encoder hide state output think vector review step think vectors use input attention mechanism decoder show conventional encoder decoders special case framework empirically show framework improve state art encoder decoder systems task image caption source code caption
previous study open information extraction open ie mainly base extraction pattern manually define pattern automatically learn large corpus however approach limit grasp context sentence fail capture implicit relations paper address problem follow methods first exploit long short term memory lstm network extract higher level feature along shortest dependency paths connect headwords relations arguments path level feature lstm network provide useful clue regard contextual information validity arguments second construct sample train lstm network without need manual label particular feedback negative sample pick highly negative sample among non positive sample model train positive sample experimental result show approach produce precise abundant extractions state art open ie systems best knowledge first work apply deep learn open ie
developers often wonder implement certain functionality eg parse xml file use apis obtain api usage sequence base api relate natural language query helpful regard give query exist approach utilize information retrieval model search match api sequence approach treat query apis bag word ie keyword match word word alignment lack deep understand semantics query propose deepapi deep learn base approach generate api usage sequence give natural language query instead bag word assumption learn sequence word query sequence associate apis deepapi adapt neural language model name rnn encoder decoder encode word sequence user query fix length context vector generate api sequence base context vector also augment rnn encoder decoder consider importance individual apis empirically evaluate approach seven million annotate code snippets collect github result show approach generate largely accurate api sequence outperform relate approach
ensembling methods well know improve prediction accuracy however limit sense discriminate among component model effectively paper propose stack auxiliary feature learn fuse relevant information multiple systems improve performance auxiliary feature enable stacker rely systems agree output also provenance output demonstrate approach three different difficult problems cold start slot fill tri lingual entity discovery link imagenet object detection task obtain new state art result first two task substantial improvements detection task thus verify power generality approach
paper present systems develop lium cvc wmt16 multimodal machine translation challenge explore various comparative methods namely phrase base systems attentional recurrent neural network model train use monomodal multimodal data also perform human evaluation order estimate usefulness multimodal data human machine translation image description generation systems obtain best result task accord automatic evaluation metrics bleu meteor
technical report detail several improvements visual concept detector bank build image multilingual visual sentiment ontology mvso detector bank train detect total nine thousand, nine hundred and eighteen sentiment bias visual concepts six major languages english spanish italian french german chinese original mvso release adjective noun pair anp detectors train six languages use alexnet style architecture fine tune deepsentibank extensive set experiment parameter tune train run detail release higher accuracy model detect anps across six languages image pool set original release use modern architecture googlenet provide comparable better performance reduce network parameter cost addition since image pool mvso corrupt user noise social interactions partition sub corpus mvso image base tag restrict query higher fidelity label show result higher fidelity label higher perform alexnet style anp detectors train use tag restrict image subset compare model full corpus release newly train model public research use along list tag restrict image mvso dataset
attention mechanisms recently introduce deep learn various task natural language process computer vision despite popularity correctness implicitly learn attention map assess qualitatively visualization several examples paper focus evaluate improve correctness attention neural image caption model specifically propose quantitative evaluation metric consistency generate attention map human annotations use recently release datasets alignment regions image entities caption propose novel model different level explicit supervision learn attention map train supervision strong alignment regions caption entities available weak object segment categories provide show popular flickr30k coco datasets introduce supervision attention map train solidly improve attention correctness caption quality show promise make machine perception human like
specialize dictionaries use understand concepts specific domains especially concepts part general vocabulary mean differ ordinary languages first step create specialize dictionary involve detect characteristic vocabulary domain question classical methods detect vocabulary involve gather domain corpus calculate statistics term find compare statistics background general language corpus term find significantly often specialize corpus background corpus candidates characteristic vocabulary domain present two tool direct crawler distributional semantics package use together circumvent need background corpus tool available web
paper tackle reduction redundant repeat generation often observe rnn base encoder decoder model basic idea jointly estimate upper bind frequency target vocabulary encoder control output word base estimation decoder method show significant improvement strong rnn base encoder decoder baseline achieve best result abstractive summarization benchmark
introduce multiresolution recurrent neural network extend sequence sequence framework model natural language generation two parallel discrete stochastic process sequence high level coarse tokens sequence natural language tokens many ways estimate learn high level coarse tokens argue simple extraction procedure sufficient capture wealth high level discourse semantics procedure allow train multiresolution recurrent neural network maximize exact joint log likelihood sequence contrast standard log likelihood objective wrt natural language tokens word perplexity optimize joint log likelihood bias model towards model high level abstractions apply propose model task dialogue response generation two challenge domains ubuntu technical support domain twitter conversations ubuntu model outperform compete approach substantial margin achieve state art result accord automatic evaluation metrics human evaluation study twitter model appear generate relevant topic responses accord automatic evaluation metrics finally experiment demonstrate propose model adept overcome sparsity natural language better able capture long term structure
word embed map word low dimensional continuous embed space exploit local word collocation pattern small context window hand topic model map document onto low dimensional topic space utilize global word collocation pattern document two type pattern complementary paper propose generative topic embed model combine two type pattern model topics represent embed vectors share across document probability word influence local context topic variational inference method yield topic embeddings well topic mix proportion document jointly represent document low dimensional continuous space two document classification task method perform better eight exist methods fewer feature addition illustrate example method generate coherent topics even base one document
layer wise relevance propagation lrp recently propose technique explain predictions complex non linear classifiers term input variables paper apply lrp first time natural language process nlp precisely use explain predictions convolutional neural network cnn train topic categorization task analysis highlight word relevant specific prediction cnn compare technique standard sensitivity analysis qualitatively quantitatively use word delete perturbation experiment pca analysis various visualizations experiment validate suitability lrp explain cnn predictions also line result report recent image classification study
textual review become prominent many recommendation base systems automate frameworks provide relevant visual cue text review picture available new form task confront data mine machine learn researchers suggestions picture relevant content review could significantly benefit users increase effectiveness review propose deep learn base framework automatically one tag image available review dataset two generate caption image one three enhance review recommend relevant image might upload correspond reviewer evaluate propose framework use yelp challenge dataset subset image particular dataset correctly caption majority picture associate text moreover map review image image correspond business tag picture take though overall data set unavailability crucial piece require map make problem recommend image review major challenge qualitative quantitative evaluations indicate propose framework provide high quality enhancements automatic caption tag recommendation map review image
people observe assortatively connect set traits phenomenon term assortative mix sometimes homophily quantify assortativity coefficient social network uncover exact cause strong assortative mix find social network research challenge among main suggest cause sociology tendency similar individuals connect often refer homophily social influence among already connect individuals important question researchers practice tackle present understand exact mechanisms interplay tendencies underlie social network structure namely addition mention assortativity coefficient several static temporal network properties substructures link tendencies homophily social influence social network herein investigate concretely tackle computer mediate textitcommunication network base twitter mention particular type assortative mix infer semantic feature communication content term textitsemantic homophily work best knowledge first offer depth analysis semantic homophily communication network interplay quantify diverse level semantic homophily identify semantic aspects drivers observe homophily show insights temporal evolution finally present intricate interplay communication network twitter analyze mechanisms increase understand semantic aspects shape shape human computer mediate communication
smallest grammar problem problem find smallest context free grammar generate exactly one give sequence never successfully apply grammatical inference investigate reason propose extend formulation seek minimize non recursive grammars instead straight line program addition provide efficient algorithms approximate minimization problem class grammars empirical evaluation show able find smaller model current best approximations smallest grammar problem standard benchmarks infer rule capture much better syntactic structure natural language
neural network nn achieve state art performance various applications unfortunately applications train data insufficient often prone overfitting one effective way alleviate problem exploit bayesian approach use bayesian neural network bnn another shortcoming nn lack flexibility customize different distributions weight neurons accord data often do probabilistic graphical model address problems propose class probabilistic neural network dub natural parameter network npn novel lightweight bayesian treatment nn npn allow usage arbitrary exponential family distributions model weight neurons different traditional nn bnn npn take distributions input go layer transformation produce distributions match target output distributions bayesian treatment efficient backpropagation bp perform learn natural parameters distributions weight neurons output distributions layer byproducts may use second order representations associate task link prediction experiment real world datasets show npn achieve state art performance
hybrid methods utilize content rat information commonly use many recommender systems however use either handcraft feature bag word representation surrogate content information neither effective natural enough address problem develop collaborative recurrent autoencoder crae denoising recurrent autoencoder drae model generation content sequence collaborative filter cf set model generalize recent advance recurrent deep learn iid input non iid cf base input provide new denoising scheme along novel learnable pool scheme recurrent autoencoder first develop hierarchical bayesian model drae generalize cf set synergy denoising cf enable crae make accurate recommendations learn fill blank sequence experiment real world datasets different domains citeulike netflix show jointly model order aware generation sequence content information perform cf rat crae able significantly outperform state art recommendation task base rat sequence generation task base content information
find relate publish article important task science explosion new work biomedical domain become especially challenge exist methodologies use text similarity metrics identify whether two article relate however biomedical knowledge discovery hypothesis drive relate article may ones highest text similarities study first develop innovative crowd source approach build expert annotate document rank corpus use corpus gold standard evaluate approach use text similarity rank relatedness article finally develop evaluate new supervise model automatically rank relate scientific article result show author rank differ significantly rank text similarity base model train learn rank model subset annotate corpus find best supervise learn rank model svm rank significantly surpass state art baseline systems
generative model latent dirichlet allocation lda prove fruitful topic model often require detail assumptions careful specification hyperparameters model complexity issue compound try generalize generative model incorporate human input introduce correlation explanation corex alternative approach topic model assume underlie generative model instead learn maximally informative topics information theoretic framework framework naturally generalize hierarchical semi supervise extensions additional model assumptions particular word level domain knowledge flexibly incorporate within corex anchor word allow topic separability representation promote minimal human intervention across variety datasets metrics experiment demonstrate corex produce topics comparable quality produce unsupervised semi supervise variants lda
natural language interface exploit conceptual simplicity naturalness language create high level user friendly communication channel humans machine one promise applications interfaces generate visual interpretations semantic content give natural language visualize either static scene dynamic animation survey discuss requirements challenge develop systems report twenty-six graphical systems exploit natural language interfaces address artificial intelligence visualization aspects work serve frame reference researchers enable advance field
categorical compositional distributional model natural language provide conceptually motivate procedure compute mean sentence give grammatical structure mean word approach outperform model mainstream empirical language process task however recently lack crucial feature lexical entailment distributional model mean paper solve problem entailment categorical compositional distributional semantics take advantage abstract categorical framework allow us vary choice model enable introduction notion entailment exploit ideas categorical semantics partial knowledge quantum computation new model language use density matrices introduce novel robust grade order capture entailment strength concepts grade measure emerge general framework approximate entailment induce commutative monoid quantum logic embed grade order main theorem show entailment strength lift compositionally sentence level give lower bind sentence entailment describe essential properties grade entailment continuity provide procedure calculate entailment strength
widespread integration cameras hand hold head wear devices well ability share content online enable large diverse visual capture world millions users build collectively every day envision image well associate meta information gps coordinate timestamps form collective visual memory query automatically take ever change context mobile users account first step towards vision work present xplore ego novel media retrieval system allow users query dynamic database image videos use spatio temporal natural language query evaluate system use new dataset real user query well usability study one key find considerable amount inter user variability example resolution spatial relations natural language utterances show retrieval system cope variability use personalisation online learn base retrieval formulation
computation classically study term automata formal languages algorithms yet relation neural dynamics symbolic representations operations still unclear traditional eliminative connectionism therefore suggest unique perspective central issue would like refer transparent connectionism propose account symbolic computation implement neural substrates study first introduce new model dynamics symbolic space versatile shift show support real time simulation range automata show goedelization versatile shift define nonlinear dynamical automata dynamical systems evolve vectorial space finally present map nonlinear dynamical automata recurrent artificial neural network map define architecture characterize granular modularity data symbolic operations control distinguishable activation space also spatially localizable network maintain distribute encode symbolic representations result network simulate automata real time program directly absence network train discuss unique characteristics architecture consequences present two examples design central pattern generator finite state locomotive controller ii creation network simulate system interactive automata support parse garden path sentence investigate psycholinguistics experiment
paper introduce new method discrimination document give different script document map uniformly cod text numerical value derive position letter text line base typographical characteristics code consider gray level accordingly cod text determine one image texture analysis run length statistics local binary pattern perform define feature vectors represent script content document modify cluster approach employ document feature vector group document write script experimentation perform two custom orient databases historical document old cyrillic angular round glagolitic well antiqua fraktur script demonstrate superiority propose method respect well know methods state art
together development accurate methods computer vision natural language understand holistic architectures answer question content real world image emerge tutorial build neural base approach answer question image base tutorial two datasets mostly daquar bite vqa small tweak model present achieve competitive performance datasets fact among best methods use combination lstm global full frame cnn representation image hope read tutorial reader able use deep learn frameworks keras introduce kraino build various architectures lead performance improvement challenge task
modern automatic speech recognition asr systems need robust acoustic variability arise environmental speaker channel record condition ensure robustness variability challenge modern day neural network base asr systems especially type variability see train attempt address problem encourage neural network acoustic model learn invariant feature representations use ideas recent research image generation use generative adversarial network domain adaptation ideas extend adversarial gradient base train recent work ganin et al propose use adversarial train image domain adaptation use intermediate representation main target classification network deteriorate domain classifier performance separate neural network work focus investigate neural architectures produce representations invariant noise condition asr evaluate propose architecture aurora four task popular benchmark noise robust asr show method generalize better standard multi condition train especially noise categories see train
build neural network query knowledge base table natural language emerge research topic deep learn executor table query typically require multiple step execution query may complicate structure previous study researchers develop either fully distribute executors symbolic executors table query distribute executor train end end fashion weak term execution efficiency explicit interpretability symbolic executor efficient execution difficult train especially initial stag paper propose couple distribute symbolic execution natural language query symbolic executor pretrained distribute executor intermediate execution result step step fashion experiment show approach significantly outperform distribute symbolic executors exhibit high accuracy high learn efficiency high execution efficiency high interpretability
despite grow importance multilingual aspect web search appropriate offline metrics evaluate quality propose far time personal language preferences regard intents query approach translate multilingual search problem particular task search diversification furthermore standard intent aware approach could adopt build diversify metric multilingual search basis classical ir metric err intent aware approach estimate user satisfaction user behavior model show however underlie user behavior model realistic multilingual case produce intent aware metric appropriately estimate user satisfaction develop novel approach build intent aware user behavior model overcome limitations convert quality metrics better correlate standard online metrics user satisfaction
current mainstream approach train natural language systems expose large amount text passive learn problematic interest develop interactive machine conversational agents propose framework language learn rely multi agent communication study learn context referential game game sender receiver see pair image sender tell one target allow send message fix arbitrary vocabulary receiver receiver must rely message identify target thus agents develop language interactively need communicate show two network simple configurations able learn coordinate referential game explore make change game environment word mean induce game better reflect intuitive semantic properties image addition present simple strategy ground agents code natural language necessary step towards develop machine able communicate humans productively
big data trend enforce data centric systems continuous fast data stream recent years real time analytics stream data form new research field aim answer query happen negligible delay real challenge real time stream data process impossible store instance data therefore online analytical algorithms utilize perform real time analytics pre process data perform way short summary stream store main memory addition due high speed arrival average process time instance data way incoming instance lose without capture lastly learner need provide high analytical accuracy measure sentinel distribute system write java aim solve challenge enforce process learn process do distribute form sentinel build top apache storm distribute compute platform sentinels learner vertical hoeffding tree parallel decision tree learn algorithm base vfdt ability enable parallel classification distribute environments sentinel also use spacesaving keep summary data stream store summary synopsis data structure application sentinel twitter public stream api show result discuss
recent approach base artificial neural network anns show promise result short text classification however many short texts occur sequence eg sentence document utterances dialog exist ann base systems leverage precede short texts classify subsequent one work present model base recurrent neural network convolutional neural network incorporate precede short texts model achieve state art result three different datasets dialog act prediction
many model purpose detect occurrence significant events financial systems task provide qualitative detail developments usually well automate present deep learn approach detect relevant discussion text extract natural language descriptions events supervise small set event information comprise entity name date model leverage unsupervised learn semantic vector representations extensive text data demonstrate applicability study financial risk base news 66m article particularly bank distress government interventions two hundred and forty-three events indices signal level bank stress relate report entity level aggregate national european level couple explanations thus exemplify text timely widely available descriptive data serve useful complementary source information financial systemic risk analytics
researchers often summarize work form posters posters provide coherent efficient way convey core ideas scientific paper generate good scientific poster however complex time consume cognitive task since posters need readable informative visually aesthetic paper first time study challenge problem learn generate posters scientific paper end data drive framework utilize graphical model propose specifically give content display key elements good poster include panel layout attribute panel learn infer data give infer layout attribute composition graphical elements within panel synthesize learn validate model collect make public poster paper dataset consist scientific paper correspond posters exhaustively label panel attribute qualitative quantitative result indicate effectiveness approach
feature extraction gain increase attention field machine learn order detect pattern extract information predict future observations big data urge informative feature crucial process extract feature highly link dimensionality reduction imply transformation data sparse high dimensional space higher level meaningful abstractions dissertation employ neural network distribute paragraph representations latent dirichlet allocation capture higher level feature paragraph vectors although neural network distribute paragraph representations consider state art extract paragraph vectors show quick topic analysis model latent dirichlet allocation provide meaningful feature evaluate two methods cmu movie summary corpus collection twenty-five thousand, two hundred and three movie plot summaries extract wikipedia finally approach use k nearest neighbor discover similar movies plot project representations use distribute stochastic neighbor embed depict context similarities similarities express movie distance use movies recommendation recommend movies approach compare recommend movies imdb use collaborative filter recommendation approach show two model could constitute either alternative supplementary recommendation approach
public debate common platform present juxtapose diverge view important issue work propose methodology track ideas flow participants throughout debate use approach case study oxford style debate competitive format winner determine audience vote show outcome debate depend aspects conversational flow particular find winners tend make better use debate interactive component losers actively pursue opponents point rather promote ideas course conversation
group discussions essential organize every aspect modern life faculty meet senate debate grant review panel papal conclaves costly term time organization effort group discussions commonly see way reach better decisions compare solutions require coordination individuals eg vote discussion sum become greater part however assumption irrefutable anecdotal evidence wasteful discussions abound experiment find thirty discussions unproductive propose framework analyze conversational dynamics order determine whether give task orient discussion worth exploit conversational pattern reflect flow ideas balance participants well linguistic choices apply framework conversations naturally occur online collaborative world exploration game develop deploy support research use set show linguistic cue conversational pattern extract first twenty second team discussion predictive whether wasteful productive one