paper describe extrans answer extraction system answer extraction ae aim retrieve exact passages document directly answer give user question ae ambitious information retrieval information extraction retrieval result phrase entire document query may arbitrarily specific less ambitious full fledge question answer answer generate knowledge base look text document current version extrans able parse unedited unix man page derive logical form sentence user query also translate logical form theorem prover retrieve relevant phrase present selective highlight context
study distributional similarity measure purpose improve probability estimation unseen cooccurrences contributions three fold empirical comparison broad range measure classification similarity function base information incorporate introduction novel function superior evaluate potential proxy distributions
thesis present attempt use syntactic structure natural language improve language model speech recognition structure language model merge techniques automatic parse language model use original probabilistic parameterization shift reduce parser maximum likelihood reestimation procedure belong class expectation maximization algorithms employ train model experiment wall street journal switchboard broadcast news corpora show improvement perplexity word error rate word lattice rescoring standard three gram language model significance thesis lie present original approach language model use hierarchical syntactic structure natural language improve current three gram model techniques large vocabulary speech recognition
new language model speech recognition inspire linguistic analysis present model develop hide hierarchical structure incrementally use extract meaningful information word history thus enable use extend distance dependencies attempt complement locality currently use n gram markov model model probabilistic parametrization reestimation algorithm model parameters set experiment mean evaluate potential speech recognition present
new language model speech recognition inspire linguistic analysis present model develop hide hierarchical structure incrementally use extract meaningful information word history thus enable use extend distance dependencies attempt complement locality currently use trigram model structure language model probabilistic parameterization performance two pass speech recognizer present experiment switchboard corpus show improvement perplexity word error rate conventional trigram model
new language model speech recognition present model develop hide hierarchical syntactic like structure incrementally use extract meaningful information word history thus complement locality currently use trigram model structure language model slm performance two pass speech recognizer lattice decode present experiment wsj corpus show improvement perplexity ppl word error rate wer conventional trigram model
text process systems expand scope require ever larger lexicons along parse capability discriminate among many sense word exist systems incorporate subtleties mean lexicons ordinary dictionaries contain information largely untapped content dictionaries scrutinize reveal many requirements must satisfy represent mean develop semantic parsers requirements identify research design find primitive verb concepts requirements outline general procedures satisfy use ordinary dictionaries describe illustrate build frame examine definitions change use hypernym definitions
new word usage measure propose base psychophysical relations allow reveal word degree importance make basic dictionaries sublanguages
trigrams n tag tnt efficient statistical part speech tagger contrary claim find elsewhere literature argue tagger base markov model perform least well current approach include maximum entropy framework recent comparison even show tnt perform significantly better test corpora describe basic model tnt techniques use smooth handle unknown word furthermore present evaluations two corpora
customer care technical domains increasingly base e mail communication allow reproduction approve solutions identify customer problem often time consume problem space change new products launch paper describe new approach classification e mail request base shallow text process machine learn techniques implement within assistance system call center agents use commercial set
finite state method base leftmost longest match replacement present segment word graphemes convert graphemes phonemes small set hand craft conversion rule dutch achieve phoneme accuracy ninety-three accuracy system improve use transformation base learn phoneme accuracy best system use large set rule templates lazy variant brill algoritm train 40k word reach ninety-nine accuracy
rate occurrence word uniform vary document document despite observation parameters conventional n gram language model usually derive use assumption constant word rate paper investigate use variable word rate assumption model poisson distribution continuous mixture poissons present approach estimate relative frequencies word n grams take prior information occurrences account discount smooth scheme also consider use broadcast news task approach demonstrate reduction perplexity ten
paper describe method linear text segmentation twice accurate seven time fast state art reynar one thousand, nine hundred and ninety-eight inter sentence similarity replace rank local context boundary locations discover divisive cluster
paper discuss development trainable statistical model extract content television radio news broadcast particular concentrate statistical finite state model identify proper name name entities broadcast speech two model present first represent name class information word attribute second represent word word class class transition explicitly common n gram base formulation use model task name entity identification characterize relatively sparse train data issue relate smooth discuss experiment report use darpa nist hub 4e evaluation north american broadcast news
paper aim report development application computer model discourse analysis segmentation segmentation refer principled division texts contiguous constituents study look application number model analysis discourse computer segmentation procedure develop present investigation call lsm link set median apply three corpus three hundred texts three different genres result obtain application lsm procedure corpus compare segmentation carry random statistical analyse suggest lsm significantly outperform random segmentation thus indicate segmentation meaningful
paper present corpus base approach word sense disambiguation build ensemble naive bayesian classifiers base lexical feature represent co occur word vary size windows context despite simplicity approach empirical result disambiguate widely study nouns line interest show ensemble achieve accuracy rival best previously publish result
performance machine learn algorithms improve combine output different systems paper apply idea recognition noun phraseswe generate different classifiers use different representations data combine result vote techniques describe van halteren etal one thousand, nine hundred and ninety-eight manage improve best report performances standard data set base noun phrase arbitrary noun phrase
paper explore usefulness technique software engineer namely code instrumentation development large scale natural language grammars information usage grammar rule test sentence use detect untested rule redundant test sentence likely cause overgeneration result show less half large coverage grammar german actually test two large testsuites ten thirty test time redundant methodology apply see use grammar write knowledge testsuite compilation
paper report scalability answer extraction system extrans answer extraction system locate exact phrase document contain explicit answer user query answer extraction systems therefore convenient document retrieval systems situations user want find specific information limit time extrans perform answer extraction unix manpages construct combine available linguistic resources implement modules scratch resolution procedure minimal logical form user query minimal logical form manpage sentence find answer query answer display user together pointers respective manpages exact phrase contribute answer highlight paper show increase response time big issue scale system thirty five hundred document response time five hundred document still acceptable real time answer extraction system
reduplication central instance prosodic morphology particularly challenge state art computational morphology since involve copy part phonological string paper advocate finite state method combine enrich lexical representations via intersection implement copy proposal include resource conscious variant automata benefit existence lazy algorithms finally implementation complex case koasati present
paper describe system rank suspect answer natural language question process corpus query use new technique predictive annotation augment phrase texts label anticipate target certain kinds question give natural language question ir system return set match passages analyze rank accord various criteria describe paper provide evaluation techniques base result trec qanda evaluation system participate
three state art statistical parsers combine produce accurate parse well new bound achievable treebank parse accuracy two general approach present two combination techniques describe approach parametric non parametric model explore result parsers surpass best previously publish performance result penn treebank
bag boost two effective machine learn techniques apply natural language parse experiment use techniques trainable statistical parser describe best result system provide roughly large gain f measure double corpus size error analysis result boost technique reveal inconsistent annotations penn treebank suggest semi automatic method find inconsistent treebank annotations
popularity apply machine learn methods computational linguistics problems produce large supply trainable natural language process systems problems interest array shelf products downloadable code implement solutions use various techniques solutions develop independently observe errors tend independently distribute thesis concern approach capitalize situation sample problem domain penn treebank style parse machine learn community provide techniques combine output classifiers parser output structure interdependent classifications address discrepancy two novel strategies combine parsers use learn control switch parsers construct hybrid parse multiple parsers output shelf parsers develop intention perform well collaborative ensemble two techniques present produce ensemble parsers collaborate ensemble members create use underlie parser induction algorithm method produce complementary parsers loosely constrain choose algorithm
describe architecture implement speak natural language dialogue interfaces semi autonomous systems central idea transform input speech signal successive level representation correspond roughly linguistic knowledge dialogue knowledge domain knowledge final representation executable program simple script language equivalent subset cshell stage translation process input transform output produce byproduct meta output describe nature transformation perform show consistent use output meta output distinction permit simple perspicuous treatment apparently diverse topics include resolution pronouns correction user misconceptions optimization script methods describe concretely realize prototype speech interface simulation personal satellite assistant
describe architecture speak dialogue interfaces semi autonomous systems transform speech signal successive representations linguistic dialogue domain knowledge step produce output meta output describe transformation executable program simple script language final result output meta output distinction permit perspicuous treatment diverse task resolve pronouns correct user misconceptions optimize script
people develop something intend large broad coverage grammar usually specific goal mind sometimes goal cover corpus sometimes developers theoretical ideas wish investigate often work drive combination two main type goal tend happen community people work grammar start think phenomena central make serious efforts deal phenomena label marginal ignore long distinction central marginal become ingrain automatic people virtually stop think marginal phenomena practice way bring marginal things back focus look people compare one work paper take two large grammars xtag cle examine point view find case important things miss perspective offer grammar suggest simple practical ways fill hole turn please symmetry picture xtag good treatment complement structure cle extent lack conversely cle offer powerful general account adjuncts xtag grammar fully duplicate examine way grammar thing good find relevant methods quite easy port framework fact involve generalization systematization exist mechanisms
systems exist able compile unification grammars language model include speech recognizer far unclear whether non trivial linguistically principled grammars use purpose describe series experiment investigate question empirically incrementally construct grammar discover problems emerge successively larger versions compile finite state graph representations use language model medium vocabulary recognition task
describe statistical approach model dialogue act conversational speech ie speech act like units statement question backchannel agreement disagreement apology model detect predict dialogue act base lexical collocational prosodic cue well discourse coherence dialogue act sequence dialogue model base treat discourse structure conversation hide markov model individual dialogue act observations emanate model state constraints likely sequence dialogue act model via dialogue act n gram statistical dialogue grammar combine word n grams decision tree neural network model idiosyncratic lexical prosodic manifestations dialogue act develop probabilistic integration speech recognition dialogue model improve speech recognition dialogue act classification accuracy model train evaluate use large hand label database one thousand, one hundred and fifty-five conversations switchboard corpus spontaneous human human telephone speech achieve good dialogue act label accuracy sixty-five base errorful automatically recognize word prosody seventy-one base word transcripts compare chance baseline accuracy thirty-five human accuracy eighty-four small reduction word recognition error
identify whether utterance statement question greet forth integral effective automatic understand natural dialog little know however dialog act das automatically classify truly natural conversation study ask whether current approach use mainly word information could improve add prosodic information study base one thousand conversations switchboard corpus das hand annotate prosodic feature duration pause f0 energy speak rate automatically extract da train decision tree base feature infer tree apply unseen test data evaluate performance performance evaluate prosody model alone combine prosody model word information either true word output automatic speech recognizer overall classification task well three subtasks prosody make significant contributions classification feature specific analyse reveal although canonical feature f0 question important less obvious feature could compensate canonical feature remove finally task integrate prosodic model da specific statistical language model improve performance language model alone especially case recognize word result suggest das redundantly mark natural conversation variety automatically extractable prosodic feature could aid dialog process speech applications
criterion prune parameters n gram backoff language model develop base relative entropy original prune model show relative entropy result prune single n gram compute exactly efficiently backoff model relative entropy measure express relative change train set perplexity lead simple prune criterion whereby n grams change perplexity less threshold remove model experiment show production quality hub4 lm reduce twenty-six original size without increase recognition error also compare approach heuristic prune criterion seymore rosenfeld one thousand, nine hundred and ninety-six show approach interpret approximation relative entropy criterion experimentally approach select similar set n grams eighty-five overlap exact relative entropy criterion give marginally better performance
present three systems surface natural language generation trainable annotate corpora first two systems call nlg1 nlg2 require corpus mark domain specific semantic attribute last system call nlg3 require corpus mark semantic attribute syntactic dependency information systems attempt produce grammatical natural language phrase domain specific semantic representation nlg1 serve baseline system use phrase frequencies generate whole phrase one step nlg2 nlg3 use maximum entropy probability model individually generate word phrase systems nlg2 nlg3 learn determine word choice word order phrase present experiment generate phrase describe flight air travel domain
crucial step process speech audio data information extraction topic detection browse playback segment input sentence topic units speech segmentation challenge since cue typically present segment text headers paragraph punctuation absent speak language investigate use prosody information glean time melody speech task use decision tree hide markov model techniques combine prosodic cue word base approach evaluate performance two speech corpora broadcast news switchboard result show prosodic model alone perform par better word base statistical language model true automatically recognize word news speech prosodic model achieve comparable performance significantly less train data require hand label prosodic events across task corpora obtain significant improvement word model use probabilistic combination prosodic lexical information inspection reveal prosodic model capture language independent boundary indicators describe literature finally cue usage task corpus dependent example pause pitch feature highly informative segment news speech whereas pause duration word base cue dominate natural conversation
previous work frank satta one thousand, nine hundred and ninety-eight karttunen one thousand, nine hundred and ninety-eight show optimality theory gradient constraints generally finite state new finite state treatment gradient constraints present improve upon approximation karttunen one thousand, nine hundred and ninety-eight method turn exact compact syllabification analysis prince smolensky one thousand, nine hundred and ninety-three
finite state morphology general tradition two level xerox implementations prove successful production robust morphological analyzer generators include many large scale commercial systems however long recognize implementations serious limitations handle non concatenative phenomena describe new technique construct finite state transducers involve reapplying regular expression compiler output implement algorithm call compile replace technique prove useful handle non concatenative phenomena demonstrate malay full stem reduplication arabic stem interdigitation
paper describe new method construct minimal deterministic acyclic finite state automata set string traditional methods consist two phase first construct trie second one minimize approach construct minimal automaton single phase add new string one one minimize result automaton fly present general algorithm well specialization rely upon lexicographical order input string
paper describe new method combi bootstrap exploit exist taggers lexical resources annotation corpora new tagsets combi bootstrap use exist resources feature second level machine learn module train make map new tagset small sample annotate corpus material experiment show combi bootstrap integrate wide variety exist resources ii achieve much higher accuracy four hundred and forty-seven error reduction best single tagger ensemble tagger construct small train sample
describe formal model annotate linguistic artifacts derive application program interface api suite tool manipulate annotations abstract logical model provide range storage format promote reuse tool interact api focus first annotation graph graph model annotations linear signal text speech index intervals efficient database storage query techniques applicable note wide range exist annotate corpora map annotation graph model model generalize encompass wider variety linguistic signal include naturally occur phenomena record image video multi modal interactions etc well derive resources increasingly important engineer natural language process systems word list dictionaries align bilingual corpora etc conclude review current efforts towards implement key piece architecture
paper discuss challenge arise large speech corpora receive ever broaden range diverse distinct annotations two case study process present switchboard corpus telephone conversations tdt2 corpus broadcast news switchboard undergo two independent transcriptions various type additional annotation carry separate project disperse geographically chronologically tdt2 corpus also receive variety annotations directly create manage core group case issue arise involve propagation repair consistency reference ability integrate annotations different format level detail describe general framework whereby issue address successfully
model rank polysemantic distribution minimal number fit parameters offer ideal case parameter free description dependence basis one several immediate feature distribution possible
present robust approach link already exist lexical semantic hierarchies use constraint satisfaction algorithm relaxation label select among set candidates node target taxonomy best match node source taxonomy particular use map nominal part wordnet fifteen onto wordnet sixteen high precision low remain ambiguity
formal language techniques use past study autonomous dynamical systems however control systems new feature need distinguish information generate system input control show model framework control dynamical systems lead naturally formulation term context dependent grammars learn algorithm propose line generation grammar productions formulation use model control anomaly detection practical applications describe electromechanical drive grammatical interpolation techniques yield accurate result pattern detection capabilities language base formulation make promise technique early detection anomalies faulty behaviour
constraint base grammars principle serve major linguistic knowledge source parse generation surface generation start input semantics representations may vary across grammars many declarative grammars concept derivation implicitly build parse may thus interpretable generation algorithm show linguistically plausible semantic analyse severe problems semantic head drive approach generation shdg use sereal variant shdg disco grammar german source examples propose new general approach explicitly account interface grammar generation algorithm add control orient layer linguistic knowledge base reorganize semantics way suitable generation
grammatical relationships grs form important level natural language process different set grs useful different purpose therefore one may often time obtain small train corpus desire gr annotations small train corpus compare two systems use different learn techniques find difference minor effect larger factor english different gr length measure appear better suit find simple argument grs find modifier grs also find partition data may help memory base learn
statistical significance test differences value metrics like recall precision balance f score necessary part empirical natural language process unfortunately find set experiment many commonly use test often underestimate significance less likely detect differences exist different techniques underestimation come independence assumption often violate point useful test make assumption include computationally intensive randomization test
present methods evaluate human automatic taggers extend current practice three ways first show evaluate taggers assign multiple tag test instance even assign probabilities second show accommodate common property manually construct gold standards typically use objective evaluation namely often one correct answer third show measure performance set possible tag tree structure hierarchy illustrate methods use measure inter annotator agreement show compute kappa coefficient hierarchical tag set
use seven machine learn algorithms one task identify base noun phrase result process different system combination methods outperform best individual result apply seven learners best combinator majority vote top five systems standard data set manage improve best publish result data set
apply rule induction classifier combination meta learn stack classifiers problem bootstrapping high accuracy automatic annotation corpora pronunciation information task address paper consist generate phonemic representations reflect flemish dutch pronunciations word basis orthographic representation turn base actual speech record compare several possible approach achieve text pronunciation map task memory base learn transformation base learn rule induction maximum entropy model combination classifiers stack learn stack meta learners interest optimal accuracy obtain insight linguistic regularities involve far accuracy concern already high accuracy level ninety-three celex eighty-six fonilex word level single classifiers boost significantly additional error reductions thirty-one thirty-eight respectively use combination classifiers five use combination meta learners bring overall word level accuracy ninety-six dutch variant ninety-two flemish variant also show application machine learn methods indeed lead increase insight linguistic regularities determine variation two pronunciation variants study
data orient parse dop rank among best parse scheme pair state art parse accuracy psycholinguistic insight larger chunk syntactic structure relevant grammatical probabilistic units parse dop model however seem involve lot cpu cycle considerable amount double work bring concept multiple derivations necessary probabilistic process convincingly relate proper linguistic backbone however possible interpret dop model pattern match model try maximize size substructures construct parse rather probability parse emphasize memory base aspect dop model possible away multiple derivations open possibilities efficient viterbi style optimizations still retain acceptable parse accuracy enhance context sensitivity
temiar reduplication difficult piece prosodic morphology paper present first computational analysis temiar reduplication use novel finite state approach one level prosodic morphology originally develop walther 1999b two thousand review data basic tenets one level prosodic morphology analysis lay detail use notation fsa utilities finite state toolkit van noord one thousand, nine hundred and ninety-seven one important discovery approach one easily define regular expression operator ambiguously scan string leave rightward direction certain prosodic property yield elegant account base length dependent trigger reduplication find temiar
paper examine efficient predictive broad coverage parse without dynamic program contrast bottom methods depth first top parse produce partial parse fully connect tree span entire leave context kind non local dependency partial semantic interpretation principle read contrast two predictive parse approach top leave corner parse find viable addition find enhancement non local information improve parser accuracy also substantially improve search efficiency
leave corner transform remove leave recursion probabilistic context free grammars unification grammars permit simple top parse techniques use unfortunately grammars produce standard leave corner transform usually much larger original selective leave corner transform describe paper produce transform grammar simulate leave corner recognition user specify set original productions top recognition others combine two factorizations produce non leave recursive grammars much larger original
selectional restrictions semantic sortal constraints impose participants linguistic constructions capture contextually dependent constraints interpretation despite limitations selectional restrictions prove useful natural language applications use frequently word sense disambiguation syntactic disambiguation anaphora resolution give practical value explore two methods incorporate selectional restrictions hpsg theory assume reader familiar hpsg first method employ hpsg background feature constraint satisfaction component pipe line parser second method use subsorts referential indices block read violate selectional restrictions parse theoretically less satisfactory find second method particularly useful development practical systems
argue computational complexity associate estimation stochastic attribute value grammars reduce train upon informative subset full train set result use parse wall street journal corpus show circumstances possible obtain better estimation result use informative sample train upon available material experimentation demonstrate unlexicalised model gaussian prior reduce overfitting however model lexicalise contain overlap feature overfitting seem problem gaussian prior make minimal difference performance approach applicable situations infeasibly large number parse train set else recovery parse pack representation computationally expensive
generate semantic lexicons semi automatically could great time saver relative create hand paper present algorithm extract potential entries category line corpus base upon small set exemplars algorithm find correct term fewer incorrect ones previous work area additionally entries generate potentially provide broader coverage category would occur individual cod hand algorithm find many term include within wordnet many previous algorithms could view enhancer exist broad coverage resources
little attention pay comparison efficiency high accuracy statistical parsers paper propose one machine independent metric general enough allow comparisons across different parse architectures metric call events consider measure number events however define particular parser probability must calculate order find parse applicable single pass multi stage parsers discuss advantage metric demonstrate usefulness use compare two parsers differ several fundamental ways
log linear model provide statistically sound framework stochastic unification base grammars subgs stochastic versions kinds grammars describe two computationally tractable ways estimate parameters grammars train corpus syntactic analyse apply estimate stochastic version lexical functional grammar
paper describe method estimate conditional probability distributions parse unification base grammars utilize auxiliary distributions estimate mean show use incorporate information lexical selectional preferences gather source stochastic unification base grammars subgs apply estimator stochastic lexical functional grammar method general applicable stochastic versions hpsgs categorial grammars transformational grammars
develop example base method metonymy interpretation one advantage method hand build database metonymy necessary instead use examples form noun x noun noun noun x another advantage able interpret newly coin metonymic sentence use new corpus experiment metonymy interpretation obtain precision rate sixty-six use method
paper describe two new bunsetsu identification methods use supervise learn since japanese syntactic analysis usually do bunsetsu identification bunsetsu identification important analyze japanese sentence experiment compare four previously available machine learn methods decision tree maximum entropy method example base approach decision list two new methods use category exclusive rule new method use category exclusive rule highest similarity perform best
robertson two poisson information retrieve model use location category information construct framework use location category information two poisson model submit two systems base framework irex contest japanese language information retrieval contest hold japan one thousand, nine hundred and ninety-nine precision judgement measure score four thousand, nine hundred and twenty-six four thousand, eight hundred and twenty-seven highest value among fifteen team twenty-two systems participate irex contest describe systems comparative experiment do various parameters change experiment confirm effectiveness use location category information
paper describe outline method translate japanese temporal expressions english argue temporal expressions form special subset language best handle special module machine translation paper deal problems lexical idiosyncrasy well choice article prepositions within temporal expressions addition temporal expressions consider part larger structure question whether translate noun phrase adverbials address
present new approach stochastic model constraint base grammars base log linear model use estimation unannotated data techniques apply lfg grammar german evaluation exact match task yield eighty-six precision ambiguity rate fifty-four ninety precision subcat frame match ambiguity rate twenty-five experimental comparison train parsebank show ten gain train also new class base grammar lexicalization present show ten gain unlexicalized model
paper present use probabilistic class base lexica disambiguation target word selection method employ minimal precise contextual information disambiguation information provide target verb enrich condense information probabilistic class base lexicon use induction class fine tune verbal arguments do unsupervised manner base cluster techniques method show promise result evaluation real world translations
thesis present two approach rigorous mathematical algorithmic foundation quantitative statistical inference constraint base natural language process first approach call quantitative constraint logic program conceptualize clear logical framework present sound complete system quantitative inference definite clauses annotate subjective weight approach combine rigorous formal semantics quantitative inference base subjective weight efficient weight base prune constraint base systems second approach call probabilistic constraint logic program introduce log linear probability distribution proof tree constraint logic program algorithm statistical inference parameters properties probability model incomplete ie unparsed data possibility define arbitrary properties proof tree properties log linear probability model efficiently estimate appropriate parameter value permit probabilistic model arbitrary context dependencies constraint logic program usefulness ideas evaluate empirically small scale experiment find correct parse constraint base grammar addition address problem computational intractability calculation expectations inference task present various techniques approximately solve task moreover present approximate heuristic technique search probable analysis probabilistic constraint logic program
present novel machine learn techniques identification subcategorization information verbs czech compare three different statistical techniques apply problem show learn algorithm use discover previously unknown subcategorization frame czech prague dependency treebank algorithm use label dependents verb czech treebank either arguments adjuncts use techniques ar able achieve eighty-eight precision unseen parse text
describe conll two thousand share task divide text syntactically relate non overlap group word call text chunk give background information data set present general overview systems take part share task briefly discuss performance
anaphora resolution one major problems natural language process also one important task machine translation man machine dialogue solve problem use surface expressions examples surface expressions word sentence provide clue anaphora resolution examples linguistic data actually use conversations texts method use surface expressions examples practical method thesis handle almost kinds anaphora referential property number noun phrase ii noun phrase direct anaphora iii noun phrase indirect anaphora iv pronoun anaphora v verb phrase ellipsis
cop ambiguity recently receive lot attention natural language process work focus semantic representation ambiguous expressions paper complement work two ways first provide entailment relation language ambiguous expressions second give sound complete tableaux calculus reason statements involve ambiguous quantification calculus interleave partial disambiguation step step traditional deductive process minimize postpone branch proof process thereby increase efficiency
common wisdom bias stochastic grammars favor shorter derivations sentence harmful redress show common wisdom wrong stochastic grammars use elementary tree instead context free rule stochastic tree substitution grammars use data orient parse model grammars non probabilistic metric base shortest derivation outperform probabilistic metric atis ovis corpora obtain competitive result wall street journal corpus paper also contain first publish experiment dop wall street journal
present lfg dop parser use fragment lfg annotate sentence parse new sentence experiment verbmobil homecentre corpora show one viterbi n best search perform one hundred time faster monte carlo search achieve accuracy two dop hypothesis state parse accuracy increase increase fragment size confirm lfg dop three lfg dop relative frequency estimator perform worse discount frequency estimator four lfg dop significantly outperform tree dop evaluate tree structure
describe new framework distil information word lattices improve accuracy speech recognition obtain perspicuous representation set alternative hypotheses standard map decode approach recognizer output string word correspond path highest posterior probability give acoustics language model however even give optimal model map decoder necessarily minimize commonly use performance metric word error rate wer describe method explicitly minimize wer extract word hypotheses highest posterior probabilities word lattices change standard problem formulation replace global search large set sentence hypotheses local search small set word candidates addition improve accuracy recognizer method produce new representation set candidate hypotheses specify sequence word level confusions compact lattice format study properties confusion network examine use task lattice compression word spot confidence annotation reevaluation recognition hypotheses use higher level knowledge source
grammatical relationships grs form important level natural language process different set grs useful different purpose therefore one may often time obtain small train corpus desire gr annotations boost performance use small train corpus transformation rule learner use exist systems find relate type annotations
effective paradigm word sense disambiguation supervise learn seem stick knowledge acquisition bottleneck paper take depth study performance decision list two publicly available corpora additional corpus automatically acquire web use fine grain highly polysemous sense wordnet decision list show versatile state art technique experiment reveal among facts semcor acceptable seven precision polysemous word start point word system result dso corpus show highly polysemous word seven precision seem current state art limit hand independently construct hand tag corpora mutually useful corpus automatically acquire web show fail
paper deal exploitation dictionaries semi automatic construction lexicons lexical knowledge base final goal research enrich basque lexical database semantic information sense definitions semantic relations etc extract basque monolingual dictionary work present focus extraction semantic relations best characterise headword synonymy antonymy hypernymy relations mark specific relators derivation nominal verbal adjectival entries treat basque use morphological inflection mark case therefore semantic relations infer suffix rather prepositions approach combine morphological analyser surface syntax parse base constraint grammar prove successful highly inflect languages basque effort write rule actual process time dictionary low present extract forty-two thousand, five hundred and thirty-three relations leave two thousand, nine hundred and forty-three nine definitions without extract relation error rate extremely low twenty-two extract relations wrong
paper explore possibility exploit text world wide web order enrich concepts exist ontologies first method retrieve document www relate concept describe document collections use one construct topic signatures list topically relate word concept wordnet two build hierarchical cluster concepts word sense lexicalize give word overall goal overcome two shortcomings wordnet lack topical link among concepts proliferation sense topic signatures validate word sense disambiguation task good result improve hierarchical cluster use
paper revisit one sense per collocation hypothesis use fine grain sense distinctions two different corpora show hypothesis weaker fine grain sense distinctions seventy vs ninety-nine report earlier two way ambiguities also show one sense per collocation hold across corpora collocations vary one corpus follow genre topic variations explain low result perform word sense disambiguation across corpora fact demonstrate two independent corpora share relate genre topic word sense disambiguation result would better future work word sense disambiguation take account genre topic important parameters model
article describe algorithm reduce intermediate alphabets cascade finite state transducers fsts although method modify component fsts change overall relation describe whole cascade additional information special algorithm could decelerate process input require runtime two examples natural language process use illustrate effect algorithm size fsts alphabets fsts number arc symbols shrink considerably
paper propose method extract descriptions technical term web page order utilize world wide web encyclopedia use linguistic pattern html text structure extract text fragment contain term descriptions also use language model discard extraneous descriptions cluster method summarize resultant descriptions show effectiveness method way experiment
information retrieval research precision recall long use evaluate ir systems however give number retrieval systems resemble one another already available public valuable retrieve novel relevant document ie document retrieve exist systems view problem propose evaluation method favor systems retrieve many novel document possible also use method evaluate systems participate irex workshop
cross language information retrieval clir query document different languages need translation query document standardize common representation purpose use machine translation effective approach however computational cost prohibitive translate large scale document collections resolve problem propose two stage clir method first translate give query document language retrieve limit number foreign document second machine translate document user language rank base translation result also show effectiveness method way experiment use japanese query english technical document
paper explore usefulness technique software engineer code instrumentation development large scale natural language grammars information usage grammar rule test corpus sentence use improve grammar testsuite well adapt grammar specific genre result show less half large coverage grammar german actually test two large testsuites ten thirty test time redundant methodology apply see use grammar write knowledge testsuite compilation
besides temporal information explicitly available verbs adjuncts temporal interpretation text also depend general world knowledge default assumptions present theory describe relation one hand verbs tense adjuncts eventualities periods time represent relative temporal locations theory formulate logic practical implementation concepts describe ness schelkens et al show abductive resolution procedure use representation extract temporal information texts
texts natural language contain lot temporal information explicit implicit verbs temporal adjuncts carry explicit information full understand general world knowledge default assumptions take account present theory describe relation one hand verbs tense adjuncts eventualities periods time represent relative temporal locations allow interaction general world knowledge theory formulate extension first order logic practical implementation concepts describe van eynde two thousand and one schelkens et al two thousand show abductive resolution procedure use representation extract temporal information texts theory present extension verdoolaege et al two thousand adapt vaneynde two thousand and one simplify extend analysis adjuncts emphasis model construct
aim find minimal set fragment achieve maximal parse accuracy data orient parse experiment penn wall street journal treebank show count almost arbitrary fragment within parse tree important lead improve parse accuracy previous model test treebank isolate number dependency relations previous model neglect contribute higher parse accuracy
present non vacuous definition compositionality base idea combine minimum description length principle original definition compositionality mean whole function mean part new definition intuitive allow us distinguish compositional non compositional semantics idiomatic non idiomatic expressions ad hoc since make reference non intrinsic properties mean function like polynomial moreover allow us compare different mean function respect compositional bridge linguistic corpus base statistical approach natural language understand
recent issue linguistics philosophy kasmi pelletier one thousand, nine hundred and ninety-eight kandp westerstahl one thousand, nine hundred and ninety-eight criticize zadrozny one thousand, nine hundred and ninety-four argument semantics represent compositionally argument base upon zadrozny theorem every mean function encode function mu expression e specify language l recover mue ii mu homomorphism syntactic structure l interpretations l case primary motivation objections bring zadrozny argument view encode original mean function properly reflect synonymy relations posit language paper argue technical criticisms go particular prove mu properly encode synonymy relations ie two expressions synonymous compositional mean identical correct misconceptions function mu eg janssen one thousand, nine hundred and ninety-seven suggest reason semanticists anxious preserve compositionality significant constraint semantic theory mistakenly regard condition must satisfy theory sustain systematic connection mean expression mean part recent developments formal computational semantics show systematic theories mean need compositional
much research hard problem depth story understand computer perform start 1970s interest shift 1990s information extraction word sense disambiguation degree success achieve easier problems propose time return depth story understand paper examine shift away story understand discuss major problems build story understand system present possible solutions involve set interact understand agents provide pointers useful tool resources build story understand systems
since script propose one thousand, nine hundred and seventy inferencing mechanism ai natural language process program attempt build database script paper describe database lexicon script add thoughttreasure commonsense platform database provide follow information script sequence events roles prop entry condition result goals emotions place duration frequency cost english french word phrase link script concepts
paper study notion supposition encode non archimedean conditional probability reveal acceptance call indicative conditionals notion qualitative change view thus arise axiomatized compare standard notions like agm update applications follow field discuss one theory game decisions two causal model three non monotonic logic
paper report qaviar experimental automate evaluation system question answer applications goal research find automatically calculate measure correlate well human judge assessment answer correctness context question answer task qaviar judge response compute recall stem content word human generate answer key count answer correct exceed agiven recall threshold determine answer correctness predict qaviar agree human ninety-three ninety-five time forty-one question answer systems rank qaviar human assessors rank correlate kendall tau measure nine hundred and twenty compare correlation nine hundred and fifty-six human assessors data
recently argue naive bayesian classifier use filter unsolicited bulk e mail spam conduct thorough evaluation proposal corpus make publicly available contribute towards standard benchmarks time investigate effect attribute set size train corpus size lemmatization stop list filter performance issue previously explore introduce appropriate cost sensitive evaluation measure reach conclusion additional safety net need naive bayesian anti spam filter viable practice
speech become increasingly popular interface modality especially hand eye busy situations use keyboard mouse difficult however despite fact many hail speech inherently usable since everyone already know talk users speech input leave feel disappoint quality interaction clearly much work do design usable speak interfaces believe two major problems design speech interfaces namely people currently work design speech interfaces part interface designers therefore much experience usability issue chi community b speech interface modality vastly different properties modalities therefore require different usability measure
first discuss respective advantage language interaction virtual worlds use 3d image dialogue systems describe example verbal interaction system virtual reality ulysse ulysse conversational agent help user navigate virtual worlds design embed representation participant virtual conference respond positively motion order ulysse navigate user viewpoint behalf virtual world test carry discover users novices well experience ones difficulties move 3d environment agents ulysse enable user carry navigation motion would impossible classical interaction devices whole ulysse system strip skeleton architecture port vrml java prolog hope skeleton help design language applications virtual worlds
paper present semantic parse approach unrestricted texts semantic parse one major bottleneck natural language understand nlu systems usually require build expensive resources easily portable domains approach obtain case role analysis semantic roles verb identify order cover possible syntactic realisations verb system combine argument structure set general semantic label diatheses model combine system build set syntactic semantic pattern role case representation pattern build use approximate tree pattern match algorithm identify reliable pattern sentence pattern match perform syntactic semantic pattern feature structure tree represent morphological syntactical semantic information analyse sentence sentence assign correct model semantic parse system present identify correctly seventy-three possible semantic case roles
aim work explore new methodologies semantic parse unrestricted texts approach follow current trend information extraction ie base application verbal subcategorization lexicon lexpir mean complex pattern recognition techniques lexpir frame theoretical model verbal subcategorization develop pirapides project
paper schapire singer adaboostmh boost algorithm apply word sense disambiguation wsd problem initial experiment set fifteen select polysemous word show boost approach surpass naive bay exemplar base approach represent state art accuracy supervise wsd order make boost practical real learn domain thousands word several ways accelerate algorithm reduce feature space study best variant call lazyboosting test largest sense tag corpus available contain one hundred and ninety-two thousand, eight hundred examples one hundred and ninety-one frequent ambiguous english word boost compare favourably benchmark algorithms
paper describe experimental comparison two standard supervise learn methods namely naive bay exemplar base classification word sense disambiguation wsd problem aim work twofold firstly attempt contribute clarify confuse information comparison methods appear relate literature several directions explore include test several modifications basic learn algorithms vary feature space secondly improvement algorithms propose order deal large attribute set modification basically consist use positive information appear examples allow improve greatly efficiency methods loss accuracy experiment perform largest sense tag corpus available contain frequent ambiguous english word result show exemplar base approach wsd generally superior bayesian approach especially specific metric deal symbolic attribute use
years caisse des depots et consignations produce information filter applications operational applications require high filter performances achieve use rule base filter technique administrator tune set rule topic however filter become obsolescent time decrease performances due diachronic polysemy term involve loss precision diachronic polymorphism concepts involve loss recall help administrator maintain filter develop method automatically detect filter obsolescence consist make learn base control filter use set document already categorise relevant relevant rule base filter idea supervise filter process differential comparison outcomes control one method many advantage simple implement since train set use learn supply rule base filter thus make use control filter fully automatic automatic detection obsolescence learn base filter find rich application offer interest prospect
constraint handle rule chr provide realistic solution arch problem many field deal constraint logic program combine recursive function relations constraints avoid non termination problems paper focus benefit chr specifically implementation sicstus prolog provide computational linguists work grammar design tool chr rule apply mean subsumption check check make variables instantiate bind former functionality best difficult simulate use primitive coroutining statements sicstus two latter simply exist form chr sake provide case study apply grammar development consider attribute logic engine ale prolog preprocessor logic program type feature structure extension complete grammar development system head drive phrase structure grammar hpsg popular constraint base linguistic theory use type feature structure context chr use extend constraint language feature structure descriptions include relations declarative way also provide support constraints complex antecedents constraints co occurrence feature value necessary interpret type system hpsg properly
trec eight rout one specific filter build topic filter classifier train recognize document relevant topic present document classifier estimate probability document relevant topic train since procedure build filter topic independent system fully automatic make use sample document previously evaluate relevant relevant particular topic term selection perform neural network train document represent vector frequencies list select term list depend topic filter construct two step first step define characteristic word use relevant document corpus second one choose among previous list discriminant ones length vector optimize automatically topic end term selection vector typically twenty-five word define topic document process represent vector term frequencies vector subsequently input classifier train sample train classifier estimate document test set probability relevant submission trec top one thousand document rank order decrease relevance
multidimensional heterogeneous temporal nature speech databases raise interest challenge representation query recently annotation graph propose general purpose representational framework speech databases typical query annotation graph require path expressions similar use semistructured query languages however underlie model rather different customary graph model semistructured data graph acyclic unrooted temporal inclusion relationships important develop query language describe optimization techniques underlie relational representation
speech repair occur often spontaneous speak dialogues ability detect correct repair necessary speak language system present framework detect correct speech repair relevant level information ie acoustics lexis syntax semantics integrate basic idea reduce search space repair soon possible cascade filter involve feature first acoustic module generate hypotheses existence repair second stochastic model suggest correction every hypothesis well score corrections insert new paths word lattice finally lattice parser decide accept rep air
paper present bayesian model unsupervised learn verb selectional preferences verb model create bayesian network whose architecture determine lexical hierarchy wordnet whose parameters estimate list verb object pair find corpus explain away well know property bayesian network help model deal natural fashion word sense ambiguity train data word sense disambiguation test model perform better state art systems unsupervised learn selectional preferences computational complexity problems ways improve approach methods implement explain away graphical frameworks discuss
snow base learn approach shallow parse task present study experimentally approach learn identify syntactic pattern combine simple predictors produce coherent inference two instantiations approach study experimental result noun phrase np subject verb sv phrase compare favorably best publish result present compare two ways model problem learn recognize pattern suggest shallow parse pattern better learn use open close predictors use inside outside predictors
present framework analyze color document complex layout addition assumption make layout framework combine content drive bottom approach two different source information textual spatial analyze text shallow natural language process tool taggers partial parsers use infer relations logical layout resort qualitative spatial calculus closely relate allen calculus evaluate system document color journal present result extract read order journal page case analysis successful extract intend read order document
paper application automate theorem prove techniques computational semantics consider order compute presuppositions natural language discourse several inference task arise instead treat inferences independently show integrate techniques formal approach context deduction help compute presuppositions efficiently contexts represent discourse representation structure way nest make explicit addition tableau calculus present keep track contextual information thereby allow avoid carry redundant inference step happen approach neglect explicit nest contexts
present tableau calculus reason fragment natural language focus problem pronoun resolution way complicate automate theorem prove natural language process method explicitly manipulate contextual information deduction propose pronouns resolve context deduction result pronoun resolution deduction interleave way pronouns resolve license deduction rule help us avoid combinatorial complexity total pronoun disambiguation
paper apply resolution theorem prove natural language semantics aim circumvent computational complexity trigger natural language ambiguities like pronoun bind interleave pronoun bind resolution deduction therefore disambiguation apply expression actually occur derivations
paper describe automate deduction methods natural language process apply efficiently encode context elaborate way work base formal approach context provide tableau calculus contextual reason explain consider example problem area presupposition projection
paper describe set comparative experiment include cross corpus evaluation five alternative algorithms supervise word sense disambiguation wsd namely naive bay exemplar base learn snow decision list boost two main conclusions draw one lazyboosting algorithm outperform four state art algorithms term accuracy ability tune new domains two domain dependence wsd systems seem strong suggest kind adaptation tune require cross corpus application
xml document describe document type definition dtd xml grammar formal grammar capture syntactic feature dtd investigate properties family grammars show every xml language basically unique xml grammar give two characterizations languages generate xml grammars one set theoretic kind saturation property investigate decidability problems prove properties undecidable general context free languages become decidable xml languages also characterize xml grammars generate regular xml languages
might appear natural language process improve accuracy information retrieval systems make available detail analysis query document although past result appear show focus shift short phrase rather full document situation become somewhat different anvil system use natural language technique obtain high accuracy retrieval image annotate descriptive textual caption natural language techniques also allow additional contextual information derive relation query caption help users understand overall collection retrieval result techniques successfully use information retrieval system form testbed research basis commercial system
world wide web grow big anarchic fashion difficult describe one evident intrinsic characteristics world wide web multilinguality present technique estimate size language specific corpus give frequency commonly occur word corpus apply technique estimate number word available web browsers give languages compare data one thousand, nine hundred and ninety-six data one thousand, nine hundred and ninety-nine two thousand calculate growth number european languages web expect non english languages grow faster pace english though position english still dominant
grow problem unsolicited bulk e mail also know spam generate need reliable anti spam e mail filter filter type far base mostly manually construct keyword pattern alternative approach recently propose whereby naive bayesian classifier train automatically detect spam message test approach large collection personal e mail message make publicly available encrypt form contribute towards standard benchmarks introduce appropriate cost sensitive measure investigate time effect attribute set size train corpus size lemmatization stop list issue explore previous experiment finally naive bayesian filter compare term performance filter use keyword pattern part widely use e mail reader
investigate performance two machine learn algorithms context anti spam filter increase volume unsolicited bulk e mail spam generate need reliable anti spam filter filter type far base mostly keyword pattern construct hand perform poorly naive bayesian classifier recently suggest effective method construct automatically anti spam filter superior performance investigate thoroughly performance naive bayesian filter publicly available corpus contribute towards standard benchmarks time compare performance naive bayesian filter alternative memory base learn approach introduce suitable cost sensitive evaluation measure methods achieve accurate spam filter outperform clearly keyword base filter widely use e mail reader
paper investigate formal pragmatics ambiguous expressions model ambiguity multi agent system framework allow us give refine notion kind information convey ambiguous expressions analyze ambiguity affect knowledge dialog participants especially know ambiguous sentence utter agents communicate mean tell function whose application constrain implementation grice maxims information state multi agent system represent kripke structure tell update function structure framework enable us distinguish information convey ambiguous sentence vs information convey disjunctions semantic ambiguity vs perceive ambiguity
eventual goal language model accurately predict value miss word give context present approach word prediction base learn representation word function word linguistics predicate context approach raise new question address first order learn good word representations necessary use expressive representation context present way use external knowledge generate expressive context representations along learn method capable handle large number feature generate way potentially contribute prediction second since number word compete prediction large need focus attention smaller subset exhibit contribution focus attention mechanism performance word predictor finally describe large scale experimental study approach present show yield significant improvements word prediction task
study effect additive white noise cepstral representation speech signal distribution individual cepstrum coefficient speech show depend strongly noise overlap significantly cepstrum distribution noise base study suggest scalar quantity v equal sum weight cepstral coefficients able classify frame contain speech noise like frame distributions v speech noise frame reasonably well separate snr five db demonstrate feasibility robust speech detector base v
linguistic annotation cover descriptive analytic notations apply raw language data basic data may form time function audio video physiological record may textual add notations may include transcriptions sort phonetic feature discourse structure part speech sense tag syntactic analysis name entity identification co reference annotation several ongoing efforts provide format tool annotations publish annotate linguistic databases lack widely accept standards become critical problem propose standards extent exist focus file format paper focus instead logical structure linguistic annotations survey wide variety exist annotation format demonstrate common conceptual core annotation graph provide formal framework construct maintain search linguistic annotations remain consistent many alternative data structure file format
paper explore kinds probabilistic relations important syntactic disambiguation propose two widely use kinds relations lexical dependencies structural relations complementary disambiguation capabilities present new model base structural relations tree gram model report experiment show structural relations benefit enrichment lexical dependencies
present multi document summarizer call mead generate summaries use cluster centroids produce topic detection track system also describe two new techniques base sentence utility subsumption apply evaluation single multiple document summaries finally describe two user study test model multi document summarization