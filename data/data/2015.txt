research stylistic properties translations issue receive attention computational stylistics previous work rybicki two thousand and six distinguish character idiolects work polish author henryk sienkiewicz two correspond english translations use burrow delta method conclude idiolectal differences could observe source texts variation preserve large degree translations study also find two translations also highly distinguishable one another burrow two thousand and two examine english translations juvenal also use delta method result work suggest translators adept conceal style translate work another author whereas author tend imprint style greater extent work translate work examine write single author norwegian playwright henrik ibsen write translate german english norwegian attempt investigate preservation characterization define distinctiveness textual contributions character
paper present reg graph base approach study fundamental problem natural language process nlp automatic text summarization algorithm map document graph compute weight sentence apply approach summarize document three languages
part speech pos vital topic natural language process nlp task language involve analyse construction language behaviours dynamics language knowledge could utilize computational linguistics analysis automation applications context deal unknown word word appear lexicon refer unknown word also important task since grow nlp systems use new applications one aid predict lexical categories unknown word use syntactical knowledge language distinction open class word close class word together syntactical feature language use research predict lexical categories unknown word tag process experiment perform investigate ability approach parse unknown word use syntactical knowledge without human intervention experiment show performance tag process enhance word class distinction use together syntactic rule parse sentence contain unknown word sinhala language
analysis script play important role paleography quantitative linguistics especially field digital paleography quantitative feature much need differentiate glyphs describe elaborate set metrics quantify qualitative information contain character hence indirectly also quantify scribal feature broadly divide metrics several categories describe individual metric underlie qualitative significance metrics largely derive relate area gesture design recognition also propose several novel metrics propose metrics soundly ground principles handwrite production handwrite analysis compute metrics could serve descriptors script also use compare analyze script illustrate quantitative analysis base propose metrics apply paleographic evolution medieval tamil script brahmi also outline future work
paper concern nearest neighbor search distributional semantic model normal nearest neighbor search return rank list neighbor information structure topology local neighborhood potentially serious shortcoming mode query distributional semantic model since rank list neighbor may conflate several different sense argue topology neighborhoods semantic space provide important information different sense term topological structure use word sense induction also argue topology neighborhoods semantic space use determine semantic horizon point define set neighbor direct connection point introduce relative neighborhood graph method uncover topological properties neighborhoods semantic model also provide examples relative neighborhood graph three well know semantic model pmi model glove model skipgram model
turkic languages exhibit extensive diverse etymological relationships among lexical items relationships make turkic languages promise explore automate translation lexicon induction leverage cognate etymological information however due extent diversity type relationships word clear annotate information paper present methodology annotate cognates etymological origin turkic languages method strive balance amount research effort annotator expend utility annotations support research improve automate translation lexicon induction
consider phrase base language model lm generalize commonly use word level model similar concept phrase base lms appear speech recognition rather specialize thus less suitable machine translation mt contrast dependency lm first introduce exhaustive phrase base lms tailor mt use preliminary experimental result show approach outperform word base lms respect perplexity translation quality
reorder challenge machine translation mt systems mt widely use approach apply word base language model lm consider constituent units sentence word speech recognition sr phrase base lm propose however lms necessarily suitable optimal reorder propose two phrase base lms consider constituent units sentence phrase experiment show phrase base lms outperform word base lm respect perplexity n best list rank
syntactic parse necessary task require nlp applications include machine translation challenge task develop qualitative parser morphological rich agglutinative languages syntactic analysis use understand grammatical structure natural language sentence output grammatical information word constituent also issue relate help us understand language detail way literature survey groundwork understand different parser development indian languages various approach use develop tool techniques paper provide survey research paper well know journals conferences
reduce phrase representation parse dependency parse reduction ground new intermediate representation head order dependency tree show isomorphic constituent tree encode order information dependency label show shelf trainable dependency parser use produce constituents parser non projective perform discontinuous parse natural manner despite simplicity approach experiment show result parsers par strong baselines berkeley parser english best single system spmrl two thousand and fourteen share task result particularly strike discontinuous parse german surpass current state art wide margin
present novel learn method word embeddings design relation classification word embeddings train predict word noun pair use lexical relation specific feature large unlabeled corpus allow us explicitly incorporate relation specific information word embeddings learn word embeddings use construct feature vectors relation classification model well establish semantic relation classification task method significantly outperform baseline base previously introduce word embed method compare favorably previous state art model use syntactic information manually construct external resources
commonly accept machine translation complex task part speech tag much complex paper make attempt develop general framework methodology compute informational process complexity nlp applications task define universal framework akin turn machine attempt fit nlp task one paradigm calculate complexities various nlp task use measure shannon entropy compare simple ones part speech tag complex ones machine translation paper provide first though far perfect attempt quantify nlp task uniform paradigm point current deficiencies suggest avenues fruitful research
hyperlinks relations wikipedia extraordinary resource still fully understand paper study different type link wikipedia contrast use full graph respect direct link apply well know random walk algorithm two task word relatedness name entity disambiguation show use full graph effective direct link large margin non reciprocal link harm performance benefit categories infoboxes coherent result task set new state art figure systems base wikipedia link comparable systems exploit several information source supervise machine learn approach open source instruction reproduce result amenable integrate complementary text base methods
state art systems today produce morphological analysis base orthographic pattern contrast propose model unsupervised morphological analysis integrate orthographic semantic view word model word formation term morphological chain base word observe word break chain parent child relations use log linear model morpheme word level feature predict possible parent include modifications word limit set candidate parent word render contrastive estimation feasible model consistently match outperform five state art systems arabic english turkish
recent work end end neural network base architectures machine translation show promise result en fr en de translation arguably one major factor behind success availability high quality parallel corpora work investigate leverage abundant monolingual corpora neural machine translation compare phrase base hierarchical baseline obtain one hundred and ninety-six bleu improvement low resource language pair turkish english one hundred and fifty-nine bleu focus domain task chinese english chat message method initially target toward task less parallel data show also extend high resource languages cs en de en obtain improvement thirty-nine forty-seven bleu score neural machine translation baselines respectively
morphological analysis important branch linguistics natural language process technology morphology study word structure formation word language current scenario nlp research morphological analysis techniques become popular day day process language morphology word first analyze assamese language contain complex morphological structure work use apertium base finite state transducers develop morphological analyzer assamese language limit domain get seven hundred and twenty-seven accuracy
propose novel convolutional architecture name gencnn word sequence prediction different previous work neural network base language model generation eg rnn lstm choose greedily summarize history word fix length vector instead use convolutional neural network predict next word history word variable length also different exist feedforward network language model model effectively fuse local correlation global correlation word sequence convolution gate strategy specifically design task argue model give adequate representation history therefore naturally exploit short long range dependencies model fast easy train readily parallelize extensive experiment text generation n best rank machine translation show gencnn outperform state arts big margins
word2vec afford simple yet powerful approach extract quantitative variables unstructured textual data half healthcare data unstructured therefore hard model without involve expertise data engineer natural language process word2vec serve bridge quickly gather intelligence data source study run six hundred and fifty megabytes unstructured medical chart note providence health service electronic medical record word2vec use two different approach create predictive variables test risk readmission patients copd chronic obstructive lung disease comparative benchmark run test use lace risk model single score base length stay acuity comorbid condition emergency department visit use free text mathematical might find word2vec comparable lace predict risk readmission copd patients
machine translation common phenomenon machine readable dictionaries standard parse rule enough ensure accuracy parse translate english phrase korean language reveal mislead translation result due consequent structural semantic ambiguities paper aim suggest solution structural semantic ambiguities due idiomaticity non grammaticalness phrase commonly use english language apply bilingual phrase database english korean machine translation ekmt paper firstly clarify phrase unit ekmt base definition english phrase secondly clarify kind language unit target phrase database ekmt thirdly suggest way build phrase database present format phrase database examples finally discuss briefly method apply bilingual phrase database ekmt structural semantic disambiguation
paper discuss structure syntagma lexical database focus italian basic database consist four table table form contain word inflections use pos tagger identification input word form relate lemma table lemma store kinds grammatical feature word word level semantic data restrictions table mean mean relate data store definition examples domain semantic information table valency contain argument structure mean syntactic semantic feature argument extend version sld contain link syntagma semantic net wordnet synsets languages
work address problem measure many languages person effectively speak give languages close word assign meaningful number language portfolio intuition say someone speak fluently spanish portuguese linguistically less proficient compare someone speak fluently spanish chinese since take effort native spanish speaker learn chinese portuguese number languages grow proficiency level vary get even complicate assign score language portfolio article propose measure linguistic quotient lq account effect define properties measure base idea coherent risk measure mathematical finance lay foundation propose one measure together algorithm work languages classification tree input algorithm together input available online lingvometercom
open domain relation extraction systems identify relation argument phrase sentence without rely underlie schema however current state art relation extraction systems available english heavy reliance linguistic tool part speech taggers dependency parsers present cross lingual annotation projection method language independent relation extraction evaluate method manually annotate test set present result three typologically different languages release manual annotations extract relations sixty-one languages wikipedia
dependency parsers among crucial tool natural language process many important applications downstream task information retrieval machine translation knowledge acquisition introduce yara parser fast accurate open source dependency parser base arc eager algorithm beam search achieve unlabeled accuracy nine thousand, three hundred and thirty-two standard wsj test set rank among top dependency parsers fastest yara parse four thousand sentence per second greedy mode one beam optimize accuracy use sixty-four beam brown cluster feature yara parse forty-five sentence per second parser train syntactic dependency treebank different options provide order make flexible tunable specific task release apache version twenty license use commercial academic purpose parser find https githubcom yahoo yaraparser
unsupervised word embeddings show valuable feature supervise learn problems however role unsupervised problems less thoroughly explore paper show embeddings likewise add value problem unsupervised pos induction two representative model pos induction replace multinomial distributions vocabulary multivariate gaussian distributions word embeddings observe consistent improvements eight languages also analyze effect various choices induce word embeddings downstream pos induction result
pymorphy2 morphological analyzer generator russian ukrainian languages use large efficiently encode lexi con build opencorpora languagetool data set linguistically motivate rule develop enable morphological analysis generation vocabulary word observe real world document russian pymorphy2 provide state arts morphological analysis quality analyzer implement python program language optional c extensions emphasis put ease use documentation extensibility package distribute permissive open source license encourage use academic commercial set
describe technique attribute part write text set unknown author nothing assume know priori write style potential author use multiple independent cluster input text identify part similar dissimilar one another describe algorithms necessary combine multiple cluster meaningful output show result application technique texts multiple write style
paper present text normalization integral part text speech synthesis system text normalization set methods task write non standard word like number date time abbreviations acronyms common symbols full expand form present whole taxonomy classification non standard word croatian language together rule base normalization methods combine lookup dictionary propose achieve token rate normalization croatian texts ninety-five eighty expand word correct morphological form
discourse markers universal linguistic events subject language variation although extensive literature already report language specific traits events little say cross language behavior build inventory multilingual lexica discourse markers work describe new methods approach description classification annotation discourse markers specific domain europarl corpus study discourse markers context translation crucial due idiomatic nature structure multilingual lexica together functional analysis structure useful tool hard task translate discourse markers possible equivalents one language another use daniel marcu validate discourse markers english extract brown corpus purpose build multilingual lexica discourse markers languages base machine translation techniques major assumption study usage discourse marker independent language ie rhetorical function discourse marker sentence one language equivalent rhetorical function discourse marker another language
paper present sabrina sentiment analysis broad resource italian natural language applications manually annotate prior polarity lexical resource italian natural language applications field opinion mine sentiment induction resource consist two different set italian dictionary two hundred and seventy-seven thousand word tag prior polarity value set polarity modifiers contain two hundred word use combination non neutral term dictionary order induce sentiment italian compound term best knowledge first prior polarity manually annotate resource develop italian natural language
describe simple approach semantic parse base tensor product kernel extract two feature vectors one query one candidate logical form train classifier use tensor product two vectors use simple feature system achieve average f1 score four hundred and one webquestions dataset comparable complex systems simpler implement run faster
present textitautoextend system learn embeddings synsets lexemes flexible take word embeddings input need additional train corpus synset lexeme embeddings obtain live vector space word embeddings sparse tensor formalization guarantee efficiency parallelizability use wordnet lexical resource autoextend easily apply resources like freebase autoextend achieve state art performance word similarity word sense disambiguation task
consider large number text data set cook recipes term distribution distributional properties data investigate aim look various analytical approach allow mine information high low detail scale metric space embed fundamental interest semantic properties data consider projection data analyse aggregate versions data contrast projection aggregate versions data analyse data analogously term set look analysis select term also look inherent term associations singular plural addition use correspondence analysis r latent semantic space map also use apache solr set solr server carry query describe novelty query support solr base principal factor plane map data use bound box query base factor projections
paper describe possible directions deeper understand help bridge gap psychology cognitive science computational approach sentiment opinion analysis literature focus opinion holder underlie need resultant goals utilitarian model sentiment provide basis explain reason sentiment valence hold thoughts still immature scatter unstructured even imaginary believe perspectives might suggest fruitful avenues various kinds future work
call control natural language cnl traditionally give many different name especially last four decades wide variety languages design apply improve communication among humans improve translation provide natural intuitive representations formal notations despite apparent differences seem sensible put languages umbrella bring order variety languages general classification scheme present comprehensive survey exist english base cnls give list describe one hundred languages one thousand, nine hundred and thirty today classification languages reveal form single scatter cloud fill conceptual space natural languages english one end formal languages propositional logic goal article provide common terminology common model cnl contribute understand general nature provide start point researchers interest area help developers make design decisions
large societies like india huge demand convert one human language another lot work do area many transfer base mts develop english languages mantra cdac pune matra cdac pune shakti iisc bangalore iiit hyderabad still little work do hindi languages currently work paper focus design system translate document hindi english use transfer base approach system take input text check structure parse reorder rule use generate text target language better corpus base mts corpus base mts require large amount word align data translation available many languages transfer base mts require knowledge languagessource language target language make transfer rule get correct translation simple assertive sentence almost correct complex compound sentence
usernames ubiquitous internet often suggestive user demographics work look degree gender language infer username alone make use unsupervised morphology induction decompose usernames sub units experimental result two task demonstrate effectiveness propose morphological feature compare character n gram baseline
exist multi document summarization systems usually rely specific summarization model ie summarization method specific parameter set extract summaries different document set different topics however accord quantitative analysis none exist summarization model always produce high quality summaries different document set even summarization model good overall performance may produce low quality summaries document set contrary baseline summarization model may produce high quality summaries document set base observations treat summaries produce different summarization model candidate summaries explore discriminative reranking techniques identify high quality summaries candidates difference document set propose extract set candidate summaries document set base ilp framework leverage rank svm summary reranking various useful feature develop reranking process include word level feature sentence level feature summary level feature evaluation result benchmark duc datasets validate efficacy robustness propose approach
coordinate relation refer relation instance concept relation directly hyponyms concept paper focus task extract term coordinate user give seed term chinese group term belong different concepts seed term several mean propose semi supervise method integrate manually define linguistic pattern automatically learn semi structural pattern extract coordinate term chinese web search result addition term group different concepts base co occur term contexts calculate saliency score extract term rank accordingly experimental result demonstrate propose method generate result high quality wide coverage
paper propose novel word alignment base method solve faq base question answer task first employ neural network model calculate question similarity word alignment two question use extract feature second design bootstrap base feature extraction method extract small set effective lexical feature third propose learn rank algorithm train parameters suitable rank task experimental result conduct three languages english spanish japanese demonstrate question similarity model effective baseline systems sparse feature bring five improvements top one accuracy learn rank algorithm work significantly better traditional method evaluate method answer sentence selection task method outperform previous systems standard trec data set
machine translation one research field computational linguistics objective many mt researchers develop mt system produce good quality high accuracy output translations also cover maximum language pair internet globalization increase day day need way improve quality translation reason develop classifier base text simplification model english hindi machine translation systems use support vector machine nai bay classifier develop model also evaluate performance classifiers
paper describe hierarchical system predict one label time automate student response analysis task build classification binary tree delay easily confuse label later stag use hierarchical process particular paper describe hierarchical classifier build classification task break binary subtasks finally discuss motivations fundamentals approach
dialog state tracker important component modern speak dialog systems present incremental dialog state tracker base lstm network directly use automatic speech recognition hypotheses track state also present key non standard aspects model bring performance close state art experimentally analyze contribution include asr confidence score abstract scarcely represent value include transcriptions train data model average
dialogue state track dst process estimate distribution dialogue state dialogue progress recent study constrain markov bayesian polynomial cmbp framework take first step towards bridge gap rule base statistical approach dst paper gap bridge novel framework recurrent polynomial network rpn rpn unique structure enable framework advantage cmbp include efficiency portability interpretability additionally rpn achieve properties statistical approach cmbp rpn evaluate data corpora second third dialog state track challenge dstc two three experiment show rpn significantly outperform traditional rule base approach statistical approach similar feature set compare state art statistical dst approach lot richer feature rpn also competitive
associative measure mathematical formulas determine strength association two word base occurrences cooccurrences text corpus pecina two thousand and ten p one hundred and thirty-eight purpose paper test twelve associative measure text nsp banerjee pedersen two thousand and three contain ten million word subcorpus turkish national corpus tnc aksan etal two thousand and twelve statistical comparison measure scope study measure evaluate accord linguistic relevance rank provide focus study basically optimize corpus data apply measure evaluate rank produce measure whole linguistic relevance individual n grams find include intra linguistically relevant associative measure comma delimit sentence splitted lower case well balance representative ten million word corpus turkish
analyze three critical components word embed train model corpus train parameters systematize exist neural network base word embed algorithms compare use corpus evaluate word embed three ways analyze semantic properties use feature supervise task use initialize neural network also provide several simple guidelines train word embeddings first discover corpus domain important corpus size recommend choose corpus suitable domain desire task use larger corpus yield better result second find faster model provide sufficient performance case complex model use train corpus sufficiently large third early stop metric iterate rely development set desire task rather validation loss train embed
paper explain reason lead research conceive novel technology dependency parse mix together strengths data drive transition base constraint base approach particular highlight problem infer reliability result data drive transition base parser extremely important high level process expect use correct parse result briefly introduce number note new parser model i work capable proceed analysis aware way robust concept robustness
discriminative segmental model segmental conditional random field scrfs segmental structure support vector machine ssvms success speech recognition via lattice rescoring first pass decode however model suffer slow decode hamper use computationally expensive feature segment neural network high order feature typical solution use approximate decode either beam prune single pass beam prune generate lattice follow second pass work study discriminative segmental model train hinge loss ie segmental structure svms show beam search suitable learn rescoring model approach though give good approximate decode performance model already well train instead consider approach inspire structure prediction cascade use max marginal prune generate lattices obtain high accuracy phonetic recognition system several expensive feature type segment neural network second order language model second order phone boundary feature
recent work explore methods learn continuous vector space word representations reflect underlie semantics word simple vector space arithmetic use cosine distance show capture certain type analogies reason plurals singulars past tense present tense etc paper introduce new approach capture analogies continuous word representations base model individual word vectors rather subspaces span group word exploit property set subspaces n dimensional euclidean space form curve manifold space call grassmannian quotient subgroup lie group rotations n dimension base mathematical model develop modify cosine distance model base geodesic kernels capture relation specific distance across word categories experiment analogy task show approach perform significantly better previous approach give task
statistical methods widely employ recent years grasp many language properties application techniques allow improvement several linguistic applications encompass machine translation automatic summarization document classification latter many approach emphasize semantical content texts case bag word language model approach certainly yield reasonable performance however potential feature structural organization texts use study context probe feature derive textual structure analysis effectively employ classification task specifically perform supervise classification aim discriminate informative imaginative document use network model describe local topological dynamical properties function word achieve accuracy rate ninety-five much higher similar network approach systematic analysis feature relevance reveal symmetry accessibility measurements among prominent network measurements result suggest measurements could use relate language applications play complementary role characterize texts
introduce approach train lexicalize parsers use bilingual corpora obtain merge harmonize treebanks different languages produce parsers analyze sentence either learn languages even sentence mix test approach universal dependency treebanks train maltparser maltoptimizer result show bilingual parsers competitive combinations preserve accuracy even achieve significant improvements correspond monolingual parsers preliminary experiment also show approach promise texts code switch languages add
present novel approach sentence simplification depart previous work two main ways first require neither hand write rule train corpus align standard simplify sentence second sentence split operate deep semantic structure show unsupervised framework propose competitive four state art supervise systems ii semantic base approach allow principled effective handle sentence split
recently focus complex network research shift analysis isolate properties system toward realistic model multiple phenomena multilayer network motivate prosperity multilayer approach social transport trade systems propose introduction multilayer network language multilayer network language unify framework model linguistic subsystems structural properties enable exploration mutual interactions various aspects natural language systems represent complex network whose vertices depict linguistic units link model relations multilayer network language define three aspects network construction principle linguistic subsystem language interest precisely construct word level syntax co occurrence shuffle counterpart subword level syllables graphemes network layer five variations original text model language obtain result suggest substantial differences network structure different language subsystems hide exploration isolate layer word level layer share structural properties regardless language eg croatian english syllabic subword level express language dependent structural properties preserve weight overlap quantify similarity word level layer weight direct network moreover analysis motifs reveal close topological structure syntactic syllabic layer languages find corroborate multilayer network framework powerful consistent systematic approach model several linguistic subsystems simultaneously hence provide unify view language
describe lstm base model call byte span bts read text bytes output span annotations form start length label start position lengths label separate entries vocabulary operate directly unicode bytes rather language specific word character analyze text many languages single model due small vocabulary size multilingual model compact produce result similar better state art part speech tag name entity recognition use provide train datasets external data source model learn scratch rely elements standard pipeline natural language process include tokenization thus run standalone fashion raw text
pivot language employ way solve data sparseness problem machine translation especially data particular language pair exist combination source pivot pivot target translation model induce new translation model pivot language however errors two model may compound noise still combine model may suffer serious phrase sparsity problem paper directly employ word lexical model ibm model additional resource augment pivot phrase table addition also propose phrase table prune method take account source target phrasal coverage experimental result show prune method significantly outperform conventional one consider source side phrasal coverage furthermore include entries lexicon model phrase coverage increase achieve improve result chinese japanese translation use english pivot language
emergence global adoption social media render possible real time estimation population scale sentiment bear profound implications understand human behavior give grow assortment sentiment measure instrument comparisons evidently require perform detail test six dictionary base methods apply four different corpora briefly examine twenty methods show dictionary base method perform reliably meaningfully one dictionary cover sufficiently large enough portion give text lexicon weight word usage frequency two word score continuous scale
present dataset manually annotate relationships character literary texts order support train evaluation automatic methods relation type prediction domain makazhanov et al two thousand and fourteen kokkinakis two thousand and thirteen broader computational analysis literary character elson et al two thousand and ten bamman et al two thousand and fourteen vala et al two thousand and fifteen flekova gurevych two thousand and fifteen work solicit annotations workers amazon mechanical turk one hundred and nine texts range homer iliad joyce ulysses four dimension interest give pair character collect judgments coarse grain category professional social familial fine grain category friend lover parent rival employer affinity positive negative neutral describe primary relationship text assume relationship static also collect judgments whether change point course text
target dependent sentiment classification remain challenge model semantic relatedness target context word sentence different context word different influence determine sentiment polarity sentence towards target therefore desirable integrate connections target word context word build learn system paper develop two target dependent long short term memory lstm model target information automatically take account evaluate methods benchmark dataset twitter empirical result show model sentence representation standard lstm perform well incorporate target information lstm significantly boost classification accuracy target dependent lstm model achieve state art performances without use syntactic parser external sentiment lexicons
paper present end end neural network model name neural generative question answer genqa generate answer simple factoid question base facts knowledge base specifically model build encoder decoder framework sequence sequence learn equip ability enquire knowledge base train corpus question answer pair associate triple knowledge base empirical study show propose model effectively deal variations question answer generate right natural answer refer facts knowledge base experiment question answer demonstrate propose model outperform embed base qa model well neural dialogue model train data
artificial intelligence area one ultimate goals make computers understand human language offer assistance order achieve ideal researchers computer science put forward lot model algorithms attempt enable machine analyze process human natural language different level semantics although recent progress field offer much hope still ask whether current research provide assistance people really desire read comprehension end conduct read comprehension test two scientific paper write different style use semantic link model analyze understand obstacles people face process read figure make difficult human understand scientific literature analysis summarize characteristics problems reflect people different level knowledge comprehension difficult science technology literature model semantic link network believe characteristics problems help us examine exist machine model helpful design new one
question form integral part everyday communication offline online get responses question others fundamental satisfy information need extend knowledge boundaries question may represent use various factor social syntactic semantic etc hypothesize factor contribute vary degrees towards get responses others give question perform thorough empirical study measure effect factor use novel question answer dataset website redditcom best knowledge first analysis kind important topic also use sparse nonnegative matrix factorization technique automatically induce interpretable semantic factor question dataset also document various pattern response prediction observe analysis data instance find preference probe question scantily answer method robust capture latent response factor hope make code datasets publicly available upon publication paper
propose minimum risk train end end neural machine translation unlike conventional maximum likelihood estimation minimum risk train capable optimize model parameters directly respect arbitrary evaluation metrics necessarily differentiable experiment show approach achieve significant improvements maximum likelihood estimation state art neural machine translation system across various languages pair transparent architectures approach apply neural network potentially benefit nlp task
show end end deep learn approach use recognize either english mandarin chinese speech two vastly different languages replace entire pipelines hand engineer components neural network end end learn allow us handle diverse variety speech include noisy environments accent different languages key approach application hpc techniques result 7x speedup previous system efficiency experiment previously take weeks run days enable us iterate quickly identify superior architectures algorithms result several case system competitive transcription human workers benchmarked standard datasets finally use technique call batch dispatch gpus data center show system inexpensively deploy online set deliver low latency serve users scale
mine semantic analysis msa novel concept space model employ unsupervised learn generate semantic representations text msa represent textual structure term phrase document bag concepts boc concepts derive concept rich encyclopedic corpora traditional concept space model exploit target corpus content construct concept space msa alternatively uncover implicit relations concepts mine associations eg mine wikipedia see also link graph evaluate msa performance benchmark datasets measure semantic relatedness word sentence empirical result show competitive performance msa compare prior state art methods additionally introduce first analytical study examine statistical significance result report different semantic relatedness methods study show nuances result across top perform methods could statistically insignificant study position msa one state art methods measure semantic relatedness besides inherent interpretability simplicity generate semantic representation
paper present experiment carry us jadavpur university part participation fire two thousand and fifteen task entity extraction social media text indian languages esm il tool develop task base trigram hide markov model utilize information like gazetteer list pos tag word level feature enhance observation probabilities know tokens well unknown tokens submit run english statistical hmm hide markov model base model use implement system system train test datasets release fire two thousand and fifteen task entity extraction social media text indian languages esm il system best performer english language obtain precision recall f measure six thousand, one hundred and ninety-six three thousand, nine hundred and forty-six four thousand, eight hundred and twenty-one respectively
attentional mechanism prove effective improve end end neural machine translation however due intricate structural divergence natural languages unidirectional attention base model might capture partial aspects attentional regularities propose agreement base joint train bidirectional attention base end end neural machine translation instead train source target target source translation model independentlyour approach encourage two complementary model agree word alignment matrices train data experiment chinese english english french translation task show agreement base joint train significantly improve alignment translation quality independent train
morpho syntactic lexicons provide information morphological syntactic roles word language lexicons available languages even available coverage limit present graph base semi supervise learn method use morphological syntactic semantic relations word automatically construct wide coverage lexicons small seed set method language independent show expand one thousand word seed lexicon one hundred time size high quality eleven languages addition automatically create lexicons provide feature improve performance two downstream task morphological tag dependency parse
model pair sentence critical issue many nlp task answer selection paraphrase identification pi textual entailment te prior work deal one individual task fine tune specific system ii model sentence representation separately rarely consider impact sentence iii rely fully manually design task specific linguistic feature work present general attention base convolutional neural network abcnn model pair sentence make three contributions abcnn apply wide variety task require model sentence pair ii propose three attention scheme integrate mutual influence sentence cnn thus representation sentence take consideration counterpart interdependent sentence pair representations powerful isolate sentence representations iii abcnn achieve state art performance pi te task
one thousand, one hundred and fifteen rule astadhyayi a4176 a54160 deal generation derivative nouns make one largest topical section astadhyayi call taddhita section owe head rule a4176 section systematic arrangement rule enumerate various affix use derivation specific semantic relations propose system automate process generation derivative nouns per rule astadhyayi propose system follow completely object orient approach model rule class group rule group rule group decide basis selective group rule virtue anuvrtti group rule result inheritance network rule direct acyclic graph every rule group head rule head rule notify direct member rule group environment contain detail data entities participate derivation process system implement mechanism use multilevel inheritance observer design pattern system focus generation desire final form also correctness sequence rule apply make sure derivation take place strict adherence astadhyayi propose system design allow incorporate various conflict resolution methods mention authentic texts hence effectiveness rule validate result system also present case check applicability system rule specifically applicable derivation derivative nouns order see effectiveness propose schema generic system model astadhyayi
generate article automatically computer program challenge task artificial intelligence natural language process paper target essay generation take input topic word mind generate organize article theme topic follow idea text plan citereiter1997 develop essay generation framework framework consist three components include topic understand sentence extraction sentence reorder component study several statistical algorithms empirically compare term qualitative quantitative analysis although run experiment chinese corpus method language independent easily adapt language lay remain challenge suggest avenues future research
morphological inflection generation task generate inflect form give lemma correspond particular linguistic transformation model problem inflection generation character sequence sequence learn problem present variant neural encoder decoder model solve model language independent train supervise semi supervise settings evaluate system seven datasets morphologically rich languages achieve either better comparable result exist state art model inflection generation
describe university sheffield system participation two thousand and fifteen multi genre broadcast mgb challenge task transcribe multi genre broadcast show transcription one four task propose mgb challenge aim advance state art automatic speech recognition speaker diarisation automatic alignment subtitle broadcast media four topics investigate work data selection techniques train unreliable data automatic speech segmentation broadcast media show acoustic model adaptation highly variable environments language model multi genre show final system operate multiple pass use initial unadapted decode stage refine segmentation follow three adapt pass hybrid dnn pass input feature normalise speaker base cepstral normalisation another hybrid stage input feature normalise speaker feature mllr transformations finally bottleneck base tandem stage noise speaker factorisation combination three system output provide final error rate two hundred and seventy-five official development set consist forty-seven multi genre show
paper describe algorithm translate english negative sentence korean english korean machine translation ekmt propose algorithm base comparative study english korean negative sentence earlier translation software translate english negative sentence accurate korean equivalents establish new algorithm negative sentence translation evaluate
despite loss semantic information bag ngram base methods still achieve state art result task sentiment classification long movie review many document embeddings methods propose capture semantics still outperform bag ngram base methods task paper modify architecture recently propose paragraph vector allow learn document vectors predict word n gram feature well model able capture semantics word order document keep expressive power learn vectors experimental result imdb movie review dataset show model outperform previous deep learn model bag ngram base model due advantage robust result also obtain model combine model source code model also publish together paper
propose algorithmic approach deal find sense word electronic data dayin different communication mediums like internet mobile service etc people use word slang nature approach detect abusive word use supervise learn procedure real life scenario slang word use complete word form always time word use different abbreviate form like sound alike form taboo morphemes etc propose approach detect abbreviate form also use semi supervise learn procedure use synset concept analysis text probability suspicious word slang word also evaluate
article deal analysis semantic content anonymous russian speak forum 2chhk different verbal mean express emotional state aggression reveal site aggression classify directions lexis different russian english speak anonymous forums 2chhk iichanhk 4chanorg public community mdk russian speak social network vk analyze compare open corpus russian language opencorporaorg brown corpus analysis show anonymity influence amount invective items usage effectiveness moderation show anonymous forums establish russian obscene lexis use express emotional state aggression six hundred and four case 2chhk preliminary result show russian obscene lexis internet direct dependence emotional state aggression
address recent criticisms liu et al two thousand and fifteen ferrer cancho g omez rodr iguez two thousand and fifteen work empirical evidence dependency length minimization across languages futrell et al two thousand and fifteen first acknowledge error fail acknowledge liu two thousand and eight previous work corpora twenty languages similar aim correction appear pnas nevertheless argue work provide novel strong evidence dependency length minimization universal quantitative property languages beyond previous work provide baselines focus word order preferences second argue choices baselines appropriate control alternative theories
search engine log store detail information web users interactions thus people use search engines daily basis important trail users common knowledge record file previous research show possible extract concept taxonomies full text document scholars propose methods obtain similar query query log propose mixture line research mine query log find relate query query hierarchies actual term taxonomies could use improve search engine effectiveness efficiency result study develop method combine lexical heuristics supervise classification model successfully extract hyponymy relations specialization search pattern reveal log missions additional source information language independent way
past years neural network emerge powerful machine learn model yield state art result field image recognition speech process recently neural network model start apply also textual natural language signal promise result tutorial survey neural network model perspective natural language process research attempt bring natural language researchers speed neural techniques tutorial cover input encode natural language task fee forward network convolutional network recurrent network recursive network well computation graph abstraction automatic gradient computation
syllable contact pair crosslinguistically tend fall sonority slope constraint call syllable contact law scl study phonotactics syllable contact four thousand, two hundred and two cvccvc word persian lexicon investigate consonants persian divide five sonority categories frequency possible sonority slop compute lexicon type frequency corpus token frequency since unmarked phonological structure show diachronically become frequent expect see pattern syllable contact pair fall sonority slope correlation sonority categories two consonants syllable contact pair measure use pointwise mutual information
cognitive acoustic cue important role shape phonological structure language mean optimal communication paper introduce p trac procedure order track dispersion contrast different contexts lexicon result apply p trac procedure case dispersion contrast pre consonantal contexts consonantal position cvcc sequence persian provide evidence favor phonetic basis dispersion argue license cue hypothesis dispersion theory contrast p trac procedure prove effective reveal dispersion contrast lexicon especially compare dispersion contrast different contexts
recent study revisit whole word basic model unit speech recognition query applications instead phonetic units whole word segmental systems rely function map variable length speech segment vector fix dimensional space result acoustic word embeddings need allow accurate discrimination different word type directly embed space compare several old new approach word discrimination task best approach use side information form know word pair train siamese convolutional neural network cnn pair tie network take two speech segment input produce embeddings train hinge loss separate word pair different word pair margin word classifier cnn perform similarly require much stronger supervision type cnns yield large improvements best previously publish result word discrimination task
work describe system perform morphological analysis generation pali word system work regular inflectional paradigms lexical database generator use build collection inflect derive word turn use analyzer generate store morphological form along correspond morphological information allow efficient simple look analyzer indeed look word extract attach morphological information analyzer compute information must however assume lexical database incomplete system also work without dictionary component use rule base approach
language segmentation consist find boundaries one language end another language begin text write one language important natural language process task problem solve train language model language data however case low resource languages problematic therefore investigate whether unsupervised methods perform better supervise methods difficult impossible train supervise approach special focus give difficult texts ie texts rather short one sentence contain abbreviations low resource languages non standard language compare three approach supervise n gram language model unsupervised cluster weakly supervise n gram language model induction devise weakly supervise approach order deal difficult text specifically order test approach compile small corpus different text type range one sentence texts texts three hundred word weakly supervise language model induction approach work well short difficult texts outperform cluster algorithm reach score vicinity supervise approach result look promise room improvement thorough investigation undertake
paper introduce novel approach tackle exist gap message translations dialogue systems currently submit message dialogue systems consider isolate sentence thus miss context information impede disambiguation homographs word ambiguous sentence approach solve disambiguation problem use concepts exist ontologies
propose approach help agents compose email reply customer request enable use lda extract latent topics collection email exchange use latent topics label data obtain call silver standard topic label exploit label set train classifier predict topic distribution entire agent email response base feature customer email ii predict topic distribution next sentence agent reply base customer email feature feature agent current sentence experimental result large email collection contact center tele com domain show propose ap proach effective predict best topic agent next sentence eighty case correct topic present among top five recommend topics fifty possible ones show potential method apply interactive set agent present small list likely topics choose next sentence
common use language refer visually present object model computers require model link language perception word classifiers model ground semantics view word classifiers perceptual contexts compose mean phrase composition denotations component word recently show perform well game play scenario small number object type apply two large set real world photograph contain much larger variety type refer expressions available use pre train convolutional neural network extract image feature augment picture positional information show model achieve performance competitive state art reference resolution task give expression find bound box referent argue conceptually simpler flexible
name game study explore role self organization development negotiation linguistic conventions paper define automata network approach name game two problems face one definition automata network multi party communicative interactions two proof convergence three different order individuals update update scheme finally computer simulations explore two dimensional lattices purpose recover main feature name game describe dynamics different update scheme
experimental approach study properties word embeddings propose control experiment achieve modifications train corpus permit demonstration direct relations word properties word vector direction length approach demonstrate use word2vec cbow model experiment independently vary word frequency word co occurrence noise experiment reveal word vector length depend less linearly word frequency level noise co occurrence distribution word coefficients linearity depend upon word special point feature space define artificial word pure noise co occurrence distribution find small non zero
languages use relative order word encode mean relations languages differ however order use order map onto different mean test hypothesis despite differences human languages might constitute different solutions common pressure language use use monte carlo simulations data five languages find word order efficient process term dependency length local lexical probability suggest bias originate brain understand language strongly constrain human languages change generations
sequence sequence neural network model generation conversational responses tend generate safe commonplace responses eg know regardless input suggest traditional objective function ie likelihood output response give input message unsuited response generation task instead propose use maximum mutual information mmi objective function neural model experimental result demonstrate propose mmi model produce diverse interest appropriate responses yield substantive gain bleu score two conversational datasets human evaluations
work analyze utility two dimensional document map exploratory analysis polish case law start compare two methods generate visualizations first base linear principal component analysis pca second make use modern nonlinear distribute stochastic neighbor embed method sne apply pca sne corpus judgments different court poland emerge sne provide better interpretable result pca next test apply sne randomly select sample common court judgments correspond different keywords show sne case reveal hide topical structure document relate keywordpension conclusion find sne method could promise tool facilitate exploitative analysis legal texts eg complement search browse functionality legal databases
recently lot interest learn common representations multiple view data typically common representations learn use parallel corpus two view say 1m image english caption work address real world scenario direct parallel data available two view interest say v1 v2 parallel data available view pivot view v3 propose model learn common representation v1 v2 v3 use parallel data available v1v3 v2v3 propose model generic even work n view interest one pivot view act bridge two specific downstream applications focus transfer learn languages l1l2ln use pivot language l ii cross modal access image language l1 use pivot language l2 model achieve state art performance multilingual document classification publicly available multilingual ted corpus promise result multilingual multimodal retrieval new dataset create release part work
paper present hybrid dialog state tracker combine rule base machine learn base approach belief state track therefore call hybrid tracker machine learn tracker realize long short term memory lstm network knowledge hybrid tracker set new state art result dialog state track challenge dstc two dataset system use live slu input
paper present result experiment next utterance rank ubuntu dialog corpus largest publicly available multi turn dialog corpus first use house implementation previously report model independent evaluation use data second evaluate performances various lstms bi lstms cnns dataset third create ensemble average predictions multiple model ensemble improve performance achieve state art result next utterance rank dataset finally discuss future plan use corpus
text alignment text quality critical accuracy machine translation mt systems nlp tool text process task require bilingual data research propose language independent bi sentence filter approach base polish position sensitive language english experiment clean approach develop ted talk corpus also initially test wikipedia comparable corpus use text domain language pair propose approach implement various heuristics sentence comparison leverage synonyms semantic structural analysis text additional information minimization data loss ensure improvement mt system score text process use tool discuss
machine translation evolve quite rapidly term quality nowadays several machine translation systems available web provide reasonable translations however systems perfect quality may decrease specific domains paper examine effect different train methods come polish english statistical machine translation system use medical data numerous elements emea parallel text corpora relate opus open subtitle project use grind creation phrase table different language model include development tune test translation systems bleu nist meteor ter metrics use order evaluate result various systems experiment deal systems include pos tag factor phrase model hierarchical model syntactic taggers alignment methods also execute deep analysis polish data preparatory work automatize data process true case punctuation normalization phase normalize metrics use compare result score lower fifteen mean machine translation engine unable provide satisfy quality score greater thirty mean translations understandable without problems score fifty reflect adequate translations average result polish english translations score bleu nist meteor ter relatively high range seven thousand and fifty-eight eight thousand, two hundred and seventy-two lowest score six thousand, four hundred and thirty-eight average result range english polish translations little lower six thousand, seven hundred and fifty-eight seven thousand, eight hundred and ninety-seven real life implementations present high quality machine translation systems anticipate general medical practice telemedicine
objective natural language process methods medical auto cod automatic generation medical bill cod electronic health record generally assign code independently others may thus assign cod closely relate procedures diagnose document even tend occur together practice simply right choice difficult infer clinical narrative materials methods propose method inject awareness propensities code co occurrence process first model train estimate conditional probability one code assign human coder give another code know assign document runtime iterative algorithm use apply model output exist statistical auto coder modify confidence score cod result test method combination primary auto coder icd ten procedure cod achieve twelve relative improvement f score primary auto coder baseline discussion propose method use appropriate feature combination auto coder generate cod different level confidence conclusion promise result obtain icd ten procedure cod suggest propose method may wider applications auto cod
year nara institute science technology naist submission two thousand and fifteen workshop asian translation base syntax base statistical machine translation addition reranking component use neural attentional machine translation model experiment confirm result previous work state neural mt reranking provide large gain objective evaluation measure bleu also confirm first time result also carry manual evaluation perform detail analysis reason increase find main contributions neural model lie improvement grammatical correctness output oppose improvements lexical choice content word
bidirectional long short term memory recurrent neural network blstm rnn show effective tag sequential data eg speech utterances handwritten document word embed demo powerful representation characterize statistical properties natural language study propose use blstm rnn word embed part speech pos tag task test penn treebank wsj test set state art performance nine thousand, seven hundred and forty tag accuracy achieve without use morphological feature approach also achieve good performance comparable stanford pos tagger
rational speech act rsa model treat language use recursive process probabilistic speaker listener agents reason intentions enrich literal semantics language along broadly gricean line rsa show capture many kinds conversational implicature criticize unrealistic model speakers far require manual specification semantic lexicon prevent use natural language process applications learn lexical knowledge data address concern show define optimize train statistical classifier use intermediate agents rsa hide layer representation form non linear activation function treatment open new application domains new possibilities learn effectively data validate model referential expression generation task show best performance achieve incorporate feature approximate well establish insights natural language generation rsa
paper propose joint algorithm word segmentation chinese social media previous work mainly focus word segmentation plain chinese text order develop chinese social media process tool need take main feature social media account whose grammatical structure rigorous tendency use colloquial internet term make exist chinese process tool inefficient obtain good performance social media approach combine crf mmseg algorithm extend feature traditional crf algorithm train model word segmentation use internet lexicon order improve performance model chinese social media experimental result sina weibo show approach outperform state art model
research statistical parse english enjoy decade successful result however adapt model languages meet difficulties previous comparative work show modern arabic one difficult languages parse due rich morphology free word order classical arabic ancient form arabic understudy computational linguistics relative worldwide reach language quran thesis base seven publications make significant contributions knowledge relate annotate parse classical arabic central argument thesis use hybrid representation closely align traditional grammar lead improve parse arabic test hypothesis two approach compare reference pure dependency parser adapt use graph transformations result eight thousand, seven hundred and forty-seven f1 score compare integrate parse model f1 score eight thousand, nine hundred and three demonstrate joint dependency constituency parse better suit classical arabic
run time complexity state art inference algorithms graph base dependency parse super linear number input word n recently prune algorithms model show cut large portion graph edge minimal damage result parse tree solve inference problem run time complexity determine solely number edge hence obvious importance propose inference algorithm first order model encode problem minimum span tree mst problem undirected graph allow us utilize state art undirected mst algorithms whose run time ofm expectation high probability direct parse tree infer undirected mst subsequently improve respect direct parse model local greedy update step run ofn time experiment eighteen languages variant first order mstparser mcdonald et al 2005b employ algorithm perform similarly original parser run ofn2 direct mst inference
develop novel technique parse english sentence abstract mean representation amr use searn learn search approach model concept relation learn unify framework evaluate parser multiple datasets vary domains show absolute improvement two six state art additionally show use frequent concept give us baseline stronger state art concept prediction plan release parser public use
paper provide overview various project carry within iso committee tc thirty-seven sc four deal management language digital resources basis technical experience gain committee wider standardization landscape paper identify possible trend future
popular approach sentence compression formulate task constrain optimization problem solve integer linear program ilp tool unfortunately dependence ilp may make compressor prohibitively slow thus approximation techniques propose often complex offer moderate gain speed alternative solution introduce novel compression algorithm generate k best compressions rely local deletion decisions algorithm two order magnitude faster recent ilp base method produce better compressions moreover extensive evaluation demonstrate quality compressions degrade much move single best top five result
online write lack non verbal cue present face face communication provide additional contextual information utterance speaker intention affective state fill void number orthographic feature emoticons expressive lengthen non standard punctuation become popular social media service include twitter instagram recently emojis introduce social media increasingly popular raise question whether predefined pictographic character come replace earlier orthographic methods paralinguistic communication abstract attempt would light question use match approach causal inference test whether adoption emojis cause individual users employ fewer emoticons text twitter
derive prior polarity lexica sentiment analysis positive negative score associate word context challenge task usually trade precision coverage hard find depend methodology use build lexicon manually annotate lexica provide high precision lack coverage whereas automatic derivation pre exist knowledge guarantee high coverage cost lower precision since automatic derivation prior polarities less time consume manual annotation great bloom approach particular base sentiwordnet resource paper compare frequently use techniques base sentiwordnet newer ones blend learn framework call ensemble method take advantage manually build prior polarity lexica ensemble method better able predict prior value unseen word outperform sentiwordnet approach use technique build sentiwords prior polarity lexicon approximately one hundred and fifty-five thousand word high precision high coverage finally show sentiment analysis task use lexicon allow us outperform single metrics derive sentiwordnet popular manually annotate sentiment lexica
learn representations semantic relations important various task analogy detection relational search relation classification although several proposals learn representations individual word learn word representations explicitly capture semantic relations word remain develop propose unsupervised method learn vector representations word learn representations sensitive semantic relations exist two word first extract lexical pattern co occurrence contexts two word corpus represent semantic relations exist two word second represent lexical pattern weight sum representations word co occur lexical pattern third train binary classifier detect relationally similar vs non similar lexical pattern pair propose method unsupervised sense lexical pattern pair use train data automatically sample corpus without require manual intervention propose method statistically significantly outperform current state art word representations three benchmark datasets proportional analogy detection demonstrate ability accurately capture semantic relations among word
recent years variety research discourse parse particularly rst discourse parse recent work rst parse focus implement new type feature learn algorithms order improve accuracy relatively little focus efficiency robustness practical use also implementations widely available describe rst segmentation parse system adapt model feature set various previous work describe accuracy near state art develop fast robust practical example process short document news article essay less second
text segmentation task essential process task many natural language process nlp text summarization text translation dialogue language understand among others turn segmentation consider key player dialogue understand task build automatic human computer systems paper introduce novel approach turn segmentation utterances egyptian spontaneous dialogues instance message i use machine learn ml approach part automatic understand egyptian spontaneous dialogues i task due lack egyptian dialect dialogue corpus system evaluate corpus include three thousand and one turn collect segment annotate manually egyptian call center system achieve f1 score nine thousand and seventy-four accuracy nine thousand, five hundred and ninety-eight
build dialogues systems interaction recently gain considerable attention resources systems build far tailor english indo european languages need design systems languages increase arabic language reason interest arabic dialogue act classification task key player arabic language understand build systems paper survey different techniques dialogue act classification arabic describe main exist techniques utterances segmentations classification annotation schemas test corpora arabic dialogues understand introduce literature
sarcasm consider one difficult problem sentiment analysis ob servation indonesian social media cer tain topics people tend criticize something use sarcasm propose two additional feature detect sarcasm common sentiment analysis conduct feature negativity information number interjection word also employ translate sentiwordnet sentiment classification classifications conduct machine learn algorithms experimental result show additional feature quite effective sarcasm detection
rise social media blog social network fuel interest sentiment analysis proliferation review rat recommendations form online expression online opinion turn kind virtual currency businesses look market products identify new opportunities manage reputations therefore many look field sentiment analysis paper present feature base sentence level approach arabic sentiment analysis approach use arabic idioms say phrase lexicon key importance improve detection sentiment polarity arabic sentence well number novels rich set linguistically motivate feature contextual intensifiers contextual shifter negation handle syntactic feature conflict phrase enhance sentiment classification accuracy furthermore introduce automatic expandable wide coverage polarity lexicon arabic sentiment word lexicon build gold standard sentiment word seed manually collect annotate expand detect sentiment orientation automatically new sentiment word use synset aggregation technique free online arabic lexicons thesauruses data focus modern standard arabic msa egyptian dialectal arabic tweet microblogs hotel reservation product review etc experimental result use resources techniques svm classifier indicate high performance level accuracies ninety-five
automatic speech recognition involve mainly two step feature extraction classification mel frequency cepstral coefficient use one prominent feature extraction techniques asr usually set twelve mfcc coefficients use feature vector classification step question whether improve classification accuracy achieve use subset twelve mfcc feature vector paper fisher ratio technique use select subset twelve mfcc coefficients contribute discriminate pattern select coefficients use classification hide markov model algorithm classification accuracies get use twelve coefficients use select coefficients compare
statistical study languages focus rank frequency distribution word instead introduce measure word rank change time call distribution emphrank diversity calculate diversity book publish six european languages since one thousand, eight hundred find follow universal lognormal distribution base mean standard deviation associate lognormal distribution define three different word regimes languages head consist word almost change rank time body word general use tail comprise context specific word vary rank considerably time head body reflect size language core identify linguists basic communication propose gaussian random walk model reproduce rank variation word time thus diversity rank diversity word understand result random variations rank size variation depend rank find core size similar languages study
present annotation schema part effort create manually annotate corpus arabic dialogue language understand include speak dialogue write chat dialogue inquiry answer domain propose schema handle mainly request response act occur frequently inquiry answer debate conversations express request service suggest offer apply propose schema eighty-three arabic inquiry answer dialogues
twitter popular social media outlet evolve vast source linguistic data rich opinion sentiment discussion due increase popularity twitter perceive potential exert social influence lead rise diverse community automatons commonly refer bots inorganic semi organic twitter entities range benevolent eg weather update bots help want alert bots malevolent eg spamming message advertisements radical opinions exist detection algorithms typically leverage meta data time tweet number followers etc identify robotic account present powerful classification scheme exclusively use natural language text organic users provide criterion identify account post automate message since classifier operate text alone flexible may apply textual data beyond twitter sphere
thesis present study integration information multiword expressions mwes parse combinatory categorial grammar ccg build previous work show benefit add information mwes syntactic parse implement similar pipeline ccg parse specifically collapse mwes one token train test data ccgbank corpus contain sentence annotate ccg derivations collapse algorithm however deal mwes form constituent data one limitations approach study effect collapse train test data parse effect obtain collapse data help parser decisions train effect obtain train collapse data improve result also collapse gold standard show model significantly outperform baseline model gold standard indicate train effect show baseline model perform significantly better gold standard data collapse parse data collapse parse indicate parse effect show result lead improve performance non collapse standard benchmark although fail show significantly conclude despite limit settings noticeable improvements use mwes parse discuss ways incorporation mwes parse improve hypothesize lead substantial result finally show turn mwe recognition part pipeline experimental part useful thing obtain different result different recognizers
word embed refer low dimensional dense vector representations natural word demonstrate power many natural language process task however may suffer inaccurate incomplete information contain free text corpus train data tackle challenge quite work leverage knowledge graph additional information source improve quality word embed although work achieve certain success neglect important facts knowledge graph many relationships knowledge graph emphmany one emphone many even emphmany many rather simply emphone one ii head entities tail entities knowledge graph come different semantic space address issue paper propose new algorithm name projectnet projecnet model relationships head tail entities transform different low rank projection matrices low rank projection allow non emphone one relationships entities different projection matrices head tail entities allow originate different semantic space experimental result demonstrate projectnet yield accurate word embed previous work thus lead clear improvements various natural language process task
state art name entity recognition ner systems rely handcraft feature output nlp task part speech pos tag text chunk work propose language independent ner system use automatically learn feature approach base charwnn deep neural network use word level character level representations embeddings perform sequential classification perform extensive number experiment use two annotate corpora two different languages harem corpus contain texts portuguese spa conll two thousand and two corpus contain texts spanish experimental result shade light contribution neural character embeddings ner moreover demonstrate neural network successfully apply pos tag also achieve state art result language independet ner use hyperparameters without handcraft feature harem corpus charwnn outperform state art system seventy-nine point f1 score total scenario ten ne class seventy-two point f1 selective scenario five ne class
knowledge graph embed refer project entities relations knowledge graph continuous vector space state art methods transe transh transr build embeddings treat relation translation head entity tail entity however previous model deal reflexive one many many one many many relations properly lack scalability efficiency thus propose novel method flexible translation name transf address issue transf regard relation translation head entity vector tail entity vector flexible magnitude evaluate propose model conduct link prediction triple classification benchmark datasets experimental result show method remarkably improve performance compare several state art baselines
translation memory tm systems one widely use translation technologies important part tm systems match algorithm determine translations get retrieve bank available translations assist human translator although detail account match algorithms use commercial systems find literature widely believe edit distance algorithms use paper investigate evaluate use several match algorithms include edit distance algorithm believe heart modern commercial tm systems paper present result show well various match algorithms correlate human judgments helpfulness collect via crowdsourcing amazon mechanical turk new algorithm base weight n gram precision adjust translator length preferences consistently return translations judge helpful translators multiple domains language pair
describe latest improvements ibm english conversational telephone speech recognition system techniques find beneficial maxout network anneal dropout rat network large number output train two thousand hours data joint model partially unfold recurrent neural network convolutional net combine bottleneck output layer retrain result model lastly sophisticate language model rescoring exponential neural network lms techniques result eighty word error rate switchboard part hub5 two thousand evaluation test set twenty-three relative better previous best publish result
development methods deal informative content text units match process major challenge automatic summary evaluation systems use fix n gram match limitation cause inaccurate match units peer reference summaries present study introduce new keyphrase base summary evaluator kpeval evaluate automatic summaries kpeval rely keyphrases since convey important concepts text evaluation process keyphrases use lemma form match text unit system apply evaluate different summaries arabic multi document data set present tac2011 result show new evaluation technique correlate well know evaluation systems rouge1 rouge2 rougesu4 autosummeng memog kpeval strongest correlation autosummeng memog pearson spearman correlation coefficient measure eight thousand, eight hundred and forty nine thousand, six hundred and sixty-seven respectively
nlp task differ semantic information require time single se mantic representation fulfill requirements logic base representations characterize sentence structure capture grade aspect mean distributional model give grade similarity rat word phrase capture sentence structure detail logic base approach argue two complementary adopt hybrid approach combine logic base distributional semantics probabilistic logic inference markov logic network mlns paper focus three components practical system integrate logical distributional model one parse task representation logic base part input problems represent probabilistic logic quite different represent standard first order logic two knowledge base construction form weight inference rule integrate compare distributional information source notably wordnet exist paraphrase collection particular use system evaluate distributional lexical entailment approach use variant robinson resolution determine necessary inference rule source easily add map logical rule system learn resource specific weight correct scale differences resources three discuss probabilistic inference show solve inference problems efficiently evaluate approach use task textual entailment rte utilize strengths logic base distributional representations particular focus sick dataset achieve state art result
mean word vary one domain another despite important domain dependence word semantics exist word representation learn methods bind single domain give pair emphsource emphtarget domains propose unsupervised method learn domain specific word representations accurately capture domain specific aspects word semantics first select subset frequent word occur domains emphpivots next optimize objective function enforce two constraints source target domain document pivot appear document must accurately predict co occur non pivot b word representations learn pivot must similar two domains moreover propose method perform domain adaptation use learn word representations propose method significantly outperform competitive baselines include state art domain insensitive word representations report best sentiment classification accuracies domain pair benchmark dataset
paper give overview share task 4th ccf conference natural language process chinese compute nlpcc two thousand and fifteen chinese word segmentation part speech pos tag micro blog texts different popular use newswire datasets dataset share task consist relatively informal micro texts share task two sub task one individual chinese word segmentation two joint chinese word segmentation pos tag subtask three track distinguish systems different resources first introduce dataset task characterize different approach participate systems report test result provide overview analysis result online system available open registration evaluation http nlpfudaneducn nlpcc2015
learn vector representation word important research field may benefit many natural language process task two limitations exist nearly available model bias cause context definition lack knowledge utilization difficult tackle algorithms essentially unsupervised learn approach inspire deep learn author propose supervise framework learn vector representation word provide additional supervise fine tune unsupervised learn framework knowledge rich approacher compatible numerical vectors word representation author perform intrinsic evaluation like attributional relational similarity prediction extrinsic evaluations like sentence completion sentiment analysis experiment result six embeddings four task ten datasets show propose fine tune framework may significantly improve quality vector representation word
introductory article present basics approach implement computational interpret natural language aim model mean word phrase unlike approach attempt define mean text fragment composable computer interpretable way discuss model ideas detect different type semantic incomprehension choose interpretation make sense give context knowledge representation design handle context sensitive uncertain imprecise knowledge easy accommodation new information store quantitative information capture essence concepts crucial work natural language understand reason still representation general enough allow new knowledge learn even generate system article conclude discuss reason relate topics possible approach generation new abstract concepts describe situations concepts word eg specify interpretation difficulties
paper exploratory analysis fraud detection take enron email corpus case study paper posit conclusions like strict servitude unquestionable faith among employees breed ground sham among higher executives also try infer nature communication fraudulent employees non fraudulent fraudulent employees
commonly acknowledge temporal expression extractors important components larger natural language process systems like information retrieval question answer systems extraction normalization temporal expressions turkish give attention far except extraction date time expressions within course name entity recognition timeml current standard temporal expression event annotation natural language texts paper present analysis temporal expressions turkish base relate timeml classification ie date time duration set expressions create lexicon turkish temporal expressions devise considerably wide coverage pattern use lexical class build block believe propose pattern together convenient normalization rule readily use prospective temporal expression extraction tool turkish
canonical correlation analysis cca method reduce dimension data represent use two view previously use derive word embeddings one view indicate word view indicate context describe way incorporate prior knowledge cca give theoretical justification test derive word embeddings evaluate myriad datasets
paper hypothesize chunk play important role reduce dependency distance dependency cross computer simulations compare natural languagesshow chunk reduce mean dependency distance mdd linear sequence nod constrain continuity projectivity natural languages interestingly chunk alone bring less dependency cross well though fail reduce rarity find human languages result suggest chunk may play vital role minimization dependency distance somewhat contribute role rarity dependency cross addition result point possibility rarity dependency cross mere side effect minimization dependency distance linguistic phenomenon motivations
recent work word embeddings show simple vector subtraction pre train embeddings surprisingly effective capture different lexical relations despite lack explicit supervision prior work evaluate intrigue result use word analogy prediction formulation hand select relations generality find broader range lexical relation type different learn settings evaluate paper carry evaluation two learn settings one spectral cluster induce word relations two supervise learn classify vector differences relation type find word embeddings capture surprise amount information suitable supervise train vector subtraction generalise well broad range relations include unseen lexical items
recent article christiansen chater two thousand and sixteen present fundamental constraint language ie never bottleneck arise fleet memory explore implications eg chunk pass process outline framework promise unify different areas research explore additional support constraint suggest connections quantitative linguistics information theory
paper propose algorithm improve calculation confidence measure speak term detection std give input query term algorithm first calculate measurement name document rank weight document speech database reflect relevance query term sum confidence measure hypothesize term occurrences document confidence measure term occurrence estimate linear interpolation calculate document rank weight improve reliability integrate document level information experiment conduct three standard std task tamil vietnamese english respectively experimental result demonstrate propose algorithm achieve consistent improvements state art method confidence measure calculation furthermore algorithm still effective even high accuracy speech recognizer available make applicable languages limit speech resources
statistical machine translation dialectal arabic characterize lack data since data acquisition involve transcription translation speak language study develop techniques extract parallel data one particular dialect arabic iraqi arabic domain corpora different dialects arabic modern standard arabic compare two different data selection strategies cross entropy base submodular selection demonstrate small highly target amount find data improve performance baseline machine translation system furthermore report preliminary experiment use automatically translate speech data additional train data
techniques unsupervised discovery acoustic pattern get increasingly attractive huge quantities speech data become available manual annotations remain hard acquire paper propose approach unsupervised discovery linguistic structure target speak language give raw speech data linguistic structure include two level subword like word like acoustic pattern lexicon word like pattern term subword like pattern n gram language model base word like pattern pattern model parameters automatically learn unlabelled speech corpus achieve initialization step follow three cascade stag acoustic linguistic lexical iterative optimization lexicon word like pattern define allow consecutive sequence hmms subword like pattern iteration model train decode produce update label lexicon hmms update way model parameters decode label respectively optimize iteration knowledge linguistic structure learn gradually layer layer propose approach test preliminary experiment corpus mandarin broadcast news include task speak term detection performance compare parallel test use model train supervise way result show propose system yield reasonable performance also complimentary exist large vocabulary asr systems
paper present new approach unsupervised speak term detection speak query use multiple set acoustic pattern automatically discover target corpus different pattern hmm configurationsnumber state per model number distinct model number gaussians per stateform three dimensional model granularity space different set acoustic pattern automatically discover different point properly distribute three dimensional space complementary one another thus jointly capture characteristics speak term represent speak content speak query sequence acoustic pattern series approach match pattern index sequence consider signal variations develop way line computation load reduce signal distributions cause different speakers acoustic condition reasonably take care result indicate approach significantly outperform unsupervised feature base dtw baseline one thousand, six hundred and sixteen mean average precision timit corpus
paper present novel approach enhance multiple set acoustic pattern automatically discover give corpus previous work propose different hmm configurations number state per model number distinct model acoustic pattern form two dimensional space multiple set acoustic pattern automatically discover hmm configurations properly locate different point two dimensional space show complementary one another jointly capture characteristics give corpus represent give corpus sequence acoustic pattern different hmm set pattern indices sequence relabeled consider context consistency across different sequence good improvements observe preliminary experiment pattern speak term detection std perform timit mandarin broadcast news enhance pattern
many fundamental problems natural language process rely determine entities appear give text commonly reference entity link step fundamental component many nlp task text understand automatic summarization semantic search machine translation name ambiguity word polysemy context dependencies heavy tail distribution entities contribute complexity problem propose probabilistic approach make use effective graphical model perform collective entity disambiguation input mention ielinkable token span disambiguate jointly across entire document combine document level prior entity co occurrences local information capture mention surround context model base simple sufficient statistics extract data thus rely parameters learn method require extensive feature engineer expensive train procedure use loopy belief propagation perform approximate inference low complexity model make step sufficiently fast real time usage demonstrate accuracy approach wide range benchmark datasets show match many case outperform exist state art methods
speech recognition systems often highly domain dependent fact widely report literature however concept domain complex bind clear criteria hence often evident data consider domain acoustic language model domain specific work paper concentrate acoustic model present novel method perform unsupervised discovery domains use latent dirichlet allocation lda model set hide domains assume exist data whereby audio segment consider weight mixture domain properties classification audio segment domains allow creation domain specific acoustic model automatic speech recognition experiment conduct dataset diverse speech data cover speech radio tv broadcast telephone conversations meet lecture read speech joint train set sixty hours test set six hours maximum posteriori map adaptation lda base domains show yield relative word error rate wer improvements sixteen relative compare pool train ten compare model adapt human label prior domain knowledge
label user utterances understand attend call dialogue act da classification consider key player dialogue language understand layer automatic dialogue systems paper propose novel approach user utterances label egyptian spontaneous dialogues instant message use machine learn ml approach without rely special lexicons cue rule due lack egyptian dialect dialogue corpus system evaluate multi genre corpus include four thousand, seven hundred and twenty-five utterances three domains collect annotate manually egyptian call center system achieve f1 score seventy thirty-six overall domains
revisit levin theory correspondence verb mean syntax infer semantic class large syntactic classification six hundred german verbs take clausal non finite arguments grasp mean components levin class know hard address challenge set multi perspective semantic characterization infer class end link infer class english translation independently construct semantic class three different lexicons german wordnet germanet verbnet framenet perform detail analysis evaluation result german english classification available wwwukptu darmstadtde modality verbclasses
describe set bilingual english french english german parallel corpora direction translation accurately reliably annotate corpora diverse consist parliamentary proceed literary work transcriptions ted talk political commentary instrumental research translationese applications human machine translation specifically use task translationese identification research direction enjoy grow interest recent years validate quality reliability corpora replicate previous result supervise unsupervised identification translationese extend experiment additional datasets languages
distant supervision widely apply approach automatic train relation extraction systems advantage generate large amount label data minimal effort however data may contain errors consequently systems train use distant supervision tend perform well base manually label data work propose novel method detect potential false negative train examples use knowledge inference method result show approach improve performance relation extraction systems train use distantly supervise data
university sheffield usfd participate international workshop speak language translation iwslt two thousand and fourteen paper introduce usfd slt system iwslt automatic speech recognition asr achieve two multi pass deep neural network systems adaptation rescoring techniques machine translation mt achieve phrase base system usfd primary system incorporate state art asr mt techniques give bleu score two thousand, three hundred and forty-five one thousand, four hundred and seventy-five english french english german speech text translation task iwslt two thousand and fourteen data usfd contrastive systems explore integration asr mt use quality estimation system rescore asr output optimise towards better translation give fifty-four twenty-six bleu improvement respectively iwslt two thousand and twelve two thousand and fourteen evaluation data
name entity recognition classification nerc process identification proper nouns text classification nouns certain predefined categories like person name location organization date time etc nerc kannada essential challenge task aim work develop novel model nerc base multinomial nai bay mnb classifier methodology adopt paper base feature extraction train corpus use term frequency inverse document frequency fit tf idf vectorizer paper discuss various issue develop propose model detail implementation performance evaluation discuss experiment conduct train corpus size ninety-five thousand, one hundred and seventy tokens test corpus five thousand tokens observe model work precision recall f1 measure eighty-three seventy-nine eighty-one respectively
recent issue pnas futrell et al claim study thirty-seven languages give first large scale cross language evidence dependency length minimization overstatement ignore similar previous research additionthis study seem pay attention factor like uniformity genreswhich weaken validity argument dlm universal another problem study set baseline random language projective fail truly uncover difference natural language random language since projectivity important feature many natural languages finally paper contend apparent relationship head finality dependency length despite lack explicit statistical comparison render conclusion rather hasty improper
compound highly productive word formation process languages often problematic natural language process applications paper investigate whether distributional semantics form word embeddings enable deeper ie knowledge rich process compound standard string base methods present unsupervised approach exploit regularities semantic vector space base analogies bookshop shop bookshelf shelf produce compound analyse high quality subsequent compound split algorithm base analyse highly effective particularly ambiguous compound german english machine translation experiment show semantic analogy base compound splitter lead better translations commonly use frequency base method
recently knowledge graph embed project symbolic entities relations continuous vector space become new hot topic artificial intelligence paper address new issue multiple relation semantics relation may multiple mean reveal entity pair associate correspond triple propose novel gaussian mixture model embed transg new model discover latent semantics relation leverage mixture relation component vectors embed fact triple best knowledge first generative model knowledge graph embed able deal multiple relation semantics extensive experiment show propose model achieve substantial improvements state art baselines
knowledge representation major topic ai many study attempt represent entities relations knowledge base continuous vector space among attempt translation base methods build entity relation vectors minimize translation loss head entity tail one spite success methods translation base methods also suffer oversimplify loss metric competitive enough model various complex entities relations knowledge base address issue propose textbftransa adaptive metric approach embed utilize metric learn ideas provide flexible embed method experiment conduct benchmark datasets propose method make significant consistent improvements state art baselines
paper describe free open source implementation light slide window lsw part speech tagger apertium free open source machine translation platform firstly mechanism train process tagger review new method incorporate linguistic rule propose secondly experiment conduct compare performances tagger different window settings without apertium style forbid rule without constraint grammar also respect traditional hmm tagger apertium
text classification widely study problem consider solve domains certain circumstances scenarios however receive little attention despite relevance applicability one scenarios early text classification one need know category document use partial information document process sequence term goal devise method make predictions fast possible importance variant text classification problem evident domains like sexual predator detection one want identify offender early possible paper analyze suitability standard naive bay classifier approach problem specifically assess performance classify document see increasingly number term simple modification standard naive bay implementation allow us make predictions partial information best knowledge naive bay use purpose throughout extensive experimental evaluation show effectiveness classifier early text classification show simple solution competitive compare state art methodologies elaborate foresee work pave way development effective early text classification techniques base naive bay formulation
investigate different approach dialect identification arabic broadcast speech use phonetic lexical feature obtain speech recognition system acoustic feature use vector framework study generative discriminate classifiers combine feature use multi class support vector machine svm validate result arabic english language identification task accuracy one hundred use feature binary classifier discriminate modern standard arabic msa dialectal arabic accuracy one hundred report result use propose method discriminate five widely use dialects arabic namely egyptian gulf levantine north african msa accuracy fifty-two discuss dialect identification errors context dialect code switch dialectal arabic msa compare error pattern manually label data output classifier also release train test data standard corpus dialect identification
swiss avalanche bulletin produce twice day four languages due lack time available manual translation fully automate translation system employ base catalogue predefined phrase predetermine rule phrase combine produce sentence catalogue phrase limit small sublanguage system able automatically translate sentence german target languages french italian english without subsequent proofread correction operational two winter season assess quality produce texts base two different survey participants rat texts real avalanche bulletin origins catalogue phrase versus manually write translate texts mean recognition rate fifty-five users hardly distinguish thetwo type texts give similar rat respect language quality overall output catalogue system consider virtually equivalent text write avalanche forecasters manually translate professional translators furthermore forecasters declare relevant situations capture system sufficient accuracy forecaster work load change introduction catalogue extra time find match sentence compensate fact longer need double check manually translate texts reduction daily translation cost expect offset initial development cost within years
propose new model learn bilingual word representations non parallel document align data follow recent advance word representation learn model learn dense real value word vectors bilingual word embeddings bwes unlike prior work induce bwes heavily rely parallel sentence align corpora readily available translation resources dictionaries article reveal bwes may learn solely basis document align comparable data without additional lexical resources syntactic information present comparison approach previous state art model learn bilingual word representations comparable data rely framework multilingual probabilistic topic model muptm well distributional local context count model demonstrate utility induce bwes two semantic task one bilingual lexicon extraction two suggest word translations context polysemous word simple yet effective bwe base model significantly outperform muptm base context count representation model comparable data well prior bwe base model acquire best report result task three test language pair
document describe odin framework domain independent platform develop rule base event extraction model odin aim powerful rule language allow model complex syntactic structure robust recover syntactic parse errors syntactic pattern freely mix surface token base pattern remain simple domain grammars run minutes fast odin process one hundred sentence second real world domain two hundred rule include thorough definition odin rule language together description odin api scala language allow one apply rule arbitrary texts
state art benchmarks twitter sentiment analysis consider fact half tweet public stream distinct sentiment choose paper provide new perspective twitter sentiment analysis highlight necessity explicitly incorporate uncertainty moreover dataset high quality evaluate solutions new problem introduce make publicly available
new generation emoticons call emojis increasingly use mobile communications social media past two years ten billion emojis use twitter emojis unicode graphic symbols use shorthand express concepts ideas contrast small number well know emoticons carry clear emotional content hundreds emojis emotional content provide first emoji sentiment lexicon call emoji sentiment rank draw sentiment map seven hundred and fifty-one frequently use emojis sentiment emojis compute sentiment tweet occur engage eighty-three human annotators label sixteen million tweet thirteen european languages sentiment polarity negative neutral positive four annotate tweet contain emojis sentiment analysis emojis allow us draw several interest conclusions turn emojis positive especially popular ones sentiment distribution tweet without emojis significantly different inter annotator agreement tweet emojis higher emojis tend occur end tweet sentiment polarity increase distance observe significant differences emoji rank thirteen languages emoji sentiment rank consequently propose emoji sentiment rank european language independent resource automate sentiment analysis finally paper provide formalization sentiment novel visualization form sentiment bar
divide oral histories topically coherent segment make accessible online people regularly make judgments coherent segment extract oral histories make judgments tax automate assistance potentially attractive speed task extract segment open end interview different people ask extract coherent segment oral histories often agree precisely segment begin end low agreement make evaluation algorithmic segmenters challenge reason believe segment oral history transcripts approach promise others bayesseg algorithm perform slightly better texttiling texttiling perform significantly better uniform segmentation bayesseg might use suggest boundaries someone segment oral histories segmentation task need better define
research explore effect various train settings polish english statistical machine translation systems speak language various elements ted parallel text corpora iwslt two thousand and fourteen evaluation campaign use basis train language model development tune test translation system well wikipedia base comparable corpora prepare us bleu nist meteor ter metrics use evaluate effect data preparations translation result experiment include systems use lemma morphological information polish word also conduct deep analysis provide polish data preparatory work automatic data correction clean phase
language allow complex ideas communicate symbolic sequence characteristic feature species manifest multitude form use large write corpora many different languages script show occurrence probability distributions sign leave right end word distinct heterogeneous nature characterize asymmetry use quantitative inequality measure viz information entropy gini index show begin word less restrictive sign usage end property simply attributable use common affix see even word root consider use existence asymmetry infer direction write undeciphered inscriptions agree archaeological evidence unlike traditional investigations phonotactic constraints focus language specific pattern study reveal property valid across languages write systems language write unique aspects species universal signature may reflect innate feature human cognitive phenomenon
paper go find mean word base distinct situations word sense disambiguation use find mean word base live contexts use supervise unsupervised approach unsupervised approach use online dictionary learn supervise approach use manual learn set hand tag data populate might effective sufficient learn procedure limitation information main flaw supervise approach propose approach focus overcome limitation use learn set enrich dynamic way maintain new data trivial filter method utilize achieve appropriate train data introduce mix methodology modify lesk approach bag word enrich bag use learn methods approach establish superiority individual modify lesk bag word approach base experimentation
distributional model learn rich semantic word representations success story recent nlp research however develop model learn useful representations phrase sentence prove far harder propose use definitions find everyday dictionaries mean bridge gap lexical phrasal semantics neural language embed model effectively train map dictionary definitions phrase lexical representations word define definitions present two applications architectures reverse dictionaries return name concept give definition description general knowledge crossword question answerers task neural language embed model train definitions handful freely available lexical resources perform well better exist commercial systems rely significant task specific engineer result highlight effectiveness neural embed architectures definition base train develop model understand phrase sentence
machine dialect interpretation assume real part encourage man machine correspondence addition men men correspondence natural language process nlp machine translation mt allude utilize machine change one dialect alternate statistical machine translation type mt consist language model lm translation model tm decoder paper bengali assamese statistical machine translation model create utilize moses translation tool like irstlm language model giza pp v107 translation model utilize within framework accessible linux situations purpose lm encourage fluent output purpose tm encourage similarity input output decoder increase probability translate text target language parallel corpus seventeen thousand, one hundred sentence bengali assamese utilize prepare within framework measurable mt procedures far generally investigate indian dialects might intrigue discover degree model help immense continuous mt deliberations nation
ability classify speak speech base style speak important problem advent bpo recent time specifically cater population local population become necessary bpo identify people certain style speak american british etc today bpo employ accent analysts identify people require style speak process involve human bias become increasingly infeasible high attrition rate bpo industry paper propose new metric robustly accurately help classify speak speech base style speak role propose metric substantiate use classify real speech data collect seventy different people work bpo compare performance metric human experts independently carry classification process experimental result show performance system use novel metric perform better two different human expert
general self help systems increasingly deploy service base industries capable deliver better customer service increasingly switch voice base self help systems provide natural interface human interact machine speech base self help system ideally need speech recognition engine convert speak speech text addition language process engine take care misrecognitions speech recognition engine shelf speech recognition engine generally combination acoustic process speech grammar norm believe ideally speech recognition application addition speech recognition engine separate language process engine give system better performance paper discuss ways speech recognition engine language process engine combine give better user experience
paper contribute joint embed model predict relations pair entities scenario relation inference differ stand alone approach separately operate either knowledge base free texts propose model simultaneously learn low dimensional vector representations triplets knowledge repositories mention relations free texts leverage evidence resources make accurate predictions use nell evaluate performance approach compare cut edge methods result extensive experiment show model achieve significant improvement relation extraction
several characteristics write texts infer statistical analysis derive network model even though many network measurements adapt study textual properties several level complexity textual aspects disregard paper study symmetry word adjacency network well know representation text graph statistical analysis symmetry distribution perform several novels show word display symmetric pattern connectivity specifically merge symmetry display distribution similar ubiquitous power law distribution experiment also reveal study metrics correlate traditional network measurements degree betweenness centrality effectiveness symmetry measurements verify authorship attribution task interestingly find specific author prefer particular type symmetric motifs consequence authorship book could accurately identify eight hundred and twenty-five case dataset comprise book write eight author propose measurements text analysis complementary traditional approach use improve characterization text network might useful relate applications rely identification topical word information retrieval
applications involve conversational speech data sparsity limit factor build better language model propose simple language independent method quickly harvest large amount data twitter supplement smaller train set closely match domain techniques lead significant reduction perplexity four low resource languages even though presence twitter languages relatively small also find twitter text useful learn word class domain text use word class lead reductions perplexity additionally introduce method use social textual information prioritize download queue twitter crawl maximize amount useful data collect impact perplexity vocabulary coverage
pronoun resolution system require limit syntactic knowledge identify antecedents personal reflexive pronouns turkish present counterparts languages like english spanish french core system constraints preferences determine empirically evaluation phase perform considerably better baseline algorithm use comparison system significant first fully specify knowledge poor computational framework pronoun resolution turkish turkish possess different structural properties languages knowledge poor systems develop
word embeddings distribute word representations learn unlabelled data show high utility many natural language process applications paper perform extrinsic evaluation five popular word embed methods context four sequence label task pos tag syntactic chunk ner mwe identification particular focus paper analyse effect task base update word representations show use word embeddings feature several hundred train instance sufficient achieve competitive result word embeddings lead improvements oov word domain perhaps surprisingly result indicate little difference different word embed methods simple brown cluster often competitive word embeddings across task consider
recent years numerous study infer personality traits people online write study encourage information need order use techniques confidence linguistic feature vary across different online media much text require representative sample person paper examine several large set online user generate text draw twitter email blog online discussion forums examine compare population wide result linguistic measure liwc infer traits big5 personality basic human value also empirically measure stability traits across different size sample individual result highlight importance tune model online medium include guidelines minimum amount text require representative result
entity disambiguation aim link mention ambiguous entities knowledge base eg wikipedia model topical coherence crucial task base assumption information semantic context tend belong topic paper present novel deep semantic relatedness model dsrm base deep neural network dnn semantic knowledge graph kgs measure entity semantic relatedness topical coherence model dsrm directly train large scale kgs map heterogeneous type knowledge entity kgs numerical feature vectors latent space distance two semantically relate entities minimize compare state art relatedness approach propose milne witten 2008a dsrm obtain one hundred and ninety-four two hundred and forty-five reductions entity disambiguation errors two publicly available datasets respectively
paper present pattern base method use infer adjectival scale corpus specifically propose method use lexical pattern automatically identify order pair scalemates follow filter phase unrelated pair discard filter phase several different similarity measure implement compare model present paper evaluate use current standard along novel evaluation set show least good current state art
distribute vector representations natural language vocabulary get lot attention contemporary computational linguistics paper summarize experience apply neural network language model task calculate semantic similarity russian experiment perform course russian semantic similarity evaluation track model take 2nd 5th position depend task introduce tool corpora use comment nature share task describe achieve result find continuous skip gram continuous bag word model previously successfully apply english material use semantic model russian well moreover show texts russian national corpus rnc provide excellent train material model outperform much larger corpora especially true semantic relatedness task although stack model train larger corpora top rnc model improve performance even high quality semantic vectors learn way use variety linguistic task promise excite field study
paper report comparison subject object verbs usage phishing email legitimate email purpose research explore whether syntactic structure subject object verbs distinguishable feature phishing detection achieve objective conduct two series experiment syntactic similarity sentence subject object verb comparison result experiment indicate feature use verbs work do others
sequence sequence translation methods base generation side condition language model recently show promise result several task machine translation model condition source side word use produce target language text image caption model condition image use generate caption text past work approach focus large vocabulary task measure quality term bleu paper explore applicability model qualitatively different grapheme phoneme task input output side vocabularies small plain n gram model well credit give output exactly correct find simple side condition generation approach able rival state art able significantly advance stat art bi directional long short term memory lstm neural network use alignment information use conventional approach
describe approach create diverse set predictions spectral learn latent variable pcfgs l pcfgs approach work create multiple spectral model noise add underlie feature train set estimation model describe three ways decode multiple model addition describe simple variant spectral algorithm l pcfgs fast lead compact model experiment natural language parse english german show get significant improvement baselines comparable state art english achieve f1 score nine thousand and eighteen german achieve f1 score eight thousand, three hundred and thirty-eight
representation learn knowledge base kbs aim embed entities relations low dimensional space exist methods consider direct relations representation learn argue multiple step relation paths also contain rich inference pattern entities propose path base representation learn model model consider relation paths translations entities representation learn address two key challenge one since relation paths reliable design path constraint resource allocation algorithm measure reliability relation paths two represent relation paths via semantic composition relation embeddings experimental result real world datasets show compare baselines model achieve significant consistent improvements knowledge base completion relation extraction text
paper propose two new feature estimate phrase base machine translation parameters mainly monolingual data method base two recently introduce neural network vector representation model word sentence first time model use end end phrase base machine translation system score obtain method recover eighty bleu loss cause remove phrase table probabilities also show feature combine phrase table probabilities improve bleu score absolute seventy-four point
paper present novel approach medical synonym extraction aim integrate term embed medical domain knowledge healthcare applications one advantage method scalable experiment dataset 1m term pair show propose approach outperform baseline approach large margin
present three pronged approach improve statistical machine translation smt build recent success application neural network smt first propose new feature base neural network model various non local translation phenomena second augment architecture neural network tensor layer capture important higher order interaction among network units third apply multitask learn estimate neural network parameters jointly propose methods result significant improvements complementary overall improvement twenty-seven eighteen bleu point arabic english chinese english translation state art system already include neural network feature
article present analysis influence context information dialog act recognition perform experiment widely explore switchboard corpus well data annotate accord recent iso twenty-four thousand, six hundred and seventeen two standard latter obtain tilburg dialogbank map annotations subset let us go corpus use classification approach base svms prove successful previous work allow us limit amount context information provide way able observe influence pattern amount context information increase base feature consist n grams punctuation wh word context information obtain one five precede segment provide either n grams dialog act classifications latter typically lead better result stable influence pattern addition conclusions importance influence context information experiment switchboard corpus also lead result advance state art dialog act recognition task corpus furthermore result obtain data annotate accord iso twenty-four thousand, six hundred and seventeen two standard define baseline future work contribute standardization experiment area
natural language generation coherent long texts like paragraph longer document challenge problem recurrent network model paper explore important step toward generation task train lstm long short term memory auto encoder preserve reconstruct multi sentence paragraph introduce lstm model hierarchically build embed paragraph embeddings sentence word decode embed reconstruct original paragraph evaluate reconstruct paragraph use standard metrics like rouge entity grid show neural model able encode texts way preserve syntactic semantic discourse coherence first step toward generate coherent text units neural model work potential significantly impact natural language generation summarizationfootnotecode three model describe paper find wwwstanfordedu jiweil
neural network successfully apply many nlp task result vector base model difficult interpret example clear achieve compositionality build sentence mean mean word phrase paper describe four strategies visualize compositionality neural model nlp inspire similar work computer vision first plot unit value visualize compositionality negation intensification concessive clauses allow us see well know markedness asymmetries negation introduce three simple straightforward methods visualize unit salience amount contribute final compose mean one gradient back propagation two variance token average word node three lstm style gate measure information flow test methods sentiment use simple recurrent net lstms general purpose methods may wide applications understand compositionality semantic properties deep network also would light lstms outperform simple recurrent net
learn distinct representation sense ambiguous word could lead powerful fine grain model vector space representations yet multi sense methods propose test artificial word similarity task know improve real natural language understand task paper introduce multi sense embed model base chinese restaurant process achieve state art performance match human word similarity judgments propose pipelined architecture incorporate multi sense embeddings language understand test performance model part speech tag name entity recognition sentiment analysis semantic relation identification semantic relatedness control embed dimensionality find multi sense embeddings improve performance task part speech tag semantic relation identification semantic relatedness others name entity recognition various form sentiment analysis discuss differences may cause different role word sense information task result highlight importance test embed model real applications
interest statistical machine translation systems increase currently due political social events world propose statistical machine translation smt base model use translate sentence source language english target language arabic automatically efficiently incorporate different statistical natural language process nlp model language model alignment model phrase base model reorder model translation model model combine enhance performance statistical machine translation smt many implementation tool use work moses gizaa irstlm kenlm bleu base implementation evaluation model compare generate translation implement machine translation systems like google translate prove propose model enhance result statistical machine translation form reliable efficient model field research
although fair amount work sentiment analysis sa opinion mine om systems last decade respect performance systems still desire performance especially morphologically rich language mrl arabic due complexities challenge exist nature languages one challenge detection idioms proverbs phrase within writer text comment idiom proverb form speech expression peculiar grammatically understand individual mean elements yield different sentiment treat separate word consequently order facilitate task detection classification lexical phrase automate sa systems paper present aipselex novel idioms proverbs sentiment lexicon modern standard arabic msa colloquial aipselex manually collect annotate sentence level semantic orientation positive negative efforts manually build annotate lexicon report moreover build classifier extract idioms proverbs phrase text use n gram similarity measure methods finally several experiment carry various data include arabic tweet arabic microblogs hotel reservation product review tv program comment publicly available arabic online review websites social media blog forums e commerce web sit evaluate coverage accuracy aipselex
quality quantity article wikipedia language vary greatly translate another wikipedia natural way add content translation process properly support software use wikipedia past computer assist translation tool build wikipedia commonly use create tool adapt specific need open community kind content wikipedia qualitative quantitative data indicate new tool help users translate article easier faster
current distribute representations word show little resemblance theories lexical semantics former dense uninterpretable latter largely base familiar discrete class eg supersenses relations eg synonymy hypernymy propose methods transform word vectors sparse optionally binary vectors result representations similar interpretable feature typically use nlp though discover automatically raw corpora vectors highly sparse computationally easy work importantly find outperform original vectors benchmark task
twitter often use quantitative study identify geographically prefer topics write style entities study rely either gps coordinate attach individual message user supply location field profile paper compare data acquisition techniques quantify bias introduce also measure effect linguistic analysis text base geolocation gps tag self report locations yield measurably different corpora linguistic differences partially attributable differences dataset composition age gender use latent variable model induce age gender show demographic variables interact geography affect language use also show accuracy text base geolocation vary population demographics give best result men age forty
introduce corpus seven thousand and thirty-two sentence rat human annotators formality informativeness implicature one seven scale corpus annotate use amazon mechanical turk reliability obtain judgments examine compare mean rat across two mturk experiment correlation pilot annotations sentence formality conduct control set despite subjectivity inherent difficulty annotation task correlations mean rat quite encourage especially formality informativeness explore correlation three linguistic variables genre wise variation rat correlations within genres compatibility automatic stylistic score sentential make document term style date corpus largest sentence level annotate corpus release formality informativeness implicature
natural language process nlp systems commonly leverage bag word co occurrence techniques capture semantic syntactic word relationships result word level distribute representations often ignore morphological information though character level embeddings prove valuable nlp task propose new neural language model incorporate word order character order embed model produce several vector space meaningful substructure evidence performance eight hundred and fifty-eight recent word analogy task exceed best publish syntactic word analogy score fifty-eight error margin furthermore model include several parallel train methods notably allow skip gram network one hundred and sixty billion parameters train overnight three multi core cpus 14x larger previous largest neural network
particular choice predicate eg x violate writer subtly connote range imply sentiments presuppose facts entities x one writer perspective project x antagonistand victim two entities perspective probably dislike x three effect something bad happen four value something valuable five mental state distress event introduce connotation frame representation formalism organize rich dimension connotation use type relations first investigate feasibility obtain connotative label crowdsourcing experiment present model predict connotation frame verb predicate base distributional word representations interplay different type connotative relations empirical result confirm connotation frame induce various data source reflect people use language give rise connotative mean conclude analytical result show potential use connotation frame analyze subtle bias online news media
abstract mean representation amr representation open domain rich semantics potential use field like event extraction machine translation node generation typically do use simple dictionary lookup currently important limit factor amr parse propose small set action derive amr subgraphs transformations span text allow robust learn stage set construction action generalize better previous approach learn simple classifier improve previous state art result amr parse boost end end performance three f1 ldc2013e117 ldc2014t12 datasets
communicative interactions involve kind procedural knowledge use human brain process verbal nonverbal input language production although considerable work do model human language abilities difficult bring together comprehensive tabula rasa system compatible current knowledge verbal information process brain work present cognitive system entirely base large scale neural architecture develop would light procedural knowledge involve language elaboration main component system central executive supervise system coordinate components work memory model central executive neural network take input neural activation state short term memory yield output mental action control flow information among work memory components neural gate mechanisms propose system capable learn communicate natural language start tabula rasa without priori knowledge structure phrase mean word role different class word interact human text base interface use open end incremental learn process able learn nouns verbs adjectives pronouns word class use expressive language model validate corpus one thousand, five hundred and eighty-seven input sentence base literature early language assessment level four years old child produce five hundred and twenty-one output sentence express broad range language process functionalities
build unify timelines collection write news article require cross document event coreference resolution temporal relation extraction paper present approach event coreference resolution accord similar temporal information b similar semantic arguments temporal information detect use automatic temporal information system tipsem semantic information represent mean lda topic model evaluation approach show obtain highest micro average f score result semeval2015 task four timeline cross document event order two thousand, five hundred and thirty-six trackb two thousand, three hundred and fifteen subtrackb improvement six comparison systems however experiment also show draw back topic model approach degrade performance system
paraphrase database ppdb ganitkevitch et al two thousand and thirteen extensive semantic resource consist list phrase pair heuristic confidence estimate however still unclear best use due heuristic nature confidences necessarily incomplete coverage propose model leverage phrase pair ppdb build parametric paraphrase model score paraphrase pair accurately ppdb internal score simultaneously improve coverage allow learn phrase embeddings well improve word embeddings moreover introduce two new manually annotate datasets evaluate short phrase paraphrase model use paraphrase model train use ppdb achieve state art result standard word bigram similarity task beat strong baselines new short phrase paraphrase task
propose imaginet model learn visually ground representations language couple textual visual input model consist two gate recurrent unit network share word embeddings use multi task objective receive textual description scene try concurrently predict visual representation next word sentence mimic important aspect human language learn acquire mean representations individual word descriptions visual scenes moreover learn effectively use sequential structure semantic interpretation multi word phrase
dictionary base lemmatizer bulgarian language present distribute free software publicly available download use gpl v3 license present software write entirely java distribute gate plugin best knowledge time write article free lemmatization tool specifically target bulgarian language present lemmatizer work progress currently yield accuracy ninety-five comparison manually annotate corpus bultreebank morph contain two hundred and seventy-three thousand, nine hundred and thirty-three tokens
paper reveal result analysis accuracy develop software automatic lemmatization bulgarian language lemmatization software write entirely java distribute gate plugin certain statistical methods use define accuracy software result analysis show ninety-five lemmatization accuracy
propose simple scalable fully generative model transition base dependency parse high accuracy model parameterized hierarchical pitman yor process overcome limitations previous generative model allow fast accurate inference propose efficient decode algorithm base particle filter adapt beam size uncertainty model jointly predict pos tag parse tree uas parser par greedy discriminative baseline language model obtain better perplexity n gram model perform semi supervise learn large unlabelled corpus show model able generate locally syntactically coherent sentence open door applications language generation
present work semi supervise parse natural language sentence focus multi source crosslingual transfer delexicalized dependency parsers first evaluate influence treebank annotation style parse performance focus adposition attachment style present klcpos3 empirical language similarity measure design tune source parser weight multi source delexicalized parser transfer finally introduce novel resource combination method base interpolation train parser model
low frequency word place major challenge automatic speech recognition asr probabilities word often important name entities generally estimate language model lm due limit occurrences train data recently propose word pair approach deal problem borrow information frequent word enhance probabilities low frequency word paper present extension word pair method involve multiple predict word produce better estimation low frequency word also employ approach deal language word task multi lingual speech recognition
data drive representation learn word technique central importance nlp indisputably useful source feature downstream task vectors tend consist uninterpretable components whose relationship categories traditional lexical semantic theories tenuous best present method construct interpretable word vectors hand craft linguistic resources like wordnet framenet etc vectors binary ie contain zero one nine hundred and ninety-nine sparse analyze performance state art evaluation methods distributional model word vectors find competitive standard distributional approach
quest give formal compositional semantics natural languages semanticists start turn attention phenomena also consider part pragmatics eg discourse anaphora presupposition projection account phenomena kinds mean assign word phrase often revisit specific prevalent paradigm model natural language denotations use simply type lambda calculus higher order logic mean revisit type denotations assign individual part speech however lambda calculus also serve fundamental theory computation study computation similar type shift employ give mean side effect side effect program languages correspond action go beyond lexical scope expression throw exception might propagate throughout program variable modify one point might later read another even beyond scope program program might interact outside world eg print document make sound operate robotic limbs
recent years witness increase competition science promote quality research many case intense competition among scientists also trigger unethical scientific behaviors increase total number publish paper author even resort software tool able produce grammatical meaningless scientific manuscripts automatically generate paper misunderstand real paper become paramount importance develop mean identify scientific frauds paper devise methodology distinguish real manuscripts generate scigen automatic paper generator upon model texts complex network cn possible discriminate real fake paper least eighty-nine accuracy systematic analysis feature relevance reveal accessibility betweenness useful particular case even though relevance depend upon dataset successful application methods describe show proof principle network feature use identify scientific gibberish paper addition cn base approach combine straightforward fashion traditional statistical language process methods improve performance identify artificially generate paper
recently lot effort represent word continuous vector space representations show capture semantic syntactic information word however distribute representations phrase remain challenge introduce novel model jointly learn word vector representations summation word representations learn use word co occurrence statistical information embed sequence word ie phrase different size common semantic space propose average word vector representations contrast previous methods report posteriori compositionality aspects simple summation simultaneously train word sum keep maximum information original vectors evaluate quality word representations several classical word evaluation task introduce novel task evaluate quality phrase representations distribute representations compete methods learn word representations word evaluations show give better performance phrase evaluation representations phrase could interest many task natural language process
conversational model important task natural language understand machine intelligence although previous approach exist often restrict specific domains eg book airline ticket require hand craft rule paper present simple approach task use recently propose sequence sequence framework model converse predict next sentence give previous sentence sentence conversation strength model train end end thus require much fewer hand craft rule find straightforward model generate simple conversations give large conversational train dataset preliminary result suggest despite optimize wrong objective function model able converse well able extract knowledge domain specific dataset large noisy general domain dataset movie subtitle domain specific helpdesk dataset model find solution technical problem via conversations noisy open domain movie transcript dataset model perform simple form common sense reason expect also find lack consistency common failure mode model
present structure perceptron train neural network transition base dependency parse learn neural network representation use gold corpus augment large number automatically parse sentence give fix network representation learn final layer use structure perceptron beam search decode penn treebank parser reach nine thousand, four hundred and twenty-six unlabeled nine thousand, two hundred and forty-one label attachment accuracy knowledge best accuracy stanford dependencies date also provide depth ablative analysis determine aspects model provide largest gain accuracy
introduce discriminative bleu deltableu novel metric intrinsic evaluation generate text task admit diverse range possible output reference string score quality human raters scale one one weight multi reference bleu task involve generation conversational responses deltableu correlate reasonably human judgments outperform sentence level ibm bleu term spearman rho kendall tau
effective way quick translation tremendous amount explosively increase science technique information material develop practicable machine translation system introduce translation practice essay treat problems arise translation isolate units basis practical materials experiment obtain development introduction english korean machine translation system word essay consider establishment information isolate units korean equivalents word order
pharmacovigilance field science devote collection analysis prevention adverse drug reactions adrs efficient strategies extraction information adrs free text resources essential support work experts employ crucial task detect classify unexpected pathologies possibly relate drug assumptions narrative adr descriptions may collect several way eg monitor social network call spontaneous report main method pharmacovigilance adopt order identify adrs encode free text adr descriptions accord meddra standard terminology central report analysis complex work manually implement pharmacovigilance experts manual encode expensive term time moreover problem accuracy encode may occur since number report grow day day paper propose magicoder efficient natural language process algorithm able automatically derive meddra terminologies free text adr descriptions magicoder part vigiwork web application online adr report analysis practical view point magicoder radically reduce revision time adr report pharmacologist simply revise validate automatic solution versus hard task choose solutions 70k term meddra improvement expert work efficiency meaningful impact quality data analysis moreover procedure general purpose develop magicoder italian pharmacovigilance language preliminarily analyse show robust language dictionary change
statistical methods widely employ study fundamental properties language recent years methods complex dynamical systems prove useful create several language model despite large amount study devote represent texts physical model limit number study show properties underlie physical systems employ improve performance natural language process task paper address problem devise complex network methods able improve performance current statistical methods use fuzzy classification strategy show topological properties extract texts complement traditional textual description several case performance obtain hybrid approach outperform result obtain traditional network methods use propose model generic framework devise could straightforwardly use study similar textual applications topology play pivotal role description interact agents
common evaluation practice vector space model vsms literature measure model ability predict human judgments lexical semantic relations word pair exist evaluation set however consist score collect english word pair ignore potential impact judgment language word pair present human score paper translate two prominent evaluation set wordsim353 association simlex999 similarity english italian german russian collect score dataset crowdworkers fluent language analysis reveal human judgments strongly impact judgment language moreover show predictions monolingual vsms necessarily best correlate human judgments make language use model train suggest model humans affect differently language use make semantic judgments finally show large number setups multilingual vsm combination result improve correlations human judgments suggest multilingualism may partially compensate judgment language effect human judgments
two important aspects semantic parse question answer breadth knowledge source depth logical compositionality exist work trade one aspect another paper simultaneously make progress front new task answer complex question semi structure table use question answer pair supervision central challenge arise two compound factor broader domain result open end set relations deeper compositionality result combinatorial explosion space logical form propose logical form drive parse algorithm guide strong type constraints show obtain significant improvements natural baselines evaluation create new dataset twenty-two thousand and thirty-three complex question wikipedia table make publicly available
present extensions continuous state dependency parse method make applicable morphologically rich languages start high performance transition base parser use long short term memory lstm recurrent neural network learn representations parser state replace lookup base word representations representations construct orthographic representations word also use lstms allow statistical share across word form similar surface experiment morphologically rich languages show parse model benefit incorporate character base encode word
paper make survey word sense disambiguation wsd near major languages around world research wsd conduct upto different extents paper go survey regard different approach adopt different research work state art performance domain recent work different indian languages finally survey bengali language make survey different competitions field bench mark result obtain competitions
base sense definition word available bengali wordnet attempt make classify bengali sentence automatically different group accordance underlie sense input sentence collect fifty different categories bengali text corpus develop tdil project govt india information different sense particular ambiguous lexical item collect bengali wordnet experimental basis use naive bay probabilistic model useful classifier sentence apply algorithm one thousand, seven hundred and forty-seven sentence contain particular bengali lexical item ambiguous nature able trigger different sense render sentence different mean experiment achieve around eighty-four accurate result sense classification total input sentence analyze residual sentence comply experiment affect result note many case wrong syntactic structure less semantic information main hurdle semantic classification sentence applicational relevance study attest automatic text classification machine learn information extraction word sense disambiguation
concept hierarchy backbone ontology concept hierarchy acquisition hot topic field ontology learn paper propose hyponymy extraction method domain ontology concept base cascade conditional random fieldccrfs hierarchy cluster take free text extract object adopt ccrfs identify domain concepts first low layer ccrfs use identify simple domain concept result send high layer nest concepts recognize next adopt hierarchy cluster identify hyponymy relation domain ontology concepts experimental result demonstrate propose method efficient
paper analyze impact confusions robustness phoneme recognitions system confusions detect pronunciation confusions matrices phoneme recognizer confusions show similarities phonemes pronunciation affect significantly recognition rat paper propose understand confusions order improve performance phoneme recognition system isolate problematic phonemes confusion analysis lead build new hierarchical recognizer use new phoneme distribution information confusion matrices new hierarchical phoneme recognition system show significant improvements recognition rat timit database
natural language generation nlg critical component speak dialogue significant impact usability perceive quality nlg systems common use employ rule heuristics tend generate rigid stylise responses without natural variation human language also easily scale systems cover multiple domains languages paper present statistical language generator base semantically control long short term memory lstm structure lstm generator learn unaligned data jointly optimise sentence plan surface realisation use simple cross entropy train criterion language variation easily achieve sample output candidates fewer heuristics objective evaluation two differ test domains show propose method improve performance compare previous methods human judge score lstm system higher informativeness naturalness overall prefer systems
natural language generation nlg component speak dialogue system sds usually need substantial amount handcraft well label dataset train limitations add significantly development cost make cross domain multi lingual dialogue systems intractable moreover human languages context aware natural response directly learn data rather depend predefined syntaxes rule paper present statistical language generator base joint recurrent convolutional neural network structure train dialogue act utterance pair without semantic alignments predefined grammar tree objective metrics suggest new model outperform previous methods experimental condition result evaluation human judge indicate produce high quality linguistically vary utterances prefer compare n gram rule base systems
paper propose variety long short term memory lstm base model sequence tag model include lstm network bidirectional lstm bi lstm network lstm conditional random field crf layer lstm crf bidirectional lstm crf layer bi lstm crf work first apply bidirectional lstm crf denote bi lstm crf model nlp benchmark sequence tag data set show bi lstm crf model efficiently use past future input feature thank bidirectional lstm component also use sentence level tag information thank crf layer bi lstm crf model produce state art close accuracy pos chunk ner data set addition robust less dependence word embed compare previous observations
syntax base metrics obtain similarity compare sub structure extract tree hypothesis reference sub structure define human express information tree limit length sub structure addition overlap part sub structure compute repeatedly avoid problems propose novel automatic evaluation metric base dependency parse model need define sub structure human first train dependency parse model reference dependency tree generate hypothesis dependency tree correspond probability dependency parse model quality hypothesis judge probability order obtain lexicon similarity also introduce unigram f score new metric experiment result show new metric get state art performance system level comparable meteor sentence level
paper propose methodology generate stopword list online social network osn corpora egyptian dialected aim paper investigate effect remove stopwords sentiment analysis sa task stopwords list generate modern standard arabic msa common language use osn generate stopword list egyptian dialect use osn corpora compare efficiency text classification use generate list along previously generate list msa combine egyptian dialect list msa list text classification perform use nai bay decision tree classifiers two feature selection approach unigram bigram experiment show remove ed stopwords give better performance use list msa stopwords
introduce model construct vector representations word compose character use bidirectional lstms relative traditional word representation model independent vectors word type model require single vector per character type fix set parameters compositional model despite compactness model importantly arbitrary nature form function relationship language compose word representations yield state art result language model part speech tag benefit traditional baselines particularly pronounce morphologically rich languages eg turkish
orthographic similarities across languages provide strong signal probabilistic decipherment especially closely relate language pair exist decipherment model however well suit exploit orthographic similarities propose log linear model latent variables incorporate orthographic similarity feature maximum likelihood train computationally expensive propose log linear model address challenge perform approximate inference via mcmc sample contrastive divergence result show propose log linear model contrastive divergence scale large vocabularies outperform exist generative decipherment model exploit orthographic feature
widely use automatic evaluation metrics adequately reflect fluency translations n gram base metrics like bleu limit maximum length match fragment n catch match fragment longer n reflect fluency indirectly meteor limit n gram use number match chunk consider length chunk paper propose entropy base method sufficiently reflect fluency translations distribution match word method easily combine widely use automatic evaluation metrics improve evaluation fluency experiment show correlations bleu meteor improve sentence level combine entropy base method wmt two thousand and ten wmt two thousand and twelve
previous study show health report social media dailystrength twitter potential monitor health condition eg adverse drug reactions infectious diseases particular communities however order machine understand make inferences health condition ability recognise laymen term refer particular medical concept ie text normalisation require achieve propose adapt exist phrase base machine translation mt technique vector representation word map social media phrase medical concept evaluate propose approach use collection phrase tweet relate adverse drug reactions experimental result show combination phrase base mt technique similarity word vector representations outperform baselines apply either fifty-five
distribute representations word real value vectors relatively low dimensional space aim extract syntactic semantic feature large text corpora recently introduce neural network name word2vec mikolov et al 2013a mikolov et al 2013b show encode semantic information direction word vectors brief report propose use length vectors together term frequency measure word significance corpus experimental evidence use domain specific corpus abstract present support proposal useful visualization technique text corpora emerge word map onto two dimensional plane automatically rank significance
trainable machine translation mt metrics train weight human judgments state art mt systems output make trainable metrics bias many ways one prefer longer translations bias metrics use tune evaluate different type translations n best list translations diverse quality systems tune metrics tend produce overly long translations prefer metric humans usually solve manually tweak metric weight equally value recall precision solution general one address recall bias also bias might present data two require knowledge type feature use useful case manual tune metric weight possible accomplish self train unlabeled n best list use metric initially train standard human judgments one way look domain adaptation domain state art mt translations diverse n best list translations
investigate evolution syntax need ascertain evolutionary role syntax nature syntax assume syntax compute since computationally turing complete meet evolutionary anomaly anomaly sytax syntactically competent syntax assume problem solve compute realize evolutionary advantage turing completeness full problem solve syntactic proficiency explain anomaly syntax postulate syntax problem solve co evolve humans towards turing completeness examine requirements full problem solve impose language find firstly semantics sufficient syntax necessary represent problems final conclusion full problem solve require functional semantics infinite tree structure syntax besides result introduction turing completeness problem solve explain evolution syntax help us fit evolution language within evolution cognition give us new clue understand elusive relation language think
attentional mechanism lately use improve neural machine translation nmt selectively focus part source sentence translation however little work explore useful architectures attention base nmt paper examine two simple effective class attentional mechanism global approach always attend source word local one look subset source word time demonstrate effectiveness approach wmt translation task english german directions local attention achieve significant gain fifty bleu point non attentional systems already incorporate know techniques dropout ensemble model use different attention architectures establish new state art result wmt fifteen english german translation task two hundred and fifty-nine bleu point improvement ten bleu point exist best system back nmt n gram reranker
word embeddings distribute representations word deep learn beneficial many task natural language process nlp however different embed set vary greatly quality characteristics capture semantics instead rely advance algorithm embed learn paper propose ensemble approach combine different public embed set aim learn meta embeddings experiment word similarity analogy task part speech tag show better performance meta embeddings compare individual embed set one advantage meta embeddings increase vocabulary coverage release meta embeddings publicly
thesis investigate sub structure word account probabilistic model language model play important role natural language process task translation speech recognition often rely simplistic assumption word opaque symbols assumption fit morphologically complex language well word rich internal structure sub word elements share across distinct word form approach encode basic notions morphology assumptions three different type language model intention leverage share sub word structure improve model performance help overcome data sparsity arise morphological process context n gram language model formulate new bayesian model rely decomposition compound word attain better smooth develop new distribute language model learn vector representations morphemes leverage link together morphologically relate word case show account word sub structure improve model intrinsic performance provide benefit apply task include machine translation shift focus beyond model word sequence consider model automatically learn sub word elements give language give unannotated list word formulate novel model learn discontiguous morphemes addition conventional contiguous morphemes previous model limit approach demonstrate semitic languages find model discontiguous sub word structure lead improvements task segment word contiguous morphemes
metonym word figurative mean similar metaphor metonyms closely relate metaphors apply feature use successfully metaphor recognition task detect metonyms acl semeval two thousand and seven task eight data gold standard metonym annotations system achieve eight thousand, six hundred and forty-five accuracy location metonyms code find github
neural network show improve performance across range natural language task however design train complicate frequently researchers resort repeat experimentation pick optimal settings paper address issue choose correct number units hide layer introduce method automatically adjust network size prune hide units ellinfty1 ell21 regularization apply method language model demonstrate ability correctly choose number hide units maintain perplexity also include model machine translation decoder show smaller neural model maintain significant improvements unpruned versions
many model natural language process define probabilistic distributions linguistic structure argue one quality model posterior distribution directly evaluate whether probabilities correspond empirical frequencies two nlp uncertainty project pipeline components also exploratory data analysis tell user trust trust nlp analysis present method analyze calibration apply compare miscalibration several commonly use model also contribute coreference sample algorithm create confidence intervals political event extraction task
understand entailment contradiction fundamental understand natural language inference entailment contradiction valuable test grind development semantic representations however machine learn research area dramatically limit lack large scale resources address introduce stanford natural language inference corpus new freely available collection label sentence pair write humans novel ground task base image caption 570k pair two order magnitude larger resources type increase scale allow lexicalize classifiers outperform sophisticate exist entailment model allow neural network base model perform competitively natural language inference benchmarks first time
visualize nlp annotation useful collection train data statistical nlp approach exist toolkits either provide limit visual aid introduce comprehensive operators realize sophisticate linguistic rule workers must well train use audience thus hardly scale large amount non expert crowdsourced workers paper present crowdanno visualization toolkit allow crowd source workers annotate two general categories nlp problems cluster parse workers finish task simplify operators interactive interface fix errors conveniently user study show toolkit friendly nlp non experts allow produce high quality label several sophisticate problems release source code toolkit spur future research
paper describe alignment base model interpret natural language instructions context approach instruction follow search plan score sequence action condition structure observations text environment explicitly model low level compositional structure individual action high level structure full plan able learn ground representations sentence mean pragmatic constraints interpretation demonstrate model flexibility apply diverse set benchmark task every task outperform strong task specific baselines achieve several new state art result
distribute word representations useful capture semantic information successfully apply variety nlp task especially english work innovatively develop two component enhance chinese character embed model bigram extensions distinguish english word embeddings model explore compositions chinese character often serve semantic indictors inherently evaluations word similarity text classification demonstrate effectiveness model
language social phenomenon variation inherent social nature recently surge interest within computational linguistics cl community social dimension language article present survey emerge field computational sociolinguistics reflect increase interest aim provide comprehensive overview cl research sociolinguistic theme feature topics relation language social identity language use social interaction multilingual communication moreover demonstrate potential synergy research communities involve show large scale data drive methods widely use cl complement exist sociolinguistic study sociolinguistics inform challenge methods assumptions employ cl study hope convey possible benefit closer collaboration two communities conclude discussion open challenge
paper event network present explore open information linguistic units event organize analyse process divide three step document event detection event network construction event network analysis first implement event detection track document retrospectively line organize document events secondly document event linguistic units extract combine event network thirdly various analytic methods propose event network analysis application methodologies present explore open information
neural machine translation nmt model typically operate fix vocabulary translation open vocabulary problem previous work address translation vocabulary word back dictionary paper introduce simpler effective approach make nmt model capable open vocabulary translation encode rare unknown word sequence subword units base intuition various word class translatable via smaller units word instance name via character copy transliteration compound via compositional translation cognates loanwords via phonological morphological transformations discuss suitability different word segmentation techniques include simple character n gram model segmentation base byte pair encode compression algorithm empirically show subword model improve back dictionary baseline wmt fifteen translation task english german english russian eleven thirteen bleu respectively
bidirectional long short term memory recurrent neural network blstm rnn show effective model predict sequential data eg speech utterances handwritten document study propose use blstm rnn unify tag solution apply various tag task include part speech tag chunk name entity recognition instead exploit specific feature carefully optimize task solution use one set task independent feature internal representations learn unlabeled text tasksrequiring task specific knowledge sophisticate feature engineer approach get nearly state art performance three tag task
describe sparse non negative matrix snm language model estimation use multinomial loss hold data able train hold data important practical situations train data usually mismatch hold test data also less constrain previous train algorithm use leave one train data allow use richer meta feature adjustment model eg diversity count use kneser ney smooth would difficult deal correctly leave one train experiment one billion word language model benchmark able slightly improve previous result use different loss function employ leave one train subset main train set surprisingly adjustment model meta feature discard lexical information perform well lexicalize meta feature find fairly small amount hold data order thirty seventy thousand word sufficient train adjustment model real life scenario train data mix data source imbalanced size different degrees relevance hold test data take account data source give skip n gram feature combine best performance hold test data improve skip n gram snm model train pool data eight smt setup much fifteen asr ime setup ability mix various data source base relevant mismatch hold set probably attractive feature new estimation method snm lm
article word embeddings use feature chinese sentiment classification present firstly chinese opinion corpus build million comment hotel review websites word embeddings represent comment use input different machine learn methods sentiment classification include svm logistic regression convolutional neural network cnn ensemble methods methods get better performance compare n gram model use naive bay nb maximum entropy finally combination machine learn methods propose present outstanding performance precision recall f1 score select useful methods construct combinational model test corpus final f1 score nine hundred and twenty
development plot story novels reflect content word use flow sentiments one aspect write style quantify analyze flow word study explore literary work signal word embed space try compare write style popular classic novels use dynamic time warp
present study focus automatic identification description freeze similes british french novels write nineteen th century begin twenty th century two main pattern freeze similes consider adjectival grind simile marker nominal vehicle eg happy lark eventuality simile marker nominal vehicle eg sleep like top potential similes components first extract use rule base algorithm freeze similes identify base reference list exist similes semantic distance tenor vehicle result obtain tend confirm fact freeze similes use haphazardly literary texts addition contrary often present freeze similes often go beyond grind eventuality vehicle also include tenor
order demonstrate important correctly account serial dependent structure temporal data document apparently spectacular relationship population size lexical diversity five seven investigate languages strong relationship population size lexical diversity primary language country show relationship result misspecified model consider temporal aspect data present similar nonsensical relationship global annual mean sea level lexical diversity give fact recent past several study publish present surprise link different economic cultural political socio demographical variables one hand cultural linguistic characteristics hand seem suffer exactly problem explain misspecification show profound consequences demonstrate simple transformation time series often solve problems type argue evaluation plausibility relationship important context hope paper help researchers reviewers understand important use special model analysis data natural temporal order
new approach call skyset synthetic knowledge yield social entities translation propose validate completeness reduce ambiguity write instructional documentation skyset utilize quintuple set standardize categories differ traditional approach typically use triple skyset system define categories require form standard template represent information portable across different domains provide standardize framework enable sentence write instructions translate set category type entities table database skyset entities contain conceptual units phrase represent information original source documentation skyset enable information concatenation multiple document different domains translate combine single common filterable searchable table entities
introduce new test well language model capture mean children book unlike standard language model benchmarks distinguish task predict syntactic function word predict lower frequency word carry greater semantic content compare range state art model different way encode previously read show model store explicit representations long term contexts outperform state art neural language model predict semantic content word although advantage observe syntactic function word interestingly find amount text encode single memory representation highly influential performance sweet spot big small single word full sentence allow meaningful information text effectively retain recall attention window base memories train effectively self supervision assess generality principle apply cnn qa benchmark involve identify name entities paraphrase summaries news article achieve state art performance
paper propose method imitate translation expert use korean translation information analyse performance korean good tag chinese use property chinese pos tag
similes play important role literary texts rhetorical devices figure speech also evocative power aptness description relative ease combine figure speech israel et al two thousand and four detect type simile constructions particular text therefore seem crucial analyse style author research study however dedicate study less prominent simile markers fictional prose relevance stylistic study present paper study frequency adjective verb simile markers corpus british french novels order determine ones really informative worth include stylistic analysis furthermore adjectives verb simile markers use differently languages
paper describe pilot ner system twitter comprise usfd system entry w nut two thousand and fifteen ner share task goal correctly label entities tweet dataset use inventory ten type employ structure learn draw gazetteers take link data unsupervised cluster feature attempt compensate stylistic topic drift key challenge social media text result competitive provide analysis components methodology examination target dataset context task
work propose novel method incorporate corpus level discourse information language model call larger context language model introduce late fusion approach recurrent language model base long short term memory units lstm help lstm unit keep intra sentence dependencies inter sentence dependencies separate evaluation three corpora imdb bbc penntree bank demon strate propose model improve perplexity significantly experi ments evaluate propose approach vary number context sentence observe propose late fusion superior usual way incorporate additional input lstm analyze train larger context language model discover content word include nouns adjec tives verbs benefit increase number context sentence analysis suggest larger context language model improve unconditional language model capture theme document better easily
berkeley framenet lexico semantic resource english base theory frame semantics exploit range natural language process applications inspire development framenets many languages present methodological approach extraction generation computational multilingual framenet base grammar lexicon approach leverage framenet annotate corpora automatically extract set cross lingual semantico syntactic valence pattern base data berkeley framenet swedish framenet propose approach implement grammatical framework gf categorial grammar formalism specialize multilingual grammars implementation grammar lexicon support design framenet provide frame semantic abstraction layer interlingual semantic api application program interface interlingual syntactic api already provide gf resource grammar library evaluation acquire grammar lexicon show feasibility approach additionally illustrate framenet base grammar lexicon exploit two distinct multilingual control natural language applications produce resources available open source license
introduce neural machine translation model view input output sentence sequence character rather word since word level information provide crucial source bias input model compose representations character sequence representations word determine whitespace boundaries translate use joint attention translation model target language translation model sequence word vectors word generate one character time conditional previous character generations word representation generation word perform character level model capable interpret generate unseen word form secondary benefit approach alleviate much challenge associate preprocessing tokenization source target languages show model achieve translation result par conventional word base model
present neural network architecture base bidirectional lstms compute representations word sentential contexts context sensitive word representations suitable eg distinguish different word sense context modulate variations mean learn parameters model use cross lingual supervision hypothesize good representation word context one sufficient select correct translation second language evaluate quality representations feature three downstream task prediction semantic supersenses assign nouns verbs dozen semantic class low resource machine translation lexical substitution task obtain state art result
social media data arabic language become abundant consensus valuable information lie social media data mine data make process easier gain momentum industries paper describe enterprise system develop extract sentiment large volumes social data arabic dialects first give overview big data system information extraction multilingual social data variety source focus arabic sentiment analysis capability build top system include normalize write arabic dialects build sentiment lexicons sentiment classification performance evaluation lastly demonstrate value enrich sentiment result user profile understand sentiments specific user group
paper present new method discovery latent domains diverse speech data use adaptation deep neural network dnns automatic speech recognition work focus transcription multi genre broadcast media often categorise broadly term high level genres sport news documentary etc however term acoustic model categories coarse instead expect mixture latent domains better represent complex diverse behaviours within tv show therefore lead better robust performance propose new method whereby latent domains discover latent dirichlet allocation unsupervised manner use adapt dnns use unique binary code ubic representation lda domains experiment conduct set bbc tv broadcast two thousand show train forty-seven show test show use lda ubic dnns reduce error thirteen relative compare baseline hybrid dnn model
many proper name pns vocabulary oov word speech recognition systems use process diachronic audio data help recovery pns miss system relevant oov pns retrieve many oovs exploit semantic context speak content paper propose two neural network model target retrieve oov pns relevant audio document document level continuous bag word cbow b document level continuous bag weight word cbow2 model take document word input learn objective maximise retrieval co occur oov pns cbow2 model propose new approach input embed layer augment context anchor layer layer learn assign importance input word ability capture task specific key word bag word neural network model experiment french broadcast news videos show two model outperform baseline methods base raw embeddings lda skip gram paragraph vectors combine cbow cbow2 model give faster convergence train
recently word representation increasingly focus excellent properties represent word semantics previous work mainly suffer problem polysemy phenomenon address problem previous model represent word multiple distribute vectors however reflect rich relations word represent word point embed space paper propose gaussian mixture skip gram gmsg model learn gaussian mixture embeddings word base skip gram framework word regard gaussian mixture distribution embed space gaussian component represent word sense since number sense vary word word propose dynamic gmsg gmsg model adaptively increase sense number word train experiment four benchmarks show effectiveness propose model
propose two methods learn vector representations word phrase combine sentence context structural feature extract dependency tree use several variations neural network classifier show combine methods lead improve performance use input feature supervise term match
question answer task show remarkable progress distribute vector representation paper investigate recently propose facebook babi task consist twenty different categories question require complex reason previous work babi end end model errors could come either imperfect understand semantics certain step reason clearer analysis propose two vector space model inspire tensor product representation tpr perform knowledge encode logical reason base common sense inference together achieve near perfect accuracy categories include positional reason path find prove difficult previous approach hypothesize difficulties categories due multi relations contrast uni relational characteristic categories exploration shed light design sophisticate dataset move one step toward integrate transparent interpretable formalism tpr exist learn paradigms
computational semantics logic base control natural languages cnl address systematically word sense disambiguation problem content word ie tend interpret functional word crucial construction discourse representation structure show micro ontologies multi word units allow integration rich polysemous multi domain background knowledge cnl thus provide interpretation content word propose approach demonstrate extend attempto control english ace polysemous procedural construct result natural cnl name pao cover narrative multi domain texts
neural machine translation nmt obtain state art performance several language pair use parallel data train target side monolingual data play important role boost fluency phrase base statistical machine translation investigate use monolingual data nmt contrast previous work combine nmt model separately train language model note encoder decoder nmt architectures already capacity learn information language model explore strategies train monolingual data without change neural network architecture pair monolingual train data automatic back translation treat additional parallel train data obtain substantial improvements wmt fifteen task english german twenty-eight thirty-seven bleu low resourced iwslt fourteen task turkish english twenty-one thirty-four bleu obtain new state art result also show fine tune domain monolingual parallel data give substantial improvements iwslt fifteen task english german
speak language translation slt become important increasingly globalize world social economic point view one major challenge automatic speech recognition asr machine translation mt drive intense research activities areas past research slt due technology limitations deal mostly speech record control condition today major challenge translation speak language find real life consider application scenarios range portable translators tourists lecture presentations translation broadcast news show live caption would like present pjiit experience slt gain eu bridge 7th framework project star consortium activities polish english language pair present research concentrate asr adaptation polish state art acoustic model dbn blstm train kaldi ldamlltsatmmi language model asr mt text normalization rnn base lms n gram model domain interpolation statistical translation techniques hierarchical model factor translation model automatic case punctuation comparable bilingual corpora preparation result well define domains phrase travelers parliament speeches medical documentation movie subtitle encourage less define domains presentation lecture still form challenge progress iwslt ted task mt present well current progress polish asr
text segmentation ts aim divide long text coherent segment reflect subtopic structure text beneficial many natural language process task information retrieval ir document summarisation current approach text segmentation similar use word frequency metrics measure similarity two regions text document segment base lexical cohesion word various nlp task move towards semantic web ontologies ontology base ir systems capture conceptualizations associate user need content text segmentation base lexical cohesion word hence sufficient anymore task paper propose ontoseg novel approach text segmentation base ontological similarity text block propose method use ontological similarity explore conceptual relations text segment hierarchical agglomerative cluster hac algorithm represent text tree like hierarchy conceptually structure rich structure create tree allow segmentation text linear fashion various level granularity propose method evaluate wellknown dataset result show use ontological similarity text segmentation promise also enhance propose method combine ontological similarity lexical similarity result show enhancement segmentation quality
distribute word representations demonstrate effective capture semantic syntactic regularities unsupervised representation learn large unlabeled corpora learn similar representations word present similar co occurrence statistics besides local occurrence statistics global topical information also important knowledge may help discriminate word another paper incorporate category information document learn word representations learn propose model document wise manner model outperform several state art model word analogy word similarity task moreover evaluate learn word vectors sentiment analysis text classification task show superiority learn word vectors also learn high quality category embeddings reflect topical mean
neural network model demonstrate capable achieve remarkable performance sentence document model convolutional neural network cnn recurrent neural network rnn two mainstream architectures model task adopt totally different ways understand natural languages work combine strengths architectures propose novel unify model call c lstm sentence representation text classification c lstm utilize cnn extract sequence higher level phrase representations feed long short term memory recurrent neural network lstm obtain sentence representation c lstm able capture local feature phrase well global temporal sentence semantics evaluate propose architecture sentiment classification question classification task experimental result show c lstm outperform cnn lstm achieve excellent performance task
paper go focus speed word sense disambiguation procedure filter relevant sense ambiguous word part speech tag first propose approach perform part speech tag operation disambiguation procedure use bigram approximation result exact part speech ambiguous word particular text instance derive next stage dictionary definitions gloss retrieve online dictionary associate particular part speech disambiguate exact sense ambiguous word train phase use brown corpus part speech tag wordnet online dictionary propose approach reduce execution time upto half approximately normal execution time text contain around two hundred sentence find several instance correct sense ambiguous word find use part speech tag disambiguation procedure
statistical methods widely employ many practical natural language process applications specifically complex network concepts methods dynamical systems theory successfully apply recognize stylistic pattern write texts despite large amount study devote represent texts physical model study assess relevance attribute derive analysis stylistic fluctuations fluctuations represent pivotal factor characterize myriad real systems study focus analysis properties stylistic fluctuations texts via topological analysis complex network intermittency measurements result show different author display distinct fluctuation pattern particular find possible identify authorship book use intermittency specific word take together result describe suggest pattern find stylistic fluctuations could use analyze relate complex systems furthermore discovery novel pattern relate textual stylistic fluctuations indicate pattern could useful improve state art many stylistic base natural language process task
give set term give domain structure taxonomy without manual intervention task seventeen semeval two thousand and fifteen present simple taxonomy structure techniques despite simplicity rank first two thousand and fifteen benchmark use large quantities text english wikipedia simple heuristics term overlap document sentence co occurrence produce hypernym list describe techniques pre send initial evaluation result
language model one important modules statistical machine translation currently word base language model dominants community however many translation model eg phrase base model generate target language sentence render compositing phrase rather word thus much reasonable model dependency phrase research work succeed solve problem paper tackle problem design novel phrase base language model attempt solve three key sub problems one define phrase language model two determine phrase boundary large scale monolingual data order enlarge train set three alleviate data sparsity problem due huge vocabulary size phrase carefully handle issue extensive experiment chinese english translation show phrase base language model significantly improve translation quality one hundred and forty-seven absolute bleu score
present language complexity analysis world warcraft wow community texts compare texts general corpus web english result several complexity type present include lexical diversity density readability syntactic complexity language wow texts find comparable general corpus complexity measure yet specialize measure find use educators will include game relate activities school curricula
generate novel textual description image interest problem connect computer vision natural language process paper present simple model able generate descriptive sentence give sample image model strong focus syntax descriptions train purely bilinear model learn metric image representation generate previously train convolutional neural network phrase use describe system able infer phrase give image sample base caption syntax statistics propose simple language model produce relevant descriptions give test image use phrase infer approach considerably simpler state art model achieve comparable result two popular datasets task flickr30k recently propose microsoft coco
paper discuss new metric apply verify quality translation sentence pair parallel corpora arabic english metric combine two techniques one base sentence length base compression code length experiment sample test parallel arabic english corpora indicate combination two techniques improve accuracy identification satisfactory unsatisfactory sentence pair compare sentence length compression code length alone new method propose research effective filter noise reduce mis translations result greatly improve quality
paper present generalize probabilistic model high order projective dependency parse algorithmic framework learn statistical model involve dependency tree partition function marginals high order dependency tree compute efficiently adapt algorithms extend inside outside algorithm higher order case show effectiveness algorithms perform experiment three languages english chinese czech use maximum conditional likelihood estimation model train l bfgs parameter estimation methods achieve competitive performance english outperform previously report dependency parsers chinese czech
word reorder one difficult aspects statistical machine translation smt important factor quality efficiency despite vast amount research publish date interest community problem decrease single method appear strongly dominant across language pair instead choice optimal approach new translation task still seem mostly drive empirical trials orientate reader vast complex research area present comprehensive survey word reorder view statistical model challenge natural language phenomenon survey describe detail word reorder model within different string base tree base smt frameworks stand alone task include systematic overviews literature advance reorder model question approach successful others different language pair argue besides measure amount reorder important understand kinds reorder occur give language pair end conduct qualitative analysis word reorder phenomena diverse sample language pair base large collection linguistic knowledge empirical result smt literature show support hypothesis linguistic facts useful anticipate reorder characteristics language pair select smt framework best suit
develop novel first second order feature dependency parse base google syntactic ngrams corpus collection subtree count parse sentence scan book also extend previous work surface n gram feature web1t google book corpus first order second order compare analyse performance newswire web treebanks surface syntactic n grams produce substantial complementary gain parse accuracy across domains best system combine two feature set achieve eight absolute uas improvements newswire fourteen web text
recently propose skip gram model powerful method learn high dimensional word representations capture rich semantic relationships word however skip gram well prior work learn word representations take account word ambiguity maintain single representation per word although number skip gram modifications propose overcome limitation learn multi prototype word representations either require know number word mean learn use greedy heuristic approach paper propose adaptive skip gram model nonparametric bayesian extension skip gram capable automatically learn require number representations word desire semantic resolution derive efficient online variational learn algorithm model empirically demonstrate efficiency word sense induction task
paper address problems arabic text classification stem use transducers rational kernels introduce new stem technique base use arabic pattern pattern base stemmer pattern model use transducers stem do without depend dictionary use transducers stem document transform finite state transducers document representation allow us use explore rational kernels framework arabic text classification stem experiment conduct three word collections classification experiment do saudi press agency dataset result show approach compare approach promise specially term accuracy recall f1
statistical machine translation model make great progress improve translation quality however exist model predict target translation source target side local context information practice distinguish good translations bad ones depend local feature also rely global sentence level information paper explore source side global sentence level feature target side local translation prediction propose novel bilingually constrain chunk base convolutional neural network learn sentence semantic representations sentence level feature representation design fee forward neural network better predict translations use local global information large scale experiment show method obtain substantial improvements translation quality strong baseline hierarchical phrase base translation model augment neural network joint model
paper motivate need publicly available generic software framework question answer qa systems present open source qa framework qanus researchers leverage build new qa systems easily rapidly framework implement much code otherwise repeat across different qa systems demonstrate utility practicality framework present fully function factoid qa system qa sys build top qanus
paper propose new method provide personalize tour recommendation museum visit combine optimization preference criteria visitors automatic extraction artwork importance museum information base natural language process use textual energy project include researchers computer social sciences result obtain numerical experiment show model clearly improve satisfaction visitor follow propose tour work foreshadow interest outcomes applications demand personalize visit museums near future
text categorization process group document categories base content process important make information retrieval easier become important due huge textual information available online main problem text categorization improve classification accuracy although arabic text categorization new promise field research field paper propose new method arabic text categorization use vector evaluation propose method use categorize arabic document corpus weight test document word calculate determine document keywords compare keywords corpus categorize determine test document best category
convince customer always consider challenge task every business come online business task become even difficult online retailers try everything possible gain trust customer one solutions provide area exist users leave comment service effectively develop trust customer however normally customer comment product native language use roman script hundreds comment make difficulty even native customers make buy decision research propose system extract comment post roman urdu translate find polarity give us rat product rat help native non native customers make buy decision efficiently comment post roman urdu
systematic study ancient texts include production transmission interpretation greatly aid digital methods start take 1970s research turn transmit new generations researchers tell story bible computer across decades point current challenge one find stable data representation change methods computation two share result inter intra disciplinary ways reproducibility cross fertilization report recent developments meet challenge scene text database hebrew bible construct eep talstra centre bible computer etcbc still grow detail sophistication show subtle mix computational ingredients enable scholars research transmission interpretation hebrew bible new ways one standard data format linguistic annotation framework laf two methods scientific compute make accessible interactive python associate ecosystem additionally show efforts culminate construction new publicly accessible search engine shebanq text hebrew bible underlie data query simple yet powerful query language mql query save share
automate image analysis progress increase interest richer linguistic annotation picture attribute object eg furry brown attract attention build recent zero shoot learn approach pay attention linguistic nature attribute noun modifiers specifically adjectives show possible tag image attribute denote adjectives even train data contain relevant annotation available approach rely two key observations first object see bundle attribute typically express adjectival modifiers dog something furry brown etc thus function train map visual representations object nominal label implicitly learn map attribute adjectives second object attribute come together picture thing dog brown thus achieve better attribute object label retrieval treat image visual phrase decompose linguistic representation attribute denote adjective object denote noun approach perform comparably method exploit manual attribute annotation outperform various competitive alternatives attribute object annotation automatically construct attribute centric representations significantly improve performance supervise object recognition
hashtags semantico syntactic construct use across various social network microblogging platforms enable users start topic specific discussion classify post desire category segment link entities present within hashtags could therefore help better understand extraction information share across social media however due lack space delimiters hashtags eg nsavssnowden segmentation hashtags constituent entities nsa edward snowden case trivial task current state art social media analytics systems like sentiment analysis entity link tend either ignore hashtags treat single word paper present context aware approach segment link entities hashtags knowledge base kb entry base context within tweet approach segment link entities hashtags coherence hashtag semantics tweet maximize best knowledge exist study address issue link entities hashtags extract semantic information evaluate method two different datasets demonstrate effectiveness technique improve overall entity link tweet via additional semantic information provide segment link entities hashtag
interest mathematical structure poetry date back least 19th century retire mathematics position j j sylvester write book prosody call textitthe laws verse today interest computer analysis poems paper discuss statistical approach apply task start definition middle english alliteration textitsir gawain green knight william langland textitpiers plowman use illustrate methodology theory first develop analyze data riemannian manifold turn applicable string allow one compute generalize mean variance textual data apply poems ratio two variances produce analogue f test resampling allow p value estimate consequently methodology provide way compare prosodic variability two texts
article focus description evaluation new unsupervised learn method cluster definitions spanish accord semantic textual energy use cluster measure study adaptation precision recall evaluate method
paper present methods deep multimodal learn fuse speech visual modalities audio visual automatic speech recognition av asr first study approach uni modal deep network train separately final hide layer fuse obtain joint feature space another deep network build audio network alone achieve phone error rate per forty-one clean condition ibm large vocabulary audio visual studio dataset fusion model achieve per three thousand, five hundred and eighty-three demonstrate tremendous value visual channel phone classification even audio high signal noise ratio second present new deep network architecture use bilinear softmax layer account class specific correlations modalities show combine posteriors bilinear network fuse model mention result significant phone error rate reduction yield final per three thousand, four hundred and three
software quality use comprehend quality user perspectives gain importance e learn applications mobile service base applications project management tool user decisions software acquisitions often ad hoc base preference due difficulty quantitatively measure software quality use however quality use measurement difficult although many software quality model knowledge work survey challenge relate software quality use measurement paper two main contributions one present major issue challenge measure software quality use context iso square series relate software quality model two present novel framework use predict software quality use three present preliminary result quality use topic prediction concisely issue relate complexity current standard model limitations incompleteness customize software quality model propose framework employ sentiment analysis techniques predict software quality use
paper propose novel framework generate lingual descriptions indoor scenes whereas substantial efforts make tackle problem previous approach focus primarily generate single sentence image sufficient describe complex scenes attempt go beyond generate coherent descriptions multiple sentence approach distinguish conventional ones several aspects one 3d visual parse system jointly infer object attribute relations two generative grammar learn automatically train text three text generation algorithm take account coherence among sentence experiment augment nyu v2 dataset show framework generate natural descriptions substantially higher rogue score compare produce baseline
modern statistical machine translation smt systems usually use linear combination feature model quality translation hypothesis linear combination assume feature linear relationship constrain feature interact rest feature linear manner might limit expressive power model lead fit model current data paper propose non linear model quality translation hypotheses base neural network allow complex interaction feature learn framework present train non linear model also discuss possible heuristics design network structure may improve non linear learn performance experimental result show basic feature hierarchical phrase base machine translation system method produce translations better linear model
recursive neural model use syntactic parse tree recursively generate representations bottom popular architecture rigorous evaluations show exactly task syntax base method appropriate paper benchmark bf recursive neural model sequential bf recurrent neural model simple recurrent lstm model enforce apples apples comparison much possible investigate four task one sentiment classification sentence level phrase level two match question answer phrase three discourse parse four semantic relation extraction eg component whole nouns goal understand better recursive model outperform simpler model find recursive model help mainly task like semantic relation extraction require associate headwords across long distance particularly long sequence introduce method allow recurrent model achieve similar performance break long sentence clause like units punctuation process separately combine result thus help understand limitations class model suggest directions improve recurrent model
article address question relevant conceptualization guillemotleftaltruismguillemotright russian perspective sociological research operationalization investigate spheres social application word guillemotleftaltruismguillemotright include russian equivalent guillemotleftvzaimopomoshhguillemotright mutual help data study come russian national corpus russian theoretical framework consist paul f lazarsfelds theory sociological research methodology natural semantic metalanguage nsm quantitative analysis show feature representation altruism russian sociologists need know preparation questionnaires interview guide analysis transcripts
problem spatiotemporal event visualization base report entail subtasks range name entity recognition relationship extraction map events present approach event extraction drive data mine visualization goals particularly thematic map trend analysis paper focus bridge information extraction visualization task investigate topic model approach develop static finite topic model examine potential benefit feasibility extend dynamic topic model large number topics continuous time describe experimental test bed event map use end end information retrieval system report preliminary result geoinformatics problem track methamphetamine lab seizure events across time space
linguistic structure exhibit rich array global phenomena however commonly use markov model unable adequately describe phenomena due strong locality assumptions propose novel hierarchical model structure prediction sequence tree exploit global context condition generation decision unbounded context prior decisions build success markov model without impose fix bind order better represent global phenomena facilitate learn large unbounded model use hierarchical pitman yor process prior provide recursive form smooth propose prediction algorithms base markov chain monte carlo sample empirical result demonstrate potential model compare baseline finite context markov model part speech tag syntactic parse
due computational storage efficiencies compact binary cod hash widely use large scale similarity search unfortunately many exist hash methods base observe keyword feature effective short texts due sparseness shortness recently researchers try utilize latent topics certain granularity preserve semantic similarity hash cod beyond keyword match however topics certain granularity adequate represent intrinsic semantic information paper present novel unify approach short text hash use multi granularity topics tag dub hmtt particular propose selection method choose optimal multi granularity topics depend type dataset design two distinct hash strategies incorporate multi granularity topics also propose simple effective method exploit tag enhance similarity relate texts carry extensive experiment one short text dataset well one normal text dataset result demonstrate approach effective significantly outperform baselines several evaluation metrics
yli multimedia event detection corpus public domain index videos annotations compute feature specialize research multimedia event detection med ie automatically identify happen video analyze audio visual content videos index yli med corpus subset larger yli feature corpus develop international computer science institute lawrence livermore national laboratory base yahoo flickr creative commons one hundred million yfcc100m dataset videos yli med categorize depict one ten target events target event annotate additional attribute like language speak whether video musical score annotations also include degree annotator agreement average annotator confidence score event categorization video version ten yli med include one thousand, eight hundred and twenty-three positive videos depict target events forty-eight thousand, one hundred and thirty-eight negative videos well one hundred and seventy-seven supplementary videos similar event videos positive examples goal produce yli med open data procedures possible report describe procedures use collect corpus give detail descriptive statistics corpus makeup video attribute affect annotators judgments discuss possible bias corpus introduce procedural choices compare similar exist dataset trecvid med havic corpus give overview future plan expand annotation effort
explore use semantic word embeddings text segmentation algorithms include c99 segmentation algorithm new algorithms inspire distribute word vector representation develop general framework discuss class segmentation objectives study effectiveness greedy versus exact optimization approach suggest new iterative refinement technique improve performance greedy strategies compare result know benchmarks use know metrics demonstrate state art performance untrained method content vector segmentation cvs choi test set finally apply segmentation procedure wild dataset consist text extract scholarly article arxivorg database
demonstrate dependency parser build use credit assignment compiler remove burden worry low level machine learn detail parser implementation result simple parser robustly apply many languages provide similar statistical computational performance best date transition base parse approach avoid various downsides include randomization extra feature requirements custom learn algorithms
software quality use comprise quality user perspective gain importance e government applications mobile base applications embed systems even business process development user decisions software acquisitions often ad hoc base preference due difficulty quantitatively measure software quality use quality use measurement difficult although many software quality model author knowledge work survey challenge relate software quality use measurement article two main contributions one identify explain major issue challenge measure software quality use context iso square series relate software quality model highlight open research areas two shed light research direction use predict software quality use short quality use measurement issue relate complexity current standard model limitations incompleteness customize software quality model sentiment analysis software review propose deal issue
grow number people change way consume news replace traditional physical newspapers magazines virtual online versions weblogs interactivity immediacy present online news change way news produce expose media corporations news websites create effective strategies catch people attention attract click paper investigate possible strategies use online news corporations design news headline analyze content sixty-nine thousand, nine hundred and seven headline produce four major global media corporations minimum eight consecutive months two thousand and fourteen order discover strategies could use attract click extract feature text news headline relate sentiment polarity headline discover sentiment headline strongly relate popularity news also dynamics post comment particular news
paper consider problem knowledge inference large scale imperfect repositories incomplete coverage mean embed entities relations first attempt propose iike imperfect incomplete knowledge embed probabilistic model measure probability belief ie langle hrtrangle large scale knowledge base nell freebase objective learn better low dimensional vector representation entity h relation r process minimize loss fit correspond confidence give machine learn nell crowdsouring freebase use bf h bf r bf assess plausibility belief conduct inference use subsets inexact knowledge base train model test performances link prediction triplet classification grind truth beliefs respectively result extensive experiment show iike achieve significant improvement compare baseline state art approach
introduce neural network recurrent attention model possibly large external memory architecture form memory network weston et al two thousand and fifteen unlike model work train end end hence require significantly less supervision train make generally applicable realistic settings also see extension rnnsearch case multiple computational step hop perform per output symbol flexibility model allow us apply task diverse synthetic question answer language model former approach competitive memory network less supervision latter penn treebank text8 datasets approach demonstrate comparable performance rnns lstms case show key concept multiple computational hop yield improve result
summarization one key feature human intelligence play important role understand representation rapid continual expansion texts picture videos cyberspace automatic summarization become desirable text summarization study half century still hard automatically generate satisfy summary traditional methods process texts empirically neglect fundamental characteristics principles language use understand paper summarize previous text summarization approach multi dimensional classification space introduce multi dimensional methodology research development unveil basic characteristics principles language use understand investigate fundamental mechanisms summarization study dimension form representations propose multi dimensional evaluation mechanisms investigation extend incorporation picture summary summarization videos graph picture reach general summarization framework
despite apparent clarity give legal provision application may result outcome exactly conform semantic level statute vagueness within legal text induce intentionally accommodate possible scenarios norms apply thus make role pragmatics important aspect also representation legal norm reason top notion pragmatics consider paper focus aspects associate judicial decision make paper aim would light aspects pragmatics legal linguistics mainly focus domain patent law knowledge representation perspective philosophical discussions present paper ground base legal theories grice marmor
paper address question language use affect community reaction comment online discussion forums relative importance message vs messenger new comment rank task propose base community annotate karma reddit discussions control topic time comment experimental work discussion thread six subreddits show importance different type language feature vary community interest
text mine process extract information interest text method include techniques various areas information retrieval ir natural language process nlp information extraction ie study text mine methods apply extract causal relations maritime accident investigation report collect marine accident investigation branch maib causal relations provide information various mechanisms behind accidents include human organizational factor relate accident objective study facilitate analysis maritime accident investigation report mean extract contributory cause feasibility careful investigation contributory cause report provide opportunity improve safety future two methods employ study extract causal relations one pattern classification method two connectives method earlier one use naive bay support vector machine svm classifiers latter simply search word connect effect sentence causal pattern extract use two methods compare manual human expert extraction pattern classification method show fair sensible performance f measureaverage sixty-five compare connectives method f measureaverage fifty-eight study evidence text mine methods could employ extract causal relations marine accident investigation report
increase amount online content motivate development multi document summarization methods work explore straightforward approach extend single document summarization methods multi document summarization propose methods base hierarchical combination single document summaries achieve state art result
goal answer elementary level science question use knowledge extract automatically science textbooks express subset first order logic give incomplete noisy nature automatically extract rule markov logic network mlns seem natural model use exact way leverage mlns mean obvious investigate three ways apply mlns task first simply use extract science rule directly mln clauses unlike typical mln applications domain long complex rule lead unmanageable number ground exploit structure present hard constraints improve tractability formulation remain ineffective second approach instead interpret science rule describe prototypical entities thus map rule directly ground mln assertions whose constants cluster use exist entity resolution methods drastically simplify network still suffer brittleness finally third approach call praline use mlns align lexical elements well define control inference perform task experiment demonstrate fifteen accuracy boost 10x reduction runtime suggest flexibility different inference semantics praline better fit natural language question answer task
stem influential part information retrieval search engines tremendous endeavour make stemmer efficient accurate stemmers three method stem dictionary base stemmer statistical base stemmers rule base stemmers paper aim build hybrid stemmer use dictionary base method rule base method stem ultimately help efficacy accurateness stemmer
paper describe parse model combine exact dynamic program crf parse rich nonlinear featurization neural net approach model structurally crf factor anchor rule productions instead linear potential function base sparse feature use nonlinear potentials compute via feedforward neural network potentials still local anchor rule structure inference cky unchanged sparse case compute gradients learn involve backpropagating error signal form standard crf sufficient statistics expect rule count use dense feature neural crf already exceed strong baseline crf model hall et al two thousand and fourteen combination sparse feature system achieve nine hundred and eleven f1 section twenty-three penn treebank generally outperform best prior single parser result range languages
speech recognition system performance degrade noisy environments acoustic model build use feature clean utterances feature noisy test utterance would acoustically mismatch train model give poor likelihoods poor recognition accuracy model adaptation feature normalisation two broad areas address problem former often give better performance latter involve estimation lesser number parameters make system feasible practical implementations research focus efficacies various subspace statistical stereo base feature normalisation techniques subspace projection base method investigate standalone adjunct technique involve reconstruction noisy speech feature precomputed set clean speech build block build block learn use non negative matrix factorisation nmf log mel filter bank coefficients form basis clean speech subspace work provide detail study method incorporate extraction process mel frequency cepstral coefficients experimental result show new feature robust noise achieve better result combine exist techniques work also propose modification train process splice algorithm noise robust speech recognition base feature correlations enable stereo base algorithm improve performance noise condition especially unseen case modify framework extend work non stereo datasets clean noisy train utterances stereo counterparts require mllr base computationally efficient run time noise adaptation method splice framework propose
human languages change time others address longstanding question computational perspective focus case sound change sound change arise pronunciation variability ubiquitous every speech community variability lead change hence adequate model must allow stability well change exist theories sound change tend emphasize factor level individual learners promote one outcome channel bias favor change inductive bias favor stability consider interaction bias lead stability change population set find population structure act source stability stability change possible type bias active suggest possible understand sound change occur time others population level result interplay force promote outcome individual speakers addition assume learners learn two teachers transition stability change mark phase transition consistent abrupt transition see many empirical case sound change predictions multiple teacher model thus match empirical case sound change better predictions single teacher model underscore importance model language change population set
paper propose algorithm script recognition base texture characteristics image texture achieve cod letter equivalent script type number code accord position text line code transform equivalent gray level pixel create one image image texture subject run length analysis analysis extract run length feature classify make distinction script consideration experiment custom orient database subject propose algorithm database consist text document write cyrillic latin glagolitic script furthermore divide train test part result experiment show three five run length feature use effective differentiation analyze south slavic script
study persistent homology data set syntactic parameters world languages show homology generators behave erratically whole data set non trivial persistent homology appear one restrict specific language families different families exhibit different persistent homology focus case indo european niger congo families compare persistent homology different cluster filter value investigate possible significance historical linguistic term presence persistent generators first homology particular show persistent first homology generator find indo european family due one might guess anglo norman bridge indo european phylogenetic network relate position ancient greek hellenic branch within network
know classification performance support vector machine svm conveniently affect different parameters kernel trick regularization parameter c thus article propose study order find suitable kernel svm may achieve good generalization performance well parameters use need analyze behavior svm classifier parameters take small large value study conduct multi class vowel recognition use timit corpus furthermore experiment use different feature representations mfcc plp finally comparative study do point impact choice parameters kernel trick feature representations performance svm classifier
article conduct study performance supervise learn algorithms vowel recognition study aim compare accuracy algorithm thus present empirical comparison five supervise learn classifiers two combine classifiers svm knn naive bay quadratic bay normal qdc nearst mean algorithms test vowel recognition use timit corpus mel frequency cepstral coefficients mfccs
cluster ensembles mingle numerous partition specify data single cluster solution cluster ensemble emerge potent approach ameliorate forcefulness stability unsupervised classification result one major problems cluster ensembles find best consensus function find final partition different cluster result require skillfulness robustness classification algorithm addition major problem consensus function sensitivity use data set quality limitation due existence noisy silence redundant data paper propose novel consensus function cluster ensembles base multilayer network technique maintenance database method maintenance database approach use order handle give noisy speech thus guarantee quality databases generate good result efficient data partition show effectiveness support strategy empirical evaluation use distort speech aurora speech databases
support vector machine svm method widely use numerous classification task main idea algorithm base principle margin maximization find hyperplane separate data two different classesin paper svm apply phoneme recognition task however many real world problems phoneme data set recognition problems may differ degree significance due noise inaccuracies abnormal characteristics problems lead inaccuracies prediction phase unfortunately standard formulation svm take account problems particular variation speech input paper present new formulation svm b svm attribute phoneme confidence degree compute base geometric position space degree use order strengthen class membership test phoneme hence introduce reformulation standard svm incorporate degree belief experimental performance timit database show effectiveness propose method b svm phoneme recognition problem
use digital technology grow fast pace lead emergence systems base cognitive infocommunications expansion sector impose use combine methods order ensure robustness cognitive systems
many exist speaker verification systems report vulnerable different spoof attack example speaker adapt speech synthesis voice conversion play back etc order detect spoof speech signal countermeasure propose score level fusion approach several different vector subsystems show acoustic level mel frequency cepstral coefficients mfcc feature phase level modify group delay cepstral coefficients mgdcc phonetic level phoneme posterior probability ppp tandem feature effective countermeasure furthermore feature level fusion feature vector model also enhance performance polynomial kernel support vector machine adopt supervise classifier order enhance generalizability countermeasure also adopt cosine similarity plda score one class classifications methods combine propose vector subsystems opensmile baseline cover acoustic prosodic information improve final performance propose fusion system achieve twenty-nine three hundred and twenty-six ever development test set database provide interspeech two thousand and fifteen automatic speaker verification spoof countermeasures challenge
introduce new rule base system belief track dialog systems despite simplicity rule consider propose belief tracker rank favourably compare previous submissions second third dialog state track challenge result simple tracker allow reconsider performances previous submissions use elaborate techniques
performance automatic speech recognition asr improve tremendously due application deep neural network dnns despite progress build new asr system remain challenge task require various resources multiple train stag significant expertise paper present eesen framework drastically simplify exist pipeline build state art asr systems acoustic model eesen involve learn single recurrent neural network rnn predict context independent target phonemes character remove need pre generate frame label adopt connectionist temporal classification ctc objective function infer alignments speech label sequence distinctive feature eesen generalize decode approach base weight finite state transducers wfsts enable efficient incorporation lexicons language model ctc decode experiment show compare standard hybrid dnn systems eesen achieve comparable word error rat wers time speed decode significantly
one task include manage document find substantial information inside topic model technique develop produce document representation form keywords keywords use index process document retrieval need users research discuss specifically probabilistic latent semantic analysis plsa cover plsa mechanism involve expectation maximization train algorithm conduct test obtain accuracy result
automatically recognize argument component essay writers inspections texts write assist essay score process objectively precisely essay grader able see well argument components construct reseachers try argument detection classification along implementation domains common approach feature extraction text generally feature structural lexical syntactic indicator contextual research add new feature exist feature adopt keywords list knott dale one thousand, nine hundred and ninety-three experiment result show argument classification achieve seven thousand, two hundred and forty-five accuracy moreover still get accuracy without keyword list conclude keyword list affect significantly feature feature still weak classify major claim claim need feature useful differentiate two kind argument components
lever data social media twitter facebook require information retrieval algorithms become able relate short text fragment traditional text similarity methods tf idf cosine similarity base word overlap mostly fail produce good result case since word overlap little non existent recently distribute word representations word embeddings show successfully allow word match semantic level order pair short text fragment concatenation separate word adequate distribute sentence representation need exist literature often obtain naively combine individual word representations therefore investigate several text representations combination word embeddings context semantic pair match paper investigate effectiveness several naive techniques well traditional tf idf similarity fragment different lengths main contribution first step towards hybrid method combine strength dense distribute representations oppose sparse term match strength tf idf base methods automatically reduce impact less informative term new approach outperform exist techniques toy experimental set lead conclusion combination word embeddings tf idf information might lead better model semantic content within short text fragment
music industry one hundred and thirty billion industry predict whether song catch pulse audience impact industry paper analyze language inside lyric songs use several computational linguistic algorithms predict whether song would make top bottom billboard rank base language feature train test svm classifier radial kernel function linguistic feature result indicate classify whether song belong top bottom billboard chart precision seventy-six
knowledge graph embed aim represent entities relations large scale knowledge graph elements continuous vector space exist methods eg transe transh learn embed representation define global margin base loss function data however optimal loss function determine experiment whose parameters examine among close set candidates moreover embeddings two knowledge graph different entities relations share set candidate loss function ignore locality graph lead limit performance embed relate applications paper propose locally adaptive translation method knowledge graph embed call transa find optimal loss function adaptively determine margin different knowledge graph experiment two benchmark data set demonstrate superiority propose method compare state art ones
many real systems model term network concepts write texts particular example information network recent years use network methods analyze language allow discovery several interest find include proposition novel model explain emergence fundamental universal pattern syntactical network one prevalent network model write texts display scale free small world properties representation fail capture textual feature organization topics subject context propose novel network representation whose main purpose capture semantical relationships word simple way link word co occur semantic context define threefold way show propose representations favour emergence communities semantically relate word feature may use identify relevant topics propose methodology detect topics apply segment select wikipedia article find general methods outperform traditional bag word representations suggest high level textual representation may useful study semantical feature texts
paper attempt improve statistical machine translation smt systems diverse set language pair directions czech english vietnamese english french english german english accomplish perform translation model train create adaptations train settings language pair obtain comparable corpora smt systems innovative tool data adaptation techniques employ ted parallel text corpora iwslt two thousand and fifteen evaluation campaign use train language model develop tune test system addition prepare wikipedia base comparable corpora use smt system data specify permissible iwslt two thousand and fifteen evaluation explore use domain adaptation techniques symmetrize word alignment model unsupervised transliteration model kenlm language model tool evaluate effect different preparations translation result conduct experiment use bleu nist ter metrics result indicate approach produce positive impact smt quality
multilingual nature world make translation crucial requirement today parallel dictionaries construct humans widely available resource limit provide enough coverage good quality translation purpose due vocabulary word neologisms motivate use statistical translation systems unfortunately dependent quantity quality train data systems limit availability especially languages narrow text domains research present improvements current comparable corpora mine methodologies implementation comparison algorithms use needleman wunch algorithm introduction tune script computation time improvement gpu acceleration experiment carry bilingual data extract wikipedia various domains wikipedia additional cross lingual comparison heuristics introduce modifications make positive impact quality quantity mine data translation quality
last years thousands scientific paper investigate sentiment analysis several startups measure opinions real data emerge number innovative products relate theme develop multiple methods measure sentiments include lexical base supervise machine learn methods despite vast interest theme wide popularity methods unclear one better identify polarity ie positive negative message accordingly strong need conduct thorough apple apple comparison sentiment analysis methods textitas use practice across multiple datasets originate different data source comparison key understand potential limitations advantage disadvantage popular methods article aim fill gap present benchmark comparison twenty four popular sentiment analysis methods call state practice methods evaluation base benchmark eighteen label datasets cover message post social network movie product review well opinions comment news article result highlight extent prediction performance methods vary considerably across datasets aim boost development research area open methods cod datasets use article deploy benchmark system provide open api access compare sentence level sentiment analysis methods
speech data crucially important speech recognition research quite speech databases purchase price reasonable research institute however young people start research activities gain initial interest direction cost data still annoy barrier support free data movement speech recognition research institute particularly support public fund publish data freely new researchers obtain sufficient data kick career paper follow trend release free chinese speech database thchs thirty use build full edge chinese speech recognition system report baseline system establish database include performance highly noisy condition
describe simple bag word baseline visual question answer baseline concatenate word feature question cnn feature image predict answer evaluate challenge vqa dataset two show comparable performance many recent approach use recurrent neural network explore strength weakness train model also provide interactive web demo open source code
introduce movieqa dataset aim evaluate automatic story comprehension video text dataset consist fourteen thousand, nine hundred and forty-four question four hundred and eight movies high semantic diversity question range simpler certain events occur question come set five possible answer correct one four deceive answer provide human annotators dataset unique contain multiple source information video clip plot subtitle script dvs analyze data various statistics methods extend exist qa techniques show question answer open end semantics hard make data set public along evaluation benchmark encourage inspire work challenge domain
goal project develop accurate tagger question post stack exchange problem instance general problem develop accurate classifiers large scale text datasets tackle multilabel classification problem item case question belong multiple class case tag predict tag keywords particular stack exchange post give question text title post process compare performance support vector classification svc different kernel function loss function etc find linear svc crammer singer technique produce best result
train neural network language model large vocabularies still computationally costly compare count base model kneser ney time neural language model gain popularity many applications speech recognition machine translation whose success depend scalability present systematic comparison strategies represent train large vocabularies include softmax hierarchical softmax target sample noise contrastive estimation self normalization extend self normalization proper estimator likelihood introduce efficient variant softmax evaluate method three popular benchmarks examine performance rare word speed accuracy trade complementarity kneser ney
dictionary base entity extraction involve find mention dictionary entities text text mention often noisy contain spurious miss word efficient algorithms detect approximate entity mention follow one two general techniques first approach build index entities perform index lookups document substrings second approach recognize number substrings generate document explode large number get around use filter prune many substrings match dictionary entity verify remain substrings entity mention dictionary entities mean text join choice index base approach filter verification base approach case case decision best approach depend characteristics input entity dictionary example frequency entity mention choose right approach set make substantial difference execution time make choice however non trivial parameters within approach make space possible approach large paper present cost base operator make choice among execution plan entity extraction since need deal large dictionaries even larger large datasets operator develop implementations mapreduce distribute algorithms
question answer forums rapidly grow size effective automate ability refer reuse answer already available previous post question paper develop methodology find semantically relate question task difficult since one key piece information often bury extraneous detail question body two available annotations similar question scarce fragment design recurrent convolutional model gate convolution effectively map question semantic representations model pre train within encoder decoder framework body title basis entire raw corpus fine tune discriminatively limit annotations evaluation demonstrate model yield substantial gain standard ir baseline various neural network architectures include cnns lstms grus
today world follow news distribute globally significant events report different source different languages work address problem track events large multilingual stream within recently develop system event registry examine two aspects problem compare article different languages link collections article different languages refer event take multilingual stream cluster article language compare different cross lingual document similarity measure base wikipedia allow us compute similarity two article regardless language build previous work show methods scale well compute meaningful similarity article languages little direct overlap train data use capability propose approach link cluster article across languages represent event provide extensive evaluation system whole well evaluation quality robustness similarity measure link algorithm
propose use structure natural language english specify service choreographies focus rather require coordination participant service realise business application scenario declarative approach propose use omg standard semantics business vocabulary rule sbvr model language service choreography approach propose describe global order invocations interfaces participant service therefore extend sbvr notion time capture coordination participant service term observable message exchange extension do use exist model construct sbvr hence respect standard specification idea users domain specialists rather implementation specialists verify request service composition directly read structure english use sbvr time sbvr model represent formal logic parse execute machine
name game simulate process name object single word population communicate agents reach global consensus asymptotically iteratively pair wise conversations propose extension single word model multi word name game mwng simulate case describe complex object sentence multiple word word define categories organize sentence combine different categories refer format combination several word pattern mwng pair wise conversation require hearer achieve consensus speaker respect every single word sentence well sentence pattern guarantee correct mean say otherwise fail reach consensus interaction validate model three typical topologies underlie communication network employ conventional man design pattern perform mwng
paper propose tbcnn pair model recognize entailment contradiction two sentence model tree base convolutional neural network tbcnn capture sentence level semantics heuristic match layer like concatenation element wise product difference combine information individual sentence experimental result show model outperform exist sentence encode base approach large margin
walter skeat publish critical edition william langland 14th century alliterative poem piers plowman one thousand, eight hundred and eighty-six preparation locate forty five manuscripts compare dialects publish excerpt paper three statistical analyse use excerpt mimic task write critical edition first combine multiple versions poetic line create best line compare mean string compute generalization arithmetic mean use edit distance second claim certain subset manuscripts vary little quantify compute string variance closely relate generalization mean third claim manuscripts fall three group cluster problem address use edit distance overall goal develop methodology would use literary critic
article withdraw arxiv administrators submitter legal authority grant license apply work
research explore effect various train settings polish english statistical machine translation system speak language various elements ted europarl opus parallel text corpora use basis train language model development tune test translation system bleu nist meteor ter metrics use evaluate effect data preparations translation result
paper present system visualize rdf knowledge graph graph obtain knowledge extraction system design geolsemantics extraction perform use natural language process trigger detection user visualize subgraphs select ontology feature like concepts individuals system also multilingual use annotate ontology english french arabic chinese
study rank frequency relations phonemes minimal units still relate linguistic mean show relations describe dirichlet distribution direct analogue ideal gas model statistical mechanics description allow us demonstrate rank frequency relations phonemes text depend author author dependency effect cause author vocabulary common word use different texts confirm several alternative mean suggest directly relate phonemes feature contrast rank frequency relations word author text independent govern zipf law
recent progress image recognition language model make automatic description image content reality however stylize non factual aspects write description miss current systems one style descriptions emotions commonplace everyday communication influence decision make interpersonal relationships design system describe image emotions present model automatically generate caption positive negative sentiments propose novel switch recurrent neural network word level regularization able produce emotional image caption use two thousand train sentence contain sentiments evaluate caption different automatic crowd source metrics model compare favourably common quality metrics image caption eight hundred and forty-six case generate positive caption judge least descriptive factual caption positive caption eighty-eight confirm crowd source workers appropriate sentiment
information retrieval ir model need deal two difficult issue vocabulary mismatch term dependencies vocabulary mismatch correspond difficulty retrieve relevant document contain exact query term semantically relate term term dependencies refer need consider relationship word query estimate relevance document multitude solutions propose solve two problems principled model solve parallel last years language model base neural network use cope complex natural language process task like emotion paraphrase detection although present good abilities cope term dependencies vocabulary mismatch problems thank distribute representation word base upon model could use readily ir estimation one language model per document query require computationally unfeasible prone fit base recent work propose learn generic language model modify set document specific parameters explore use new neural network model adapt ad hoc ir task within language model ir framework propose study use generic language model well document specific language model use smooth component latter adapt document hand potential use full document language model experiment model analyze result trec one eight datasets
present new platform regulus lite support rapid development web deployment several type phrasal speech translation systems use minimal formalism distinguish feature development work perform directly domain experts motivate need platforms type discuss three specific case medical speech translation speech sign language translation voice questionnaires briefly describe initial experience develop practical systems
prominences boundaries essential constituents prosodic structure speech provide mean chunk speech stream linguistically relevant units provide relative saliences demarcate within coherent utterance structure prominences boundaries widely use basic research prosody well text speech synthesis however representation scheme would provide estimate model unify fashion present unsupervised unify account estimate represent prosodic prominences boundaries use scale space analysis base continuous wavelet transform methods evaluate compare earlier work use boston university radio news corpus result show propose method comparable best publish supervise annotation methods
consider supervise train set learn task specific word embeddings assume start initial embeddings learn unlabelled data update learn task specific embeddings word supervise train data however new word test set must use either initial embeddings single unknown embed often lead errors address learn neural network map initial embeddings task specific embed space via multi loss objective function technique general demonstrate use improve dependency parse especially sentence vocabulary word well downstream improvements sentiment analysis
content base document classification one biggest challenge context free text mine current algorithms document classifications mostly rely cluster analysis base bag word approach however method still apply many modern scientific dilemmas establish strong presence field like economics social science merit serious attention researchers paper would like propose explore alternative ground securely dictionary classification correlatedness word phrase expect application exist knowledge underlie classification structure may lead improvement classifier performance
omnigraph novel representation support range nlp classification task integrate lexical items syntactic dependencies frame semantic parse graph feature engineer fold learn convolution graph kernel learn explore different extents graph high dimensional space feature include individual nod well complex subgraphs experiment text forecast problem predict stock price change news company mention omnigraph beat several benchmarks base bag word syntactic dependencies semantic tree highly expressive feature omnigraph discover provide insights semantics across distinct market sectors demonstrate method generality also report high performance result fine grain sentiment corpus
analyze historical literary document chinese gain insights research issue overview study utilize four different source text materials paper investigate history concepts transliterate word china database study modern china think literature contain historical document china one thousand, eight hundred and thirty one thousand, nine hundred and thirty also attempt disambiguate name share multiple government officer serve six hundred and eighteen one thousand, nine hundred and twelve record chinese local gazetteers showcase potentials challenge computer assist analysis chinese literatures explore interest yet non trivial question two four great classical novels china one monsters attempt consume buddhist monk xuanzang journey west jttw publish 16th century two powerful monster jttw three major role smile dream red chamber publish 18th century similar approach apply analysis study modern document newspaper article publish two hundred and twenty-eight incident occur one thousand, nine hundred and forty-seven taiwan
multilingual speak dialogue systems gain prominence recent past necessitate requirement front end language identification lid system exist lid systems rely model language discriminative information low level acoustic feature due variabilities speech speaker emotional variabilities etc large scale lid systems develop use low level acoustic feature suffer degradation performance approach attempt model higher level language discriminative phonotactic information develop lid system paper input speech signal tokenized phone sequence use language independent phone recognizer language discriminative phonotactic information obtain phone sequence model use statistical recurrent neural network base language model approach approach rely higher level phonotactical information robust variabilities speech propose approach computationally light weight highly scalable use complement exist lid systems
estimate internal state robotic system complex perform multiple heterogeneous sensor input knowledge source discretization input do capture saliences represent symbolic information often present structure recurrence sequence use reason complex scenarios compact representation would aid exactness technical cognitive reason capabilities today constrain computational complexity issue fallback representational heuristics human intervention problems need address ensure timely meaningful human robot interaction work towards understand variability learn informativeness train subsets give input dataset view reduce train size retain majority symbolic learn potential prove concept human write texts conjecture work reduce train data size sequential instructions preserve semantic relations gather information large remote source
present question answer system dbpedia fill gap user information need express natural language structure query interface express sparql underlie knowledge base kb give kb goal comprehend natural language query provide correspond accurate answer focus solve non aggregation question paper construct subgraph knowledge base detect entities propose graph traversal method solve semantic item map problem disambiguation problem joint way compare exist work simplify process query intention understand pay attention answer path rank evaluate method non aggregation question dataset complete dataset experimental result show method achieve best performance compare several state art systems
infer latent attribute people online important social compute task require integrate many heterogeneous source information available web propose learn individual representations people use neural net integrate rich linguistic network evidence gather social media algorithm able combine diverse cue text person write attribute eg gender employer education location social relations people show integrate textual network evidence representations offer improve performance four important task social media inference twitter predict one gender two occupation three location four friendships users approach scale large datasets learn representations use general feature potential benefit large number downstream task include link prediction community detection probabilistic reason social network
twitter gold market tool entities concern online reputation automatically monitor online reputation entities systems deal ambiguous entity name polarity detection topic detection propose three approach tackle first issue monitor twitter order find relevant tweet give entity evaluate within framework replab two thousand and thirteen filter task show competitive state art approach mainly investigate much merge strategies may impact performances filter task accord evaluation measure
software development life cycle sdlc start elicit requirements customers form software requirement specification srs srs document need software development mostly write natural languagenl convenient client srs document class name attribute function incorporate body class trace base pre knowledge analyst paper intend present review object orient oo analysis use natural language process nlp techniques analysis manual domain expert help generate require diagram automate system system generate require diagram input form srs
methods insights statistical physics find increase variety applications one seek understand emergent properties complex interact system one area concern dynamics language variety level description behaviour individual agents learn simple artificial languages change structure languages share large group speakers historical timescales colloquium survey hierarchy scale language linguistic behaviour describe along main progress understand make much come statistical physics community argue future developments may arise link different level hierarchy together coherent fashion particular allow effective use rich empirical data set
propose task free form open end visual question answer vqa give image natural language question image task provide accurate natural language answer mirror real world scenarios help visually impair question answer open end visual question selectively target different areas image include background detail underlie context result system succeed vqa typically need detail understand image complex reason system produce generic image caption moreover vqa amenable automatic evaluation since many open end answer contain word close set answer provide multiple choice format provide dataset contain 025m image 076m question 10m answer wwwvisualqaorg discuss information provide numerous baselines methods vqa provide compare human performance vqa demo available cloudcv http cloudcvorg vqa
present approach extract measure information text eg one thousand, three hundred and seventy degrees c melt point bmi greater two hundred and ninety-nine kg m2 extractions critically important across wide range domains especially involve search exploration scientific technical document first propose rule base entity extractor mine measure quantities ie numeric value pair measurement unit support vast comprehensive set common obscure measurement units method highly robust correctly recover valid measure quantities even significant errors introduce process convert document format like pdf plain text next describe approach extract properties measure eg property pixel pitch phrase pixel pitch high three hundred and fifty-two mum finally present mqsearch realization search engine full support measure information
open access movement scientific publish search engines like google scholar make scientific article broadly accessible last decade availability scientific paper full text become widespread thank grow number publications online platforms arxiv citeseer efforts provide article machine readable format rise open access publish result number standardize format scientific paper nlm jats tei docbook aim stimulate research intersection bibliometrics computational linguistics order study ways bibliometrics benefit large scale text analytics sense mine scientific paper thus explore interdisciplinarity bibliometrics natural language process
display document middle eastern languages require contextual analysis due different presentational form character alphabet word document form join correct positional glyphs represent correspond presentational form character set rule define join glyphs usual rule vary language language subject interpretation software developers paper propose machine learn approach contextual analysis base first order hide markov model design build model farsi language exhibit technology farsi model achieve ninety-four accuracy train base short list eighty-nine farsi vocabularies consist two thousand, seven hundred and eighty farsi character experiment easily extend many languages include arabic urdu sindhi furthermore advantage approach software use perform contextual analysis without cod complex rule specific language particular interest languages fewer speakers greater representation web since typically ignore software developers due lack financial incentives
entity link indispensable operation populate knowledge repositories information extraction study align textual entity mention correspond disambiguate entry knowledge repository paper propose new paradigm name distantly supervise entity link dsel sense disambiguate entities belong huge knowledge repository freebase automatically align correspond descriptive webpages wiki page way large scale weakly label data generate without manual annotation feed classifier link newly discover entities compare traditional paradigms base solo knowledge base dsel benefit via jointly leverage respective advantage freebase wikipedia specifically propose paradigm facilitate bridge disambiguate label freebase entities textual descriptions wikipedia web scale entities experiment conduct dataset one hundred and forty thousand items sixty thousand feature achieve baseline f1 measure five hundred and seventeen furthermore analyze feature performance improve f1 measure five hundred and forty-five
idea universal grammar ug hypothetical linguistic structure share human languages harken back least 13th century best know modern elaborations idea due chomsky follow devastate critique theoretical typological field linguistics elaborations idea ug general idea language universals stand untenable largely abandon proposal tackle hypothetical content ug use dependent polymorphic type theory framework different chomskyan ones introduce type logic precise universal parsimonious representation natural language morphosyntax compositional semantics logic handle grammatical ambiguity polymorphic type selectional restrictions diverse kinds anaphora dependent type feature partly universal set morphosyntactic type curry howard isomorphism
user review mobile apps often contain complaints suggestions valuable app developers improve user experience satisfaction however due large volume noisy nature review manually analyze useful opinions inherently challenge address problem propose mark keyword base framework semi automate review analysis mark allow analyst describe interest one mobile apps set keywords find list review relevant keywords analysis also draw trend time keywords detect sudden change might indicate occurrences serious issue help analysts describe interest effectively mark automatically extract keywords raw review rank associations negative review addition base vector base semantic representation keywords mark divide large set keywords cohesive subsets suggest keywords similar select ones
flickr30k dataset become standard benchmark sentence base image description paper present flickr30k entities augment 158k caption flickr30k 244k coreference chain link mention entities across different caption image associate 276k manually annotate bound box annotations essential continue progress automatic image description ground language understand enable us define new benchmark localization textual entity mention image present strong baseline task combine image text embed detectors common object color classifier bias towards select larger object baseline rival accuracy complex state art model show gain easily parlay improvements task image sentence retrieval thus underline limitations current methods need research
suppose give set videos along natural language descriptions form multiple sentence eg manual annotations movie script sport summaries etc sentence appear temporal order visual counterparts propose paper method align two modalities ie automatically provide time stamp every sentence give vectorial feature video text propose cast task temporal assignment problem implicit linear map two feature modalities formulate problem integer quadratic program solve continuous convex relaxation use efficient conditional gradient algorithm several round procedures propose construct final integer solution demonstrate significant improvements state art relate task align video symbolic label seven evaluate method challenge dataset videos associate textual descriptions thirty-six use bag word continuous representations text
present pair learn inference algorithms significantly reduce computation increase speed vector dot products classifiers heart many nlp components accomplish partition feature sequence templates order high confidence often reach use small fraction feature parameter estimation arrange maximize accuracy early confidence sequence approach simpler better suit nlp relate cascade methods present experiment leave right part speech tag name entity recognition transition base dependency parse typical benchmarking datasets preserve pos tag accuracy ninety-seven parse las eight hundred and eighty-five five fold reduction run time ner f1 eighty-eight 2x increase speed
semantic relation extraction one frontiers biomedical natural language process research gold standards key tool advance research challenge generate standards high cost expert time difficulty establish agreement annotators implement evaluate microtask crowdsourcing approach produce gold standard extract drug disease relations aggregate crowd judgment agree expert annotations pre exist corpus forty-three sixty sentence test level crowd agreement vary similar manner level agreement among original expert annotators work rein force power crowdsourcing process assemble gold standards relation extraction high light importance expose level agreement human annotators expert crowd gold standard corpora reproducible signal indicate ambiguities data annotation guidelines
ability map descriptions scenes 3d geometric representations many applications areas art education robotics however prior work text 3d scene generation task use manually specify object categories language identify introduce dataset 3d scenes annotate natural language descriptions learn data grind textual descriptions physical object method successfully ground variety lexical term concrete referents show quantitatively method improve 3d scene generation previous work use purely rule base methods evaluate fidelity plausibility 3d scenes generate ground approach human judgments ease evaluation task also introduce automate metric strongly correlate human judgments
demonstrate concern express garcia et al misplace due one misread find one two widespread failure examine present word support assert summary quantities base word usage frequencies three range misconceptions word usage frequency word rank expert construct word list particular show english component study compare well statistically two relate survey survey design influence apparent estimate measurement error explain positivity bias report work others demonstrate frequency dependence positivity explore nuances great detail one garcia et al perform reanalysis data instead carry analysis different statistically improper data set introduce nonlinearity perform linear regression
study analyze political interactions european parliament ep consider political agenda plenary sessions evolve time manner members european parliament meps react external internal stimuli make parliamentary speeches consider context speeches make content speeches detect latent theme legislative speeches time speech content analyze use new dynamic topic model method base two layer matrix factorization method apply new corpus english language legislative speeches ep plenary period one thousand, nine hundred and ninety-nine two thousand and fourteen find suggest political agenda ep evolve significantly time impact upon committee structure parliament react exogenous events eu treaty referenda emergence euro crisis significant impact discuss parliament
summarization base text extraction inherently limit generation style abstractive methods prove challenge build work propose fully data drive approach abstractive sentence summarization method utilize local attention base model generate word summary condition input sentence model structurally simple easily train end end scale large amount train data model show significant performance gain duc two thousand and four share task compare several strong baselines
several techniques use generate weather forecast texts paper case base reason cbr propose weather forecast text generation similar weather condition occur time similar forecast texts cbr meteo system generate weather forecast texts develop use generic framework jcolibri provide modules standard components cbr architecture advantage cbr approach systems build minimal time far less human effort initial consultation experts approach depend heavily goodness retrieval revision components cbr process evaluate cbrmeteo nist automate metric show correlate well human judgements domain system show comparable performance nlg systems perform task
discourse structure hide link surface feature document level properties sentiment polarity show discourse analyse produce rhetorical structure theory rst parsers improve document level sentiment analysis via composition local information discourse tree first show reweighting discourse units accord position dependency representation rhetorical structure yield substantial improvements lexicon base sentiment analysis next present recursive neural network rst structure offer significant improvements classification base methods
article offer empirical exploration use character level convolutional network convnets text classification construct several large scale datasets show character level convolutional network could achieve state art competitive result comparisons offer traditional model bag word n grams tfidf variants deep learn model word base convnets recurrent neural network
current state art entity link el systems gear towards corpora heterogeneous web therefore perform sub optimally domain specific corpora key open problem construct effective el systems specific domains knowledge local context principle increase rather decrease effectiveness paper propose hybrid use simple specialist linkers combination exist generalist system address problem main find follow first construct new reusable benchmark el corpus domain specific conversations second test performance range approach condition show specialist linkers obtain high precision isolation high recall combine generalist linkers hence effectively exploit local context get best worlds
hierarchy classical chinese poetry broadly acknowledge number study chinese literature however quantitative investigations evolutionary linkages classical chinese poetry limit primary goal study provide quantitative evidence evolutionary linkages emphasis character usage among different period genres classical chinese poetry specifically various statistical analyse perform find compare pattern character usage poems nine period genres include shi jing chu ci han shi jin shi tang shi song shi yuan shi ming shi qing shi result analysis indicate nine period genres unique pattern character usage chinese character preferably use poems particular period genre analysis general pattern character preference imply decrease trend use chinese character rarely occur modern chinese literature along timeline dynastic type classical chinese poetry phylogenetic analysis base distance matrix suggest evolutionary linkages different type classical chinese poetry congruent chronological order suggest character frequencies contain phylogenetic information useful infer evolutionary linkages among various type classical chinese poetry estimate phylogenetic tree identify four group shi jing chu ci han shi jin shi tang shi song shi yuan shi ming shi qing shi statistical analyse conduct study generalize analyze data set general chinese literature analyse provide quantitative insights evolutionary linkages general chinese literature
amlite framework develop map ascii transliterate amharic texts back original amharic letter texts aim framework make exist amharic linguistic data consistent interoperable among researchers achieve objective key map dictionary construct use possible ascii combinations actively use transliterate amharic letter map combinations correspond amharic letter do map use replace amharic linguistic text back form original amharic letter text framework indicate nine hundred and seventy-seven nine hundred and ninety-seven nine hundred and eighty-four percentage accuracy convert three sample random test data however possible improve accuracy framework add exception implementation algorithm preprocessing input text prior conversion paper outline rationales behind need develop framework process undertake development
systematic use publish result randomize clinical trials increasingly important evidence base medicine order collate analyze result potentially numerous trials evidence table use represent trials concern set interventions interest evidence table columns patient group interventions compare criterion comparison eg proportion survive five years treatment result currently labour intensive activity read publish paper extract information field evidence table nlp study investigate feature paper extract least relevant sentence identify however lack nlp system systematic extraction item information require evidence table address need combination maximum entropy classifier integer linear program use later handle constraints acceptable classification feature extract experimental result demonstrate substantial advantage use global constraints feature describe patient group interventions must occur feature describe result comparison
prepare domain specific datasets play important role supervise learn approach article new sentence dataset software quality use propose three experts choose annotate data use propose annotation scheme data reconcile match eliminate process reduce bias kappa k statistics reveal acceptable level agreement moderate substantial agreement experts build data use evaluate software quality use model sentiment analysis model moreover annotation scheme use extend current dataset
paper present contribution third chime speech separation recognition challenge include front end signal process back end speech recognition front end multi channel wiener filter mwf design achieve background noise reduction different traditional mwf optimize parameter tradeoff noise reduction target signal distortion build accord desire noise reduction level back end several techniques take advantage improve noisy automatic speech recognition asr performance include deep neural network dnn convolutional neural network cnn long short term memory lstm use medium vocabulary lattice rescoring big vocabulary language model finite state transducer rover scheme experimental result show propose system combine front end back end effective improve asr performance
many work relate twitter aim characterize users way role service spammers bots organizations etc nature user socio professional category age etc topics interest others however give user classification problem difficult select set appropriate feature many feature describe literature heterogeneous name overlap collisions numerous close variants article review wide range feature order present clear state art description unify name definitions relationships propose new neutral typology illustrate interest review apply selection feature offline influence detection problem task consist identify users influential real life base twitter account relate data show feature deem efficient predict online influence number retweets followers relevant problem however propose several content base approach label twitter users influencers also rank accord predict influence level proposals evaluate clef replab two thousand and fourteen dataset outmatch state art methods
paper lingban entry third chime speech separation recognition challenge present time frequency mask base speech enhancement front end propose suppress environmental noise utilize multi channel coherence spatial cue state art speech recognition techniques namely recurrent neural network base acoustic language model state space minimum bay risk base discriminative acoustic model vector base acoustic condition model carefully integrate speech recognition back end improve system performance fully exploit advantage different technologies final recognition result obtain lattice combination rescoring evaluations carry official dataset prove effectiveness propose systems compare best baseline result propose system obtain consistent improvements fifty-seven relative word error rate reduction real data test set
convolutional neural network cnns standard component many current state art large vocabulary continuous speech recognition lvcsr systems however cnns lvcsr keep pace recent advance domains deeper neural network provide superior performance paper propose number architectural advance cnns lvcsr first introduce deep convolutional network architecture fourteen weight layer multiple convolutional layer pool layer small 3x3 kernels inspire vgg imagenet two thousand and fourteen architecture introduce multilingual cnns multiple untie layer finally introduce multi scale input feature aim exploit context negligible computational cost evaluate improvements first babel task low resource speech recognition obtain absolute five hundred and seventy-seven wer improvement baseline plp dnn train cnn combine data six different languages evaluate deep cnns hub5 zero benchmark use two hundred and sixty-two hours swb one train data achieve word error rate one hundred and eighteen cross entropy train fourteen wer improvement one hundred and six relative best publish cnn result far
research extend bilingual evaluation understudy bleu evaluation technique statistical machine translation make adjustable robust intend adapt resemble human evaluation perform experiment evaluate performance technique primary exist evaluation methods describe show improvements make exist methods well correlation human translators translate text often use synonyms different word order style similar variations propose smt evaluation technique enhance bleu metric consider variations
research investigate statistical machine translation approach translate speech real time automatically systems use pipeline speech recognition synthesis software order produce real time voice communication system foreigners obtain three main data set speak proceed represent three different type human speech ted europarl opus parallel text corpora use basis train language model developmental tune test translation system also conduct experiment involve part speech tag compound split linear language model interpolation truecasing morphosyntactic analysis evaluate effect variety data preparations translation result use bleu nist meteor ter metrics try give answer metric suitable pl en language pair
text alignment crucial accuracy machine translation mt systems nlp tool text process task require bilingual data research propose language independent sentence alignment approach base polish position sensitive language english experiment alignment approach develop ted talk corpus use text domain language pair propose approach implement various heuristics sentence recognition value synonyms semantic text structure analysis part additional information minimization data loss ensure solution compare sentence alignment implementations also improvement mt system score text process describe tool show
research explore effect various train settings polish english statistical machine translation system speak language various elements ted parallel text corpora iwslt two thousand and thirteen evaluation campaign use basis train language model development tune test translation system bleu nist meteor ter metrics use evaluate effect data preparations translation result experiment include systems use stem morphological information polish word also conduct deep analysis provide polish data preparatory work automatic data correction clean phase
arabic morphology encapsulate many valuable feature word root arabic root utilize many task process extract word root refer stem stem essential part natural language process task especially derivative languages arabic however stem face problem ambiguity two root could extract word hand distributional semantics powerful co occurrence model capture mean word base context paper distributional semantics model utilize smooth pointwise mutual information spmi construct investigate effectiveness stem analysis task show accuracy eight hundred and fifteen least ninety-four improvement stemmers
paper describe microsoft coco caption dataset evaluation server complete dataset contain one half million caption describe three hundred and thirty thousand image train validation image five independent human generate caption provide ensure consistency evaluation automatic caption generation algorithms evaluation server use evaluation server receive candidate caption score use several popular metrics include bleu meteor rouge cider instructions use evaluation server provide
traditional way store facts triplets headentity relation tailentity abbreviate h r make knowledge intuitively display easily acquire mankind hardly compute even reason ai machine inspire success apply distribute representations ai relate field recent study expect represent entity relation unique low dimensional embed different symbolic atomic framework display knowledge triplets way knowledge compute reason essentially facilitate mean simple vector calculation ie bf h bf r approx bf thus contribute effective model learn better embeddings satisfy formula pull positive tail entities bf get together close bf h bf r nearest neighbor simultaneously push negative bf away positives bf via keep large margin also design correspond learn algorithm efficiently find optimal solution base stochastic gradient descent iterative fashion quantitative experiment illustrate approach achieve state art performance compare several latest methods benchmark datasets two classical applications ie link prediction triplet classification moreover analyze parameter complexities among evaluate model analytical result indicate model need fewer computational resources outperform methods
process language learn involve mastery countless task make constituent sound language learn learn grammatical pattern acquire requisite vocabulary reception production plethora computational tool exist facilitate first second task number challenge arise respect enable third paper describe tool design support language learners challenge understand use close class lexical items process learn arabic office mktb relatively simple possible mean simple repetition word however much difficult learn correctly use arabic equivalent word current paper describe mechanism delivery diagnostic information regard specific lexical examples aim clearly demonstrate particular translation give close class item may appropriate certain situations others thereby help learners understand use term correctly
application usage opinion mine especially business intelligence product recommendation target market etc fascinate many research attentions around globe various research efforts attempt mine opinions customer review different level granularity include word sentence document level however development fully automatic opinion mine sentiment analysis system still elusive though development opinion mine sentiment analysis systems get momentum attempt perform document level sentiment analysis classify review document positive negative neutral document level opinion mine approach fail provide insight users sentiment individual feature product service therefore seem great help customers manufacturers review could process finer grain level present summarize form visual mean highlight individual feature product users sentiment express paper design unify opinion mine sentiment analysis framework present intersection machine learn natural language process approach also design novel feature level review summarization scheme propose visualize mine feature opinions polarity value comprehendible way
ord graph simple graphical method display frequency distributions data theoretical distributions two dimensional plane coordinate proportion first three moments either empirical theoretical ones modification ord graph base proportion indices qualitative variation present modification make graph applicable also data categorical character addition indices normalize value zero one enable compare data file divide different number categories original new graph use display grapheme frequencies eleven slavic languages original ord graph require assignment number categories graphemes order decreasingly accord frequencies data take parallel corpora ie work grapheme frequencies russian novel translations ten slavic languages cluster analysis apply graph coordinate original graph yield result linguistically interpretable modification reveal meaningful relations among languages
report describe minimalistic set methods engineer anchor clinical events onto temporal space specifically describe methods extract clinical events eg problems treatments test temporal expressions ie time date duration frequency temporal link eg overlap events temporal entities methods develop validate use high quality datasets
present self train approach unsupervised dependency parse reuse exist supervise unsupervised parse algorithms approach call iterate reranking ir start dependency tree generate unsupervised parser iteratively improve tree use richer probability model use supervise parse turn train tree system achieve eighteen accuracy higher state part parser spitkovsky et al two thousand and thirteen wsj corpus
modality one important components grammar linguistics let us speaker express attitude towards give assessment potentiality state affairs imply different sense thus different perceptions per context paper present account show gap functionality current state art natural language process nlp systems contextual nature linguistic modality study paper work logical approach employ natural language process systems deal modality review see human cognition intelligence multi layer approach implement intelligent systems learn lastly current flow research go within field talk provide futurology
access web scale corpora gradually bring robust automatic knowledge base creation extension within reach exploit large unannotated extremely difficult annotate corpora unsupervised machine learn methods require probabilistic model text recently find success tool scalability remain obstacle application standard approach rely sample scheme know difficult scale report therefore present empirical assessment sublinear time sparse stochastic variational inference ssvi scheme apply rellda demonstrate online inference lead relatively strong qualitative result also identify pathologies model need overcome ssvi use large scale relation extraction
present novel hierarchical distance dependent bayesian model event coreference resolution exist generative model event coreference resolution completely unsupervised model allow incorporation pairwise distance event mention information widely use supervise coreference model guide generative cluster process better event cluster within across document model distance event mention use feature rich learnable distance function encode bayesian priors nonparametric cluster experiment ecb corpus show model outperform state art methods within cross document event coreference resolution
important data lock ancient literature would uneconomic produce data today extract without help text mine technologies vespa text mine project whose aim extract data pest crop interactions model predict attack crop reduce use pesticides attempt propose agricultural information access another originality work parse document dependency document architecture
relation extraction accurate precision still challenge process full text databases propose approach base cooccurrence analysis document use document organization improve accuracy relation extraction approach implement r package call emphxent another facet extraction rely use extract relation query system expert end users two datasets use one get interest specialists epidemiology plant health dataset usage dedicate plant disease exploration agricultural information news open data platform exploit export emphxent publicly available
present new r package take numerical matrix format data input compute cluster use support vector cluster method svc implement original 2d grid label approach speed cluster extraction sense svc see efficient cluster extraction cluster separable two map secondly show svc approach use jaccard radial base kernel help classify well enough set term ontological class help define regular expression rule information extraction document case study concern set term document developmental molecular biology
paper describe approach automatic construction dictionaries name entity recognition ner use large amount unlabeled data seed examples use canonical correlation analysis cca obtain lower dimensional embeddings representations candidate phrase classify phrase use small number label examples method achieve one hundred and sixty-five one hundred and thirteen f one score improvement co train disease virus ner respectively also show add candidate phrase embeddings feature sequence tagger give better performance compare use word embeddings
rise interest vector space word embeddings use nlp especially give recent methods fast estimation large scale nearly work however assume single vector per word type ignore polysemy thus jeopardize usefulness downstream task present extension skip gram model efficiently learn multiple embeddings per word type differ recent relate work jointly perform word sense discrimination embed learn non parametrically estimate number sense per word type efficiency scalability present new state art result word similarity context task demonstrate scalability train one machine corpus nearly one billion tokens less six hours
previous work knowledge base kb completion focus problem relation extraction work focus task infer miss entity type instance kb fundamental task kb competition yet receive little attention due novelty task construct large scale dataset design automatic evaluation methodology knowledge base completion method use information within exist kb external information wikipedia show individual methods train global objective consider unobserved cells entity type side give consistently higher quality predictions compare baseline methods also perform manual evaluation small subset data verify effectiveness knowledge base completion methods correctness propose automatic evaluation method
knowledge base kb completion add new facts kb make inferences exist facts example infer high likelihood nationalityxy borninxy previous methods infer simple one hop relational synonyms like use evidence multi hop relational path treat atomic feature like borninxz containedinzy paper present approach reason conjunctions multi hop relations non atomically compose implications path use recursive neural network rnn take input vector embeddings binary relation path allow us generalize paths unseen train time also single high capacity rnn predict new relation type see compositional model train zero shoot learn assemble new dataset 52m relational triple show method improve traditional classifier eleven method leverage pre train embeddings seven
present parser abstract mean representation amr treat english amr conversion within framework string tree syntax base machine translation sbmt make work transform amr structure form suitable mechanics sbmt useful model introduce amr specific language model add data feature draw semantic resources result amr parser improve upon state art result seven smatch point
paper present web application sere design explore semantically relate concepts wikipedia dbpedia rich data source extract relate entities give topic like link broader narrower term categorisation information etc use wikipedia full text body compute semantic relatedness extract term result list entities relevant topic give query user interface sere visualize relate concepts order semantic relatedness snippets wikipedia article explain connection two entities user study examine sere use find important entities relationships give topic answer question classification system use filter
propose new mds paradigm call reader aware multi document summarization ra mds specifically set reader comment associate news report also collect generate summaries report event salient accord report also reader comment tackle ra mds problem propose sparse cod base method able calculate salience text units jointly consider news report reader comment another reader aware characteristic framework improve linguistic quality via entity rewrite rewrite consideration jointly assess together summarization requirements unify optimization model support generation compressive summaries via optimization explore finer syntactic unit namely noun verb phrase work also generate data set conduct ra mds extensive experiment data set classical data set demonstrate effectiveness propose approach
present commentwatcher open source tool aim analyze discussions web forums construct web platform commentwatcher feature automatic mass fetch user post forum multiple sit extract topics visualize topics expression cloud explore temporal evolution underlie social network users simultaneously construct use citation relations users visualize graph structure platform address issue diversity dynamics structure webpages host forums implement parser architecture independent html structure webpages allow easy fly add new websites two type users target end users seek study discuss topics temporal evolution researchers need establish forum benchmark dataset compare performances analysis tool
universal human conceptual structure way concepts organize human brain may reflect distinct feature cultural historical environmental background addition properties universal human cognition semantics mean express language provide direct access underlie conceptual structure mean notoriously difficult measure let alone parameterize provide empirical measure semantic proximity concepts use cross linguistic dictionaries across languages carefully select phylogenetically geographically stratify sample genera translations word reveal case particular language use single polysemous word express concepts represent distinct word another use frequency polysemies link two concepts measure semantic proximity represent pattern linkages weight network network highly uneven fragment certain concepts far prone polysemy others emerge naturally interpretable cluster loosely connect statistical analysis show structural properties consistent across different language group largely independent geography environment literacy therefore possible conclude conceptual structure connect basic vocabulary study primarily due universal feature human cognition language use
paper propose concept level emotion model cecm instead mere word level model discover cause microblogging users diversify emotions specific hot event modify topic supervise biterm topic model utilize cecm detect emotion topics event relate tweet context sensitive topical pagerank utilize detect meaningful multiword expressions emotion cause experimental result dataset sina weibo one largest microblogging websites china show cecm better detect emotion cause baseline methods
describe matrix multiplication recognition algorithm subset binary linear context free rewrite systems lcfrs run time ofnomega mm ofmomega run time time matrix multiplication contact rank lcfrs maximal number combination non combination point appear grammar rule also show algorithm use subroutine get recognition algorithm general binary lcfrs run time ofnomega one currently best know omega smaller two hundred and thirty-eight result provide another proof best know result parse mildly context sensitive formalisms combinatory categorial grammars head grammars linear index grammars tree adjoin grammars parse time ofn476 also show inversion transduction grammars parse time ofn576 addition binary lcfrs subsume many formalisms type grammars also improve asymptotic complexity parse
paper introduce new dataset consist three hundred and sixty thousand and one focus natural language descriptions ten thousand, seven hundred and thirty-eight image dataset visual madlibs dataset collect use automatically produce fill blank templates design gather target descriptions people object appearances activities interactions well inferences general scene broader context provide several analyse visual madlibs dataset demonstrate applicability two new description generation task focus description generation multiple choice question answer image experiment use joint embed deep learn methods show promise result task
work build knowledge base focus collect entities facts large collection document possible argue describe new paradigm focus high recall extraction small collection document supervision human expert call interactive knowledge base population ikbp
significant performance reduction often observe speech recognition rate speech ros low high present approach address ros variation focus change speech signal dynamic properties cause ros accordingly modify dynamic model eg transition probabilities hide markov model hmm however abnormal ros change dynamic also static property speech signal thus compensate purely modify dynamic model paper propose ros learn approach base deep neural network dnn involve ros feature input dnn model spectrum distortion cause ros learn compensate experimental result show approach deliver better performance slow fast utterances demonstrate conjecture ros impact dynamic static property speech addition propose approach combine conventional hmm transition adaptation method offer additional performance gain
popularity mobile devices personalize speech recognizer become realizable today highly attractive mobile device primarily use single user possible personalize recognizer well match characteristics individual user although acoustic model personalization investigate decades much less work report personalize language model probably difficulties collect enough personalize corpora previous work use corpora collect social network solve problem construct personalize model user troublesome paper propose universal recurrent neural network language model user characteristic feature users share model except different user characteristic feature user characteristic feature obtain crowdsouring social network include huge quantity texts post users know friend relationships may share subject topics word pattern preliminary experiment facebook corpus show propose approach drastically reduce model perplexity offer good improvement recognition accuracy n best rescoring test approach also mitigate data sparseness problem personalize language model
propose abstraction base multi document summarization framework construct new sentence explore fine grain syntactic units sentence namely noun verb phrase different exist abstraction base approach method first construct pool concepts facts represent phrase input document new sentence generate select merge informative phrase maximize salience phrase meanwhile satisfy sentence construction constraints employ integer linear optimization conduct phrase selection merge simultaneously order achieve global optimal solution summary experimental result benchmark data set tac two thousand and eleven show framework outperform state art model automate pyramid evaluation metric achieve reasonably well result manual linguistic quality evaluation
generate descriptions videos many applications include assist blind people human robot interaction recent advance image caption well release large scale movie description datasets mpii movie description allow study task depth many propose methods image caption rely pre train object classifier cnns long short term memory recurrent network lstms generate descriptions image description focus object argue important distinguish verbs object place challenge set movie description work show learn robust visual classifiers weak annotations sentence descriptions base visual classifiers learn generate description use lstm explore different design choices build train lstm achieve best performance date challenge mpii md dataset compare analyze approach prior work along various dimension better understand key challenge movie description task
train large scale question answer systems complicate train source usually cover small portion range possible question paper study impact multitask transfer learn simple question answer set reason require answer quite easy long one retrieve correct evidence give question difficult large scale condition end introduce new dataset 100k question use conjunction exist benchmarks conduct study within framework memory network weston et al two thousand and fifteen perspective allow us eventually scale complex reason show memory network successfully train achieve excellent performance
dysarthria malfunction motor speech cause faintness human nervous system characterize slur speech along physical impairment restrict communication create lack confidence affect lifestyle paper attempt increase efficiency automatic speech recognition asr system unimpaired speech signal describe state art research improve asr speakers dysarthria mean incorporate knowledge speech production hybridize approach feature extraction acoustic model technique along evolutionary algorithm propose increase efficiency overall system number feature vectors vary test system performance observe system performance boost genetic algorithm system sixteen acoustic feature optimize genetic algorithm obtain highest recognition rate nine thousand, eight hundred and twenty-eight train time fifty-three thousand and seventeen
paper address problem determine best answer community base question answer cqa websites focus content particular present system acqua http acquakmiopenacuk instal onto majority browsers plugin service offer seamless accurate prediction answer accept previous research topic rely exploitation community feedback answer involve rat either users eg reputation answer eg score manually assign answer propose new technique leverage content textual feature answer novel way approach deliver better result relate linguistics base solutions manage match rat base approach specifically gain performance achieve render value feature discretised form also show technique manage deliver equally good result real time settings oppose rely information always readily available user rat answer score run evaluation twenty-one stackexchange websites cover around four million question eight million answer obtain eighty-four average precision seventy recall show technique robust effective widely applicable
present novel approach automatic report generation time series data context student feedback generation propose methodology treat content selection multi label classification mlc problem take input time series data students learn data output summary data feedback unlike previous work method consider data simultaneously use ensembles classifiers therefore achieve higher accuracy f score compare meaningful baselines
introduce language drive image generation task generate image visualize semantic content word embed eg give word embed grasshopper generate natural image grasshopper implement simple method base two map function first take input word embed produce eg word2vec toolkit map onto high level visual space eg space define one top layer convolutional neural network second function map abstract visual representation pixel space order generate target image several user study suggest current system produce image capture general visual properties concepts encode word embed color typical environment sufficient discriminate general categories object
owe rapidly grow multimedia content available internet extractive speak document summarization purpose automatically select set representative sentence speak document concisely express important theme document active area research experimentation hand word embed emerge newly favorite research subject excellent performance many natural language process nlp relate task however far aware relatively study investigate use extractive text speech summarization common thread leverage word embeddings summarization process represent document sentence average word embeddings word occur document sentence intuitively cosine similarity measure employ determine relevance degree pair representations beyond continue efforts make improve representation word paper focus build novel efficient rank model base general word embed methods extractive speech summarization experimental result demonstrate effectiveness propose methods compare exist state art methods
distil knowledge well train cumbersome network small one recently become new research topic lightweight neural network high performance particularly need various resource restrict systems paper address problem distil word embeddings nlp task propose encode approach distill task specific knowledge set high dimensional embeddings reduce model complexity large margin well retain high accuracy show good compromise efficiency performance experiment two task reveal phenomenon distil knowledge cumbersome embeddings better directly train neural network small embeddings
research automatically geolocating social media users conventionally base text content post give user social network user little crossover two bench mark two approach compara ble datasets bring two thread research together first propose text base method base adaptive grids follow hybrid network text base method evaluate three twitter datasets show empirical difference text network base methods great hybridisation two superior component methods especially contexts user graph well connect achieve state art result three datasets
objective critical distance ocd define space adjacent formants level valley reach mean spectral level measure ocd lie range viz three thirty-five bark critical distance determine subjective experiment similar experimental condition level spectral valley serve purpose similar space formants add advantage measure spectral envelope without explicit knowledge formant frequencies base relative space formant frequencies level spectral valley vi f1 f2 much higher level vii spectral valley f2 f3 back vowels vice versa front vowels classification vowels front back distinction difference vi vii acoustic feature test use timit ntimit tamil kannada language databases give average accuracy ninety-five comparable accuracy nine hundred and six obtain use neural network classifier train test use mfcc feature vector timit database acoustic feature vi vii also test robustness timit database additive white babble noise accuracy ninety-five obtain snrs twenty-five db type noise
tree structure neural network encode particular tree geometry sentence network design however model best slightly outperform simpler sequence base model hypothesize neural sequence model like lstms fact able discover implicitly use recursive compositional structure least task clear cue structure data demonstrate possibility use artificial data task recursive compositional structure crucial find lstm base sequence model indeed learn exploit underlie tree structure however performance consistently lag behind tree model even large train set suggest tree structure model effective exploit recursive structure
lambeks syntactic calculus commonly refer lambek calculus innovative many ways notably precursor linear logic also show could treat grammatical framework logic oppose logical theory however though successful give least basic treatment many linguistic phenomena also clear slightly expressive logical calculus need many case therefore many extensions variants lambek calculus propose since eighties present day result large class calculi empirical successes theoretical result also logical primitives raise question compare evaluate different logical formalisms answer question present two unify frameworks extend lambek calculi proof net calculi graph contraction criteria first calculus general system specify structure sequents give connectives contractions correspond calculus extend structural rule translate directly graph rewrite rule second calculus first order multiplicative intuitionistic linear logic turn several independently propose extensions lambek calculus fragment illustrate use calculus build bridge analyse propose different frameworks highlight differences help identify problems
book rich source fine grain information character object scene look like well high level semantics someone think feel state evolve story paper aim align book movie release order provide rich descriptive explanations visual content go semantically far beyond caption available current datasets align movies book exploit neural sentence embed train unsupervised way large corpus book well video text neural embed compute similarities movie clip sentence book propose context aware cnn combine information multiple source demonstrate good quantitative performance movie book alignment show several qualitative examples showcase diversity task model use
describe approach unsupervised learn generic distribute sentence encoder use continuity text book train encoder decoder model try reconstruct surround sentence encode passage sentence share semantic syntactic properties thus map similar vector representations next introduce simple vocabulary expansion method encode word see part train allow us expand vocabulary million word train model extract evaluate vectors linear model eight task semantic relatedness paraphrase detection image sentence rank question type classification four benchmark sentiment subjectivity datasets end result shelf encoder produce highly generic sentence representations robust perform well practice make encoder publicly available
dialog state track key component many modern dialog systems design single well define domain mind paper show dialog data draw different dialog domains use train general belief track model operate across domains exhibit superior performance domain specific model propose train procedure use domain data initialise belief track model entirely new domains procedure lead improvements belief track performance regardless amount domain data available train model
syntactic feature play essential role identify relationship sentence previous neural network model often suffer irrelevant information introduce subject object long distance paper propose learn robust relation representations shortest dependency path convolution neural network propose straightforward negative sample strategy improve assignment subject object experimental result show method outperform state art methods semeval two thousand and ten task eight dataset
propose label propagation approach geolocation prediction base modify adsorption two enhancements1 removal celebrity nod increase location homophily boost tractability two incorporation text base geolocation priors test users experiment three twitter benchmark datasets achieve state art result demonstrate effectiveness enhancements
latent dirichlet allocation lda mine thematic structure document play important role nature language process machine learn areas however probability distribution lda describe statistical relationship occurrences corpus usually practice probability best choice feature representations recently embed methods propose represent word document learn essential concepts representations word2vec doc2vec embed representations show effectiveness lda style representations many task paper propose topic2vec approach learn topic representations semantic vector space word alternative probability experimental result show topic2vec achieve interest meaningful result
recent years central components new approach linguistics minimalist program mp come closer physics feature minimalist program unconstrained nature recursive merge operation label algorithm operate interface narrow syntax conceptual intentional sensory motor interfaces difference pronounce un pronounce copy elements sentence build fibonacci sequence syntactic derivation sentence structure directly accessible representation term algebraic formalism although scheme linguistic structure classical ones find interest productive isomorphism establish mp structure algebraic structure many body field theory open new avenues inquiry dynamics underlie central aspects linguistics
paper consider task learn control policies text base game game interactions virtual world text underlie state observe result language barrier make environments challenge automatic game players employ deep reinforcement learn framework jointly learn state representations action policies use game reward feedback framework enable us map text descriptions vector representations capture semantics game state evaluate approach two game worlds compare baselines use bag word bag bigrams state representations algorithm outperform baselines worlds demonstrate importance learn expressive representations
distribute representations word paragraph semantic embeddings high dimensional data use across number natural language understand task retrieval translation classification work propose class vectors framework learn vector per class embed space word paragraph embeddings similarity class vectors word vectors use feature classify document class experiment several sentiment analysis task yelp review amazon electronic product review class vectors show better comparable result classification learn meaningful class embeddings
paper propose technique spectral envelope estimation use maximum value sub band fourier magnitude spectrum msasb methods literature parametrize spectral envelope cepstral domain mel generalize cepstrum etc cepstral domain representations although compact readily interpretable difficulty overcome method parametrizes spectral domain experiment spectral envelope estimate use msasb method incorporate straight vocoder objective subjective result analysis synthesis indicate propose method comparable straight also evaluate effectiveness propose parametrization statistical parametric speech synthesis framework use deep neural network
study extent online social network connect open knowledge base problem refer learn social knowledge graph propose multi modal bayesian embed model genvector learn latent topics generate word network embeddings genvector leverage large scale unlabeled data embeddings represent data two modalities ie social network users knowledge concepts share latent topic space experiment three datasets show propose method clearly outperform state art methods deploy method aminer large scale online academic search system network thirty-eight million, forty-nine thousand, one hundred and eighty-nine researchers knowledge base thirty-five million, four hundred and fifteen thousand and eleven concepts method significantly decrease error rate online b test live users
topic model techniques lda recently apply speech transcripts ocr output corpora may contain noisy erroneous texts may undermine topic stability therefore important know well topic model algorithm perform apply noisy data paper show different type textual noise diverse effect stability different topic model observations propose guidelines text corpus generation focus automatic speech transcription also suggest topic model selection methods noisy corpora
paper present find readability assessment sentiment analysis select six philippine senators microposts popular twitter microblog use simple measure gobbledygook smog tweet senators cayetano defensor santiago pangilinan marcos guingona escudero assess sentiment analysis also do determine polarity senators respective microposts result show average six senators tweet eight ten smog level mean least sixth grader able understand senators tweet moreover tweet mostly neutral sentiments vary unison period time could mean senator tweet sentiment affect specific philippine base events
classification television content help users organise navigate large list channel program available paper address problem television content classification exploit text information extract program transcriptions present analysis adapt model sentiment widely successfully apply field music blog post use real world dataset obtain boxfish api compare performance classifiers train number different feature set experiment show large collection television content program genres represent three dimensional space valence arousal dominance promise classification result achieve use feature base representation find support use propose representation television content feature space similarity computation recommendation generation
artificial communities agents develop language scale relations close zipf law preliminary answer question propose automata network model formation vocabulary population individuals two principle opposite strategies alignment least effort principle within previous account emergence linguistic conventions specially name game focus model speaker hearer efforts action vocabularies study impact action formation share language numerical simulations essentially base energy function measure amount local agreement vocabularies result suggest one dimensional lattices best strategy formation share languages one minimize efforts speakers communicative task
work attempt give new theoretical insights absence intermediate stag evolution language particular develop automata network approach crucial question population language users reach agreement linguistic convention describe appearance sharp transition self organization language adopt extremely simple model work memory time step language users simply loss part word memories computer simulations low dimensional lattices appear sharp transition critical value depend size vicinities individuals
apply general deep learn framework address non factoid question answer task approach rely linguistic tool apply different languages domains various architectures present compare create release qa corpus setup new qa task insurance domain experimental result demonstrate superior performance compare baseline methods various technologies give improvements highly challenge task top one accuracy reach six hundred and fifty-three test set indicate great potential practical use
current research use contexts yous presidential debate negotiations examine whether match linguistic style opponent two party exchange affect reactions third party observers build communication accommodation theory cat interaction alignment theory iat process fluency propose language style match lsm improve subsequent third party evaluations match opponent linguistic style reflect greater perspective take make one arguments easier process contrast research status inferences predict lsm negatively impact third party evaluations lsm imply followership conduct two study test compete hypotheses study one analyze transcripts yous presidential debate one thousand, nine hundred and seventy-six two thousand and twelve find candidates match opponent linguistic style increase stand poll study two demonstrate causal relationship lsm third party observer evaluations use negotiation transcripts
examine possibility recent promise result automatic caption generation due primarily language model vary image representation quality produce convolutional neural network find state art neural caption algorithm able produce quality caption even provide surprisingly poor image representations replicate result new fine grain transfer learn caption domain consist 66k recipe image title pair also provide experiment regard appropriateness datasets automatic caption find multiple caption per image beneficial absolute requirement
structural kernels flexible learn paradigm widely use natural language process however problem model selection kernel base methods usually overlook previous approach mostly rely set default value kernel hyperparameters use grid search slow coarse grain contrast bayesian methods allow efficient model selection maximize evidence train data gradient base methods paper show perform context structural kernels use gaussian process experimental result tree kernels show procedure result better prediction performance compare hyperparameter optimization via grid search framework propose paper adapt structure besides tree eg string graph thereby extend utility kernel base methods
show train fast dependency parser smith eisner two thousand and eight improve accuracy parser consider higher order interactions among edge retain ofn3 runtime output parse maximum expect recall speed expectation take posterior distribution construct approximately use loopy belief propagation structure factor show adjust model parameters compensate errors introduce approximation follow gradient actual loss train data find gradient back propagation treat entire parser approximations differentiable circuit stoyanov et al two thousand and eleven domke two thousand and ten loopy crfs result train parser obtain higher accuracy fewer iterations belief propagation one train conditional log likelihood
train statistical speak dialogue system sds essential accurate method measure task success available date train rely present task either simulate pay users infer dialogue success observe whether present task achieve aim however able learn real users act volition case non trivial rate success prior knowledge task simply unavailable user feedback may utilise find inconsistent hence present two neural network model evaluate sequence turn level feature rate success dialogue importantly model make use prior knowledge user task model train dialogues generate simulate user best model use train policy line show perform least well baseline system use prior knowledge user task note model also interest evaluate sds monitor dialogue rule base sds
statistical speak dialogue systems attractive property able optimise data via interactions real users however reinforcement learn paradigm dialogue manager agent often require significant time explore state action space learn behave desirable manner critical issue system train line real users learn cost expensive reward shape one promise technique address concern examine three recurrent neural network rnn approach provide reward shape information addition primary task orientate environmental feedback rnns train return dialogues generate simulate user attempt diffuse overall evaluation dialogue back turn level guide agent towards good behaviour faster simulate real user scenarios rnns show increase policy learn speed importantly require prior knowledge user goal
program question answer q websites quora stack overflow yahoo answer etc help us understand program concepts easily quickly way test apply many software developers stack overflow one frequently use program qanda website question answer post presently analyze manually require huge amount time resource save effort present topic model base technique analyze word original texts discover theme run also propose method automate process review quality question stack overflow dataset order avoid balloon stack overflow insignificant question propose method also recommend appropriate tag new post avert creation unnecessary tag stack overflow
relation classification important research arena field natural language process nlp paper present sdp lstm novel neural network classify relation two entities sentence neural architecture leverage shortest dependency path sdp two entities multichannel recurrent neural network long short term memory lstm units pick heterogeneous information along sdp propose model several distinct feature one shortest dependency paths retain relevant information relation classification eliminate irrelevant word sentence two multichannel lstm network allow effective information integration heterogeneous source dependency paths three customize dropout strategy regularize neural network alleviate overfitting test model semeval two thousand and ten relation classification task achieve f1 score eight hundred and thirty-seven higher compete methods literature
paper aim compare different regularization strategies address common phenomenon severe overfitting embed base neural network nlp choose two widely study neural model task testbed try several frequently apply newly propose regularization strategies include penalize weight embeddings exclude penalize embeddings embed word dropout also emphasize incremental hyperparameter tune combine different regularizations result provide picture tune hyperparameters neural nlp model
short note present extension long short term memory lstm neural network use depth gate connect memory cells adjacent layer introduce linear dependence lower upper layer recurrent units importantly linear dependence gate gate function call depth gate gate function lower layer memory cell input past memory cell layer conduct experiment verify new architecture lstms able improve machine translation language model performances
success deep learn often derive well choose operational build block work revise temporal convolution operation cnns better adapt text process instead concatenate word representations appeal tensor algebra use low rank n gram tensors directly exploit interactions word already convolution stage moreover extend n gram convolution non consecutive word recognize pattern intervene word combination low rank tensors pattern weight efficiently evaluate result convolution operation via dynamic program test result architecture standard sentiment classification news categorization task model achieve state art performance term accuracy train speed instance obtain five hundred and twelve accuracy fine grain sentiment classification task
precise geocoding time normalization text require location time phrase identify many state art geoparsers temporal parsers suffer low recall categories commonly miss parsers nouns use non spatiotemporal sense adjectival adverbial phrase prepositional phrase numerical phrase collect annotate data set query commercial web search api spatiotemporal expressions miss state art parsers due high cost sentence annotation active learn use label train data new strategy design better select train examples reduce label cost learn algorithm apply average perceptron train featurized hide markov model fhmm five fhmm instance use create ensemble output phrase select vote ensemble model test range sequential label task show competitive performance contributions include one new dataset annotate name entities expand spatiotemporal expressions two comparison inference algorithms ensemble model show superior accuracy belief propagation viterbi decode three new example weight method active ensemble learn memorize latest examples train four spatiotemporal parser jointly recognize expand spatiotemporal expressions well name entities
weak topic correlation across document collections different number topics individual collections present challenge exist cross collection topic model paper introduce two probabilistic topic model correlate lda c lda correlate hdp c hdp address problems arise analyze large asymmetric potentially weakly relate collections topic correlations weakly relate collections typically lie tail topic distribution would overlook model unable fit large number topics efficiently model long tail large scale analysis model implement parallel sample algorithm base metropolis hastings alias methods yuan et al two thousand and fifteen model first evaluate synthetic data generate simulate various collection level asymmetries present case study model 300k document collections sciences humanities research jstor
text mine apply many field one application use text mine digital newspaper politic sentiment analysis paper sentiment analysis apply get information digital news article positive negative sentiment regard particular politician paper suggest simple model analyze digital newspaper sentiment polarity use naive bay classifier method model use set initial data begin update new information appear model show promise result test implement sentiment analysis problems
present general framework compare multiple group document bipartite graph model propose document group represent one node set comparison criteria represent node set use model present basic algorithms extract insights similarities differences among document group finally demonstrate versatility framework analysis nsf fund program basic research
rouge widely adopt automatic evaluation measure text summarization show correlate well human judgements bias towards surface lexical similarities make unsuitable evaluation abstractive summarization summaries substantial paraphrase study effectiveness word embeddings overcome disadvantage rouge specifically instead measure lexical overlap word embeddings use compute semantic similarity word use summaries instead experimental result show proposal able achieve better correlations human judgements measure spearman kendall rank coefficients
long short term memory lstm network type recurrent neural network complex computational unit successfully apply variety sequence model task paper develop tree long short term memory treelstm neural network model base lstm design predict tree rather linear sequence treelstm define probability sentence estimate generation probability dependency tree time step node generate base representation generate sub tree enhance model power treelstm explicitly represent correlations leave right dependents application model msr sentence completion challenge achieve result beyond current state art also report result dependency parse reranking achieve competitive performance
prosody affect naturalness intelligibility speech however automatic prosody prediction text chinese speech synthesis still great challenge traditional conditional random field crf base method always heavily rely feature engineer paper propose use neural network predict prosodic boundary label directly chinese character without feature engineer experimental result show stack fee forward bidirectional long short term memory blstm recurrent network layer achieve superior performance crf base method embed feature learn raw text enhance performance
wikipedia widely use find general information wide variety topics vocation provide local information example provide plot cast production information give movie show time local movie theatre describe connect local information wikipedia without alter content case study present involve find local scientific experts use third party taxonomy independent wikipedia category hierarchy index information connect local experts present activity report index wikipedia content use taxonomy connections wikipedia page local expert report store relational database accessible public sparql endpoint wikipedia gadget plugin activate interest user access endpoint wikipedia page access additional tab wikipedia page allow user open list team local experts associate subject matter wikipedia page technique though present way identify local experts generic third party taxonomy use connect wikipedia non wikipedia data source
present two approach use unlabeled data improve sequence learn recurrent network first approach predict come next sequence conventional language model natural language process second approach use sequence autoencoder read input sequence vector predict input sequence two algorithms use pretraining step later supervise sequence learn algorithm word parameters obtain unsupervised step use start point supervise train model experiment find long short term memory recurrent network pretrained two approach stable generalize better pretraining able train long short term memory recurrent network hundred timesteps thereby achieve strong performance many text classification task imdb dbpedia twenty newsgroups
method multi lingual geoparsing use monolingual tool resources along machine translation alignment return location word many languages method save time cost develop geoparsers language separately also allow possibility wide range language capabilities within single interface evaluate method languagebridge prototype location name entities use newswire broadcast news telephone conversations english arabic chinese data linguistic data consortium ldc result geoparsing chinese arabic text use multi lingual geoparsing method comparable result geoparsing english text english tool furthermore experiment use machine translation approach result accuracy comparable result data translate manually
recent paper levy goldberg point interest connection prediction base word embed model count model base pointwise mutual information certain condition show model end optimize equivalent objective function paper explore connection detail lay factor lead differences model find relevant differences optimization perspective predict model work low dimensional space embed vectors interact heavily ii since predict model fewer parameters less prone overfitting motivate insight analysis show count model regularize principled manner provide close form solutions l1 l2 regularization finally propose new embed model convex objective additional benefit intelligible
predict mild cognitive impairment mci currently challenge exist diagnostic criteria rely neuropsychological examinations automate machine learn ml model train verbal utterances mci patients aid diagnosis use combination skip gram feature model learn several linguistic biomarkers distinguish nineteen patients mci nineteen healthy control individuals dementiabank language transcript clinical dataset result show model compound skip grams better auc could help ml prediction small mci data sample
emoticons eg widely use sentiment analysis nlp task feature chine learn algorithms entries sentiment lexicons paper argue emoticons strong common signal sentiment expression social media relationship emoticons sentiment polarity always clear thus algorithm deal sentiment polarity take emoticons account extreme cau tion exercise emoticons depend first demonstrate prevalence emoticons social media analyze frequency emoticons large cent twitter data set carry four analyse examine relationship emoticons sentiment polarity well contexts emoticons use first analysis survey group participants perceive sentiment polarity frequent emoticons second analysis examine cluster word emoti con better understand mean convey emoti con third analysis compare sentiment polarity microblog post emoticons remove text last analysis test hypothesis remove emoticons text hurt sentiment classification train two machine learn model without emoticons text respectively result confirm arguments one emoticons strong reliable signal sentiment polarity one take advantage senti ment analysis two large group emoticons convey com plicate sentiment hence treat extreme caution
describe method visual question answer capable reason content image basis information extract large scale knowledge base method answer natural language question use concepts contain image provide explanation reason develop answer method capable answer far complex question predominant long short term memory base approach outperform significantly test also provide dataset protocol evaluate methods thus address one key issue general visual ques tion answer
propose system automate essay grade use ontologies textual entailment process textual entailment guide hypotheses extract domain ontology textual entailment check truth hypothesis follow give text enact textual entailment compare students answer model answer obtain ontology validate solution various essay write students chemistry domain
recommender system basic task estimate users respond unseen items typically model term user might rate product aim extend approach model user would write product design character level recurrent neural network rnn generate personalize product review network convincingly learn style opinions nearly one thousand distinct author use large corpus review beeradvocatecom also tailor review describe specific items categories star rat use simple input replication strategy generative concatenative network gcn preserve signal static auxiliary input across wide sequence intervals without additional train generative model classify review identify author review product category sentiment rat remarkable accuracy evaluation show gcn capture complex dynamics text effect negation misspell slang large vocabularies gracefully absent machinery explicitly dedicate purpose
work study representational map across multimodal data give piece raw data one modality correspond semantic description term raw data another modality immediately obtain representational map find wide spectrum real world applications include image video retrieval object recognition action behavior recognition event understand prediction end introduce simplify train objective learn multimodal embeddings use skip gram architecture introduce convolutional pseudowords embeddings compose additive combination distribute word representations image feature convolutional neural network project multimodal space present extensive result representational properties embeddings various word similarity benchmarks show promise approach
paper apply general deep learn dl framework answer selection task depend manually define feature linguistic tool basic framework build embeddings question answer base bidirectional long short term memory bilstm model measure closeness cosine similarity extend basic model two directions one direction define composite representation question answer combine convolutional neural network basic framework direction utilize simple efficient attention mechanism order generate answer representation accord question context several variations model provide model examine two datasets include trec qa insuranceqa experimental result demonstrate propose model substantially outperform several strong baselines
paper address task natural language object retrieval localize target object within give image base natural language query object natural language object retrieval differ text base image retrieval task involve spatial information object within scene global scene context address issue propose novel spatial context recurrent convnet scrc model score function candidate box object retrieval integrate spatial configurations global scene level contextual information network model process query text local image descriptors spatial configurations global context feature recurrent network output probability query text condition candidate box score box transfer visual linguistic knowledge image caption domain task experimental result demonstrate method effectively utilize local global information outperform previous baseline methods significantly different datasets scenarios exploit large scale vision language datasets knowledge transfer
development community base question answer qanda service large scale qanda archive accumulate important information knowledge resource web question answer match attach much importance ability reuse knowledge store systems useful enhance user experience recurrent question paper try improve match accuracy overcome lexical gap question answer pair word embed base correlation wec model propose integrate advantage translation model word embed give random pair word wec score co occurrence probability qanda pair also leverage continuity smoothness continuous space word representation deal new pair word rare train parallel text experimental study yahoo answer dataset baidu zhidao dataset show new method promise potential
lot prior work representation learn speech recognition applications much emphasis give investigation effective representations affect speech paralinguistic elements speech separate verbal content paper explore denoising autoencoders learn paralinguistic attribute ie categorical dimensional affective traits speech show representations learn bottleneck layer autoencoder highly discriminative activation intensity separate negative valence sadness anger positive valence happiness experiment different input speech feature fft log mel spectrograms temporal context windows different autoencoder architectures stack deep autoencoders also learn utterance specific representations combination denoising autoencoders blstm base recurrent autoencoders emotion classification perform learn temporal dynamic representations evaluate quality representations experiment well establish real life speech dataset iemocap show learn representations comparable state art feature extractors voice quality feature mfccs competitive state art approach emotion dimensional affect recognition
recent deep neural network model achieve promise result image caption task rely largely availability corpora pair image sentence caption describe object context work propose deep compositional captioner dcc address task generate descriptions novel object present pair image sentence datasets method achieve leverage large object recognition datasets external text corpora transfer knowledge semantically similar concepts current deep caption model describe object contain pair image sentence corpora despite fact pre train large object recognition datasets namely imagenet contrast model compose sentence describe novel object interactions object demonstrate model ability describe novel concepts empirically evaluate performance mscoco show qualitative result imagenet image object pair image caption data exist extend approach generate descriptions object video clip result show dcc distinct advantage exist image video caption approach generate descriptions new object context
last decade witness success traditional feature base method exploit discrete structure word lexical pattern extract relations text recently convolutional recurrent neural network provide effective mechanisms capture hide structure within sentence via continuous representations thereby significantly advance performance relation extraction advantage convolutional neural network capacity generalize consecutive k grams sentence recurrent neural network effective encode long range sentence context paper propose combine traditional feature base method convolutional recurrent neural network simultaneously benefit advantage systematic evaluation different network architectures combination methods demonstrate effectiveness approach result state art performance ace two thousand and five semeval dataset
introduce segmental recurrent neural network srnns define give input sequence joint probability distribution segmentations input label segment representations input segment ie contiguous subsequences input compute encode constituent tokens use bidirectional recurrent neural net segment embeddings use define compatibility score output label local compatibility score integrate use global semi markov conditional random field fully supervise train segment boundaries label observe well partially supervise train segment boundaries latent straightforward experiment handwrite recognition joint chinese word segmentation pos tag show compare model explicitly represent segment bio tag scheme connectionist temporal classification ctc srnns obtain substantially higher accuracies
transfer learn vital technique generalize model train one set task settings task example speech recognition acoustic model train one language use recognize speech another language little train data transfer learn closely relate multi task learn cross lingual vs multilingual traditionally study name model adaptation recent advance deep learn show transfer learn become much easier effective high level abstract feature learn deep model transfer conduct data distributions data type also model structure eg shallow net deep net even model type eg bayesian model neural model review paper summarize recent prominent research towards direction particularly speech language process also report result group highlight potential interest research field
crucial aspect knowledge base population system extract new facts text corpora generation train data relation extractors paper present method maximize effectiveness newly train relation extractors minimal annotation cost manual label significantly reduce distant supervision method construct train data automatically align large text corpus exist knowledge base know facts example sentence mention barack obama us may serve positive train instance relation borninsubjectobject however distant supervision typically result highly noisy train set many train sentence really express intend relation propose combine distant supervision minimal manual supervision technique call feature label eliminate noise large noisy initial train set result significant increase precision improve approach introduce semantic label propagation method use similarity low dimensional representations candidate train instance extend train set order increase recall maintain high precision propose strategy generate train data study evaluate establish test collection design knowledge base population task experimental result show semantic label propagation strategy lead substantial performance gain compare exist approach require almost negligible manual annotation effort
parallel sentence relatively scarce extremely useful resource many applications include cross lingual retrieval statistical machine translation research explore new methodologies mine data previously obtain comparable corpora task highly practical since non parallel multilingual data exist far greater quantities parallel corpora parallel sentence much useful resource propose web crawl method build subject align comparable corpora eg wikipedia dump euronews web page improvements machine translation show polish english language pair various text domains also test another method build parallel corpora base comparable corpora data let us automatically broad exist corpus sentence subject corpora base analogies
recurrent neural network convenient efficient model language model however apply level character instead word suffer several problems order successfully model long term dependencies hide representation need large turn imply higher computational cost become prohibitive practice propose two alternative structural modifications classical rnn model first one consist condition character level representation previous word representation one use character history condition output probability evaluate performance two propose modifications challenge multi lingual real world data
message often refer entities people place events correct identification intend reference essential part communication lack share unique name often complicate entity reference share knowledge use construct uniquely identify descriptive reference entities ambiguous name introduce mathematical model reference description derive result condition high probability program construct unambiguous reference entities domain discourse provide empirical validation result
standard recurrent neural network language model rnnlm generate sentence one word time work explicit global sentence representation work introduce study rnn base variational autoencoder generative model incorporate distribute latent representations entire sentence factorization allow explicitly model holistic properties sentence style topic high level syntactic feature sample prior sentence representations remarkably produce diverse well form sentence simple deterministic decode examine paths latent space able generate coherent novel sentence interpolate know sentence present techniques solve difficult learn problem present model demonstrate effectiveness impute miss word explore many interest properties model latent sentence space present negative result use model language model
accurate representational learn explicit implicit relationships within data critical ability machine perform complex abstract reason task describe efficient weakly supervise learn inferences dynamic adaptive network intelligence dani model report state art result dani question answer task babi dataset prove difficult contemporary approach learn representation weston et al two thousand and fifteen
neural word representations prove useful natural language process nlp task due ability efficiently model complex semantic syntactic word relationships however techniques model one representation per word despite fact single word multiple mean sense techniques model word use multiple vectors cluster base context however recent neural approach rarely focus application consume nlp algorithm furthermore train process recent word sense model expensive relative single sense embed process paper present novel approach address concern model multiple embeddings word base supervise disambiguation provide fast accurate way consume nlp model select sense disambiguate embed demonstrate embeddings disambiguate contrastive sense nominal verbal sense well nuanced sense sarcasm evaluate part speech disambiguate embeddings neural dependency parse yield greater eight average error reduction unlabeled attachment score across six languages
universal schema build knowledge base kb entities relations jointly embed relation type input kbs well textual pattern express relations raw text previous applications universal schema textual pattern represent single embed prevent generalization unseen pattern recent work employ neural network capture pattern compositional semantics provide generalization possible input text response paper introduce significant improvements coverage flexibility universal schema relation extraction predictions entities unseen train multilingual transfer learn domains annotation evaluate model extensive experiment english spanish tac kbp benchmark outperform top system tac two thousand and thirteen slot fill use handwritten pattern additional annotation also consider multilingual set english train data entities overlap seed kb spanish text despite annotation spanish data train accurate predictor additional improvements obtain tie word embeddings across languages furthermore find multilingual train improve english relation extraction accuracy approach thus suit broad coverage automate knowledge base construction variety languages domains
recent methods learn vector space representations word succeed capture fine grain semantic syntactic regularities use vector arithmetic however vector space representations create large scale text analysis typically store verbatim since internal structure opaque use word analogy test monitor level detail store compress representations vector space trade off reduction memory usage expressiveness investigate simple scheme outline reduce memory footprint state art embed factor ten minimal impact performance use bite budget binary approximate factorisation space also explore aim create equivalent representation better interpretability
integration multiple microphone data one key ways achieve robust speech recognition noisy environments speaker locate distance input device signal process techniques beamforming widely use extract speech signal interest background noise techniques however highly dependent prior spatial information microphones environment system use work present neural attention network directly combine multi channel audio generate phonetic state without require prior knowledge microphone layout explicit signal preprocessing speech enhancement embed attention mechanism within recurrent neural network rnn base acoustic model automatically tune attention reliable input source unlike traditional multi channel preprocessing system optimize towards desire output one step although attention base model recently achieve impressive result sequence sequence learn attention mechanisms previously apply learn potentially asynchronous non stationary multiple input evaluate neural attention model chime three challenge task show model achieve comparable performance beamforming use purely data drive method
methods learn word representations use large text corpora receive much attention lately due impressive performance numerous natural language process nlp task semantic similarity measurement word analogy detection despite success data drive word representation learn methods consider rich semantic relational structure word co occur context hand already much manual effort go construction semantic lexicons wordnet represent mean word define various relationships exist among word language consider question improve word representations learn use corpora integrate knowledge semantic lexicons purpose propose joint word representation learn method simultaneously predict co occurrences two word sentence subject relational constrain give semantic lexicon use relations exist word lexicon regularize word representations learn corpus propose method statistically significantly outperform previously propose methods incorporate semantic lexicons word representations several benchmark datasets semantic similarity word analogy
integrate higher level visual linguistic interpretations heart human intelligence automatic visual category recognition image approach human performance high level understand dynamic spatiotemporal domain videos translation natural language still far solve work vision text translations use pre learn pre establish computational linguistic model paper present approach use vision alone efficiently learn translate language video content discover simple form story play main actors use visual cue represent object interactions method learn hierarchical manner higher level representations recognize subject action object involve relevant contextual background interaction one another time three stage approach first take consideration feature individual entities local level appearance consider relationship object action video background third consider spatiotemporal relations input classifiers highest level interpretation thus approach find coherent linguistic description videos form subject verb object base role play overall visual story learn directly train data without use know language model test efficiency approach large scale dataset contain youtube clip take wild demonstrate state art performance often superior current approach use complex pre learn linguistic knowledge
many natural language process applications use language model generate text model typically train predict next word sequence give previous word context image however test time model expect generate entire sequence scratch discrepancy make generation brittle errors may accumulate along way address issue propose novel sequence level train algorithm directly optimize metric use test time bleu rouge three different task approach outperform several strong baselines greedy generation method also competitive baselines employ beam search several time faster
aim name entity recognition ner identify reference name entities unstructured document classify pre define semantic categories ner often aid add background knowledge form gazetteers however use collection deal name variants resolve ambiguities associate identify entities context associate predefined categories present semi supervise ner approach start identify name entities small set train data use identify name entities word context feature use define pattern pattern name entity category use seed pattern identify name entities test set pattern score tuple value score enable generation new pattern identify name entity categories evaluate propose system english language dataset tag ieer untagged conll two thousand and three name entity corpus tamil language document fire corpus yield average f measure seventy-five languages
consider visual sentiment task map image adjective noun pair anp cute baby capture two factor structure anp semantics well overcome annotation noise ambiguity propose novel factorize cnn model learn separate representations adjectives nouns optimize classification performance product experiment publicly available sentibank dataset show model significantly outperform independent anp classifiers unseen anps retrieve image novel anps also image caption model capture word semantics co occurrence natural text latter turn surprisingly poor capture sentiment evoke pure visual experience factorize anp cnn train better noisy label generalize better new image also expand anp vocabulary
long term goal machine learn build intelligent conversational agents one recent popular approach train end end model large amount real dialog transcripts humans sordoni et al two thousand and fifteen vinyals le two thousand and fifteen shang et al two thousand and fifteen however approach leave many question unanswered understand precise successes shortcomings model hard assess contrast recent proposal babi task weston et al 2015b synthetic data measure ability learn machine various reason task toy language unfortunately test small hence may encourage methods scale work propose suite new task much larger scale attempt bridge gap two regimes choose domain movies provide task test ability model answer factual question utilize omdb provide personalization utilize movielens carry short conversations two finally perform natural dialogs reddit provide dataset cover 75k movie entities 35m train examples present result various model task evaluate performance
work leverage linear algebraic structure distribute word representations automatically extend knowledge base allow machine learn new facts world goal extract structure facts corpora simpler manner without apply classifiers pattern use co occurrence statistics word demonstrate linear algebraic structure word embeddings use reduce data requirements methods learn facts particular demonstrate word belong common category pair word satisfy certain relation form low rank subspace project space compute basis low rank subspace use singular value decomposition svd use basis discover new facts fit vectors less frequent word yet vectors
non sentential utterances nsus utterances lack complete sentential form whose mean infer dialogue context ok probably apartment interpretation non sentential utterances important problem computational linguistics since constitute frequent phenomena dialogue intrinsically context dependent interpretation nsus task retrieve full semantic content form dialogue context first half thesis devote nsu classification task work build upon fern andez et al two thousand and seven present series machine learn experiment classification nsus extend approach combination new feature semi supervise learn techniques empirical result present thesis show modest significant improvement state art classification performance consecutive yet independent problem infer appropriate semantic representation nsus basis dialogue context fern andez two thousand and six formalize task term resolution rule build top type theory record ttr work focus reimplementation resolution rule fern andez two thousand and six probabilistic account dialogue state probabilistic rule formalism lison two thousand and fourteen particularly suit task similarly framework develop ginzburg two thousand and twelve fern andez two thousand and six involve specification update rule variables dialogue state capture dynamics conversation however probabilistic rule also encode probabilistic knowledge thereby provide principled account ambiguities nsu resolution process
propose model learn visually ground word embeddings vis w2v capture visual notions semantic relatedness word embeddings train use text extremely successful uncover notions semantic relatedness implicit visual world instance although eat star seem unrelated text share semantics visually people eat something also tend stare food ground diverse relations like eat star vision remain challenge despite recent progress vision note visual ground word depend semantics literal pixels thus use abstract scenes create clipart provide visual ground find embeddings learn capture fine grain visually ground notions semantic relatedness show improvements text word embeddings word2vec three task common sense assertion classification visual paraphrase text base image retrieval code datasets available online
lecture note course ds ga three thousand and one center data science new york university fall two thousand and fifteen name course suggest lecture note introduce readers neural network base approach natural language understand process order make self contain possible spend much time describe basics machine learn neural network use natural languages introduce language front almost solely focus language model machine translation two personally find fascinate fundamental natural language understand
development intelligent machine one biggest unsolved challenge computer science paper propose fundamental properties machine focus particular communication learn discuss simple environment could use incrementally teach machine basics natural language base communication prerequisite complex interaction human users also present conjecture sort algorithms machine support order profitably learn environment
consider problem learn general purpose paraphrastic sentence embeddings base supervision paraphrase database ganitkevitch et al two thousand and thirteen compare six compositional architectures evaluate annotate textual similarity datasets draw distribution train data wide range domains find complex architectures long short term memory lstm recurrent neural network perform best domain data however domain scenarios simple architectures word average vastly outperform lstms simplest average model even competitive systems tune particular task also extremely efficient easy use order better understand architectures compare conduct experiment three supervise nlp task sentence similarity entailment sentiment classification find word average model perform well sentence similarity entailment outperform lstms however sentiment classification find lstm perform strongly even record new state art performance stanford sentiment treebank demonstrate combine pretrained sentence embeddings supervise task use prior black box feature extractor lead performance rival state art sick similarity entailment task release resources research community hope serve new baseline work universal sentence embeddings
additive composition foltz et al one thousand, nine hundred and ninety-eight landauer dumais one thousand, nine hundred and ninety-seven mitchell lapata two thousand and ten widely use method compute mean phrase take average vector representations constituent word article prove upper bind bias additive composition first theoretical analysis compositional frameworks machine learn point view bind write term collocation strength prove exclusively two successive word tend occur together accurate one guarantee additive composition approximation natural phrase vector proof rely properties natural language data empirically verify theoretically derive assumption data generate hierarchical pitman yor process theory endorse additive composition reasonable operation calculate mean phrase suggest ways improve additive compositionality include transform entries distributional word vectors function meet specific condition construct novel type vector representations make additive composition sensitive word order utilize singular value decomposition train word vectors
development summarization research significantly hamper costly acquisition reference summaries paper propose effective way automatically collect large scale news relate multi document summaries reference social media reactions utilize two type social label tweet ie hashtags hyper link hashtags use cluster document different topic set also tweet hyper link often highlight certain key point correspond document synthesize link document cluster form reference summary cover key point aim adopt rouge metrics measure coverage ratio develop integer linear program solution discover sentence set reach upper bind rouge since allow summary sentence select document high quality tweet generate reference summaries could abstractive informativeness readability collect summaries verify manual judgment addition train support vector regression summarizer duc generic multi document summarization benchmarks collect data extra train resource performance summarizer improve lot test set release dataset research
binary relation extraction methods widely study recent years however methods develop higher n ary relation extraction one limit factor effort require generate train data binary relations one provide dozen pair entities per relation train data ternary relations n3 train instance triplet entities place greater cognitive load people example many people know google acquire youtube dollar amount date acquisition many people know hillary clinton marry bill clinton location date wed make higher n nary train data generation time consume exercise search web present resource train ternary relation extractors generate use minimally supervise yet effective approach present statistics size quality dataset
suicide global public health problem early detection individual suicide risk play key role suicide prevention paper propose look individual suicide risk time series analysis personal linguistic expression social media weibo examine temporal pattern linguistic expression individuals chinese social media weibo use temporal pattern predictor variables build classification model estimate level individual suicide risk characteristics time sequence curve linguistic feature include parentheses auxiliary verbs personal pronouns body word report affect performance suicide predict model accuracy higher sixty show result paper confirm efficiency social media data detect individual suicide risk result study may insightful improve performance suicide prevention program
study character play vital role computationally represent interpret narratives unlike previous work focus infer character roles focus problem model relationships rather assume fix relationship character pair hypothesize relationships dynamic temporally evolve progress narrative formulate problem relationship model structure prediction problem propose semi supervise framework learn relationship sequence fully well partially label data present markovian model capable accumulate historical beliefs relationship status change use set rich linguistic semantically motivate feature incorporate world knowledge investigate textual content narrative empirically demonstrate framework outperform competitive baselines
speak language translation slt become important increasingly globalize world machine translation mt automatic speech recognition asr systems major challenge great interest research investigate automatic sentence segmentation speech important enrich speech recognition output aid downstream language process article focus automatic sentence segmentation speech improve mt result explore problem identify sentence boundaries transcriptions produce automatic speech recognition systems polish language also experiment reverse normalization recognize speech sample
ability comprehend wish desire fulfillment important natural language understand paper introduce task identify desire express subject give short piece text fulfil propose various unstructured structure model capture fulfillment cue subject emotional state action experiment two different datasets demonstrate importance understand narrative discourse structure address task
paper investigate scale properties recurrent neural network language model rnnlms discuss train large rnns gpus address question rnnlms scale respect model size train set size computational cost memory analysis show despite costly train rnnlms obtain much lower perplexities standard benchmarks n gram model train largest know rnns present relative word error rat gain eighteen asr task also present new lowest perplexities recently release billion word language model benchmark one bleu point gain machine translation seventeen relative hit rate gain word prediction
article demontrates apply deep learn text understand character level input way abstract text concepts use temporal convolutional network convnets apply convnets various large scale datasets include ontology classification sentiment analysis text categorization show temporal convnets achieve astonish performance without knowledge word phrase sentence syntactic semantic structure regard human language evidence show model work english chinese
paper describe novel framework discovery topical content data corpus track complex structural change across temporal dimension contrast previous work model impose prior rate document add corpus adopt markovian assumption overly restrict type change model capture key technical contribution framework base discretization time epochs ii epoch wise topic discovery use hierarchical dirichlet process base model iii temporal similarity graph allow model complex topic change emergence disappearance evolution split merge power propose framework demonstrate medical literature corpus concern autism spectrum disorder asd increasingly important research subject significant social healthcare importance addition collect asd literature corpus make freely available contributions also include two free online tool build aid asd researchers use semantically meaningful navigation search well knowledge discovery large rapidly grow corpus literature
term frequency normalization serious issue since lengths document various generally document become long due two different reason verbosity multi topicality first verbosity mean topic repeatedly mention term relate topic term frequency increase well summarize one second multi topicality indicate document broad discussion multi topics rather single topic although document characteristics differently handle previous methods term frequency normalization ignore differences use simplify length drive approach decrease term frequency length document cause unreasonable penalization attack problem propose novel tf normalization method type partially axiomatic approach first formulate two formal constraints retrieval model satisfy document verbose multi topicality characteristic respectively modify language model approach better satisfy two constraints derive novel smooth methods experimental result show propose method increase significantly precision keyword query substantially improve map mean average precision verbose query
sentiment analysis user review help keep track user reactions towards products make advices users buy state art review level sentiment classification techniques could give pretty good precisions ninety however current phrase level sentiment analysis approach might give sentiment polarity label precisions around seven thousand and eighty far satisfaction restrict application many practical task paper focus problem phrase level sentiment polarity label attempt bridge gap phrase level review level sentiment analysis investigate inconsistency numerical star rat sentiment orientation textual user review although long treat identical serve basic assumption previous work find assumption necessarily true propose leverage result review level sentiment classification boost performance phrase level polarity label use novel constrain convex optimization framework besides framework capable integrate various kinds information source heuristics give global optimal solution due convexity experimental result english chinese review show framework achieve high label precisions eighty-nine significant improvement current approach
address paper co cluster co classification bilingual data lay two linguistic similarity space comparability measure define map two space available new approach characterize three mode analysis scheme propose mix comparability measure two similarity measure aim improve jointly accuracy classification cluster task perform two linguistic space well quality final alignment comparable cluster obtain use first purely synthetic random data set assess formal similarity comparability mix model propose two variants comparability measure define li gaussier two thousand and ten context bilingual lexicon extraction adapt cluster categorize task two variant measure subsequently use evaluate similarity comparability mix model context co classification co cluster comparable textual data set collect wikipedia categories english french languages experiment show clear improvements cluster classification accuracies mix comparability similarity measure expect higher robustness obtain two comparability variant measure propose use believe approach particularly well suit construction thematic comparable corpora controllable quality
article analyze users edit wikipedia article okinawa japan english japanese find users among active dedicate users primary languages make many large high quality edit however users edit non primary languages tend make edit different type overall smaller size often restrict narrow set article exist languages design change motivate wider contributions users non primary languages encourage multilingual users transfer information across language divide present
tempt treat frequency trend google book data set indicators true popularity various word phrase allow us draw quantitatively strong conclusions evolution cultural perception give topic time gender however google book corpus suffer number limitations make obscure mask cultural popularity primary issue corpus effect library contain one book single prolific author thereby able noticeably insert new phrase google book lexicon whether author widely read understand google book corpus remain important data set consider lexicon like text like show distinct problematic feature arise inclusion scientific texts become increasingly substantive portion corpus throughout 1900s result surge phrase typical academic article less common general reference time form citations highlight dynamics examine compare major contributions statistical divergence english data set decades period one thousand, eight hundred two thousand find english fiction data set second version corpus heavily affect professional texts clear contrast first version fiction data set unfiltered english data set find emphasize need fully characterize dynamics google book corpus use data set draw broad conclusions cultural linguistic evolution
topic model way discover underlie theme otherwise unstructured collection document study specifically use latent dirichlet allocation lda topic model dataset yelp review classify restaurants base review furthermore hypothesize within city restaurants group similar cluster base location similarity use several different cluster methods include k mean cluster probabilistic mixture model order uncover classify district well know hide ie cultural areas like chinatown hearsay like best street italian restaurants within city use model display label different cluster map also introduce topic similarity heatmap display similarity distribution city new restaurant
descriptive video service dvs provide linguistic descriptions movies allow visually impair people follow movie along peer descriptions design mainly visual thus naturally form interest data source computer vision computational linguistics work propose novel dataset contain transcribe dvs temporally align full length hd movies addition also collect align movie script use prior work compare two different source descriptions total movie description dataset contain parallel corpus fifty-four thousand sentence video snippets seventy-two hd movies characterize dataset benchmarking different approach generate video descriptions compare dvs script find dvs far visual describe precisely show rather happen accord script create prior movie production
extend skip gram model mikolov et al 2013a take visual information account like skip gram multimodal model mmskip gram build vector base word representations learn predict linguistic contexts text corpora however restrict set word model also expose visual representations object denote extract natural image must predict linguistic visual feature jointly mmskip gram model achieve good performance variety semantic benchmarks moreover since propagate visual information word use improve image label retrieval zero shoot setup test concepts never see model train finally mmskip gram model discover intrigue visual properties abstract word pave way realistic implementations embody theories mean
progress language image understand machine sparkle interest research community open end holistic task refuel old ai dream build intelligent machine discuss prominent challenge characterize holistic task argue question answer image particular appeal instance holistic task particular point version turing test likely robust interpretations contrast task like ground generation descriptions finally discuss tool measure progress field
apply traditional collaborative filter digital publish challenge user data sparse due high volume document relative number users content base approach hand attractive textual content often informative paper describe large scale content base collaborative filter digital publish solve digital publish recommender problem compare two approach latent dirichlet allocation lda deep belief net dbn find low dimensional latent representations document efficient retrieval carry latent representation work public benchmarks digital media content provide issuu online publish platform article also come newly develop deep belief net toolbox topic model tailor towards performance evaluation dbn model comparisons lda model
computer communication technologies provide effective mean scale many aspects education submission grade assessments homework assignments test remain weak link paper study problem automatically grade kinds open response mathematical question figure prominently stem science technology engineer mathematics course data drive framework mathematical language process mlp leverage solution data large number learners evaluate correctness solutions assign partial credit score provide feedback learner likely locations errors mlp take inspiration success natural language process text data comprise three main step first convert solution open response mathematical question series numerical feature second cluster feature several solutions uncover structure correct partially correct incorrect solutions develop two different cluster approach one leverage generic cluster algorithms one base bayesian nonparametrics third automatically grade remain potentially large number solutions base assign cluster one instructor provide grade per cluster bonus track cluster assignment step multistep solution determine depart cluster correct solutions enable us indicate likely locations errors learners test validate mlp real world mooc data demonstrate substantially reduce human effort require large scale educational platforms
web service allow communication heterogeneous systems distribute environment enormous success increase use lead fact thousands web service present internet significant number web service cease increase lead problems difficulty locate classify web service problems encounter mainly operations web service discovery substitution traditional ways search base keywords successful context result support structure web service consider search identifiers web service description language wsdl interface elements methods base semantics wsdls owls sawsdl increase wsdl description web service semantic description allow raise partially problem complexity difficulty delay adoption real case measure similarity web service interfaces suitable solution kind problems classify available web service know best match search profile match thus main goal work study degree similarity two web service offer new method effective exist work
importance research article routinely measure count many time cite however treat citations equal weight ignore wide variety function citations perform want automatically identify subset reference bibliography central academic influence cite paper purpose examine effectiveness variety feature determine academic influence citation ask author identify key reference work create data set citations label accord academic influence use automatic feature selection supervise machine learn find model predict academic influence achieve good performance data set use four feature best feature among evaluate base number time reference mention body cite paper performance feature inspire us design influence prim h index hip index unlike conventional h index weight citations many time reference mention accord experiment hip index better indicator researcher performance conventional h index
new algorithm voice automatic syllabic split portuguese language propose base envelope speech signal input audio file computational implementation matlabtm present make available url http www2eeufpebr codec divisaosilabicahtml due straightforwardness propose method attractive embed systems eg phone also use screen assist sophisticate methods voice excerpt contain one syllable identify envelope name super syllables subsequently separate result indicate sample correspond begin end detect syllable preliminary test perform fifty word identification rate circa seventy improvements may incorporate treat particular phonemes algorithm also useful voice command systems tool teach portuguese language even patients speech pathology
superior ability preserve sequence information time long short term memory lstm network type recurrent neural network complex computational unit obtain strong result variety sequence model task underlie lstm structure explore far linear chain however natural language exhibit syntactic properties would naturally combine word phrase introduce tree lstm generalization lstms tree structure network topologies tree lstms outperform exist systems strong lstm baselines two task predict semantic relatedness two sentence semeval two thousand and fourteen task one sentiment classification stanford sentiment treebank
study variation word frequencies russian literary texts find indicate standard deviation word frequency across texts depend average frequency accord power law exponent sixty-two show rarer word relatively larger degree frequency volatility ie burstiness several latent factor model estimate investigate structure word frequency distribution dependence word frequency volatility average frequency explain asymmetry distribution latent factor
apply machine learn problems nlp many choices make represent input texts choices big effect performance often uninteresting researchers practitioners simply need module perform well propose approach optimize space choices formulate problem global optimization apply sequential model base optimization technique show method make standard linear model competitive sophisticate expensive state art methods base latent variable model neural network various topic classification sentiment analysis problems approach first step towards black box nlp systems work raw text require manual tune
prior knowledge show useful address many natural language process task many approach propose formalise variety knowledge however whether propose approach robust sensitive knowledge supply model rarely discuss paper propose three regularization term top generalize expectation criteria conduct extensive experiment justify robustness propose methods experimental result demonstrate propose methods obtain remarkable improvements much robust baselines
case many signal produce complex systems language present statistical structure balance order disorder review extend recent result quantitative characterisations degree order linguistic sequence give insights two relevant aspects language presence statistical universals word order link semantic information statistical linguistic structure first analyse measure relative entropy assess much order word contribute overall statistical structure language measure present almost constant value close thirty-five bits word across several linguistic families show direct application information theory lead entropy measure quantify extract semantic structure linguistic sample even without prior knowledge underlie language
although analyze user behavior within individual communities active rich research domain people usually interact multiple communities line users act multi community environments although host intrigue aspects question receive much less attention research community comparison intra community case paper examine three aspects multi community engagement sequence communities users post language users employ communities feedback users receive use longitudinal post behavior reddit main data source dblp auxiliary experiment also demonstrate effectiveness feature draw aspects predict users future level activity one might expect user trajectory mimic settle process real life initial exploration sub communities settle niches however find users data continually post new communities moreover time go post increasingly evenly among diverse set smaller communities interestingly seem users eventually leave community destine begin sense show significantly different wander pattern early trajectories find potentially important design implications community maintainers multi community perspective also allow us investigate situation vs personality debate language usage across different communities
explore train automatic modality tagger modality attitude speaker might toward event state one main hurdle train linguistic tagger gather train data particularly problematic train tagger modality modality trigger sparse overwhelm majority sentence investigate approach automatically train modality tagger first gather sentence base high recall simple rule base modality tagger provide sentence mechanical turk annotators annotation use result set train data train precise modality tagger use multi class svm deliver good performance
many inference problems structure prediction naturally solve augment tractable dependency structure complex non local auxiliary objectives include mean field family variational inference algorithms soft hard constrain inference use lagrangian relaxation linear program collective graphical model form semi supervise learn posterior regularization present method discriminatively learn broad families inference objectives capture powerful non local statistics latent variables maintain tractable provably fast inference use non euclidean project gradient descent distance generate function give bethe entropy demonstrate performance flexibility method one extract structure citations research paper learn soft global constraints two achieve state art result widely use handwrite recognition task use novel learn non convex inference procedure three provide fast highly scalable algorithm challenge problem inference collective graphical model apply bird migration
present novel method align sequence instructions video someone carry task particular focus cook domain instructions correspond recipe technique rely hmm align recipe step automatically generate speech transcript refine alignment use state art visual food detector base deep convolutional neural network show technique outperform simpler techniques base keyword spot also enable interest applications automatically illustrate recipes keyframes search within video events interest
recently propose neural network joint model nnjm devlin et al two thousand and fourteen augment n gram target language model heuristically choose source context window achieve state art performance smt paper give systematic treatment summarize relevant source information convolutional architecture guide target information different guide signal decode specifically design convolutiongating architectures pinpoint part source sentence relevant predict target word fuse context entire source sentence form unify representation representation together target language word feed deep neural network dnn form stronger nnjm experiment two nist chinese english translation task show propose model achieve significant improvements previous nnjm one hundred and eight bleu point average
present bayesian approach adapt parameters well train context dependent deep neural network hide markov model cd dnn hmm improve automatic speech recognition performance give abundance dnn parameters limit amount data effectiveness adapt dnn model often compromise formulate maximum posteriori map adaptation parameters specially design cd dnn hmm augment linear hide network connect output tie state senones compare feature space map linear regression previously propose experimental evidence twenty thousand word open vocabulary wall street journal task demonstrate feasibility propose framework supervise adaptation propose map adaptation approach provide ten relative error reduction consistently outperform conventional transformation base methods furthermore present initial attempt generate hierarchical priors improve adaptation efficiency effectiveness limit adaptation data exploit similarities among senones
effort better understand mean natural language texts explore methods aim organize lexical object contexts number methods organization fall family define word order unlike demographic spatial partition data collocation model special importance universal applicability interest text frame treatment appropriately work potentially applicable areas research eg speech genomics mobility pattern one order categorical data eg sound genes locations approach focus phrase whether word larger primary mean bear lexical unit object study employ previously develop framework generate word conserve phrase frequency data upon train model wiktionary extensive online collaborative open source dictionary contain one hundred thousand phrasal definitions develop highly effective filter identification meaningful miss phrase entries predictions engage editorial community wiktionary propose short list potential miss entries definition develop breakthrough lexical extraction technique expand knowledge define english lexicon phrase
propose novel method translation selection statistical machine translation convolutional neural network employ judge similarity phrase pair two languages specifically design convolutional architecture encode semantic similarity translation pair also context contain phrase source language therefore approach able capture context dependent semantic similarities translation pair adopt curriculum learn strategy train model classify train examples easy medium difficult categories gradually build ability represent phrase sentence level context use train examples easy difficult experimental result show approach significantly outperform baseline system fourteen bleu point
propose neural respond machine nrm neural network base response generator short text conversation nrm take general encoder decoder framework formalize generation response decode process base latent representation input text encode decode realize recurrent neural network rnn nrm train large amount one round conversation data collect microblogging service empirical study show nrm generate grammatically correct content wise appropriate responses seventy-five input text outperform state arts set include retrieval base smt base model
many task natural language process range machine translation question answer reduce problem match two sentence generally two short texts propose new approach problem call deep match tree deepmatchtree general set approach consist two components one mine algorithm discover pattern match two short texts define product space dependency tree two deep neural network match short texts use mine pattern well learn algorithm build network sparse structure test algorithm problem match tweet response social media hard match problem propose wang et al two thousand and thirteen show deepmatchtree outperform number competitor model include one without use dependency tree one base word embed large margins
propose extension recursive neural network make use variant long short term memory architecture extension allow information low parse tree store memory register memory cell use much later higher parse tree provide solution vanish gradient problem allow network capture long range dependencies experimental result show composition outperform traditional neural network composition stanford sentiment treebank
semantic match central importance many natural language task citebordes2014semanticretrievalqa successful match algorithm need adequately model internal structure language object interaction step toward goal propose convolutional neural network model match two sentence adapt convolutional strategy vision speech propose model nicely represent hierarchical structure sentence layer layer composition pool also capture rich match pattern different level model rather generic require prior knowledge language hence apply match task different nature different languages empirical study variety match task demonstrate efficacy propose model variety match task superiority competitor model
article provide comprehensive investigation relations virality news article emotions find evoke virality view phenomenon many facets ie generic term several different effect persuasive communication comprise exploit high coverage bilingual corpus document contain metrics spread social network well massive affective annotation provide readers present thorough analysis interplay evoke emotions viral facets highlight discuss find light cross lingual approach discover differences evoke emotions correspond viral effect provide preliminary evidence generalize explanatory model root deep structure emotions valence arousal dominance vad circumplex find viral facets appear consistently affect particular vad configurations configurations indicate clear connection distinct phenomena underlie persuasive communication
chain structure long short term memory lstm show effective wide range problems speech recognition machine translation paper propose extend tree structure memory cell reflect history memories multiple child cells multiple descendant cells recursive process call model lstm provide principled way consider long distance interaction hierarchies eg language image parse structure leverage model semantic composition understand mean text fundamental problem natural language understand show outperform state art recursive model replace composition layer lstm memory block also show utilize give structure helpful achieve performance better without consider structure
paper describe novel approach categorize users review accord three quality use qu indicators define iso effectiveness efficiency freedom risk tremendous amount review publish day need automatically summarize user review inform us software able meet requirement company accord quality requirements implement method latent semantic analysis lsa subspace predict qu indicators build reduce dimensionality universal semantic space information system journals amazon review next project set indicators measurement scale universal semantic space represent subspace subspace map similar measurement scale unseen review predict qu indicators preliminary study able obtain average f measure three thousand, six hundred and twenty-seven
traditional relational topic model provide way discover hide topics document network many theoretical practical task dimensional reduction document cluster link prediction benefit reveal knowledge however exist relational topic model base assumption number hide topics know advance impractical many real world applications therefore order relax assumption propose nonparametric relational topic model paper instead use fix dimensional probability distributions generative model use stochastic process specifically gamma process assign document represent topic interest document although method provide elegant solution bring additional challenge mathematically model inherent network structure typical document network ie two spatially closer document tend similar topics furthermore require topics share document order resolve challenge use subsampling strategy assign document different gamma process global gamma process subsampling probabilities document assign markov random field constraint inherit document network structure design posterior inference algorithm discover hide topics number simultaneously experimental result synthetic real world network datasets demonstrate capabilities learn hide topics importantly number topics
lshtc series challenge aim assess performance classification systems large scale classification large number class hundreds thousands paper describe dataset release along lshtc series paper detail construction datsets design track well evaluation measure implement quick overview result datasets available online run may still submit online server challenge
whereas deep neural network first mostly use classification task rapidly expand realm structure output problems observe target compose multiple random variables rich joint distribution give input focus paper case input also rich structure input output structure somehow relate describe systems learn attend different place input element output variety task machine translation image caption generation video clip description speech recognition systems base share set build block gate recurrent neural network convolutional neural network along train attention mechanisms report experimental result systems show impressively good performance advantage attention mechanism
recent work language model shift focus count base model neural model work word sentence always consider leave right order paper show improve performance recurrent neural network rnn language model incorporate syntactic dependencies sentence effect bring relevant contexts closer word predict evaluate approach microsoft research sentence completion challenge show dependency rnn propose improve rnn ten point accuracy furthermore achieve result comparable state art model task
paper introduce grid long short term memory network lstm cells arrange multidimensional grid apply vectors sequence higher dimensional data image network differ exist deep lstm architectures cells connect network layer well along spatiotemporal dimension data network provide unify way use lstm deep sequential computation apply model algorithmic task fifteen digit integer addition sequence memorization able significantly outperform standard lstm give result two empirical task find 2d grid lstm achieve one hundred and forty-seven bits per character wikipedia character prediction benchmark state art among neural approach addition use grid lstm define novel two dimensional translation model reencoder show outperform phrase base reference system chinese english translation task
sentence model classification convolutional neural network approach recently achieve state art result efforts process word vectors sequentially neglect long distance dependencies exploit deep learn linguistic structure propose tree base convolutional neural network model exploit various long distance relationships word model improve sequential baselines three sentiment question classification task achieve highest publish accuracy trec
commonplace observe face deluge online information researchers course long acknowledge potential value information since digital trace make possible directly observe describe analyze social facts co evolution ideas communities time however online information express text mean directly usable machine since computers require structure organize type information order able manipulate goal thus twofold one provide new natural language process techniques aim automatically extract relevant information texts especially context social sciences connect piece information obtain relevant socio semantic network two provide new ways explore socio semantic network thank tool allow one dynamically navigate network de construct construct interactively different point view follow need express domain experts
future work scientific article valuable researchers guide researchers new research directions ideas paper mine future work scientific article order one provide insight future work analysis two facilitate researchers search browse future work research area first study problem future work extraction propose regular expression base method address problem second define four different categories future work observe data investigate multi class future work classification problem third apply extraction method classification model paper dataset computer science field conduct analysis future work finally design prototype system search demonstrate future work mine scientific paper evaluation result show extraction method get high precision recall value classification model also get good result outperform several baseline model analysis future work sentence also indicate interest result
classify twenty one indo european languages start write text use neural network order define distance among different languages construct dendrogram analyze ultrametric structure emerge four five subgroups languages identify accord cut dendrogram draw entropic criterion result method discuss
previous research relation classification verify effectiveness use dependency shortest paths subtrees paper explore make full use combination dependency information first propose new structure term augment dependency path adp compose shortest dependency path two entities subtrees attach shortest path exploit semantic representation behind adp structure develop dependency base neural network depnn recursive neural network design model subtrees convolutional neural network capture important feature shortest path experiment semeval two thousand and ten dataset show propose method achieve state art result
continue collect store textual data multitude domains regularly confront material whose largely unknown thematic structure want uncover unsupervised exploratory analysis prior knowledge content require highly open end task support past years probabilistic topic model emerge popular approach problem nevertheless representation latent topics aggregations semi coherent term limit interpretability level detail paper present alternative approach topic model map topics network exploration base distributional semantics use learn word vectors granular level term semantic similarity relations global topic structure emerge cluster regions gradients concepts moreover paper discuss visual interactive representation topic map play important role support exploration
investigate task build open domain conversational dialogue systems base large dialogue corpora use generative model generative model produce system responses autonomously generate word word open possibility realistic flexible interactions support goal extend recently propose hierarchical recurrent encoder decoder neural network dialogue domain demonstrate model competitive state art neural language model back n gram model investigate limitations similar approach show performance improve bootstrapping learn larger question answer pair corpus pretrained word embeddings
efficient maximum inner product search mips important task wide applicability recommendation systems classification large number class solutions base locality sensitive hash lsh well tree base solutions investigate recent literature perform approximate mips sublinear time paper compare another extremely simple approach solve approximate mips base variants k mean cluster algorithm specifically propose train spherical k mean reduce mips problem maximum cosine similarity search mcss experiment two standard recommendation system benchmarks well large vocabulary word embeddings show simple approach yield much higher speedups retrieval precision current state art hash base tree base methods simple method also yield robust retrievals query corrupt noise
paper present polylingual label topic model model combine characteristics exist polylingual topic model label lda model account multiple languages separate topic distributions language restrict permit topics document set predefined label explore properties model two language set dataset social science domain experiment show model outperform lda label lda term hold perplexity produce semantically coherent topics well interpretable human subject
recently show deep long short term memory lstm recurrent neural network rnns outperform fee forward deep neural network dnns acoustic model speech recognition recently show performance sequence train context dependent cd hide markov model hmm acoustic model use lstm rnns equal sequence train phone model initialize connectionist temporal classification ctc paper present techniques improve performance lstm rnn acoustic model large vocabulary speech recognition show frame stack reduce frame rate lead accurate model faster decode cd phone model lead improvements also present initial result lstm rnn model output word directly
paragraph vectors recently propose unsupervised method learn distribute representations piece texts work author show method learn embed movie review texts leverage sentiment analysis proof concept encourage rather narrow consider task sentiment analysis provide thorough comparison paragraph vectors document model algorithms latent dirichlet allocation evaluate performance method vary dimensionality learn representation benchmarked model two document similarity data set one wikipedia one arxiv observe paragraph vector method perform significantly better methods propose simple improvement enhance embed quality somewhat surprisingly also show much like word embeddings vector operations paragraph vectors perform useful semantic result
date massive semi structure document ssds evolution internet ssds contain unstructured feature eg plain text metadata eg tag previous work focus model unstructured text recently methods propose model unstructured text specific tag build general model ssds remain important problem term model fitness efficiency propose novel method model ssds call tag weight topic model twtm twtm framework leverage tag word information learn document topic topic word distributions also infer tag topic distributions text mine task present efficient variational inference method algorithm estimate model parameters meanwhile propose three large scale solutions model mapreduce distribute compute platform model large scale ssds experimental result show effectiveness efficiency robustness compare model state art methods document model tag prediction text classification also show performance three distribute solutions term time accuracy document model
characterize relationships people fundamental understand narratives work address problem infer polarity relationships people narrative summaries formulate problem joint structure prediction narrative present model combine evidence linguistic semantic feature well feature base structure social community text also provide cluster base approach exploit regularities narrative type eg learn affinity love triangles romantic stories dataset movie summaries wikipedia structure model provide thirty error reduction competitive baseline consider pair character isolation
artificial neural network powerful model widely apply many aspects machine translation language model translation model though notable improvements make areas reorder problem still remain challenge statistical machine translations paper present novel neural reorder model directly model word pair alignment utilize lstm recurrent neural network much longer context could learn reorder prediction experimental result nist openmt12 arabic english chinese english one thousand best rescoring task show lstm neural reorder feature robust achieve significant improvements various baseline systems
propose new zero shoot event detection method multi modal distributional semantic embed videos model embed object action concepts well available modalities videos distributional semantic space knowledge first zero shoot event detection model build top distributional semantics extend follow directions semantic embed multimodal information videos focus visual modalities b automatically determine relevance concepts attribute free text query could useful applications c retrieve videos free text event query eg change vehicle tire base content embed videos distributional semantic space measure similarity videos event query free text form validate method large trecvid med multimedia event detection challenge use event title query method outperform state art use big descriptions one hundred and twenty-six one hundred and thirty-five map metric seventy-three eighty-three roc auc metric also order magnitude faster
propose neural enquirer neural network architecture execute natural language nl query knowledge base kb answer basically neural enquirer find distribute representation query execute knowledge base table obtain answer one value table unlike similar efforts end end train semantic parsers neural enquirer fully neuralized give distributional representation query knowledge base also realize execution compositional query series differentiable operations intermediate result consist annotations table different level save multiple layer memory neural enquirer train gradient descent parameters control components semantic parse component also embeddings table query word learn scratch train do end end fashion take stronger guidance eg step step supervision complicate query benefit neural enquirer one step towards build neural network systems seek understand language execute real world experiment show neural enquirer learn execute fairly complicate nl query table rich structure
microbloging extremely prevalent broadcast medium amidst internet fraternity days people share opinions sentiments variety subject like products news institutions etc every day microbloging websites sentiment analysis play key role prediction systems opinion mine systems etc twitter one microbloging platforms allow limit one hundred and forty character users restriction stimulate users concise opinion twitter ocean sentiments analyze twitter also provide developer friendly stream api data retrieval purpose allow analyst search real time tweet various users paper discuss state art work focus twitter online social network platform sentiment analysis survey various lexical machine learn hybrid approach sentiment analysis twitter
present new perspective neural knowledge base kb embeddings build framework model symbolic knowledge kb together learn process show framework well regularize previous neural kb embed model superior performance reason task capabilities deal unseen entities learn embeddings natural language descriptions like human behavior learn semantic concepts
paper present formal computational framework model manipulation action introduce formalism lead semantics manipulation action applications observe understand human manipulation action well execute robotic mechanism eg humanoid robot base combinatory categorial grammar goal introduce framework one represent manipulation action syntax semantic part semantic part employ lambda calculus two enable probabilistic semantic parse schema learn lambda calculus representation manipulation action annotate action corpus videos three use one two develop system visually observe manipulation action understand mean reason beyond observations use propositional logic axiom schemata experiment conduct public available large manipulation action dataset validate theoretical framework implementation
describe application encoder decoder recurrent neural network lstm units attention generate headline text news article find model quite effective concisely paraphrase news article furthermore study neural network decide input word pay attention specifically identify function different neurons simplify attention mechanism interestingly simplify attention mechanism perform better complex attention mechanism hold set article
exist theory single general purpose learn algorithm could explain principles operation assume initial rough architecture small library simple innate circuit prewired birth propose significant mental algorithms learn give current understand observations paper review list ingredients algorithm architectural functional perspectives
model document structure great importance discourse analysis relate applications goal research capture document intent structure model document mixture topic word rhetorical word topics relatively unchanged one document rhetorical function sentence usually change follow certain order discourse propose gmm lda topic model base bayesian unsupervised model analyze document intent structure cooperate order information model flexible ability combine annotations supervise learn additionally entropic regularization introduce model significant divergence topics intents perform experiment unsupervised supervise settings result show superiority model several state art baselines
distribute adaptive algorithm estimation sparse unknown parameters presence nongaussian noise propose paper base normalize least mean fourth nlmf criterion first step local adaptive nlmf algorithm modify zero norm order speed convergence rate also reduce steady state error power sparse condition propose algorithm extend distribute scenario improvement estimation performance achieve due cooperation local adaptive filter simulation result show superiority propose algorithm comparison conventional nlmf algorithms
paper consider problem continuously discover image content actively ask image base question subsequently answer question ask key components include visual question generation vqg module visual question answer module recurrent neural network rnn convolutional neural network cnn use give dataset contain image question answer modules train time difference vqg use image input correspond question output vqa use image question input correspond answer output evaluate self talk process subjectively use amazon mechanical turk show effectiveness propose method
despite success distributional semantics compose phrase word vectors remain important challenge several methods try benchmark task sentiment classification include word vector average matrix vector approach base parse fly learn paragraph vectors model usually omit stop word composition instead yes decision consider several grade scheme word weight accord discriminatory relevance respect use document eg idf methods particularly tf idf see result significant improvement performance prior state art combine approach ensemble base alternate classifiers rnn model result sixteen performance improvement standard imdb movie review dataset seven hundred and one improvement amazon product review since language free model obtain unsupervised manner interest also resourced languages hindi well many languages demonstrate language free aspects show gain twelve two review datasets earlier result also release new larger dataset future test singh2015
speech recognition deep neural network dnns significantly improve recognition accuracy benchmark datasets application domains however compare conventional gaussian mixture model dnn base acoustic model usually much larger number model parameters make challenge applications resource constrain platforms eg mobile devices paper study application recently propose highway network train small footprint dnns thinner deeper significantly smaller number model parameters compare conventional dnns investigate approach ami meet speech transcription corpus around seventy hours audio data highway neural network constantly outperform plain dnn counterparts number model parameters reduce significantly without sacrifice recognition accuracy
humor integral part human live despite tremendously impactful perhaps surprise detail understand humor yet interactions humans ai systems increase imperative systems teach understand subtleties human expressions humor work interest question content scene cause funny first step towards understand visual humor analyze humor manifest abstract scenes design computational model collect two datasets abstract scenes facilitate study humor scene level object level analyze funny scenes explore different type humor depict via human study model two task believe demonstrate understand aspects visual humor task involve predict funniness scene alter funniness scene show model perform well quantitatively qualitatively human study datasets publicly available
distributional semantic model provide vector representations word gather co occurrence frequencies corpora text compositional distributional model extend word phrase sentence categorical compositional distributional semantics phrase sentence representations function grammatical structure representations word therein set grammatical structure formalise morphisms compact close category mean word formalise object category instantiate form vectors density matrices paper concern applications model phrase sentence level entailment argue entropy base distance vectors density matrices provide good candidate measure word level entailment show advantage density matrices vectors word level entailments prove distance extend compositionally word phrase sentence exemplify theoretical constructions real data toy entailment dataset provide preliminary experimental evidence
paper aim develop method automatically detect track topics broadcast news present hierarchical graph aog jointly represent latent structure texts visuals aog embed context sensitive grammar describe hierarchical composition news topics semantic elements people involve relate place happen model contextual relationships elements hierarchy detect news topics cluster sample process group stories closely relate events swendsen wang cut swc effective cluster sample algorithm adopt traverse solution space obtain optimal cluster solutions maximize bayesian posterior probability topics track deal continuously update news stream generate topic trajectories show topics emerge evolve disappear time experimental result show method explicitly describe textual visual data news videos produce meaningful topic trajectories method achieve superior performance compare state art methods public dataset reuters twenty-one thousand, five hundred and seventy-eight self collect dataset name ucla broadcast news dataset
cultural scale model full text document prone interpretation researchers make unintentionally strong socio linguistic claim pechenick et al two thousand and fifteen without recognize even large digital libraries merely sample book ever produce study test sensitivity topic model sample process take random sample book hathi trust digital library different areas library congress classification outline classification area train several topic model entire class different random seed generate set span model train topic model random sample book classification area generate set sample model finally perform topic alignment pair model compute jensen shannon distance jsd word probability distributions topic take two measure model alignment alignment distance topic overlap find sample model large sample size typically alignment distance fall range alignment distance span model unsurprisingly sample size increase alignment distance decrease also find topic overlap increase sample size increase however decomposition measure sample size differ number topics classification area speculate measure could use find class common canon discuss among book area show high topic overlap low alignment distance even small sample size
recent language model especially base recurrent neural network rnns make possible generate natural language learn probability language generation wide applications include machine translation summarization question answer conversation systems etc exist methods typically learn joint probability word condition additional information either statically dynamically feed rnn hide layer many applications likely impose hard constraints generate texts ie particular word must appear sentence unfortunately exist approach could solve problem paper propose novel backward forward language model provide specific word use rnns generate previous word future word either simultaneously asynchronously result two model variants way give word could appear position sentence experimental result show generate texts comparable sequential lms quality
sina weibo china popular microblogging platform currently use 500m users consider proxy chinese social life study contrast discussions occur sina weibo chinese language twitter order observe two different strand chinese culture people within china use sina weibo government impose restrictions outside free speak completely anonymously first propose simple ad hoc algorithm identify topics tweet weibo different previous work micro message topic detection algorithm consider topics content different tag algorithm also detect topics tweet weibos without tag use large corpus weibo chinese language tweet cover period january one december thirty-one two thousand and twelve obtain list topics use cluster tag use compare two platforms surprisingly find common entries among top one hundred popular topics furthermore ninety-two tweet correspond top one thousand topics sina weibo platform conversely forty-four weibos find discuss popular twitter topics result reveal significant differences social attention two platforms popular topics sina weibo relate entertainment tweet correspond cultural political content practically non existent sina weibo
natural language inference nli fundamentally important task natural language process many applications recently release stanford natural language inference snli corpus make possible develop evaluate learn center methods deep neural network natural language inference nli paper propose special long short term memory lstm architecture nli model build top recently propose neural attention model nli base significantly different idea instead derive sentence embeddings premise hypothesis use classification solution use match lstm perform word word match hypothesis premise lstm able place emphasis important word level match result particular observe lstm remember important mismatch critical predict contradiction neutral relationship label snli corpus model achieve accuracy eight hundred and sixty-one outperform state art
paper propose context aware keyword spot model employ character level recurrent neural network rnn speak term detection continuous speech rnn end end train connectionist temporal classification ctc generate probabilities character word boundary label need phonetic transcription senone model system dictionary train test also keywords easily add modify edit text base keyword list without retrain rnn moreover unidirectional rnn process infinitely long input audio stream without pre segmentation keywords detect low latency utterance finish experimental result show propose keyword spotter significantly outperform deep neural network dnn hide markov model hmm base keyword filler model even less computations
several domains linguistics molecular biology social sciences holistic effect hardly well define model single units study tend understand macro structure help meaningful useful associations field social network systems biology semantic web stochastic multi agent system offer accurate theoretical framework operational compute implementations model large scale associations dynamics pattern extraction show cluster around target object set associations object prove similarity specific data two case study gene gene term term relationships lead idea common organize principle cognition random deterministic effect
neural language model powerful tool embed word semantic vector space however learn model generally rely availability abundant diverse train examples highly specialise domains requirement may meet due difficulties obtain large corpus limit range expression average use domains may encode prior knowledge entities knowledge base ontology propose generative model integrate evidence diverse data source enable share semantic information achieve generalise concept co occurrence distributional semantics include relationships entities word model affine transformations embed space demonstrate effectiveness approach outperform recent model link prediction task demonstrate ability profit partially fully unobserved data train label demonstrate usefulness learn different data source overlap vocabularies
quantify similarity symbolic sequence traditional problem information theory require compare frequencies symbols different sequence numerous modern applications range dna music texts distribution symbol frequencies characterize heavy tail distributions eg zipf law large number low frequency symbols distributions pose major difficulties estimation similarity sequence eg hinder accurate finite size estimation entropies show analytically systematic bias statistical fluctuations errors estimations depend sample sizen exponentgamma heavy tail distribution result valid shannon entropy alpha1 correspond similarity measure eg jensen shanon divergence also measure base generalize entropy order alpha small alpha include alpha1 errors decay slower one n decay observe short tail distributions alpha larger critical value alpha eleven gamma leq two one n decay recover show practical significance result quantify evolution english language last two centuries use complete alpha spectrum measure find frequent word change slowly less frequent word alpha2 provide robust measure quantify language change
evaluate impact change observation scale entropy measure text descriptions midi cod music computer code two human natural languages study scale character word fundamental scale result adjust symbols length use interpret text description produce minimum entropy result show fundamental scale method comparable use word measure entropy level write texts however method also use communication systems lack word music measure symbolic entropy fundamental scale allow calculate quantitatively relative level complexity different communication systems result open novel vision differences among structure communication systems study
introduce new structure memory neural network call feedforward sequential memory network fsmn learn long term dependency without use recurrent feedback propose fsmn standard feedforward neural network equip learnable sequential memory block hide layer work apply fsmn several language model lm task experimental result show memory block fsmn learn effective representations long history experiment show fsmn base language model significantly outperform feedforward neural network fnn base lms also popular recurrent neural network rnn lms
study first exploratory attempt use quantitative semantics techniques topological analysis analyze systemic pattern arise complex political system particular use rich data set cover speeches debate uk house commons one thousand, nine hundred and seventy-five two thousand and fourteen use dynamic topic model dtm topological data analysis tda show members party feature specific roles within system consistent time extract global pattern indicate level political cohesion result provide wide array novel hypotheses complex dynamics political systems valuable policy applications
convolutional neural network cnns recently achieve remarkably strong performance practically important task sentence classification kim two thousand and fourteen kalchbrenner two thousand and fourteen johnson two thousand and fourteen however model require practitioners specify exact model architecture set accompany hyperparameters include filter region size regularization parameters currently unknown sensitive model performance change configurations task sentence classification thus conduct sensitivity analysis one layer cnns explore effect architecture components model performance aim distinguish important comparatively inconsequential design decisions sentence classification focus one layer cnns exclusion complex model due comparative simplicity strong empirical performance make modern standard baseline method akin support vector machine svms logistic regression derive practical advice extensive empirical result interest get cnns sentence classification real world settings
paper present approach multi language image description bring together insights neural machine translation neural image description create description image give target language sequence generation model condition feature vectors image description source language multimodal vector compute image description source language image description experiment iapr tc12 dataset image align english german sentence find significant substantial improvements bleu4 meteor score model train multiple languages compare monolingual baseline
analyze ri timexes temporally annotate corpora propose two hypotheses regard normalization ri timexes clinical narrative domain anchor point hypothesis anchor relation hypothesis annotate ri timexes three corpora study characteristics ri tmexes different domains inform design ri timex normalization system clinical domain consist anchor point classifier anchor relation classifier rule base ri timex text span parser experiment different feature set perform error analysis system component annotation confirm hypotheses simplify ri timexes normalization task use two multi label classifiers system achieve anchor point classification anchor relation classification rule base parse accuracy seven thousand, four hundred and sixty-eight eight thousand, seven hundred and seventy-one five hundred and seventy-two eight thousand, two hundred and nine relax match criteria respectively hold test set two thousand and twelve i2b2 temporal relation challenge experiment feature set reveal interest find verbal tense feature inform anchor relation classification clinical narratives much tokens near ri timex error analysis show underrepresented anchor point anchor relation class difficult detect formulate ri timex normalization problem pair multi label classification problems consider ri timex extraction normalization system achieve statistically significant improvement ri timex result best systems two thousand and twelve i2b2 challenge
propose new method base sparse distribute memory kanerva network study dependency relations different syntactic parameters principles parameters model syntax store data syntactic parameters world languages kanerva network check recoverability corrupt parameter data network find different syntactic parameters different degrees recoverability identify two different effect overall underlie relation prevalence parameters across languages degree recoverability finer effect make parameters easily recoverable beyond prevalence would indicate interpret higher recoverability syntactic parameter indication existence dependency relation give parameter determine use remain uncorrupted data
explosion data document content people require tool analyze interpret tool turn content information knowledge topic model develop solve problems topic model lda blei et al two thousand and three allow salient pattern data extract automatically analyze texts pattern call topics among numerous extensions lda reliably analyze multiple group document extract topic similarities recently introduction differential topic model spdp chen et al two thousand and twelve perform uniformly better many topic model discriminative set also need improve sample speed topic model effort make distribute algorithms work currently do use graphical process units gpu note gpu framework already become cost efficient platform many problems thesis propose implement scalable multi gpu distribute parallel framework approximate spdp experiment show algorithms gain speed fifty time almost accurate one single cheap laptop gpu furthermore show speed improvement sublinearly scalable multiple gpus use fairly maintain accuracy therefore medium size gpu cluster speed improvement could potentially reach factor thousand note spdp representative extensions lda although algorithm implement work spdp design general enough work topic model speed smaller collections ie 1000s document mean complex lda extensions could do real time thus open new way use lda model industry
hyper parameters play major role learn inference process latent dirichlet allocation lda order begin lda latent variables learn process hyper parameters value need pre determine propose extension lda call latent dirichlet allocation gibbs newton lda gn place non informative priors hyper parameters use gibbs sample learn appropriate value heart lda gn propose gibbs newton algorithm new technique learn parameters multivariate polya distributions report gibbs newton performance result compare two prominent exist approach latter task minka fix point iteration method moments method evaluate lda gn two ways compare standard lda term ability result topic model generalize unseen document ii compare standard lda performance binary classification task
present new computational technique detect analyze statistically significant geographic variation language meta analysis approach capture statistical properties word usage across geographical regions use statistical methods identify significant change specific regions previous approach primarily focus lexical variation regions method identify word demonstrate semantic syntactic variation well extend recently develop techniques neural language model learn word representations capture differ semantics across geographical regions order quantify variation ensure robust detection true regional differences formulate null model determine whether observe change statistically significant method first approach explicitly account random variation due chance detect regional variation word mean validate model study analyze two different massive online data set millions tweet twitter span four different countries also fifty state well millions phrase contain google book ngrams analysis reveal interest facets language change multiple scale geographic resolution neighbor state distant continents finally use model propose measure semantic distance languages analysis british american english period one hundred years reveal semantic variation dialects shrink
project outline vedalia high performance distribute network perform inference latent variable model context amazon review visualization introduce new model rlda extend latent dirichlet allocation lda blei et al two thousand and three review space incorporate auxiliary data available online review improve model simultaneously remain compatible pre exist fast sample techniques yao et al two thousand and nine li et al 2014a achieve high performance network design computation efficiently offload client devices use chital system robinson li two thousand and fifteen improve response time reduce server cost result system able rapidly compute large number specialize latent variable model require minimal server resources
paper explore deep learn model memory component attention mechanism question answer task combine compare three model neural machine translation neural turing machine memory network simulate qa data set paper first one use neural machine translation neural turing machine solve qa task result suggest combination attention memory potential solve certain qa problem
paper investigate use prediction adaptation correction recurrent neural network pac rnns low resource speech recognition pac rnn comprise pair neural network correction network use auxiliary information give prediction network help estimate state probability information correction network also use prediction network recurrent loop model outperform state art neural network dnns lstms iarpa babel task moreover transfer learn language similar target language help improve performance
introduce novel schema sequence sequence learn deep q network dqn decode output sequence iteratively aim enable decoder first tackle easier portion sequence turn cope difficult part specifically iteration encoder decoder long short term memory lstm network employ input sequence automatically create feature represent internal state formulate list potential action dqn take rephrase natural sentence example list contain rank potential word next dqn learn make decision action eg word select list modify current decode sequence newly modify output sequence subsequently use input dqn next decode iteration iteration also bias reinforcement learn attention explore sequence portion previously difficult decode evaluation propose strategy train decode ten thousands natural sentence experiment indicate compare leave right greedy beam search lstm decoder propose method perform competitively well decode sentence train set significantly outperform baseline decode unseen sentence term bleu score obtain
present approach detection coordinate term relationships entities software domain refer java class usually relations find examine corpus statistics associate text entities technical domains however access additional information real world object name entities suggest couple information ground entities corpus statistics might lead improve methods relation discovery end develop similarity measure java class use distributional information use software combine corpus statistics distribution contexts class appear text use approach cross validation accuracy dataset improve dramatically around sixty eighty-eight human label result show classifier f1 score eighty-six top one thousand predict pair
frdcqa team participate qa lab english subtask ntcir eleven paper describe system solve real world university entrance exam question relate world history wikipedia use main external resource system since problems choose right wrong sentence multiple sentence choices account two thirds total individually design classification base model solve type question type question also design simple methods
address question answer task real world image set visual turing test combine latest advance image representation natural language process propose neural image qa end end formulation problem part train jointly contrast previous efforts face multi modal problem language output answer condition visual natural language input image question approach neural image qa double performance previous best approach problem provide additional insights problem analyze much information contain language part provide new human baseline study human consensus relate ambiguities inherent challenge task propose two novel metrics collect additional answer extend original daquar dataset daquar consensus
paper propose new fix size ordinally forget encode fofe method almost uniquely encode variable length sequence word fix size representation fofe model word order sequence use simple ordinally forget mechanism accord position word work apply fofe feedforward neural network language model fnn lms experimental result show without use recurrent feedbacks fofe base fnn lms significantly outperform standard fix input fnn lms also popular rnn lms
two recent approach achieve state art result image caption first use pipelined process set candidate word generate convolutional neural network cnn train image maximum entropy language model use arrange word coherent sentence second use penultimate activation layer cnn input recurrent neural network rnn generate caption sequence paper compare merit different language model approach first time use state art cnn input examine issue different approach include linguistic irregularities caption repetition data set overlap combine key aspects rnn methods achieve new record performance previously publish result benchmark coco dataset however gain see bleu translate human judgments
work aim address problem image base question answer qa new model datasets work propose use neural network visual semantic embeddings without intermediate stag object detection image segmentation predict answer simple question image model perform eighteen time better publish result exist image qa dataset also present question generation algorithm convert image descriptions widely available qa form use algorithm produce order magnitude larger dataset evenly distribute answer suite baseline result new dataset also present
hierarchies frequently use organization object give hierarchy class two main approach use automatically classify new instance flat classification cascade classification flat classification ignore hierarchy cascade classification greedily traverse hierarchy root predict leaf paper propose new approach extend cascade classification predict right leaf estimate probability root leaf path provide experimental result indicate use classification algorithm one achieve better result approach compare traditional flat cascade classifications
compositional embed model build representation embed linguistic structure base component word embeddings propose feature rich compositional embed model fcm relation extraction expressive generalize new domains easy implement key idea combine unlexicalized hand craft feature learn word embeddings model able directly tackle difficulties meet traditional compositional embeddings model handle arbitrary type sentence annotations utilize global information composition test propose model two relation extraction task demonstrate model outperform previous compositional model traditional feature rich model ace two thousand and five relation extraction task semeval two thousand and ten relation classification task combination model log linear classifier hand craft feature give state art result
work extend set work deal popular problem sentiment analysis twitter investigate popular document tweet representation methods fee sentiment evaluation mechanisms particular study bag word n grams n gram graph approach evaluate performance lexicon base seven learn base classification algorithms namely svm nai bayesian network logistic regression multilayer perceptrons best first tree functional tree c45 well combinations use set four thousand, four hundred and fifty-one manually annotate tweet result demonstrate superiority learn base methods particular n gram graph approach predict sentiment tweet also show combinatory approach impressive effect n grams raise confidence eight thousand, three hundred and fifteen five grams use majority vote balance dataset equal number positive negative neutral tweet train n gram graph case improvement small none reach nine thousand, four hundred and fifty-two four gram graph use orthodromic distance threshold one
recurrent neural network rnns particularly long short term memory lstm gain much attention automatic speech recognition asr although successful stories report train rnns remain highly challenge especially limit train data recent research find well train model use teacher train child model use predictions generate teacher model supervision knowledge transfer learn employ train simple neural net complex one final performance reach level infeasible obtain regular train paper employ knowledge transfer learn approach train rnns precisely lstm use deep neural network dnn model teacher different exist research knowledge transfer learn since teacher dnn assume weaker child rnn however experiment asr task show work fairly well without apply trick learn scheme approach train rnns successfully even limit train data
write rap lyric require creativity construct meaningful interest story lyrical skills produce complex rhyme pattern form cornerstone good flow present rap lyric generation method capture aspects first develop prediction model identify next line exist lyric set candidate next line model base two machine learn techniques ranksvm algorithm deep neural network model novel structure result show prediction model identify true next line among two hundred and ninety-nine randomly select line accuracy seventeen ie fifty time likely random second employ prediction model combine line exist songs produce lyric rhyme mean evaluation produce lyric show term quantitative rhyme density method outperform best human rappers twenty-one rap lyric generator deploy online tool call deepbeat performance tool assess analyze usage log analysis show machine learn rank correlate user preferences
paper present mqa model able answer question content image answer sentence phrase single word model contain four components long short term memory lstm extract question representation convolutional neural network cnn extract visual representation lstm store linguistic context answer fuse component combine information first three components generate answer construct freestyle multilingual image question answer fm iqa dataset train evaluate mqa model contain one hundred and fifty thousand image three hundred and ten thousand freestyle chinese question answer pair english translations quality generate answer mqa model dataset evaluate human judge turing test specifically mix answer provide humans model human judge need distinguish model human also provide score ie zero one two larger better indicate quality answer propose strategies monitor quality evaluation process experiment show six hundred and forty-seven case human judge distinguish model humans average score one thousand, four hundred and fifty-four one thousand, nine hundred and eighteen human detail work include fm iqa dataset find project page http idlbaiducom fm iqahtml
work address problem model nod word phrase dependency tree dense representations propose recursive convolutional neural network rcnn architecture capture syntactic compositional semantic representations phrase word dependency tree different original recursive neural network introduce convolution pool layer model variety compositions feature map choose informative compositions pool layer base rcnn use discriminative model rank k best list candidate dependency parse tree experiment show rcnn effective improve state art dependency parse english chinese datasets
categorical compositional distributional model coecke sadrzadeh clark provide linguistically motivate procedure compute mean sentence function distributional mean word therein theoretical framework allow reason compositional aspects language offer structural ways study underlie relationships model far apply level syntactic structure sentence bring extra information convey utterances via intonational mean current paper extend framework order accommodate additional information use frobenius algebraic structure canonically induce basis finite dimensional vector space detail theory provide truth theoretic distributional semantics mean intonationally mark utterances present justifications extensive examples
recent research show deep neural network dnns use extract deep speaker vectors vectors preserve speaker characteristics use speaker verification new method test text dependent speaker verification task improvement report combine conventional vector method paper extend vector approach semi text independent speaker verification task ie text speech limit set short phrase explore various settings dnn structure use vector extraction present phone dependent train employ posterior feature obtain asr system experimental result show possible apply vectors semi text independent speaker recognition phone dependent train improve system performance
theory agents come understand language present understand sentence alpha associate operator alpha transform representational state agent intend sender come know language involve come know operators correspond mean sentence involve higher order operator operate possible transformations operate representational capacity agent formalize construct use concepts diagram analogous category theory
intelligence quotient iq test set standardize question design evaluate human intelligence verbal comprehension question appear frequently iq test measure human verbal ability include understand word multiple sense synonyms antonyms analogies among word work explore whether test solve automatically artificial intelligence technologies especially deep learn technologies recently develop successfully apply number field however find task quite challenge simply apply exist technologies eg word embed could achieve good performance mainly due multiple sense word complex relations among word tackle challenge propose novel framework consist three components first build classifier recognize specific type verbal question eg analogy classification synonym antonym second obtain distribute representations word relations leverage novel word embed method consider multi sense nature word relational knowledge among word sense contain dictionaries third type question propose specific solver base obtain distribute word representations relation representations experimental result show propose framework outperform exist methods solve verbal comprehension question also exceed average performance amazon mechanical turk workers involve study result indicate appropriate use deep learn technologies might step closer human intelligence
propose technique learn representations parser state transition base dependency parsers primary innovation new control structure sequence sequence neural network stack lstm like conventional stack data structure use transition base parse elements push pop top stack constant time addition lstm maintain continuous space embed stack content let us us formulate efficient parse model capture three facets parser state unbounded look ahead buffer incoming word ii complete history action take parser iii complete content stack partially build tree fragment include internal structure standard backpropagation techniques use train yield state art parse performance
speech enhancement algorithms make use short time fourier transform stft simple flexible time frequency decomposition estimate short time spectrum signal however duration short stft frame inherently limit nonstationarity speech signal main contribution paper demonstration speech enhancement automatic speech recognition presence reverberation noise extend length analysis windows accomplish extension perform enhancement short time fan chirp transform stfcht domain overcomplete time frequency representation coherent speech signal longer analysis window durations stft extend coherence gain use linear model fundamental frequency variation voice speech signal approach center around use single channel minimum mean square error log spectral amplitude mmse lsa estimator propose habets scale coefficients time frequency domain suppress noise reverberation case multiple microphones preprocess data either minimum variance distortionless response mvdr beamformer delay sum beamformer dsb evaluate algorithm speech enhancement recognition task reverb challenge dataset compare process do stft domain approach achieve significant improvement term objective enhancement metrics include pesq itu standard measurement speech quality term automatic speech recognition asr performance measure word error rate wer experiment indicate stft long window effective asr
propose end end domain independent neural encoder aligner decoder model selective generation ie joint task content selection surface realization model first encode full set determine database event record via lstm base recurrent neural network utilize novel coarse fine aligner identify small subset salient record talk finally employ decoder generate free form descriptions align select record model achieve best selection generation result report date fifty-nine relative improvement generation benchmark weathergov dataset despite use specialize feature linguistic resources use improve k nearest neighbor beam filter help also perform series ablations visualizations elucidate contributions key model components lastly evaluate generalizability model robocup dataset get result competitive better state art despite severely data starve
applications learn opinionated document like tweet product review face two challenge first opinionated document constitute evolve stream author attitude vocabulary may change second label document scarce label word unreliable sentiment word depend unknown context author mind research mine opinionated stream focus first aspect problem whereas second continuous supply label stream assume assumption though utopian stream infinite label cost prohibitive end investigate potential active stream learn algorithms ask label demand propose acostream one approach work limit label use initial seed label document occasionally request additional label document human expert incrementally adapt underlie stream exploit available label document core acostream consist mnb classifier couple sample strategies request class label new unlabeled document experiment evaluate classifier performance time vary class distribution opinionated stream assume set word vocabulary fix polarities may change class distribution b number unknown word arrive moment class polarity may also change result show active learn stream opinionated document deliver good performance require small selection label
present sample weight min hash swmh randomize approach automatically mine topics large scale corpora swmh generate multiple random partition corpus vocabulary base term co occurrence agglomerate highly overlap inter partition cells produce mine topics approach define topic probabilistic distribution vocabulary swmh topics order subsets vocabulary interestingly topics mine swmh underlie theme corpus different level granularity extensively evaluate meaningfulness mine topics qualitatively quantitatively nip seventeen k document twenty newsgroups twenty k reuters eight hundred k wikipedia four corpora additionally compare quality swmh online lda topics document representation classification
paper present new script classification method discrimination south slavic medieval label consist textural analysis script type first step letter cod equivalent script type define typographical feature obtain cod text subject run length statistical analysis adjacent local binary pattern analysis order extract feature result show diversity extract feature script make feature classification effective basis classification process script identification use extension state art approach document cluster propose method evaluate example hand engrave stone hand print paper label old cyrillic angular round glagolitic experiment demonstrate positive result prove effectiveness propose method
negative transfer train acoustic model automatic speech recognition report several contexts domain change speaker characteristics paper propose novel technique overcome negative transfer efficient selection speech data acoustic model train data choose relevance specific target submodular function base likelihood ratios use determine acoustically similar train utterance target test set approach evaluate wide domain data set cover speech radio tv broadcast telephone conversations meet lecture read speech experiment demonstrate propose technique find relevant data limit negative transfer result six hour test set show relative improvement four data selection use data plp base model two dnn feature
past decade humans experience exponential growth use online resources particular social media microblogging websites facebook twitter youtube also mobile applications whatsapp line etc many company identify resources rich mine market knowledge knowledge provide valuable feedback allow develop next generation product paper sentiment analysis product perform extract tweet product classify tweet show positive negative sentiment author propose hybrid approach combine unsupervised learn form k mean cluster cluster tweet perform supervise learn methods decision tree support vector machine classification
commentary article large scale evidence dependency length minimization thirty-seven languages futrell mahowald gibson pnas two thousand and fifteen one hundred and twelve thirty-three ten thousand, three hundred and thirty-six ten thousand, three hundred and forty-one
project address problem sentiment analysis twitter classify tweet accord sentiment express positive negative neutral twitter online micro blogging social network platform allow users write short status update maximum length one hundred and forty character rapidly expand service two hundred million register users one hundred million active users half log twitter daily basis generate nearly two hundred and fifty million tweet per day due large amount usage hope achieve reflection public sentiment analyse sentiments express tweet analyse public sentiment important many applications firm try find response products market predict political elections predict socioeconomic phenomena like stock exchange aim project develop functional classifier accurate automatic sentiment classification unknown tweet stream
use methods borrow statistics physics analyze write texts allow discovery unprecedent pattern human behavior cognition establish link model feature language structure current model useful unveil pattern via analysis syntactical semantical network work probe relevance investigate structure arise relationship relevant entities character locations organizations study represent entities appear context co occurrence network link establish accord null model base random shuffle texts computational simulations perform novels reveal propose model display interest topological feature small world feature characterize high value cluster coefficient effectiveness model verify practical pattern recognition task real network compare traditional word adjacency network model display optimize result identify unknown reference texts propose representation play complementary role characterize unstructured document via topological analysis name entities believe could useful improve characterization write texts relate systems specially combine traditional approach base statistical deeper paradigms
continuous vector representations word object appear carry surprisingly rich semantic content paper advance conceptual theoretical understand word embeddings three ways first grind embeddings semantic space study cognitive psychometric literature introduce new evaluation task second contrast prior work take metric recovery key object study unify exist algorithms consistent metric recovery methods base co occurrence count simple markov random walk propose new recovery algorithm third generalize metric recovery graph manifold relate co occurence count random walk graph random process manifold underlie metric recover thereby reconcile manifold estimation embed algorithms compare embed algorithms across range task nonlinear dimensionality reduction three semantic language task include analogies sequence completion classification
pet fish phenomenon often cite paradigm example non compositionality human concept use show phenomenon naturally accommodate within compositional distributional model mean model describe mean composite concept account interaction constituents via grammatical roles give two illustrative examples show qualitative phenomena exhibit go apply model experimental data finally discuss extensions formalism
approach automatically recognize entailment relations use classifiers employ hand engineer feature derive complex natural language process pipelines practice performance slightly better bag word pair classifiers use lexical similarity attempt far build end end differentiable neural network entailment fail outperform simple similarity classifier paper propose neural model read two sentence determine entailment use long short term memory units extend model word word neural attention mechanism encourage reason entailments pair word phrase furthermore present qualitative analysis attention weight produce model demonstrate reason capabilities large entailment dataset model outperform previous best neural model classifier engineer feature substantial margin first generic end end differentiable system achieve state art accuracy textual entailment dataset
illinoissl java library learn structure prediction model support structure support vector machine structure perceptron library consist core learn module several applications execute command line documentation provide guide users comparison structure learn libraries illinoissl efficient general easy use
complex event retrieval challenge research problem especially train videos available alternative collect train videos train large semantic concept bank priori give text description event event retrieval perform select concepts linguistically relate event description fuse concept responses unseen videos however define exhaustive concept lexicon pre train require vast computational resources therefore recent approach automate concept discovery train leverage large amount weakly annotate web data compact visually salient concepts automatically obtain use concept pair generally n grams however visually salient n grams necessarily useful event query combinations concepts may visually compact irrelevant drastically affect performance propose event retrieval algorithm construct pair automatically discover concepts prune concepts unlikely helpful retrieval prune depend query specific video instance evaluate approach also address calibration domain adaptation issue arise apply concept detectors unseen videos demonstrate large improvements vision base systems trecvid med thirteen dataset
multilingual nature world make translation crucial requirement today parallel dictionaries construct humans widely available resource limit provide enough coverage good quality translation purpose due vocabulary word neologisms motivate use statistical translation systems unfortunately dependent quantity quality train data limit availability especially languages narrow text domains research present improvements yalign mine methodology reimplementing comparison algorithm introduce tune script improve performance use gpu compute acceleration experiment conduct various text domains bi data extract wikipedia dump
quality machine translation rapidly evolve today one find several machine translation systems web provide reasonable translations although systems perfect specific domains quality may decrease recently propose approach domain neural machine translation aim build jointly tune single neural network maximize translation performance different approach traditional statistical machine translation recently propose neural machine translation model often belong encoder decoder family source sentence encode fix length vector turn decode generate translation present research examine effect different train methods polish english machine translation system use medical data european medicine agency parallel text corpus use basis train neural statistical network base translation systems main machine translation evaluation metrics also use analysis systems comparison implementation real time medical translator main focus experiment
parallel sentence relatively scarce extremely useful resource many applications include cross lingual retrieval statistical machine translation research explore methodology mine data previously obtain comparable corpora task highly practical since non parallel multilingual data exist far greater quantities parallel corpora parallel sentence much useful resource propose web crawl method build subject align comparable corpora wikipedia article also introduce method extract truly parallel sentence filter noisy comparable sentence pair describe implementation specialize tool task well train adaption machine translation system supply filter additional information similarity comparable sentence pair
new research explore effect various train methods polish english statistical machine translation system medical texts various elements emea parallel text corpora opus project use basis train phrase table language model development tune test translation system bleu nist meteor rib ter metrics use evaluate effect various system data preparations translation result experiment include systems use pos tag factor phrase model hierarchical model syntactic taggers many different alignment methods also conduct deep analysis polish data preparatory work automatic data correction true case punctuation normalization phase
humans learn use language physical interaction environment semiotic communication people important obtain computational understand humans form symbol system obtain semiotic skills autonomous mental development recently many study conduct construction robotic systems machine learn methods learn use language embody multimodal interaction environment systems understand human social interactions develop robot smoothly communicate human users long term require understand dynamics symbol systems crucially important embody cognition social interaction participants gradually change symbol system constructive manner paper introduce field research call symbol emergence robotics ser ser constructive approach towards emergent symbol system emergent symbol system socially self organize semiotic communications physical interactions autonomous cognitive developmental agents ie humans developmental robots specifically describe state art research topics concern ser eg multimodal categorization word discovery double articulation analysis enable robot obtain word embody mean raw sensory motor information include visual information haptic information auditory information acoustic speech signal totally unsupervised manner finally suggest future directions research ser
traditional disease surveillance systems suffer several disadvantage include report lag antiquate technology cause movement towards internet base disease surveillance systems internet systems particularly attractive disease outbreaks provide data near real time verify individuals around globe however exist systems focus disease monitor provide data repository policy makers researchers order fill gap analyze wikipedia article content demonstrate name entity recognizer train tag case count death count hospitalization count article narrative achieve f1 score seven hundred and fifty-three also show use two thousand and fourteen west african ebola virus disease epidemic article case study detail time series data consistently update closely align grind truth data argue wikipedia use create first community drive open source emerge disease detection monitor repository system
last decade increase concern bias embody traditional evaluation methods natural language process learn particularly methods borrow information retrieval without knowledge bias prevalence contingency test equivalently expectation due chance simple conditional probabilities recall precision accuracy meaningful evaluation measure either individually combinations f factor existence bias nlp measure lead improvement systems increase bias practice improve tag parse score use common value eg water always noun rather attempt discover correct one measure cohen kappa power informedness discuss unbiased alternative recall relate psychologically significant measure deltap paper analyze bias unbiased measure theoretically characterize precise relationship measure well evaluate evaluation measure empirically use monte carlo simulation
paper propose tree base convolutional neural network tbcnn discriminative sentence model model leverage either constituency tree dependency tree sentence tree base convolution process extract sentence structural feature feature aggregate max pool architecture allow short propagation paths output layer underlie feature detectors enable effective structural feature learn extraction evaluate model two task sentiment analysis question classification experiment tbcnn outperform previous state art result include exist neural network dedicate feature rule engineer also make efforts visualize tree base convolution process shed light model work
paper present new semi supervise framework convolutional neural network cnns text categorization unlike previous approach rely word embeddings method learn embeddings small text regions unlabeled data integration supervise cnn propose scheme embed learn base idea two view semi supervise learn intend useful task interest even though train do unlabeled data model achieve better result previous approach sentiment classification topic classification task
give extremely large pool events stories available media outlets need focus subset issue aspects convey audience outlets often accuse exhibit systematic bias selection process different outlets portray different versions reality however absence objective measure empirical evidence direction extent systematicity remain widely dispute paper propose framework base quote pattern quantify characterize degree media outlets exhibit systematic bias apply framework massive dataset news article span six years obama presidency speeches reveal systematic pattern indeed emerge outlet quote behavior moreover show pattern successfully exploit unsupervised prediction set determine new quote outlet select broadcast encode bias pattern low rank space provide analysis structure political media coverage reveal latent media bias space align surprisingly well political ideology outlet type linguistic analysis expose strike differences across latent dimension show different type media outlets portray different realities even report events example outlets map mainstream conservative side latent space focus quote portray presidential persona disproportionately characterize negativity
present novel deep recurrent neural network rnn model acoustic model automatic speech recognition asr term contribution tc dnn blstm dnn model model combine deep neural network dnn time convolution tc follow bidirectional long short term memory blstm final dnn first dnn act feature processor model blstm generate context sequence acoustic signal final dnn take context model posterior probabilities acoustic state achieve three hundred and forty-seven wer wall street journal wsj eval92 task eight relative improvement baseline dnn model
deep neural network dnn acoustic model yield many state art result automatic speech recognition asr task recently recurrent neural network rnn model show outperform dnns counterparts however state art dnn rnn model tend impractical deploy embed systems limit computational capacity traditionally approach embed platforms either train small dnn directly train small dnn learn output distribution large dnn paper utilize state art rnn transfer knowledge small dnn use rnn model generate soft alignments minimize kullback leibler divergence small dnn small dnn train soft rnn alignments achieve three hundred and ninety-three wer wall street journal wsj eval92 task compare baseline four hundred and fifty-four wer thirteen relative improvement
present result expand content china biographical database text mine historical local gazetteers difangzhi goal database see people connect together kinship social connections place offices serve gazetteers single important collection name offices cover song qing periods although begin local officials shall eventually include list local examination candidates people locality serve government notable local figure biographies data collect connections emerge value systematic text mine work identify relevant connections either directly informative become useful without deep historical research academia sinica develop name database officials central governments ming qing dynasties
compute linguistic information lexical syntactic semantic level recognize inference text rite task traditional simplify chinese ntcir nine ntcir ten techniques syntactic parse name entity recognition near synonym recognition employ feature like count common word statement lengths negation word antonyms consider judge entailment relationships two statements explore heuristics base function machine learn approach report systems show robustness simultaneously achieve second position binary classification subtasks simplify traditional chinese ntcir ten rite two conduct experiment test data ntcir nine rite good result also extend work search better configurations classifiers investigate contributions individual feature extend work show interest result encourage discussion
present computational framework automatically quantify verbal nonverbal behaviors context job interview propose framework train analyze videos one hundred and thirty-eight interview sessions sixty-nine internship seek undergraduates massachusetts institute technology mit automate analysis include facial expressions eg smile head gesture facial track point language eg word count topic model prosodic information eg pitch intonation pause interviewees grind truth label derive take weight average rat nine independent judge framework automatically predict rat interview traits excitement friendliness engagement correlation coefficients seventy-five higher quantify relative importance prosody language facial expressions analyze relative feature weight learn regression model framework recommend speak fluently use less filler word speak vs use unique word smile also find students rat highly answer first interview question also rat highly overall ie first impression matter finally mit interview dataset make available researchers validate expand find
order assist security analysts obtain information pertain network novel vulnerabilities exploit patch information retrieval methods tailor security domain need label text data scarce expensive follow developments semi supervise natural language process implement bootstrapping algorithm extract security entities relationships text algorithm require little input data specifically relations pattern heuristics identify relations incorporate active learn component query user important decisions prevent drift desire relations preliminary test small corpus show promise result obtain precision eighty-two
understand things conceptual state reason object fact object concepts refer manipulate long acknowledge infinitely extend notions space time size colour etc short reasonable quality object subject become infeasible affirm atomicity concept refer object however formal symbolic logics typically presume atomic entities upon expressions build reflect intuition concept onto formal symbolic logics assure usual perspective atomicity need inspect work present gradual logic materialise observation tell apart whether regard atomic entity atomic atomic enough consider non atomic motivation capture certain phenomena naturally occur around concepts attribute include presupposition contraries present logical particulars logic map onto formal semantics two linguistically interest semantics consider decidability show
ability accurately model sentence vary stag eg word phrase sentence play central role natural language process effort towards goal propose self adaptive hierarchical sentence model adasent adasent effectively form hierarchy representations word phrase sentence recursive gate local composition adjacent segment design competitive mechanism gate network allow representations sentence engage particular learn task eg classification therefore effectively mitigate gradient vanish problem persistent recursive model qualitative quantitative analysis show adasent automatically form select representations suitable task hand train yield superior classification performance competitor model five benchmark data set
paper propose multimodal convolutional neural network cnns match image sentence cnn provide end end framework convolutional architectures exploit image representation word composition match relations two modalities specifically consist one image cnn encode image content one match cnn learn joint representation image sentence match cnn compose word different semantic fragment learn inter modal relations image compose fragment different level thus fully exploit match relations image sentence experimental result benchmark databases bidirectional image sentence retrieval demonstrate propose cnns effectively capture information necessary image sentence match specifically propose cnns bidirectional image sentence retrieval flickr30k microsoft coco databases achieve state art performances
within natural language process nlp community active learn widely investigate apply order alleviate annotation bottleneck face developers new nlp systems technologies paper present first theoretical analysis stop active learn base stabilize predictions sp analysis reveal three elements central success sp method one bound cohen kappa agreement successively train model impose bound differences f measure performance model two since stop set label make large practice help guarantee result transfer previously unseen stream examples test application time three good low variance sample estimate kappa successive model obtain proof relationships level kappa agreement difference performance consecutive model present specifically kappa agreement two model exceed threshold t0 difference f measure performance model bound frac41 tt case precision positive conjunction model assume p bind tighten frac41 tp1t
relation classification important semantic process task state ofthe art systems still rely costly handcraft feature work tackle relation classification task use convolutional neural network perform classification rank cr cnn propose new pairwise rank loss function make easy reduce impact artificial class perform experiment use semeval two thousand and ten task eight dataset design task classify relationship two nominals mark sentence use crcnn outperform state art dataset achieve f1 eight hundred and forty-one without use costly handcraft feature additionally experimental result show one approach effective cnn follow softmax classifier two omit representation artificial class improve precision recall three use word embeddings input feature enough achieve state art result consider text two target nominals
paper address task learn novel visual concepts interactions concepts image sentence descriptions use linguistic context visual feature method able efficiently hypothesize semantic mean new word add word dictionary use describe image contain novel concepts method image caption module base rnn several improvements particular propose transpose weight share scheme improve performance image caption also make model suitable novel concept learn task propose methods prevent overfitting new concepts addition three novel concept datasets construct new task experiment show method effectively learn novel visual concepts examples without disturb previously learn concepts project page http wwwstatuclaedu junhuamao project childlearninghtml
social media become increasingly important source information complement traditional pharmacovigilance methods order identify signal potential adverse drug reactions necessary first identify medical concepts social media text exist study use dictionary base methods evaluate independently overall signal detection task compare different approach automatically identify normalise medical concepts consumer review medical forums specifically implement several dictionary base methods popular relevant literature well method suggest base state art machine learn method entity recognition metamap popular biomedical concept extraction tool use baseline evaluations perform control set common corpus collection medical forum post annotate concepts link control vocabularies meddra snomed ct knowledge study first systematically examine effect popular concept extraction methods area signal detection adverse reactions show choice algorithm control vocabulary significant impact concept extraction impact overall signal detection process also show propose machine learn approach significantly outperform methods identification adverse reactions drug even train relatively small set annotate text
common representation learn crl wherein different descriptions view data embed common subspace receive lot attention recently two popular paradigms canonical correlation analysis cca base approach autoencoder ae base approach cca base approach learn joint representation maximize correlation view project common subspace ae base methods learn common representation minimize error reconstruct two view approach advantage disadvantage example cca base approach outperform ae base approach task transfer learn scalable latter work propose ae base approach call correlational neural network corrnet explicitly maximize correlation among view project common subspace series experiment demonstrate propose corrnet better mention approach respect ability learn correlate common representations employ corrnet several cross language task show representations learn use corrnet perform better ones learn use state art approach
many recent advance structure measurement distribute language model map word vector space rich information word choice composition vector space distribute language representation goal note point distribute representation turn classifier inversion via bay rule approach simple modular work language representation whose train formulate optimize probability model application two million sentence yelp review also find perform well better complex purpose build algorithms
paper combine advantage model use global source sentence contexts discriminative word lexicon neural network use deep neural network instead linear maximum entropy model discriminative word lexicon model able leverage dependencies different source word due non linearity furthermore model different target word share parameters therefore data sparsity problems effectively reduce use approach state art translation system improve performance five bleu point three different language pair ted translation task
recurrent neural network rnns become increasingly popular task language understand task semantic tagger deploy associate semantic label word input sequence success rnn may attribute ability memorize long term dependence relate current time semantic label prediction observations many time instance away however memory capacity simple rnns limit gradient vanish explode problem propose use external memory improve memorization capability rnns conduct experiment atis dataset observe propose model able achieve state art result compare propose model alternative model report analysis result may provide insights future research
paper propose employ convolutional neural network cnn image question answer qa propose cnn provide end end framework convolutional architectures learn image question representations also inter modal interactions produce answer specifically model consist three cnns one image cnn encode image content one sentence cnn compose word question one multimodal convolution layer learn joint representation classification space candidate answer word demonstrate efficacy propose model daquar coco qa datasets two benchmark datasets image qa performances significantly outperform state art
social media rich source rumour correspond community reactions rumour reflect different characteristics share individual formulate problem classify tweet level judgements rumour supervise learn task supervise unsupervised domain adaptation consider tweet rumour classify basis annotate rumour demonstrate multi task learn help achieve good result rumour two thousand and eleven england riot
paper present multilingual study per single post microblog text much say b much write term character bytes c much say term information content post different organizations different languages focus three different languages english chinese japanese research analyse weibo twitter account major embassies news agencies first establish criterion quantify much say digital text base openly available universal declaration human right translate subtitle ted talk parallel corpora allow us determine number character bits need represent content different languages character encode derive amount information actually contain microblog post author select account weibo twitter result confirm languages larger character set chinese japanese contain information per character english actual information content contain within microblog text vary depend type organization language post conclude discussion design implications microblog text limit different languages
information theoretic measure relative entropy correlation extremely useful model analyze interaction probabilistic systems survey quantum generalization five measure point commonalities interpretations particular find application information theory distributional semantics useful model distributional mean word density operators rather vectors semantic structure may exploit furthermore properties interactions word ambiguity similarity entailment simulate richly intuitively use methods quantum information theory
faster connection speed internet users make social network huge reservoir texts image video clip gif sentiment analysis online platform use predict political elections evaluate economic indicators however gif sentiment analysis quite challenge hinge spatio temporal visual contentabstraction also relationship abstraction final sentiment remain unknownin paper dedicate find relationshipwe propose sentipairsequence basedspatiotemporal visual sentiment ontology form midlevel representations gifsentiment establishment process sentipair contain two step first construct synset forest define semantic tree structure visual sentiment label elements thesynset forest organically select combine sentiment label elements form mid level visual sentiment representation experiment indicate sentipair outperform compete mid level attribute use sentipair analysis frameworkcan achieve satisfy prediction accuracy seven hundred and twenty-six also open ourdataset gso two thousand and fifteen research community gso two thousand and fifteen contain six thousand manually annotate gifs forty thousand candidates label sentiment sentipair sequence
paper tackle problem endogenous link prediction knowledge base completion knowledge base represent direct graph whose nod correspond entities edge relationships previous attempt either consist powerful systems high capacity model complex connectivity pattern unfortunately usually end overfitting rare relationships approach trade capacity simplicity order fairly model relationships frequent paper propose tatec happy medium obtain complement high capacity model simpler one pre train separately combine present several variants model different kinds regularization combination strategies show approach outperform exist methods different type relationships achieve state art result four benchmarks literature
path query knowledge graph use answer compositional question languages speak people live lisbon however knowledge graph often miss facts edge disrupt path query recent model knowledge base completion impute miss facts embed knowledge graph vector space show model recursively apply answer path query suffer cascade errors motivate new compositional train objective dramatically improve model ability answer path query case double accuracy standard knowledge base completion task also demonstrate compositional train act novel form structural regularization reliably improve performance across base model reduce errors forty-three achieve new state art result
assess performance generic text summarization algorithms apply film documentaries use well know behavior summarization news article reference use three datasets news article ii film script subtitle iii documentary subtitle standard rouge metrics use compare generate summaries news abstract plot summaries synopses show best perform algorithms lsa news article documentaries lexrank support set film despite different nature film documentaries relative behavior accordance obtain news article
recurrent neural network rnns specifically variant long short term memory lstm enjoy renew interest result successful applications wide range machine learn problems involve sequential data however lstms provide exceptional result practice source performance limitations remain rather poorly understand use character level language model interpretable testbed aim bridge gap provide analysis representations predictions error type particular experiment reveal existence interpretable cells keep track long range dependencies line lengths quote bracket moreover comparative analysis finite horizon n gram model trace source lstm improvements long range structural dependencies finally provide analysis remain errors suggest areas study
paper summarize work do author zero resource speech challenge organize technical program interspeech two thousand and fifteen goal challenge discover linguistic units directly unlabeled speech data multi layer acoustic tokenizer mat propose work automatically discover multiple set acoustic tokens give corpus acoustic token set specify set hyperparameters describe model configuration set acoustic tokens carry different characteristics give corpus language behind thus mutually reinforce multiple set token label use target multi target dnn mdnn train low level acoustic feature bottleneck feature extract mdnn use feedback mat mdnn call iterative system multi layer acoustic tokenizing deep neural network mat dnn generate high quality feature track one challenge acoustic tokens track two challenge
recently strong result demonstrate deep recurrent neural network natural language transduction problems paper explore representational power model use synthetic grammars design exhibit phenomena similar find real transduction problems machine translation experiment lead us propose new memory base recurrent network implement continuously differentiable analogues traditional data structure stack queue deques show architectures exhibit superior generalisation performance deep rnns often able learn underlie generate algorithms transduction experiment
embed word vector space gain lot attention recent years state art methods provide efficient computation word similarities via low dimensional matrix embed motivation often leave unclear paper argue word embed naturally view rank problem due rank nature evaluation metrics base insight propose novel framework wordrank efficiently estimate word representations via robust rank attention mechanism robustness noise readily achieve via dcg like rank losses performance wordrank measure word similarity word analogy benchmarks result compare state art word embed techniques algorithm competitive state arts large corpora outperform significant margin train set limit ie sparse noisy seventeen million tokens wordrank perform almost well exist methods use seventy-two billion tokens popular word similarity benchmark multi node distribute implementation wordrank publicly available general usage
recurrent neural network train produce sequence tokens give input exemplify recent result machine translation image caption current approach train consist maximize likelihood token sequence give current recurrent state previous token inference unknown previous token replace token generate model discrepancy train inference yield errors accumulate quickly along generate sequence propose curriculum learn strategy gently change train process fully guide scheme use true previous token towards less guide scheme mostly use generate token instead experiment several sequence prediction task show approach yield significant improvements moreover use successfully win entry mscoco image caption challenge two thousand and fifteen
teach machine read natural language document remain elusive challenge machine read systems test ability answer question pose content document see large scale train test datasets miss type evaluation work define new methodology resolve bottleneck provide large scale supervise read comprehension data allow us develop class attention base deep neural network learn read real document answer complex question minimal prior knowledge language structure
sentiment classification widely use product review online social media forums twitter blog however problem classify sentiment user comment news sit address yet news sit cover wide range domains include politics sport technology entertainment contrast online social sit forums review sit specific particular domain user associate news site likely post comment diverse topics eg politics smartphones sport diverse entities eg obama iphone google classify sentiment users tie various entities may help obtain holistic view personality could useful applications online advertise content personalization political campaign plan paper formulate problem entity specific sentiment classification comment post news article yahoo news propose novel feature specific news comment experimental result show model outperform state art baselines
calculation log normalizer major computational obstacle applications log linear model large output space problem fast normalizer computation therefore attract significant attention theoretical apply machine learn literature paper analyze recently propose technique know self normalization introduce regularization term train penalize log normalizers deviate zero make possible use unnormalized model score approximate probabilities empirical evidence suggest self normalization extremely effective theoretical understand work generally apply largely lack prove generalization bound estimate variance normalizers upper bound loss accuracy due self normalization describe class input distributions self normalize easily construct explicit examples high variance input distributions theoretical result make predictions difficulty fit self normalize model several class distributions conclude empirical validation predictions
recurrent neural network rnns good model flow text typically need train far larger corpus available pan two thousand and fifteen author identification task paper describe novel approach output layer character level rnn language model split several independent predictive sub model represent author recurrent layer share allow recurrent layer model language whole without fit output select aspects underlie model reflect author style method prove competitive rank first two four languages
paper method propose detect emotion song base lyrical audio feature lyrical feature generate segmentation lyric process data extraction anew wordnet knowledge incorporate compute valence arousal value addition linguistic association rule apply ensure issue ambiguity properly address audio feature use supplement lyrical ones include attribute like energy tempo danceability feature extract echo nest widely use music intelligence platform construction train test set do basis social tag extract lastfm website classification do apply feature weight stepwise threshold reduction k nearest neighbor algorithm provide fuzziness classification
workshop mine scientific paper computational linguistics bibliometrics clbib two thousand and fifteen co locate 15th international society scientometrics informetrics conference issi two thousand and fifteen bring together researchers bibliometrics computational linguistics order study ways bibliometrics benefit large scale text analytics sense mine scientific paper thus explore interdisciplinarity bibliometrics natural language process nlp goals workshop answer question like enhance author network analysis bibliometrics use data obtain text analytics insights nlp provide structure scientific write citation network text citation analysis workshop first step foster reflection interdisciplinarity benefit two discipline bibliometrics natural language process drive
one biggest challenge multimedia information retrieval understand bridge semantic gap properly model concept semantics context presence vocabulary oov concepts exacerbate difficulty address semantic gap issue formulate problem learn contextualized semantics descriptive term propose novel siamese architecture model contextualized semantics descriptive term mean pattern aggregation probabilistic topic model siamese architecture capture contextualized semantics co occur descriptive term via unsupervised learn lead concept embed space term context furthermore co occur oov concepts easily represent learn concept embed space main properties concept embed space demonstrate via visualization use various settings semantic prim carry thorough evaluation compare approach number state art methods six annotation corpora different domains ie magtag5k cal500 million song dataset music domain well corel5k labelme sundatabase image domain experimental result semantic prim suggest approach outperform state art methods considerably various aspects
automatic text summarization widely regard highly difficult problem partially lack large text summarization data set due great challenge construct large scale summaries full text paper introduce large corpus chinese short text summarization dataset construct chinese microblogging website sina weibo release public http icrchitszeducn article show 139html corpus consist two million real chinese short texts short summaries give author text also manually tag relevance ten thousand, six hundred and sixty-six short summaries correspond short texts base corpus introduce recurrent neural network summary generation achieve promise result show usefulness propose corpus short text summarization research also provide baseline research topic
information extraction ie aim automatically generate large knowledge base natural language text progress remain slow supervise learn require copious human annotation unsupervised weakly supervise approach deliver competitive accuracy result field applications ie well lead tac kbp systems rely significant amount manual engineer even extreme methods report freedman et al two thousand and eleven require ten hours expert labor per relation paper show reduce effort order magnitude present novel system instaread streamline author ensemble methods one encode extraction rule expressive compositional representation two guide user promise rule base corpus statistics mine resources three introduce new interactive development cycle provide immediate feedback even large datasets experiment show experts create quality extractors hour even nlp novices author good extractors extractors equal outperform ones obtain comparably supervise state art distantly supervise approach
propose deepmemory novel deep architecture sequence sequence learn perform task series nonlinear transformations representation input sequence eg chinese sentence final output sequence eg translation english inspire recently propose neural turing machine grave et al two thousand and fourteen store intermediate representations stack layer memories use read write operations memories realize nonlinear transformations representations type transformations design advance parameters learn data layer layer transformations deepmemory model complicate relations sequence necessary applications machine translation distant languages architecture train normal back propagation sequenceto sequence data learn easily scale large corpus deepmemory broad enough subsume state art neural translation model bahdanau et al two thousand and fifteen special case significantly improve upon model deeper architecture remarkably deepmemory purely neural network base achieve performance comparable traditional phrase base machine translation system moses small vocabulary modest parameter size
paper answer selection problem community question answer cqa regard answer sequence label task novel approach propose base recurrent architecture problem approach apply convolution neural network cnns learn joint representation question answer pair firstly use joint representation input long short term memory lstm learn answer sequence question label match quality answer experiment conduct semeval two thousand and fifteen cqa dataset show effectiveness approach
human infants discover word directly unsegmented speech signal without explicitly label data paper develop novel machine learn method call nonparametric bayesian double articulation analyzer npb daa directly acquire language acoustic model observe continuous speech signal purpose propose integrative generative model combine language model acoustic model single generative model call hierarchical dirichlet process hide language model hdp hlm hdp hlm obtain extend hierarchical dirichlet process hide semi markov model hdp hsmm propose johnson et al inference procedure hdp hlm derive use block gibbs sampler originally propose hdp hsmm procedure enable simultaneous direct inference language acoustic model continuous speech signal base hdp hlm inference procedure develop novel double articulation analyzer assume hdp hlm generative model observe time series data infer latent variables model method analyze latent double articulation structure ie hierarchically organize latent word phonemes data unsupervised manner novel unsupervised double articulation analyzer call npb daa npb daa automatically estimate double articulation structure embed speech signal also carry two evaluation experiment use synthetic data actual human continuous speech signal represent japanese vowel sequence word acquisition phoneme categorization task npb daa outperform conventional double articulation analyzer daa baseline automatic speech recognition system whose acoustic model train supervise manner
present novel response generation system train end end large quantities unstructured twitter conversations neural network architecture use address sparsity issue arise integrate contextual information classic statistical model allow system take account previous dialog utterances dynamic context generative model show consistent gain context sensitive non context sensitive machine translation information retrieval baselines
recognize emotion speech become one active research theme speech process applications base human computer interaction paper conduct experimental study recognize emotions human speech emotions consider experiment include neutral anger joy sadness distinuishability emotional feature speech study first follow emotion classification perform custom dataset classification perform different classifiers one main feature attribute consider prepare dataset peak peak distance obtain graphical representation speech signal perform classification test dataset form thirty different subject find get better accuracy one consider data collect one person rather consider data group people
integrate vision language long dream work artificial intelligence ai past two years witness explosion work bring together vision language image videos beyond available corpora play crucial role advance area research paper propose set quality metrics evaluate analyze vision language datasets categorize accordingly analyse show recent datasets use complex language abstract concepts however different strengths weaknesses
financial news contain useful information public company market paper apply popular word embed methods deep neural network leverage financial news predict stock price movements market experimental result show propose methods simple effective significantly improve stock prediction accuracy standard financial database baseline system use historical price information
task natural language process cast question answer qa problems language input introduce dynamic memory network dmn neural network architecture process input sequence question form episodic memories generate relevant answer question trigger iterative attention process allow model condition attention input result previous iterations result reason hierarchical recurrent sequence model generate answer dmn train end end obtain state art result several type task datasets question answer facebook babi dataset text classification sentiment analysis stanford sentiment treebank sequence model part speech tag wsj ptb train different task rely exclusively train word vector representations input question answer triplets
replicate softmax model well know undirected topic model powerful extract semantic representations document traditional learn strategies contrastive divergence inefficient paper provide novel estimator speed learn base noise contrastive estimate extend document variant lengths weight input experiment two benchmarks show new estimator achieve great learn efficiency high accuracy document retrieval classification
recurrent sequence generators condition input data attention mechanism recently show good performance range task cluding machine translation handwrite synthesis image caption gen eration extend attention mechanism feature need speech recognition show adaptation model use machine translation reach competitive one hundred and eighty-seven phoneme error rate per timit phoneme recognition task apply utterances roughly long ones train offer qualitative explanation failure propose novel generic method add location awareness attention mechanism alleviate issue new method yield model robust long input achieve eighteen per single utterances twenty ten time longer repeat utterances finally propose change tention mechanism prevent concentrate much single frame reduce per one hundred and seventy-six level
article extend version paper present wsom two thousand and twelve conference one display combination factorial projections som algorithm graph techniques apply text mine problem corpus contain eight medieval manuscripts use teach arithmetic techniques merchants among techniques data analysis use lexicometry factorial analysis highlight discrepancies manuscripts reason focus deviation independence word manuscripts still also want discover characterize common vocabulary among whole corpus use properties stochastic kohonen map define neighborhood input non deterministic way highlight word seem play special role vocabulary call fickle use improve kohonen map robustness significance fca visualization finally use graph algorithmic exploit fickleness classification word
new yorker publish weekly captionless cartoon five thousand readers submit caption editors select three ask readers pick funniest one describe experiment compare dozen automatic methods select funniest caption show negative sentiment human centeredness lexical centrality strongly match funniest caption follow positive sentiment result useful understand humor also design engage conversational agents text multimodal visiontext systems part work large set cartoon caption make available community
deep learn approach propose recently derive speaker identify vector deep neural network dnn approach apply text dependent speaker recognition task show reasonable performance gain combine conventional vector approach although promise exist vector implementation still compete vector baseline paper present two improvements deep learn approach phonedependent dnn structure normalize phone variation new score approach base dynamic time warp dtw experiment text dependent speaker recognition task demonstrate propose methods provide considerable performance improvement exist vector implementation
visual layout webpage provide valuable clue certain type information extraction ie task traditional rule base ie frameworks layout cue map rule operate html source webpages contrast develop framework rule specify directly layout level many advantage since higher level abstraction lead simpler extraction rule largely independent source code page therefore robust also enable specification new type rule otherwise possible best knowledge general framework allow declarative specification information extraction rule base spatial layout framework complementary traditional text base rule framework allow seamless combination spatial layout base rule traditional text base rule describe algebra enable system efficient implementation use standard relational text index feature relational database demonstrate simplicity efficiency system task involve extraction software system requirements software product page
requirements traceability essential step ensure quality software early stag development life cycle requirements trace usually consist document parse candidate link generation evaluation traceability analysis paper demonstrate applicability statistical term extraction metrics generate candidate link apply validate use two data set four type filter two data set two twenty-five modis zero five cm1 method generate requirements traceability matrices textual requirements artifacts high level requirements trace low level requirements propose method include ten word frequency metrics divide three main group calculate frequency term result show propose method give better result compare traditional tf idf method
paper introduce ubuntu dialogue corpus dataset contain almost one million multi turn dialogues total seven million utterances one hundred million word provide unique resource research build dialogue managers base neural language model make use large amount unlabeled data dataset multi turn property conversations dialog state track challenge datasets unstructured nature interactions microblog service twitter also describe two neural learn architectures suitable analyze dataset provide benchmark performance task select best next response
unsupervised text embed methods skip gram paragraph vector attract increase attention due simplicity scalability effectiveness however compare sophisticate deep learn architectures convolutional neural network methods usually yield inferior result apply particular machine learn task one possible reason text embed methods learn representation text fully unsupervised way without leverage label information available task although low dimensional representations learn applicable many different task particularly tune task paper fill gap propose semi supervise representation learn method text data call textitpredictive text embed pte predictive text embed utilize label unlabeled data learn embed text label information different level word co occurrence information first represent large scale heterogeneous text network embed low dimensional space principled efficient algorithm low dimensional embed preserve semantic closeness word document also strong predictive power particular task compare recent supervise approach base convolutional neural network predictive text embed comparable effective much efficient fewer parameters tune
use sswl database syntactic parameters world languages mit media lab data language interactions construct spin glass model language evolution treat binary syntactic parameters spin state languages vertices graph assign interaction energies along edge study rough model syntax evolution assumption strong interaction energy tend parameters align case ferromagnetic materials also study spin glass model need modify account entailment relations syntactic parameters modification lead naturally generalization potts model external magnetic field consist couple vertices ising model potts model q3 edge interactions describe result simulations dynamics model different temperature energy regimes discuss linguistic interpretation parameters physical model
hierarchical latent tree analysis hlta recently propose new method topic detection differ fundamentally lda base methods term topic definition topic document relationship learn method show discover significantly coherent topics better topic hierarchies however hlta rely expectation maximization algorithm parameter estimation hence efficient enough deal large datasets paper propose method drastically speed hlta use technique inspire recent advance moments method empirical experiment show method greatly improve efficiency hlta efficient state art lda base method hierarchical topic detection find substantially better topics topic hierarchies
deep learn gain much success sentence level relation classification example convolutional neural network cnn deliver competitive performance without much effort feature engineer conventional pattern base methods thus lot work produce base cnn structure however key issue well address cnn base method lack capability learn temporal feature especially long distance dependency nominal pair paper propose simple framework base recurrent neural network rnn compare cnn base model show limitation popular use semeval two thousand and ten task eight dataset introduce another dataset refine mimlreangeli et al two thousand and fourteen experiment two different datasets strongly indicate rnn base model deliver better performance relation classification particularly capable learn long distance relation pattern make suitable real world applications complicate expressions often involve
latent dirichlet allocation lda three level hierarchical bayesian model topic inference spite great success infer latent topic distribution lda time consume motivate transfer learn approach propose bynewcitehinton2015distilling present novel method use lda supervise train deep neural network dnn dnn approximate costly lda inference less computation experiment document classification task show simple dnn learn lda behavior pretty well inference speed tens hundreds time
present listen attend spell las neural network learn transcribe speech utterances character unlike traditional dnn hmm model model learn components speech recognizer jointly system two components listener speller listener pyramidal recurrent network encoder accept filter bank spectra input speller attention base recurrent network decoder emit character output network produce character sequence without make independence assumptions character key improvement las previous end end ctc model subset google voice search task las achieve word error rate wer one hundred and forty-one without dictionary language model one hundred and three language model rescoring top thirty-two beam comparison state art cldnn hmm model achieve wer eighty
report describe initial replication study precise system develop clearer formal description approach base evaluation conclude precise result fully replicate however formalization develop suggest road map enhance extend approach pioneer precise long productive discussion ana maria popescu one author precise get clarity precise approach lexicon author geo evaluation base build direct implementation repair formalism although new evaluation yet complete clear system perform much better continue develop ideas implementation generate future report publication accurately evaluate precise like approach
state art extractive multi document summarization systems usually design without concern privacy issue mean document open third party paper propose privacy preserve approach multi document summarization approach enable party obtain summaries without learn anything else original document content use hash scheme know secure binary embeddings convert document representation contain key phrase bag word bite string allow computation approximate distance instead exact ones experiment indicate system yield similar result non private counterpart standard multi document evaluation datasets
logic base machine understandable framework semantic web often challenge naive users try query ontology base knowledge base exist research efforts approach problem introduce natural language nl interfaces ontologies nl interfaces ability construct sparql query base nl user query however efforts restrict query express english often benefit advancement english nlp tool however little research do support query arabic content semantic web use nl query paper present domain independent approach translate arabic nl query sparql leverage linguistic analysis base special consideration noun phrase nps approach use language parser extract nps relations arabic parse tree match underlie ontology utilize knowledge ontology group nps triple base representations sparql query finally generate extract target modifiers interpret sparql interpretation advance semantic feature include negation conjunctive disjunctive modifiers also support approach evaluate use two datasets consist owl test data query obtain result confirm feasibility translate arabic nl query sparql
decision analytics commonly focus text mine financial news source order provide managerial decision support predict stock market movements exist predictive frameworks almost exclusively apply traditional machine learn methods whereas recent research indicate traditional machine learn methods sufficiently capable extract suitable feature capture non linear nature complex task remedy novel deep learn model aim overcome issue extend traditional neural network model additional hide layer indeed deep learn show outperform traditional methods term predictive performance paper adapt novel deep learn technique financial decision support instance aim predict direction stock movements follow financial disclosures result show deep learn outperform accuracy random forest benchmark machine learn five hundred and sixty-six
deep compositional model mean act distributional representations word order produce vectors larger text constituents evolve popular area nlp research detail compositional distributional framework base rich form word embeddings aim facilitate interactions word context sentence embeddings composition layer jointly learn generic objective enhance vectors syntactic information surround context furthermore word associate number sense plausible select dynamically composition process evaluate produce vectors qualitatively quantitatively positive result sentence level effectiveness framework demonstrate msrpar task report result within state art range
explore methods content selection address issue coherence context generation multimedia artifacts use audio video present two case study generation film tributes lecture drive science talk content selection use centrality base diversity base summarization along topic analysis establish coherence use emotional content music film tributes ensure topic similarity lecture documentaries science talk composition techniques production multimedia artifacts address mean organize content order improve coherence discuss result consider aspects
present general theory correspond declarative model embody ground natural language base analytical summarisation dynamic visuo spatial imagery declarative model ecompassing spatio linguistic abstractions image schemas spatio temporal feature base language generator modularly implement within constraint logic program clp implement model primitives theory eg pertain space motion image schemata available first class object deep semantics suit inference query demonstrate model select examples broadly motivate areas film design geography smart environments analytical natural language base externalisations move image central viewpoint human interaction evidence base qualitative analysis sensemaking keywords move image visual semantics embodiment visuo spatial cognition computation cognitive vision computational model narrative declarative spatial reason
develop information theoretical concepts require study statistical dependencies among three variables dependencies pure triple interactions sense explain term combination pairwise correlations derive bound triple dependencies characterize shape joint probability distribution three binary variables high triple interaction analysis also allow us quantify amount redundancy mutual information pair variables assess whether information two variables mediate third variable concepts apply analysis write texts find probability give word find particular location within text modulate presence absence nearby word also presence absence nearby pair word identify word enclose key semantic concepts text triplets word high pairwise triple interactions word mediate pairwise interactions word
exist word embed methods categorize neural embed model matrix factorization mf base methods however model opaque probabilistic interpretation mf base methods typically solve use singular value decomposition svd may incur loss corpus information addition desirable incorporate global latent factor topics sentiments write style word embed model since generative model provide principled way incorporate latent factor propose generative word embed model easy interpret serve basis sophisticate latent factor model model inference reduce low rank weight positive semidefinite approximation problem optimization approach eigendecomposition submatrix follow online blockwise regression scalable avoid information loss svd experiment seven common benchmark datasets vectors competitive word2vec better mf base methods
investigate extension continuous online learn recurrent neural network language model model keep separate vector representation current unit text process adaptively adjust prediction initial experiment give promise result indicate method able increase language model accuracy also decrease parameters need store model along computation require step
every culture language unique work expressly focus uniqueness culture language relation human affect specifically sentiment emotion semantics manifest social multimedia develop set sentiment emotion polarize visual concepts adapt semantic structure call adjective noun pair originally introduce borth et al two thousand and thirteen multilingual context propose new language dependent method automatic discovery adjective noun construct show pipeline apply social multimedia platform creation large scale multilingual visual sentiment concept ontology mvso unlike flat structure borth et al two thousand and thirteen unify ontology organize hierarchically multilingual cluster visually detectable nouns subclusters emotionally bias versions nouns addition present image base prediction task show generalizable language specific model multilingual context new publicly available dataset 156k sentiment bias visual concepts across twelve languages language specific detector bank 736m image metadata also release
many current state art large vocabulary continuous speech recognition systems lvcsr hybrids neural network hide markov model hmms systems contain separate components deal acoustic model language model sequence decode investigate direct approach hmm replace recurrent neural network rnn perform sequence prediction directly character level alignment input feature desire character sequence learn automatically attention mechanism build rnn predict character attention mechanism scan input sequence choose relevant frame propose two methods speed operation limit scan subset promise frame pool time information contain neighbor frame thereby reduce source sequence length integrate n gram language model decode process yield recognition accuracies similar hmm free rnn base approach
propose neural reasoner framework neural network base reason natural language sentence give question neural reasoner infer multiple support facts find answer question specific form neural reasoner one specific interaction pool mechanism allow examine multiple facts two deep architecture allow model complicate logical relations reason task assume particular structure exist question facts neural reasoner able accommodate different type reason different form language expressions despite model complexity neural reasoner still train effectively end end manner empirical study show neural reasoner outperform exist neural reason systems remarkable margins two difficult artificial task positional reason path find propose eight example improve accuracy path finding10k three hundred and thirty-four six ninety-eight
develop necessary sufficient condition novel provably consistent efficient algorithm discover topics latent factor observations document realize probabilistic mixture share latent factor certain properties focus class topic model share latent factor contain novel word unique factor property come know separability algorithm base key insight novel word correspond extreme point convex hull form row vectors suitably normalize word co occurrence matrix leverage geometric insight establish polynomial computation sample complexity bound base isotropic random projections row normalize word co occurrence matrix propose random projections base algorithm naturally amenable efficient distribute implementation attractive modern web scale distribute data mine applications
effect various lexical syntactic semantic stylistic feature address persuasive language computational point view persuasive effect phonetics receive little attention model notion euphony analyze four datasets comprise persuasive non persuasive sentence different domains political speeches movie quote slogans tweet explore impact sound different form persuasiveness conduct series analyse prediction experiment within across datasets result highlight positive role phonetic devices persuasion
paper method measure synchronic corpus dis similarity put forward kilgarriff two thousand and one adapt extend identify trend correlate change diachronic text data use corpus historical american english davies 2010a google ngram corpora michel et al 2010a paper show fully data drive method extract word type undergo pronounce change frequency give period time computationally cheap allow interpretations diachronic trend intuitively plausible motivate perspective information theory furthermore demonstrate method able identify correlate linguistic change diachronic shift link historical events finally help improve diachronic pos tag complement exist nlp approach indicate approach facilitate improve understand diachronic process language change
syntactic structure sentence exhibit strike regularity dependencies tend cross draw sentence investigate two compete explanations traditional hypothesis trend arise independent principle syntax reduce cross practically zero alternative view hypothesis cross side effect dependency lengths ie sentence shorter dependency lengths tend fewer cross able reject traditional view majority languages consider alternative hypothesis lead parsimonious theory language
describe simple neural language model rely character level input predictions still make word level model employ convolutional neural network cnn highway network character whose output give long short term memory lstm recurrent neural network language model rnn lm english penn treebank model par exist state art despite sixty fewer parameters languages rich morphology arabic czech french german spanish russian model outperform word level morpheme level lstm baselines fewer parameters result suggest many languages character input sufficient language model analysis word representations obtain character composition part model reveal model able encode character semantic orthographic information
multilingualism common offline limit understand ways multilingualism display online roles multilinguals play spread content speakers different languages take computational approach study multilingualism use one largest user generate content platforms wikipedia study multilingualism collect analyze large dataset content write multilingual editors english german spanish editions wikipedia dataset contain two million paragraph edit fifteen thousand multilingual users july eight august nine two thousand and thirteen analyze multilingual editors term engagement interest language proficiency primary non primary secondary languages find english edition wikipedia display different dynamics spanish german editions users primarily edit spanish german editions make complex edit users edit editions second language contrast users edit english edition second language make edit complex edit users primarily edit english edition way english serve special role bring together content write multilinguals many language editions nonetheless language remain formidable hurdle spread content find evidence complexity barrier whereby editors less likely edit complex content second language addition find multilinguals less engage show lower level language proficiency second languages also examine topical interest multilingual editors find significant difference primary non primary editors language
word representations induce model discrete latent variables eg hmms show beneficial many nlp applications work exploit label syntactic dependency tree formalize induction problem unsupervised learn tree structure hide markov model syntactic function use additional observe variables model influence transition emission components syntactic information potentially lead capture fine grain functional distinctions word turn may desirable many nlp applications evaluate word representations two task name entity recognition semantic frame identification observe improvements exploit syntactic function information case result rival state art representation learn methods additionally revisit relationship sequential unlabeled tree model find advantage latter self evident
modern science characterize exponential growth scientific literature increase publication volume clearly reflect expansion cognitive boundaries science nevertheless metrics assess vitality science make fund policy decisions base productivity similarly increase level knowledge production large science team whose result often enjoy greater visibility necessarily mean big science lead cognitive expansion present novel big data method quantify extents cognitive domains different body scientific literature independently publication volume apply twenty million article publish sixty one hundred and thirty years physics astronomy biomedicine method base lexical diversity title fix quotas research article owe large size quotas method overcome inherent stochasticity article title achieve one precision show periods cognitive growth necessarily coincide trend publication volume furthermore show article produce larger team cover significantly smaller cognitive territory quota article smaller team find provide new perspective role small team individual researchers expand cognitive boundaries science propose method quantify extent cognitive territory also apply study many aspects science science
many methods propose detect emerge events text stream use topic model however methods shortcomings make unsuitable rapid detection locally emerge events massive text stream describe spatially compact semantic scan scss develop specifically overcome shortcomings current methods detect new spatially compact events text stream scss employ alternate optimization use semantic scan estimate contrastive foreground topics document discover spatial neighborhoods high occurrence document contain foreground topics evaluate method emergency department chief complaints dataset ed dataset verify effectiveness method detect real world disease outbreaks free text ed chief complaint data
count number alignments n ge one sequence match type specify set ssubseteq mathbbnn equivalently count number nonnegative integer matrices whose row sum give fix vector whose columns lie provide new asymptotic formula case ss1ldotssn 1le sile two
paper explore different neural network architectures predict speaker give utterance ask question make statement com pare outcomes regularization methods popularly use train deep neural network study different context function affect classification performance also compare efficacy gate activation function favorably use recurrent neural network study combine multimodal input evaluate model two multimodal datasets msr skype callhome
paper empirical study distribute deep learn question answer subtasks answer selection question classification comparison study sgd msgd adadelta adagrad adam adamax rmsprop downpour easgd eamsgd algorithms present experimental result show distribute framework base message pass interface accelerate convergence speed sublinear scale paper demonstrate importance distribute train example forty-eight workers 24x speedup achievable answer selection task run time decrease one thousand, three hundred and eighty-two hours five hundred and eighty-one hours increase productivity significantly
zipf law appear many application areas close form expression may make use cumbersome since coincide truncate version zeta distribution paper propose three approximate close form expressions truncate zeta distribution may employ zipf law well three approximations base replacement sum occur zipf law integral name respectively integral approximation average integral approximation trapezoidal approximation first one show little use trapezoidal approximation exhibit error typically lower one low one range value zipf parameter one
person name location name essential build block identify events social network historical document write literary chinese take lead explore research algorithmically recognize name entities literary chinese historical study language model base conditional random field base methods extend work mine document structure historical document practical evaluations conduct texts extract two hundred and twenty volumes local gazetteers difangzhi difangzhi huge single important collection contain information officer serve local government chinese history methods perform well realistic test thousands name address identify texts good portion extract name match biographical information currently record china biographical database cbdb harvard university many others verify historians become new additions cbdb
complete tang poems ctp important source study tang poems look ctp computational tool specific linguistic perspectives include distributional semantics collocational analysis quantitative viewpoints compare usage wind moon poems li bai du fu color poems function like sound movies play crucial role imageries poems thus word color study white main focus frequent color ctp also explore case use color word antithesis pair central foster imageries poems ctp also contain useful historical information extract person name ctp study social network tang poets information integrate china biographical database harvard university
paper present stack attention network sans learn answer natural language question image sans use semantic representation question query search regions image relate answer argue image question answer qa often require multiple step reason thus develop multiple layer san query image multiple time infer answer progressively experiment conduct four image qa data set demonstrate propose sans significantly outperform previous state art approach visualization attention layer illustrate progress san locate relevant visual clue lead answer question layer layer
propose method generate unambiguous description know refer expression specific object region image also comprehend interpret expression infer object describe show method outperform previous methods generate descriptions object without take account potentially ambiguous object scene model inspire recent successes deep learn methods image caption image caption difficult evaluate task allow easy objective evaluation also present new large scale dataset refer expressions base ms coco release dataset toolbox visualization evaluation see https githubcom mjhucla googlerefexptoolbox
propose effective technique solve review level sentiment classification problem use sentence level polarity correction polarity correction technique take account consistency polarities positive negative sentence within product review perform actual machine learn task sentence inconsistent polarities remove sentence consistent polarities use learn state art classifiers technique achieve better result different type products review outperform baseline model without correction technique experimental result show average eighty-two f measure four different product review domains
paper propose structure deep neural network structure dnn structure deep learn framework approach learn find best structure object label sequence give structure input vector sequence globally consider map relationships structure rather item item automatic speech recognition view special case structure learn problem acoustic vector sequence input phoneme label sequence output become possible comprehensively learn utterance utterance whole rather frame frame structure support vector machine structure svm propose perform asr structure learn previously limit linear nature svm propose structure dnn use nonlinear transformations multi layer structure deep learn approach approach show beat structure svm preliminary experiment timit
visual question answer fundamentally compositional nature question like dog share substructure question like color dog cat paper seek simultaneously exploit representational capacity deep network compositional linguistic structure question describe procedure construct learn neural module network compose collections jointly train neural modules deep network question answer approach decompose question linguistic substructures use structure dynamically instantiate modular network reusable components recognize dog classify color etc result compound network jointly train evaluate approach two challenge datasets visual question answer achieve state art result vqa natural image dataset new dataset complex question abstract shape
aim extract information literary character unstructured texts employ natural language process reason domain ontologies first task identify main character part story character describe act illustrate system scenario folktale domain system rely folktale ontology develop base propp model folktales morphology
paper propose construction linguistic descriptions image achieve extraction scene description graph sdgs visual scenes use automatically construct knowledge base sdgs construct use vision reason specifically commonsense reason apply detections obtain exist perception methods give image b commonsense knowledge base construct use natural language process image annotations c lexical ontological knowledge resources wordnet amazon mechanical turkamt base evaluations flickr8k flickr30k ms coco datasets show case sentence auto construct sdgs obtain method give relevant thorough description image recent state art image caption base approach image sentence alignment evaluation result also comparable recent state art approach
much information sit unprecedented amount text data manage allocation large scale text data important problem many areas topic model perform well problem traditional generative model plsalda state art approach topic model recent research topic generation focus improve extend model however result traditional generative model sensitive number topics k must specify manually problem generate topics corpus resemble community detection network many effective algorithms automatically detect communities network without manually specify number communities inspire algorithms paper propose novel method name hierarchical latent semantic map hlsm automatically generate topics corpus hlsm calculate association pair word latent topic space construct unipartite network word association hierarchically generate topics network apply hlsm several document collections experimental comparisons several state art approach demonstrate promise performance
paper present model take input corpus image relevant speak caption find correspondence two modalities employ pair convolutional neural network model visual object speech signal word level tie network together embed alignment model learn joint semantic space modalities evaluate model use image search annotation task flickr8k dataset augment collect corpus forty thousand speak caption use amazon mechanical turk
ground ie localize arbitrary free form textual phrase visual content challenge problem many applications human computer interaction image text reference resolution datasets provide grind truth spatial localization phrase thus desirable learn data little ground supervision propose novel approach learn ground reconstruct give phrase use attention mechanism either latent optimize directly train approach encode phrase use recurrent network language model learn attend relevant image region order reconstruct input phrase test time correct attention ie ground evaluate ground supervision available directly apply via loss attention mechanism demonstrate effectiveness approach flickr 30k entities referitgame datasets different level supervision range supervision partial supervision full supervision supervise variant improve large margin state art datasets
text document structure multiple level detail individual word relate syntax larger units text relate discourse structure exist language model generally fail account discourse structure crucial language model reward coherence generate coherent texts present empirically evaluate set multi level recurrent neural network language model call document context language model dclm incorporate contextual information within beyond sentence comparison word level recurrent neural network language model dclm model obtain slightly better predictive likelihoods considerably better assessments document coherence
paper extend symbolic association framework able handle miss elements multimodal sequence general scope work symbolic associations object word mappings happen language development infants word two different representations abstract concepts associate directions scenario long interest artificial intelligence psychology neuroscience work extend recent approach multimodal sequence visual audio also cope miss elements one modalities method use two parallel long short term memories lstms learn rule base algorithm align lstm output via dynamic time warp dtw propose include extra step combination max operation exploit common elements sequence motivation behind combination act condition selector choose best representation lstms evaluate propose extension follow scenarios miss elements one modality visual audio miss elements modalities visual sound performance extension reach better result original model similar result individual lstm train modality
task associate image videos natural language description attract great amount attention recently rapid progress make term develop novel algorithms release new datasets indeed state art result standard datasets push regime become difficult make significant improvements instead propose new model work investigate possibility empirically establish performance upper bound various visual caption datasets without extra data label effort human evaluation particular assume visual caption decompose two step visual input visual concepts visual concepts natural language descriptions one would able obtain upper bind assume first step perfect require train conditional language model second step demonstrate construction bound ms coco youtube2text lsmdc combination vad mpii md surprisingly despite imperfect process use visual concept extraction first step simplicity language model second step show current state art model fall short compare learn upper bound furthermore bind quantify several important factor concern image video caption number visual concepts capture different model trade amount visual elements capture accuracy intrinsic difficulty bless different datasets
paper introduce novel architecture reinforcement learn deep neural network design handle state action space characterize natural language find text base game term deep reinforcement relevance network drrn architecture represent action state space separate embed vectors combine interaction function approximate q function reinforcement learn evaluate drrn two popular text game show superior performance deep q learn architectures experiment paraphrase action descriptions show model extract mean rather simply memorize string text
deep neural network achieve impressive supervise classification performance many task include image recognition speech recognition sequence sequence learn however success translate applications like question answer may involve complex arithmetic logic reason major limitation model inability learn even simple arithmetic logic operations example show neural network fail learn add two binary number reliably work propose neural programmer end end differentiable neural network augment small set basic arithmetic logic operations neural programmer call augment operations several step thereby induce compositional program complex build operations model learn weak supervision signal result execution correct program hence require expensive annotation correct program decisions operations call data segment apply infer neural programmer decisions train do differentiable fashion entire network train jointly gradient descent find train model difficult greatly improve add random noise gradient fairly complex synthetic table comprehension dataset traditional recurrent network attentional model perform poorly neural programmer typically obtain nearly perfect accuracy
sequence sequence model achieve impressive result various task however unsuitable task require incremental predictions make data arrive task long input sequence output sequence generate output sequence condition entire input sequence paper present neural transducer make incremental predictions input arrive without redo entire computation unlike sequence sequence model neural transducer compute next step distribution condition partially observe input sequence partially generate sequence time step transducer decide emit zero many output symbols data process use encoder present input transducer discrete decision emit symbol every time step make difficult learn conventional backpropagation however possible train transducer use dynamic program algorithm generate target discrete decisions experiment show neural transducer work well settings require produce output predictions data come also find neural transducer perform well long sequence even attention mechanisms use
study scalable uniform understand facts image exist visual recognition systems typically model differently fact type object action interactions propose set facts model simultaneously capacity understand unbounded number facts structure way train data come structure facts image include one object eg boy two attribute eg boy tall three action eg boy play four interactions eg boy rid horse fact semantic language view eg boy play visual view image fact show learn visual facts structure way enable uniform also generalizable visual understand propose investigate recent strong approach multiview learn literature also introduce two learn representation model potential baselines apply investigate methods several datasets augment structure facts large scale dataset two hundred and two thousand facts eight hundred and fourteen thousand image experiment show advantage relate facts structure propose model compare design baselines bidirectional fact retrieval
complex compositional structure language make problems intersection vision language challenge language also provide strong prior result good superficial performance without underlie model truly understand visual content hinder progress push state art computer vision aspects multi modal ai paper address binary visual question answer vqa abstract scenes formulate problem visual verification concepts inquire question specifically convert question tuple concisely summarize visual concept detect image concept find image answer question yes otherwise abstract scenes play two roles one allow us focus high level semantics vqa task oppose low level recognition problems perhaps importantly two provide us modality balance dataset language priors control role vision essential particular collect fine grain pair scenes every question answer question yes one scene exact question indeed language priors alone perform better chance balance dataset moreover propose approach match performance state art vqa approach unbalance dataset outperform balance dataset
address problem visual question answer vqa require joint image language understand answer question give photograph recent approach apply deep image caption methods base convolutional recurrent network problem fail model spatial inference remedy propose model call spatial memory network apply vqa task memory network recurrent neural network explicit attention mechanism select certain part information store memory spatial memory network store neuron activations different spatial regions image memory use question choose relevant regions compute answer process constitute single hop network propose novel spatial attention architecture align word image patch first hop obtain improve result add second attention hop consider whole question choose visual evidence base result first hop better understand inference process learn network design synthetic question specifically require spatial inference visualize attention weight evaluate model two publish visual question answer datasets daquar one vqa two obtain improve result compare strong deep baseline model ibowimg concatenate image question feature predict answer three
describe method learn word embeddings data dependent dimensionality stochastic dimensionality skip gram sd sg stochastic dimensionality continuous bag word sd cbow nonparametric analogs mikolov et al two thousand and thirteen well know word2vec model vector dimensionality make dynamic employ techniques use cote larochelle two thousand and sixteen define rbm infinite number hide units show qualitatively quantitatively sd sg sd cbow competitive fix dimension counterparts provide distribution embed dimensionalities offer window semantics distribute across dimension
order robots operate effectively home workplaces must able manipulate articulate object common within environments build humans previous work learn kinematic model prescribe manipulation visual demonstrations lingual signal natural language descriptions instructions offer complementary mean convey knowledge manipulation model suitable wide range interactions eg remote manipulation paper present multimodal learn framework incorporate visual lingual information estimate structure parameters define kinematic model articulate object visual signal take form rgb image stream opportunistically capture object motion unprepared scene accompany natural language descriptions motion constitute lingual signal present probabilistic language model use word embeddings associate lingual verbs correspond kinematic structure exploit complementary nature visual lingual input method infer correct kinematic structure various multiple part object previous state art visual system fail evaluate multimodal learn framework dataset comprise variety household object demonstrate thirty-six improvement model accuracy vision baseline
tackle image question answer imageqa problem learn convolutional neural network cnn dynamic parameter layer whose weight determine adaptively base question adaptive parameter prediction employ separate parameter prediction network consist gate recurrent unit gru take question input fully connect layer generate set candidate weight output however challenge construct parameter prediction network large number parameters fully connect dynamic parameter layer cnn reduce complexity problem incorporate hash technique candidate weight give parameter prediction network select use predefined hash function determine individual weight dynamic parameter layer propose network joint network cnn imageqa parameter prediction network train end end back propagation weight initialize use pre train cnn gru propose algorithm illustrate state art performance available public imageqa benchmarks
recent advance neural variational inference spawn renaissance deep latent variable model paper introduce generic variational inference framework generative conditional model text traditional variational methods derive analytic approximation intractable distributions latent variables construct inference network condition discrete text input provide variational distribution validate framework two different text model applications generative document model supervise question answer neural variational document model combine continuous stochastic document representation bag word generative model achieve lowest report perplexities two standard test corpora neural answer selection model employ stochastic representation layer within attention mechanism extract semantics question answer pair two question answer benchmarks model exceed previous publish benchmarks
variation language ubiquitous particularly newer form write social media fortunately variation random often link social properties author paper show exploit social network make sentiment analysis robust social language variation key idea linguistic homophily tendency socially link individuals use language similar ways formalize idea novel attention base neural network architecture attention divide among several basis model depend author position social network effect smooth classification function across social network make possible induce personalize classifiers even author label data demographic metadata model significantly improve accuracies sentiment analysis twitter review data
paper propose method learn joint embeddings image text use two branch neural network multiple layer linear projections follow nonlinearities network train use large margin objective combine cross view rank constraints within view neighborhood structure preservation constraints inspire metric learn literature extensive experiment show approach gain significant improvements accuracy image text text image retrieval method achieve new state art result flickr30k mscoco image sentence datasets show promise new task phrase localization flickr30k entities dataset
sequence sequence learn recently emerge new paradigm supervise learn date applications focus one task much work explore framework multiple task paper examine three multi task learn mtl settings sequence sequence model oneto many set encoder share several task machine translation syntactic parse b many one set useful decoder share case translation image caption generation c many many set multiple encoders decoders share case unsupervised objectives translation result show train small amount parse image caption data improve translation quality english german fifteen bleu point strong single task baselines wmt benchmarks furthermore establish new state art result constituent parse nine hundred and thirty f1 lastly reveal interest properties two unsupervised learn objectives autoencoder skip think mtl context autoencoder help less term perplexities bleu score compare skip think
hypernymy textual entailment image caption see special case single visual semantic hierarchy word sentence image paper advocate explicitly model partial order structure hierarchy towards goal introduce general method learn order representations show apply variety task involve image language show result representations improve performance current approach hypernym prediction image caption retrieval
sequence become first class citizens supervise learn thank resurgence recurrent neural network many complex task require map sequence observations formulate sequence sequence seq2seq framework employ chain rule efficiently represent joint probability sequence many case however variable size input output might naturally express sequence instance clear input set number model task sort similarly know organize output correspond random variables task model unknown joint probability paper first show use various examples order organize input output data matter significantly learn underlie model discuss extension seq2seq framework go beyond sequence handle input set principled way addition propose loss search possible order train deal lack structure output set show empirical evidence claim regard order modifications seq2seq framework benchmark language model parse task well two artificial task sort number estimate joint probability unknown graphical model
question answer qa fundamental natural language process nlp problems phrase qa kumar et al two thousand and fifteen current weakly supervise memory network model propose far struggle answer question involve relations among multiple entities facebook babi qa5 three arg relations weston et al two thousand and fifteen address problem learn multi argument multi hop semantic relations purpose qa propose method combine jointly learn long term read write memory attentive inference components end end memory network memn2n sukhbaatar et al two thousand and fifteen distribute sentence vector representations encode skip think model kiros et al two thousand and fifteen choice append skip think vectors exist memn2n framework motivate fact skip think vectors show accurately model multi argument semantic relations kiros et al two thousand and fifteen
propose general framework topic specific summarization large text corpora illustrate use analysis two quite different contexts osha database fatality catastrophe report facilitate surveillance pattern circumstances lead injury death legal decisions workers compensation claim explore relevant case law summarization framework build sparse classification methods compromise simple word frequency base methods currently wide use heavyweight model intensive methods latent dirichlet allocation lda particular topic interest eg mental health disability chemical reactions regress label document onto high dimensional count word phrase document result small set phrase find predictive harvest summary use branch bind approach method extend allow phrase arbitrary length allow potentially rich summarization discuss focus purpose summaries inform choices regularization parameters model constraints evaluate tool compare computational time summary statistics result word list three methods literature also present new r package textreg overall argue sparse methods much offer text analysis branch research consider context
propose blackout approximation algorithm efficiently train massive recurrent neural network language model rnnlms million word vocabularies blackout motivate use discriminative loss describe new sample strategy significantly reduce computation improve stability sample efficiency rate convergence one way understand blackout view extension dropout strategy output layer wherein use discriminative train loss weight sample scheme also establish close connections blackout importance sample noise contrastive estimation nce experiment recently release one billion word language model benchmark demonstrate scalability accuracy blackout outperform state art achieve lowest perplexity score dataset moreover unlike establish methods typically require gpus cpu cluster show carefully implement version blackout require one ten days single machine train rnnlm million word vocabulary billions parameters one billion word although describe blackout context rnnlm train use network large softmax output layer
recently develop software able gather information social network write texts software character place interaction network chaplin tool implement visual basic mean character place literary work extract list raw word software interface help users select name list set parameters chaplin create network nod represent character place edge give interactions nod edge label performances paper propose use chaplin analysis william shakespeare play famous tragedy hamlet prince denmark performances character play whole act give graph
recognition human activities one key problems video understand action recognition challenge even specific categories videos sport contain small set action interestingly sport videos accompany detail commentaries available online could use perform action annotation weakly supervise set specific case cricket videos address challenge temporal segmentation annotation ctions semantic descriptions solution consist two stag first stage video segment scenes utilize scene category information extract text commentary second stage consist classify video shots well phrase textual description various categories relevant phrase suitably map video shots novel aspect work fine temporal scale semantic information assign video result approach enable retrieval specific action last second several hours video solution yield large number label exemplars manual effort could use machine learn algorithms learn complex action
embed learn aka representation learn show able model large scale semantic knowledge graph key concept map knowledge graph tensor representation whose entries predict model use latent representations generalize entities latent variable model well suit deal high dimensionality sparsity typical knowledge graph recent publications embed model extend also consider time evolutions time pattern subsymbolic representations paper map embed model develop purely solutions technical problems model temporal knowledge graph various cognitive memory function particular semantic concept memory episodic memory sensory memory short term memory work memory discuss learn query answer path sensory input semantic decode relationship episodic memory semantic memory introduce number hypotheses human memory derive develop mathematical model
match natural language sentence central many applications information retrieval question answer exist deep model rely single sentence representation multiple granularity representations match however methods well capture contextualized local information match process tackle problem present new deep architecture match two sentence multiple positional sentence representations specifically positional sentence representation sentence representation position generate bidirectional long short term memory bi lstm match score finally produce aggregate interactions different positional sentence representations k max pool multi layer perceptron model several advantage one use bi lstm rich context whole sentence leverage capture contextualized local information positional sentence representation two match multiple positional sentence representations flexible aggregate different important contextualized local information sentence support match three experiment different task question answer sentence completion demonstrate superiority model
paper attempt classify tweet root categories amazon browse node hierarchy use set tweet browse node id label much larger set tweet without label set amazon review examine twitter data present unique challenge sample short one hundred and forty character often contain misspell abbreviations trivial human decipher difficult computer parse variety query document expansion techniques implement effort improve information retrieval modest success
name entity recognition challenge task traditionally require large amount knowledge form feature engineer lexicons achieve high performance paper present novel neural network architecture automatically detect word character level feature use hybrid bidirectional lstm cnn architecture eliminate need feature engineer also propose novel method encode partial lexicon match neural network compare exist approach extensive evaluation show give tokenized text publicly available word embeddings system competitive conll two thousand and three dataset surpass previously report state art performance ontonotes fifty dataset two hundred and thirteen f1 point use two lexicons construct publicly available source establish new state art performance f1 score nine thousand, one hundred and sixty-two conll two thousand and three eight thousand, six hundred and twenty-eight ontonotes surpass systems employ heavy feature engineer proprietary lexicons rich entity link information
stabilize activations recurrent neural network rnns penalize square distance successive hide state norms penalty term effective regularizer rnns include lstms irnns improve performance character level language model phoneme recognition outperform weight noise dropout achieve competitive performance one hundred and eighty-six per timit phoneme recognition task rnns evaluate without beam search rnn transducer penalty term irnn achieve similar performance lstm language model although add penalty term lstm result superior performance penalty term also prevent exponential growth irnn activations outside train horizon allow generalize much longer sequence
human language recognize complex domain since decades computer system able reach human level performance far know computational system capable proper language process human brain gather data brain fundamental computational process still remain obscure lack sound computational brain theory also prevent fundamental understand natural language process always science lack theoretical foundation statistical model apply accommodate many sample real world data possible unsolved fundamental issue actual representation language data within brain denote representational problem start jeff hawkins hierarchical temporal memory htm theory consistent computational theory human cortex develop correspond theory language data representation semantic fold theory process encode word use topographic semantic space distributional reference frame sparse binary representational vector call semantic fold central topic document semantic fold describe method convert language symbolic representation text explicit semantically ground representation generically process hawkins htm network turn change representation solve many complex nlp problems apply boolean operators generic similarity function like euclidian distance many practical problems statistical nlp systems like high cost computation fundamental incongruity precision recall complex tune procedures etc elegantly overcome apply semantic fold
automate sentiment analysis opinion mine complex process concern extraction useful subjective information text explosion user generate content web especially fact millions users daily basis express opinions products service blog wikis social network message board etc render reliable automate export sentiments opinions unstructured text crucial several commercial applications paper present novel hybrid vectorization approach textual resources combine weight variant popular word2vec representation base term frequency inverse document frequency representation bag word representation vector lexicon base sentiment value propose text representation approach assess application several machine learn classification algorithms dataset use extensively literature sentiment detection classification accuracy derive propose hybrid vectorization approach higher individual components use text represenation comparable state art sentiment detection methodologies
paper consider aspect base opinion summarization aos review particular products enable real applications aos system need address two core subtasks aspect extraction sentiment classification exist approach aspect extraction use linguistic analysis topic model general across different products precise enough suitable particular products instead take less general precise scheme directly map review sentence pre define aspects tackle aspect map sentiment classification propose two convolutional neural network cnn base methods cascade cnn multitask cnn cascade cnn contain two level convolutional network multiple cnns level one deal aspect map task single cnn level two deal sentiment classification multitask cnn also contain multiple aspect cnns sentiment cnn different network share word embeddings experimental result indicate cascade multitask cnns outperform svm base methods large margins multitask cnn generally perform better cascade cnn
people typically learn exposure visual concepts associate linguistic descriptions instance teach visual object categories children often accompany descriptions text speech machine learn context observations motivate us ask whether learn process could computationally model learn visual classifiers specifically main question work utilize purely textual description visual class train image learn explicit visual classifiers propose investigate two baseline formulations base regression domain transfer predict linear classifier propose new constrain optimization formulation combine regression function knowledge transfer function additional constraints predict parameters linear classifier also propose generic kernelized model kernel classifier predict form define representer theorem kernelized model allow define utilize two rkhs reproduce kernel hilbert space kernel function visual space text space respectively finally propose kernel function unstructured text descriptions build distributional semantics show advantage set could useful applications apply study model predict visual classifiers two fine grain challenge categorization datasets cu bird flower datasets result indicate successful predictions final model several baselines design
ability describe image natural language sentence hallmark image language understand system wide range applications annotate image use natural sentence search imagesin project focus task bidirectional image retrieval asystem capable retrieve image base sentence image search andretrieve sentence base image query image annotation present asystem base global rank objective function use combinationof convolutional neural network cnn multi layer perceptrons mlpit take pair image sentence process different channelsfinally embed common multimodal vector space embeddingsencode abstract semantic information two input comparedusing traditional information retrieval approach pair modelreturns score interpretted similarity metric score highthe image sentence likely convey similar mean score low likely visual input model via deep convolutional neural network theother hand explore three model textual module first one isbag word mlp second one use n grams bigram trigramsand combination trigram skip grams mlp third morespecialized deep network specific model variable length sequence ssewe report comparable performance recent work field even though ouroverall model simpler also show train time choice wecan generate negative sample significant impact performance use specialize bi directional system one particular task
populate database unstructured information long stand problem industry research encompass problems extraction clean integration recent name use problem include deal dark data knowledge base construction kbc work describe deepdive system combine database machine learn ideas help develop kbc systems present techniques make kbc process efficient observe kbc process iterative develop techniques incrementally produce inference result kbc systems propose two methods incremental inference base respectively sample variational techniques also study tradeoff space methods develop simple rule base optimizer deepdive include contributions evaluate deepdive five kbc systems show speed kbc inference task two order magnitude negligible impact quality
originally inspire categorical quantum mechanics abramsky coecke lics four categorical compositional distributional model natural language mean coecke sadrzadeh clark provide conceptually motivate procedure compute mean sentence give grammatical structure within lambek pregroup vectorial representation mean part predictions first model outperform model mainstream empirical language process task large scale data moreover like cqm allow vary model interpret quantum axioms one also vary model interpret word mean paper show developments categorical quantum mechanics relevant natural language process firstly selinger cpm construction allow explicitly take account lexical ambiguity distinguish two inherently different notions homonymy polysemy term model interpret word mean mean passage vector space model density matrices despite change model standard empirical methods compare mean easily adopt demonstrate small scale experiment real world data experiment moreover provide preliminary evidence validity propose new model word mean secondly commutative classical structure well non commutative counterparts arise image cpm construction allow encode relative pronouns verbs adjectives finally iteration cpm construction something counterpart quantum realm enable one accommodate entailment ambiguity
paper describe resource system build efforts eight week johns hopkins university human language technology center excellence summer camp apply language exploration scale two thousand and nine semantically inform machine translation simt describe new modality negation mn annotation scheme creation publicly available mn lexicon two automate mn taggers build use annotation scheme lexicon annotation scheme isolate three components modality negation trigger word convey modality negation target action associate modality negation holder experiencer modality describe mn lexicon semi automatically produce demonstrate structure base mn tagger result precision around eighty-six depend genre tag standard ldc data set apply mn annotation scheme statistical machine translation use syntactic framework support inclusion semantic annotations syntactic tag enrich semantic annotations assign parse tree target language train texts process tree graft focus work modality negation tree graft procedure general support type semantic information exploit capability include name entities produce pre exist tagger addition mn elements produce taggers describe paper result system significantly outperform linguistically naive baseline model hiero reach highest score yet report nist two thousand and nine urdu english test set find support hypothesis syntactic semantic information improve translation quality
base aristotelian concept potentiality vs actuality allow study energy dynamics language propose field approach lexical analysis fall back distributional hypothesis statistically model word mean use evolve field metaphor express time dependent change vector space model combination random index evolve self organize map esom monitor semantic drift within observation period experiment carry term space collection one hundred and twenty-eight million amazon book review evaluation semantic consistency esom term cluster compare respective neighbourhoods wordnet contrast distance among term vectors random index find five level significance term cluster show high level semantic consistency track drift distributional pattern term space across time periods find consistency decrease statistically significant level method highly scalable interpretations philosophy
semantic word embeddings represent mean word via vector create diverse methods many use nonlinear operations co occurrence statistics hand tune hyperparameters reweighting methods paper propose new generative model dynamic version log linear topic model ofcitetmnih2007three methodological novelty use prior compute close form expressions word statistics provide theoretical justification nonlinear model like pmi word2vec glove well hyperparameter choices also help explain low dimensional semantic embeddings contain linear algebraic structure allow solution word analogies show bycitetmikolov2013efficient many subsequent paper experimental support provide generative model assumptions important latent word vectors fairly uniformly disperse space
topic model textual corpora important challenge problem previous work bag word assumption usually make ignore order word assumption simplify computation unrealistically lose order information semantic word context paper present gaussian mixture neural topic model gmntm incorporate order word semantic mean sentence topic model specifically represent topic cluster multi dimensional vectors embed corpus collection vectors generate gaussian mixture model word affect topic also embed vector surround word context gaussian mixture components topic document sentence word learn jointly extensive experiment show model learn better topics accurate word distributions topic quantitatively compare state art topic model approach gmntm obtain significantly better performance term perplexity retrieval accuracy classification accuracy
background amount biomedical literature rapidly grow become increasingly difficult keep manually curated knowledge base ontologies date study apply word2vec deep learn toolkit medical corpora test potential identify relationships unstructured text evaluate efficiency word2vec identify properties pharmaceuticals base mid size unstructured medical text corpora available web properties include relationships diseases may treat physiological process physiological effect compare relationships identify word2vec manually curated information national drug file reference terminology ndf rt ontology gold standard result result reveal maximum accuracy four thousand, nine hundred and twenty-eight suggest limit ability word2vec capture linguistic regularities collect medical corpora compare publish result able document influence different parameter settings result accuracy find unexpected trade rank quality accuracy pre process corpora reduce syntactic variability prove good strategy increase utility train vector model conclusions word2vec efficient implementation compute vector representations ability identify relationships textual data without prior domain knowledge find rank retrieve result generate word2vec sufficient quality automatic population knowledge base ontologies could serve start point manual curation
electronic health record capture patient information use structure control vocabularies unstructured narrative text structure data typically encode lab value encounter medication list unstructured data capture physician interpretation patient condition prognosis response therapeutic intervention paper demonstrate information extraction unstructured clinical narratives essential clinical applications perform empirical study validate argument show structure data alone insufficient resolve eligibility criteria recruit patients onto clinical trials chronic lymphocytic leukemia cll prostate cancer unstructured data essential solve fifty-nine cll trial criteria seventy-seven prostate cancer trial criteria specifically resolve eligibility criteria temporal constraints show need temporal reason information integration medical events within across unstructured clinical narratives structure data
low dimensional representations word allow accurate nlp model train limit annotate data representations ignore word local context natural way induce context dependent representations perform inference probabilistic latent variable sequence model give recent success continuous vector space word representations provide inference procedure continuous state word representations give posterior mean linear dynamical system efficient inference perform use kalman filter learn algorithm extremely scalable operate simple cooccurrence count parameter initialization use method moments subsequent iterations experiment employ infer word embeddings feature standard tag task obtain significant accuracy improvements finally kalman filter update see linear recurrent neural network demonstrate use parameters model initialize non linear recurrent neural network language model reduce train time day yield lower perplexity
paper investigate problem arabic name write multiple ways someone search one form name neither exact approximate match appropriate return multiple variants name exact match require user enter form name search approximate match yield name among variations one seek paper attempt solve problem dictionary arabic name map different alternative write form generate alternatives base rule derive review first name ninety-nine million citizens former citizens jordan dictionary use standardize write form insert new name database search name alternative write form create dictionary automatically base rule result least seven erroneous acceptance errors seventy-nine erroneous rejection errors address errors manually edit dictionary dictionary help real world databases qualification manual edit guarantee one hundred correctness
last five years flurry work information extraction clinical document ie algorithms capable extract informal unstructured texts generate everyday clinical practice mention concepts relevant practice literature methods base supervise learn ie methods train information extraction system manually annotate examples lot work devote devise learn methods generate accurate information extractors work devote investigate effect quality train data learn process low quality train data often derive fact person annotate data different one whose judgment automatically annotate data must evaluate paper test impact data quality issue accuracy information extraction systems apply clinical domain compare accuracy derive train data annotate authoritative coder ie one also annotate test data whose judgment must abide accuracy derive train data annotate different coder result indicate although disagreement two coders measure train set substantial difference surprisingly enough always statistically significant
one long term goal machine learn research produce methods applicable reason natural language particular build intelligent dialogue agent measure progress towards goal argue usefulness set proxy task evaluate read comprehension via question answer task measure understand several ways whether system able answer question via chain facts simple induction deduction many task design prerequisites system aim capable converse human believe many exist learn systems currently solve hence aim classify task skill set researchers identify rectify fail systems also extend improve recently introduce memory network model show able solve task
normalize web distance nwd similarity normalize semantic distance base world wide web another large electronic database instance wikipedia search engine return reliable aggregate page count set search term nwd give common similarity common semantics scale zero identical one completely different nwd approximate similarity members set accord upper semicomputable properties develop theory give applications classify use amazon wikipedia ncbi website national institute health last give new correlations health hazard restriction nwd set two yield earlier normalize google distance ngd combination ngd pair set extract information nwd extract set nwd enable new contextual different databases learn approachbased kolmogorov complexity theory incorporate knowledge databases
paper use natural language process create first machine cod democracy index call automate democracy score ads ads base forty-two million news article six thousand and forty-three different source cover independent countries one thousand, nine hundred and ninety-three two thousand and twelve period unlike democracy indices today ads replicable standard errors small enough actually distinguish case ads produce supervise learn three approach try combination latent semantic analysis tree base regression methods b combination latent dirichlet allocation tree base regression methods c wordscores algorithm wordscores algorithm outperform alternatives one ads base web application anyone change train set see result change democracy scoresorg
paper develop model address sentence embed hot topic current natural language process research use recurrent neural network long short term memory lstm cells due ability capture long term memory lstm rnn accumulate increasingly richer information go sentence reach last word hide layer network provide semantic representation whole sentence paper lstm rnn train weakly supervise manner user click data log commercial web search engine visualization analysis perform understand embed process work model find automatically attenuate unimportant word detect salient keywords sentence furthermore detect keywords find automatically activate different cells lstm rnn word belong similar topic activate cell semantic representation sentence embed vector use many different applications automatic keyword detection topic allocation abilities enable lstm rnn allow network perform document retrieval difficult language process task similarity query document measure distance correspond sentence embed vectors compute lstm rnn web search task lstm rnn embed show significantly outperform several exist state art methods emphasize propose model generate sentence embed vectors specially useful web document retrieval task comparison well know general sentence embed method paragraph vector perform result show propose method paper significantly outperform web document retrieval task
author name ambiguity decrease quality reliability information retrieve digital libraries exist methods try solve problem predefining feature set base expert knowledge specific dataset paper propose new approach use deep neural network learn feature automatically data additionally propose general system architecture author name disambiguation dataset research evaluate propose method dataset contain vietnamese author name result show method significantly outperform methods use predefined feature set propose method achieve nine thousand, nine hundred and thirty-one term accuracy prediction error rate decrease one hundred and eighty-three sixty-nine ie decrease one hundred and fourteen six hundred and twenty-three relatively compare methods use predefined feature set table three
work propose new approach discover various relationships among keywords scientific publications base markov chain model important problem since keywords basic elements represent abstract object document user profile topics many things else model effective since combine four important factor scientific publications content publicity impact randomness particularly recommendation system call scirecsys present support users efficiently find relevant article
basic interest quantification long term growth language lexicon develop completely cover culture communication requirements knowledge space explore usage dynamics word english language reflect google book two thousand and twelve english fiction corpus critique earlier method find decrease birth increase death rat word second half 20th century show death rat strongly affect impose time cutoff arbitrary present increase dramatically provide robust principled approach examine lexical evolution track volume word flux across various relative frequency thresholds show overall statistical structure english language remain stable time term raw zipf distribution find evidence endure lexical turbulence flux word across frequency thresholds decade decade scale superlinearly word rank exhibit scale break connect zipf law better understand change lexicon examine contributions jensen shannon divergence individual word cross frequency thresholds also find indications scholarly work fiction strongly represent two thousand and twelve english fiction corpus suggest future revision corpus attempt separate critical work fiction
paper cover two approach sentiment analysis lexicon base method ii machine learn method describe several techniques implement approach discuss adopt sentiment classification twitter message present comparative study different lexicon combinations show enhance sentiment lexicons emoticons abbreviations social media slang expressions increase accuracy lexicon base classification twitter discuss importance feature generation feature selection process machine learn sentiment classification quantify performance main sentiment analysis methods twitter run algorithms benchmark twitter dataset semeval two thousand and thirteen competition task two b result show machine learn method base svm naive bay classifiers outperform lexicon method present new ensemble method use lexicon base sentiment score input feature machine learn approach combine method prove produce precise classifications also show employ cost sensitive classifier highly unbalance datasets yield improvement sentiment classification performance seven
word order evolution hypothesize constrain word order permutation ring transition involve order closer permutation ring likely hypothesis see particular case kauffman adjacent possible word order evolution consider problem association six possible order v yield couple primary alternate order window word order evolution evaluate suitability various compete hypotheses predict one member couple help information theoretic model selection ensemble model include six way model base word order permutation ring kauffman adjacent possible another model base dual two way standard typology reduce word order basic order preferences eg preference sv vs another os analysis indicate permutation ring yield best model favor parsimony strongly provide support kauffman general view six way typology
past decade several areas speech language understand witness substantial breakthroughs use data drive model area dialogue systems trend less obvious practical systems still build significant engineer expert knowledge nevertheless several recent result suggest data drive approach feasible quite promise facilitate research area carry wide survey publicly available datasets suitable data drive learn dialogue systems discuss important characteristics datasets use learn diverse dialogue strategies potential use also examine methods transfer learn datasets use external knowledge finally discuss appropriate choice evaluation metrics learn objective
paper extend deep long short term memory dlstm recurrent neural network introduce gate direct connections memory cells adjacent layer direct link call highway connections enable unimpeded information flow across different layer thus alleviate gradient vanish problem build deeper lstms introduce latency control bidirectional lstms blstms exploit whole history keep latency control efficient algorithms propose train novel network use frame sequence discriminative criteria experiment ami distant speech recognition dsr task indicate train deeper lstms achieve better improvement sequence train highway lstms hlstms novel model obtain four hundred and thirty-nine four hundred and seventy-seven wer ami sdm dev eval set outperform previous work beat strong dnn dlstm baselines one hundred and fifty-seven fifty-three relative improvement respectively
thesis contribute ongoing research relate categorical compositional model natural language coecke sadrzadeh clark three ways firstly propose concrete instantiation abstract framework base frobenius algebras joint work sadrzadeh theory improve shortcomings previous proposals extend coverage language support experimental work improve exist result propose framework describe new class compositional model find intuitive interpretations number linguistic phenomena secondly propose evaluate practice new compositional methodology explicitly deal different level lexical ambiguity joint work pulman concrete algorithm present base separation vector disambiguation composition explicit prior step extensive experimental work show propose methodology indeed result accurate composite representations framework coecke et al particular every class compositional model general last contribution formalize explicit treatment lexical ambiguity context categorical framework resort categorical quantum mechanics joint work coecke propose extension concept distributional vector replace density matrix compactly represent probability distribution potential different mean specific word composition take form quantum measurements lead interest analogies quantum physics linguistics
search environment uncertain distribution resources involve trade exploitation past discoveries exploration extend information forage knowledge seeker shift read depth study new domains study decision make process examine read choices make one celebrate scientists modern era charles darwin full text book list chronologically organize read journals generate topic model quantify local text text global text past read decisions use kullback liebler divergence cognitively validate information theoretic measure relative surprise rather pattern surprise minimization correspond pure exploitation strategy darwin behavior shift early exploitation later exploration seek unusually high level cognitive surprise relative previous eras shift detect unsupervised bayesian model correlate major intellectual epochs career identify qualitative scholarship darwin self commentary methods allow us compare consumption texts publication order find darwin consumption exploratory culture production suggest underneath gradual societal change explorations individual synthesis discovery quantitative methods advance study cognitive search framework test interactions individual collective behavior short long term consumption choices novel application topic model characterize individual read complement widespread study collective scientific behavior
learn feature representations sub phoneme posteriors deep neural network dnns use separately produce significant performance gain speaker language recognition task work show gain possible use single dnn speaker language recognition unify dnn approach show yield substantial performance improvements two thousand and thirteen domain adaptation challenge speaker recognition task fifty-five reduction ever domain condition nist two thousand and eleven language recognition evaluation forty-eight reduction ever 30s test condition
languages across world exhibit zipf law abbreviation namely frequent word tend shorter generalize version law inverse relationship frequency unit magnitude hold also behaviours species genetic code apparent universality pattern human language ubiquity domains call theoretical understand origins end generalize information theoretic concept mean code length mean energetic cost function probability magnitude type repertoire show minimization cost function negative correlation probability magnitude type intimately relate
propose neural sequence sequence model direction follow task essential realize effective autonomous agents alignment base encoder decoder model long short term memory recurrent neural network lstm rnn translate natural language instructions action sequence base upon representation observable world state introduce multi level aligner empower model focus sentence regions salient current world state use multiple abstractions input sentence contrast exist methods model use specialize linguistic resources eg parsers task specific annotations eg seed lexicons therefore generalizable yet still achieve best result report date benchmark single sentence dataset competitive result limit train multi sentence set analyze model series ablations elucidate contributions primary components model
interpersonal relations fickle close friendships often dissolve enmity work explore linguistic cue presage transition study dyadic interactions online strategy game players form alliances break alliances betrayal characterize friendships unlikely last examine temporal pattern foretell betrayal reveal subtle sign imminent betrayal encode conversational pattern dyad even victim aware relationship fate particular find last friendships exhibit form balance manifest language contrast sudden change balance certain conversational attribute positive sentiment politeness focus future plan signal impend betrayal
categorical compositional distributional model coecke et al two thousand and ten suggest way combine grammatical composition formal type logical model corpus base empirical word representations distributional semantics paper contribute project expand model also capture entailment relations achieve extend representations word point mean space density operators probability distributions subspaces space symmetric measure similarity asymmetric measure entailment define lexical entailment measure use von neumann entropy quantum variant kullback leibler divergence lexical entailment combine composition map word representations provide method obtain entailment relations level sentence truth theoretic corpus base examples provide
well know speaker verification systems subject spoof attack automatic speaker verification spoof countermeasures challenge asvspoof2015 provide standard spoof database contain attack base synthetic speech along protocol experiment paper describe cpqd systems submit asvspoof2015 challenge base deep neural network work classifier feature extraction module gmm svm classifier result show validity approach achieve less five ever know attack
present unify framework support ground natural language semantics robotic drive framework support acquisition learn ground mean nouns prepositions human annotation robotic drive paths generation use acquire mean generate sentential description new robotic drive paths comprehension use acquire mean support automate drive accomplish navigational goals specify natural language evaluate performance three task independent human judge rate semantic fidelity sentence associate paths achieve overall average correctness nine hundred and forty-six overall average completeness eight hundred and fifty-six
paper map large scale variation spanish language employ corpus base geographically tag twitter message lexical dialects extract analysis variants tens concepts result map show linguistic variation unprecedented scale across globe discuss properties main dialects within machine learn approach find varieties speak urban areas international character contrast country areas dialects show regional uniformity
recent progress use recurrent neural network rnns image description motivate exploration application video description however image static work videos require model dynamic temporal structure properly integrate information natural language description context propose approach successfully take account local global temporal structure videos produce descriptions first approach incorporate spatial temporal three convolutional neural network three cnn representation short temporal dynamics three cnn representation train video action recognition task produce representation tune human motion behavior second propose temporal attention mechanism allow go beyond local temporal model learn automatically select relevant temporal segment give text generate rnn approach exceed current state art bleu meteor metrics youtube2text dataset also present result new larger challenge dataset pair video natural language descriptions
f measure f score one commonly use single number measure information retrieval natural language process machine learn base mistake flaw assumptions render unsuitable use contexts fortunately better alternatives
advance state art biomolecular interaction extraction three contributions show deep abstract mean representations amr significantly improve accuracy biomolecular interaction extraction system compare baseline rely solely surface syntax base feature ii contrast previous approach infer relations sentence sentence basis expand framework enable consistent predictions set sentence document iii modify expand graph kernel learn framework enable concurrent exploitation automatically induce amr semantic dependency structure syntactic representations experiment show approach yield interaction extraction systems robust environments significant mismatch train test condition