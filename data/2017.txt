use social media grow dramatically last years users follow informal languages communicate social media language communication often mix nature people transcribe regional language english technique find extremely popular natural language process nlp aim infer information text part speech pos tag play important role get prosody write text task pos tag code mix indian social media text develop supervise system base conditional random field classifier order tackle problem effectively focus extract rich linguistic feature participate three different language pair ie english hindi english bengali english telugu three different social media platforms twitter facebook whatsapp propose system able successfully assign coarse well fine grain pos tag label give code mix sentence experiment show system quite generic show encourage performance level three language pair domains
paper address task amr text generation leverage synchronous node replacement grammar train graph string rule learn use heuristic extraction algorithm test time graph transducer apply collapse input amrs generate output sentence evaluate semeval two thousand and sixteen task eight method give bleu score two thousand, five hundred and sixty-two best report far
wikipedia article represent entity topic different language editions evolve independently within scope language specific user communities lead different point view reflect article well complementary inconsistent information analysis information propagate across wikipedia language editions provide important insights article evolution along temporal cultural dimension support quality control facilitate analysis present multiwiki novel web base user interface provide overview similarities differences across article pair originate different language editions timeline multiwiki enable users observe change interlingual article similarity time perform detail visual comparison article snapshots particular time point
natural language inherently discrete symbolic representation human knowledge recent advance machine learn ml natural language process nlp seem contradict intuition discrete symbols fade away erase vectors tensors call distribute distributional representations however strict link distribute distributional representations discrete symbols first approximation second clearer understand strict link distribute distributional representations symbols may certainly lead radically new deep learn network paper make survey aim renew link symbolic representations distribute distributional representations right time revitalize area interpret discrete symbols represent inside neural network
accurate prediction suitable discourse connectives however furthermore etc key component system aim build coherent fluent discourse shorter sentence passages example dialog system might assemble long informative answer sample passages extract different document retrieve web formulate task discourse connective prediction release dataset 29m sentence pair separate discourse connectives task evaluate hardness task human raters apply recently propose decomposable attention da model task observe automatic predictor higher f1 human raters thirty-two vs thirty nevertheless specific condition raters still outperform da model suggest headroom future improvements
propose novel discriminative model learn embeddings multilingual multi modal data mean model take advantage image descriptions multiple languages improve embed quality end introduce modification pairwise contrastive estimation optimisation function train objective evaluate embeddings image sentence rank isr semantic textual similarity sts neural machine translation nmt task find additional multilingual signal lead improvements isr sts task discriminative cost also use rank n best list produce nmt model yield strong improvements
neural machine translation nmt model able partially learn syntactic information sequential lexical information still complex syntactic phenomena prepositional phrase attachment poorly model work aim answer two question one explicitly model target language syntax help nmt two tight integration word syntax better multitask train introduce syntactic information form ccg supertags decoder interleave target supertags word sequence result wmt data show explicitly model target syntax improve machine translation quality german english high resource pair romanian english low resource pair also several syntactic phenomena include prepositional phrase attachment furthermore tight couple word syntax improve translation quality multitask train combine target syntax add source side dependency label embed layer obtain total improvement nine bleu german english twelve bleu romanian english
introduce multi modal neural machine translation model doubly attentive decoder naturally incorporate spatial visual feature obtain use pre train convolutional neural network bridge gap image description translation decoder learn attend source language word part image independently mean two separate attention mechanisms generate word target language find model efficiently exploit back translate domain multi modal data also large general domain text mt corpora also report state art result multi30k data set
acoustic unit discovery aud process automatically identify categorical acoustic unit inventory speech produce correspond acoustic unit tokenizations aud provide important avenue unsupervised acoustic model train zero resource set expert provide linguistic knowledge transcribe speech unavailable therefore facilitate zero resource aud process paper demonstrate acoustic feature representations significantly improve perform linear discriminant analysis lda unsupervised self train fashion ii leverage resources languages build multilingual bottleneck bn feature extractor give effective cross lingual generalization moreover perform comprehensive evaluations aud efficacy multiple downstream speech applications correlate performance suggest aud evaluations feasible use different alternative language resources subset evaluation resources available typical zero resource applications
prepositions highly polysemous variegate sense encode significant semantic information paper match preposition complement attachment interplay crucially geometry word vectors leave right preposition extract feature vast number instance preposition cluster make efficient preposition sense disambigution psd algorithm comparable better state art two benchmark datasets reliance external linguistic resource allow us scale psd algorithm large wikicorpus learn sense specific preposition representations show encode semantic relations paraphrase verb particle compound via simple vector operations
present opinion recommendation novel task jointly predict custom review rat score certain user would give certain product service give exist review rat score product service users review user give products service characteristic opinion recommendation reliance multiple data source multi task joint learn strength neural model use single neural network model users products capture correlation generate customise product representations use deep memory network customise rat review construct jointly result show opinion recommendation system give rat closer real user rat yelpcom data compare yelp rat methods give better result compare several pipelines baselines use state art sentiment rat summarization systems
fundamental challenge develop semantic parsers paucity strong supervision form language utterances annotate logical form paper propose exploit structural regularities language different domains train semantic parsers multiple knowledge base kbs share information across datasets find substantially improve parse accuracy train single sequence sequence model multiple kbs provide encode domain decode time model achieve state art performance overnight dataset contain eight domains improve performance single kb baseline seven hundred and fifty-six seven hundred and ninety-six obtain 7x reduction number model parameters
paper extend combine approach phrase base statistical machine translation smt example base mt ebmt rule base mt rbmt propose develop novel hybrid data drive mt system capable outperform baseline smt ebmt rbmt systems derive short propose hybrid mt process guide rule base mt get set partial candidate translations provide ebmt smt subsystems previous work show ebmt systems capable outperform phrase base smt systems rbmt approach strength generate structurally morphologically accurate result hybrid approach increase fluency accuracy grammatical precision improve quality machine translation system comparison propose hybrid machine translation htm model renowned translators ie google bing babylonian also present show propose model work better sentence ambiguity well comprise idioms others
paper present simple robust almost unsupervised dictionary base method qwn ppv q wordnet personalize pageranking vector automatically generate polarity lexicons show qwn ppv outperform automatically generate lexicons four extrinsic evaluations present also show competitive robust result respect manually annotate ones result suggest single lexicon best every task dataset intrinsic evaluation polarity lexicons good performance indicator sentiment analysis task qwn ppv method allow easily create quality polarity lexicons whenever domain base annotate corpora available give language
paper propose exploit automatic quality estimation qe asr hypotheses perform unsupervised adaptation deep neural network model acoustic probabilities hypothesis significant improvements achieve iautomatically transcribe evaluation data currently try recognise ii select subset good quality instance base word error rate wer score predict qe component validate hypothesis run several experiment evaluation data set release chime three challenge first operate oracle condition manual transcriptions evaluation data available thus allow us compute true sentence wer scenario perform adaptation variable amount data characterise different level quality move realistic condition manual transcriptions evaluation data available case adaptation perform data select accord wer score predict qe component result indicate qe predictions allow us closely approximate adaptation result obtain oracle condition ii overall asr performance base propose qe drive adaptation method significantly better strong recent chime three baseline
aspect base sentiment analysis exist methods either focus aspect opinion term extraction aspect term categorization however task provide partial information end users generate detail structure opinion analysis propose finer grain problem call category specific aspect opinion term extraction problem involve identification aspect opinion term within sentence well categorization identify term end propose end end multi task attention model task correspond aspect opinion term extraction specific category model benefit explore commonalities relationships among different task address data sparsity issue demonstrate state art performance three benchmark datasets
knowledge distillation describe method train student network perform better learn stronger teacher network translate sentence neural machine translation nmt engine time expensive smaller model speed process demonstrate transfer translation quality ensemble oracle bleu teacher network single nmt system present translation improvements teacher network architecture dimension student network train student model still expensive introduce data filter method base knowledge teacher model speed train also lead better translation quality techniques need code change easily reproduce nmt architecture speed decode process
basic concept neural machine translation nmt train large neural network maximize translation performance give parallel corpus nmt use simple leave right beam search decoder generate new translations approximately maximize train conditional probability current beam search strategy generate target sentence word word leave right keep fix amount active candidates time step first simple search less adaptive also expand candidates whose score much worse current best secondly expand hypotheses within best score candidates even score close best one latter one avoid increase beam size performance improvement observe reach better performance draw back slower decode speed paper concentrate speed decoder apply flexible beam search strategy whose candidate size may vary time step depend candidate score speed original decoder forty-three two language pair german english chinese english without lose translation quality
reference crucial property language allow us connect linguistic expressions world model require handle continuous discrete aspects mean data drive model excel former struggle latter reverse true symbolic model paper introduce concrete referential task test aspects call cross modal entity track b propose neural network architecture use external memory build entity library inspire drss drt mechanism dynamically introduce new referents add information referents already library model show promise beat traditional neural network architectures task however still outperform memory network another model external memory
writer style depend personal traits also intent mental state paper show variants write task lead measurable differences write style present case study base story cloze task mostafazadeh et al 2016a annotators assign similar write task different constraints one write entire story two add story end give story context three add incoherent end story show simple linear classifier inform stylistic feature able successfully distinguish among three case without even look story context addition combine stylistic feature language model predictions reach state art performance story cloze challenge result demonstrate different task frame dramatically affect way people write
deep neural network dnn revolutionize field natural language process nlp convolutional neural network cnn recurrent neural network rnn two main type dnn architectures widely explore handle various nlp task cnn suppose good extract position invariant feature rnn model units sequence state art many nlp task often switch due battle cnns rnns work first systematic comparison cnn rnn wide range representative nlp task aim give basic guidance dnn selection
neural network model capable generate extremely natural sound conversational interactions nevertheless model yet demonstrate incorporate content form factual information entity ground opinion would enable serve task orient conversational applications paper present novel fully data drive knowledge ground neural conversation model aim produce contentful responses without slot fill generalize widely use seq2seq approach condition responses conversation history external facts allow model versatile applicable open domain set approach yield significant improvements competitive seq2seq baseline human judge find output significantly informative
paper present supervise aspect base sentiment analysis absa system aim develop modular platform allow easily conduct experiment replace modules add new feature obtain best result opinion target extraction ote task slot two use shelf sequence labeler target polarity classification slot three address mean multiclass svm algorithm include lexical base feature polarity value obtain domain open polarity lexicons system obtain accuracies seventy seventy-three restaurant laptop domain respectively perform second best domain hotel achieve accuracy eighty
today many practitioners run basic nlp entire web large volume traffic faster methods paramount save time energy cost recent advance gpu hardware lead emergence bi directional lstms standard method obtain per token vector representations serve input label task ner often follow prediction linear chain crf though expressive accurate model fail fully exploit gpu parallelism limit computational efficiency paper propose faster alternative bi lstms ner iterate dilate convolutional neural network id cnns better capacity traditional cnns large context structure prediction unlike lstms whose sequential process sentence length n require ofn time even face parallelism id cnns permit fix depth convolutions run parallel across entire document describe distinct combination network structure parameter share train procedures enable dramatic fourteen 20x test time speedups retain accuracy comparable bi lstm crf moreover id cnns train aggregate context entire document even accurate maintain 8x faster test time speed
maybe single important goal representation learn make subsequent learn faster surprisingly fact well reflect way embeddings evaluate addition recent practice word embeddings point towards importance learn specialize representations argue focus word representation evaluation reflect trend shift towards evaluate useful information easily accessible specifically propose evaluation focus data efficiency simple supervise task amount available data vary score supervise model report subset commonly do transfer learn order illustrate significance analysis comprehensive evaluation select word embeddings present propose approach yield complete picture bring new insight performance characteristics instance information word similarity analogy tend non linearly encode embed space question cosine base unsupervised evaluation methods result analysis script available online
show task question answer qa significantly benefit transfer learn model train different large fine grain qa dataset achieve state art two well study qa datasets wikiqa semeval two thousand and sixteen task 3a basic transfer learn technique squad wikiqa model outperform previous best model eight demonstrate finer supervision provide better guidance learn lexical syntactic information coarser supervision quantitative result visual analysis also show similar transfer learn procedure achieve state art entailment task
present unsupervised language agnostic method learn root pattern morphology semitic languages form morphology abundant semitic languages handle prior unsupervised approach harness syntactico semantic information distribute word representations solve long stand problem root pattern discovery semitic languages moreover construct unsupervised root extractor base learn rule prove validity learn rule across arabic hebrew amharic alongside show root extractor compare favorably widely use carefully engineer root extractor isri
present paper novel framework morpheme segmentation use morpho syntactic regularities preserve word representations addition orthographic feature segment word morphemes framework first consider vocabulary wide syntactico semantic information task also analyze deficiencies available benchmarking datasets introduce dataset create basis compositionality validate algorithm across datasets present state art result
widespread use social media lead generation substantial amount information individuals include health relate information social media provide opportunity study health relate information select population group may interest particular study paper explore possibility utilize social media perform target data collection analysis particular population group pregnant women hypothesize use social media identify cohorts pregnant women follow time analyze crucial health relate information identify potentially pregnant women employ simple rule base search attempt detect pregnancy announcements moderate precision filter false positives noise employ supervise classifier use small number hand annotate data collect post time create longitudinal health timelines attempt divide timelines different pregnancy trimesters finally assess usefulness timelines perform preliminary analysis estimate drug intake pattern cohort different trimesters rule base cohort identification technique collect fifty-three thousand, eight hundred and twenty users thirty months twitter pregnancy announcement classification technique achieve f measure eighty-one pregnancy class result thirty-four thousand, eight hundred and ninety-five user timelines analysis timelines reveal pertinent health relate information drug intake adverse reactions mine data approach use user timelines fashion produce encourage result employ important task cohorts health relate information may available source require follow time derive population base estimate
paper present novel neural machine translation model jointly learn translation source side latent graph representations sentence unlike exist pipelined approach use syntactic parsers end end model learn latent graph parser part encoder attention base neural machine translation model thus parser optimize accord translation objective experiment first show model compare favorably state art sequential pipelined syntax base nmt model also show performance model improve pre train small amount treebank annotations final ensemble model significantly outperform previous best model standard english japanese translation dataset
turkish wikipedia name entity recognition text categorization twnertc dataset collection automatically categorize annotate sentence obtain wikipedia construct large scale gazetteers use graph crawler algorithm extract relevant entity domain information semantic knowledge base freebase construct gazetteers contain approximately 300k entities thousands fine grain entity type seventy-seven different domains since automate process prone ambiguity also introduce two new content specific noise reduction methodologies moreover map fine grain entity type equivalent four coarse grain type person loc org misc eventually construct six different dataset versions evaluate quality annotations compare grind truths human annotators make datasets publicly available support study turkish name entity recognition ner text categorization tc
people information need vary complexity solve intelligent agent able answer question formulate proper way eventually consider user context preferences scenario user profile consider question intelligent agents able answer question use find relevant answer give user work propose novel model base artificial neural network answer question multiple answer exploit multiple facts retrieve knowledge base model evaluate factoid question answer top n recommendation task babi movie dialog dataset assess performance model task try define long term goal conversational recommender system able interact use natural language support users information seek process personalize way
paper explore effect architectural choices learn variational autoencoder vae text generation contrast previously introduce vae model text encoder decoder rnns propose novel hybrid architecture blend fully fee forward convolutional deconvolutional components recurrent language model architecture exhibit several attractive properties faster run time convergence ability better handle long sequence importantly help avoid major difficulties pose train vae model textual data
fundamental advantage neural model nlp ability learn representations scratch however practice often mean ignore exist external linguistic resources eg wordnet domain specific ontologies unify medical language system umls propose general novel method exploit resources via weight share prior work weight share neural network consider largely mean model compression contrast treat weight share flexible mechanism incorporate prior knowledge neural model show approach consistently yield improve performance classification task compare baseline strategies exploit weight share
purpose automatically evaluate speakers humor usage build presentation corpus contain humorous utterances base ted talk compare previous data resources support humor recognition research several advantage include positive negative instance come homogeneous data set b contain large number speakers c open focus use lexical cue humor recognition systematically compare newly emerge text classification method base convolutional neural network cnns well establish conventional method use linguistic knowledge advantage cnn method get higher detection accuracies able learn essential feature automatically
paper enhance traditional confusion network system combination approach additional model train neural network work motivate fact commonly use binary system vote model assign input system global weight responsible global impact input system translations prevent individual systems low system weight influence system combination output although situations could helpful word see one systems rarely chance present combine output train local system vote model neural network base word combinatorial occurrences different system output give system combination option prefer systems different word position even sentence
paper propose use distribute representation word word embeddings cross language textual similarity detection main contributions paper follow introduce new cross language similarity detection methods base distribute representation word b combine different methods propose verify complementarity finally obtain overall f1 score eight thousand, nine hundred and fifteen english french similarity detection chunk level eight hundred and eighty-five sentence level challenge corpus
universal dependencies ud offer uniform cross lingual syntactic representation aim advance multilingual applications recent work show semantic parse accomplish transform syntactic dependencies logical form however work limit english process dependency graph allow handle complex phenomena control work introduce udeplambda semantic interface ud map natural language logical form almost language independent fashion process dependency graph perform experiment question answer freebase provide german spanish translations webquestions graphquestions datasets facilitate multilingual evaluation result show udeplambda outperform strong baselines across languages datasets english achieve forty-nine f1 point improvement state art graphquestions code data download https githubcom sivareddyg udeplambda
many language technology applications would benefit ability represent negation scope top widely use linguistic resources paper investigate possibility obtain first order logic representation negation scope mark use universal dependencies enhance udeplambda framework convert dependency graph logical form result udeplambdalnot able handle phenomena relate scope mean higher order type theory relevant negation also universal quantification complex semantic phenomena initial conversion english promise one represent scope negation also presence complex phenomena universal quantifiers
explicit concept space model prove efficacy text representation many natural language text mine applications idea embed textual structure semantic space concepts capture main ideas object characteristics structure call bag concepts boc representation suffer data sparsity cause low similarity score similar texts due low concept overlap address problem propose two neural embed model learn continuous concept vectors learn propose efficient vector aggregation method generate fully continuous boc representations evaluate concept embed model three task one measure entity semantic relatedness rank achieve sixteen improvement correlation score two dataless concept categorization achieve state art performance reduce categorization error rate five compare five prior word entity embed model three dataless document classification model outperform sparse boc representations addition exploit efficient linear time vector aggregation method achieve better accuracy score much less concept dimension compare previous boc densification methods operate polynomial time require hundreds dimension boc representation
use deep learn different machine learn task image classification word embed recently gain many attentions appeal performance report across specific natural language process nlp task comparison approach reason popularity word embed task map word phrase low dimensional numerical vector paper use deep learn embed wikipedia concepts entities english version wikipedia contain five million page suggest capability cover many english entities phrase concepts wikipedia page consider concept concepts correspond entities person name organization place contrary word embed wikipedia concepts embed ambiguous different vectors concepts similar surface form different mention propose several approach evaluate performance base concept analogy concept similarity task result show propose approach performance comparable case even higher state art methods
relatively little attention incorporate linguistic prior neural machine translation much previous work constrain consider linguistic prior source side paper propose hybrid model call nmtrnng learn parse translate combine recurrent neural network grammar attention base neural machine translation approach encourage neural machine translation model incorporate linguistic prior train let us translate afterward extensive experiment four language pair show effectiveness propose nmtrnng
agglutinative languages turkish finnish hungarian require morphological disambiguation process due complex morphology word morphological disambiguator use select correct morphological analysis word morphological disambiguation important generally one first step natural language process performance affect subsequent analyse paper propose system use deep learn techniques morphological disambiguation many state art result computer vision speech recognition natural language process obtain deep learn model however apply deep learn techniques morphologically rich languages well study work focus turkish morphological disambiguation also present result french german order show propose architecture achieve high accuracy language specific feature engineer additional resource experiment achieve eight thousand, four hundred and twelve eight thousand, eight hundred and thirty-five nine thousand, three hundred and seventy-eight morphological disambiguation accuracy among ambiguous word turkish german french respectively
paper develop deep neural network dnn learn solve simultaneously three task cqa challenge propose semeval two thousand and sixteen task three ie question comment similarity question question similarity new question comment similarity latter main task exploit previous two achieve better result dnn train jointly three cqa task learn encode question comment single vector representation share across multiple task result official challenge test set show approach produce higher accuracy faster convergence rat individual neural network additionally method use manual feature engineer approach state art establish methods make heavy use
explore problem translate speech text low resource scenarios neither automatic speech recognition asr machine translation mt available train data form audio pair text translations present first system problem apply realistic multi speaker dataset callhome spanish english speech translation corpus approach use unsupervised term discovery utd cluster repeat pattern audio create pseudotext pair translations create parallel text train simple bag word mt model identify challenge face system find difficulty cross speaker utd result low recall system still able correctly translate content word test data
parallel mean bank corpus translations annotate share formal mean representations comprise eleven million word divide four languages english german italian dutch approach base cross lingual projection automatically produce manually correct semantic annotations english sentence map onto word align translations assume translations mean preserve semantic annotation consist five main step segmentation text sentence lexical items ii syntactic parse combinatory categorial grammar iii universal semantic tag iv symbolization v compositional semantic analysis base discourse representation theory step perform use statistical model train semi supervise manner employ annotation model language neutral first result promise
present new parallel corpus jhu fluency extend gug corpus jfleg develop evaluate grammatical error correction gec unlike corpora represent broad range language proficiency level use holistic fluency edit correct grammatical errors also make original text native sound describe type corrections make benchmark four lead gec systems corpus identify specific areas well improve jfleg fulfill need new gold standard properly assess current state gec
previous study support idea merge auditory base gabor feature deep learn architectures achieve robust automatic speech recognition however behind gain combination still unknown believe representations provide deep learn decoder discriminable cue aim paper validate hypothesis perform experiment three different recognition task aurora four chime two chime three assess discriminability information encode gabor filterbank feature additionally identify contribution low medium high temporal modulation frequencies subsets gabor filterbank use feature dub ltm mtm htm respectively temporal modulation frequencies sixteen twenty-five hz htm consistently outperform remain ones every condition highlight robustness representations channel distortions low signal noise ratios acoustically challenge real life scenarios relative improvements eleven fifty-six mel filterbank dnn baseline explain result measure similarity phoneme class dnn activations propose link acoustic properties find measure consistent observe error rat highlight specific differences phoneme level pinpoint benefit propose feature
many low resource endanger languages speak language resources likely annotate translations transcriptions recent work exploit annotations produce speech translation alignments without access text transcriptions investigate whether provide information aid produce better mismatch crowdsourced transcriptions turn could valuable train speech recognition systems show indeed beneficial small scale case study proof concept also present simple phonetically aware string average technique produce transcriptions higher quality
one fundamental task text analysis phrase mine aim extract quality phrase text corpus phrase mine important various task information extraction retrieval taxonomy construction topic model exist methods rely complex train linguistic analyzers thus likely unsatisfactory performance text corpora new domains genres without extra expensive adaption recently data drive methods develop successfully extraction phrase massive domain specific text however none state art model fully automate require human experts design rule label phrase since one easily obtain many quality phrase public knowledge base scale much larger produce human experts paper propose novel framework automate phrase mine autophrase leverage large amount high quality phrase effective way achieve better performance compare limit human label phrase addition develop pos guide phrasal segmentation model incorporate shallow syntactic information part speech pos tag enhance performance pos tagger available note autophrase support language long general knowledge base eg wikipedia language available benefit require pos tagger compare state art methods new method show significant improvements effectiveness five real world datasets across different domains languages
recent study show effectiveness use neural network chinese word segmentation however model rely large scale data less effective low resource datasets insufficient train data propose transfer learn method improve low resource word segmentation leverage high resource corpora first train teacher model high resource corpora use learn knowledge initialize student model second weight data similarity method propose train student model low resource data experiment result show work significantly improve performance low resource datasets twenty-three fifteen f score pku ctb datasets furthermore paper achieve state art result nine hundred and sixty-one nine hundred and sixty-two f score pku ctb datasets
machine translation mt involve translate two languages significant differences word order determine correct word order translate word major challenge dependency parse tree source sentence help determine correct word order translate word paper present novel reorder approach utilize neural network dependency base embeddings predict whether translations two source word link dependency relation remain order swap translate sentence experiment chinese english translation show approach yield statistically significant improvement fifty-seven bleu point benchmark nist test set compare prior state art statistical mt system use sparse dependency base reorder feature
congestive heart failure chf serious medical condition result fluid buildup body result weak heart heart pump enough blood efficiently deliver nutrients oxygen body kidney function may impair result fluid retention chf patients require broad drug regimen maintain delicate system balance particularly heart kidneys drug include ace inhibitors beta blockers control blood pressure anticoagulants prevent blood clot diuretics reduce fluid overload many drug may interact potential effect interactions must weigh benefit project consider set forty-four drug identify specifically relevant treat chf pediatric cardiologists lucile packard children hospital list generate part current work lpch heart center goal project identify evaluate potentially harmful drug drug interactions ddis within pediatric patients congestive heart failure identification do autonomously may continuously update evaluate newly publish literature
interpret performance deep learn model beyond test set accuracy challenge characteristics individual data point often consider evaluation data point treat equally examine impact test set question difficulty determine relationship difficulty performance model difficulty use well study psychometric methods human response pattern experiment natural language inference nli sentiment analysis sa show likelihood answer question correctly impact question difficulty dnns train data easy examples learn quickly hard examples
paper explore use unsupervised methods detect cognates multilingual word list use online train sound segment similarity weight compute similarity two word test online systems geographically spread sixteen different language group world show online pmi system pointwise mutual information outperform hmm base system two linguistically motivate systems lexstat aline result suggest pmi system train online fashion use historical linguists fast accurate identification cognates well study language families
neural attention model achieve great success different nlp task ever fulfil promise amr parse task due data sparsity issue paper de scribe sequence sequence model amr parse present different ways tackle data sparsity problem show methods achieve significant improvement baseline neural atten tion model result also compet itive state art systems use extra linguistic resources
propose deep learn model identify structure within experiment narratives scientific literature take sequence label approach problem label clauses within experiment narratives identify different part experiment dataset consist paragraph take open access pubmed paper label rhetorical information result pilot annotation model recurrent neural network rnn long short term memory lstm cells label clauses clause representations compute combine word representations use novel attention mechanism involve separate rnn compare model lstms input layer simple attention feature rich crf model furthermore describe work could useful information extraction scientific literature
paper one show simple linear classifier compete complex deep learn algorithms text classification applications combine bag word bow linear classification techniques fasttext one attain slightly lower accuracy deep learn algorithms two nine order magnitude slower prove formally fasttext transform simpler equivalent classifier unlike fasttext hide layer also prove necessary sufficient dimensionality word vector embed space exactly number document class result help construct optimal linear text classifiers guarantee maximum classification capabilities result prove exactly pure formal algebraic methods without attract empirical data
word vector representations associate high dimensional real vector every word corpus recently neural network base methods propose learn representation large corpora type word vector embed able keep learn vector space syntactic semantic relationships present original word corpus turn serve address different type language classification task algebraic operations define vectors general practice assume semantic relationships word infer application priori specify algebraic operations general goal paper show possible learn methods word composition semantic space instead express compositional method algebraic operation encode program linear nonlinear involve intricate expressions remarkably program evolve set initial random program mean genetic program gp show method able reproduce behavior human design algebraic operators use word analogy task benchmark also show gp generate program able obtain accuracy value produce commonly use human design rule algebraic manipulation word vectors finally show robustness approach execute evolve program word2vec googlenews vectors learn three billion run word assess accuracy word analogy task
paper report write style analysis hyperpartisan ie extremely one side news connection fake news present large corpus one thousand, six hundred and twenty-seven article manually fact check professional journalists buzzfeed article originate nine well know political publishers three mainstream hyperpartisan leave wing hyperpartisan right wing sum corpus contain two hundred and ninety-nine fake news ninety-seven originate hyperpartisan publishers propose demonstrate new way assess style similarity text categories via unmask meta learn approach originally devise authorship verification reveal style leave wing right wing news lot common two mainstream furthermore show hyperpartisan news discriminate well style mainstream f1078 satire f1081 unsurprisingly style base fake news detection live scratch f1046 nevertheless former result important implement pre screen fake news detectors
work present systematic theoretical empirical comparison major algorithms propose learn harmonic optimality theory grammars hg ot respectively compare learn algorithms also able compare closely relate ot hg frameworks experimental result show additional expressivity hg framework ot afford performance gain task predict surface word order czech sentence compare perceptron classic gradual learn algorithm gla learn ot grammars well popular maximum entropy model addition show perceptron theoretically appeal work show performance hg model learn approach upper bind prediction accuracy hold test set capable accurately model observe variation
text generation increasingly common often require manual post edit high precision critical end users however manual edit expensive want ensure effort focus high value task want maintain stylistic consistency particular challenge crowd settings present case study analyse human post edit context template base biography generation system edit flow visualisation combine manual characterisation edit help identify prioritise work improve end end efficiency accuracy
present dialogue generation model directly capture variability possible responses give input reduce bore output issue deterministic dialogue model experiment show model generate diverse output baseline model also generate consistently acceptable output sample deterministic encoder decoder model
evolutionary model emergence diversity language develop investigate effect two real life observations namely people prefer people communicate well people interact people physically close clearly group relatively small compare entire population restrict selection teachers small group call imitation set around parent child learn language teacher select within imitation set parent result subcommunities languages develop within subcommunity comprehension find high number languages relate relative size imitation set power law
paper explore simple solution multi source neural machine translation msnmt rely preprocessing n way multilingual corpus without modify neural machine translation nmt architecture train procedure simply concatenate source sentence form single long multi source input sentence keep target side sentence train nmt system use preprocessed corpus evaluate method resource poor well resource rich settings show effectiveness four bleu use two source languages six bleu use five source languages also compare exist methods msnmt show solution give competitive result despite simplicity also provide insights nmt system leverage multilingual information scenario visualize attention
investigate generation one sentence wikipedia biographies facts derive wikidata slot value pair train recurrent neural network sequence sequence model attention select facts generate textual summaries model incorporate novel secondary objective help ensure generate sentence contain input facts model achieve bleu score forty-one improve significantly upon vanilla sequence sequence model score roughly twice simple template baseline human preference evaluation suggest model nearly good wikipedia reference manual analysis explore content selection suggest model trade ability infer knowledge risk hallucinate incorrect information
argument component detection acd important sub task argumentation mine acd aim detect classify different argument components natural language texts historical annotations important feature human annotators consider manually perform acd task however largely ignore exist automatic acd techniques reinforcement learn rl prove effective method use natural language process task work propose rl base acd technique evaluate performance two well annotate corpora result suggest term classification accuracy augment rl outperform plain rl one thousand, seven hundred and eighty-five outperform state art supervise learn algorithm one thousand, one hundred and ninety-four
paper present hybrid dialog state tracker enhance trainable speak language understand slu slot fill dialog systems architecture inspire previously propose neural network base belief track systems addition extend part modular architecture differentiable rule allow end end train hypothesize rule allow tracker generalize better pure machine learn base systems evaluation use dialog state track challenge dstc two dataset popular belief track testbed dialogs restaurant information system knowledge hybrid tracker set new state art result three four categories within dstc2
segmental conditional random field scrfs connectionist temporal classification ctc two sequence label methods use end end train speech recognition model model define transcription probability marginalize decisions latent segmentation alternatives derive sequence probability former use globally normalize joint model segment label durations latter classify frame either output symbol continuation previous label paper train recognition model optimize interpolation scrf ctc losses recurrent neural network rnn encoder use feature extraction output find multitask objective improve recognition accuracy decode either scrf ctc model additionally show ctc also use pretrain rnn encoder improve convergence rate learn joint model
advance natural language process task gain momentum recent years due increasingly popular neural network methods paper explore deep learn techniques answer multi step reason question operate semi structure table challenge arise level logical compositionality express question well domain openness approach weakly supervise train question answer table triple without require intermediate strong supervision perform two phase first machine understandable logical form program generate natural language question follow work pasupat liang two thousand and fifteen second paraphrase logical form question embed jointly learn vector space use word character convolutional neural network neural score function use rank retrieve probable logical form interpretation question best single model achieve three hundred and forty-eight accuracy wikitablequestions dataset best ensemble model push state art score task three hundred and eighty-seven thus slightly surpass engineer feature score baseline well neural programmer model neelakantan et al two thousand and sixteen
study parse complexity combinatory categorial grammar ccg formalism vijay shanker weir one thousand, nine hundred and ninety-four main result prove parse algorithm formalism take worst case exponential time size grammar length input sentence include analysis set formalism vijay shanker weir one thousand, nine hundred and ninety-four apart weakly equivalent formalisms tree adjoin grammar tag parse perform time polynomial combine size grammar input sentence result contribute refine understand class mildly context sensitive grammars inform search new mildly context sensitive versions ccg
children use statistical regularities environment learn word mean mechanism know cross situational learn take computational approach investigate information present observation cross situational framework affect overall acquisition word mean formulate various moment learn mechanisms sensitive different statistics environment count conditional probabilities mechanism introduce unique source competition mutual exclusivity bias model mechanism maximally use model knowledge word mean perform best moreover gap mechanism others amplify challenge learn scenarios learn examples
derivational morphology fundamental complex characteristic language paper propose new task predict derivational form give base form lemma appropriate give context present encoder decoder style neural network produce derive form character character base correspond character level representation base form context demonstrate model able generate valid context sensitive derivations know base form less accurate lexicon agnostic set
paper investigate whether priori disambiguation word sense strictly necessary whether mean word context disambiguate composition alone evaluate performance shelf single vector multi sense vector model benchmark phrase similarity task novel task word sense discrimination find single sense vector model perform well better multi sense vector model despite arguably less clean elementary representations find furthermore show simple composition function pointwise addition able recover sense specific information single sense vector model remarkably well
people speak different level specificity different situations depend knowledge interlocutors mood etc conversational agent ability know specific general propose approach give neural network base conversational agent ability approach involve alternate emphdata distillation model train remove train examples closest responses commonly produce model train last round retrain model remain dataset dialogue generation model train different degrees data distillation manifest different level specificity train reinforcement learn system select among pool generation model choose best level specificity give input compare original generative model train without distillation propose system capable generate interest higher quality responses addition appropriately adjust specificity depend context research constitute specific case broader approach involve train multiple subsystems single dataset distinguish differences specific property one wish model show set subsystems one use reinforcement learn build system tailor output different input contexts test time
fine grain entity type classification fetc task classify entity mention broad set type distant supervision paradigm extensively use generate train data task however generate train data assign set label every mention entity without consider local context exist fetc systems two major drawbacks assume train data noise free use hand craft feature work overcome drawbacks propose neural network model jointly learn entity mention context representation eliminate use hand craft feature model treat train data noisy use non parametric variant hinge loss function experiment show propose model outperform previous state art methods two publicly available datasets namely figer gold bbn average relative improvement two hundred and sixty-nine micro f1 score knowledge learn model one dataset transfer datasets use model fetc systems approach transfer knowledge improve performance respective model
dependency parsers reach high overall accuracy dependency relations much harder others particular dependency parsers perform poorly coordination construction ie correctly attach conj relation extend state art dependency parser conjunction specific feature focus similarity conjuncts head word train extend parser yield improvement conj attachment well overall dependency parse accuracy stanford dependency conversion penn treebank
previous study chinese semantic role label srl concentrate single semantically annotate corpus train data single corpus often limit meanwhile usually exist semantically annotate corpora chinese srl scatter across different annotation frameworks data sparsity remain bottleneck situation call larger train datasets effective approach take advantage highly heterogeneous data paper focus mainly latter improve chinese srl use heterogeneous corpora together propose novel progressive learn model augment progressive neural network gate recurrent adapters model accommodate heterogeneous input effectively transfer knowledge also release new corpus chinese sembank chinese srl experiment cpb ten show model outperform state art methods
error propagation common problem nlp reinforcement learn explore erroneous state train therefore robust mistake make early process paper apply reinforcement learn greedy dependency parse know suffer error propagation reinforcement learn improve accuracy label unlabeled dependencies stanford neural dependency parser high performance greedy parser maintain efficiency investigate portion errors result error propagation confirm reinforcement learn reduce occurrence error propagation
present unsupervised explainable word embed technique call eve build upon structure wikipedia propose model define dimension semantic vector represent word use human readable label thereby readily interpretable specifically vector construct use wikipedia category graph structure together wikipedia article link structure test effectiveness propose word embed model consider usefulness three fundamental task one intruder detection evaluate ability identify non coherent vector list coherent vectors two ability cluster evaluate tendency group relate vectors together keep unrelated vectors separate cluster three sort relevant items first evaluate ability rank vectors items relevant query top order result task also propose strategy generate task specific human interpretable explanation model demonstrate overall effectiveness explainable embeddings generate eve finally compare eve word2vec fasttext glove embed techniques across three task report improvements state art
paper focus unsupervised model morphological families collectively comprise forest language vocabulary formulation enable us capture edgewise properties reflect single step morphological derivations along global distributional properties entire forest global properties constrain size affix set encourage formation tight morphological families result objective solve use integer linear program ilp pair contrastive estimation train model alternate optimize local log linear model global ilp objective evaluate system three task root detection cluster morphological families segmentation experiment demonstrate model yield consistent gain three task compare best publish result
hand engineer feature set well understand method create robust nlp model require lot expertise effort create work describe automatically generate rich feature set simple units call featlets require less engineer use information gain guide generation process train model rival state art two standard semantic role label datasets almost task linguistic insight
topic model widely use discover latent topics share across document text mine vector representations word embeddings topic embeddings map word topics low dimensional dense real value vector space obtain high performance nlp task however exist model assume result train one perfect correct use prior knowledge improve model model use information train external large corpus help improve smaller corpus paper aim build algorithm framework make topic model vector representations mutually improve within corpus style algorithm framework employ iteratively optimize topic model vector representations experimental result show model outperform state art methods various nlp task
investigate pivot base translation relate languages low resource phrase base smt set show subword level pivot base smt model use relate pivot language substantially better word morpheme level pivot model also highly competitive best direct translation model encourage direct source target train corpus use also show combine multiple relate language pivot model rival direct translation model thus use subwords translation units couple multiple relate pivot languages compensate lack direct parallel corpus
emojis ideograms naturally combine plain text visually complement condense mean message despite widely use social media underlie semantics receive little attention natural language process standpoint paper investigate relation word emojis study novel task predict emojis evoke text base tweet message train several model base long short term memory network lstms task experimental result show neural model outperform two baselines well humans solve task suggest computational model able better capture underlie semantics emojis
recurrent neural network model phonological pattern learn propose model relatively simple neural network one recurrent layer display bias learn mimic observe bias human learn single feature pattern learn faster two feature pattern vowel consonant pattern learn faster pattern involve vowels consonants mimic result laboratory learn experiment non recurrent model capture bias require use alpha feature representation repeat feature recurrent neural network elaborations necessary
document multi document von mises fisher mixture model dirichlet prior refer vmfmix vmfmix analogous latent dirichlet allocation lda capture co occurrence pattern acorss multiple document difference vmfmix topic word distribution define continuous n dimensional hypersphere hence vmfmix use derive topic embeddings ie representative vectors multiple set embed vectors efficient variational expectation maximization inference algorithm derive performance vmfmix two document classification task report preliminary analysis
year ago state art coreference resolvers use extensive amount surface feature recently paradigm shift towards use word embeddings deep neural network use surface feature limit paper show simple svm model surface feature outperform complex neural model detect anaphoric mention analysis suggest use generalize representations surface feature different strength take account improve coreference resolution
deep learn approach widely use automatic speech recognition asr achieve significant accuracy improvement especially convolutional neural network cnns revisit asr recently however cnns use exist work less ten layer may deep enough capture human speech signal information paper propose novel deep wide cnn architecture denote rcnn ctc residual connections connectionist temporal classification ctc loss function rcnn ctc end end system exploit temporal spectral structure speech signal simultaneously furthermore introduce ctc base system combination different conventional frame wise senone base one basic subsystems adopt combination different type thus mutually complementary experimental result show propose single system rcnn ctc achieve lowest word error rate wer wsj tencent chat data set compare several widely use neural network systems asr addition propose system combination offer error reduction two data set result relative wer reductions one thousand, four hundred and ninety-one six hundred and fifty-two wsj dev93 tencent chat data set respectively
availability corpora major factor build natural language process applications however cost acquire corpora prevent researchers go endeavour ease access freely available corpora urgent need nlp research community especially language arabic currently easy access comprehensive update list freely available arabic corpora present paper result recent survey conduct identify list freely available arabic corpora language resources preliminary result show initial list sixty-six source present find various categories study provide direct link get data possible
present robust approach detect intrinsic sentence importance news train two corpora document summary pair use single document summarization approach combine begin document heuristic outperform state art summarizer begin article baseline automatic manual evaluations result represent important advance absence cross document repetition single document summarizers news able consistently outperform strong begin article baseline
stance detection task identify speaker opinion towards particular target attract attention researchers paper describe novel approach detect stance twitter define set feature order consider context surround target interest final aim train model predict stance towards mention target particular interest investigate political debate social media reason evaluate approach focus two target semeval two thousand and sixteen task6 detect stance tweet relate political campaign two thousand and sixteen yous presidential elections hillary clinton vs donald trump sake comparison state art evaluate model dataset release semeval two thousand and sixteen task six share task competition result outperform best ones obtain participate team show information enemies friends politicians help detect stance towards
paper present work case study statistical machine translation smt rule base machine translation rbmt translation english malayalam malayalam english one motivations study make three way performance comparison smt rbmt b english malayalam smt malayalam english smt c english malayalam rbmt malayalam english rbmt describe development english malayalam malayalam english baseline phrase base smt system evaluation performance compare rbmt system base study observations smt systems outperform rbmt systems b case smt english malayalam systems perform better malayalam english systems c case rbmt malayalam english systems perform better english malayalam systems base evaluations detail error analysis describe requirements incorporate morphological process smt improve accuracy translation
multi task learn mtl deep neural network nlp recently receive increase interest due compel benefit include potential efficiently regularize model reduce need label data bring significant improvements number nlp task mix result report little know condition mtl lead gain nlp paper shed light specific task relations lead gain mtl model single task setups
word sense disambiguation improve many natural language process nlp applications information retrieval information extraction machine translation lexical simplification roughly speak aim choose word text best sense one popular method estimate local semantic similarity relatedness two word sense extend word text direct method compute rough score every pair word sense choose lexical chain best score imagine exponential complexity return comprehensive approach paper propose use combinatorial optimization metaheuristic choose nearest neighbor obtain distributional selection around word disambiguate test evaluation method concern corpus write french mean semantic network babelnet obtain accuracy rate seventy-eight name verbs choose evaluation
word sense disambiguation wsd improve many natural language process nlp applications information retrieval machine translation lexical simplification wsd ability determine word sense among different ones within polysemic lexical unit take account context straightforward approach use semantic proximity measure word sense candidates target word context method easily entail combinatorial explosion paper propose two methods base distributional analysis enable reduce exponential complexity without lose coherence present comparison selection distributional neighbor linearly nearest neighbor figure obtain show select distributional neighbor lead better result
often multiple label obtain train example assume element noise must account show disagreement consider signal instead noise work investigate use soft label train data improve generalization machine learn model however use soft label train deep neural network dnns practical due cost involve obtain multiple label large data set propose soft label memorization generalization slmg fine tune approach use soft label train dnns assume differences label provide human annotators represent ambiguity true label instead noise experiment slmg demonstrate improve generalization performance natural language inference nli task experiment show inject small percentage soft label train data three train set size improve generalization performance several baselines
introduce new paradigm learn reason understand prediction well scaffold network implement paradigm scaffold network embody incremental learn approach formulate teacher student network architecture teach machine understand text reason key computational scaffold approach interactions teacher student sequential question student observe sentence text incrementally use attention base neural net discover register key information relation current memory meanwhile teacher ask question observe text student network get reward correctly answer question entire network update continually use reinforcement learn experimental result synthetic real datasets show scaffold network outperform state art methods also learn reason scalable way even little human generate input
present result empirical study positive speech twitter positive speech understand speech work betterment give situation case relations different communities conflict prone country work four twitter data set semi manual opinion mine find positive speech account one data fully automate study test two approach unsupervised statistical analysis supervise text classification base distribute word representation discuss benefit challenge approach report empirical evidence obtain study
prior work revision identification typically use pipeline method revision extraction first conduct identify locations revisions revision classification conduct identify revisions set propagate errors revision extraction step revision classification step paper propose approach identify revision location revision type jointly solve issue error propagation utilize sequence representation revisions conduct sequence label revision identification mutation base approach utilize update identification sequence result demonstrate propose approach yield better performance revision location extraction revision type classification compare pipeline baseline
background electronic health record ehr note contain abundant medical jargon difficult patients comprehend one way help patients reduce information overload help focus medical term matter objective aim work develop fit find important term patients unsupervised natural language process nlp system rank medical term ehr note base importance patients methods build fit new unsupervised ensemble rank model derive bias random walk algorithm combine heterogeneous information resources rank candidate term ehr note specifically fit integrate four single view term importance patient use medical concepts document level term salience word occurrence base term relatedness topic coherence also incorporate partial information term importance convey term unfamiliarity level semantic type evaluate fit ninety expert annotate ehr note compare three benchmark unsupervised ensemble rank methods result fit achieve eight hundred and eighty-five auc roc rank candidate term ehr note identify important term include term identification performance fit identify important term ehr note eight hundred and thirteen auc roc outperform three ensemble rankers metrics performance relatively insensitive parameter conclusions fit automatically identify ehr term important patients may help develop personalize interventions improve quality care use unsupervised learn well robust flexible framework information fusion fit readily apply domains applications
deep neural network machine comprehension typically utilize word character embeddings without explicitly take advantage structure linguistic information constituency tree dependency tree paper propose structural embed syntactic tree sest algorithm framework utilize structure information encode vector representations boost performance algorithms machine comprehension evaluate approach use state art neural attention model squad dataset experimental result demonstrate model accurately identify syntactic boundaries sentence extract answer syntactically coherent baseline methods
dependency parse important nlp task popular approach dependency parse structure perceptron still graph base dependency parse time complexity ofn3 suffer slow train deal problem propose parallel algorithm call parallel perceptron parallel algorithm make full use multi core computer save lot train time base experiment observe dependency parse parallel perceptron achieve eight fold faster train speed traditional structure perceptron methods use ten thread loss accuracy
focus past machine learn research read comprehension task primarily design novel deep learn architectures show seemingly minor choices make one use pre train word embeddings two representation vocabulary tokens test time turn larger impact architectural choices final performance systematically explore several options choices provide recommendations researchers work area
train data rapid growth large scale parallel train multi gpus cluster widely apply neural network model learn currentlywe present new approach apply exponential move average method large scale parallel train neural network model non interference strategy exponential move average model broadcast distribute workers update local model model synchronization train process implement final model train system fully connect fee forward neural network dnns deep unidirectional long short term memory lstm recurrent neural network rnns successfully train propose method large vocabulary continuous speech recognition shenma voice search data mandarin character error rate cer mandarin speech recognition degrade state art approach parallel train
paper describe ways utilize various lexical resources improve quality statistical machine translation system augment train corpus various lexical resources indowordnet semantic relation set function word kridanta pair verb phrase etc research usage lexical resources mainly focus two ways augment parallel corpus vocabulary augment various word form describe case study evaluations detail error analysis marathi hindi hindi marathi machine translation systems evaluations observe incremental growth quality machine translation usage various lexical resources increase moreover usage various lexical resources help improve coverage quality machine translation limit parallel corpus available
inverse relationship length word frequency use first identify gk zipf one thousand, nine hundred and thirty-five classic empirical law hold across wide range human languages demonstrate length one aspect much general property word distinctive respect word language distinctiveness play critical role recognize word fluent speech reflect strength potential competitors select best candidate ambiguous signal phonological information content measure word string probability statistical model language sound character sequence concisely capture distinctiveness examine large scale corpora thirteen languages find distinctiveness significantly outperform word length predictor frequency find provide evidence listeners process constraints shape fine grain aspects word form across languages
computation semantic similarity concepts important foundation many research work paper focus ic compute methods ic measure estimate semantic similarities concepts exploit topological parameters taxonomy base analyze representative ic compute methods typical semantic similarity measure propose new hybrid ic compute method adopt parameter dhyp lch utilize new ic compute method propose novel comprehensive measure semantic similarity concepts experiment base wordnet taxonomy design test representative measure measure benchmark dataset randg result show measure obviously improve similarity accuracy evaluate propose approach compare correlation coefficients five measure artificial data result show proposal outperform previous measure
humans often detect persons utterances favor give target entity topic product another person etc perspective computer need mean automatically deduce stance tweeter give tweet text paper present result perform stance detection twitter data use supervise approach begin extract bag word perform classification use timbl try optimize feature improve stance detection accuracy follow extend dataset two set lexicons argue mpqa subjectivity next explore malt parser construct feature use dependency triple finally perform analysis use scikit learn random forest implementation
show random vectors random projection implement usual vector space model construct euclidean semantic space french synonym dictionary evaluate theoretically result noise show experimental distribution similarities term neighborhood accord choice parameters also show schmidt orthogonalization process applicable use separate homonyms distinct semantic mean neighbor term easily arrange semantically significant cluster well suit generation realistic list synonyms applications word selection automatic text generation process applicable language easily extend collocations extremely fast update real time whenever new synonyms propose
one difficult speech recognition task accurate recognition human human communication advance deep learn last years produce major speech recognition improvements representative switchboard conversational corpus word error rat years ago fourteen drop eighty sixty-six recently fifty-eight believe within strike range human performance raise two issue human performance far still drive speech recognition error rat recent paper microsoft suggest already achieve human performance try verify statement perform independent set human performance measurements two conversational task find human performance may considerably better earlier report give community significantly harder goal achieve also report efforts area present set acoustic language model techniques lower word error rate english conversational telephone lvcsr system level fifty-five one hundred and three switchboard callhome subsets hub5 two thousand evaluation least write paper new performance milestone albeit measure human performance acoustic side use score fusion three model one lstm multiple feature input second lstm train speaker adversarial multi task learn third residual net resnet twenty-five convolutional layer time dilate convolutions language model side use word character lstms convolutional wavenet style language model
word segmentation basic problem natural language process languages complex write system like khmer language southern vietnam problem really intractable pose significant challenge although experts vietnam well international deeply research problem still reasonable result meet demand particular treat thoroughly ambiguous phenomenon process khmer language process far paper present solution base syllable division component cluster use two syllable model propose thereby build khmer syllable database still actually available method use lexical database update online khmer dictionaries support dictionaries serve role train data complementary linguistic characteristics component cluster label locate first last letter identify entirety syllable approach workable test result achieve high accuracy eliminate ambiguity contribute solve problem word segmentation apply efficiency khmer language process
opaque phonological pattern sometimes claim difficult learn specific hypotheses advance relative difficulty particular kinds opaque process kiparsky one thousand, nine hundred and seventy-one one thousand, nine hundred and seventy-three kind data helpful learn opaque pattern kiparsky two thousand paper present computationally implement learn theory one grammatical theory opacity maximum entropy version stratal ot berm udez otero one thousand, nine hundred and ninety-nine kiparsky two thousand test simplify versions opaque french tense lax vowel alternations opaque interaction diphthong raise flap canadian english find difficulty opacity influence evidence stratal affiliation canadian english case easier learner encounter application raise outside flap context non application raise word ie raise vowel non raise vowel
train recurrent neural network model long term dependencies difficult hence propose use external linguistic knowledge explicit signal inform model memories utilize specifically external knowledge use augment sequence type edge arbitrarily distant elements result graph decompose direct acyclic subgraphs introduce model encode graph explicit memory recurrent neural network use model coreference relations text apply model several text comprehension task achieve new state art result consider benchmarks include cnn babi lambada babi qa task model solve fifteen twenty task one thousand train examples per task analysis learn representations demonstrate ability model encode fine grain entity information across document
divergent word usages reflect differences among people paper present novel angle study word usage divergence word interpretations propose approach quantify semantic differences interpretations among different group people effectiveness approach validate quantitative evaluations experiment result indicate divergences word interpretations exist apply approach two well study type differences people gender region detect word divergent interpretations reveal unique feature specific group people gender discover certain different interest social attitudes character males females reflect divergent interpretations many word region find specific interpretations certain word reveal geographical cultural feature different regions
convolutional neural network cnns typically associate computer vision cnns responsible major breakthroughs image classification core computer vision systems today recently cnns apply problems natural language process get interest result paper try explain basics cnns different variations apply nlp
paper explore problem sockpuppet detection deceptive opinion spam use authorship attribution verification approach two methods explore first feature subsampling scheme use kl divergence stylistic language model author find discriminative feature second transduction scheme spy induction leverage diversity author unlabeled test set send set spy positive sample train set retrieve hide sample unlabeled test set use nearest farthest neighbor experiment use grind truth sockpuppet data show effectiveness propose scheme
sparsity one major problems natural language process problem become even severe agglutinate languages highly prone inflect deal sparsity turkish adopt morphological feature part speech tag learn inflectional derivational morpheme tag turkish use conditional random field crf employ morpheme tag part speech pos tag use hide markov model hmms mitigate sparsity result show use morpheme tag pos tag help alleviate sparsity emission probabilities model outperform hide markov model base pos tag model small train datasets turkish obtain accuracy nine hundred and forty-one morpheme tag eight hundred and ninety-two pos tag 5k train dataset
compositionality language refer much mean phrase decompose mean constituents way constituents combine base premise substitution synonyms mean preserve compositionality approximate semantic similarity phrase version phrase word replace synonyms different ways represent phrase exist eg vectors one language model two choice representation affect measurement semantic similarity propose new compositionality detection method represent phrase rank list term weight method approximate semantic similarity two rank list representations use range well know distance correlation metrics contrast state art approach compositionality detection method completely unsupervised experiment publicly available dataset one thousand and forty-eight human annotate phrase show compare strong supervise baselines approach provide superior measurement compositionality use distance correlation metrics consider
present paper work comparison statistical machine translation smt rule base machine translation translation marathi hindi rule base systems although robust take lot time build hand statistical machine translation systems easier create maintain improve upon describe development basic marathi hindi smt system evaluate performance detail error analysis point relative strengths weaknesses systems effectively shall see even small amount train corpus statistical machine translation system many advantage high quality domain specific machine translation rule base counterpart
consider semantics prepositions revisit broad coverage annotation scheme use annotate four thousand, two hundred and fifty preposition tokens fifty-five thousand word corpus english attempt apply scheme adpositions case markers languages well problematic case english lead us reconsider assumption preposition lexical contribution equivalent role relation mediate proposal embrace potential construal adposition use express phenomena directly token level manage complexity avoid sense proliferation suggest framework represent scene role adposition lexical function annotate scale support automatic statistical process domain general language sketch representation would inform constructional analysis
exemplar model popular class model use describe language change study limit memory capacity individual model affect system behaviour particular demonstrate effect change extinction categories previous work exemplar dynamics address question order investigate inspect simplify exemplar model prove simplify model sound categories one always become extinct whether memory storage limit however computer simulations show change number store memories alter fast categories become extinct
neural machine translation nmt show remarkable progress past years production systems deploy end users one major drawback current architectures expensive train typically require days weeks gpu time converge make exhaustive hyperparameter search commonly do neural network architectures prohibitively expensive work present first large scale analysis nmt architecture hyperparameters report empirical result variance number several hundred experimental run correspond two hundred and fifty thousand gpu hours standard wmt english german translation task experiment lead novel insights practical advice build extend nmt architectures part contribution release open source nmt framework enable researchers easily experiment novel techniques reproduce state art result
key challenge automatic hate speech detection social media separation hate speech instance offensive language lexical detection methods tend low precision classify message contain particular term hate speech previous work use supervise learn fail distinguish two categories use crowd source hate speech lexicon collect tweet contain hate speech keywords use crowd source label sample tweet three categories contain hate speech offensive language neither train multi class classifier distinguish different categories close analysis predictions errors show reliably separate hate speech offensive language differentiation difficult find racist homophobic tweet likely classify hate speech sexist tweet generally classify offensive tweet without explicit hate keywords also difficult classify
study taxonomies hypernymy relations extensive natural language process nlp literature however evaluation taxonomy learn approach traditionally troublesome mainly rely ad hoc experiment hardly reproducible manually expensive partly current research lately focus hypernymy detection task paper reflect trend analyze issue relate current evaluation procedures finally propose three potential avenues future work relations resources base play important role downstream nlp applications
mine textual pattern news tweet paper many kinds text corpora active theme text mine nlp research previous study adopt dependency parse base pattern discovery approach however parse result lose rich context around entities pattern process costly corpus large scale study propose novel type textual pattern structure call meta pattern extend frequent informative precise subsequence pattern certain context propose efficient framework call metapad discover meta pattern massive corpora three techniques one develop context aware segmentation method carefully determine boundaries pattern learn pattern quality assessment function avoid costly dependency parse generate high quality pattern two identify group synonymous meta pattern multiple facets type contexts extractions three examine type distributions entities instance extract group pattern look appropriate type level make discover pattern precise experiment demonstrate propose framework discover high quality type textual pattern efficiently different genres massive corpora facilitate information extraction
paper describe two supervise baseline systems story cloze test share task mostafazadeh et al 2016a first build classifier use feature base word embeddings semantic similarity computation implement neural lstm system different encode strategies try model relation story provide end experiment show model use representation feature base average word embed vectors give story word candidate end sentence word joint similarity feature story candidate end representations perform better neural model best model achieve accuracy seven thousand, two hundred and forty-two rank 3rd official evaluation
present nematus toolkit neural machine translation toolkit prioritize high translation accuracy usability extensibility nematus use build top perform submissions share translation task wmt iwslt use train systems production environments
work present compact modular framework construct novel recurrent neural architectures basic module new generic unit transition base recurrent unit tbru addition hide layer activations tbrus discrete state dynamics allow network connections build dynamically function intermediate activations connect multiple tbrus extend combine commonly use architectures sequence sequence attention mechanisms cursive tree structure model tbru also serve encoder downstream task decoder task simultaneously result accurate multi task learn call approach dynamic recurrent acyclic graphical neural network dragnn show dragnn significantly accurate efficient seq2seq attention syntactic dependency parse yield accurate multi task learn extractive summarization task
explore inflectional morphology example relationship discrete continuous linguistics grammar request form lexeme specify set feature value correspond corner hypercube feature value space morphology respond request provide morpheme set morphemes whose vector sum geometrically closest corner short choose morpheme mu morpheme set morphemes maximize inner product mu
last several years see intensive interest explore neural network base model machine comprehension mc question answer qa paper approach problems closely model question neural network framework first introduce syntactic information help encode question view model different type question information share among adaptation task propose adaptation model stanford question answer dataset squad show approach help attain better result competitive baseline
stream word produce automatic speech recognition asr systems typically devoid punctuations format natural language process applications expect segment well format texts input available asr output paper propose novel technique jointly model multiple correlate task punctuation capitalization use bidirectional recurrent neural network lead improve performance task method could extend joint model correlate sequence label task
present automatic discourse analysis relevant research topic field nlp however discourse one phenomena difficult process although discourse parsers already develop several languages tool exist catalan order implement kind parser first step develop discourse segmenter article present first discourse segmenter texts catalan segmenter base rhetorical structure theory rst spanish use lexical syntactic information translate rule valid spanish rule catalan evaluate system use gold standard corpus include manually segment texts result promise
name entity classification task classify text base elements various categories include place name date time monetary value bottleneck name entity classification however data problem sparseness new name entities continually emerge make rather difficult maintain dictionary name entity classification thus paper address problem name entity classification use matrix factorization overcome problem feature sparsity experimental result show propose model fewer feature smaller size achieve competitive accuracy state art model
paper propose approach apply gans nmt build conditional sequence generative adversarial net comprise two adversarial sub model generator discriminator generator aim generate sentence hard discriminate human translate sentence ie golden target sentence discriminator make efforts discriminate machine generate sentence human translate ones two sub model play mini max game achieve win win situation reach nash equilibrium additionally static sentence level bleu utilize reinforce objective generator bias generation towards high bleu point train dynamic discriminator static bleu objective employ evaluate generate sentence feedback evaluations guide learn generator experimental result show propose model consistently outperform traditional rnnsearch newly emerge state art transformer english german chinese english translation task
describe baseline dependency parse system conll2017 share task system call parseysaurus use dragnn framework kong et al two thousand and seventeen combine transition base recurrent parse tag character base word representations v13 universal dependencies treebanks new system outpeforms publicly available state art parsey cousins model three hundred and forty-seven absolute label accuracy score las across fifty-two treebanks
code mix code switch effortless phenomena natural switch two languages single conversation use foreign word language however necessarily mean speaker code switch often languages borrow lexical items languages word borrow become part lexicon language whereas code switch speaker aware conversation involve foreign word phrase identify whether foreign word use bilingual speaker due borrow code switch fundamental importance theories multilingualism essential prerequisite towards development language speech technologies multilingual communities paper present series novel computational methods identify borrow likeliness word base social media signal first propose context base cluster method sample set candidate word social media datanext propose three novel similar metrics base usage word users different tweet metrics use score rank candidate word indicate borrow likeliness compare rank grind truth rank construct human judgment experiment spearman rank correlation two rank nearly sixty-two three metric variants double value twenty-six competitive exist baseline report literature strike observations correlation higher grind truth data elicit younger participants age less thirty older participants ii participants use mix language tweet least provide best signal borrow
end end design dialogue systems recently become popular research topic thank powerful tool encoder decoder architectures sequence sequence learn yet current approach cast human machine dialogue management supervise learn problem aim predict next utterance participant give full history dialogue vision simplistic render intrinsic plan problem inherent dialogue well ground nature make context dialogue larger sole history chit chat question answer task address far use end end architectures paper introduce deep reinforcement learn method optimize visually ground task orient dialogues base policy gradient algorithm approach test dataset 120k dialogues collect mechanical turk provide encourage result solve problem generate natural dialogues task discover specific object complex picture
paper describe neural network model perform competitively top six semeval two thousand and seventeen cross lingual semantic textual similarity sts task system employ attention base recurrent neural network model optimize sentence similarity paper describe participation multilingual sts task measure similarity across english spanish arabic
evaluation distribute word representation generally conduct use word similarity task word analogy task many datasets readily available task english however evaluate distribute representation languages resources eg japanese difficult therefore first step toward evaluate distribute representations japanese construct japanese word similarity dataset best knowledge dataset first resource use evaluate distribute representations japanese moreover dataset contain various part speech include rare word addition common word
internet lead dramatic increase amount available information context read understand flow information become costly task last years assist people understand textual data various natural language process nlp applications base combinatorial optimization devise however multi sentence compression msc method reduce sentence length without remove core information insertion optimization methods require study improve performance msc article describe method msc use combinatorial optimization graph theory generate informative sentence maintain grammaticality experiment lead corpus forty cluster sentence show system achieve good quality better state art
ensemble methods use multiple classifiers prove successful approach task native language identification nli achieve current state art however systematic examination ensemble methods nli yet conduct additionally deeper ensemble architectures classifier stack closely evaluate present set experiment use three ensemble base model test multiple configurations algorithms include rigorous application meta classification model nli achieve state art result three datasets different languages also present first use statistical significance test compare nli systems show result significantly better previous state art make available collection test set predictions facilitate future statistical test
recurrent neural network rnns especially long short term memory lstm rnns effective network sequential task like speech recognition deeper lstm model perform well large vocabulary continuous speech recognition impressive learn ability however difficult train deeper network introduce train framework layer wise train exponential move average methods deeper lstm model competitive framework lstm model seven layer successfully train shenma voice search data mandarin outperform deep lstm model train conventional approach moreover order online stream speech recognition applications shallow model low real time factor distil deep model recognition accuracy little loss distillation process therefore model train propose train framework reduce relative fourteen character error rate compare original model similar real time capability furthermore novel transfer learn strategy segmental minimum bay risk also introduce framework strategy make possible train small part dataset could outperform full dataset train begin
new python api integrate within nltk suite offer access framenet seventeen lexical database lexicon structure term frame well annotate sentence process programatically browse human readable display via interactive python prompt
modern topic identification topic id systems speech use automatic speech recognition asr produce speech transcripts perform supervise classification asr output however resource limit condition manually transcribe speech require develop standard asr systems severely limit unavailable paper investigate alternative unsupervised solutions obtain tokenizations speech term vocabulary automatically discover word like phoneme like units without depend supervise train asr systems moreover use automatic phoneme like tokenizations demonstrate convolutional neural network base framework learn speak document representations provide competitive performance compare standard bag word representation evidence comprehensive topic id evaluations single label multi label classification task
speaker change detection scd important task dialog model paper address problem text base scd differ exist audio base study useful various scenarios example process dialog transcripts speaker identities miss eg opensubtitle enhance audio scd textual information formulate text base scd match problem utterances certain decision point propose hierarchical recurrent neural network rnn static sentence level attention experimental result show neural network consistently achieve better performance feature base approach attention base model significantly outperform non attention neural network
feedforward neural network fnn base language model estimate probability next word base history last n word whereas recurrent neural network rnn perform task base last word context information cycle network paper present novel approach bridge gap two categories network particular propose architecture take advantage explicit sequential enumeration word history fnn structure enhance word representation projection layer recurrent context information evolve network context integration perform use additional word dependent weight matrix also learn train extensive experiment conduct penn treebank ptb large text compression benchmark ltcb corpus show significant reduction perplexity compare state art feedforward well recurrent neural network architectures
state art neural machine translation attention mechanism use decode enhance translation every step decoder use mechanism focus different part source sentence gather useful information output target word recently effectiveness attention mechanism also explore multimodal task become possible focus sentence part image regions approach pool two modalities usually include element wise product sum concatenation paper evaluate advance multimodal compact bilinear pool method take outer product two vectors combine attention feature two modalities previously investigate visual question answer try approach multimodal image caption translation show improvements compare basic combination methods
commercial establishments like restaurants service centre retailers several source customer feedback products service need structure rat review provide service like yelp amazon term sentiment convey instance amazon provide fine grain score numeric scale product review source however like social media twitter facebook mail list google group forums quora contain text data much voluminous unstructured unlabelled might best interest business establishment assess general sentiment towards brand platforms well text could pipelined system build prediction model objective generate real time graph opinion sentiment trend although task like one describe explore respect document classification problems past implementation describe paper virtue learn continuous function rather discrete one offer lot depth insight compare document classification approach study aim explore validity continuous function predict model quantify sentiment entity without additional overhead manual label computational preprocessing feature extraction research project also aim design implement usable document regression pipeline framework rapid rate use predict document score real time
present dataset contain every instance tokens word ever write undeleted non redirect english wikipedia article october two thousand and sixteen total thirteen billion, five hundred and forty-five million, three hundred and forty-nine thousand, seven hundred and eighty-seven instance token annotate article revision originally create ii list revisions token ever delete potentially add delete article enable complete straightforward track history data would exceedingly hard create average potential user expensive compute ii accurately track history token revisioned document non trivial task adapt state art algorithm produce dataset allow range analyse metrics already popular research go beyond generate complete wikipedia scale ensure quality allow researchers forego expensive text comparison computation far hinder scalable usage show data enable token level computation provenance measure survival content time detail conflict metrics fine grain interactions editors like partial revert additions metrics process gain several novel insights
code switch phenomenon bilingual speakers switch multiple languages communication importance develop language technologies codeswitching data immense give large populations routinely code switch high quality linguistic annotations extremely valuable nlp task performance often limit amount high quality label data however little data exist code switch paper describe crowd source universal part speech tag miami bangor corpus spanish english code switch speech split annotation task three subtasks one subset tokens label automatically one question specifically design disambiguate subset high frequency word general cascade approach remain data question display worker follow decision tree structure subtask extend adapt multilingual set universal tagset quality annotation process measure use hide check question annotate gold label overall agreement gold standard label majority vote ninety-five ninety-six three label average recall across part speech tag eighty-seven ninety-nine depend task
start work hope generate text synthesizer like musical synthesizer imitate certain linguistic style report focus text simplification use statistical machine translation smt techniques apply moses parallel corpus bible king jam version easy read version wikipedia article normal simplify report importance three main components smt phrase translation language model record change weight compare result quality simplify text term meteor bleu toward end report present examples text synthesize king jam style
maltese morphologically rich language hybrid morphological system feature concatenative non concatenative process paper analyse impact hybridity performance machine learn techniques morphological label cluster particular analyse dataset morphologically relate word cluster evaluate difference result concatenative nonconcatenative cluster also describe research carry morphological label particular focus verb category two evaluations carry one use unseen dataset another one use gold standard dataset manually label gold standard dataset split concatenative non concatenative analyse difference result two morphological systems
machine translation mt develop one hottest research topics natural language process nlp literature one important issue mt evaluate mt system reasonably tell us whether translation system make improvement traditional manual judgment methods expensive time consume unrepeatable sometimes low agreement hand popular automatic mt evaluation methods weaknesses firstly tend perform well language pair english target language weak english use source secondly methods rely many additional linguistic feature achieve good performance make metric unable replicate apply language pair easily thirdly popular metrics utilize incomprehensive factor result low performance practical task thesis address exist problems design novel mt evaluation methods investigate performances different languages firstly design augment factor yield highly accurate evaluationsecondly design tunable evaluation model weight factor optimise accord characteristics languages thirdly enhance version methods design concise linguistic feature use pos show methods yield even higher performance use external linguistic resources finally introduce practical performance metrics acl wmt workshop share task show propose methods robust across different languages
learn useful information across long time lag critical difficult problem temporal neural model task language model exist architectures address issue often complex costly train differential state framework dsf simple high perform design unify previously introduce gate neural model dsf model maintain longer term memory learn interpolate fast change data drive representation slowly change implicitly stable state require hardly parameters classical simple recurrent network within dsf framework new architecture present delta rnn language model word character level delta rnn outperform popular complex architectures long short term memory lstm gate recurrent unit gru regularize perform comparably several state art baselines subword level delta rnn performance comparable complex gate architectures
open domain question answer qa systems must interact external knowledge source web page find relevant information information source like wikipedia however well structure difficult utilize comparison knowledge base kbs work present two step approach question answer unstructured text consist retrieval step comprehension step comprehension present rnn base attention model novel mixture mechanism select answer either retrieve article fix vocabulary retrieval introduce hand craft model neural model rank relevant article achieve state art performance w iki ovies dataset reduce error forty experimental result demonstrate importance introduce components
demo paper present text simplification approach direct improve performance state art open relation extraction systems syntactically complex sentence often pose challenge current open approach develop simplification framework perform pre process step take single sentence input use set syntactic base transformation rule create textual input easier process subsequently apply open systems
significant source errors automatic speech recognition asr systems due pronunciation variations occur spontaneous conversational speech usually asr systems use finite lexicon provide one pronunciations word paper focus learn similarity function two pronunciations pronunciations canonical surface pronunciations word two surface pronunciations different word task generalize problems lexical access problem learn map word possible pronunciations define word neighborhoods also use dynamically increase size pronunciation lexicon predict asr errors propose two methods base recurrent neural network learn similarity function first base binary classification second base learn rank pronunciations demonstrate efficiency approach task lexical access use subset switchboard conversational speech corpus result suggest task methods superior previous methods base graphical bayesian methods
paper propose extension graph base sentiment lexicon induction methods incorporate distribute semantic word representations build similarity graph expand three dimensional sentiment lexicon also implement evaluate label propagation use four different word representations similarity metrics comprehensive evaluation four approach perform single data set demonstrate four methods generate significant number new sentiment assignments high accuracy highest correlations tau051 lowest error mean absolute error eleven obtain combine semantic distributional feature outperform distributional base semantic base label propagation model approach supervise algorithm
daily communications arabs use local dialects hard identify automatically use conventional classification methods dialect identification challenge task become complicate deal resourced dialects belong county region paper start analyze statistically algerian dialects order capture specificities relate prosody information extract utterance level coarse grain consonant vowel segmentation accord analysis find propose hierarchical classification approach speak arabic algerian dialect identification hadid take advantage fact dialects inherent property naturally structure hierarchy within hadid top hierarchical classification apply use deep neural network dnns method build local classifier every parent node hierarchy dialect structure framework implement evaluate algerian arabic dialects corpus whereas hierarchy dialect structure deduce historic linguistic knowledges result reveal within hd best classifier dnns compare support vector machine addition compare baseline flat classification system hadid give improvement six hundred and thirty-five term precision furthermore overall result evidence suitability prosody base hadid speaker independent dialect identification require less 6s test utterances
comparison document summarization article social media newswire argumentative zone az important task scientific paper analysis traditional methodology carry task rely feature engineer different level paper three model generate sentence vectors task sentence classification explore compare propose approach build sentence representations use learn embeddings base neural network learn word embeddings form feature space examine sentence map feature input classifiers supervise classification use ten cross validation scheme evaluation conduct argumentative zone az annotate article result show simply average word vectors sentence work better paragraph vector algorithm integrate specific cuewords loss function neural network improve classification performance comparison hand craft feature word2vec method categories however hand craft feature show strength classify categories
present model pragmatic refer expression interpretation ground communication task identify color descriptions draw upon predictions two recurrent neural network classifiers speaker listener unify recursive pragmatic reason framework experiment show combine pragmatic model interpret color descriptions accurately classifiers build much improvement result combine speaker listener perspectives observe pragmatic reason help primarily hardest case model must distinguish similar color utterances adequately express target color find make use newly collect corpus human utterances color reference game exhibit variety pragmatic behaviors also show embed speaker model reproduce many pragmatic behaviors
bangla handwrite recognition become important issue nowadays potentially important task specially bangla speak population bangladesh west bengal keep mind introduce comprehensive bangla handwritten character dataset name banglalekha isolate dataset contain bangla handwritten numerals basic character compound character dataset collect multiple geographical location within bangladesh include sample collect variety age group dataset also use classification problems ie gender age district largest dataset bangla handwritten character yet
investigate effective memory depth rnn model use n gram language model lm smooth experiment small corpus upenn treebank one million word train data 10k vocabulary find lstm cell dropout best model encode n gram state compare fee forward vanilla rnn model preserve sentence independence assumption lstm n gram match lstm lm performance n9 slightly outperform n13 allow dependencies across sentence boundaries lstm thirteen gram almost match perplexity unlimited history lstm lm lstm n gram smooth also desirable property improve increase n gram order unlike katz kneser ney back estimators use multinomial distributions target train instead usual one hot target slightly beneficial low n gram order experiment one billion word benchmark show result hold larger scale lstm smooth short n gram contexts provide significant advantage classic n gram model become effective long contexts n five depend task amount data match fully recurrent lstm model n13 may implications model short format text eg voice search query lms build lstm n gram lms may appeal practical situations state n gram lm succinctly represent n fourteen bytes store identity word context batch n gram contexts process parallel downside n gram context encode compute lstm discard make model expensive regular recurrent lstm lm
paper propose efficient less resource intensive strategies parse code mix data strategies constrain domain annotations rather leverage pre exist monolingual annotate resources train show methods produce significantly better result compare inform baseline besides also present data set four hundred and fifty hindi english code mix tweet hindi multilingual speakers evaluation data set manually annotate universal dependencies
investigate problem sentence level support argument detection relevant document user specify claim dataset contain claim associate citation article collect online debate website idebateorg manually label sentence level support arguments document along type study factual opinion reason characterize arguments different type explore whether leverage type information facilitate support arguments detection task experimental result show lambdamart burges two thousand and ten ranker use feature inform argument type yield better performance ranker train without type information
pre train word embeddings learn unlabeled text become standard component neural network architectures nlp task however case recurrent network operate word level representations produce context sensitive representations train relatively little label data paper demonstrate general semi supervise approach add pre train context embeddings bidirectional language model nlp systems apply sequence label task evaluate model two standard datasets name entity recognition ner chunk case achieve state art result surpass previous systems use form transfer joint learn additional label data task specific gazetteers
paper make focus contribution supervise aspect extraction show system perform aspect extraction many past domains retain result knowledge conditional random field crf leverage knowledge lifelong learn manner extract new domain markedly better traditional crf without use prior knowledge key innovation even crf train model still improve extraction experience applications
deep latent variable model show facilitate response generation open domain dialog systems however latent variables highly randomize lead uncontrollable generate responses paper propose framework allow conditional response generation base specific attribute attribute either manually assign automatically detect moreover dialog state speakers model separately order reflect personal feature validate framework two different scenarios attribute refer genericness sentiment state respectively experiment result testify potential model meaningful responses generate accordance specify attribute
consider problem learn general purpose paraphrastic sentence embeddings revisit set wieting et al 2016b find lstm recurrent network underperform word average present several developments together produce opposite conclusion include train sentence pair rather phrase pair average state represent sequence regularize aggressively improve lstms transfer learn supervise settings also introduce new recurrent architecture gate recurrent average network inspire average lstms outperform analyze learn model find evidence preferences particular part speech dependency relations
paper describe duluth systems participate task fourteen semeval two thousand and sixteen semantic taxonomy enrichment three relate systems formal evaluation discuss along numerous post evaluation run systems identify synonyms wordnet dictionaries measure gloss overlap systems perform better random baseline one post evaluation variation within respectable margin median result attain participate systems
dependency parse effective way inject linguistic knowledge many downstream task many practitioners wish efficiently parse sentence scale recent advance gpu hardware enable neural network achieve significant gain previous best model model still fail leverage gpus capability massive parallelism due requirement sequential process sentence response propose dilate iterate graph convolutional neural network dig cnns graph base dependency parse graph convolutional architecture allow efficient end end gpu parse experiment english penn treebank benchmark show dig cnns perform par best neural network parsers
cross lingual model transfer compel popular method predict annotations low resource language whereby parallel corpora provide bridge high resource language associate annotate corpora however parallel data readily available many languages limit applicability approach address drawbacks framework take advantage cross lingual word embeddings train solely high coverage bilingual dictionary propose novel neural network model joint train source data base cross lingual word embeddings show substantial empirical improvements baseline techniques also propose several active learn heuristics result improvements competitive benchmark methods
quality neural machine translation system depend substantially availability sizable parallel corpora low resource language pair case result poor translation quality inspire work computer vision propose novel data augmentation approach target low frequency word generate new sentence pair contain rare word new synthetically create contexts experimental result simulate low resource settings show method improve translation quality twenty-nine bleu point baseline thirty-two bleu back translation
distribute word representations widely use model word nlp task exist model generate one representation per word consider different mean word present two approach learn multiple topic sensitive representations per word use hierarchical dirichlet process observe model topics integrate topic distributions document obtain representations able distinguish different mean give word model yield statistically significant improvements lexical substitution task indicate commonly use single word representations even combine contextual information insufficient task
paper describe participation task five track two semeval two thousand and seventeen predict sentiment financial news headline specific company continuous scale one one tackle problem use number approach utilise support vector regression svr bidirectional long short term memory blstm find improvement four six use lstm model svr come fourth track report number different evaluations use finance specific word embed model reflect effect use different evaluation metrics
paper present computationally efficient machine learn method natural language response suggestion fee forward neural network use n gram embed feature encode message vectors optimize give message response pair high dot product value optimize search find response suggestions method evaluate large scale commercial e mail application inbox gmail compare sequence sequence approach new system achieve quality small fraction computational requirements latency
recently emerge intelligent assistants smartphones home electronics eg siri alexa see novel hybrids domain specific task orient speak dialogue systems open domain non task orient ones realize hybrid dialogue systems paper investigate determine whether user go chat system address lack benchmark datasets task construct new dataset consist fifteen one hundred and sixty utterances collect real log data commercial intelligent assistant release dataset facilitate future research activity addition investigate use tweet web search query handle open domain user utterances characterize task chat detection experiment demonstrate simple supervise methods effective use tweet search query improve f1 score eight thousand, six hundred and twenty-one eight thousand, seven hundred and fifty-three
end end neural machine translation nmt make remarkable progress recently still suffer data scarcity problem low resource language pair domains paper propose method zero resource nmt assume parallel sentence close probabilities generate sentence third language base assumption method able train source target nmt model student without parallel corpora available guide exist pivot target nmt model teacher source pivot parallel corpus experimental result show propose method significantly improve baseline pivot base model thirty bleu point across various language pair
even though linguistics free sequence sequence model neural machine translation nmt certain capability implicitly learn syntactic information source sentence paper show source syntax explicitly incorporate nmt effectively provide improvements specifically linearize parse tree source sentence obtain structural label sequence basis propose three different sort encoders incorporate source syntax nmt one parallel rnn encoder learn word label annotation vectors parallelly two hierarchical rnn encoder learn word label annotation vectors two level hierarchy three mix rnn encoder stitchingly learn word label annotation vectors sequence word label mix experimentation chinese english translation demonstrate three propose syntactic encoders able improve translation accuracy interest note simplest rnn encoder ie mix rnn encoder yield best performance significant improvement fourteen bleu point moreover depth analysis several perspectives provide reveal source syntax benefit nmt
paper introduce new model use name entity recognition coreference resolution entity link techniques approach task link people entities wikipedia people page correspond wikipedia page applicable task different general traditional entity link work limit domain namely people entities include pronouns entities whereas past pronouns never consider entities entity link build two model outperform baseline model significantly purpose project build model could use generate cleaner data future entity link task contribution include clean data set consist 50wikipedia people page two entity link model specifically tune domain
multi party conversational systems systems natural language interaction one people systems moment utterance send group moment reply group member several activities must do system utterance understand information search reason among others paper present challenge design build multi party conversational systems state art propose hybrid architecture use rule machine learn insights implement evaluate one finance domain
word cluster empirically show offer important performance improvements various task despite importance incorporation standard pipeline feature engineer rely trial error procedure one evaluate several hyper parameters like number cluster use order better understand role feature systematically evaluate effect four task name entity segmentation classification well five point sentiment classification quantification result strongly suggest cluster membership feature improve performance
typical neural machine translationnmt decoder generate sentence word word pack linguistic granularities time scale rnn paper propose new type decoder nmt split decode state two part update two different time scale specifically first predict chunk time scale state phrasal model top multiple word time scale state generate way target sentence translate hierarchically chunk word information different granularities leverage experiment show propose model significantly improve translation performance state art nmt model
linguistic typology study range structure present human language main goal field discover set possible phenomena universal merely frequent example languages vowels languages sound paper present first probabilistic treatment basic question phonological typology make natural vowel inventory introduce series deep stochastic point process contrast previous computational simulation base approach provide comprehensive suite experiment two hundred distinct languages
article describe software module call akshara prosodeme a2p converter hindi convert input grapheme prosedeme sequence phonemes specification syllable boundaries prosodic label software base two propose finite state machinestextemdash one syllabification another syllable label addition also use set nonlinear phonological rule propose foot formation hindi encompass solutions schwa deletion simple compound derive inflect word nonlinear phonological rule base metrical phonology provision recursive foot structure software module implement python test software syllabification syllable label schwa deletion prosodic label yield accuracy ninety-nine lexicon size twenty-eight thousand, six hundred and sixty-four word
attentional sequence sequence model become new standard machine translation one challenge model significant increase train decode cost compare phrase base systems focus efficient decode goal achieve accuracy close state art neural machine translation nmt achieve cpu decode speed throughput close phrasal decoder approach problem two angle first describe several techniques speed nmt beam search decoder obtain 44x speedup efficient baseline decoder without change decoder output second propose simple powerful network architecture use rnn gru lstm layer bottom follow series stack fully connect layer apply every timestep architecture achieve similar accuracy deep recurrent model small fraction train decode cost combine techniques best system achieve competitive accuracy three hundred and eighty-three bleu wmt english french newstest2014 decode one hundred word sec single thread cpu believe best publish accuracy speed trade nmt system
propose recurrent neural model generate natural language question document condition answer show train model use combination supervise reinforcement learn teacher force standard maximum likelihood train fine tune model use policy gradient techniques maximize several reward measure question quality notably one reward performance question answer system motivate question generation mean improve performance question answer systems model train evaluate recent question answer dataset squad
paper present senti17 system use ten convolutional neural network convnet assign sentiment label tweet network consist convolutional layer follow fully connect layer softmax top ten instance network initialize word embeddings input different initializations network weight combine result instance select sentiment label give majority ten voters system rank fourth semeval two thousand and seventeen task4 thirty-eight systems six hundred and seventy-four
cross lingual text classificationcltc task classify document write different languages taxonomy categories paper present novel approach cltc build model distillation adapt extend framework originally propose model compression use soft probabilistic predictions document label rich language induce supervisory label parallel corpus document train classifiers successfully new languages label train data available adversarial feature adaptation technique also apply model train reduce distribution mismatch conduct experiment two benchmark cltc datasets treat english source language german french japan chinese unlabeled target languages propose approach advantageous comparable performance state art methods
argumentation mine aim automatically extract premise claim discourse structure natural language texts great demand argumentation corpora customer review however due controversial nature argumentation annotation task exist large scale argumentation corpora customer review work novelly use crowdsourcing technique collect argumentation annotations chinese hotel review first chinese argumentation dataset corpus include four thousand, eight hundred and fourteen argument component annotations four hundred and eleven argument relation annotations annotations qualities comparable widely use argumentation corpora languages
argument component boundary detection acbd important sub task argumentation mine aim identify word sequence constitute argument components usually consider first sub task argumentation mine pipeline exist acbd methods heavily depend task specific knowledge require considerable human efforts feature engineer tackle problems work formulate acbd sequence label problem propose variety recurrent neural network rnn base methods use domain specific handcraft feature beyond relative position sentence document particular propose novel joint rnn model predict whether sentence argumentative use predict result precisely detect argument component boundaries evaluate techniques two corpora two different genres result suggest joint rnn model obtain state art performance datasets
present deep speaker neural speaker embed system map utterances hypersphere speaker similarity measure cosine similarity embeddings generate deep speaker use many task include speaker identification verification cluster experiment rescnn gru architectures extract acoustic feature mean pool produce utterance level speaker embeddings train use triplet loss base cosine similarity experiment three distinct datasets suggest deep speaker outperform dnn base vector baseline example deep speaker reduce verification equal error rate fifty relatively improve identification accuracy sixty relatively text independent dataset also present result suggest adapt model train mandarin improve accuracy english speaker recognition
paper build morphological chain agglutinative languages use log linear model morphological segmentation task model base unsupervised morphological segmentation system call morphochains extend morphochains log linear model expand candidate space recursively cover split point agglutinative languages turkish whereas original model candidates generate consider binary segmentation word result show improve state art turkish score twelve f measure seventy-two improve english score three f measure seventy-four eventually system outperform morphochains well know unsupervised morphological segmentation systems result indicate candidate generation play important role unsupervised log linear model learn use contrastive estimation negative sample
many modern nlp systems rely word embeddings previously train unsupervised manner large corpora base feature efforts obtain embeddings larger chunk text sentence however successful several attempt learn unsupervised representations sentence reach satisfactory enough performance widely adopt paper show universal sentence representations train use supervise data stanford natural language inference datasets consistently outperform unsupervised methods like skipthought vectors wide range transfer task much like computer vision use imagenet obtain feature transfer task work tend indicate suitability natural language inference transfer learn nlp task encoder publicly available
voice browser applications text speech tts automatic speech recognition asr systems crucially depend pronunciation lexicon present paper describe model pronunciation lexicon hindi develop automatically generate output form hindi two level ps short prosodic structure latter level involve syllable division stress placement paper describe tool develop generate two level output lexica hindi
major system mnemonic system use memorize sequence number work present method automatically generate sentence encode give number propose several encode model compare promise ones password memorability study result study show model combine part speech sentence templates n gram language model produce memorable password representations
nowadays geographic information relate twitter crucially important fine grain applications however amount geographic information avail able twitter low make pursuit many applications challenge circumstances estimate location tweet important goal study unlike previous study estimate pre define district classification task study employ probability distribution represent richer information tweet location also ambiguity realize model propose convolutional mixture density network cmdn use text data estimate mixture model parameters experimentally obtain result reveal cmdn achieve highest prediction performance among method predict exact coordinate also provide quantitative representation location ambiguity tweet properly work extract reliable location estimations
paper introduce reinforce mnemonic reader machine read comprehension task enhance previous attentive readers two aspects first reattention mechanism propose refine current attentions directly access past attentions temporally memorize multi round alignment architecture avoid problems attention redundancy attention deficiency second new optimization approach call dynamic critical reinforcement learn introduce extend standard supervise method always encourage predict acceptable answer address convergence suppression problem occur traditional reinforcement learn algorithms extensive experiment stanford question answer dataset squad show model achieve state art result meanwhile model outperform previous systems six term exact match f1 metrics two adversarial squad datasets
type level word embeddings use set parameters represent instance word regardless context ignore inherent lexical ambiguity language instead embed semantic concepts synsets define wordnet represent word token particular context estimate distribution relevant semantic concepts use new context sensitive embeddings model predict prepositional phrasepp attachments jointly learn concept embeddings model parameters show use context sensitive embeddings improve accuracy pp attachment model fifty-four absolute point amount three hundred and forty-four relative reduction errors
prevalent approach sequence sequence learn map input sequence variable length output sequence via recurrent neural network introduce architecture base entirely convolutional neural network compare recurrent model computations elements fully parallelize train optimization easier since number non linearities fix independent input length use gate linear units ease gradient propagation equip decoder layer separate attention module outperform accuracy deep lstm setup wu et al two thousand and sixteen wmt fourteen english german wmt fourteen english french translation order magnitude faster speed gpu cpu
knowledge graph kgs could provide essential relational information entities widely utilize various knowledge drive applications since overall human knowledge innumerable still grow explosively change frequently knowledge construction update inevitably involve automatic mechanisms less human supervision usually bring plenty noise conflict kgs however conventional knowledge representation learn methods assume triple facts exist kgs share significance without noise address problem propose novel confidence aware knowledge representation learn framework ckrl detect possible noise kgs learn knowledge representations confidence simultaneously specifically introduce triple confidence conventional translation base methods knowledge representation learn make triple confidence flexible universal utilize internal structural information kgs propose three kinds triple confidences consider local global structural information experiment evaluate model knowledge graph noise detection knowledge graph completion triple classification experimental result demonstrate confidence aware model achieve significant consistent improvements task confirm capability ckrl model confidence structural information kg noise detection knowledge representation learn
prosody describe form function sentence use suprasegmental feature speech prosody phenomena explore domain higher phonological constituents word phonological phrase intonational phrase study prosody word level call word prosody word level call sentence prosody word prosody describe stress pattern compare prosodic feature constituent syllables sentence prosody involve study phrase pattern intonatonal pattern language aim study summarize exist work hindi prosody carry different domain language speech process review present systematic fashion could useful resource one want build exist work
drug drug interaction ddi vital information physicians pharmacists intend co administer two drug thus several ddi databases construct avoid mistakenly combine use recent years automatically extract ddis biomedical text draw researchers attention however exist work utilize either complex feature engineer nlp tool insufficient sentence comprehension inspire deep learn approach natural language process propose recur rent neural network model multiple attention layer ddi classification evaluate model two thousand and thirteen semeval ddiextraction dataset experiment show model classify drug pair correct ddi categories outperform exist nlp deep learn methods
semantic parse emerge significant powerful paradigm natural language interface question answer systems traditional methods build semantic parser rely high quality lexicons hand craft grammars linguistic feature limit apply domain representation paper propose general approach learn denotations base seq2seq model augment attention mechanism encode input sequence vectors use dynamic program infer candidate logical form utilize fact similar utterances similar logical form help reduce search space learn policy seq2seq model learn mappings gradually noise curriculum learn adopt make learn smoother test method arithmetic domain show model successfully infer correct logical form learn word mean compositionality operation order simultaneously
present triviaqa challenge read comprehension dataset contain 650k question answer evidence triple triviaqa include 95k question answer pair author trivia enthusiasts independently gather evidence document six per question average provide high quality distant supervision answer question show comparison recently introduce large scale datasets triviaqa one relatively complex compositional question two considerable syntactic lexical variability question correspond answer evidence sentence three require cross sentence reason find answer also present two baseline algorithms feature base classifier state art neural network perform well squad read comprehension neither approach come close human performance twenty-three forty vs eighty suggest triviaqa challenge testbed worth significant future study data code available http nlpcswashingtonedu triviaqa
relation extraction important sub task information extraction potential employ deep learn dl model creation large datasets use distant supervision review compare contributions pitfalls various dl model use task help guide path ahead
recently several data set associate data text create train data text surface realisers unclear however extent surface realisation task exercise data set linguistically challenge data set provide enough variety encourage development generic high quality data text surface realisers paper argue data set important drawbacks back claim use statistics metrics manual evaluation conclude elicit set criteria creation data text benchmark could help better support development evaluation comparison linguistically sophisticate data text surface realisers
work present minimal neural model constituency parse base independent score label span show model compatible classical dynamic program techniques also admit novel greedy top inference algorithm base recursive partition input demonstrate empirically prediction scheme competitive recent work combine basic extensions score model capable achieve state art single model performance penn treebank nine thousand, one hundred and seventy-nine f1 strong performance french treebank eight thousand, two hundred and twenty-three f1
distant supervision significantly reduce human efforts build train data many classification task promise technique often introduce noise generate train data severely affect model performance paper take deep look application distant supervision relation extraction show dynamic transition matrix effectively characterize noise train data build distant supervision transition matrix effectively train use novel curriculum learn base method without direct supervision noise thoroughly evaluate approach wide range extraction scenarios experimental result show approach consistently improve extraction result outperform state art various evaluation scenarios
short message service sms spam serious problem vietnam availability cheap pre pay sms package systems detect filter spam message english use machine learn techniques analyze content message classify vietnamese research spam email filter none focus sms work propose first system filter vietnamese spam sms first propose appropriate preprocessing method since exist tool vietnamese preprocessing give good accuracy dataset experiment vector representations classifiers find best model problem system achieve accuracy ninety-four label spam message misclassification rate legitimate message relatively small four encourage result compare english serve strong baseline future development vietnamese sms spam prevention systems
semantic role label srl task natural language process detect classify semantic arguments associate predicate sentence important step towards understand mean natural language exist srl systems well study languages like english chinese japanese system vietnamese language paper present first srl system vietnamese encourage accuracy first demonstrate simple application srl techniques develop english could give good accuracy vietnamese introduce new algorithm extract candidate syntactic constituents much accurate common node map algorithm usually use identification step finally classification step addition common linguistic feature propose novel useful feature use srl srl system achieve f1 score seven thousand, three hundred and fifty-three vietnamese propbank corpus system include software corpus available open source project believe good baseline development future vietnamese srl systems
paper demonstrate end end neural network architectures vietnamese name entity recognition best model combination bidirectional long short term memory bi lstm convolutional neural network cnn conditional random field crf use pre train word embeddings input achieve f1 score eight thousand, eight hundred and fifty-nine standard test set system able achieve comparable performance first rank system vlsp campaign without use syntactic hand craft feature also give extensive empirical study use common deep learn model vietnamese ner word character level
tree structure neural network prove effective learn semantic representations exploit syntactic information spite success exist model suffer underfitting problem recursively use share compositional function throughout whole compositional process lack expressive power due inability capture richness compositionality paper address issue introduce dynamic compositional neural network tree structure dc treenn compositional function dynamically generate meta network role meta network capture metaknowledge across different compositional rule formulate experimental result two typical task show effectiveness propose model
propose new fast word embed technique use hash function method derandomization new type random projections disregard classic constraint use design random projections ie preserve pairwise distance particular normed space solution exploit extremely sparse non negative random projections experiment show propose method achieve competitive result comparable neural embed learn techniques however fraction computational complexity methods propose derandomization enhance computational space complexity method possibility apply weight methods positive pointwise mutual information ppmi model construction reduce dimensionality impart high discriminatory power result embeddings obviously method come know benefit random projection base techniques ease update
attentional rnn base encoder decoder model abstractive summarization achieve good performance short input output sequence longer document summaries however model often include repetitive incoherent phrase introduce neural network model novel intra attention attend input continuously generate output separately new train method combine standard supervise word prediction reinforcement learn rl model train supervise learn often exhibit exposure bias assume grind truth provide step train however standard word prediction combine global sequence prediction train rl result summaries become readable evaluate model cnn daily mail new york time datasets model obtain four thousand, one hundred and sixteen rouge one score cnn daily mail dataset improvement previous state art model human evaluation also show model produce higher quality summaries
replace hand engineer pipelines end end deep learn systems enable strong result applications like speech object recognition however causality latency constraints production systems put end end speech model back underfitting regime expose bias model show overcome scale ie train bigger model data work systematically identify address source bias reduce error rat twenty remain practical deployment achieve utilize improve neural architectures stream inference solve optimization issue employ strategies increase audio label model versatility
vector space representations provide geometric tool reason similarity set object relationships recent machine learn methods derive vector space embeddings word eg word2vec achieve considerable success natural language process vector space also show exhibit surprise capacity capture verbal analogies similar result natural image give new life classic model analogies parallelograms first propose cognitive scientists evaluate parallelogram model analogy apply modern word embeddings provide detail analysis extent approach capture human relational similarity judgments large benchmark dataset find semantic relationships better capture others provide evidence deeper limitations parallelogram model base intrinsic geometric constraints vector space parallel classic result first order similarity
transition base dependency parsers often need sequence local shift reduce operations produce certain attachments correct individual decisions hence require global information sentence context mistake error propagation paper propose novel transition system arc swift enable direct attachments tokens farther apart single transition allow parser leverage lexical information directly transition decisions hence arc swift achieve significantly better performance small beam size parsers reduce error thirty-seven seventy-six relative use exist transition systems penn treebank dependency parse task english universal dependencies
consider problem translate high level textual descriptions formal representations technical documentation part effort model mean documentation focus specifically problem learn translational correspondences text descriptions ground representations target documentation formal representation function code templates approach exploit parallel nature documentation tight couple high level text low level representations aim learn data collect mine technical document parallel text representation pair use train simple semantic parse model report new baseline result sixteen novel datasets include standard library documentation nine popular program languages across seven natural languages small collection unix utility manuals
empathy define behavioral sciences express ability human be recognize understand react emotions attitudes beliefs others lack operational definition empathy make difficult measure paper address two relate problems automatic affective behavior analysis design annotation protocol automatic recognition empathy speak conversations propose evaluate annotation scheme empathy inspire modal model emotions annotation scheme evaluate corpus real life dyadic speak conversations context behavioral analysis design automatic segmentation classification system empathy give different speech language level representation empathy may communicate investigate feature derive lexical acoustic space feature development process design support fusion automatic selection relevant feature high dimensional space automatic classification system evaluate call center conversations show significantly better performance baseline
present joint model approach identify salient discussion point speak meet well label discourse relations speaker turn variation model also discuss discourse relations treat latent variables experimental result two popular meet corpora show joint model outperform state art approach phrase base content selection discourse relation prediction task also evaluate model predict consistency among team members understand group decisions classifiers train feature construct model achieve significant better predictive performance state art
debate deliberation play essential roles politics government model presume debate mainly via superior style agenda control ideally however debate would merit function side stronger arguments propose predictive model debate estimate effect linguistic feature latent persuasive strengths different topics well interactions two use dataset one hundred and eighteen oxford style debate model combination content latent topics style linguistic feature allow us predict audience adjudicate winners seventy-four accuracy significantly outperform linguistic feature alone sixty-six model find win side employ stronger arguments allow us identify linguistic feature associate strong weak arguments
drug reposition dr refer identification novel indications approve drug requirement huge investment time well money risk failure clinical trials lead surge interest drug reposition dr exploit two major aspects associate drug diseases existence similarity among drug among diseases due share involve genes pathways common biological effect exist methods identify drug disease association majorly rely information available structure databases hand abundant information available form free texts biomedical research article fully exploit word embed obtain vector representation word large corpora free texts use neural network methods show give significant performance several natural language process task work propose novel way representation learn obtain feature drug diseases combine complementary information available unstructured texts structure datasets next use matrix completion approach feature vectors learn projection matrix drug disease vector space propose method show competitive performance state art methods case study alzheimer hypertension diseases show predict associations match exist knowledge
neural task orient dialogue systems often struggle smoothly interface knowledge base work seek address problem propose new neural dialogue agent able effectively sustain ground multi domain discourse novel key value retrieval mechanism model end end differentiable need explicitly model dialogue state belief trackers also release new dataset three thousand and thirty-one dialogues ground underlie knowledge base span three distinct task car personal assistant space calendar schedule weather information retrieval point interest navigation architecture simultaneously train data domains significantly outperform competitive rule base system exist neural dialogue architectures provide domains accord automatic human evaluation metrics
biomedical information extraction excite field crossroads natural language process biology medicine encompass variety different task require application state art nlp techniques ner relation extraction paper provide overview problems field discuss techniques use solve
paper argue judicial use formal language theory grammatical inference invaluable tool understand deep neural network represent learn long term dependencies temporal sequence learn experiment conduct two type recurrent neural network rnns six formal languages draw strictly local sl strictly piecewise sp class network simple rnns rnns long short term memory rnns lstms vary size sl sp class among simplest mathematically well understand hierarchy subregular class encode local long term dependencies respectively grammatical inference algorithm regular positive negative inference rpni provide baseline accord earlier research lstm architecture capable learn long term dependencies outperform rnns result experiment challenge narrative first lstms performance generally worse sp experiment sl ones second rnns perform lstms complex sp experiment perform comparably others
present novel neural network model learn pos tag graph base dependency parse jointly model use bidirectional lstms learn feature representations share pos tag dependency parse task thus handle feature engineer problem extensive experiment nineteen languages universal dependencies project show model outperform state art neural network base stack propagation model joint pos tag transition base dependency parse result new state art code open source available together pre train model https githubcom datquocnguyen jptdp
frame stack broadly apply end end neural network train like connectionist temporal classification ctc lead accurate model faster decode however well suit conventional neural network base context dependent state acoustic model decoder unchanged paper propose novel frame retain method apply decode system combine frame retain frame stack could reduce time consumption train decode long short term memory lstm recurrent neural network rnns use achieve almost linear train speedup reduce relative forty-one real time factor rtf time recognition performance degradation improve sightly shenma voice search dataset mandarin
present semi supervise way train character base encoder decoder recurrent neural network morphological reinflection task generate one inflect word form another achieve use unlabeled tokens random string train data autoencoding task adapt network morphological reinflection perform multi task train thus use limit label data effectively obtain ninety-nine improvement state art baselines eight different languages
paper discuss machine learn could use produce systematic objective political discourse analysis political footprints vector space model vsms apply political discourse vectors represent word produce train english lexicon large text corpora paper present simple implementation political footprints heuristics use application four case youn kyoto protocol paris agreement two yous presidential elections reader offer number reason believe political footprints produce meaningful result along suggestions improve implementation
distribute representations sentence develop recently represent mean real value vectors however clear much information representations retain polarity sentence study question decode sentiment unsupervised sentence representations learn different architectures sensitive order word order sentence none nine typologically diverse languages sentiment result recursive composition lexical items grammatical strategies negation concession result manifold show one size fit representation architecture outperform others across board rather top rank architectures depend language data hand moreover find several case additive composition model base skip gram word vectors may surpass supervise state art architectures bidirectional lstms finally provide possible explanation observe variation base type negative constructions language
german relative clauses position situ extraposed potential factor variation might information density study hypothesis test corpus 17th century german funeral sermons referent relative clauses matrix clauses attention state determine first calculation second calculation word surprisal value determine use bi gram language model third calculation surprisal value accommodate whether first occurrence word question three calculations point direction situ relative clauses rate new referents lower average surprisal value lower especially accommodate surprisal value extraposed relative clauses indicate formation density factor govern choice situ extraposed relative clauses study also shed light intrinsic relation ship information theoretic concept information density formation structural concepts givenness use linguistic perspective
singlish interest acl community linguistically major creole base english computationally information extraction sentiment analysis regional social media investigate dependency parse singlish construct dependency treebank universal dependencies scheme train neural network model integrate english syntactic knowledge state art parser train singlish treebank result show english knowledge lead twenty-five relative error reduction result parser eight thousand, four hundred and forty-seven accuracies best knowledge first use neural stack improve cross lingual dependency parse low resource languages make annotation parser available research
introduce parlai pronounce par lay open source software platform dialog research implement python available http parlai goal provide unify framework share train test dialog model integration amazon mechanical turk data collection human evaluation online reinforcement learn repository machine learn model compare others model improve upon exist architectures twenty task support first release include popular datasets squad babi task mctest wikiqa qacnn qadailymail cbt babi dialog ubuntu opensubtitles vqa several model integrate include neural model memory network seq2seq attentive lstms
psycholinguistic properties word use various approach natural language process task text simplification readability assessment properties subjective involve costly time consume survey gather recent approach use limit datasets psycholinguistic properties extend automatically large lexicons however resources use approach available languages study present method infer psycholinguistic properties brazilian portuguese bp use regressors build light set feature usually available less resourced languages word length frequency list lexical databases compose school dictionaries word embed model correlations properties infer close obtain relate work result resource contain twenty-six thousand, eight hundred and seventy-four word bp annotate concreteness age acquisition imageability subjective frequency
paper reformulate spell correction problem machine translation task encoder decoder framework reformulation enable us use single model solve problem traditionally formulate learn language model error model model employ multi layer recurrent neural network encoder decoder demonstrate effectiveness model use internal dataset train data automatically obtain user log model offer competitive performance compare state art methods require feature engineer hand tune model
introduce recurrent additive network rans new gate rnn distinguish use purely additive latent state update every time step new state compute gate component wise sum input previous state without non linearities commonly use rnn transition dynamics formally show run state weight sum input vectors gate contribute compute weight sum despite relatively simple functional form experiment demonstrate rans perform par lstms benchmark language model problems result show many non linear computations lstms relate network essential least problems consider suggest gate computational work previously understand
increase online customer opinions specialise websites social network necessity automatic systems help organise classify customer review domain specific aspect categories sentiment polarity important ever supervise approach aspect base sentiment analysis obtain good result domain language train manually label data train supervise systems domains languages usually costly time consume work describe w2vlda almost unsupervised system base topic model combine unsupervised methods minimal configuration perform aspect category classifiation aspect term opinion word separation sentiment polarity classification give domain language evaluate performance aspect sentiment classification multilingual semeval two thousand and sixteen task five absa dataset show competitive result several languages english spanish french dutch domains hotels restaurants electronic devices
evolution neural network base methods automatic speech recognition asr field advance level build application speech interface reality spite advance build real time speech recogniser face several problems low recognition accuracy domain constraint vocabulary word low recognition accuracy problem address improve acoustic model language model decoder rescoring n best list output decoder consider n best list rescoring approach improve recognition accuracy methods literature use grammatical lexical syntactic semantic connection word recognise sentence feature rescore paper try see semantic relatedness word sentence rescore n best list semantic relatedness compute use transecitebordes2013translating method low dimensional embed triple knowledge graph novelty paper application semantic web automatic speech recognition
past century personality theory research successfully identify core set characteristics consistently describe explain fundamental differences way people think feel behave characteristics derive theory dictionary analyse survey research use explicit self report availability social media data span millions users make possible automatically derive characteristics language use large scale take advantage linguistic information available facebook study process infer new set potential human traits base unprompted language use subject new traits comprehensive set evaluations compare popular five factor model personality find language base trait construct often generalizable often predict non questionnaire base outcomes better questionnaire base traits eg entities someone like income intelligence quotient factor remain nearly stable traditional factor approach suggest value new construct personality derive everyday human language use
recently encoder decoder neural network show impressive performance many sequence relate task architecture commonly use attentional mechanism allow model learn alignments source target sequence attentional mechanisms use today base global attention property require computation weight summarization whole input sequence generate encoder state however computationally expensive often produce misalignment longer input sequence furthermore fit monotonous leave right nature several task automatic speech recognition asr grapheme phoneme g2p etc paper propose novel attention mechanism local monotonic properties various ways control properties also explore experimental result asr g2p machine translation two languages similar sentence structure demonstrate propose encoder decoder model local monotonic attention could achieve significant performance improvements reduce computational complexity comparison one use standard global attention architecture
introduce architecture tensor product recurrent network tprn application tprn internal representations learn end end optimization deep neural network perform textual question answer qa task interpret use basic concepts linguistic theory performance penalty need pay increase interpretability propose model perform comparably state art system squad qa task internal representation interpret tensor product representation input word model select symbol encode word role place symbol bind two together selection via soft attention overall interpretation build interpretations symbols recruit train model interpretations roles use model find support initial hypothesis symbols interpret lexical semantic word mean roles interpret approximations grammatical roles categories subject wh word determiner etc fine grain analysis reveal specific correspondences learn roles part speech assign standard tagger toutanova et al two thousand and three find several discrepancies model favor sense model learn significant aspects grammar expose solely linguistically unannotated text question answer prior linguistic knowledge give model give mean build representations use symbols roles inductive bias favor use approximately discrete manner
paper deep investigation cross language plagiarism detection methods new recently introduce open dataset contain parallel comparable collections document multiple characteristics different genres languages size texts investigate cross language plagiarism detection methods six language pair two granularities text units order draw robust conclusions best methods deeply analyze correlations across document style languages
syntactic parse key task natural language process task dominate symbolic grammar base parsers neural network distribute representations challenge methods article show exist symbolic parse algorithms cross border entirely formulate distribute representations end introduce version traditional cocke younger kasami cyk algorithm call cyk entirely define distribute representations cyk use matrix multiplication real number matrices size independent length input string operations compatible traditional neural network experiment show cyk approximate original cyk algorithm show cyk entirely perform distribute representations open way definition recurrent layer cyk inform neural network
number word form agglutinative languages theoretically infinite variety word form introduce sparsity many natural language process task part speech tag pos tag one task often suffer sparsity paper present unsupervised bayesian model use hide markov model hmms joint pos tag stem agglutinative languages use stem reduce sparsity pos tag two task jointly perform provide mutual benefit task result show joint pos tag stem improve pos tag score present result turkish finnish agglutinative languages english morphologically poor language
introduce technique augment neural text speech tts lowdimensional trainable speaker embeddings generate different voice single model start point show improvements two state ofthe art approach single speaker neural tts deep voice one tacotron introduce deep voice two base similar pipeline deep voice one construct higher performance build block demonstrate significant audio quality improvement deep voice one improve tacotron introduce post process neural vocoder demonstrate significant audio quality improvement demonstrate technique multi speaker speech synthesis deep voice two tacotron two multi speaker tts datasets show single neural tts system learn hundreds unique voice less half hour data per speaker achieve high audio quality synthesis preserve speaker identities almost perfectly
recognize textual entailment fundamental task variety text mine natural language process applications paper propose simple neural model rte problem first match word hypothesis similar word premise produce augment representation hypothesis condition premise sequence word pair lstm model use model augment sequence final output lstm feed softmax layer make prediction besides base model order enhance performance also propose three techniques integration multiple word embed library bi way integration ensemble base model average experimental result snli dataset show three techniques effective boost predicative accuracy method outperform several state state ones
introduce neural network represent sentence compose word accord induce binary parse tree use tree lstm composition function apply along tree structure find fully differentiable natural language chart parser model simultaneously optimise composition function parser thus eliminate need externally provide parse tree normally require tree lstm therefore see tree base rnn unsupervised respect parse tree fully differentiable model easily train shelf gradient descent method backpropagation demonstrate achieve better performance compare various supervise tree lstm architectures textual entailment task reverse dictionary task
biomedical events describe complex interactions various biomedical entities event trigger word phrase typically signify occurrence event event trigger identification important first step event extraction methods however many current approach either rely complex hand craft feature consider feature within window paper propose method take advantage recurrent neural network rnn extract higher level feature present across sentence thus hide state representation rnn along word entity type embed feature avoid rely complex hand craft feature generate use various nlp toolkits experiment show achieve state art f1 score multi level event extraction mlee corpus also perform category wise analysis result discuss importance various feature trigger identification task
individuals social media may reveal various state crisis eg suicide self harm abuse eat disorder detect crisis social media text automatically accurately profound consequences however detect general state crisis without explain limit applications explanation context coherent concise subset text rationalize crisis detection explore several methods detect explain crisis use combination neural non neural techniques evaluate techniques unique data set obtain koko anonymous emotional support network available various message applications annotate small subset sample label crisis correspond explanations best technique significantly outperform baseline detection explanation
conversational large vocabulary continuous speech recognition lvcsr task two thousand hours audio commonly use train state art model collection label conversational audio however prohibitively expensive laborious error prone furthermore academic corpora like fisher english two thousand and four switchboard one thousand, nine hundred and ninety-two inadequate train model sufficient accuracy unbounded space conversational speech corpora also timeworn due date acoustic telephony feature rapid advancement colloquial vocabulary idiomatic speech last decades utilize colossal scale unlabeled telephony dataset propose technique construct modern high quality conversational speech train corpus order hundreds millions utterances tens thousands hours acoustic language model train describe data collection selection train evaluate result update speech recognition system test corpus 7k manually transcribe utterances show relative word error rate wer reductions thirty-five nineteen agent caller utterances seed model five absolute wer improvements ibm watson stt conversational speech task
liu et al two thousand and seventeen provide comprehensive account research dependency distance human languages article rich useful report complex subject expand specific issue research computational linguistics specifically natural language process inform ddm research vice versa aspects explore much article liu et al elsewhere probably due little overlap research communities may provide interest insights improve understand evolution human languages mechanisms brain process understand language construction effective computer systems achieve goal
body research abusive language detection analysis grow need critical consideration relationships different subtasks group label base work hate speech cyberbullying online abuse propose typology capture central similarities differences subtasks discuss implications data annotation feature construction emphasize practical action take researchers best approach abusive language detection subtask interest
one long term goals artificial intelligence build agent communicate intelligently human natural language exist work natural language learn rely heavily train pre collect dataset annotate label lead agent essentially capture statistics fix external train data train data essentially static snapshot representation knowledge annotator agent train way limit adaptiveness generalization behavior moreover different language learn process humans language acquire communication take speak action learn consequences speak action interactive manner paper present interactive set ground natural language learn agent learn natural language interact teacher learn feedback thus learn improve language skills take part conversation achieve goal propose model incorporate imitation reinforcement leverage jointly sentence reward feedbacks teacher experiment conduct validate effectiveness propose approach
evaluate character level translation method neural semantic parse large corpus sentence annotate abstract mean representations amrs use sequence sequence model trivial preprocessing postprocessing amrs obtain baseline accuracy five hundred and thirty-one f score amr triple examine five different approach improve baseline result reorder amr branch match word order input sentence increase performance five hundred and eighty-three ii add part speech tag automatically produce input show improvement well five hundred and seventy-two iii introduction super character conflate frequent sequence character single character reach five hundred and seventy-four iv optimize train process use pre train average set model increase performance five hundred and eighty-seven v add silver standard train data obtain shelf parser yield biggest improvement result f score six hundred and forty combine five techniques lead f score seven hundred and ten holdout data state art amr parse remarkable relative simplicity approach
micro blogging service twitter lucrative source data mine applications global sentiment due omnifariousness subject mention data item inefficient run data mine algorithm raw data paper discuss algorithm accurately classify entire stream give number mutually exclusive collectively exhaustive stream upon data mine algorithm run separately yield relevant result high efficiency
extract opinion target important task sentiment analysis product review complementary entities products one important type opinion target may work together review product paper address problem complementary entity recognition cer supervise sequence label capability expand domain knowledge key value pair unlabeled review automatically learn enhance knowledge base feature use conditional random field crf base learner augment crf knowledge base feature call knowledge base crf kcrf short conduct experiment show kcrf effectively improve performance supervise cer task
study overall structure vocabulary dynamics become possible due creation diachronic text corpora especially google book ngram article discuss question core change rate degree core word cover texts different periods last three centuries six main european languages present google book ngram compare main result high stability core change rate analogous stability swadesh list
products review one major resources determine public sentiment exist literature review sentiment analysis mainly utilize supervise paradigm need label data train suffer domain dependency article address issue describe completely automatic approach sentiment analysis base unsupervised ensemble learn method consist two phase first phase contextual analysis five process namely one data preparation two spell correction three intensifier handle four negation handle five contrast handle second phase comprise unsupervised learn approach ensemble cluster classifiers use majority vote mechanism different weight scheme base classifier ensemble method modify k mean algorithm base classifier modify extract initial centroids feature set via use sentwordnet swn also introduce new sentiment analysis problems australian airlines home builders offer potential benchmark problems sentiment analysis field experiment datasets different domains show contextual analysis ensemble phase improve cluster performance term accuracy stability generalization ability
humor define characteristic human be goal develop methods automatically detect humorous statements rank continuous scale paper report result use language model approach outline plan use methods deep learn
authorship attribution natural language process task widely study often consider small order statistics paper explore complex network approach assign authorship texts base mesoscopic representation attempt capture flow narrative indeed report work approach allow identification dominant narrative structure study author achieve due ability mesoscopic approach take account relationships different necessarily adjacent part text able capture story flow potential propose approach illustrate principal component analysis comparison chance baseline method network visualization visualizations reveal individual characteristics author understand kind calligraphy
paper present state art system vietnamese name entity recognition ner incorporate automatic syntactic feature word embeddings input bidirectional long short term memory bi lstm system although simpler deep learn architectures achieve much better result vietnamese ner propose method achieve overall f1 score nine thousand, two hundred and five test set evaluation campaign organize late two thousand and sixteen vietnamese language speech process vlsp community name entity recognition system outperform best previous systems vietnamese ner large margin
language variety identification aim label texts native language eg spanish portuguese english specific variation eg argentina chile mexico peru spain brazil portugal uk us work propose low dimensionality representation ldr address task five different varieties spanish argentina chile mexico peru spain compare ldr method common state art representations show increase accuracy thirty-five furthermore compare ldr two reference distribute representation model experimental result show competitive performance dramatically reduce dimensionality increase big data suitability six feature per variety additionally analyse behaviour employ machine learn algorithms discriminate feature finally employ alternative dataset test robustness low dimensionality representation another set similar languages
present transition base dependency parser use convolutional neural network compose word representations character character composition model show great improvement word lookup model especially parse agglutinative languages improvements even better use pre train word embeddings extra data spmrl data set system outperform previous best greedy parser ballesteros et al two thousand and fifteen margin three average
investigate pertinence methods algebraic topology text data analysis methods enable development mathematically principled isometric invariant mappings set vectors document embed stable respect geometry document select metric space work evaluate utility topology base document representations traditional nlp task specifically document cluster sentiment classification find embeddings benefit text analysis fact performance worse simple techniques like textittf idf indicate geometry document provide enough variability classification basis topic sentiment choose datasets
paper investigate analyze effect dependency information predicate argument structure analysis pasa zero anaphora resolution zar japanese show straightforward approach pasa zar work effectively even dependency information available construct analyzer directly predict relationships predicate arguments semantic roles pos tag corpus feature system design compensate absence syntactic information use feature use dependency parse reference also construct analyzers use oracle dependency real dependency parse result compare system use syntactic information verify improvement provide dependencies crucial
past years attention mechanisms become indispensable component end end neural machine translation model however previous attention model always refer source word predict target word contradict fact target word correspond source word motivate observation propose novel attention model capability determine decoder attend source word experimental result nist chinese english translation task show new model achieve improvement eight bleu score state art baseline
distributional word representation methods exploit word co occurrences build compact vector encode word representations enjoy widespread use modern natural language process unclear whether accurately encode necessary facets conceptual mean paper evaluate well representations predict perceptual conceptual feature concrete concepts draw two semantic norm datasets source human participants find several standard word representations fail encode many salient perceptual feature concepts show deficits correlate word word similarity prediction errors analyse provide motivation ground embody language learn approach may help remedy deficits
different theories posit different source feel well happiness appraisal theory ground emotional responses goals desire fulfillment lack fulfillment self determination theory posit basis well rest assessment competence autonomy social connection survey measure happiness empirically note people require basic need meet food shelter beyond tend happiest socialize eat sex analyze corpus private microblogs well application call echo users label write post daily events happiness score one nine goal grind linguistic descriptions events users experience theories well happiness examine extent different theoretical account explain variance happiness score show recurrent event type obligation incompetence affect people feel well capture current lexical semantic resources
functional distributional semantics framework aim learn text semantic representations interpret term truth make two contributions framework first show type logical inference perform evaluate conditional probabilities second make calculations tractable mean variational approximation approximation also enable faster convergence train allow us close gap state art vector space model evaluate semantic similarity demonstrate promise performance two task
semantic composition remain open problem vector space model semantics paper explain probabilistic graphical model use framework functional distributional semantics interpret probabilistic version model theory build explain various semantic phenomena recast term conditional probabilities graphical model connection formal semantics machine learn helpful directions give us explicit mechanism model context dependent mean challenge formal semantics also give us well motivate techniques compose distribute representations challenge distributional semantics present result two datasets go beyond word similarity show semantically motivate techniques improve performance vector model
present neural transition base parser spinal tree dependency representation constituent tree parser use stack lstms compose constituent nod dependency base derivations experiment show model adapt different style dependency relations choice little effect predict constituent structure suggest lstms induce useful state
word segmentation play pivotal role improve arabic nlp application therefore lot research spend improve accuracy shelf tool however complicate use ii domain dialect dependent explore three language independent alternatives morphological segmentation use data drive sub word units ii character unit learn iii word embeddings learn use character cnn convolution neural network task machine translation pos tag find methods achieve close occasionally surpass state art performance analysis show neural machine translation system sensitive ratio source target tokens ratio close one greater give optimal performance
learn algorithms natural language process nlp task traditionally rely manually define relevant contextual feature hand neural network model use distributional representation word successfully apply several nlp task model learn feature automatically avoid explicit feature engineer across several domains neural model become natural choice specifically limit characteristics data know however flexibility come cost interpretability paper define three different methods investigate ability bi directional recurrent neural network rnns capture contextual feature particular analyze rnns sequence tag task perform comprehensive analysis general well biomedical domain datasets experiment focus important contextual word feature easily extend analyze various feature type also investigate positional effect context word show develop methods use error analysis
main aim paper investigate automatic quality assessment speak language translation slt precisely investigate slt errors due transcription asr translation mt modules paper investigate automatic detection slt errors use single classifier base joint asr mt feature evaluate two class good bad three class good badasr badmt label task three class problem necessitate disentangle asr mt errors speech translation output propose two label extraction methods non trivial step enable product qualitative analysis slt errors origin due transcription translation step large house corpus french english speech translation
various text analysis techniques exist attempt uncover unstructured information text work explore use statistical dependence measure textual classification represent text word vectors student satisfaction score three point scale free text comment write university subject use dataset compare two textual representations frequency word representation term frequency relationship word vectors find word vectors provide greater accuracy however word vectors large number feature aggravate burden computational complexity thus explore use non linear dependency measure feature selection maximize dependence text review correspond score quantitative qualitative analysis student satisfaction dataset show approach achieve comparable accuracy full feature vector order magnitude faster test text analysis feature reduction techniques use textual data applications sentiment analysis
introduce cross match test exact distribution free high dimensional hypothesis test intrinsic evaluation metric word embeddings show cross match effective mean measure distributional similarity different vector representations evaluate statistical significance different vector embed model additionally find cross match use provide quantitative measure linguistic similarity select bridge languages machine translation demonstrate result hypothesis test align expectations note framework two sample hypothesis test limit word embeddings extend vector representations
language use online forums differ many ways traditional language resources news one difference use frequency nonliteral subjective dialogue act sarcasm whether aim develop theory sarcasm dialogue engineer automatic methods reliably detect sarcasm major challenge simply difficulty get enough reliably label examples paper describe work methods achieve highly reliable sarcasm annotations untrained annotators mechanical turk explore use number common statistical reliability measure kappa karger majority class show sophisticate measure appear yield better result data simple measure assume correct label one majority turkers apply
propose query base generative model solve task question generation qg question swering qa model follow classic encoder decoder framework encoder take passage query input perform query understand match query passage multiple per spectives decoder attention base long short term memory lstm model copy coverage mechanisms qg task question generate system give passage target answer whereas qa task answer generate give question passage train stage leverage policy gradient reinforcement learn algorithm overcome exposure bias major prob lem result sequence learn cross entropy loss qg task experiment show higher per formances state art result use additional train data automatically generate question even improve performance strong ex tractive qa system addition model show bet ter performance state art baselines generative qa task
recent work problem latent tree learn make possible train neural network learn parse sentence use result parse interpret sentence without exposure grind truth parse tree train time surprisingly model often perform better sentence understand task model use parse tree conventional parsers paper aim investigate latent tree learn model learn replicate two model share codebase find one model outperform conventional tree structure model sentence classification ii parse strategies especially consistent across random restart iii parse produce tend shallower standard penn treebank ptb parse iv resemble ptb semantic syntactic formalism author aware
measure salience word essential step numerous nlp task heuristic approach tfidf use far estimate salience word propose emphneural word salience nws score unlike heuristics learn corpus specifically learn word salience score use pre train word embeddings input accurately predict word appear sentence give word appear sentence precede succeed sentence experimental result sentence similarity prediction show learn word salience score perform comparably better state art approach represent sentence benchmark datasets sentence similarity use fraction train prediction time require prior methods moreover nws score positively correlate psycholinguistic measure concreteness imageability imply close connection salience perceive humans
satirical news consider entertainment potentially deceptive harmful despite embed genre article everyone recognize satirical cue therefore believe news true news observe satirical cue often reflect certain paragraph rather whole document exist work consider document level feature detect satire could limit consider paragraph level linguistic feature unveil satire incorporate neural network attention mechanism investigate difference paragraph level feature document level feature analyze large satirical news dataset evaluation show propose model detect satirical news effectively reveal feature important level
identify relations exist word entities important various natural language process task relational search noun modifier classification analogy detection popular approach represent relations pair word extract pattern word co occur corpus assign word pair vector pattern frequencies despite simplicity approach suffer data sparseness information scalability linguistic creativity model unable handle previously unseen word pair corpus contrast compositional approach represent relations word overcome issue use attribute individual word indirectly compose representation common relations hold two word study aim compare different operations create relation representations word level representations investigate performance compositional methods measure relational similarities use several benchmark datasets word analogy moreover evaluate different relation representations knowledge base completion task
co occurrences two word provide useful insights semantics word consequently numerous prior work word embed learn use co occurrences two word train signal learn word embeddings however natural language texts common multiple word relate co occur context extend notion co occurrences cover kgeq2 way co occurrences among set k word specifically prove theoretical relationship joint probability kgeq2 word sum ell2 norms embeddings next propose learn objective motivate theoretical result utilise k way co occurrences learn word embeddings experimental result show derive theoretical relationship indeed hold empirically despite data sparsity smaller k value k way embeddings perform comparably better two way embeddings range task
many statistical learn problems area natural language process include sequence tag sequence segmentation syntactic parse successfully approach mean structure prediction methods appeal property correspond discriminative learn algorithms ability integrate loss function interest directly optimization process potentially increase result performance accuracy demonstrate example constituency parse optimize f1 score max margin framework structural svm particular optimization respect original binarized tree
late medieval voynich manuscript vm resist decryption consider meaningless hoax unsolvable cipher provide evidence vm write natural language establish relation voynich alphabet iranian pahlavi script many voynich character upside versions pahlavi counterparts may effect different write directions voynich letter explain ligatures departures pahlavi intent cope know problems due stupendous ambiguity pahlavi text translation vm text attempt confirm voynich pahlavi relation character level transcription many word vm illustrations part main text many transcribe word identify term zoroastrian cosmology line use pahlavi script zoroastrian communities medieval time
study address problem identify mean unknown word entities discourse respect word embed approach use neural language model propose method fly construction exploitation word embeddings input output layer neural model track contexts extend dynamic entity representation use kobayashi et al two thousand and sixteen incorporate copy mechanism propose independently gu et al two thousand and sixteen gulcehre et al two thousand and sixteen addition construct new task dataset call anonymized language model evaluate ability capture word mean read experiment conduct use novel dataset show propose variant rnn language model outperform baseline model furthermore experiment also demonstrate dynamic update output layer help model predict reappear entities whereas input layer effective predict word follow reappear entities
even though sequence sequence neural machine translation nmt model achieve state art performance recent fewer years widely concern recurrent neural network rnn units hard capture long distance state information mean rnn hardly find feature long term dependency sequence become longer similarly convolutional neural network cnn introduce nmt speed recently however cnn focus capture local feature sequence relieve issue incorporate relation network standard encoder decoder framework enhance information propogation neural network ensure information source sentence flow decoder adequately experiment show propose framework outperform statistical mt model state art nmt model significantly two data set different scale
users suffer mental health condition often turn online resources support include specialize online support communities general communities twitter reddit work present neural framework support study users type communities propose methods identify post support communities may indicate risk self harm demonstrate approach outperform strong previously propose methods identify post self harm closely relate depression make identify depress users general forums crucial relate task introduce large scale general forum dataset rsdd consist users self report depression diagnose match control users show method apply effectively identify depress users use language alone demonstrate method outperform strong baselines general forum dataset
stance classification aim identify particular issue discussion whether speaker author conversational turn pro favor con stance issue detect stance tweet new task propose semeval two thousand and sixteen task6 involve predict stance dataset tweet topics abortion atheism climate change feminism hillary clinton give small size dataset team create topic specific train corpus develop set high precision hashtags topic use query twitter api aim develop large train corpus without additional human label tweet stance hashtags select topic predict stance bear experimental result demonstrate good performance feature opinion target pair base generalize dependency feature use sentiment lexicons
sarcasm occur due presence numerical portion text quote error make automatic sarcasm detection approach past present first study detect sarcasm number case sentence love wake four analyze challenge problem present rule base machine learn deep learn approach detect sarcasm numerical portion text deep learn approach outperform four past work sarcasm detection rule base machine learn approach dataset tweet obtain f1 score ninety-three show special attention text contain number may useful improve state art sarcasm detection
work present paper focus translation terminological expressions represent semantically structure resources like ontologies knowledge graph challenge translate ontology label terminological expressions document knowledge base lie highly specific vocabulary lack contextual information guide machine translation system translate ambiguous word target domain due challenge evaluate translation quality domain specific expressions medical financial domain statistical well neural machine translation methods experiment domain adaptation translation model terminological expressions furthermore perform experiment injection external terminological expressions translation systems experiment observe significant advantage domain adaptation domain specific resource medical financial domain benefit subword model word base neural machine translation model terminology translation
explore techniques maximize effectiveness discourse information task authorship attribution present novel method embed discourse feature convolutional neural network text classifier achieve state art result substantial margin empirically investigate several featurization methods understand condition discourse feature contribute non trivial performance gain analyze discourse embeddings
moore lewis method intelligent selection language model train data effective cheap efficient also structural problems one method define relevance play language model train domain domain data pool corpora powerful idea set preserve treat two corpora oppose end single spectrum lack nuance allow two corpora similar extreme case come distribution sentence moore lewis score zero result rank two select sentence guarantee able model domain data even cover domain data simply well like domain model necessary sufficient three way tell optimal number sentence select short pick various thresholds build systems present greedy lazy approximate generally efficient information theoretic method accomplish goal use vocabulary count method follow properties one responsive extent two corpora differ two quickly reach near optimal vocabulary coverage three take account already select four involve define kind domain kind classifier six know approximately stop method use inherently meaningful measure similarity measure bits information gain add one text another
frequent object study linguistic typology order elements demonstrative adjective numeral noun noun phrase goal predict relative frequencies order across languages use poisson regression statistically compare prominent account variation compare feature systems derive cinque two thousand and five feature systems give cysouw two thousand and ten dryer prep set find clear reason prefer model cinque two thousand and five dryer prep find model substantially better fit typological data model cysouw two thousand and ten
rapid progress make towards question answer qa systems extract answer text exist neural approach make use expensive bi directional attention mechanisms score possible answer span limit scalability propose instead cast extractive qa iterative search problem select answer sentence start word end word representation reduce space search step allow computation conditionally allocate promise search paths show globally normalize decision process back propagate beam search make representation viable learn efficient empirically demonstrate benefit approach use model globally normalize reader gnr achieve second highest single model performance stanford question answer dataset six hundred and eighty-four seven thousand, six hundred and twenty-one f1 dev 247x faster bi attention flow also introduce data augmentation method produce semantically valid examples align name entities knowledge base swap new entities type method improve performance model consider work independent interest variety nlp task
great need technologies predict mortality patients intensive care units high accuracy accountability present joint end end neural network architectures combine long short term memory lstm latent topic model simultaneously train classifier mortality prediction learn latent topics indicative mortality textual clinical note topic interpretability topic model layer carefully design single layer network constraints inspire lda experiment mimic iii dataset show model significantly outperform prior model base lda topics mortality prediction however achieve limit success method interpret topics train model look neural network weight
paper describe system deploy clac edlk team semeval two thousand and sixteen complex word identification task goal task identify give word give context simple complex system rely linguistic feature cognitive complexity use several supervise model however random forest model outperform others overall best configuration achieve g score six hundred and eighty-eight task rank system twenty-one forty-five
many modern day systems information extraction knowledge management agents ontologies play vital role maintain concept hierarchies select domain however ontology population become problematic process due nature heavy couple manual human intervention use word embeddings field natural language process become popular topic due ability cope semantic sensitivity hence study propose novel way semi supervise ontology population word embeddings basis build several model include traditional benchmark model new type model base word embeddings finally ensemble together come synergistic model better accuracy demonstrate ensemble model outperform individual model
propose simple flexible train decode methods influence output style topic neural encoder decoder base language generation capability desirable variety applications include conversational systems successful agents need produce language specific style generate responses steer human puppeteer external knowledge decompose neural generation process empirically easier sub problems faithfulness model decode method base selective sample also describe train sample algorithms bias generation process specific language style restriction topic restriction human evaluation result show propose methods able restrict style topic without degrade output quality conversational task
paper present apptechminer rule base information extraction framework automatically construct knowledge base application areas problem solve techniques techniques include tool methods datasets evaluation metrics also categorize individual research article base application areas techniques propose improve article system achieve high average precision eighty-two recall eighty-four knowledge base creation also perform well application technique assignment individual article average accuracy sixty-six end present two use case present trivial information retrieval system extensive temporal analysis usage techniques application areas present demonstrate framework domain computational linguistics easily generalize field research
chatbots rapidly expand application dialogue systems company switch bot service customer support new applications users interest casual conversation one style casual conversation argument many people love nothing good argument moreover number exist corpora argumentative dialogues annotate agreement disagreement stance sarcasm argument quality paper introduce debbie novel argue bot select arguments conversational corpora aim use appropriately context present initial work prototype debbie preliminary evaluation describe future work
order build dialogue systems tackle ambitious task hold social conversations argue need data drive approach include insight human conversational chit chat incorporate different natural language process modules strategy analyze index large corpora social media data include twitter conversations online debate dialogues friends blog post couple data retrieval modules perform task sentiment style analysis topic model summarization aim personal assistants learn nuanced human language grow task orient agents personable social bots
knowner multilingual name entity recognition ner system leverage different degrees external knowledge novel modular framework divide knowledge four categories accord depth knowledge convey category consist set feature automatically generate different information source knowledge base list name document specific semantic annotations use train conditional random field crf since information source usually multilingual knowner easily train wide range languages paper show incorporation deeper knowledge systematically boost accuracy compare knowner state art ner approach across three languages ie english german spanish perform amongst state art systems
despite successful applications across broad range nlp task conditional random field crfs particular linear chain variant able model local feature important benefit term inference tractability limit ability model capture long range dependencies items attempt extend crfs capture long range dependencies largely come cost computational complexity approximate inference work propose extension crfs integrate external memory take inspiration memory network thereby allow crfs incorporate information far beyond neighbour step experiment across two task show substantial improvements strong crf lstm baselines
mainly sake solve lack keyword specific data propose one keyword spot kws system use deep neural network dnn connectionist temporal classifier ctc power constrain small footprint mobile devices take full advantage general corpus continuous speech recognition great amount dnn directly predict posterior phoneme units personally customize key phrase ctc produce confidence score give phoneme sequence responsive decision make mechanism ctc kws competitive performance comparison purely dnn base keyword specific kws increase computational complexity
paper present segmentation system develop mlp two thousand and seventeen share task cross lingual word segmentation morpheme segmentation model word morpheme segmentation character level sequence label task prevalent bidirectional recurrent neural network conditional random field output interface adapt baseline system improve via ensemble decode universal system apply extensively evaluate official data set without language specific adjustment official evaluation result indicate propose model achieve outstanding accuracies word morpheme segmentation languages various type compare participate systems
flanders tv show subtitle however process subtitle time consume one speed provide output speech recognizer run audio tv show prior subtitle naturally speech recognition perform much better employ language model adapt register topic program present several language model train subtitle television show provide flemish public service broadcaster vrt data gather context project ston purpose facilitate process subtitle tv show one model train available data 46m word tokens also train model specific type tv show domain topic language model speak language quite rare due lack train data size corpus relatively large corpus speak language compare eg cgn 9m word still rather small language model thus practice advise interpolate model large background language model train write language model freely download http wwwesatkuleuvenbe psi spraak download
paper describe systran systems submit wmt two thousand and seventeen share news translation task english german translation directions systems build use opennmt open source neural machine translation system implement sequence sequence model lstm encoder decoders attention experiment use monolingual data automatically back translate result model hyper specialise adaptation technique finely tune model accord evaluation test sentence
introduce open source toolkit neural machine translation nmt support research model architectures feature representations source modalities maintain competitive performance modularity reasonable train requirements
present starspace general purpose neural embed model solve wide variety problems label task text classification rank task information retrieval web search collaborative filter base content base recommendation embed multi relational graph learn word sentence document level embeddings case model work embed entities comprise discrete feature compare learn similarities dependent task empirical result number task show starspace highly competitive exist methods whilst also generally applicable new case methods
paper show want obtain human evidence conventionalization phrase ask native speakers associations give phrase component word show component word phrase frequent associations phrase consider conventionalize another type conventionalize phrase reveal use two factor low entropy phrase associations low intersection component word phrase associations association experiment perform russian language
present hash embeddings efficient method represent word continuous vector form hash embed may see interpolation standard word embed word embed create use random hash function hash trick hash embeddings token represent k dimensional embeddings vectors one k dimensional weight vector final dimensional representation token product two rather fit embed vectors token select hash trick share pool b embed vectors experiment show hash embeddings easily deal huge vocabularies consist millions tokens use hash embed need create dictionary train perform kind vocabulary prune train show model train use hash embeddings exhibit least level performance model train use regular embeddings across wide range task furthermore number parameters need embed fraction require regular embed since standard embeddings embeddings construct use hash trick actually special case hash embed hash embeddings consider extension improvement exist regular embed type
paper study problem addressee response selection multi party conversations understand multi party conversations challenge complex speaker interactions multiple speakers exchange message play different roles sender addressee observer roles vary across turn tackle challenge propose speaker interaction recurrent neural network si rnn whereas previous state art system update speaker embeddings sender si rnn use novel dialog encoder update speaker embeddings role sensitive way additionally unlike previous work select addressee response separately si rnn select jointly view task sequence prediction problem experimental result show si rnn significantly improve accuracy addressee response selection particularly complex conversations many speakers responses distant message many turn past
dialogue act recognition associate dialogue act ie semantic label utterances conversation problem associate semantic label utterances treat sequence label problem work build hierarchical recurrent neural network use bidirectional lstm base unit conditional random field crf top layer classify utterance correspond dialogue act hierarchical network learn representations multiple level ie word level utterance level conversation level conversation level representations input crf layer take account previous utterances also dialogue act thus model dependency among label utterances important consideration natural dialogue validate approach two different benchmark data set switchboard meet recorder dialogue act show performance improvement state art methods twenty-two forty-one absolute point respectively worth note inter annotator agreement switchboard data set eighty-four method able achieve accuracy seventy-nine despite train noisy data
knowledge ground conversation domain knowledge play important role special domain music response knowledge ground conversation might contain multiple answer entities entity although exist generative question answer qa systems apply knowledge ground conversation either one entity response deal vocabulary entities propose fully data drive generative dialogue system gends capable generate responses base input message relate knowledge base kb generate arbitrary number answer entities even entities never appear train set design dynamic knowledge enquirer select different answer entities different position single response accord different local context rely representations entities enable model deal vocabulary entities collect human human conversation data conversmusic knowledge annotations propose method evaluate coversmusic public question answer dataset propose gends system outperform baseline methods significantly term bleu entity accuracy entity recall human evaluation moreoverthe experiment also demonstrate gends work better even small datasets
natural language inference nli task require agent determine logical relationship natural language premise natural language hypothesis introduce interactive inference network iin novel class neural network architectures able achieve high level understand sentence pair hierarchically extract semantic feature interaction space show interaction tensor attention weight contain semantic information solve natural language inference denser interaction tensor contain richer semantic information one instance architecture densely interactive inference network diin demonstrate state art performance large scale nli copora large scale nli alike corpus noteworthy diin achieve greater twenty error reduction challenge multi genre nli multinli dataset respect strongest publish system
paper describe use text classification methods investigate genre method variation english german translation corpus purpose use linguistically motivate feature represent texts use combination part speech tag arrange bigrams trigrams four grams classification method use paper bayesian classifier laplace smooth use output classifiers carry extensive feature analysis main difference genres methods translation
contrast goal orient dialogue social dialogue clear measure task success consequently evaluation systems notoriously hard paper review current evaluation methods focus automatic metrics conclude turn base metrics often ignore context account fact several reply valid end dialogue reward mainly hand craft lack ground human perceptions
paper fill gap aspect base sentiment analysis aim present new method prepare analyse texts concern opinion generate user friendly descriptive report natural language present comprehensive set techniques derive rhetorical structure theory sentiment analysis extract aspects textual opinions build abstractive summary set opinions moreover propose aspect aspect graph evaluate importance aspects filter unimportant ones summary additionally paper present prototype solution data flow interest valuable result propose method result prove high accuracy aspect detection apply gold standard dataset
communication tool make world like small village consequence people contact others different societies speak different languages communication happen effectively without machine translation find anytime everywhere number study develop machine translation english language many languages except arabic consider yet therefore aim highlight roadmap propose translation machine provide enhance arabic english translation base semantic
result rapid change information communication technology ict world become small village people world connect dialogue communication via internet also communications become daily routine activity due new globalization company even universities become global reside cross countries border result translation become need activity connect world ict make possible student one country take course even degree different country anytime anywhere easily result communication still need language mean help receiver understand content send message people need automate translation application human translators hard find time human translations expensive compare translations automate process several type research describe electronic process machine translation paper author go study previous research explore need tool machine translation research go contribute machine translation area help future researchers summary machine translation group research let light importance translation mechanism
present system cap two thousand and seventeen ner challenge name entity recognition french tweet system leverage unsupervised learn larger dataset french tweet learn feature feed crf model rank first without use gazetteer structure external data f measure five thousand, eight hundred and eighty-nine best knowledge first system use fasttext embeddings include subword representations embed base sentence representation ner
neural sequence sequence network attention achieve remarkable performance machine translation one reason effectiveness ability capture relevant source side contextual information time step prediction attention mechanism however target side context solely base sequence model practice prone recency bias lack ability capture effectively non sequential dependencies among word address limitation propose target side attentive residual recurrent network decode attention previous word contribute directly prediction next word residual learn facilitate flow information distant past able emphasize previously translate word hence gain access wider context propose model outperform neural mt baseline well memory self attention network three language pair analysis attention learn decoder confirm emphasize wider context capture syntactic like structure
social media platforms largely base text users often write post describe see feel write text lack emotional cue speak face face dialogue ambiguities common write language problem exacerbate short informal nature many social media post bypass issue suite special character call emojis small pictograms embed within text many emojis small depictions facial expressions design help disambiguate emotional mean text however new ambiguity arise way emojis render every platform windows mac android name render emojis accord style fact show emojis render differently look happy platforms sad others work use real world data verify existence problem verify usage emoji significantly different across platforms emojis exhibit different sentiment polarities different platforms propose solution identify intend emoji base platform specific nature emoji use author social media post apply solution sentiment analysis task benefit emoji calibration technique use work conduct experiment evaluate effectiveness map task
paraphrase generation important problem nlp especially question answer information retrieval information extraction conversation systems name paper address problem generate paraphrase automatically propose method base combination deep generative model vae sequence sequence model lstm generate paraphrase give input sentence traditional vaes combine recurrent neural network generate free text suitable paraphrase generation give sentence address problem condition encoder decoder side vae original sentence generate give sentence paraphrase unlike exist model model simple modular generate multiple paraphrase give sentence quantitative evaluation propose method benchmark paraphrase dataset demonstrate efficacy performance improvement state art methods significant margin whereas qualitative human evaluation indicate generate paraphrase well form grammatically correct relevant input sentence furthermore evaluate method newly release question paraphrase dataset establish new baseline future research
aspect term extraction eat identify opinionated aspect term texts one task semeval aspect base sentiment analysis absa contest small amount available datasets supervise eat costly human annotation aspect term label give rise need unsupervised eat paper introduce architecture achieve top rank performance supervise eat moreover use efficiently feature extractor classifier unsupervised eat second contribution method automatically construct datasets eat train classifier automatically label datasets evaluate human annotate semeval absa test set compare strong rule base baseline obtain dramatically higher f score attain precision value eighty unsupervised method beat supervise absa baseline semeval preserve high precision score
investigate problem manually correct errors automatic speech transcript cost sensitive fashion do specify fix time budget automatically choose location size segment correction number correct errors maximize core components suggest previous research one utility model estimate number errors particular segment cost model estimate annotation effort segment work propose dynamic update framework allow train cost model ongoing transcription process remove need transcriber enrollment prior actual transcription improve correction efficiency allow highly transcriber adaptive cost model first confirm analyze improvements afford method simulate study conduct realistic user study observe efficiency improvements fifteen relative average forty-two participants deviate strongly initial transcriber agnostic cost model moreover find update framework capture dynamically change factor transcriber fatigue topic familiarity observe large influence transcriber work behavior
investigate characteristics factual emotional argumentation style observe online debate use annotate set factual feel debate forum post extract pattern highly correlate factual emotional arguments apply bootstrapping methodology find new pattern larger pool unannotated forum post process automatically produce large set pattern represent linguistic expressions highly correlate factual emotional language finally analyze discriminate pattern better understand define characteristics factual emotional arguments
effective model social dialog must understand broad range rhetorical figurative devices rhetorical question rqs type figurative language whose aim achieve pragmatic goal structure argument persuasive emphasize point ironic computational model form figurative language rhetorical question receive little attention date expand small dataset previous work present corpus ten thousand, two hundred and seventy rqs debate forums twitter represent different discourse function show clearly distinguish rqs sincere question seventy-six f1 show rqs use sarcastically non sarcastically observe non sarcastic use rqs frequently argumentative forums persuasive tweet present experiment distinguish use rqs use svm lstm model represent linguistic feature post level context achieve result high seventy-six f1 sarcastic seventy-seven f1 forums eighty-three f1 sarcastic tweet supplement quantitative experiment depth characterization linguistic variation rqs
many creative figurative elements make language excite lose translation current natural language generation engines paper explore method harvest templates positive negative review restaurant domain goal vastly expand type stylistic variation available natural language generator learn hyperbolic adjective pattern representative strongly valenced expressive language commonly use either positive negative review identify delexicalize entities use heuristics extract generation templates review sentence evaluate learn templates traditional review templates use subjective measure convincingness interestingness naturalness result show learn templates score highly measure finally analyze linguistic categories characterize learn positive negative templates plan use learn templates improve conversational style dialogue systems restaurant domain
use irony sarcasm social media allow us study scale first time however diversity make difficult construct high quality corpus sarcasm dialogue describe process create large scale highly diverse corpus online debate forums dialogue novel methods operationalizing class sarcasm form rhetorical question hyperbole show use lexico syntactic cue reliably retrieve sarcastic utterances high accuracy demonstrate properties quality corpus conduct supervise learn experiment simple feature show achieve higher precision f previous work sarcasm debate forums dialogue apply weakly supervise linguistic pattern learner qualitatively analyze linguistic differences class
greatest challenge build sophisticate open domain conversational agents arise directly potential ongoing mix initiative multi turn dialogues follow particular plan pursue particular fix information need order make coherent conversational contributions context conversational agent must able track type attribute entities discussion conversation know relate case agent rely structure information source help identify relevant semantic relations produce turn case content available come search may unclear semantic relations hold search result discourse context constraint system must produce contribution ongoing conversation real time paper describe experience build slugbot two thousand and seventeen alexa prize discuss leverage search structure data different source help slugbot produce dialogic turn carry conversations whose length semi finals user evaluation period average eight hundred and seventeen minutes
give increase popularity customer service dialogue twitter analysis conversation data essential understand trend customer agent behavior purpose automate customer service interactions work develop novel taxonomy fine grain dialogue act frequently observe customer service showcasing act suit domain generic exist taxonomies use sequential svm hmm model model conversation flow predict dialogue act give turn real time characterize differences customer agent behavior twitter customer service conversations investigate effect test system different customer service industries finally use data drive approach predict important conversation outcomes customer satisfaction customer frustration overall problem resolution show type location certain dialogue act conversation significant effect probability desirable undesirable outcomes present actionable rule base find pattern rule derive use guidelines outcome drive automate customer service platforms
connectionist temporal classification ctc powerful approach sequence sequence learn popularly use speech recognition central ideas ctc include add label blank train mechanism ctc eliminate need segment alignment hence apply various sequence sequence learn problems work apply ctc abstractive summarization speak content blank case imply correspond input data less important noisy thus ignore approach show outperform exist methods term rouge score chinese gigaword matbn corpora approach also nice property order word character input document better preserve generate summaries
phrase base statistical model commonly use perform optimally term translation quality complexity system hindi general indian languages morphologically richer english hence even though phrase base systems perform well less divergent language pair english indian language translation need linguistic information morphology parse tree part speech tag etc source side factor model seem useful case factor model consider word vector factor factor contain information surface word use translate hence objective work handle morphological inflections hindi marathi use factor translation model translate english smt approach face problem data sparsity translate morphologically rich language unlikely parallel corpus contain morphological form word propose solution generate unseen morphological form inject original train corpora paper study factor model problem sparseness context translation morphologically rich languages propose simple effective solution base enrich input various morphological form word observe morphology injection improve quality translation term adequacy fluency verify experiment two morphologically rich languages hindi marathi translate english
open source mandarin speech corpus call aishell one release far largest corpus suitable conduct speech recognition research build speech recognition systems mandarin record procedure include audio capture devices environments present detail preparation relate resources include transcriptions lexicon describe corpus release kaldi recipe experimental result imply quality audio record transcriptions promise
availability collection access quantitative data well limitations often make qualitative data resource upon development program heavily rely traditional interview data social media analysis provide rich contextual information essential research appraisal monitor evaluation data may difficult process analyze systematically scale turn limit ability timely data drive decision make essential fast evolve complex social systems paper discuss potential use natural language process systematize analysis qualitative data inform quick decision make development context illustrate interview data generate format micro narratives undp fragment impact project
paper focus problem answer trigger ad dress yang et al two thousand and fifteen critical component real world question answer system employ hierarchical gate recurrent neural tensor hgrnt model capture context information deep teractions candidate answer question result f val ue achieve four hundred and twenty-six surpass baseline ten
word discovery task extract word unsegmented text paper examine extent neural network apply task realistic unwritten language scenario small corpora limit annotations available investigate two scenarios one supervision another limit supervision access frequent word obtain result show possible retrieve least twenty-seven gold standard vocabulary train encoder decoder neural machine translation system five thousand, one hundred and fifty-seven sentence result close obtain task specific bayesian nonparametric model moreover approach advantage generate translation alignments could use create bilingual lexicon future perspective approach also well suit work directly speech
remarkable progress make neural machine translation nmt research many report development evaluation practice paper try fill gap present find build house travel domain nmt system large scale e commerce set three major topics cover optimization train include different optimization strategies corpus size handle real world content evaluate result
cross lingual representation learn important step make nlp scale world languages recent work bilingual lexicon induction suggest possible learn cross lingual representations word base similarities image associate word however work focus translation select nouns work investigate whether mean part speech particular adjectives verbs learn way also experiment combine representations learn visual data embeddings learn textual data experiment across five language pair indicate previous work scale problem learn cross lingual representations beyond simple nouns
paper present approach task predict event description precede sentence text approach explore sequence sequence learn use bidirectional multi layer recurrent neural network approach substantially outperform previous work term bleu score two datasets derive wikihow descript respectively since bleu score easy interpret measure event prediction complement study second evaluation exploit rich linguistic annotation gold paraphrase set events
paper present deep reinforcement learn rl framework iterative dialog policy optimization end end task orient dialog systems popular approach learn dialog policy rl include let dialog agent learn user simulator build reliable user simulator however trivial often difficult build good dialog agent address challenge jointly optimize dialog agent user simulator deep rl simulate dialogs two agents first bootstrap basic dialog agent basic user simulator learn directly dialog corpora supervise train improve let two agents conduct task orient dialogs iteratively optimize policies deep rl dialog agent user simulator design neural network model train end end experiment result show propose method lead promise improvements task success rate total task reward compare supervise train single agent rl train baseline model
verbal metonymy receive relatively scarce attention field computational linguistics despite fact model accurately paraphrase metonymy applications academia technology sector method describe paper make use data british national corpus order create word vectors find instance verbal metonymy generate potential paraphrase two different ways create word vectors evaluate study continuous bag word skip grams skip grams find outperform continuous bag word approach furthermore skip gram model find operate better chance accuracy strong positive relationship phi coefficient sixty-one model classification human judgement rank paraphrase study lend credence viability model verbal metonymy computational methods base distributional semantics
past several years witness rapid progress end end neural machine translation nmt however exist discrepancy train inference nmt decode may lead serious problems since model might part state space never see train address issue schedule sample propose however certain limitations schedule sample propose two dynamic oracle base methods improve manage mitigate discrepancy change train process towards less guide scheme meanwhile aggregate oracle demonstrations experimental result show propose approach improve translation quality standard nmt system
propose novel approach vietnamese word segmentation approach base single classification ripple rule methodology compton jansen one thousand, nine hundred and ninety rule store exception structure new rule add correct segmentation errors give exist rule experimental result benchmark vietnamese treebank show approach outperform previous state art approach jvnsegmenter vntokenizer dongdu uetsegmenter term accuracy performance speed code open source available https githubcom datquocnguyen rdrsegmenter
sentiment analysis regard relation extraction problem sentiment opinion holder towards certain aspect product theme event need extract present novel neural architecture sentiment analysis relation extraction problem address problem divide three subtasks identification aspect opinion term ii label opinion term sentiment iii extraction relations opinion term aspect term subtask propose neural network base component combine complete system relational sentiment analysis component aspect opinion term extraction hybrid architecture consist recurrent neural network stack top convolutional neural network approach outperform standard convolutional deep neural architecture well recurrent network architecture perform competitively compare methods two datasets annotate customer review extract sentiments individual opinion term propose recurrent architecture combination word distance feature achieve promise result outperform majority baseline eighteen accuracy provide first result usage dataset relation extraction component outperform current state art aspect opinion relation extraction fifteen f measure
world wide web hold wealth information form unstructured texts customer review products events extract analyze express opinions customer review fine grain way valuable opportunities insights customers businesses gain propose neural network base system address task aspect base sentiment analysis compete task two eswc two thousand and sixteen challenge semantic sentiment analysis propose architecture divide task two subtasks aspect term extraction aspect specific sentiment extraction approach flexible allow address subtask independently first step recurrent neural network use extract aspects text frame problem sequence label task second step recurrent network process extract aspect respect context predict sentiment label system use pretrained semantic word embed feature experimentally enhance semantic knowledge extract wordnet feature extract senticnet prove beneficial extraction sentiment label best perform system category propose system prove effective approach aspect base sentiment analysis
fine grain sentiment analysis receive increase attention recent years extract opinion target expressions ote review often important step fine grain aspect base sentiment analysis retrieve information user generate text however difficult customer review instance prone contain misspell word difficult process due domain specific language work investigate whether character level model improve performance identification opinion target expressions integrate information character structure word sequence label system use character level word embeddings show positive impact system performance specifically obtain increase thirty-three point f1 score respect baseline model experiment reveal encode character pattern learn embeddings give nuanced view performance differences model
language model lms base long short term memory lstm show good gain many automatic speech recognition task paper extend lstm add highway network inside lstm use result highway lstm hw lstm model language model add highway network increase depth time dimension since typical lstm two internal state memory cell hide state compare various type hw lstm add highway network onto memory cell hide state experimental result english broadcast news conversational telephone speech recognition show propose hw lstm lm improve speech recognition accuracy top strong lstm lm baseline report fifty-one ninety-nine switchboard callhome subsets hub5 two thousand evaluation reach best performance number report task date
paper describe english audio textual dataset debate speeches unique resource grow research field computational argumentation debate technologies detail process speech record professional debaters transcription speeches automatic speech recognition asr system consequent automatic process produce text nlp friendly parallel manual transcription speeches order produce gold standard reference transcripts release sixty speeches various controversial topics five format correspond different stag production data intention allow utilize resource multiple research purpose addition domain train data debate specific asr system apply argumentation mine either noisy clean debate transcripts intend make release data future
cegs n grid two thousand and sixteen share task one clinical natural language process focus de identification psychiatric evaluation record paper describe two participate systems team base conditional random field crfs long short term memory network lstms pre process module introduce sentence detection tokenization de identification crfs manually extract rich feature utilize train model lstms character level bi directional lstm network apply represent tokens classify tag token follow decode layer stack decode probable protect health information phi term lstm base system attain i2b2 strict micro f1 measure eight thousand, nine hundred and eighty-six higher crf base system
paper present empirical study two machine translation base approach vietnamese diacritic restoration problem include phrase base neural base machine translation model first work apply neural base machine translation method problem give thorough comparison phrase base machine translation method current state art method problem large dataset phrase base approach accuracy nine thousand, seven hundred and thirty-two neural base approach nine thousand, six hundred and fifteen neural base method slightly lower accuracy twice faster phrase base method term inference speed moreover neural base machine translation method much room future improvement incorporate pre train word embeddings collect train data
paper describe arabic mgb three challenge arabic speech recognition wild unlike last year arabic mgb two challenge recognition task base one thousand, two hundred hours broadcast tv news record aljazeera arabic tv program mgb three emphasise dialectal arabic use multi genre collection egyptian youtube videos seven genres use data collection comedy cook family kid fashion drama sport science tedx total sixteen hours videos split evenly across different genres divide adaptation development evaluation data set arabic mgb challenge comprise two task speech transcription evaluate mgb three test set along ten hour mgb two test set report progress mgb two evaluation b arabic dialect identification introduce year order distinguish four major arabic dialects egyptian levantine north african gulf well modern standard arabic two hours audio per dialect release development two hours use evaluation dialect identification lexical feature vector bottleneck feature share participants addition raw audio record overall thirteen team submit ten systems challenge outline approach adopt system summarise evaluation result
estimation semantic similarity relatedness biomedical concepts utility many informatics applications automate methods fall two categories methods base distributional statistics draw text corpora methods use structure exist knowledge resources methods former category disregard taxonomic structure latter fail consider semantically relevant empirical information paper present method retrofit distributional context vector representations biomedical concepts use structural information umls metathesaurus similarity vector representations link concepts augment evaluate umnsrs benchmark result demonstrate retrofit concept vector representations lead better correlation human raters similarity relatedness surpass best result report date also demonstrate clear improvement performance reference standard retrofit vector representations compare without retrofit
mine suggestion express sentence give text less investigate sentence classification task therefore lack hand label benchmark datasets work propose evaluate two approach distant supervision suggestion mine distant supervision obtain large silver standard dataset construct use text wikihow wikipedia approach use lstm base neural network architecture learn classification model suggestion mine vary method use silver standard dataset first approach directly train classifier use dataset second approach learn word embeddings dataset second approach also learn pos embeddings interestingly give best classification accuracy
word embed natural language process nlp technique automatically map word vocabulary vectors real number embed space widely use recent years boost performance vari ety nlp task name entity recognition syntac tic parse sentiment analysis classic word embed methods word2vec glove work well give large text corpus input texts sparse many specialize domains eg cybersecurity methods often fail produce high quality vectors pa per describe novel method train domain specificword embeddings sparse texts addition domain texts method also leverage diverse type domain knowledge domain vocabulary semantic relations specifi cally first propose general framework encode diverse type domain knowledge text annotations de velop novel word annotation embed wae algorithm incorporate diverse type text annotations word bed evaluate method two cybersecurity text corpora malware description corpus common vulnerability exposure cve corpus evaluation sults demonstrate effectiveness method learn domain specific word embeddings
study problem evaluate automatic speech recognition asr systems target dialectal speech input major challenge case orthography dialects typically standardize asr evaluation perspective mean clear gold standard expect output several possible output could consider correct accord different human annotators make standard word error rate wer inadequate evaluation metric situation typical machine translation mt thus borrow ideas mt evaluation metric namely terp extension translation error rate closely relate wer particular process compare hypothesis reference make use spell variants word phrase mine twitter unsupervised fashion experiment evaluate asr output egyptian arabic manual analysis show result werd ie wer dialects metric variant terp adequate wer evaluate dialectal asr
neural language model scale well vocabulary large noise contrastive estimation nce sample base method allow fast learn large vocabularies although nce show promise performance neural machine translation consider unsuccessful approach language model sufficient investigation hyperparameters nce base neural language model also miss paper show nce successful approach neural language model hyperparameters neural network tune appropriately introduce search converge learn rate schedule nce design heuristic specify use schedule impact important hyperparameters dropout rate weight initialisation range also demonstrate show appropriate tune nce base neural language model outperform state art single model methods popular benchmark
development informatization data store form text loss text generation transmission paper aim establish language model base large scale corpus complete restoration miss text paper introduce novel measurement find miss word way establish comprehensive candidate lexicon insert correct choice word paper also introduce effective optimization methods largely improve efficiency text restoration shorten time deal one thousand sentence thirty-six second keywords language model sentence correction word imputation parallel optimization
draft textbook chapter neural machine translation comprehensive treatment topic range introduction neural network computation graph description currently dominant attentional sequence sequence model recent refinements alternative architectures challenge write chapter textbook statistical machine translation use jhu fall two thousand and seventeen class machine translation
natural spontaneous dialogue proceed incrementally word word basis contain many sort disfluency mid utterance sentence hesitations interruptions self corrections train data machine learn approach dialogue process often either clean wholly synthetic order avoid phenomena question arise well systems train clean data generalise real spontaneous dialogue indeed whether trainable naturally occur dialogue data answer question create new corpus call babi systematically add natural spontaneous incremental dialogue phenomena restart self corrections facebook ai research babi dialogues dataset explore performance state art retrieval model memn2n natural dataset result show semantic accuracy memn2n model drop drastically although principle able learn process constructions babi need impractical amount train data finally go show incremental semantic parser dylan show one hundred semantic accuracy babi babi highlight generalisation properties linguistically inform dialogue model
investigate end end method automatically induce task base dialogue systems small amount unannotated dialogue data combine incremental semantic grammar dynamic syntax type theory record ds ttr reinforcement learn rl language generation dialogue management joint decision problem systems thus produce incremental dialogues process word word show previously essential support natural spontaneous dialogue hypothesise rich linguistic knowledge within grammar enable combinatorially large number dialogue variations process even train dialogues experiment show model process seventy-four facebook ai babi dataset even train thirteen data five dialogues addition process sixty-five babi corpus create systematically add incremental dialogue phenomena restart self corrections babi compare model state art retrieval model memn2n find term semantic accuracy memn2n show poor robustness babi transformations even train full babi dataset
apply sequence sequence model mitigate impact speech recognition errors open domain end end dialog generation cast task domain adaptation problem asr transcriptions original text two different domains paper propose model include two individual encoders domain data make hide state similar ensure decoder predict dialog text method show sequence sequence model learn asr transcriptions original text pair mean eliminate speech recognition errors experimental result cornell movie dialog dataset demonstrate domain adaption system help speak dialog system generate similar responses original text answer
study present long short term memory lstm neural network approach japanese word segmentation jws previous study chinese word segmentation cws succeed use recurrent neural network lstm gate recurrent units gru however contrast chinese japanese include several character type hiragana katakana kanji produce orthographic variations increase difficulty word segmentation additionally important jws task consider global context yet traditional jws approach rely local feature order address problem study propose employ lstm base approach jws experimental result indicate propose model achieve state art accuracy respect various japanese corpora
paper address automatic extraction abbreviations encompass acronyms initialisms correspond long form expansions plain unstructured text create go release multilingual resource abbreviations correspond expansions build automatically exploit wikipedia redirect disambiguation page use benchmark evaluation address shortcoming previous work redirect page use every abbreviation single expansion even though multiple different expansions possible many abbreviations also develop principled machine learn base approach score expansion candidates use different techniques indicators near synonymy topical relatedness surface similarity show improve performance seven languages include two non latin alphabet relative strong baselines
preprint article identify phrasemes via interlingual association measure present february two thousand and sixteen leko lexical combinations typify speech multilingual context conference innsbruck
machine read comprehension mrc become enormously popular recently attract lot attention however exist read comprehension datasets mostly english add diversity read comprehension datasets paper propose new chinese read comprehension dataset accelerate relate research community propose dataset contain two different type cloze style read comprehension user query read comprehension associate large scale train data well human annotate validation hide test set along dataset also host first evaluation chinese machine read comprehension cmrc two thousand and seventeen successfully attract tens participants suggest potential impact dataset
one main difficulties sentiment analysis arabic language presence colloquialism paper examine effect use objective word conjunction sentimental word sentiment classification colloquial arabic review specifically jordanian colloquial review review often include sentimental objective word however exist sentiment analysis model ignore objective word consider useless work create two lexicons first include colloquial sentimental word compound phrase contain objective word associate value sentiment tendency base particular estimation method use lexicons extract sentiment feature would train input support vector machine svm classify sentiment polarity review review dataset collect manually jeeran website result experiment show propose approach improve polarity classification comparison two baseline model accuracy nine hundred and fifty-six
many people use yelp find good restaurant nonetheless overall rat restaurant yelp offer enough information independently judge various aspects environment service flavor paper introduce machine learn base method characterize aspects particular type restaurants main approach use paper use support vector machine svm model decipher sentiment tendency review word frequency word score generate svm model process polarity index indicate significance word special type restaurant customers overall tend express sentiment regard service distinction different cuisines result match common sense obtain japanese cuisines usually fresh french cuisines overprice italian restaurants often famous pizzas
traditional supervise learn make close world assumption class appear test data must appear train also apply text learn text classification learn use increasingly dynamic open environments new test document may belong train class identify novel document classification present important problem problem call open world classification open classification paper propose novel deep learn base approach outperform exist state art techniques dramatically
machine translation often try collect resources improve performance however language pair korean arabic korean vietnamese enough resources train machine translation systems paper propose use synthetic methods extend low resource corpus apply multi source neural machine translation model show improvement machine translation performance corpus extension use synthetic method specifically focus create source sentence make better target sentence include use synthetic methods find corpus extension could also improve performance multi source neural machine translation show corpus extension multi source model efficient methods low resource language pair furthermore methods use together find better machine translation performance
paper propose reinforce method refine output layer exist recurrent neural network rnn language model refer propose method input output gate iog iog extremely simple structure thus easily combine rnn language model experiment penn treebank wikitext two datasets demonstrate iog consistently boost performance several different type current topline rnn language model
aspect term extraction eat detect opinionated aspect term sentence text span end goal perform aspect base sentiment analysis small amount available datasets supervise eat fact cover domains raise need exploit data source new creative ways publicly available review corpora contain plethora opinionated aspect term cover larger domain spectrum paper first propose method use review corpora create new dataset eat method rely attention mechanism select sentence high likelihood contain actual opinionated aspects thus improve quality extract aspects use construct dataset train model perform eat distant supervision evaluate human annotate datasets prove method achieve significantly improve performance various unsupervised supervise baselines finally prove sentence selection matter come create new datasets eat specifically show use set select sentence lead higher eat performance compare use whole sentence set
context personalize medicine text mine methods pose interest option identify disease gene associations use generate novel link diseases genes may complement knowledge structure databases straightforward approach extract link text rely simple assumption postulate association genes diseases co occur within document however approach tend yield number spurious associations ii capture different relevant type associations iii incapable aggregate knowledge spread across document thus propose approach disease gene co occurrences gene gene interactions represent rdf graph machine learn base classifier train incorporate feature extract graph separate disease gene pair valid disease gene associations spurious ones manually curated genetic test registry approach yield thirty point increase f1 score plain co occurrence baseline
describe data drive approach automatically explain new non standard english expressions give sentence build large dataset include fifteen years crowdsourced examples urbandictionarycom unlike prior study focus match keywords slang dictionary investigate possibility learn neural sequence sequence model generate explanations unseen non standard english expressions give context propose dual encoder approach word level encoder learn representation context second character level encoder learn hide representation target non standard expression model produce reasonable definitions new non standard english expressions give context certain confidence
color name often make multiple word task natural language understand investigate depth capacity neural network base sum word embeddings sowe recurrence lstm gru base rnns convolution cnn estimate color sequence term consider point distribution estimate color argue latter particular value clear agreement people particular color describe different people different idea mean dark orange example surprisingly despite simplicity sum word embeddings generally perform best almost evaluations
impressive ability children acquire language widely study phenomenon factor influence pace pattern word learn remain subject active research although many model predict age acquisition word propose little emphasis direct raw input children achieve work present comparatively large scale multi modal corpus prosody text align child direct speech corpus contain automatically extract word level prosodic feature investigate utility information predictors age acquisition show prosody feature boost predictive power regularize regression demonstrate utility context multi modal factorize language model train test child direct speech
ever grow amount textual data large variety languages domains genres become standard evaluate nlp algorithms multiple datasets order ensure consistent performance across heterogeneous setups however multiple comparisons pose significant challenge traditional statistical analysis methods nlp lead erroneous conclusions paper propose replicability analysis framework statistically sound analysis multiple comparisons algorithms nlp task discuss theoretical advantage framework current statistically unjustified practice nlp literature demonstrate empirical value across four applications multi domain dependency parse multilingual pos tag cross domain sentiment classification word similarity prediction
process human produce text use natural language process nlp techniques two fundamental subtasks arise segmentation plain text meaningful subunits eg entities ii dependency parse establish relations subunits paper develop relatively simple effective neural joint model perform segmentation dependency parse together instead one state art work focus particular real estate ad set aim convert ad structure description name property tree comprise task one identify important entities property eg room classifieds two structure tree format work propose new joint model able tackle two task simultaneously construct property tree avoid error propagation would arise subtasks one pipelined fashion ii exploit interactions subtasks purpose perform extensive comparative study pipeline methods new propose joint model report improvement three percentage point overall edge f1 score property tree also propose attention methods encourage model focus salient tokens construction property tree thus experimentally demonstrate usefulness attentive neural architectures propose joint model showcasing improvement two percentage point edge f1 score application
name entity recognition ner one common task natural language process purpose ner find classify tokens text document predefined categories call tag person name quantity expressions percentage expressions name locations organizations well expression time currency others although number approach propose task russian language still substantial potential better solutions work study several deep neural network model start vanilla bi directional long short term memory bi lstm supplement conditional random field crf well highway network finally add external word embeddings model evaluate across three datasets gareev dataset person one thousand factrueval two thousand and sixteen find extension bi lstm model crf significantly increase quality predictions encode input tokens external word embeddings reduce train time allow achieve state art russian ner task
parallel sentence extraction task address data sparsity problem find multilingual natural language process applications propose end end deep neural network approach detect translational equivalence sentence two different languages contrast previous approach typically rely multiples model various word alignment feature leverage continuous vector representation sentence remove need domain specific feature engineer use siamese bidirectional recurrent neural network result strong baseline base state art parallel sentence extraction system show significant improvement quality extract parallel sentence translation performance statistical machine translation systems believe study first one investigate deep learn parallel sentence extraction task
order maximize applicability sentiment analysis result necessary classify overall sentiment positive negative give document also identify main word contribute classification however datasets sentiment analysis sentiment label document sentence word information word play important role sentiment classification paper propose method identify key word discriminate positive negative sentence use weakly supervise learn method base convolutional neural network cnn model word represent continuous value vector sentence represent matrix whose row correspond word vector use sentence cnn model train use sentence matrices input sentiment label output cnn model train implement word attention mechanism identify high contribute word classification result class activation map use weight fully connect layer end learn cnn model order verify propose methodology evaluate classification accuracy inclusion rate polarity word use two movie review datasets experimental result show propose model correctly classify sentence polarity also successfully identify correspond word high polarity score
paper investigate role dependency tree name entity recognizer upon use set gcn perform comparison among different ner architectures show grammar sentence positively influence result experiment ontonotes dataset demonstrate consistent performance improvements without require heavy feature engineer additional language specific knowledge
online social platforms beset hateful speech content express hatred person group people content frighten intimidate silence platform users inspire users commit violence despite widespread recognition problems pose content reliable solutions even detect hateful speech lack present work establish keyword base methods insufficient detection propose approach detect hateful speech use content produce self identify hateful communities train data approach bypass expensive annotation process often require train keyword systems perform well across several establish platforms make substantial improvements current state art approach
sentence level classification sequential label two fundamental task language understand two task usually model separately reality often correlate example intent classification slot fill topic classification name entity recognition order utilize potential benefit correlations propose jointly train model learn two task simultaneously via long short term memory lstm network model predict sentence level category word level label sequence stepwise output hide representations lstm also introduce novel mechanism sparse attention weigh word differently base semantic relevance sentence level classification propose method outperform baseline model atis trec datasets
paper introduce first evaluation chinese human computer dialogue technology detail evaluation scheme task metrics collect annotate data train develop test evaluation include two task namely user intent classification online test task orient dialogue consider different source data train develop first task also divide two sub task two task come real problems use applications develop industry evaluation data provide iflytek corporation meanwhile paper publish evaluation result present current performance participants two task chinese human computer dialogue technology moreover analyze exist problems human computer dialogue well evaluation scheme
paper propose task universal semantic tag tag word tokens language neutral semantically informative tag argue task independent nature contribute better semantic analysis wide coverage multilingual text present initial version semantic tagset show tag provide semantically fine grain information b suitable cross lingual semantic parse application semantic tag parallel mean bank support point tag contribute formal lexical semantics cross lingual projection part application annotate small corpus semantic tag present new baseline result universal semantic tag
essential meaningful interaction ground symbolic conversational societal level present ongoing work anki cozmo toy robot research platform leverage recent word classifiers model lexical semantics interactive reference resolution task language ground
analyze common five thousand male five thousand female turkish name base etymological morphological semantic attribute name statistics base turkish citizens alive two thousand and fourteen cover ninety population best knowledge study comprehensive data drive analysis turkish personal name female name greater diversity male name eg top fifteen male name cover twenty-five male population whereas top twenty-eight female name cover twenty-five female population despite diversity female name exhibit predictable pattern example certain root gul nar rise pomegranate red respectively use generate hundreds unique female name turkish personal name origins mainly arabic follow turkish persian compute overall frequencies name accord broad semantic theme identify previous study find foreign origin name olga khaled pastoral name yayougmur deniz rain sea respectively name base fruit plant filiz menekcse sprout violet respectively frequently observe among females among males name base animals arslan yunus lion dolphin respectively name base famous historical figure mustafa kemal ofyouguz kayougan founder turkish republic founder turks turkish mythology respectively observe frequently
paper show achieve state art performance industry standard nist two thousand hub5 english evaluation set explore densely connect lstms inspire densely connect convolutional network recently introduce image classification task also propose acoustic model adaptation scheme simply average parameters seed neural network acoustic model adapt version method apply callhome train corpus improve individual system performances average sixty-one relative callhome portion evaluation set performance loss switchboard portion rnn lm rescoring lattice combination five systems train across three different phone set two thousand and seventeen speech recognition system obtain fifty ninety-one switchboard callhome respectively best word error rat report thus far accord ibm latest work compare human machine transcriptions report switchboard word error rate consider surpass human parity fifty-one transcribe conversational telephone speech
generate structural query language sql query natural language long stand open problem answer natural language question database table require model complex interactions columns table question paper apply synthesize approach solve problem base structure sql query break model three sub modules design specific deep neural network take inspiration similar machine read task employ bidirectional attention mechanisms character level embed convolutional neural network cnns improve result experimental evaluations show model achieve state art result wikisql dataset
study implement vector space model approach measure sentiment orientations word two representative vectors positive negative polarity construct use high dimensional vec tor space unsupervised semi supervise manner sentiment ori entation value per word determine take difference cosine distance two reference vec tors two condition unsupervised semi supervise compare exist unsupervised method turney two thousand and two result experi ment demonstrate novel ap proach significantly outperform pre vious unsupervised approach practical data efficient well
paper propose tackle open domain question answer use wikipedia unique knowledge source answer factoid question text span wikipedia article task machine read scale combine challenge document retrieval find relevant article machine comprehension text identify answer span article approach combine search component base bigram hash tf idf match multi layer recurrent neural network model train detect answer wikipedia paragraph experiment multiple exist qa datasets indicate one modules highly competitive respect exist counterparts two multitask learn use distant supervision combination effective complete system challenge task
present novel cross lingual transfer method paradigm completion task map lemma inflect form use neural encoder decoder model state art monolingual task use label data high resource language increase performance low resource language experiment twenty-one language pair four different language families obtain fifty-eight higher accuracy without transfer show even zero shoot one shoot learn possible find degree language relatedness strongly influence ability transfer morphological knowledge
paper present frame dataset frame available http datasetsmaluubacom frame corpus one thousand, three hundred and sixty-nine human human dialogues average fifteen turn per dialogue develop dataset study role memory goal orient dialogue systems base frame introduce task call frame track extend state track set several state track simultaneously propose baseline model task show frame also use study memory dialogue management information presentation natural language generation
citation sentiment analysis important task scientific paper analysis exist machine learn techniques citation sentiment analysis focus labor intensive feature engineer require large annotate corpus automatic feature extraction tool word2vec successfully apply sentiment analysis short texts work conduct empirical research question well word2vec work sentiment analysis citations propose method construct sentence vectors sent2vec average word embeddings learn anthology collections acl embeddings also investigate polarity specific word embeddings ps embeddings classify positive negative citations sentence vectors form feature space examine citation sentence map feature input classifiers support vector machine supervise classification use ten cross validation scheme evaluation conduct set annotate citations result show word embeddings effective classify positive negative citations however hand craft feature perform better overall classification
multimodal conversation agents gain importance several domains retail travel etc deep learn research area limit primarily due lack availability large scale open chatlogs overcome bottleneck paper introduce task multimodal domain aware conversations propose mmd benchmark dataset dataset gather work close coordination large number domain experts retail domain experts suggest various conversations flow dialog state typically see multimodal conversations fashion domain keep flow state mind create dataset consist 150k conversation sessions shoppers sales agents help house annotators use semi automate manually intense iterative process dataset propose five new sub task multimodal conversations along evaluation methodology also propose two multimodal neural model encode attend decode paradigm demonstrate performance two sub task namely text response generation best image response selection experiment serve establish baseline performance open new research directions sub task sub task present per state evaluation nine significant dialog state would enable focus research understand challenge complexities involve state
recent work show synthetic parallel data automatically generate translation model effective various neural machine translation nmt issue study build nmt systems use synthetic parallel data efficient alternative real parallel data also present new type synthetic parallel corpus propose pseudo parallel data distinct previous work grind truth synthetic examples mix side sentence pair experiment czech german french german translations demonstrate efficacy propose pseudo parallel corpus show enhance result bidirectional translation task also substantial improvement aid grind truth real parallel corpus
one important problems machine translation mt evaluation evaluate similarity translation hypotheses different surface form reference especially segment level propose use word embeddings perform word alignment segment level mt evaluation perform experiment three type alignment methods use word embeddings evaluate propose methods various translation datasets experimental result show propose methods outperform previous word embeddings base methods
semantic role label srl task come utilize parse information traditional methods recent recurrent neural network rnn base methods use feature engineer way paper propose syntax aware long short time memorysa lstm structure sa lstm modify accord dependency parse information order model parse information directly architecture engineer way instead feature engineer way experimentally demonstrate sa lstm gain improvement model architecture furthermore sa lstm outperform state art cpb ten significantly accord student test p005
content dense news report important factual information event direct succinct manner information seek applications information extraction question answer summarization normally assume text deal content dense empirically test assumption news article business yous international relations sport science journalism domains find clearly indicate half news texts study fact content dense motivate development supervise content density detector heuristically label large train corpus task train two layer classify model base lexical unlexicalized syntactic feature manually annotate data compare performance domain specific classifiers train data give news domain general classifier data four domains pool together annotation prediction experiment demonstrate concept content density vary depend domain naive annotators provide judgement bias toward stereotypical domain label domain specific classifiers accurate domains content dense texts typically fewer domain independent classifiers reproduce better naive crowdsourced judgements classification prediction high across condition around eighty
present first parser ucca cross linguistically applicable framework semantic representation build extensive typological work support rapid annotation ucca pose challenge exist parse techniques exhibit reentrancy result dag structure discontinuous structure non terminal nod correspond complex semantic units knowledge conjunction formal properties support exist parser transition base parser use novel transition set feature base bidirectional lstms value ucca parse ability handle general graph structure inform development parsers semantic dag structure languages frequently use discontinuous structure
input neural sequence sequence model often determine stream system eg word segmenter part speech tagger speech recognizer stream model potentially error prone represent input word lattices allow make uncertainty explicit capture alternative sequence posterior probabilities compact form work extend treelstm tai et al two thousand and fifteen latticelstm able consume word lattices use encoder attentional encoder decoder model integrate lattice posterior score architecture extend treelstm child sum forget gate introduce bias term attention mechanism experiment speech translation lattices report consistent improvements baselines translate either one best hypothesis lattice without posterior score
increase capacity recurrent neural network rnn usually involve augment size hide layer significant increase computational cost recurrent neural tensor network rntn increase capacity use distinct hide layer weight word greater cost memory usage paper introduce restrict recurrent neural tensor network r rntn reserve distinct hide layer weight frequent vocabulary word share single set weight infrequent word perplexity evaluations show fix hide layer size r rntns improve language model performance rnns use small fraction parameters unrestricted rntns result hold r rntns use gate recurrent units long short term memory
build voice conversion vc system non parallel speech corpora challenge highly valuable real application scenarios situations source target speakers repeat texts may even speak different languages case one possible although indirect solution build generative model speech generative model focus explain observations latent variables instead learn pairwise transformation function thereby bypass requirement speech frame alignment paper propose non parallel vc framework variational autoencoding wasserstein generative adversarial network vaw gin explicitly consider vc objective build speech model experimental result corroborate capability framework build vc system unaligned data demonstrate improve conversion quality
research analysis microblogging platforms experience renew surge large number work apply representation learn model applications like sentiment analysis semantic textual similarity computation hashtag prediction etc although performance representation learn model better traditional baselines task little know elementary properties tweet encode within representations particular representations work better certain task work present constitute first step open black box vector embeddings tweet traditional feature engineer methods high level applications exploit various elementary properties tweet believe tweet representation effective application meticulously encode application specific elementary properties tweet understand elementary properties encode tweet representation evaluate representations accuracy model properties tweet length presence particular word hashtags mention capitalization etc systematic extensive study nine supervise four unsupervised tweet representations popular eight textual five social elementary properties reveal bi directional lstms blstms skip think vectors stv best encode textual social properties tweet respectively fasttext best model low resource settings provide little degradation reduction embed size finally draw interest insights correlate model performance obtain elementary property prediction task highlevel downstream applications
previous approach train syntax base sentiment classification model require phrase level annotate corpora readily available many languages english thus propose use tree structure long short term memory attention mechanism pay attention subtree parse tree experimental result indicate model achieve state art performance japanese sentiment classification task
present character base model joint segmentation pos tag chinese bidirectional rnn crf architecture general sequence tag adapt apply novel vector representations chinese character capture rich contextual information lower character level feature propose model extensively evaluate compare state art tagger respectively ctb5 ctb9 ud chinese experimental result indicate model accurate robust across datasets different size genres annotation scheme obtain state art performance ctb5 achieve nine thousand, four hundred and thirty-eight f1 score joint segmentation pos tag
present submit systems semantic textual similarity sts track four semeval two thousand and seventeen give pair spanish english sentence system must estimate semantic similarity score zero five submission use syntax base dictionary base context base mt base methods also combine methods unsupervised supervise way best run rank 1st track 4a correlation eight thousand, three hundred and two human annotations
paper explore linear methods combine several word embed model ensemble construct combine model use iterative method base either ordinary least square regression solution orthogonal procrustes problem evaluate propose approach estonian morphologically complex language available corpora train word embeddings relatively small compare combine model input word embed model use synonym analogy test result show use ordinary least square regression perform poorly experiment use orthogonal procrustes combine several word embed model ensemble model lead seven ten relative improvements mean result initial model synonym test nineteen forty-seven analogy test
pre aspiration define period glottal friction occur sequence vocalic consonantal sonorants phonetically voiceless obstruents propose two machine learn methods automatic measurement pre aspiration duration feedforward neural network work frame level structure prediction model rely manually design feature function work segment level input algorithms speech signal arbitrary length contain single obstruent output pair time constitute pre aspiration boundaries train model set manually annotate examples result suggest structure model superior frame base model yield higher accuracy predict boundaries generalize new speakers new languages finally demonstrate applicability structure prediction algorithm replicate linguistic analysis pre aspiration aberystwyth english high correlation
mra multilingual report annotator web application translate radiology text annotate radlex term goal explore solution translate non english radiology report way solve problem text mine tool develop english brief paper explain language barrier problem shortly describe application mra find https githubcom lasigebiotm mra
automatic question generation aim generate question text passage generate question answer certain sub span give passage traditional methods mainly use rigid heuristic rule transform sentence relate question work propose apply neural encoder decoder model generate meaningful diverse question natural language sentence encoder read input text answer position produce answer aware input representation feed decoder generate answer focus question conduct preliminary study neural question generation text squad dataset experiment result show method produce fluent diverse question
explore ability word embeddings capture semantic morphological similarity affect different type linguistic properties surface form lemma morphological tag use compose representation word train several model use different subset properties compose representations evaluate model semantic morphological measure reveal useful insights relationship semantics morphology
paper present novel approach model thread discussions social media use graph structure bidirectional lstm represent hierarchical temporal conversation structure experiment task predict popularity comment reddit discussions propose model outperform node independent architecture different set input feature analyse show benefit model full course discussion improve detection early late stag use language cue bidirectional tree state update help identify controversial comment
document offer detail linguistic description snacs semantic network adposition case supersenses schneider et al two thousand and eighteen inventory fifty semantic label supersenses characterize use adpositions case markers somewhat coarse level granularity demonstrate streusle corpus https githubcom nert gu streusle version forty-three track guidelines version twenty-five though snacs inventory aspire universal document specific english documentation languages publish separately version two revision supersense inventory propose english schneider et al two thousand and fifteen two thousand and sixteen henceforth v1 turn base previous scheme present inventory develop extensive review v1 corpus annotations english plus previously unanalyzed genitive case possessives blodgett schneider two thousand and eighteen well consideration adposition case phenomena hebrew hindi korean german hwang et al two thousand and seventeen present theoretical underpinnings v2 scheme schneider et al two thousand and eighteen summarize scheme application english corpus data automatic disambiguation task
evaluate semantic parser base character base sequence sequence model context semeval two thousand and seventeen share task semantic parse amrs data augmentation super character pos tag gain major improvements performance compare baseline character level model although improve previous character base neural semantic parse model overall accuracy still lower state art amr parser ensemble combine neural semantic parser exist traditional parser yield small gain performance
paper describe approach semeval two thousand and seventeen task ten extract keyphrases relations scientific publications specifically subtask b classification identify keyphrases explore three different deep learn approach character level convolutional neural network cnn stack learner mlp meta classifier attention base bi lstm approach create ensemble differently hyper parameterized systems achieve micro f1 score sixty-three test data approach rank 2nd score 1st place system sixty-four four accord official score however erroneously train two three neural net stacker cnn roughly fifteen full data namely original development set train full data trainingdevelopment ensemble micro f1 score sixty-nine code available https githubcom ukplab semeval2017 scienceie
article compare four probabilistic algorithms global algorithms word sense disambiguation wsd term number scorer call local algo rithm f1 score determine gold standard scorer two algorithms come state art simulate anneal algorithm saa genetic algorithm ga well two algorithms first adapt wsd state art probabilistic search algorithms namely cuckoo search algorithm csa bat search algorithm bs wsd require evaluate exponentially many word sense combinations branch factor six probabilistic algorithms allow find approximate solution tractable time sample search space find csa ga sa eventually converge similar result ninety-eight f1 score csa get faster fewer scorer call reach ninety-five f1 sa fewer scorer call ba strict convergence criterion prevent reach eighty-nine f1
one problem every presenter face deliver public discourse hold listeners attentions keep involve therefore many study conversation analysis work issue suggest qualitatively con structions effectively lead audience applause investigate proposals quantitatively study alyze transcripts two thousand, one hundred and thirty-five ted talk particular fo cus rhetorical devices use presenters applause elicitation conduct regression anal ysis identify interpret twenty-four rhetorical devices trigger audience applaud build model rec ognize applause evoke sentence conclude work potential implications
ever increase number social media websites electronic newspapers internet forums allow visitors leave comment others read interact exchange free participants malicious intentions contribute write conversation among different communities users adopt strategies handle users paper present comprehensive categorization troll phenomena resource inspire politeness research propose model jointly predict four crucial aspects troll intention interpretation intention disclosure response strategy finally present new annotate dataset contain excerpt conversations involve troll interactions users hope useful resource research community
consider two graph model semantic change first time series model relate embed vectors one time period embed vectors previous time periods second construct one graph word nod graph correspond time point edge weight similarity word mean across two time point apply two model corpora across three different languages find semantic change linear two sense firstly today embed vectors mean word derive linear combinations embed vectors neighbor previous time periods secondly self similarity word decay linearly time consider find new laws hypotheses semantic change
present contribution tutorial select aspects prosody rhythms melodies speech base course name summer school contemporary phonetics phonology tongji university shanghai china july two thousand and sixteen tutorial intend introduction experimental methodology overview literature topic outline observationally accessible aspects fundamental frequency time pattern aid computational visualisation situate semiotic framework sign rank interpretations informal introduction basic concepts prosody introduction discussion place prosody architecture language selection acoustic phonetic topics phonemic tone accent prosody word prosody phrasal prosody discourse prosody discuss stylisation method visualise aspects prosody introduce examples take number typologically different languages anyi agni niger congokwa ivory coast english kuki thadou sino tibetan north east india myanmar mandarin chinese tem niger congogur togo farsi main focus fundamental frequency pattern issue time rhythm also discuss final section read possible future research directions outline
implicit semantic role label isrl task predict semantic roles predicate appear explicit arguments rather regard common sense knowledge mention earlier discourse introduce approach isrl base predictive recurrent neural semantic frame model prnsfm use large unannotated corpus learn probability sequence semantic arguments give predicate leverage sequence probabilities predict prnsfm estimate selectional preferences predicate arguments nombank isrl test set approach improve state art performance implicit semantic role label less reliance prior work manually construct language resources
present simple yet effective approach link entities query key idea search sentence similar query wikipedia article directly use human annotate entities similar sentence candidate entities query employ rich set feature link probability context match word embeddings relatedness among candidate entities well relate entities rank candidates regression base framework advantage approach lie two aspects contribute rank process final link result first greatly reduce number candidate entities filter irrelevant entities word query second obtain query sensitive prior probability addition static link probability derive wikipedia article conduct experiment two benchmark datasets entity link query namely erd14 dataset gerdaq dataset experimental result show method outperform state art systems yield seven hundred and fifty f1 erd14 dataset five hundred and sixty-nine gerdaq dataset
present character word long short term memory language model reduce perplexity respect baseline word level language model reduce number parameters model character information reveal structural dissimilarities word even use word vocabulary thus improve model infrequent unknown word concatenate word character embeddings achieve two hundred and seventy-seven relative improvement english compare baseline model similar amount parameters four hundred and fifty-seven dutch moreover also outperform baseline word level model larger number parameters
recent research show brazilian students serious problems regard read skills full development skill key academic professional future every citizen tool classify complexity read materials children aim improve quality model teach read text comprehension english fengs work eleven consider state art grade level prediction achieve seventy-four accuracy automatically classify four level textual complexity close school grade classifiers nonfiction texts close grade portuguese article propose scheme manual annotation texts five grade level use customize read avoid lack interest students advance read block still need make progress obtain fifty-two accuracy classify texts five level seventy-four three level result prove promise compare state art work9
semantic role label srl natural language process task enable detection events describe sentence participants events brazilian portuguese bp two study recently conclude perform srl journalistic texts one obtain f1 measure score seven hundred and ninety-six use propbankbr corpus syntactic tree manually revise eight without use treebank train obtain f1 measure score six hundred and eighty corpus however use manually revise syntactic tree task represent real scenario application goal paper evaluate performance srl revise non revise syntactic tree use larger balance corpus bp journalistic texts first show one system also perform better eight system larger corpus second srl system train non revise syntactic tree perform better non revise tree system train gold standard data
extend periods time sequence generation model rely beam search algorithm generate output sequence however correctness beam search degrade model confident suboptimal prediction paper propose perform minimum bay risk mbr decode extra step later stage order speed mbr decode compute bay risk gpu batch mode experiment find mbr reranking work large beam size later stage mbr decode show outperform simple mbr reranking machine translation task
recent time data grow rapidly every domain news social media bank education etc due excessiveness data need automatic summarizer capable summarize data especially textual data original document without lose critical purpose text summarization emerge important research area recent past regard review exist work text summarization process useful carry research paper recent literature automatic keyword extraction text summarization present since text summarization process highly depend keyword extraction literature include discussion different methodology use keyword extraction text summarization also discuss different databases use text summarization several domains along evaluation matrices finally discuss briefly issue research challenge face researchers along future direction
ensembling well know technique neural machine translation nmt improve system performance instead single neural net multiple neural net topology train separately decoder generate predictions average individual model ensembling often improve quality generate translations drastically however suitable production systems cumbersome slow work aim reduce runtime par single system without compromise translation quality first show ensemble unfold single large neural network imitate output ensemble system show unfold already improve runtime practice since work do gpu proceed describe set techniques shrink unfold network reduce dimensionality layer japanese english report result network size decode speed single nmt network perform level three ensemble system
neural machine translation mt model obtain state art performance maintain simple end end architecture however little know model learn source target languages train process work analyze representations learn neural mt model various level granularity empirically evaluate quality representations learn morphology extrinsic part speech morphological tag task conduct thorough investigation along several parameters word base vs character base representations depth encode layer identity target language encoder vs decoder representations data drive quantitative evaluation shed light important aspects neural mt system ability capture word structure
paper describe luminoso participation semeval two thousand and seventeen task two multilingual cross lingual semantic word similarity system base conceptnet conceptnet open multilingual knowledge graph focus general knowledge relate mean word phrase submission semeval update previous work build high quality multilingual word embeddings combination conceptnet distributional semantics system take first place subtasks rank first four five separate languages also rank first ten cross lingual language pair
refer expression generation reg model use speaker dependent information require considerable amount train data produce every individual speaker may otherwise perform poorly work present simple reg experiment allow use larger train data set group speakers accord overspecification preferences intrinsic evaluation show method generally outperform personalise method find previous work
paper explore incremental train strategy skip gram model negative sample sgns empirical theoretical perspectives exist methods neural word embeddings include sgns multi pass algorithms thus perform incremental model update address problem present simple incremental extension sgns provide thorough theoretical analysis demonstrate validity empirical experiment demonstrate correctness theoretical analysis well practical usefulness incremental algorithm
propose finite state transducer fst representation model use decode keyboard input mobile devices draw learn field speech recognition describe decode framework satisfy strict memory latency constraints keyboard input extend framework support functionalities typically present speech recognition literal decode autocorrections word completions next word predictions describe general framework call short keyboard fst decoder well implementation detail new compare speech fst decoder demonstrate fst decoder enable new ux feature post corrections finally sketch decoder support advance feature personalization contextualization
propose simple yet effective text base user geolocation model base neural network one hide layer achieve state art performance three twitter benchmark geolocation datasets addition produce word phrase embeddings hide layer show useful detect dialectal term part analysis dialectal term release dare dataset evaluate dialect term detection methods
discourse segmentation crucial step build end end discourse parsers however discourse segmenters exist languages domains typically detect intra sentential segment boundaries assume gold standard sentence token segmentation rely high quality syntactic parse rich heuristics generally available across languages domains paper propose statistical discourse segmenters five languages three domains rely gold pre annotations also consider problem learn discourse segmenters label data available language fully supervise system obtain eight hundred and ninety-five f1 english newswire slight drop performance domains report supervise unsupervised cross lingual result five languages total
paper use framework neural machine translation learn joint sentence representations across six different languages aim representation independent language likely capture underlie semantics define new cross lingual similarity measure compare 14m sentence representations study characteristics close sentence provide experimental evidence sentence close embed space indeed semantically highly relate often quite different structure syntax relations also hold compare sentence different languages
recent years see rapid significant progress automatic image description open problems area work evaluate use text base similarity metrics indicate improvements without explain improve paper present detail error analysis descriptions generate state art attention base model analysis operate two level first check descriptions accuracy categorize type errors observe inaccurate descriptions find twenty descriptions free errors surprisingly twenty-six unrelated image finally manually correct frequently occur error type eg gender identification estimate performance reward address errors observe gain two one bleu point per type
paper describe method detect event descrip tions different news article model semantics events components use rdf representations compare descriptions solve cross document event coreference task com ponent approach event semantics define identity granularity events different level perform close state art approach cross document event coreference task outperform work assume similar quality event detection demonstrate granularity identity interconnect discuss se mantic anomaly could use define differences coreference subevent topical relations
chinese discourse coherence model remain challenge taskin natural language process fieldexisting approach mostlyfocus need feature engineer whichadoptthe sophisticate feature capture logic syntactic semantic relationships acrosssentences within textin paper present entity drivenrecursive deep modelfor chinese discourse coherence evaluation base current english discourse coherenceneural network model specifically overcome shortage identify entitynouns overlap across sentence currentmodel combine modelsuccessfully investigatesthe entities information recursive neural network freameworkevaluation result sentence order machine translation coherence rat task show effectiveness propose model significantly outperform exist strong baseline
translation consider document whole help resolve ambiguities inconsistencies paper propose cross sentence context aware approach investigate influence historical contextual information performance neural machine translation nmt first history summarize hierarchical way integrate historical representation nmt two strategies one warm start encoder decoder state two auxiliary context source update decoder state experimental result large chinese english translation task show approach significantly improve upon strong attention base nmt system twenty-one bleu point
neural sequence sequence model provide viable new approach abstractive text summarization mean restrict simply select rearrange passages original text however model two shortcomings liable reproduce factual detail inaccurately tend repeat work propose novel architecture augment standard sequence sequence attentional model two orthogonal ways first use hybrid pointer generator network copy word source text via point aid accurate reproduction information retain ability produce novel word generator second use coverage keep track summarize discourage repetition apply model cnn daily mail summarization task outperform current abstractive state art least two rouge point
paper investigate robustness nlp perturb word form neural approach achieve almost human like accuracy certain task condition often sensitive small change input non canonical input eg typos yet stability robustness desire properties applications involve user generate content humans easily cope noisy adversary condition paper study impact noisy input consider different noise distributions one type noise combination noise type mismatch noise distributions train test moreover empirically evaluate robustness different model convolutional neural network recurrent neural network non neural model different basic units character byte pair encode units different nlp task morphological tag machine translation
concept map use concisely represent important information bring structure large document collections therefore study variant multi document summarization produce summaries form concept map however suitable evaluation datasets task currently miss close gap present newly create corpus concept map summarize heterogeneous collections web document educational topics create use novel crowdsourcing approach allow us efficiently determine important elements large document collections release corpus along baseline system propose evaluation protocol enable research variant summarization
information extraction ie text largely focus relations individual entities award however facts never fully mention ie method perfect recall thus beneficial also tap content cardinalities relations example many award someone introduce novel problem extract cardinalities discuss specific challenge set apart standard ie present distant supervision method use conditional random field preliminary evaluation result precision three fifty-five depend difficulty relations
neural machine translation nmt new approach machine translation achieve promise result comparable traditional approach statistical machine translation smt despite recent success nmt handle larger vocabulary train complexity decode complexity proportionally increase number target word problem become even serious translate patent document contain many technical term observe infrequently paper propose select phrase contain vocabulary word use statistical approach branch entropy allow propose nmt system apply translation task language pair without language specific knowledge technical term identification select phrase replace tokens train post translate phrase translation table smt evaluation japanese chinese chinese japanese japanese english english japanese patent sentence translation prove effectiveness phrase select branch entropy propose nmt model achieve substantial improvement baseline nmt model without propose technique moreover number translation errors translation baseline nmt model without propose technique reduce around half propose nmt model
neural machine translation nmt new approach machine translation achieve promise result comparable traditional approach statistical machine translation smt despite recent success nmt handle larger vocabulary train complexity decode complexity proportionally increase number target word problem become even serious translate patent document contain many technical term observe infrequently nmts word vocabulary represent single unknown token paper propose method enable nmt translate patent sentence comprise large vocabulary technical term train nmt system bilingual data wherein technical term replace technical term tokens allow translate source sentence except technical term use decoder translate source sentence technical term tokens replace tokens technical term translations use smt also use rerank one thousand best smt translations basis average smt score nmt rescoring translate sentence technical term tokens experiment japanese chinese patent sentence show propose nmt system achieve substantial improvement thirty-one bleu point twenty-three rib point traditional smt systems improvement approximately six bleu point eight rib point equivalent nmt system without propose technique
extractive summarization methods focus main body document sentence need extract however gist document may lie side information title image caption often available newswire article propose explore side information context single document extractive summarization develop framework single document summarization compose hierarchical document encoder attention base extractor attention side information evaluate model large scale news dataset show extractive summarization side information consistently outperform counterpart use side information term informativeness fluency
abstract mean representation amr annotation efforts mostly focus english order train parsers languages propose method base annotation projection involve exploit annotations source language parallel corpus source language target language use english source language show promise result italian spanish german chinese target languages besides evaluate target parsers non gold datasets propose evaluation method exploit english gold annotations require access gold annotations target languages achieve invert projection process new english parser learn target language parser evaluate exist english gold standard
test whether distributional model one shoot learn definitional properties text use bayesian model find first learn overarch structure know data regularities textual contexts properties help one shoot learn individual context items highly informative experiment show model learn properties single exposure give informative utterance
present solution problem paraphrase identification question focus recent dataset question pair annotate binary paraphrase label show variant decomposable attention model parikh et al two thousand and sixteen result accurate performance task far simpler many compete neural architectures furthermore model pretrained noisy dataset automatically collect question paraphrase obtain best report performance dataset
paper propose address word sense ambiguity issue unsupervised manner word sense representations learn along word sense selection mechanism give contexts prior work focus design single model deliver mechanisms thus suffer either coarse grain representation learn inefficient sense selection propose modular approach muse implement flexible modules optimize distinct mechanisms achieve first purely sense level representation learn system linear time sense selection leverage reinforcement learn enable joint train propose modules introduce various exploration techniques sense selection better robustness experiment benchmark data show propose approach achieve state art performance synonym selection well contextual word similarities term maxsimc
present simple effective approach incorporate syntactic structure neural attention base encoder decoder model machine translation rely graph convolutional network gcns recent class neural network develop model graph structure data gcns use predict syntactic dependency tree source sentence produce representations word ie hide state encoder sensitive syntactic neighborhoods gcns take word representations input produce word representations output easily incorporate layer standard encoders eg top bidirectional rnns convolutional neural network evaluate effectiveness english german english czech translation experiment different type encoders observe substantial improvements syntax agnostic versions consider setups
present simple method incorporate syntactic information target language neural machine translation system translate linearize lexicalize constituency tree experiment wmt16 german english news translation task result improve bleu score compare syntax agnostic nmt baseline train dataset analysis translations syntax aware system show perform reorder translation comparison baseline small scale human evaluation also show advantage syntax aware system
propose model automatically describe change introduce source code program use natural language method receive input set code commit contain modifications message introduce user two modalities use train encoder decoder architecture evaluate approach twelve real world open source project four different program languages quantitative qualitative result show propose approach generate feasible semantically sound descriptions standard project settings also cross project set
previous work model compositionality word create character level model mean reduce problems sparsity rare word however many write systems compositionality effect even character level mean character derive sum part paper model effect create embeddings character base visual characteristics create image character run convolutional neural network produce visual character embed experiment text classification task demonstrate model allow better process instance rare character languages chinese japanese korean additionally qualitative analyse demonstrate propose model learn focus part character carry semantic content result embeddings coherent visual space
propose novel deep learn model joint document level entity disambiguation leverage learn neural representations key components entity embeddings neural attention mechanism local context windows differentiable joint inference stage disambiguation approach thereby combine benefit deep learn traditional approach graphical model probabilistic mention entity map extensive experiment show able obtain competitive state art accuracy moderate computational cost
discourse connectives eg however term explicitly convey discourse relation within text discourse connectives show effective clue automatically identify discourse relations always use convey relations thus first disambiguate discourse usage non discourse usage paper investigate applicability feature propose disambiguation english discourse connectives french result french discourse treebank fdtb show syntactic lexical feature develop english texts effective french allow disambiguation french discourse connectives accuracy nine hundred and forty-two
publicly release new large scale dataset call searchqa machine comprehension question answer unlike recently release datasets deepmind cnn dailymail squad propose searchqa construct reflect full pipeline general question answer start exist article generate question answer pair start exist question answer pair crawl j archive augment text snippets retrieve google follow approach build searchqa consist 140k question answer pair pair four hundred and ninety-six snippets average question answer context tuple searchqa come additional meta data snippet url believe valuable resources future research conduct human evaluation well test two baseline methods one simple word selection deep learn base searchqa show meaningful gap human machine performances suggest propose dataset could well serve benchmark question answer
prominent applications sentiment analysis countless cover areas market customer service communication conventional bag word approach measure sentiment merely count term frequencies however neglect position term within discourse remedy develop discourse aware method build upon discourse structure document purpose utilize rhetorical structure theory label sub clauses accord hierarchical relationships assign polarity score individual leave learn result rhetorical structure propose tensor base tree structure deep neural network name discourse lstm order process complete discourse tree underlie tensors infer salient passages narrative materials addition suggest two algorithms data augmentation node reorder artificial leaf insertion increase train set reduce overfitting benchmarks demonstrate superior performance approach moreover tensor structure reveal salient text passages thereby provide explanatory insights
recent years see revival interest textual entailment spark emergence powerful deep neural network learners natural language process ii timely development large scale evaluation datasets snli recast natural language inference problem amount detect relation pair statements either contradict entail one another mutually neutral current research natural language inference effectively exclusive english paper propose advance research snli style natural language inference toward multilingual evaluation end provide test data four major languages arabic french spanish russian experiment set baselines systems base cross lingual word embeddings machine translation best system score average accuracy seventy-five focus largely enable research multilingual inference
sentence important semantic units natural language generic distributional representation sentence capture latent semantics beneficial multiple downstream applications observe simple geometry sentence word representations give sentence average one thousand and twenty-three word semeval datasets standard deviation four hundred and eighty-four roughly lie low rank subspace roughly rank four motivate observation represent sentence low rank subspace span word vectors unsupervised representation empirically validate via semantic textual similarity task nineteen different datasets outperform sophisticate neural network model include skip think vectors fifteen average
end end neural machine translation overtake statistical machine translation term translation quality language pair specially large amount parallel data besides palpable improvement neural network provide several new properties single system train translate many languages almost additional cost train time furthermore internal representations learn network serve new semantic representation word sentence unlike standard word embeddings learn essentially bilingual even multilingual context view properties contribution present work two fold first systematically study nmt context vectors ie output encoder power interlingua representation sentence assess quality effectiveness measure similarities across translations well semantically relate semantically unrelated sentence pair second extrinsic evaluation first point identify parallel sentence comparable corpora obtain f1982 data share task use nmt context vectors use context vectors jointly similarity measure f1 reach nine hundred and eighty-nine
paper introduce multi genre natural language inference multinli corpus dataset design use development evaluation machine learn model sentence understand addition one largest corpora available task nli 433k examples corpus improve upon available resources coverage offer data ten distinct genres write speak english make possible evaluate systems nearly full complexity language offer explicit set evaluation cross genre domain adaptation
word embeddings make enormous inroads recent years wide variety text mine applications paper explore word embed base architecture predict relevance role two financial entities within context natural language sentence extend abstract propose pool approach use collection sentence train word embeddings use skip gram word2vec architecture use word embeddings obtain context vectors assign one label base manual annotations train machine learn classifier use label context vectors use train classifier predict contextual role relevance test data approach serve good minimal expertise baseline task simple intuitive use open source modules require little feature craft effort perform well across roles
tree adjoin grammars tag provide ample tool capture syntax many indian languages tamil represent special challenge computational formalisms extensive agglutinative morphology comparatively difficult argument structure model tamil syntax morphology use tag interest problem focus even though tag four decades old since inception research tamil tag show us represent syntax language extent mine semantics dependency resolution sentence order demonstrate phenomenal property need parse tamil language sentence use tag build parse obtain derivation could use resolve dependencies thus prove semantic property use house develop pseudo lexical tag chart parser algorithm give schabes joshi one thousand, nine hundred and eighty-eight generate derivations sentence use statistics rank ambiguous derivations rather use understand mention semantic relation tag tamil shall also present brief parser analysis completeness discussions
neural network model show promise opportunities multi task learn focus learn share layer extract common task invariant feature however exist approach extract share feature prone contaminate task specific feature noise bring task paper propose adversarial multi task learn framework alleviate share private latent feature space interfere conduct extensive experiment sixteen different text classification task demonstrate benefit approach besides show share knowledge learn propose model regard shelf knowledge easily transfer new task datasets sixteen task publicly available urlhttp nlpfudaneducn data
distributional semantic model learn vector representations word contexts occur although choice context often take form slide window direct influence result embeddings exact role model component still fully understand paper present systematic analysis context windows base set four distinct hyper parameters train continuous skip gram model two english language corpora various combinations hyper parameters evaluate lexical similarity analogy task notable experimental result positive impact cross sentential contexts surprisingly good performance right context windows
study problem textual relation embed distant supervision combat wrong label problem distant supervision propose embed textual relations global statistics relations ie co occurrence statistics textual knowledge base relations collect entire corpus approach turn robust train noise introduce distant supervision popular relation extraction dataset show learn textual relation embed use augment exist relation extraction model significantly improve performance remarkably top one thousand relational facts discover best exist model precision improve eight hundred and thirty-nine eight hundred and ninety-three
exist study semantic parse mainly focus domain set formulate cross domain semantic parse domain adaptation problem train semantic parser source domains adapt target domain due diversity logical form different domains problem present unique intrigue challenge convert logical form canonical utterances natural language reduce semantic parse paraphrase develop attentive sequence sequence paraphrase model general flexible adapt different domains discover two problems small micro variance large macro variance pre train word embeddings hinder direct use neural network propose standardization techniques remedy popular overnight dataset contain eight domains show cross domain train standardize pre train word embeddings bring significant improvement
investigate neural techniques end end computational argumentation mine frame token base dependency parse token base sequence tag problem include multi task learn setup contrary model operate argument component level find frame dependency parse lead subpar performance result contrast less complex local tag model base bilstms perform robustly across classification scenarios able catch long range dependencies inherent problem moreover find jointly learn natural subtasks multi task learn setup improve performance
paper address problem predict popularity comment online discussion forum use reinforcement learn particularly address two challenge arise natural language state action space first state representation characterize history comment track discussion particular point augment incorporate global context represent discussions world events available external knowledge source second two stage q learn framework introduce make feasible search combinatorial action space also account redundancy among sub action experiment five reddit communities show two methods improve previous report result task
present swellshark framework build biomedical name entity recognition ner systems quickly without hand label data approach view biomedical resources like lexicons function primitives autogenerating weak supervision use generative model unify denoise supervision construct large scale probabilistically label datasets train high accuracy ner taggers three biomedical ner task swellshark achieve competitive score state art supervise benchmarks use hand label train data drug name extraction task use patient medical record one domain expert use swellshark achieve within fifty-one crowdsourced annotation approach originally utilize twenty team course several weeks twenty-four hours
increase adaptability rnn language model lead improve predictions benefit many applications however current methods take full advantage rnn structure show widely use approach adaptation concatenate context word embed input recurrent layer outperform model low cost improvements adaptation hide output layer feature hash bias term capture context idiosyncrasies experiment language model classification task use three different corpora demonstrate advantage propose techniques
neural machine translation nmt become new approach machine translation generate much fluent result compare statistical machine translation smt however smt usually better nmt translation adequacy therefore promise direction combine advantage nmt smt paper propose neural system combination framework leverage multi source nmt take input output nmt smt systems produce final translation extensive experiment chinese english translation task show model archive significant improvement fifty-three bleu point best single system output thirty-four bleu point state art traditional system combination methods
count base distributional semantic model suffer sparsity due unobserved plausible co occurrences text collection problem amplify model like anchor pack tree apts take grammatical type co occurrence account therefore introduce novel form distributional inference exploit rich type structure apts infer miss data mechanism use semantic composition
lexical feature major source information state art coreference resolvers lexical feature implicitly model linguistic phenomena fine granularity level especially useful represent context mention paper investigate drawback use many lexical feature state art coreference resolvers show coreference resolvers mainly rely lexical feature hardly generalize unseen domains furthermore show current coreference resolution evaluation clearly flaw evaluate specific split specific dataset notable overlap train development test set
sarcasm form speech speakers say opposite truly mean order convey strong sentiment word sarcasm giant chasm say person get paper present novel task sarcasm interpretation define generation non sarcastic utterance convey message original sarcastic one introduce novel dataset three thousand sarcastic tweet interpret five human judge address task monolingual machine translation mt experiment mt algorithms evaluation measure present sign mt base sarcasm interpretation algorithm target sentiment word define element textual sarcasm show score n gram base automatic measure similar interpretation model sign interpretations score higher humans adequacy sentiment polarity conclude discussion future research directions new task
present approach automatically classify clinical text sentence level use deep convolutional neural network represent complex feature train network dataset provide broad categorization health information detail evaluation demonstrate method outperform several approach widely use natural language process task fifteen
human verbal communication include affective message convey use emotionally color word lot research direction problem integrate state art neural language model affective information remain area ripe exploration paper propose extension lstm long short term memory language model generate conversational text condition affect categories propose model affect lm enable us customize degree emotional content generate sentence additional design parameter perception study conduct use amazon mechanical turk show affect lm generate naturally look emotional sentence without sacrifice grammatical correctness affect lm also learn affect discriminative word representations perplexity experiment show additional affective information conversational text improve language model prediction
present deep neural architecture parse sentence three semantic dependency graph formalisms use efficient nearly arc factor inference bidirectional lstm compose multi layer perceptron base system able significantly improve state art semantic dependency parse without use hand engineer feature syntax explore two multitask learn approach one share parameters across formalisms one use higher order structure predict graph jointly find approach improve performance across formalisms average achieve new state art code open source available https githubcom noahs ark neurboparser
propose novel factor graph model argument mine design settings argumentative relations document necessarily form tree structure case twenty web comment dataset release model jointly learn elementary unit type classification argumentative relation prediction moreover model support svm rnn parametrizations enforce structure constraints eg transitivity express dependencies adjacent relations proposition approach outperform unstructured baselines web comment argumentative essay datasets
keyphrase provide highly condense information effectively use understand organize retrieve text content though previous study provide many workable solutions automate keyphrase extraction commonly divide summarize content multiple text chunk rank select meaningful ones approach could neither identify keyphrases appear text capture real semantic mean behind text propose generative model keyphrase prediction encoder decoder framework effectively overcome drawbacks name deep keyphrase generation since attempt capture deep semantic mean content deep learn method empirical analysis six datasets demonstrate propose model achieve significant performance boost extract keyphrases appear source text also generate absent keyphrases base semantic mean text code dataset available https githubcom memray opennmt kpg release
paper propose new method calculate output layer neural machine translation systems method base predict binary code word reduce computation time memory requirements output layer logarithmic vocabulary size best case addition also introduce two advance approach improve robustness propose model use error correct cod combine softmax binary cod experiment two english japanese bidirectional translation task show propose model achieve bleu score approach softmax reduce memory usage order less one ten improve decode speed cpus x5 x10
propose new ccg parse model probability tree decompose factor ccg categories syntactic dependencies define bi directional lstms factor model allow precomputation probabilities run efficiently model sentence structure explicitly via dependencies model achieve state art result english japanese ccg parse
fix vocabulary language model fail account one characteristic statistical facts natural language frequent creation reuse new word type although character level language model offer partial solution create word type attest train corpus capture bursty distribution word paper augment hierarchical lstm language model generate sequence word tokens character character cache mechanism learn reuse previously generate word validate model construct new open vocabulary language model corpus multilingual wikipedia corpus mwc comparable wikipedia article seven typologically diverse languages demonstrate effectiveness model across range languages
neural model minimal feature engineer achieve competitive performance traditional methods task chinese word segmentation however train work procedures current neural model computationally inefficient paper present greedy neural word segmenter balance word character embed input alleviate exist drawbacks segmenter truly end end capable perform segmentation much faster even accurate state art neural model chinese benchmark datasets
propose selective encode model extend sequence sequence framework abstractive sentence summarization consist sentence encoder selective gate network attention equip decoder sentence encoder decoder build recurrent neural network selective gate network construct second level sentence representation control information flow encoder decoder second level representation tailor sentence summarization task lead better performance evaluate model english gigaword duc two thousand and four msr abstractive sentence summarization datasets experimental result show propose selective encode model outperform state art baseline model
parse sentence linguistically expressive semantic representations key goal natural language process yet statistical parse focus almost exclusively bilexical dependencies domain specific logical form propose neural encoder decoder transition base parser first full coverage semantic graph parser minimal recursion semantics mrs model architecture use stack base embed feature predict graph jointly unlexicalized predicate token alignments parser accurate attention base baselines mrs additional abstract mean representation amr benchmark gpu batch process make order magnitude faster high precision grammar base parser eight thousand, six hundred and sixty-nine smatch score mrs parser higher upper bind amr parse make mrs attractive choice semantic representation
study symmetric collaborative dialogue set two agents private knowledge must strategically communicate achieve common goal open end dialogue state set pose new challenge exist dialogue systems collect dataset 11k human human dialogues exhibit interest lexical semantic strategic elements model structure knowledge unstructured language propose neural model dynamic knowledge graph embeddings evolve dialogue progress automatic human evaluations show model effective achieve goal human like baseline neural rule base model
present grid beam search gbs algorithm extend beam search allow inclusion pre specify lexical constraints algorithm use model generate sequence mathbfhaty y0ldots yt maximize pmathbfy mathbfx prodlimitstpyt mathbfx y0 ldots yt one lexical constraints take form phrase word must present output sequence general way incorporate additional knowledge model output without require modification model parameters train data demonstrate feasibility flexibility lexically constrain decode conduct experiment neural interactive predictive translation well domain adaptation neural machine translation experiment show gbs provide large improvements translation quality interactive scenarios even without user input gbs use achieve significant gain performance domain adaptation scenarios
translation play important role trade law commerce politics literature thousands years translators always try invisible ideal translations look write originally target language show trace source language remain translation product extent possible uncover history source language look translation specifically automatically reconstruct phylogenetic language tree monolingual texts translate several source languages signal source language powerful retain even two phase translation strongly indicate source language interference dominant characteristic translate texts overshadow subtle signal universal properties translation
paper present new graph base approach induce synsets use synonymy dictionaries word embeddings first build weight graph synonyms extract commonly available resources wiktionary second apply word sense induction deal ambiguous word finally cluster disambiguate version ambiguous input graph synsets meta cluster approach let us us use efficient hard cluster algorithm perform fuzzy cluster graph despite simplicity approach show excellent result outperform five competitive state art methods term f score three gold standard datasets english russian derive large scale manually construct lexical resources
argument mine become popular research area nlp typically include identification argumentative components eg claim central component argument perform qualitative analysis across six different datasets show appear conceptualize claim quite differently learn consequences different conceptualizations claim practical applications carry extensive experiment use state art feature rich deep learn systems identify claim cross domain fashion divergent perception claim different datasets indeed harmful cross domain classification show share properties lexical level well system configurations help overcome gap
paper introduce trie structure bayesian model unsupervised morphological segmentation adopt prior information different source model use neural word embeddings discover word morphologically derive thereby semantically similar use letter successor variety count obtain try build neural word embeddings result show use different information source neural word embeddings letter successor variety prior information improve morphological segmentation bayesian model model outperform unsupervised morphological segmentation model turkish give promise result english german scarce resources
fundamental question language learn concern role speaker first language second language acquisition present novel methodology study question analysis eye movement pattern second language read free form text use methodology demonstrate first time native language english learners predict gaze fixations read english provide analysis classifier uncertainty learn feature indicate differences english read likely root linguistic divergences across native languages present framework complement production study offer new grind advance research multilingualism
answer question machine comprehension mc task model need establish interaction question context tackle problem single pass model reflect correct answer present ruminate reader ruminate reader add second pass attention novel information fusion component bi directional attention flow model bidaf propose novel layer structure construct query aware context vector representation fuse encode representation intermediate representation top bidaf model show multi hop attention mechanism apply bi directional attention structure experiment squad find reader outperform bidaf baseline substantial margin match surpass performance publish systems
wikipedia useful knowledge source benefit many applications language process knowledge representation important feature wikipedia categories wikipedia page assign different categories accord content human annotate label use information retrieval ad hoc search improvements entity rank tag recommendations however important page usually assign many categories make difficult recognize important ones give best descriptions paper propose approach recognize descriptive wikipedia categories observe historical figure precise category presumably mutually similar categorical coherence could evaluate via texts wikipedia link correspond members category rank descriptive level wikipedia categories accord coherence rank yield overall agreement eight thousand, eight hundred and twenty-seven compare human wisdom
neural machine translation represent excite leap forward translation quality longstanding weaknesses resolve remain address question challenge set approach translation evaluation error analysis challenge set consist small set sentence hand design probe system capacity bridge particular structural divergence languages exemplify approach present english french challenge set use analyze phrase base neural systems result analysis provide fine grain picture strengths neural systems also insight linguistic phenomena remain reach
paper present first attempt knowledge classify english write style scale challenge classify day day language write writers different background cover various areas topicsthe paper propose simple machine learn algorithms simple generate feature solve hard problems rely scale data available large source knowledge like wikipedia believe source data crucial generate robust solutions web high accuracy easy deploy practice paper achieve seventy-four accuracy classify native versus non native speakers write style moreover paper show interest observations similarity different languages measure similarity users english write style technique could use show well know facts languages group families experiment support
develop stream one pass bound memory word embed algorithm base canonical skip gram negative sample algorithm implement word2vec compare stream algorithm word2vec empirically measure cosine similarity word pair algorithm apply algorithm downstream task hashtag prediction two month interval twitter sample stream discuss result experiment conclude provide partial validation approach stream replacement word2vec finally discuss potential failure modes suggest directions future work
different linguistic perspectives cause many diverse segmentation criteria chinese word segmentation cws exist methods focus improve performance single criterion however interest exploit different criteria mine common underlie knowledge paper propose adversarial multi criteria learn cws integrate share knowledge multiple heterogeneous segmentation criteria experiment eight corpora heterogeneous segmentation criteria show performance corpus obtain significant improvement compare single criterion learn source cod paper available github
part speech pos tag dependency parse observe closely relate exist work joint model manually craft feature templates suffer feature sparsity incompleteness problems paper propose approach joint pos tag dependency parse use transition base neural network three neural network base classifiers design resolve shift reduce tag label conflict experiment show approach significantly outperform previous methods joint pos tag dependency parse across variety natural languages
compositor attribution cluster page historical print document individual set type bibliographic task rely analysis orthographic variation inspection visual detail print page paper introduce novel unsupervised model jointly describe textual visual feature need distinguish compositors apply image shakespeare first folio model predict attributions agree manual judgements bibliographers accuracy eighty-seven even text output ocr
present paper approach model inter topic preferences twitter users example agree trans pacific partnership tpp also agree free trade kind knowledge useful stance detection across multiple topics also various real world applications include public opinion survey electoral predictions electoral campaign online debate order extract users preferences twitter design linguistic pattern people agree disagree specific topics eg completely wrong apply linguistic pattern collection tweet extract statements agree disagree various topics inspire previous work item recommendation formalize task model inter topic preferences matrix factorization represent users preferences user topic matrix map users topics onto latent feature space abstract preferences experimental result demonstrate propose approach useful predict miss preferences users latent vector representations topics successfully encode inter topic preferences
language model typically apply sentence level without access broader document context present neural language model incorporate document context form topic model like architecture thus provide succinct representation broader document context outside current sentence experiment range datasets demonstrate model outperform pure sentence base model term language model perplexity lead topics potentially coherent produce standard lda topic model model also ability generate relate sentence topic provide another way interpret topics
skip gram negative sample sgns word embed model well know implementation word2vec software usually optimize stochastic gradient descent however optimization sgns objective view problem search good matrix low rank constraint standard way solve type problems apply riemannian optimization framework optimize sgns objective manifold require low rank matrices paper propose algorithm optimize sgns objective use riemannian optimization demonstrate superiority popular competitors original method train sgns svd sppmi matrix
mild cognitive impairment mci mental disorder difficult diagnose linguistic feature mainly parsers use detect mci suitable large scale assessments mci disfluencies produce non grammatical speech require manual high precision automatic correction transcripts paper model transcripts complex network enrich word embed cne better represent short texts produce neuropsychological assessments network measurements apply well know classifiers automatically identify mci transcripts binary classification task comparison make performance traditional approach use bag word bow linguistic feature three datasets dementiabank english cinderella arizona battery portuguese overall cne provide higher accuracy use complex network support vector machine superior classifiers cne provide highest accuracies dementiabank cinderella bow efficient arizona battery dataset probably owe short narratives approach use linguistic feature yield higher accuracy transcriptions cinderella dataset manually revise take together result indicate complex network enrich embed promise detect mci large scale assessments
abstractive summarization aim generate shorter version document cover salient point compact coherent fashion hand query base summarization highlight point relevant context give query encode attend decode paradigm achieve notable success machine translation extractive summarization dialog systems etc suffer drawback generation repeat phrase work propose model query base summarization task base encode attend decode paradigm two key additions query attention model addition document attention model learn focus different portion query different time step instead use static representation query ii new diversity base attention model aim alleviate problem repeat phrase summary order enable test model introduce new query base summarization dataset build debatepedia experiment show two additions propose model clearly outperform vanilla encode attend decode model gain twenty-eight absolute rouge l score
word represent compose representations subword units word segment character character n grams representations effective may capture morphological regularities word systematically compare understand interact different morphological typologies language model task present experiment systematically vary one basic unit representation two composition representations three morphological typology language model result extend previous find character representations effective across typologies find previously unstudied combination character trigram representations compose bi lstms outperform others also find room improvement none character level model match predictive accuracy model access true morphological analyse even learn order magnitude data
sequence sequence model show strong performance across broad range applications however application parse generate text usingabstract mean representation amrhas limit due relatively limit amount label data non sequential nature amr graph present novel train procedure lift limitation use millions unlabeled sentence careful preprocessing amr graph amr parse model achieve competitive result 621smatch current best score report without significant use external semantic resources amr generation model establish new state art performance bleu three hundred and thirty-eight present extensive ablative qualitative analysis include strong evidence sequence base amr model robust order variations graph sequence conversions
exist question answer methods infer answer either knowledge base raw text knowledge base kb methods good answer compositional question performance often affect incompleteness kb au contraire web text contain millions facts absent kb however unstructured form universal schema support reason union structure kbs unstructured text align common embed space paper extend universal schema natural language question answer employ emphmemory network attend large body facts combination text kb model train end end fashion question answer pair evaluation result spade fill blank question answer dataset show exploit universal schema question answer better use either kb text alone model also outperform current state art eighty-five f1 pointsfootnotecode data available urlhttps rajarshdgithubio textkbqa
introduce neural semantic parser convert natural language utterances intermediate representations form predicate argument structure induce transition system subsequently map target domains semantic parser train end end use annotate logical form denotations obtain competitive result various datasets induce predicate argument structure would light type representations useful semantic parse different linguistically motivate ones
paper describe duluth systems participate semeval two thousand and seventeen task seven detection interpretation english pun duluth systems participate three subtasks rely methods include word sense disambiguation measure semantic relatedness
paper describe duluth system participate semeval two thousand and seventeen task six hashtagwars learn sense humor system participate subtasks b use n gram language model rank highly task evaluation paper discuss result system development evaluation stag two post evaluation run
neural machine translation nmt heavily rely attention network produce context vector target word prediction practice find context vectors different target word quite similar one another therefore insufficient discriminatively predict target word reason might context vectors produce vanilla attention network weight sum source representations invariant decoder state paper propose novel gru gate attention model gatt nmt enhance degree discrimination context vectors enable source representations sensitive partial translation generate decoder gatt use gate recurrent unit gru combine two type information treat source annotation vector originally produce bidirectional encoder history state correspond previous decoder state input gru gru combine information form new source annotation vector way obtain translation sensitive source representations fee attention network generate discriminative context vectors propose variant regard source annotation vector current input previous decoder state history experiment nist chinese english translation task show gatt base model achieve significant improvements vanilla attentionbased nmt analyse attention weight context vectors demonstrate effectiveness gatt improve discrimination power representations handle challenge issue translation
paper aim catalyze discussions text feature extraction techniques use neural network architectures research question discuss paper focus state art neural network techniques prove useful tool language process language generation text classification computational linguistics task
present approach rapidly easily build natural language interfaces databases new domains whose performance improve time base user feedback require minimal intervention achieve adapt neural sequence model map utterances directly sql full expressivity bypass intermediate mean representations model immediately deploy online solicit feedback real users flag incorrect query finally popularity sql facilitate gather annotations incorrect predictions use crowd directly use improve model complete feedback loop without intermediate representations database specific engineer open new ways build high quality semantic parsers experiment suggest approach deploy quickly new target domain show learn semantic parser online academic database scratch
propose directly map raw visual observations text input action instruction execution exist approach assume access structure environment representations use pipeline separately train model learn single model jointly reason linguistic visual input use reinforcement learn contextual bandit set train neural network agent guide agent exploration use reward shape different form supervision approach require intermediate representations plan procedures train different model evaluate simulate environment show significant improvements supervise learn common reinforcement learn variants
word often convey affect emotions feel attitudes lexicons word affect association applications automatic emotion analysis natural language generation however exist lexicons indicate coarse categories affect association first time create affect intensity lexicon real value score association use technique call best worst scale improve annotation consistency obtain reliable fine grain score lexicon include term common general english term specific social media communications close six thousand entries four basic emotions add entries affect dimension shortly
discourse annotate corpora important resource community often annotate accord different frameworks make comparison annotations difficult thereby also prevent researchers search corpora unify way use annotate data jointly train computational systems several theoretical proposals recently make map relational label different frameworks proposals far validate exist annotations two largest discourse relation annotate resources penn discourse treebank rhetorical structure theory discourse treebank however annotate text allow direct comparison annotation layer propose method automatically align discourse segment evaluate exist map proposals compare empirically observe propose mappings analysis highlight influence segmentation subsequent discourse relation label show agreement frameworks reasonable explicit relations agreement implicit relations low identify several source systematic discrepancies two annotation scheme discuss consequences discrepancies future annotation train automatic discourse relation labellers
neural word segmentation research benefit large scale raw texts leverage pretraining character word embeddings hand statistical segmentation research exploit richer source external information punctuation automatic segmentation pos investigate effectiveness range external train source neural word segmentation build modular segmentation model pretraining important submodule use rich external source result show pretraining significantly improve model lead accuracies competitive best methods six benchmarks
natural language generation nlg play critical role speak dialogue systems paper present new approach nlg use recurrent neural network rnn gate mechanism apply rnn computation allow propose model generate appropriate sentence rnn base generator learn unaligned data jointly train sentence plan surface realization produce natural language responses model extensively evaluate four different nlg domains result show propose generator achieve better performance nlg domains compare previous generators
natural language generation nlg critical component speak dialogue system paper present recurrent neural network base encoder decoder architecture lstm base decoder introduce select aggregate semantic elements produce attention mechanism input elements produce require utterances propose generator jointly train sentence plan surface realization produce natural language sentence propose model extensively evaluate four different nlg datasets experimental result show propose generators consistently outperform previous methods across nlg domains also show ability generalize new unseen domain learn multi domain datasets
paper describe speech process activities conduct polish consortium clarin project purpose segment project develop specific tool would allow automatic semi automatic process large quantities acoustic speech data tool include follow grapheme phoneme conversion speech text alignment voice activity detection speaker diarization keyword spot automatic speech transcription furthermore order develop tool large high quality studio speech corpus record release open license encourage development area polish speech research another purpose corpus serve reference study phonetics pronunciation tool resources release polish clarin website paper discuss current status future plan project
paper summarize development lvcsr system build part pashto speech translation system scale summer camp apply language exploration two thousand and fifteen workshop speech text translation low resource languages pashto language choose good proxy low resource language exhibit multiple phenomena make speech recognition speech text translation systems development hard even amount data seemingly sufficient give fact data originate multiple source preliminary experiment reveal little benefit merge concatenate corpora elaborate ways make use data must work paper concentrate lvcsr part present range different techniques find useful order benefit multiple different corpora
morphologically rich languages accentuate two properties distributional vector space model one difficulty induce accurate representations low frequency word form two insensitivity distinct lexical relations similar distributional signatures effect detrimental language understand systems may infer inexpensive rephrase expensive may associate acquire acquire work propose novel morph fit procedure move past use curated semantic lexicons improve distributional vector space instead method inject morphological constraints generate use simple language specific rule pull inflectional form word close together push derivational antonyms far apart intrinsic evaluation four languages show approach one improve low frequency word estimate two boost semantic quality entire word vector collection finally show morph fit vectors yield large gain downstream task dialogue state track highlight importance morphology tackle long tail phenomena language understand task
paper present nmtpy flexible python toolkit base theano train neural machine translation neural sequence sequence architectures nmtpy decouple specification network train inference utilities simplify addition new architecture reduce amount boilerplate code write nmtpy use lium top rank submissions wmt multimodal machine translation news translation task two thousand and sixteen two thousand and seventeen
grow digital archive improve algorithms automatic analysis text speech create new research opportunities fundamental research phonetics empirical approach allow statistical evaluation much larger set hypothesis phonetic variation condition factor among geographical dialectal variants paper illustrate vision propose challenge automatic methods analysis easily observable phenomenon vowel length contrast focus wolof resourced language sub saharan africa particular propose multiple feature make fine evaluation degree length contrast different factor read vs semi spontaneous speech standard vs dialectal wolof measure make fully automatically 20k vowel tokens show propose feature highlight different degrees contrast vowel consider notably show contrast weaker semi spontaneous speech non standard semi spontaneous dialect
paper describe function assistant lightweight python base toolkit query explore source code repositories use natural language toolkit design help end users target api quickly find information function high level natural language query descriptions give text query background api tool find candidate function perform translation text know representations api use semantic parse approach richardson kuhn two thousand and seventeen translations automatically learn example text code pair example apis toolkit include feature build translation pipelines query engines arbitrary source code project explore last feature perform new experiment twenty-seven well know python project host github
work present new state art result nine thousand, three hundred and fifty-nine seven thousand, nine hundred and fifty-nine turkish czech name entity recognition base model lample et al two thousand and sixteen contribute propose several scheme represent morphological analysis word context name entity recognition show concatenation representation word character embeddings improve performance effect representation scheme tag performance also investigate
speech emotion recognition important challenge task realm human computer interaction prior work propose variety model feature set train system work conduct extensive experiment use attentive convolutional neural network multi view learn objective function compare system performance use different lengths input signal different type acoustic feature different type emotion speech improvise script experimental result interactive emotional motion capture iemocap database reveal recognition performance strongly depend type speech data independent choice input feature furthermore achieve state art result improvise speech data iemocap
paper demonstrate potential convolutional neural network cnn detect classify prosodic events word specifically pitch accent phrase boundary tone frame base acoustic feature typical approach use feature representations word question also surround context show add position feature indicate current word benefit cnn addition paper discuss generalization speaker dependent model approach speaker independent setup propose method simple efficient yield strong result speaker dependent also speaker independent case
concept definition important language understand lu adaptation since literal definition difference easily lead data sparsity even different data set actually semantically correlate address issue paper novel concept transfer learn approach propose substructures within literal concept definition investigate reveal relationship concepts hierarchical semantic representation concepts propose semantic slot represent composition atomic concepts base new hierarchical representation transfer learn approach develop adaptive lu approach apply two task value set mismatch domain adaptation evaluate two lu benchmarks atis dstc 2and3 thorough empirical study validate efficiency effectiveness propose method particular achieve state art performance f1 score nine thousand, six hundred and eight atis use lexicon feature
paper propose novel framework detect redundancy supervise sentence categorisation unlike traditional singleton neural network model incorporate character aware convolutional neural network char cnn character aware recurrent neural network char rnn form convolutional recurrent neural network crnn model benefit char cnn salient feature select feed integrate char rnn char rnn effectively learn long sequence semantics via sophisticate update mechanism compare framework state art text classification algorithms four popular benchmarking corpus instance model achieve compete precision rate recall ratio f1 score google news data set twenty news group data stream algorithm obtain optimum precision rate recall ratio f1 score brown corpus framework obtain best f1 score almost equivalent precision rate recall ratio top competitor question classification collection crnn produce optimal recall rate f1 score comparable precision rate also analyse three different rnn hide recurrent cells impact performance runtime efficiency observe mgu achieve optimal runtime comparable performance gru lstm tfidf base algorithms experiment word2vec glove sent2vec embeddings report performance differences
automatic abusive language detection difficult important task online social media research explore two step approach perform classification abusive language classify specific type compare one step approach one multi class classification detect sexist racist languages public english twitter corpus twenty thousand tweet type sexism racism approach show promise performance eight hundred and twenty-seven f measure use hybridcnn one step eight hundred and twenty-four f measure use logistic regression two step
generative adversarial network gans show great promise recently image generation train gans language generation prove difficult non differentiable nature generate text recurrent neural network consequently past work either resort pre train maximum likelihood use convolutional network generation work show recurrent neural network train generate text gans scratch use curriculum learn slowly teach model generate sequence increase variable length empirically show approach vastly improve quality generate sequence compare convolutional baseline
advent informal electronic communications social media colloquial languages historically unwritten write first time heavily code switch environments present method induce portion translation lexicons use expert knowledge settings approximately zero resources available language informant potentially even large amount monolingual data investigate induce moroccan darija english translation lexicon via french loanwords bridge english find useful lexicon induce human assist translation statistical machine translation
ever increase size text present internet automatic summary generation remain important problem natural language understand work explore novel full fledge pipeline text summarization intermediate step abstract mean representation amr pipeline propose us first generate amr graph input story extract summary graph finally generate summary sentence summary graph propose method achieve state art result compare text summarization routines base amr also point significant problems exist evaluation methods make unsuitable evaluate summary quality
recently resources task propose go beyond state track dialogue systems example frame track task require record multiple frame one user goal set dialogue allow user instance compare items correspond different goals paper propose model take input list frame create far dialogue current user utterance well dialogue act slot type slot value associate utterance model output frame reference triple dialogue act slot type slot value show recently publish frame dataset model significantly outperform previously propose rule base baseline addition propose extensive analysis frame track task divide sub task assess difficulty respect model
present general purpose tagger base convolutional neural network cnn use compose word vectors encode context information cnn tagger robust across different tag task without task specific tune hyper parameters achieve state art result part speech tag morphological tag supertagging cnn tagger also robust vocabulary problem perform well artificially unnormalized texts
last years recurrent neural network rnns prove effective several nlp task despite great success ability model emphsequence label still limit lead research toward solutions rnns combine model already prove effective domain crfs work propose solution far simpler effective evolution simple jordan rnn label inject input network convert embeddings way word compare rnn variant rnn model elman jordan rnn lstm gru two well know task speak language understand slu thank label embeddings combination hide layer propose variant use parameters elman jordan rnns far fewer lstm gru effective rnns also outperform sophisticate crf model
increasingly cognitive scientists demonstrate interest apply tool deep learn one use deep learn language acquisition useful know linguistic phenomenon learn domain general mean assess whether unsupervised deep learn appropriate first pose smaller question unsupervised neural network apply linguistic rule productively use novel situations draw literature determiner noun productivity train unsupervised autoencoder network measure ability combine nouns determiners simple autoencoder create combinations previously encounter produce degree overlap match adults preliminary work provide conclusive evidence productivity warrant investigation complex model work help lay foundations future collaboration deep learn cognitive science communities
consider problem learn general purpose paraphrastic sentence embeddings set wieting et al 2016b use neural machine translation generate sentential paraphrase via back translation bilingual sentence pair evaluate paraphrase pair ability serve train data learn paraphrastic sentence embeddings find data quality stronger prior work base bitext par manually write english paraphrase pair advantage approach scale generate large train set many languages domains experiment several language pair data source develop variety data filter techniques process explore neural machine translation output differ human write sentence find clear differences length amount repetition use rare word
semantic similarity measure important part natural language process task however semantic similarity measure build general use perform well within specific domains therefore study introduce domain specific semantic similarity measure create synergistic union word2vec word embed method use semantic similarity calculation lexicon base lexical semantic similarity methods prove propose methodology perform word embed methods train generic corpus methods train domain specific corpus use lexical semantic similarity methods augment result prove text lemmatization improve performance word embed methods
study problem joint question answer qa question generation qg paper intuition qa qg intrinsic connections two task could improve one side qa model judge whether generate question qg model relevant answer side qg model provide probability generate question give answer useful evidence turn facilitate qa paper regard qa qg dual task propose train framework train model qa qg simultaneously explicitly leverage probabilistic correlation guide train process model implement qg model base sequence sequence learn qa model base recurrent neural network components qa qg model differentiable parameters involve two model could conventionally learn back propagation conduct experiment three datasets empirical result show train framework improve qa qg task improve qa model perform comparably strong baseline approach three datasets
macquarie university contribution bioasq challenge task 5b phase b focus use query base extractive summarisation techniques generation ideal answer four run submit approach range trivial system select first n snippets use deep learn approach regression framework experiment rouge result five test batch bioasq indicate surprisingly good result trivial approach overall run first three test batch achieve best rouge su4 result challenge
analogy completion popular task recent years evaluate semantic properties word embeddings standard methodology make number assumptions analogies always hold either recent benchmark datasets expand domains analysis analogies biomedical domain identify three assumptions single answer give analogy pair involve describe relationship pair informative respect propose modify standard methodology relax assumptions allow multiple correct answer report map mrr addition accuracy use multiple example pair present bmass novel dataset evaluate linguistic regularities biomedical embeddings demonstrate relationships describe dataset pose significant semantic challenge current word embed methods
understand connections unstructured text semi structure table important yet neglect problem natural language process work focus content base table retrieval give query task find relevant table collection table progress towards improve area require powerful model semantic match richer train evaluation resources remedy present rank base approach implement carefully design feature neural network architectures measure relevance query content table furthermore release open domain dataset include twenty-one thousand, one hundred and thirteen web query two hundred and seventy-three thousand, eight hundred and sixteen table conduct comprehensive experiment real world synthetic datasets result verify effectiveness approach present challenge task
current chinese social media text summarization model base encoder decoder framework although generate summaries similar source texts literally low semantic relevance work goal improve semantic relevance source texts summaries chinese social media summarization introduce semantic relevance base neural model encourage high semantic similarity texts summaries model source text represent gate attention encoder summary representation produce decoder besides similarity score representations maximize train experiment show propose model outperform baseline systems social media corpus
present deterministic algorithm russian inflection algorithm implement publicly available web service wwwpassareru provide function inflection single word word match synthesis grammatically correct russian text inflectional function test annotate corpus russian language opencorpora
present state art end end automatic speech recognition asr model learn listen write character joint connectionist temporal classification ctc attention base encoder decoder network encoder deep convolutional neural network cnn base vgg network ctc network sit top encoder jointly train attention base decoder beam search process combine ctc predictions attention base decoder predictions separately train lstm language model achieve five ten error reduction compare prior systems spontaneous japanese chinese speech end end model beat traditional hybrid asr systems
present model embed word context surround word model refer token embeddings represent characteristics word specific give context word sense syntactic category semantic role explore simple efficient token embed model base standard neural network architectures learn token embeddings large amount unannotated text evaluate feature part speech taggers dependency parsers train much smaller amount annotate data find predictors endow token embeddings consistently outperform baseline predictors across range context window train set size
endow chatbot personality identity quite challenge critical deliver realistic natural conversations paper address issue generate responses coherent pre specify agent profile design model consist three modules profile detector decide whether post respond use profile key address bidirectional decoder generate responses forward backward start select profile value position detector predict word position decode start give select profile value show general conversation data social media use generate profile coherent responses manual automatic evaluation show model deliver coherent natural diversify responses
paper give overview share task ccf conference natural language process chinese compute nlpcc two thousand and seventeen chinese news headline categorization dataset share task consist eighteen class twelve thousand short texts along correspond label class dataset example code access https githubcom fudannlp nlpcc2017newsheadlinecategorization
select representative vector set vectors common requirement many algorithmic task traditionally mean median vector select ontology class set homogeneous instance object convert vector space word vector embeddings study propose methodology derive representative vector ontology class whose instance convert vector space start derive five candidate vectors use train machine learn model would calculate representative vector class show methodology perform traditional mean median vector representations
skip think model prove effective learn sentence representations capture sentence semantics paper propose suite techniques trim improve first validate hypothesis give current sentence infer previous infer next sentence provide similar supervision power therefore one decoder predict next sentence preserve trim skip think model second present connection layer encoder decoder help model generalize better semantic relatedness task third find good word embed initialization also essential learn better sentence representations train model unsupervised large corpus contiguous sentence evaluate train model seven supervise task include semantic relatedness paraphrase detection text classification benchmarks empirically show propose model faster lighter weight equally powerful alternative original skip think model
bloom taxonomy bt use classify objectives learn outcome divide learn three different domains cognitive domain effective domain psychomotor domain paper introduce new approach classify question learn outcome statements los bloom taxonomy bt verify bt verb list cite use academicians write question los experiment design investigate semantic relationship action verbs use question los obtain accurate classification level bt sample seven hundred and seventy-five different action verbs collect different universities allow us measure accurate clear cut cognitive level action verb worth mention natural language process techniques use develop rule induce question chunk order extract action verbs propose solution able classify action verb precise level cognitive domain side test evaluate propose solution use confusion matrix result evaluation test yield ninety-seven macro average precision ninety f1 thus outcome research suggest crucial analyse verify action verbs cite use academicians write los classify question base bloom taxonomy order obtain definite accurate classification
earlier work show articulation rate swedish child direct speech cds increase function age child even utterance length differences articulation rate subject control paper show utterance level spontaneous swedish speech youngest children articulation rate cds lower adult direct speech ads ii significant negative correlation articulation rate surprisal negative log probability ads iii increase articulation rate swedish cds function age child hold even surprisal along utterance length differences articulation rate speakers control result indicate adults adjust articulation rate make fit linguistic capacity child
automate essay score aes quite popular widely use however lack appropriate methodology rat nonnative english speakers essay mean lopsided advancement field paper report initial result experiment nonnative aes learn manual evaluation nonnative essay purpose conduct exercise essay write nonnative english speakers test environment rat manually automate system design experiment process experiment feature learn nuances link nonnative evaluation propose methodology automate essay evaluation yield correlation coefficient seven hundred and fifty manual evaluation
restrict non monotonicity show beneficial projective arc eager dependency parser previous research posterior decisions repair mistake make previous state due lack information paper propose novel fully non monotonic transition system base non projective covington algorithm non monotonic system require exploration erroneous action train process develop several non monotonic variants recently define dynamic oracle covington parser base tight approximations loss experiment datasets conll x conll xi share task show non monotonic dynamic oracle outperform monotonic version majority languages
understand social context interaction affect dialog behavior great interest social scientists study human behavior well computer scientists build automatic methods infer social contexts paper study interaction power gender dialog behavior organizational interactions order perform study first construct gender identify enron corpus email semi automatically assign gender around twenty-three thousand individuals author around ninety-seven thousand email message enron corpus corpus make freely available order magnitude larger previously exist gender identify corpora email domain next use corpus perform large scale data orient study interplay gender manifestations power argue addition one gender gender environment interaction ie gender makeup one interlocutors also affect way power manifest dialog focus especially manifestations power dialog structure shallow sense disregard textual content message eg often participants contribute often get reply etc well structure express within textual content eg issue request make whose request get responses etc find gender gender environment affect ways power manifest dialog result pattern reveal underlie factor finally show utility gender information problem automatically predict direction power pair participants email interactions
paper describe stockholm university university groningen su rug system sigmorphon two thousand and seventeen share task morphological inflection system base attentional sequence sequence neural network model use long short term memory lstm cells joint train morphological inflection inverse transformation ie lemmatization morphological analysis system outperform baseline large margin submission rank 4th best team track participate task one high resource
present framework implementation rely natural language process methods aim identification exercise item candidates corpora hybrid system combine heuristics machine learn methods include number relevant selection criteria focus two fundamental aspects linguistic complexity dependence extract sentence original context previous work exercise generation address two criteria limit extent refine overall candidate sentence selection framework appear also lack addition detail description system present result empirical evaluation conduct language teachers learners indicate usefulness system educational purpose integrate system freely available online learn platform
recent work explore syntactic abilities rnns use subject verb agreement task diagnose sensitivity sentence structure rnns perform task well common case falter complex sentence linzen et al two thousand and sixteen test whether errors due inherent limitations architecture relatively indirect supervision provide agreement dependencies corpus train single rnn perform agreement task additional task either ccg supertagging language model multi task train lead significantly lower error rat particular complex sentence suggest rnns ability evolve sophisticate syntactic representations show also show easily available agreement train data improve performance syntactic task particular limit amount train data available task multi task paradigm also leverage inject grammatical knowledge language model
speech recognition systems irregularly spell languages like english normally require hand write pronunciations paper describe system automatically obtain pronunciations word pronunciations available transcribe data exist method integrate information letter sequence acoustic evidence novel aspect problem address problem prune entries lexicon since empirically lexicons many entries tend good asr performance experiment various asr task show propose framework start initial lexicon several thousand word able learn lexicon perform close full expert lexicon term wer performance test data better lexicons build use g2p alone prune criterion base pronunciation probability
learn commonsense knowledge natural language text nontrivial due report bias people rarely state obvious eg house bigger however rarely state explicitly trivial everyday knowledge influence way people talk world provide indirect clue reason world example statement like tyler enter house imply house bigger tyler paper present approach infer relative physical knowledge action object along five dimension eg size weight strength unstructured natural language text frame knowledge acquisition joint inference two closely relate problems learn one relative physical knowledge object pair two physical implications action apply object pair empirical result demonstrate possible extract knowledge action object language joint inference different type knowledge improve performance
query example search often use dynamic time warp dtw compare query propose match segment recent work show compare speech segment represent fix dimensional vectors acoustic word embeddings measure vector distance eg cosine distance discriminate word accurately dtw base approach consider approach query example search embed query database segment accord neural model follow nearest neighbor search find match segment earlier work embed base query example use template base acoustic word embeddings achieve competitive performance find embeddings base recurrent neural network train optimize word discrimination achieve substantial improvements performance run time efficiency previous approach
neural machine translation nmt model usually use large target vocabulary size capture word target language vocabulary size big factor decode new sentence final softmax layer normalize possible target word address problem widely common restrict target vocabulary candidate list base source sentence usually candidate list combination external word word aligner phrase table entries frequent word work propose simple yet novel approach learn candidate list directly attention layer nmt train candidate list highly optimize current nmt model need external computation candidate pool show significant decode speedup compare use entire vocabulary without lose translation quality two language pair
explore six challenge neural machine translation domain mismatch amount train data rare word long sentence word alignment beam search show deficiencies improvements quality phrase base statistical machine translation
ability accurately perceive whether speaker ask question make statement crucial successful interaction however learn classify tonal pattern challenge task automatic speech recognition model tonal representation tonal contour characterize significant variation paper provide classification model cypriot greek question statements evaluate two state art network architectures long short term memory lstm network convolutional network convnet convnet outperform lstm classification task exhibit excellent performance ninety-five classification accuracy
work explore multiple neural architectures adapt task automatic post edit machine translation output focus neural end end model combine input mt raw mt output src source language input single neural architecture model mt src rightarrow pe directly apart investigate influence hard attention model seem well suit monolingual task well combinations ideas report result data set provide wmt two thousand and sixteen share task automatic post edit demonstrate dual attention model incorporate available data ape scenario single model improve best share task system publish result share task dual attention model combine hard attention remain competitive despite apply fewer change input
compare three approach statistical machine translation pure phrase base factor phrase base neural perform fine grain manual evaluation via error annotation systems output error type annotation compliant multidimensional quality metrics mqm annotation perform two annotators inter annotator agreement high task result show best perform system neural reduce errors produce worst system phrase base fifty-four
idea density id measure rate ideas elementary predications express utterance text lower id find associate increase risk develop alzheimer disease ad snowdon et al one thousand, nine hundred and ninety-six engelman et al two thousand and ten id use two different versions propositional idea density pid count express ideas apply text semantic idea density sid count pre define information content units naturally applicable normative domains picture description task paper develop depid novel dependency base method compute pid version depid r enable exclude repeat ideas feature characteristic ad speech conduct first comparison automatically extract pid sid diagnostic classification task two different ad datasets cover close topic free recall domains sid perform better normative dataset add pid lead small significant improvement seventeen f score free topic dataset pid perform better sid expect seven hundred and seventy-six vs seven hundred and twenty-three f score add feature derive word embed cluster underlie automatic sid increase result considerably lead f score eight hundred and forty-eight
paper present novel approach machine read comprehension ms marco dataset unlike squad dataset aim answer question exact text span passage ms marco dataset define task answer question multiple passages word answer necessary passages therefore develop extraction synthesis framework synthesize answer extraction result specifically answer extraction model first employ predict important sub span passage evidence answer synthesis model take evidence additional feature along question passage elaborate final answer build answer extraction model state art neural network single passage read comprehension propose additional task passage rank help answer extraction multiple passages answer synthesis model base sequence sequence neural network extract evidence feature experiment show extraction synthesis method outperform state art methods
paper explore information theoretic measure entropy detect metaphoric change transfer ideas hypernym detection research language change also build first diachronic test set german standard metaphoric change annotation model show high performance unsupervised language independent generalizable process semantic change
concern analysis normative texts document base deontic notions obligation permission prohibition goal make query notions verify text satisfy certain properties concern causality action time constraints require take original text build representation model formal language case c diagram formalism present experimental semi automatic aid help bridge gap normative text natural language c diagram representation approach consist use dependency structure obtain state art stanford parser apply rule heuristics order extract relevant components result tabular data structure sentence split suitable field convert c diagram process fully automatic however post edit generally require user apply tool perform experiment document different domains report initial evaluation accuracy feasibility approach
work present novel approach automatic post edit ape word level quality estimation qe use ensembles specialize neural machine translation nmt systems word level feature prove effective qe include input factor expand representation original source machine translation hypothesis use generate automatically post edit hypothesis train suite nmt model use different input representations share output space model ensembled together tune ape qe task thus attempt connect state art approach ape qe within single framework model achieve state art result task difference tune step learn weight component ensemble
word embeddings standard technique induce mean representations word get good representations important take account different sense word paper propose mixture model learn multi sense word embeddings model generalize previous work allow induce different weight different sense word experimental result show model outperform previous model standard evaluation task
topic model jointly learn topics document level topic distribution extrinsic evaluation topic model tend focus exclusively topic level evaluation eg assess coherence topics demonstrate large discrepancies topic document level model quality base model evaluation topic level analysis highly mislead propose method automatically predict topic model quality base analysis document level topic allocations provide empirical evidence robustness
knowledge base completion kbc aim predict miss information knowledge basein paper address knowledge base ookb entity problem kbchow answer query concern test entities observe train time exist embed base kbc model assume test entities available train time make unclear obtain embeddings new entities without costly retrain solve ookb entity problem without retrain use graph neural network graph nns compute embeddings ookb entities exploit limit auxiliary knowledge provide test timethe experimental result show effectiveness propose model ookb settingadditionally standard kbc set ookb entities involve model achieve state art performance wordnet dataset code dataset available https githubcom takuo h gnn ookb
perform conceptual analysis concept philosophers interest form expression concept text direct indirect explicit implicit paper experiment topic base methods automate detection concept expressions order facilitate philosophical conceptual analysis propose six methods base lda evaluate new corpus court decision annotate experts non experts result indicate methods yield important improvements keyword heuristic often use concept detection heuristic many contexts work remain do indicate detect concepts topics serve general purpose method least form concept expression capture use naive keyword approach
train neural machine translation nmt model usually use mini batch efficiency purpose mini batch train process necessary pad shorter sentence mini batch equal length longest sentence therein efficient computation previous work note sort corpus base sentence length make mini batch reduce amount pad increase process speed however despite fact mini batch creation essential step nmt train widely use nmt toolkits implement disparate strategies empirically validate compare work investigate mini batch creation strategies experiment two different datasets result suggest choice mini batch creation strategy large effect nmt train length base sort strategies always work well compare simple shuffle
electronic health record ehrs contain important clinical information patients efficient effective use information could supplement even replace manual chart review mean study improve quality safety healthcare delivery however clinical data form free text require pre process use automate systems common free text data source radiology report typically dictate radiologists explain interpretations seek demonstrate machine learn classification compute tomography ct image report binary outcomes ie positive negative fracture use regular text classification classifiers base topic model topic model provide interpretable theme topic distributions report representation compact commonly use bag word representation process faster raw text subsequent automate process demonstrate new classifiers base topic model representation report aggregate topic classifier atc confidence base topic classifier ctc use single topic determine train dataset base different measure classify report test dataset alternatively similarity base topic classifier stc measure similarity report topic distributions determine predict class propose topic model base classifier systems show competitive exist text classification techniques provide efficient interpretable representation
paper present analysis impact float point number precision reduction quality text classification precision reduction vectors represent data eg tf idf representation case allow decrease compute time memory footprint dedicate hardware platforms impact precision reduction classification quality perform five corpora use four different classifiers also dimensionality reduction take account result indicate precision reduction improve classification accuracy case twenty-five error reduction general reduction sixty-four four bits give best score ensure result worse full float point representation
paper introduce thumt open source toolkit neural machine translation nmt develop natural language process group tsinghua university thumt implement standard attention base encoder decoder framework top theano support three train criteria maximum likelihood estimation minimum risk train semi supervise train feature visualization tool display relevance hide state neural network contextual word help analyze internal work nmt experiment chinese english datasets show thumt use minimum risk train significantly outperform groundhog state art toolkit nmt
work aim develop extractive summarizer multi document set implement rank base sentence selection use continuous vector representations along key phrase furthermore propose model tackle summary coherence increase readability conduct experiment document understand conference duc two thousand and four datasets use rouge toolkit experiment demonstrate methods bring significant improvements state art methods term informativity coherence
address problem cross language adaptation question question similarity reranking community question answer objective port system train one input language another input language give label train data first language unlabeled data second language particular propose use adversarial train neural network learn high level feature discriminative main learn task time invariant across input languages evaluation result show sizable improvements cross language adversarial neural network clann model strong non adversarial system
jatecs open source java library support research automatic text categorization relate problems ordinal regression quantification special interest opinion mine applications cover step experimental activity read corpus evaluation experimental result jatecs focus text main input data provide user many text dedicate tool eg data readers many format include commonly use text corpora lexical resources natural language process tool multi language support methods feature selection weight implementation many machine learn algorithms well wrappers well know external software eg svmlight enable full control code jatecs support expansion abstract interfaces many typical tool procedures use text process task library also provide number template implementations typical experimental setups eg train test k fold validation grid search optimization randomize run enable fast realization experiment connect templates data readers learn algorithms evaluation measure
stance detection classification problem natural language process text target pair class result set favor neither expect similar sentiment analysis problem instead sentiment text author stance express particular target investigate stance detection paper present stance detection tweet data set turkish comprise stance annotations tweet two popular sport club target additionally provide evaluation result svm classifiers target data set classifiers use unigram bigram hashtag feature study significant present one initial stance detection data set propose far first one turkish language best knowledge data set evaluation result correspond svm base approach form plausible baselines comparison future study stance detection
understand speak language highly complex problem decompose several simpler task paper focus speak language understand slu module speak dialog systems responsible extract semantic interpretation user utterance task treat label problem past slu perform wide variety probabilistic model rise neural network last couple years open new interest research directions domain recurrent neural network rnns particular able represent several piece information embeddings also thank recurrent architecture encode embeddings relatively long contexts long contexts general reach model previously use slu paper propose novel rnns architectures slu outperform previous ones start publish idea base block design new deep rnns achieve state art result two widely use corpora slu atis air travel information system english media hotel information reservation france french
recognizer output vote error reduction rover widely use system combination automatic speech recognition asr order select appropriate word insert position output transcriptions rover extensions rely critical information confidence score asr decoder feature information always available highly depend decode process sometimes tend estimate real quality recognize word paper propose novel variant rover take advantage asr quality estimation qe rank transcriptions segment level instead rely confidence score ii feed rover randomly order hypotheses first introduce effective set feature compensate absence asr decoder information apply qe techniques perform accurate hypothesis rank segment level start fusion process evaluation carry two different task respectively combine hypotheses come independent asr systems multi microphone record task assume asr decoder information available propose approach significantly outperform standard rover competitive two strong oracles e xploit prior knowledge real quality hypotheses combine compare standard rover abs olute wer improvements two evaluation scenarios range five seventy-three
end end train neural network promise approach automatic construction dialog systems use human human dialog corpus recently vinyals et al test neural conversation model use opensubtitles lowe et al release ubuntu dialogue corpus research unstructured multi turn dialogue systems furthermore approach extend accomplish task orient dialogs provide information properly natural conversation example ghazvininejad et al propose knowledge ground neural conversation model three research aim combine conversational dialogs task orient knowledge use unstructured data twitter data conversation foursquare data external knowledgehowever task still limit restaurant information service yet test wide variety dialog task addition still unclear create intelligent dialog systems respond like human agent consideration problems propose challenge track 6th dialog system technology challenge dstc6 use human human dialog data mimic human dialog behaviors focus challenge track train end end conversation model human human conversation accomplish end end dialog task various situations assume customer service system play role human agent generate natural informative sentence response user question comment give dialog context
previous neural machine translation model use heuristic search algorithms eg beam search order avoid solve maximum posteriori problem translation sentence test time paper propose gumbel greedy decode train generative network predict translation train model solve problem use gumbel softmax reparameterization make generative network differentiable trainable standard stochastic gradient methods empirically demonstrate propose model effective generate sequence discrete word
recurrent neural network model state art name entity recognition ner present two innovations improve performance model first innovation introduction residual connections stack recurrent neural network model address degradation problem deep neural network second innovation bias decode mechanism allow train system adapt non differentiable externally compute objectives entity base f measure work improve state art result spanish english languages standard train development test split conll two thousand and three share task ner dataset
smooth one technique overcome data sparsity statistical language model although mathematical definition explicit dependency upon specific natural language different natures natural languages result different effect smooth techniques true russian language show whittaker one thousand, nine hundred and ninety-eight paper compare modify kneser ney witten bell smooth techniques statistical language model bahasa indonesia use train set totally 22m word extract indonesian version wikipedia far know largest train set use build statistical language model bahasa indonesia experiment three gram five gram seven gram show modify kneser ney consistently outperform witten bell smooth technique term perplexity value interest note experiment show five gram model modify kneser ney smooth technique outperform seven gram meanwhile witten bell smooth consistently improve increase n gram order
start nmt encoder decoder neu ral network use many nlp problems graph base model transition base model borrow en coder components achieve state art performance dependency parse constituent parse respectively ever work empirically study encoder decoder neural net work transition base parse apply simple encoder decoder end achieve comparable result parser dyer et al two thousand and fifteen standard de pendency parse outperform parser vinyals et al two thousand and fifteen con stituent parse
paper introduce novel deep learn framework include lexicon base approach sentence level prediction sentiment label distribution propose first apply semantic rule use deep convolutional neural network deepcnn character level embeddings order increase information word level embed bidirectional long short term memory network bi lstm produce sentence wide feature representation word level embed evaluate approach three twitter sentiment classification datasets experimental result show model improve classification accuracy sentence level sentiment analysis twitter social network
practice evidence base medicine ebm urge medical practitioners utilise latest research evidence make clinical decisions massive grow volume publish research various medical topics practitioners often find overload information natural language process research recently commence explore techniques perform medical domain specific automate text summarisation ats techniques target towards task condense large medical texts however development effective summarisation techniques task require cross domain knowledge present survey ebm domain specific need ebm automate summarisation techniques apply hitherto envision survey serve first resource development future operational text summarisation techniques ebm
recognize entity synonyms text become crucial task many entity leverage applications however discover entity synonyms domain specific text corpora eg news article scientific paper rather challenge current systems take entity name string input find name synonymous ignore fact often time name string refer multiple entities eg apple could refer apple inc fruit apple moreover exist methods require train data manually create domain experts construct supervise learn systems paper study problem automatic synonym discovery knowledge base identify synonyms knowledge base entities give domain specific corpus manually curated synonyms entity store knowledge base form set name string disambiguate mean also serve distant supervision help determine important feature task propose novel framework call dpe integrate two kinds mutually complement signal synonym discovery ie distributional feature base corpus level statistics textual pattern base local contexts particular dpe jointly optimize two kinds signal conjunction distant supervision mutually enhance train stage inference stage signal utilize discover synonyms give entities experimental result prove effectiveness propose framework
neural machine translation nmt recently become popular field machine translation however nmt suffer problem repeat miss word translation address problem tu et al two thousand and seventeen propose encoder decoder reconstructor framework nmt use back translation method select best forward translation model manner bahdanau et al two thousand and fifteen train bi directional translation model fine tune experiment show offer significant improvement bleu score chinese english translation task confirm implementation also show tendency alleviate problem repeat miss word translation english japanese task addition evaluate effectiveness pre train compare jointly train model forward translation back translation
neural machine translation nmt achieve notable performance recently however approach widely apply translation task chinese uyghur partly due limit parallel data resource large proportion rare word cause agglutinative nature uyghur paper collect two hundred thousand sentence pair show middle scale database attention base nmt perform well chinese uyghur uyghur chinese translation tackle rare word propose novel memory structure assist nmt inference experiment demonstrate memory augment nmt nmt outperform vanilla nmt phrase base statistical machine translation smt interestingly memory structure provide elegant way deal word vocabulary
conll sigmorphon two thousand and seventeen share task supervise morphological generation require systems train test fifty-two typologically diverse languages sub task one submit systems ask predict specific inflect form give lemma sub task two systems give lemma specific inflect form ask complete inflectional paradigm predict remain inflect form sub task include high medium low resource condition sub task one receive twenty-four system submissions sub task two receive three system submissions follow success neural sequence sequence model sigmorphon two thousand and sixteen share task one submissions include neural component result show high performance achieve small train datasets long model appropriate inductive bias make use additional unlabeled data synthetic data however different bias data augmentation result disjoint set inflect form predict correctly suggest room future improvement
address task name entity disambiguation ned noisy text present wikilinksned large scale ned dataset text fragment web significantly noisier challenge exist news base datasets capture limit noisy local context surround mention design neural model train novel method sample informative negative examples also describe new way initialize word entity embeddings significantly improve performance model significantly outperform exist state art methods wikilinksned achieve comparable performance smaller newswire dataset
paper describe e2e data new dataset train end end data drive natural language generation systems restaurant domain ten time bigger exist frequently use datasets area e2e dataset pose new challenge one human reference texts show lexical richness syntactic variation include discourse phenomena two generate set require content selection learn dataset promise natural vary less template like system utterances also establish baseline dataset illustrate difficulties associate data
provide appeal brand name newly launch products newly form company rename exist company highly important play crucial role decide success failure work propose computational method generate appeal brand name base description entities use quantitative score readability pronounceability memorability uniqueness generate name rank order set diverse appeal name recommend user brand name task experimental result show name generate approach appeal name prior approach recruit humans could come
argue currently two major bottleneck commercial use statistical machine learn approach natural language generation nlg lack reliable automatic evaluation metrics nlg b scarcity high quality domain corpora address first problem thoroughly analyse current evaluation metrics motivate need new reliable metric second problem address present novel framework develop evaluate high quality corpus nlg train
neural network acoustic model significantly advance state art speech recognition past years however usually computationally expensive due large number matrix vector multiplications nonlinearity operations neural network model also require significant amount memory inference large model size two reason challenge deploy neural network base speech recognizers resource constrain platforms embed devices paper investigate use binary weight activations computation memory efficient neural network acoustic model compare real value weight matrices binary weight require much fewer bits storage thereby cut memory footprint furthermore binary weight activations matrix vector multiplications turn addition subtraction operations computationally much faster energy efficient hardware platforms paper study applications binary weight activations neural network acoustic model report encourage result wsj ami corpora
present new efficient frame semantic parser label semantic arguments framenet predicate build use extension segmental rnn emphasize recall basic system achieve competitive performance without call syntactic parser introduce method use phrase syntactic annotations penn treebank train multitask objective parse require train test time syntactic scaffold offer cheaper alternative traditional syntactic pipelining achieve state art performance
study different frame annotations complement one another learn continuous lexical semantics learn representations tensorized skip gram model consistently encode syntactic semantic content better multiple ten gain baselines
background previous state art systems drug name recognition dnr clinical concept extraction cce focus combination text feature engineer conventional machine learn algorithms conditional random field support vector machine however develop good feature inherently heavily time consume conversely modern machine learn approach recurrent neural network rnns prove capable automatically learn effective feature either random assignments automate word embeddings objectives create highly accurate dnr cce system avoid conventional time consume feature engineer ii create richer specialize word embeddings use health domain datasets mimic iii iii evaluate systems three contemporary datasets methods two deep learn methods namely bidirectional lstm bidirectional lstm crf evaluate crf model set baseline compare deep learn systems traditional machine learn approach feature use model result obtain best result bidirectional lstm crf model outperform previously propose systems specialize embeddings help cover unusual word ddi drugbank ddi medline two thousand and ten i2b2 va irb revision dataset conclusion present state art system dnr cce automate word embeddings allow us avoid costly feature engineer achieve higher accuracy nevertheless embeddings need retrain datasets adequate domain order adequately cover domain specific vocabulary
unsupervised representation learn tweet important research field help solve several business applications sentiment analysis hashtag prediction paraphrase detection microblog rank good tweet representation learn model must handle idiosyncratic nature tweet pose several challenge short length informal word unusual grammar misspell however lack prior work survey representation learn model focus tweet work organize model base objective function aid understand literature also provide interest future directions believe fruitful advance field build high quality tweet representation learn model
interest neural machine translation grow rapidly effectiveness demonstrate across language data scenarios new research regularly introduce architectural algorithmic improvements lead significant gain vanilla nmt implementations however new techniques rarely evaluate context previously publish techniques specifically widely use state theart production share task systems result often difficult determine whether improvements research carry systems deploy real world use work recommend three specific methods relatively easy implement result much stronger experimental systems beyond report significantly higher bleu score conduct depth analysis improvements originate inherent weaknesses basic nmt model address compare relative gain afford several techniques propose literature start vanilla systems versus stronger baselines show experimental conclusions may change depend baseline choose indicate choose strong baseline crucial report reliable experimental result
present data profile evaluation plan second oriental language recognition olr challenge ap17 olr compare event last year ap16 olr new challenge involve languages focus short utterances data offer speechocean nsfc m2asr project two type baselines construct assist participants one base vector model base various neural network report baseline result evaluate various metrics define ap17 olr evaluation plan demonstrate combine database reasonable data resource multilingual research data free participants kaldi recipes baselines publish online
develop technique transfer learn machine comprehension mc use novel two stage synthesis network synnet give high perform mc model one domain technique aim answer question document another domain use label data question answer pair use propose synnet pretrained model squad dataset challenge newsqa dataset achieve f1 measure four hundred and forty-three single model four hundred and sixty-six ensemble approach performance domain model f1 measure five hundred outperform domain baseline seventy-six without use provide annotations
automate metrics bleu widely use machine translation literature also use recently dialogue community evaluate dialogue response generation however previous work dialogue response generation show metrics correlate strongly human judgment non task orient dialogue set task orient dialogue responses express narrower domains exhibit lower diversity thus reasonable think automate metrics would correlate well human judgment task orient set generation task consist translate dialogue act sentence conduct empirical study confirm whether case find indicate automate metrics stronger correlation human judgments task orient set compare observe non task orient set also observe metrics correlate even better datasets provide multiple grind truth reference sentence addition show currently available corpora task orient language generation solve simple model advocate challenge datasets
paper present approach exploit phrase table generate statistical machine translation order map french discourse connectives discourse relations use approach create concoledisco lexicon french discourse connectives pdtb relations evaluate lexconn concoledisco achieve recall eighty-one average precision sixty-eight concession condition relations
semantic textual similarity sts measure mean similarity sentence applications include machine translation mt summarization generation question answer qa short answer grade semantic search dialog conversational systems sts share task venue assess current state art two thousand and seventeen task focus multilingual cross lingual pair one sub track explore mt quality estimation mtqe data task obtain strong participation thirty-one team seventeen participate language track summarize performance review selection well perform methods analysis highlight common errors provide insight limitations exist model support ongoing work semantic representations sts benchmark introduce new share train evaluation set carefully select corpus english sts share task data two thousand and twelve two thousand and seventeen
propose new share task tactical data text generation domain source code libraries specifically focus text generation function descriptions example software project data draw exist resources use study relate problem semantic parser induction richardson kuhn 2017b richardson kuhn 2017a span wide variety natural languages program languages paper describe exist resources serve train development data task discuss plan build new independent test set
coreference resolution intermediate step text understand use task domains necessarily coreference annotate corpora therefore generalization special importance coreference resolution however recent coreference resolvers notable improvements conll dataset struggle generalize properly new domains datasets paper investigate role linguistic feature build generalizable coreference resolvers show generalization improve slightly merely use set additional linguistic feature however employ feature subsets value informative coreference resolution considerably improve generalization thank better generalization system achieve state art result domain evaluations eg wikicoref system train conll achieve par performance system design dataset
characterize content technical document term learn utility useful applications relate education generate read list large collections document refer learn utility pedagogical value document learner pedagogical value important concept study extensively within education domain little work explore computational ie natural language process nlp perspective allow computational exploration concept introduce notion pedagogical roles document eg tutorial survey intermediary component study pedagogical value give lack available corpora exploration create first annotate corpus pedagogical roles use test baseline techniques automatic prediction roles
paper outline result sentence level linguistics base rule improve part speech tag well know performance complex nlp systems negatively affect one preliminary stag less perfect errors initial stag pipeline snowball effect pipeline end performance create set linguistics base rule sentence level adjust part speech tag state art taggers comparison state art taggers widely use benchmarks demonstrate significant improvements tag accuracy consequently quality accuracy nlp systems
major challenge paraphrase research lack parallel corpora paper present new method collect large scale sentential paraphrase twitter link tweet share urls main advantage method simplicity get rid classifier human loop need select data annotation subsequent application paraphrase identification algorithms previous work present largest human label paraphrase corpus date fifty-one thousand, five hundred and twenty-four sentence pair first cross domain benchmarking automatic paraphrase identification addition show thirty thousand new sentential paraphrase easily continuously capture every month seventy precision demonstrate utility downstream nlp task phrasal paraphrase extraction make code data freely available
generative model define joint distributions parse tree sentence useful parse language model impose restrictions scope feature often outperform discriminative model propose framework parse language model marry generative model discriminative recognition model encoder decoder set provide interpretations framework base expectation maximization variational inference show enable parse language model within single implementation english penn treen bank framework obtain competitive performance constituency parse match state art single model language model score
hand build verb cluster widely use levin class levin one thousand, nine hundred and ninety-three prove useful limit coverage verb class automatically induce corpus data verbkb wijaya two thousand and sixteen hand give cluster much larger coverage adapt specific corpora twitter present method cluster output verbkb verbs multiple argument type eg marryperson person feelperson emotion make use novel low dimensional embed verbs arguments produce high quality cluster verb different cluster depend argument type result verb cluster better job hand build cluster predict sarcasm sentiment locus control tweet
textual information extraction sequence label task common use recurrent neural network lstm form rich embed representations long term input co occurrence pattern representation output co occurrence pattern typically limit hand design graphical model linear chain crf represent short term markov dependencies among successive label paper present method learn embed representations latent output structure sequence data model take form finite state machine large number latent state per label latent variable crf state transition matrix factorize effectively form embed representation state transition capable enforce long term label dependencies support exact viterbi inference output label demonstrate accuracy improvements interpretable latent structure synthetic complex task base conll name entity recognition
paper offer depth analysis model search performance address question complex search algorithm necessary furthermore investigate question complex model might applicable rescoring promise separate search space model use n best list reranking analyze influence part nmt system independently compare differently perform nmt systems show better translation already search space translation systems less performance result indicate current search algorithms sufficient nmt systems furthermore could show even relatively small n best list fifty hypotheses already contain notably better translations
intelligent selection train data prove successful technique simultaneously increase train efficiency translation performance phrase base machine translation pbmt recent increase popularity neural machine translation nmt explore paper extent nmt also benefit data selection state art data selection axelrod et al two thousand and eleven consistently perform well pbmt show gain substantially lower nmt next introduce dynamic data selection nmt method vary select subset train data different train epochs experiment show best result achieve apply technique call gradual fine tune improvements twenty-six bleu original data selection approach thirty-one bleu general baseline
paper describe university edinburgh submissions wmt17 share news translation biomedical translation task participate twelve translation directions news translate english czech german latvian russian turkish chinese biomedical task submit systems english czech german polish romanian systems neural machine translation systems train nematus attentional encoder decoder follow setup last year build bpe base model parallel back translate monolingual train data novelties year include use deep architectures layer normalization compact model due weight tie improvements bpe segmentations perform extensive ablative experiment report effectivenes layer normalization deep architectures different ensembling techniques
unsupervised dependency parse aim learn dependency parser unannotated sentence exist work focus either learn generative model use expectation maximization algorithm variants learn discriminative model use discriminative cluster algorithm paper propose new learn strategy learn generative model discriminative model jointly base dual decomposition method method simple general yet effective capture advantage model improve learn result test method ud treebank achieve state art performance thirty languages
study impact big model term degree lexicalization big data term train corpus size dependency grammar induction experiment l dmv lexicalize version dependency model valence l ndmv lexicalize extension neural dependency model valence find l dmv benefit small degrees lexicalization moderate size train corpora l ndmv benefit big train data lexicalization greater degrees especially enhance good model initialization achieve result competitive current state art
human interactions human computer interactions strongly influence style well content add persona chatbot make human like contribute better engage user experience work propose design chatbot capture style star trek incorporate reference show along peculiar tone fictional character therein enterprise computer bot e2cbot treat star trek dialog style general dialog style differently use two recurrent neural network encoder decoder model star trek dialog style use sequence sequence seq2seq model sutskever et al two thousand and fourteen bahdanau et al two thousand and fourteen train star trek dialogs general dialog style use word graph shift response seq2seq model star trek domain evaluate bot term perplexity word overlap star trek vocabulary subjectively use human evaluators
introduce formal distinction contradictions disagreements natural language texts motivate need formally reason contradictory medical guidelines novel potentially useful distinction discuss far nlp logic also describe nlp system capable automate find contradictory medical guidelines system use combination text analysis information retrieval modules also report positive evaluation result small corpus contradictory medical recommendations
investigate task build domain aware chat system generate intelligent responses conversation comprise different domains domain case topic theme conversation achieve present dom seq2seq domain aware neural network model base novel technique use domain target sequence sequence model sutskever et al two thousand and fourteen domain classifier model capture feature current utterance domains previous utterances facilitate formation relevant responses evaluate model automatic metrics compare performance seq2seq model
linguistic resources part speech pos tag extensively use statistical machine translation smt frameworks yield better performances however usage linguistic annotations neural machine translation nmt systems leave explore work show multi task learn successful easy approach introduce additional knowledge end end neural attentional model jointly train several natural language process nlp task one system able leverage common information improve performance individual task analyze impact three design decisions multi task learn task use train train schedule degree parameter share across task define network architecture experiment conduct german english translation task additional linguistic resources exploit pos information name entities ne experiment show translation quality improve fifteen bleu point low resource condition performance pos tagger also improve use multi task learn scheme
unsupervised dependency parse try discover linguistic dependency structure unannotated data challenge task almost previous work task focus learn generative model paper develop unsupervised dependency parse model base crf autoencoder encoder part model discriminative globally normalize allow us use rich feature well universal linguistic priors propose exact algorithm parse well tractable learn algorithm evaluate performance model eight multilingual treebanks find model achieve comparable performance state art approach
repeval two thousand and seventeen share task aim evaluate natural language understand model sentence representation sentence represent fix length vector neural network quality representation test natural language inference task paper describe system alpha rank among top share task domain test set obtain seven hundred and forty-nine accuracy cross domain test set also attain seven hundred and forty-nine accuracy demonstrate model generalize well cross domain data model equip intra sentence gate attention composition help achieve better performance addition submit model share task also test stanford natural language inference snli dataset obtain accuracy eight hundred and fifty-five best report result snli cross sentence attention allow condition enforce repeval two thousand and seventeen
grapheme phoneme conversion g2p necessary text speech automatic speech recognition systems g2p systems monolingual require language specific data handcraft rule systems difficult extend low resource languages data handcraft rule available alternative present neural sequence sequence approach g2p train spell pronunciation pair hundreds languages system share single encoder decoder across languages allow utilize intrinsic similarities different write systems show eleven improvement phoneme error rate approach base adapt high resource monolingual g2p model low resource languages model also much compact relative previous approach
paper investigate application text classification methods predict law area decision case judge french supreme court also investigate influence time period rule make textual form case description extent necessary mask judge motivation rule emulate real world test scenario report result ninety-six f1 score predict case rule ninety f1 score predict law area case seven hundred and fifty-nine f1 score estimate time span rule issue use linear support vector machine svm classifier train lexical feature
automatic question answer classical problem natural language process aim design systems automatically answer question way human work propose deep learn base model automatic question answer first question answer embed use neural probabilistic model deep similarity neural network train find similarity score pair answer question question best answer find one highest similarity score first train model large scale public question answer database fine tune transfer customer care chat data also test framework public question answer database achieve good performance
traditional automatic evaluation measure natural language generation nlg use costly human author reference estimate quality system output paper propose referenceless quality estimation qe approach base recurrent neural network predict quality score nlg system output compare source mean representation method outperform traditional metrics constant baseline respect also show synthetic data help increase correlation result twenty-one compare base system result comparable result obtain similar qe task despite challenge set
word embed become fundamental component many nlp task name entity recognition machine translation however popular model learn embeddings unaware morphology word directly applicable highly agglutinative languages korean propose syllable base learn model korean use convolutional neural network word representation compose train syllable vectors model successfully produce morphologically meaningful representation korean word compare original skip gram embeddings result also show quite robust vocabulary problem
due large amount textual information available internet paramount relevance use techniques find relevant concise content typical task devote identification informative sentence document call extractive document summarization task paper use complex network concepts devise extractive multi document summarization mds method extract central sentence several textual source propose model texts represent network nod represent sentence edge establish base number share word differently previous work identification relevant term guide characterization nod via dynamical measurements complex network include symmetry accessibility absorption time evaluation propose system reveal excellent result obtain particular dynamical measurements include base exploration network via random walk
encoder decoder architecture neural machine translation nmt hide state recurrent structure encoder decoder carry crucial information sentencethese vectors generate parameters update back propagation translation errors time argue propagate errors end end recurrent structure direct way control hide vectors paper propose use word predictions mechanism direct supervision specifically require vectors able predict vocabulary target sentence simple mechanism ensure better representations encoder decoder without use extra data annotation also helpful reduce target side vocabulary improve decode efficiency experiment chinese english german english machine translation task show bleu improvements four hundred and fifty-three thirteen respectively
compare several language model word order task propose new bag sequence neural model base attention base sequence sequence model evaluate model large german wmt data set significantly outperform exist model also describe novel search strategy lm base word order report result english penn treebank best model setup outperform prior work term speed quality
phrase play important role natural language understand machine translation sag et al two thousand and two villavicencio et al two thousand and five however difficult integrate current neural machine translation nmt read generate sentence word word work propose method translate phrase nmt integrate phrase memory store target phrase phrase base statistical machine translation smt system encoder decoder architecture nmt decode step phrase memory first write smt model dynamically generate relevant target phrase contextual information provide nmt model propose model read phrase memory make probability estimations phrase phrase memory phrase generation carry nmt decoder select appropriate phrase memory perform phrase translation update decode state consume word select phrase otherwise nmt decoder generate word vocabulary general nmt decoder experiment result chinese english translation show propose model achieve significant improvements baseline various test set
neural machine translation nmt achieve notable success recent time however also widely recognize approach limitations handle infrequent word word pair paper present novel memory augment nmt nmt architecture store knowledge word usually infrequently encounter ones translate memory utilize assist neural model use memory mechanism combine knowledge learn conventional statistical machine translation system rule learn nmt system also propose solution vocabulary oov word base framework experiment two chinese english translation task demonstrate nmt architecture outperform nmt baseline ninety twenty-seven bleu point two task respectively additionally find architecture result much effective oov treatment compare competitive methods
transfer knowledge source domain another domain useful especially gather new data expensive time consume deep network well study question answer task recent years however prominent research transfer learn deep neural network exist question answer field paper two main methods init mult field examine new method name intelligent sample selection iss mult propose improve mult method question answer task different datasets specificay squad selqa wikiqa newwikiqa inforboxqa use evaluation moreover two different task question answer answer selection answer trigger evaluate examine effectiveness transfer learn result show use transfer learn generally improve performance corpora relate base policy addition use iss mult could finely improve mult method question answer task improvements prove significant answer trigger task
paper address problem corpus level entity type ie infer large corpus entity member class food artist application entity type interest knowledge base completion specifically learn class entity member propose figment tackle problem figment embed base combine global model score base aggregate contextual information entity ii context model first score individual occurrences entity aggregate score two propose model specific properties global model learn high quality entity representations crucial source use predictions therefore introduce representations use name contexts entities three level entity word character show complementary information multi level representation best context model need use distant supervision since context level label available entities distant supervise label noisy harm performance model therefore introduce apply new algorithms noise mitigation use multi instance learn show effectiveness model large entity type dataset build freebase
video review natural evolution write product review paper target phenomenon introduce first dataset create close caption youtube product review videos well new attention rnn model aspect extraction joint aspect extraction sentiment classification model provide state art performance aspect extraction without require usage hand craft feature semeval absa corpus outperform baseline joint task dataset attention rnn model outperform baseline task observe important performance drop model comparison semeval result well experiment domain adaptation aspect extraction suggest differences speech write text discuss extensively literature also extend domain product review relevant fine grain opinion mine
explore context representation learn methods neural base model dialog act classification propose compare extensively different methods combine recurrent neural network architectures attention mechanisms ams different context level experimental result two benchmark datasets show consistent improvements compare model without contextual information reveal suitable architecture depend nature dataset
deep learn methods employ multiple process layer learn hierarchical representations data produce state art result many domains recently variety model design methods blossom context natural language process nlp paper review significant deep learn relate model methods employ numerous nlp task provide walk evolution also summarize compare contrast various model put forward detail understand past present future deep learn nlp
cl scisumm two thousand and sixteen share task introduce interest problem give document piece text cite identify text span reference piece text share task provide first annotate dataset study problem present analysis continue work improve system performance task demonstrate topic model word embeddings use surpass previously best perform system
extract location name informal unstructured social media data require identification referent boundaries partition compound name variability particularly systematic variability location name carroll one thousand, nine hundred and eighty-three challenge identification task variability anticipate operations within statistical language model case draw gazetteers openstreetmap osm geonames dbpedia permit evaluation observe n gram twitter target text legitimate location name variant location context use n gram statistics location relate dictionaries location name extraction tool lnex handle abbreviations automatically filter augment location name gazetteers handle name contractions auxiliary content help detect boundaries multi word location name thereby delimit texts evaluate approach four thousand, five hundred event specific tweet three target stream compare performance lnex ten state art taggers rely standard semantic syntactic orthographic feature lnex improve average f score thirty-three one hundred and seventy-nine outperform taggers lnex capable stream process
neural network base dialog systems attract increase attention academia industry recently researchers begin realize importance speaker model neural dialog systems lack establish task datasets paper propose speaker classification surrogate task general speaker model collect massive data facilitate research direction investigate temporal base content base model speakers propose several hybrids experiment show speaker classification feasible hybrid model outperform single component
paper discuss different methods use meta information richer context may accompany source language input improve machine translation quality focus category information input text meta information propose methods extend textual non textual meta information might available input text automatically predict use text content main novelty work use state art neural network methods tackle problem within statistical machine translation smt framework observe translation quality improvements three term bleu score text categories
paper introduce hybrid search attention base neural machine translation nmt target phrase learn statistical mt model extend hypothesis nmt beam search attention nmt model focus source word translate phrase phrase add way score nmt model also smt feature include phrase level translation probabilities target language model experimental result german english news domain english russian e commerce domain translation task show use phrase base model nmt search improve mt quality twenty-three bleu absolute compare strong nmt baseline
character vocabulary large non alphabetic languages chinese japanese make neural network model huge process languages explore model sentiment classification take embeddings radicals chinese character ie hanzi chinese kanji japanese model compose cnn word feature encoder bi directional rnn document feature encoder result achieve par character embed base model close state art word embed base model ninety smaller vocabulary least thirteen eighty fewer parameters character embed base model word embed base model respectively result suggest radical embed base approach cost effective machine learn chinese japanese
present simple yet effective approach learn word sense embeddings contrast exist techniques either directly learn sense representations corpora rely sense inventory lexical resources approach induce sense inventory exist word embeddings via cluster ego network relate word integrate wsd mechanism enable label word context learn sense vectors give rise downstream applications experiment show performance method comparable state art unsupervised wsd systems
paper describe submission name clac two thousand and sixteen discriminate similar languages dsl share task participate close sub task one set two separate machine learn techniques first approach character base convolution neural network bidirectional long short term memory bilstm layer clstm achieve accuracy seven thousand, eight hundred and forty-five minimal tune second approach character base n gram model last approach achieve accuracy eight thousand, eight hundred and forty-five close accuracy eight thousand, nine hundred and thirty-eight achieve best submission allow us rank seven overall
argument label explicit discourse relations challenge task state art systems achieve slightly fifty-five f measure require hand craft feature paper propose long short term memory lstm base model argument label experiment multiple configurations model use pdtb dataset best model achieve f1 measure two thousand, three hundred and five without feature engineer significantly higher two thousand and fifty-two achieve state art rnn approach significantly lower feature base state art systems hand approach learn raw dataset widely applicable multiple textual genres languages
lack sufficient label data often limit applicability advance machine learn algorithms real life problems however efficient use transfer learn tl show useful across domains tl utilize valuable knowledge learn one task source task sufficient data available task interest target task biomedical clinical domain quite common lack sufficient train data allow fully exploit machine learn model work present two unify recurrent neural model lead three transfer learn frameworks relation classification task systematically investigate effectiveness propose frameworks transfer knowledge multiple aspects relate source target task similarity relatedness source target task size train data source task empirical result show propose frameworks general improve model performance however improvements depend aspects relate source target task dependence finally determine choice particular tl framework
exist methods biomedical entity recognition task rely explicit feature engineer many feature either specific particular task depend output exist nlp tool neural architectures show across various domains efforts explicit feature design reduce work propose unify framework use bi directional long short term memory network blstm name entity recognition ner task biomedical clinical domains three important characteristics framework follow one model learn contextual well morphological feature use two different blstm hierarchy two model use first order linear conditional random field crf output layer cascade blstm infer label tag sequence three model use domain specific feature dictionary ie another word set feature use three ner task namely disease name recognition disease ner drug name recognition drug ner clinical entity recognition clinical ner compare performance propose model exist state art model standard benchmark datasets three task show empirically propose framework outperform exist model analysis crf layer word embed obtain use character base embed show importance
comprehend lyric find songs poems pose challenge human machine readers alike motivate need systems understand ambiguity jargon find creative texts provide commentary aid readers reach correct interpretation introduce task automate lyric annotation ala like text simplification goal ala rephrase original text easily understandable manner however ala system must often include additional information clarify niche terminology abstract concepts stimulate research task release large collection crowdsourced annotations song lyric analyze performance translation retrieval model task measure performance automate human evaluation find model capture unique type information important task
automatic identification discourse relations still challenge task natural language process discourse connectives since informative cue identify explicit relations however discourse parsers typically use close inventory connectives result discourse relations signal markers outside inventory ie altlexes detect effectively paper propose novel method leverage parallel corpora text simplification lexical resources automatically identify alternative lexicalizations signal discourse relation apply simple wikipedia newsela corpora along wordnet ppdb method allow automatic discovery ninety-one altlexes
word embeddings become basic build block several natural language process information retrieval task pre train word embeddings use several downstream applications well construct representations sentence paragraph document recently emphasis improve pre train word vectors post process algorithms one area improvement dimensionality reduction word embeddings reduce size word embeddings dimensionality reduction improve utility memory constrain devices benefit several real world applications work present novel algorithm effectively combine pca base dimensionality reduction recently propose post process algorithm construct word embeddings lower dimension empirical evaluations twelve standard word similarity benchmarks show algorithm reduce embed dimensionality fifty achieve similar often better performance higher dimension embeddings
paper examine task detect intensity emotion text create first datasets tweet annotate anger fear joy sadness intensities use technique call best worst scale bws improve annotation consistency obtain reliable fine grain score show emotion word hashtags often impact emotion intensity usually convey intense emotion finally create benchmark regression system conduct experiment determine feature useful detect emotion intensity extent two emotions similar term manifest language
experiment dataset approximately 16m user comment greek news sport portal explore state art rnn base moderation method improve add user embeddings user type embeddings user bias user type bias observe improvements case user embeddings lead biggest performance gain
present first share task detect intensity emotion felt speaker tweet create first datasets tweet annotate anger fear joy sadness intensities use technique call best worst scale bws show annotations lead reliable fine grain intensity score rank tweet intensity data partition train development test set competition twenty two team participate share task best system obtain pearson correlation seven hundred and forty-seven gold intensity score summarize machine learn setups resources tool use participate team focus techniques resources particularly useful task emotion intensity dataset share task help improve understand convey less intense emotions language
past work relation extraction focus binary relations single sentence recent nlp inroads high value domains spark interest general set extract n ary relations span multiple sentence paper explore general relation extraction framework base graph long short term memory network graph lstms easily extend cross sentence n ary relation extraction graph formulation provide unify way explore different lstm approach incorporate various intra sentential inter sentential dependencies sequential syntactic discourse relations robust contextual representation learn entities serve input relation classifier simplify handle relations arbitrary arity enable multi task learn relate relations evaluate framework two important precision medicine settings demonstrate effectiveness conventional supervise learn distant supervision cross sentence extraction produce larger knowledge base multi task learn significantly improve extraction accuracy thorough analysis various lstm approach yield useful insight impact linguistic analysis extraction accuracy
one challenge speech emotion recognition ser wild large mismatch train test data eg speakers task order improve generalisation capabilities emotion model propose use multi task learn mtl use gender naturalness auxiliary task deep neural network method evaluate within corpus various cross corpus classification experiment simulate condition wild comparison single task learn stl base state art methods find mtl method propose improve performance significantly particularly model use gender naturalness achieve gain use either gender naturalness separately benefit also find high level representations feature space obtain method propose discriminative emotional cluster could observe
significant advance detect emotions speech image recognition emotion detection text still explore remain active research field paper introduce corpus text base emotion detection multiparty dialogue well deep neural model outperform exist approach document classification first present new corpus provide annotation seven emotions consecutive utterances dialogues extract show friends suggest four type sequence base convolutional neural network model attention leverage sequence information encapsulate dialogue best model show accuracies three hundred and seventy-nine fifty-four fine coarse grain emotions respectively give difficulty task promise
image caption far explore mostly english available datasets language however application image caption restrict language study conduct image caption cross lingual set different work manually build dataset target language aim learn cross lingual caption model fully machine translate sentence conquer lack fluency translate sentence propose paper fluency guide learn framework framework comprise module automatically estimate fluency sentence another module utilize estimate fluency score effectively train image caption model target language experiment two bilingual english chinese datasets show approach improve fluency relevance generate caption chinese without use manually write sentence target language
connectionist temporal classification recently attract lot interest offer elegant approach build acoustic model ams speech recognition ctc loss function map input sequence observable feature vectors output sequence symbols output symbols conditionally independent ctc loss language model lm incorporate conveniently decode retain traditional separation acoustic linguistic components asr fix vocabularies weight finite state transducers provide strong baseline efficient integration ctc ams n gram lms character base neural lms provide straight forward solution open vocabulary speech recognition neural model decode beam search finally sequence sequence model use translate sequence individual sound word string compare performance three approach analyze error pattern provide insightful guidance future research development important area
paper present work case study statistical machien transaltion smt rule base machine translation rbmt systems english indian langugae indian indian langugae perspective main objective study make five way performance compariosn smt rbmt b smt english indian langugae c rbmt english indian langugae smt indian indian langugae perspective e rbmt indian indian langugae perspective detail analysis describe rule base statistical machine translation system developments evaluations detail error analysis point relative strengths weaknesses systems observations base study smt systems outperform rbmt b case smt english indian language mt systmes perform better indian english langugae mt systems c case rbmt english indian langugae mt systems perofrms better indian englsih language mt systems smt systems perform better indian indian language mt systems compare rbmt effectively shall see even small amount train corpus statistical machine translation system many advantage high quality domain specific machine translation rule base counterpart
paper motivate automation neuropsychological test involve discourse analysis retell narratives patients potential cognitive impairment scenario task sentence boundary detection speech transcripts important discourse analysis involve application natural language process tool taggers parsers depend sentence process unit aim paper verify embed induction method work best sentence boundary detection task specifically whether propose capture semantic syntactic morphological similarities
paper propose new methods learn chinese word representations chinese character compose graphical components carry rich semantics common chinese learner comprehend mean word graphical components result propose model enhance word representations character glyphs character glyph feature directly learn bitmaps character convolutional auto encoderconvae glyph feature improve chinese word representations already enhance character embeddings another contribution paper create several evaluation datasets traditional chinese make public
dialog act identification play important role understand conversations widely apply many field dialogue systems automatic machine translation automatic speech recognition especially useful systems human computer natural language dialogue interfaces virtual assistants chatbots first step identify dialog act identify boundary dialog act utterances paper focus segment utterance accord dialog act boundaries ie functional segment identification vietnamese utterances investigate carefully functional segment identification two approach one machine learn approach use maximum entropy conditional random field crfs two deep learn approach use bidirectional long short term memory lstm crf layer bi lstm crf two different conversational datasets one facebook message message data two transcription phone conversations phone data best knowledge first work apply deep learn base approach dialog act segmentation result show deep learn approach perform appreciably better compare traditional machine learn approach moreover also first study tackle dialog act functional segment identification vietnamese
natural language process nlp recently gain much attention represent analyse human language computationally spread applications various field machine translation email spam detection information extraction summarization medical question answer etc paper distinguish four phase discuss different level nlp components natural language generation nlg follow present history evolution nlp state art present various applications nlp current trend challenge
lexicon base methods use syntactic rule polarity classification rely parsers dependent language treebank guidelines thus rule also dependent require adaptation especially multilingual scenarios tackle challenge context iberian peninsula release first symbolic syntax base iberian system rule share across five official languages basque catalan galician portuguese spanish model make available
stance classification determine attitude stance typically short text task powerful applications detection fake news automatic extraction attitudes toward entities events media paper describe surprisingly simple efficient classification approach open stance classification twitter rumour veracity classification approach profit novel set automatically identifiable problem specific feature significantly boost classifier accuracy achieve state art result recent benchmark datasets call question value use complex sophisticate model stance classification without first inform feature extraction
create release first publicly available commercial customer service corpus annotate relational segment human computer data three live customer service intelligent virtual agents ivas domains travel telecommunications collect reviewers mark text deem unnecessary determination user intention merge selections multiple reviewers create highlight texts second round annotation do determine class language present highlight section presence greet backstory justification gratitude rant emotions result corpus valuable resource improve quality relational abilities ivas well discuss corpus compare usage language human human interactions tripadvisor forums show removal language task base input positive effect iva understand increase confidence improvement responses demonstrate need automate methods discovery
high accuracy speech recognition require large amount transcribe data supervise train absence data domain adaptation well train acoustic model perform even high accuracy usually require significant label data target domain work propose approach domain adaptation require transcriptions instead use corpus unlabeled parallel data consist pair sample source domain well train model desire target domain perform adaptation employ teacher student learn posterior probabilities generate source domain model use lieu label train target domain model evaluate propose approach two scenarios adapt clean acoustic model noisy speech adapt adults speech acoustic model children speech significant improvements accuracy obtain reductions word error rate forty-four original source model without need transcribe data target domain moreover show increase amount unlabeled data result additional model robustness particularly beneficial use simulate train data target domain
emotion extraction aim identify reason behind certain emotion express text much difficult task compare emotion classification inspire recent advance use deep memory network question answer qa propose new approach consider emotion identification read comprehension task qa inspire convolutional neural network propose new mechanism store relevant context different memory slot model context information propose approach extract word level sequence feature lexical feature performance evaluation show method achieve state art performance recently release emotion dataset outperform number competitive baselines least three hundred and one f measure
language model agglutinative languages always hinder past due myriad agglutinations possible give word various affix propose method diminish problem vocabulary word introduce embed derive syllables morphemes leverage agglutinative property model outperform character level embed perplexity one thousand, six hundred and eighty-seven 950m parameters propose method achieve state art performance exist input prediction methods term key stroke save commercialize
paper describe deep learn system design build wassa two thousand and seventeen emotion intensity share task introduce representation learn approach base inner attention top rnn result show model offer good capabilities able successfully identify emotion bear word predict intensity without leverage lexicons obtain 13th place among twenty-two share task competitors
recent applications neural language model lead increase interest automatic generation natural language however impressive evaluation neurally generate text far remain rather informal anecdotal present attempt systematic assessment one aspect quality neurally generate text focus specific aspect neural language generation ability reproduce authorial write style use establish model authorship attribution empirically assess stylistic qualities neurally generate text comparison conventional language model neural model generate fuzzier text relatively harder attribute correctly nevertheless result also suggest neurally generate text offer valuable perspectives augmentation train data
paper present model detect agreement disagreement online discussions work show use siamese inspire architecture encode discussions longer need rely hand craft feature exploit meta thread structure evaluate model exist online discussion corpora abcd iac awtp experimental result abcd dataset show fuse lexical word embed feature model achieve state art performance eight hundred and four average f1 score also show model train abcd dataset perform competitively relatively smaller annotate datasets iac awtp
recently bidirectional recurrent network language model bi rnnlms show outperform standard unidirectional recurrent neural network language model uni rnnlms range speech recognition task indicate future word context information beyond word history useful however bi rnnlms pose number challenge make use complete previous future word context information impact train efficiency use within lattice rescoring framework paper issue address propose novel neural network structure succeed word rnnlms su rnnlms instead use recurrent unit capture complete future word contexts feedforward unit use model finite number succeed future word model train much efficiently bi rnnlms also use lattice rescoring experimental result meet transcription task ami show propose model consistently outperform uni rnnlms yield slight degradation compare bi rnnlms n best rescoring additionally performance improvements obtain use lattice rescoring subsequent confusion network decode
paper describe submission university helsinki share task cross lingual dependency parse vardial two thousand and seventeen present work annotation projection treebank translation give good result three target languages test set particular slovak seem work well information come czech treebank line relate work attachment score cross lingual model even surpass fully supervise model train target language treebank croatian difficult language test set improvements baseline rather modest norwegian work best information come swedish whereas danish contribute surprisingly little
neural machine translation nmt approach improve state art many machine translation settings last couple years require large amount train data produce sensible output demonstrate nmt use low resource languages well introduce local dependencies use word alignments learn sentence reorder translation addition novel model also present empirical evaluation low resource phrase base statistical machine translation smt nmt investigate lower limit respective technologies find smt remain best option low resource settings method produce acceptable translations seventy thousand tokens train data level baseline nmt system fail completely
common practice compare model human language process predict participant reactions read time corpora consist rich naturalistic linguistic materials however many corpora use study base naturalistic text thus contain many low frequency syntactic constructions often require distinguish process theories describe new corpus consist english texts edit contain many low frequency syntactic constructions still sound fluent native speakers corpus annotate hand correct parse tree include self pace read time data give overview content corpus release data
paper describe approach two thousand and sixteen qats quality assessment share task train three independent random forest classifiers order assess quality simplify texts term grammaticality mean preservation simplicity use language model google ngram feature predict grammaticality mean preservation predict use two complementary approach base word embed wordnet synonyms wider range feature include tf idf sentence length frequency cue phrase use evaluate simplicity aspect overall accuracy system range three thousand, three hundred and thirty-three overall aspect five thousand, eight hundred and seventy-three grammaticality
paper describe submission clac conll two thousand and sixteen share task shallow discourse parse use two complementary approach task standard machine learn approach parse explicit relations deep learn approach non explicit relations overall parser achieve f1 score two thousand, one hundred and six identification discourse relations three thousand, one hundred and ten explicit relations one thousand, two hundred and nineteen non explicit relations blind conll two thousand and sixteen test set
paper investigate influence discourse feature text complexity assessment create two data set base penn discourse treebank simple english wikipedia corpora compare influence coherence cohesion surface lexical syntactic feature assess text complexity result show data set coherence feature correlate text complexity type feature addition feature selection reveal data set top discriminate feature coherence feature
measurement phrasal semantic relatedness important metric many natural language process applications paper present three approach measure phrasal semantics one base semantic network model another distributional similarity model hybrid two hybrid approach achieve f measure seven hundred and seventy-four task evaluate semantic similarity word compositional phrase
work present paper attempt evaluate quantify use discourse relations context blog summarization compare use traditional factual texts specifically measure usefulness six discourse relations namely comparison contingency illustration attribution topic opinion attributive task text summarization blog evaluate effect relation use tac two thousand and eight opinion summarization dataset compare result duc two thousand and seven dataset result show textual genres contingency comparison illustration relations provide significant improvement summarization content attribution topic opinion attributive relations provide consistent significant improvement result indicate least summarization discourse relations useful informal affective texts traditional news article
paper describe submission kosseim15 conll two thousand and fifteen share task shallow discourse parse use uima framework develop parser use cleartk add machine learn functionality uima framework overall parser achieve result one hundred and seventy-three f1 identification discourse relations blind conll two thousand and fifteen test set rank sixth place
arabic word segmentation essential variety nlp applications machine translation information retrieval segmentation entail break word constituent stem affix clitics paper compare two approach segment four major arabic dialects use several thousand train examples dialect two approach involve pose problem rank problem svm ranker pick best segmentation sequence label problem bi lstm rnn couple crf determine best segment word able achieve solid segmentation result dialects use rather limit train data also show employ modern standard arabic data domain adaptation assume context independence improve overall result
introduce helsinki neural machine translation system hnmt apply news translation task wmt two thousand and seventeen rank first human automatic evaluations english finnish discuss success english finnish translations overall advantage nmt strong smt baseline also discuss submissions english latvian english chinese chinese english
investigate use extend context attention base neural machine translation base experiment translate movie subtitle discuss effect increase segment beyond single translation units study use extend source language context well bilingual context extensions model learn distinguish information different segment surprisingly robust respect translation quality pilot study observe interest cross sentential attention pattern improve textual coherence translation least select case
present novel end end trainable neural network model task orient dialog systems model able track dialog state issue api call knowledge base kb incorporate structure kb query result system responses successfully complete task orient dialogs propose model produce well structure system responses jointly learn belief track kb result process condition dialog history evaluate model restaurant search domain use dataset convert second dialog state track challenge dstc2 corpus experiment result show propose model robustly track dialog state give dialog history moreover model demonstrate promise result produce appropriate system responses outperform prior end end trainable neural network model use per response accuracy evaluation metrics
paper problem recovery morphological information lose abbreviate form address focus highly inflect languages evidence present correct inflect form expand abbreviation many case deduce solely morphosyntactic tag context prediction model deep bidirectional lstm network tag embed train evaluation data gather find word could abbreviate use correspond morphosyntactic tag label tag context word use input feature classification network train ten million word polish sejm corpus achieve seven hundred and forty-two prediction accuracy smaller general national corpus polish analysis errors suggest performance task may improve prior knowledge abbreviate word incorporate model
question answer qa systems sensitive many different ways natural language express information need paper turn paraphrase mean capture knowledge present general framework learn felicitous paraphrase various qa task method train end end use question answer pair supervision signal question paraphrase serve input neural score model assign higher weight linguistic expressions likely yield correct answer evaluate approach qa freebase answer sentence selection experimental result three datasets show framework consistently improve performance achieve competitive result despite use simple qa model
word embeddings find provide meaningful representations word efficient way therefore become common natural language process sys tems paper evaluate different word embed model train large portuguese corpus include brazilian european variants train thirty-one word embed model use fasttext glove wang2vec word2vec evaluate intrinsically syntactic semantic analogies extrinsically pos tag sentence semantic similarity task obtain result suggest word analogies appropriate word embed evaluation task specific evaluations appear better option
describe two thousand and seventeen version microsoft conversational speech recognition system update two thousand and sixteen system recent developments neural network base acoustic language model advance state art switchboard speech recognition task system add cnn blstm acoustic model set model architectures combine previously include character base dialog session aware lstm language model rescoring system combination adopt two stage approach whereby subsets acoustic model first combine senone frame level follow word level vote via confusion network also add confusion network rescoring step system combination result system yield fifty-one word error rate two thousand switchboard evaluation set
paper address problem extract keyphrases scientific article categorize correspond task process material cast problem sequence tag introduce semi supervise methods neural tag model build recent advance name entity recognition since annotate train data scarce domain introduce graph base semi supervise algorithm together data selection scheme leverage unannotated article inductive transductive semi supervise learn strategies outperform state art information extraction performance two thousand and seventeen semeval task ten scienceie task
paper describe experiment estimate emotion intensity tweet use generalize regressor system system combine lexical syntactic pre train word embed feature train general regressors finally combine best perform model create ensemble propose system stand 3rd twenty-two systems leaderboard wassa two thousand and seventeen share task emotion intensity
sequence sequence seq2seq model attention excel task involve generate natural language sentence machine translation image caption speech recognition performance improve leverage unlabeled data often form language model work present cold fusion method leverage pre train language model train show effectiveness speech recognition task show seq2seq model cold fusion able better utilize language information enjoy faster convergence better generalization ii almost complete transfer new domain use less ten label train data
homographs word different mean surface form long cause difficulty machine translation systems difficult select correct translation base context however advent neural machine translation nmt systems theoretically take account global sentential context one may hypothesize problem alleviate paper first provide empirical evidence exist nmt systems fact still significant problems properly translate ambiguous word proceed describe methods inspire word sense disambiguation literature model context input word context aware word embeddings help differentiate word sense fore feed encoder experiment three language pair demonstrate model improve performance nmt systems term bleu score accuracy translate homographs
paper present novel method detect negative word persian first use algorithm exceptions list later modify hand use mention list persian polarity corpus rule base algorithm detect negative word
proliferation mislead information everyday access media outlets social media feed news blog online newspapers make challenge identify trustworthy news source thus increase need computational tool able provide insights reliability online content paper focus automatic identification fake content online news contribution twofold first introduce two novel datasets task fake news detection cover seven different news domains describe collection annotation validation process detail present several exploratory analysis identification linguistic differences fake legitimate news content second conduct set learn experiment build accurate fake news detectors addition provide comparative analyse automatic manual identification fake news
paper demonstrate neural network base toolkit namely nnvlp essential vietnamese language process task include part speech pos tag chunk name entity recognition ner toolkit combination bidirectional long short term memory bi lstm convolutional neural network cnn conditional random field crf use pre train word embeddings input achieve state art result three task provide api web demo toolkit
neural network model recently receive heat research attention natural language process community compare traditional model discrete feature neural model two main advantage first take low dimensional real value embed vectors input train large raw data thereby address issue feature sparsity discrete model second deep neural network use automatically combine input feature include non local feature capture semantic pattern express use discrete indicator feature result neural network model achieve competitive accuracies compare best discrete model range nlp task hand manual feature templates carefully investigate nlp task decades typically cover useful indicator pattern solve problems information complementary feature automatically induce neural network therefore combine discrete neural feature potentially lead better accuracy compare model leverage discrete neural feature paper systematically investigate effect discrete neural feature combination range fundamental nlp task base sequence label include word segmentation pos tag name entity recognition chinese english respectively result standard benchmarks show state art neural model give accuracies comparable best discrete model literature task comb discrete neural feature unanimously yield better result
present cloudscan invoice analysis system require zero configuration upfront annotation contrast previous work cloudscan rely templates invoice layout instead learn single global model invoice naturally generalize unseen invoice layouts model train use data automatically extract end user provide feedback automatic train data extraction remove requirement users annotate data precisely describe recurrent neural network model capture long range context compare baseline logistic regression model correspond current cloudscan production system train evaluate system eight important field use dataset three hundred and twenty-six thousand, four hundred and seventy-one invoice recurrent neural network baseline model achieve eight hundred and ninety-one eight hundred and eighty-seven average f1 score respectively see invoice layouts harder task unseen invoice layouts recurrent neural network model outperform baseline eight hundred and forty average f1 compare seven hundred and eighty-eight
storytelling serve many different social function eg stories use persuade share trouble establish share value learn social behaviors entertain moreover stories often tell conversationally dialog previous work suggest information provide dialogically engage provide monolog paper present algorithms convert deep representation story dialogic storytelling vary aspects tell include personality storytellers conduct several experiment test whether dialogic storytellings engage whether automatically generate variants linguistic form correspond personality differences recognize extend storytelling dialog
centroid base model extractive document summarization simple fast baseline rank sentence base similarity centroid vector paper apply rank possible summaries instead sentence use simple greedy algorithm find best summary furthermore show possi bilities scale larger input docu ment collections select small num ber sentence document prior construct summary experiment do duc2004 dataset multi document summarization ob serve higher performance orig inal model par complex state art methods
english indian language machine translation pose challenge structural morphological divergence paper describe english indian language statistical machine translation use pre order suffix separation pre order use rule transfer structure source sentence prior train translation syntactic restructure help statistical machine translation tackle structural divergence hence better translation quality suffix separation use tackle morphological divergence english highly agglutinative indian languages demonstrate use pre order suffix separation help improve quality english indian language machine translation
discourse parse long treat stand alone problem independent constituency dependency parse attempt problem pipelined rather end end sophisticate self contain assume gold standard text segmentations elementary discourse units use external parsers syntactic feature paper propose first end end discourse parser jointly parse syntax discourse level well first syntacto discourse treebank integrate penn treebank rst treebank build upon recent span base constituency parser joint syntacto discourse parser require preprocessing whatsoever segmentation feature extraction achieve state art end end discourse parse accuracy
information web dialogic facebook newsfeeds forum conversations comment thread news article contrast traditional monologic natural language process resources news highly social dialogue frequent social media make challenge context nlp paper test bootstrapping method originally propose monologic domain train classifiers identify two different type subjective language dialogue sarcasm nastiness explore two methods develop linguistic indicators use first level classifier aim maximize precision expense recall best perform classifier first phase achieve fifty-four precision thirty-eight recall sarcastic utterances use general syntactic pattern previous work create general sarcasm indicators improve precision sixty-two recall fifty-two test generality method apply bootstrapping classifier nastiness dialogic act first phase use crowdsourced nasty indicators achieve fifty-eight precision forty-nine recall increase seventy-five precision sixty-two recall bootstrap first level generalize syntactic pattern
order tell stories different voice different audiences interactive story systems require one semantic representation story structure two ability automatically generate story dialogue semantic representation use form natural language generation nlg however limit research methods link story structure narrative descriptions scenes story events paper present automatic method convert scheherazade story intention graph semantic representation input require personage nlg engine use thirty-six aesop fables distribute dramabank collection story encode train translation rule one story test rule generate text remain thirty-five result measure term string similarity metrics levenshtein distance bleu score result show generate thirty-five stories correct content test set stories average close output scheherazade realizer customize semantic representation provide examples story variations generate personage future work experiment measure quality stories generate different voice techniques make storytelling interactive
information web dialogic facebook newsfeeds forum conversations comment thread news article contrast traditional monologic resources news highly social dialogue frequent social media aim automatically identify sarcastic nasty utterances unannotated online dialogue extend bootstrapping method previously apply classification monologic subjective sentence riloff weibe two thousand and three adapt method fit sarcastic nasty dialogic domain method follow one explore methods identify sarcastic nasty cue word phrase dialogues two use learn cue train sarcastic nasty cue base classifier three learn general syntactic extraction pattern sarcastic nasty utterances define fine tune sarcastic pattern create pattern base classifier four combine cue base fine tune pattern base classifiers maximize precision expense recall test unannotated utterances
recent explosion applications dialogue interaction range direction give tourist information interactive story systems yet natural language generation nlg component many systems remain largely handcraft limitation greatly restrict range applications also mean impossible take advantage recent work expressive statistical language generation dynamically automatically produce large number variations give content propose solution problem lie new methods develop language generation resources describe es translator computational language generator previously apply fables quantitatively evaluate domain independence est apply personal narratives weblogs take advantage recent work language generation create parameterized sentence planner story generation provide aggregation operations variations discourse point view finally present user evaluation different personal narrative retell
research storytelling last one hundred years distinguish least two level narrative representation one story fabula two discourse sujhet use distinction create fabula tales computational framework virtual storyteller tell story different ways implementation general narratological variations vary direct vs indirect speech character voice style point view focalization strength computational framework base general methods use exist story content either fables personal narratives collect blog first explain simple annotation tool allow naive annotators easily create deep representation fabula call story intention graph show use representation generate story tell automatically present result two study test narratological parameters show different tell affect reader perception story character
recent work automatic recognition conversational telephone speech cts achieve accuracy level comparable human transcribers although debate precisely quantify human performance task use nist two thousand cts evaluation set raise question systematic differences may find differentiate human machine transcription errors paper approach question compare output accurate cts recognition system standard speech transcription vendor pipeline find frequent substitution deletion insertion error type output show high degree overlap notable exception automatic recognizer tend confuse fill pause uh backchannel acknowledgments uhhuh humans tend make error presumably due distinctive oppose pragmatic function attach word furthermore quantify correlation human machine errors speaker level investigate effect speaker overlap train test data finally report informal turing test ask humans discriminate automatic human transcription error case
paper explore alternative ways train neural machine translation system multi domain scenario investigate data concatenation fine tune model stack multi level fine tune data selection multi model ensemble find show best translation quality achieve build initial system concatenation available domain data fine tune domain data model stack work best train begin furthest domain data model incrementally fine tune next furthest domain data selection give best result consider decent compromise train time translation quality weight ensemble different individual model perform better data selection beneficial scenario time fine tune already train model
present simple lstm base transition base dependency parser model compose single lstm hide layer replace hide layer usual fee forward network architecture also propose new initialization method use pre train weight fee forward neural network initialize lstm base model also show use dropout input layer positive effect performance final parser achieve nine thousand, three hundred and six unlabeled nine thousand, one hundred and one label attachment score penn treebank additionally replace lstms grus elman units model explore effectiveness initialization method individual gate constitute three type rnn units
present new corpus personabank consist one hundred and eight personal stories weblogs annotate story intention graph deep representation fabula story describe topics stories basis story intention graph representation well process annotate stories produce story intention graph challenge adapt tool new personal narrative domain also discuss corpus use applications retell story use different style tell co tell content planner
americans spend third time online many participate online conversations social political issue hypothesize social media arguments issue may engage persuasive traditional media summaries particular type people may less convince particular style argument eg emotional arguments may resonate personalities factual arguments resonate others report set experiment test large scale audience variables interact argument style affect persuasiveness argument research topic within natural language process show belief change affect personality factor conscientious open agreeable people convince emotional arguments
dialogue author large game require content creation subtlety delivery vary character character manually author dialogue tedious time consume even altogether infeasible paper utilize rich narrative representation model dialogue expressive natural language generation engine realize expand upon translation tool bridge two add functionality translator allow direct speech model narrative representation whereas original translator support narratives tell third person narrator show perform character substitution dialogues implement evaluate potential application dialogue implementation generate dialogue game big dynamic procedurally generate open worlds present pilot study human perceptions personalities character use direct speech assume unknown personality type time author
generation complex derive word form overlook problem nlp fill gap apply neural sequence sequence model task overview theoretical motivation paradigmatic treatment derivational morphology introduce task derivational paradigm completion parallel inflectional paradigm completion state art neural model adapt inflection task able learn range derivation pattern outperform non neural baseline one hundred and sixty-four however due semantic historical lexical considerations involve derivational morphology future work need achieve performance parity inflection generate systems
even common nlp task sufficient supervision available many languages morphological tag exception work present explore transfer learn scheme whereby train character level recurrent neural taggers predict morphological taggings high resource languages low resource languages together learn joint character representations among multiple relate languages successfully enable knowledge transfer high resource languages low resource ones improve accuracy thirty
paper present empirical study two widely use sequence prediction model conditional random field crfs long short term memory network lstms two fundamental task vietnamese text process include part speech tag name entity recognition show strong lower bind label accuracy obtain rely simple word base feature minimal hand craft feature engineer nine thousand and sixty-five eight thousand, six hundred and three performance score standard test set two task respectively particular demonstrate empirically surprise efficiency word embeddings two task two model point state art lstms model always outperform significantly traditional crfs model especially moderate size data set finally give suggestions discussions efficient use sequence label model practical applications
attention model become standard component neural machine translation nmt guide translation process selectively focus part source sentence predict target word however find generation target word depend source sentence also rely heavily previous generate target word especially distant word difficult model use recurrent neural network solve problem propose paper novel look ahead attention mechanism generation nmt aim directly capture dependency relationship target word design three pattern integrate look ahead attention conventional attention model experiment nist chinese english wmt english german translation task show propose look ahead attention mechanism achieve substantial improvements state art baselines
name entity recognition disambiguation nerd systems recently widely research deal significant growth web nerd systems crucial several natural language process nlp task summarization understand machine translation however standard interface specification ie systems may vary significantly either export output process input thus give company desire implement one nerd system process quite exhaustive prone failure addition industrial solutions demand critical requirements eg large scale process completeness versatility license commonly requirements impose limitation make good nerd model ignore company paper present tanker distribute architecture aim overcome scalability reliability failure tolerance limitations relate industrial need combine nerd systems end tanker rely micro service orient architecture enable agile development delivery complex enterprise applications addition tanker provide standardize api make possible combine several nerd systems
graph base synset induction methods maxmax watset induce synsets perform global cluster synonymy graph however methods sensitive structure input synonymy graph sparseness input dictionary substantially reduce quality extract synsets paper propose two different approach design alleviate incompleteness input dictionaries first one perform pre process graph add miss edge second one perform post process merge similar synset cluster evaluate approach two datasets russian language discuss impact performance synset induction methods finally perform extensive error analysis approach discuss prominent alternative methods cop problem sparsity synonymy dictionaries
first present minimal feature set transition base dependency parse continue recent trend start kiperwasser goldberg 2016a cross huang 2016a use bi directional lstm feature plug minimal feature set dynamic program framework huang sagae two thousand and ten kuhlmann et al two thousand and eleven produce first implementation worst case ofn3 exact decoders arc hybrid arc eager transition systems minimal feature also present ofn3 global train methods finally use ensembles include new parsers achieve best unlabeled attachment score report knowledge chinese treebank second best class result english penn treebank
langpro automate theorem prover natural language https githubcom kovvalsky langpro give set premise hypothesis able prove semantic relations prover base version analytic tableau method specially design natural logic proof procedure operate logical form preserve linguistic expressions large extent property make logical form easily obtainable syntactic tree particular combinatory categorial grammar derivation tree nature proof deductive transparent fracas sick textual entailment datasets prover achieve high result comparable state art
understand narrative humans draw inferences underlie relations narrative events cognitive theories narrative understand define inferences four different type causality include pair events b physically cause b x drop x break pair events cause emotional state b saw x felt fear previous work learn narrative relations text either focus strict physical causality vague relation learn paper learn pair causal events corpus film scene descriptions action rich tend tell chronological order show event pair induce use methods high quality judge stronger causal relation event pair rel grams
human engagement narrative partially drive reason discourse relations narrative events expectations likely happen next result reason researchers nlp tackle model expectations range perspectives include treat inference contingent discourse relation type common sense causal reason approach model likelihood events draw several line previous work implement evaluate different unsupervised methods learn event pair likely contingent one another refine event pair learn corpus film scene descriptions utilize web search count evaluate result collect human judgments contingency result indicate use web search count increase average accuracy best method eight thousand, five hundred and sixty-four baseline fifty compare average accuracy seven thousand, five hundred and fifteen without web search
one weakness machine learn nlp model typically perform poorly domain data work study task identify products buy sell online cybercrime forums exhibit particularly challenge cross domain effect formulate task represent hybrid slot fill information extraction name entity recognition annotate data four different forums forums constitute fine grain domain forums cover different market sectors different properties even though forums broad domain cybercrime characterize domain differences context learn base system supervise model see decrease accuracy apply new forums standard techniques semi supervise learn domain adaptation limit effectiveness data suggest need improve techniques release dataset one thousand, nine hundred and thirty-eight annotate post across four forums
semantic relatedness term represent similarity mean numerical score one hand humans easily make judgments semantic relatedness hand kind information useful language process systems semantic relatedness extensively study english use numerous language resources associative norms human judgments datasets generate lexical databases evaluation resources kind available russian date contribution address problem present five language resources different scale purpose russian semantic relatedness list triple wordi wordj relatednessij four design evaluation systems compute semantic relatedness complement term semantic relation type represent benchmarks use organize share task russian semantic relatedness attract nineteen team use one best approach identify competition generate fifth high coverage resource first open distributional thesaurus russian multiple evaluations thesaurus include large scale crowdsourcing study involve native speakers indicate high accuracy
informal first person narratives unique resource computational model everyday events people affective reactions people blogging day tend explicitly say happy instead describe situations humans readily infer affective reactions however current sentiment dictionaries miss much information need make similar inferences build recent work model affect term lexical predicate function affect predicate arguments present method learn proxies function first person narratives construct novel fine grain test set show pattern learn improve ability predict first person affective reactions everyday events stanford sentiment baseline 67f 75f
present simple method improve neural translation low resource language pair use parallel data relate also low resource language pair method base transfer method zoph et al whereas method ignore source vocabulary overlap exploit first split word use byte pair encode bpe increase vocabulary overlap train model first language pair transfer parameters include source word embeddings another model continue train second language pair experiment show transfer learn help word base translation slightly use top much stronger bpe baseline yield larger improvements forty-three bleu
language understand lu dialogue policy learn two essential components conversational systems human human dialogues well control often random unpredictable due goals speak habit paper propose role base contextual model consider different speaker roles independently base various speak pattern multi turn dialogues experiment benchmark dataset show propose role base model successfully learn role specific behavioral pattern contextual encode significantly improve language understand dialogue policy learn task
speak language understand slu essential component conversational systems slu component treat utterance independently follow components aggregate multi turn information separate phase order avoid error propagation effectively utilize contexts prior work leverage history contextual slu however previous model pay attention content history utterances without consider temporal information speaker roles dialogues recent utterances important least recent ones furthermore users usually pay attention one self history reason two others utterances listen speaker utterances may provide informative cue help understand therefore paper propose attention base network additionally leverage temporal information speaker role better slu attention contexts speaker roles automatically learn end end manner experiment benchmark dialogue state track challenge four dstc4 dataset show time aware dynamic role attention network significantly improve understand performance
vector space model word embeddings neural network parsers many advantage nlp generalise fix length word vectors vector space arbitrary linguistic structure still unclear paper propose bag vector embeddings arbitrary linguistic graph bag vector space minimal nonparametric extension vector space allow representation grow size graph tie representation specific tree graph structure propose efficient train inference algorithms base tensor factorisation embed arbitrary graph bag vector space demonstrate usefulness representation train bag vector embeddings dependency graph evaluate unsupervised semantic induction semantic textual similarity natural language inference task
text message widely use form computer mediate communication cmc previous find show linguistic factor reliably indicate message deceptive example users take longer use word craft deceptive message truthful message exist research also examine factor student status gender affect rat deception word choice deceptive message however research limit small sample size return contradict find paper aim address issue use dataset text message collect large vary set participants use android message application result paper show significant differences word choice frequency deceptive message male female participants well students non students
give constantly grow proliferation false claim online recent years also grow research interest automatically distinguish false rumor factually true claim propose general purpose framework fully automatic fact check use external source tap potential entire web knowledge source confirm reject claim framework use deep neural network lstm text encode combine semantic kernels task specific embeddings encode claim together piece potentially relevant text fragment web take source reliability account evaluation result show good performance two different task datasets rumor detection ii fact check answer question community question answer forums
explore idea automatically craft tune dataset statistical machine translation smt make hyper parameters smt system robust respect specific deficiencies parameter tune algorithms explore research direction allow better parameter tune paper achieve goal select subset available sentence pair suitable specific combinations optimizers objective function evaluation measure demonstrate potential idea pairwise rank optimization pro optimizer know yield short translations show learn problem alleviate tune subset development set select base sentence length particular use longest fifty tune sentence achieve two fold tune speedup improvements bleu score rival alternatives fix bleu1 smooth instead
natural language provide widely accessible expressive interface robotic agents understand language complex environments agents must reason full range language input correspondence world reason language vision open problem receive increase attention exist data set focus visual diversity display full range natural language expressions count set reason comparisons propose simple task natural language visual reason image pair descriptive statements task predict statement true give scene abstract describe exist synthetic image corpus current work collect real vision data
computational humor involve several task humor recognition humor generation humor score useful human curated data work present corpus twenty-seven thousand tweet write spanish crowd annotate humor value funniness score four annotations per tweet tag one thousand, three hundred people internet equally divide tweet come humorous non humorous account inter annotator agreement krippendorff alpha value five thousand, seven hundred and ten dataset available general use serve basis humor detection first step tackle subjectivity
nlp convolutional neural network cnns benefit less recurrent neural network rnns attention mechanisms hypothesize attention cnns mainly implement attentive pool ie apply pool rather attentive convolution ie integrate convolution convolution differentiator cnns powerfully model higher level representation word take account local fix size context input text tx work propose attentive convolution network attconv extend context scope convolution operation derive higher level feature word local context also information extract nonlocal context attention mechanism commonly use rnns nonlocal context come part input text tx distant ii extra ie external contexts ty experiment sentence model zero context sentiment analysis single context textual entailment multiple context claim verification demonstrate effectiveness attconv sentence representation learn incorporation context particular attentive convolution outperform attentive pool strong competitor popular attentive rnns
propose use question answer qa data web forums train chatbots scratch ie without dialog train data first extract pair question answer sentence typically much longer texts question answer forum use shorter texts train seq2seq model efficient way improve parameter optimization use new model selection strategy base qa measure finally propose use extrinsic evaluation respect qa task automatic evaluation method chatbots evaluation show model achieve map six hundred and thirty-five extrinsic task moreover answer correctly four hundred and ninety-five question similar question ask forum four hundred and seventy-three question conversational style
technical report describe framework use process three large portuguese corpora two corpora contain texts newspapers one publish brazil publish portugal third corpus colonia historical portuguese collection contain texts write 16th early 20th century report present pre process methods segmentation annotation corpora well index query methods finally present publish research paper use corpora
model hypernymy poodle dog important generalization aid many nlp task entailment coreference relation extraction question answer supervise learn label hypernym source wordnet limit coverage model address learn hypernyms unlabeled text exist unsupervised methods either scale large vocabularies yield unacceptably poor accuracy paper introduce distributional inclusion vector embed dive simple implement unsupervised method hypernym discovery via per word non negative vector embeddings preserve inclusion property word contexts low dimensional interpretable space experimental evaluations comprehensive previous literature aware evaluate eleven datasets use multiple exist well newly propose score function find method provide double precision previous unsupervised embeddings highest average performance use much compact word representation yield many new state art result
paper introduce minimal dependency translation mdt ongoing project develop rule base framework creation rudimentary bilingual lexicon grammars machine translation computer assist translation resourced languages well initial step towards implementation mdt english amharic translation basic units mdt call group head multi item sequence addition wordforms group may contain lexemes syntactic semantic categories grammatical feature group associate one translations group target language translation constraint satisfaction use select set source language group input sentence sequence word associate target language group
identify nominals head match long stand challenge coreference resolution current systems perform significantly worse humans paper present new neural network architecture outperform current state art system english portion conll two thousand and twelve share task do use logistic regression feature produce two submodels one architecture propose cm16a combine domain specific embeddings antecedent mention also propose simple additional feature seem improve performance model substantially increase f1 almost four basic logistic regression complex model
propose scale free identifier networksfin novel model event identification document general sfin first encode document multi scale memory stack extract special events via conduct multi scale action consider special type sequence label design large scale action make efficient process long document whole model train supervise learn reinforcement learn
knowledge users emotion state help improve human computer interaction work present emonet emotion detector chinese daily dialogues base deep convolutional neural network order maintain original linguistic feature order commonly use methods like segmentation keywords extraction adopt instead increase depth cnn try let cnn learn inner linguistic relationships main contribution present new model new pipeline use multi language environment solve sentimental problems experimental result show emonet great capacity learn emotion dialogues achieve better result state art detectors
despite number nlp study dedicate thematic fit estimation little attention pay relate task compose update verb argument expectations exceptions mostly model phenomenon structure distributional model implicitly assume similarly structure representation events recent experimental evidence however suggest human process system could also exploit unstructured bag arguments type event representation predict upcoming input paper implement traditional structure model adapt compare different hypotheses concern degree structure event knowledge evaluate relative performance task argument expectations update
multilinguality gradually become ubiquitous sense researchers successfully show use additional languages help improve result many natural language process task multilingual multiway corpora mmc contain sentence multiple languages corpora primarily use multi source pivot language machine translation also useful develop multilingual sequence taggers transfer learn corpora available organize multilingual experiment researchers need write boilerplate code every time want use say corpora moreover official mmc collection become difficult compare exist approach present work create unify systematically organize repository mmc span large number languages also provide train development test split corpora official split unavailable hope help speed pace multilingual nlp research ensure nlp researchers obtain result trustable since compare easily indicate corpora source extraction procedures relevant statistics also make collection public research purpose
present manually construct seed lexicon encode inferential profile french event select predicate across different use inferential profile karttunen 1971a verb design capture inferences trigger use verb context reflect influence clause embed verb factuality event describe embed clause resource develop provide evidence follow three hypotheses french implicative verbs aspect dependent profile inferential profile vary outer aspect factive verbs aspect independent profile keep inferential profile imperfective perfective aspect ii implicativity decrease imperfective aspect inferences trigger french implicative verbs combine perfective aspect often weaken verbs combine imperfective aspect iii implicativity decrease animate deep subject inferences trigger verb implicative inanimate subject weaken verb use animate subject resource additionally show verbs different inferential profile display clearly distinct sub categorisation pattern particular verbs factive implicative read show prefer infinitival clauses implicative read tense clauses factive read
explore two solutions problem mistranslate rare word neural machine translation first argue standard output layer compute inner product vector represent context possible output word embeddings reward frequent word disproportionately propose fix norms vectors constant value second integrate simple lexical module jointly train rest model evaluate approach eight language pair data size range 100k 8m word achieve improvements forty-three bleu surpass phrase base translation nearly settings
paper address problem annotation projection semantic role label resource poor languages use supervise annotations resource rich language parallel data propose transfer method employ information source target syntactic dependencies well word alignment density improve quality iterative bootstrapping method experiment yield thirty-five absolute label f score improvement standard annotation projection method
study find relevant question community forums language new question different exist question forum particular explore arabic english language pair compare kernel base system fee forward neural network scenario large parallel corpus available train machine translation system bilingual dictionaries cross language word embeddings observe approach degrade performance system work translate text especially kernel base system depend heavily syntactic kernel address issue use cross language tree kernel compare original arabic tree english tree relate question show kernel almost close performance gap respect monolingual system neural network side use parallel corpus train cross language embeddings use represent arabic input english relate question space result also improve close monolingual neural network overall kernel system show better performance compare neural network case
internet proliferation smart mobile devices change way information create share spread eg microblogs twitter weblogs livejournal social network facebook instant messengers skype whatsapp commonly use share thoughts opinions anything surround world result proliferation social media content thus create new opportunities study public opinion scale never possible naturally abundance data quickly attract business research interest various field include market political science social study among many others interest question like people like new apple watch americans support obamacare scottish feel brexit answer question require study sentiment opinions people express social media give rise fast growth field sentiment analysis social media twitter especially popular research due scale representativeness variety topics discuss well ease public access message present overview work sentiment analysis twitter
article explore potential use sentence level discourse structure machine translation evaluation first design discourse aware similarity measure use subtree kernels compare discourse parse tree accordance rhetorical structure theory rst show simple linear combination measure help improve various exist machine translation evaluation metrics regard correlation human judgments segment system level suggest discourse information complementary information use many exist evaluation metrics thus could take account develop richer evaluation metrics wmt fourteen win combine metric discotkparty also provide detail analysis relevance various discourse elements relations rst parse tree machine translation evaluation particular show aspects rst tree relevant ii nuclearity useful relation type iii similarity translation rst tree reference tree positively correlate translation quality
present depcc largest date linguistically analyze corpus english include three hundred and sixty-five million document compose two hundred and fifty-two billion tokens seventy-five billion name entity occurrences one hundred and forty-three billion sentence web scale crawl textsccommon crawl project sentence process dependency parser name entity tagger contain provenance information enable various applications range train syntax base word embeddings open information extraction question answer build index sentence linguistic meta data enable quick search across corpus demonstrate utility corpus verb similarity task show distributional model train corpus yield better result model train smaller corpora like wikipedia distributional model outperform state art model verb similarity train smaller corpora simverb3500 dataset
neural machine translation nmt recently achieve impressive result potential problem exist nmt algorithm however decode conduct leave right without consider right context paper propose two stage approach solve problem first stage conventional attention base nmt system use produce draft translation second stage novel double attention nmt system use refine translation look original input well draft translation draft refinement obtain right context information draft hence produce consistent translations evaluate approach use two chinese english translation task one 44k pair 1m pair respectively experiment show approach achieve positive improvements conventional nmt system improvements twenty-four nine bleu point small scale large scale task respectively
mobile devices use language model suggest word phrase use text entry traditional language model base contextual word frequency static corpus text however certain type phrase offer writers suggestions may systematically choose often frequency would predict paper propose task generate suggestions writers accept relate distinct task make accurate predictions although task fundamentally interactive propose counterfactual set permit offline train evaluation find even simple language model capture text characteristics improve acceptability
paper present latest investigations different feature factor language model code switch speech effect automatic speech recognition asr performance focus syntactic semantic feature extract code switch text data integrate factor language model different possible factor word part speech tag brown word cluster open class word cluster open class word embeddings explore experimental result reveal brown word cluster part speech tag open class word effective reduce perplexity factor language model mandarin english code switch corpus seame asr experiment model contain brown word cluster part speech tag model also include cluster open class word embeddings yield best mix error rate result summary best language model significantly reduce perplexity seame evaluation set one hundred and eight relative mix error rate thirty-four relative
clickbait headline make use mislead title hide critical information exaggerate content land target page entice click clickbaits often use eye catch word attract viewers target content often low quality clickbaits especially widespread social media twitter adversely impact user experience cause immense dissatisfaction hence become increasingly important put forward widely applicable approach identify detect clickbaits paper make use dataset clickbait challenge two thousand and seventeen clickbait challengecom comprise twenty-one thousand headline title annotate least five judgments crowdsourcing clickbait attempt build effective computational clickbait detection model dataset first consider total three hundred and thirty-one feature filter many feature avoid overfitting improve run time learn eventually select sixty important feature final model use feature random forest regression achieve follow result mse0035 mse accuracy082 f1 sore061 clickbait class
neural network excel many nlp task remain open question performance pretrained distribute word representations interaction weight initialization hyperparameters address question empirically use attention base sequence sequence model natural language inference nli specifically compare three type embeddings random pretrained glove word2vec retrofit pretrained plus wordnet information show pretrained embeddings outperform random retrofit ones large nli corpus experiment control data set would light contexts retrofit embeddings useful also explore two principled approach initialize rest model parameters gaussian orthogonal show latter yield gain twenty-nine nli task
less resource languages indian indian english indian language mt system developments face difficulty translate various lexical phenomena paper present work comparative study four hundred and forty phrase base statistical train model one hundred and ten language pair across eleven indian languages develop one hundred and ten baseline statistical machine translation systems augment train corpus indowordnet synset word entries lexical database train one hundred and ten model top baseline system do detail performance comparison use various evaluation metrics bleu score meteor ter observe significant improvement evaluations translation quality across four hundred and forty model use indowordnet experiment give detail insight two ways one usage lexical database synset map resource poor languages two efficient usage indowordnet sysnset map synset map lexical entries help smt system handle ambiguity great extent translation
translate morphologically rich languages statistical mt approach face problem data sparsity severity sparseness problem high corpus size morphologically richer language less even though use factor model correctly generate morphological form word problem data sparseness limit performance paper describe simple effective solution base enrich input corpora various morphological form word use method phrase base factor base experiment two morphologically rich languages hindi marathi translate english evaluate performance experiment term automatic evaluation subjective evaluation adequacy fluency observe morphology injection method help improve quality translation analyze morph injection method help handle data sparseness problem great level
present framework machine translation evaluation use neural network pairwise set goal select better translation pair hypotheses give reference translation framework lexical syntactic semantic information reference two hypotheses embed compact distribute vector representations feed multi layer neural network model nonlinear interactions hypotheses reference well two hypotheses experiment benchmark datasets wmt metrics share task obtain best result publish far basic network configuration also perform series experiment analyze understand contribution different components network evaluate variants extensions include fine tune semantic embeddings sentence base representations model convolutional recurrent neural network summary propose framework flexible generalizable allow efficient learn score provide mt evaluation metric correlate human judgments par state art
paper present work creation lexical resources machine translation english hindi describe development phrase pair mappings experiment comparative performance evaluation different train model top baseline statistical machine translation system focus augment parallel corpus vocabulary well various inflect form explore different ways augment train corpus various lexical resources lexical word synset word function word verb phrase describe case study automatic subjective evaluations detail error analysis english hindi hindi english machine translation systems analyze incremental growth quality machine translation usage various lexical resources thus lexical resources help uplift translation quality resource poor langugaes
present bpemb collection pre train subword unit embeddings two hundred and seventy-five languages base byte pair encode bpe evaluation use fine grain entity type testbed bpemb perform competitively languages bet ter alternative subword approach require vastly fewer resources tokenization bpemb available https githubcom bheinzerling bpemb
text summarization text simplification two major ways simplify text poor readers include children non native speakers functionally illiterate text summarization produce brief summary main ideas text text simplification aim reduce linguistic complexity text retain original mean recently approach text summarization text simplification base sequence sequence model achieve much success many text generation task however although generate simplify texts similar source texts literally low semantic relevance work goal improve semantic relevance source texts simplify texts text summarization text simplification introduce semantic relevance base neural model encourage high semantic similarity texts summaries model source text represent gate attention encoder summary representation produce decoder besides similarity score representations maximize train experiment show propose model outperform state art systems two benchmark corpus
paper introduce czech text document corpus v twenty collection text document automatic document classification czech language compose text document provide czech news agency freely available research purpose http ctdckivzcucz corpus create order facilitate straightforward comparison document classification approach czech data particularly dedicate evaluation multi label document classification approach one document usually label one label besides information document class corpus also annotate morphological layer paper show result select state art methods corpus offer possibility easy comparison approach
lack proper linguistic resources major challenge face machine translation system developments deal resource poor languages paper describe effective ways utilize lexical resources improve quality statistical machine translation research usage lexical resources mainly focus two ways augment parallel corpus vocabulary provide various word form augment train corpus various lexical resources lexical word function word kridanta pair verb phrase describe case study evaluations detail error analysis marathi hindi hindi marathi machine translation systems evaluations observe incremental growth quality machine translation usage various lexical resources increase moreover usage various lexical resources help improve coverage quality machine translation limit parallel corpus available
lexical entailment hyponymy fundamental issue semantics natural language paper propose distributional semantic model efficiently learn word embeddings entailment use recently propose framework model entailment vector space model postulate latent vector pseudo phrase contain two neighbour word vectors investigate model word evidence contribute phrase vector posterior distribution one word phrase vector find posterior vectors perform better result word embeddings outperform best previous result predict hyponymy word unsupervised semi supervise experiment
proliferation social media last decade determine people attitude respect specific topic document interaction events fuel research interest natural language process introduce new channel call sentiment emotion analysis instance businesses routinely look develop systems automatically understand customer conversations identify relevant content enhance market products manage reputations previous efforts assess people sentiment twitter suggest twitter may valuable resource study political sentiment reflect offline political landscape accord pew research center report january two thousand and sixteen forty-four percent us adults state learn presidential election social media furthermore twenty-four percent report use social media post two candidates source news information fifteen percent use candidates websites email combine first presidential debate trump hillary tweet debate ever one hundred and seventy-one million tweet
work focus task automatically extract bilingual lexicon language pair spanish nahuatl low resource set small amount parallel corpus available downstream methods work well low resources condition specially true approach use vectorial representations like word2vec proposal construct bilingual word vectors graph graph generate use translation pair obtain unsupervised word alignment method show low resource set type vectors successful represent word bilingual semantic space moreover linear transformation apply translate word one language another graph base representations considerably outperform popular set use word2vec
context aware language model use location user domain metadata context adapt predictions neural language model context information typically represent embed give rnn additional input show useful many applications introduce powerful mechanism use context adapt rnn let context vector control low rank transformation recurrent layer weight matrix experiment show allow greater fraction model parameters adjust benefit term perplexity classification several different type context
question classification important task wide applications however traditional techniques treat question general sentence ignore correspond answer data order consider answer information question model first introduce novel group sparse autoencoders refine question representation utilize group information answer set propose novel group sparse cnns naturally learn question representation respect answer implant group sparse autoencoders traditional cnns propose model significantly outperform strong baselines four datasets
paper describe oregon state university submissions share wmt seventeen task multimodal translation task task sentence pair image caption different languages key difference task conventional machine translation correspond image additional information sentence pair paper introduce simple effective system take image share different languages feed encode decode side report system performance english french english german flickr30k domain mscoco domain datasets system achieve best performance ter english german mscoco dataset
number document web grow exponentially multi document summarization become important since provide main ideas document set short time paper present unsupervised centroid base document level reconstruction framework use distribute bag word model specifically approach select summary sentence order minimize reconstruction error summary document apply sentence selection beam search improve performance model experimental result two different datasets show significant performance gain compare state art baselines
present iit bombay english hindi parallel corpus corpus compilation parallel corpora previously available public domain well new parallel corpora collect corpus contain one hundred and forty-nine million parallel segment 694k segment previously available public domain corpus pre process machine translation report baseline phrase base smt nmt translation result corpus corpus use two editions share task workshop asian language translation two thousand and sixteen two thousand and seventeen corpus freely available non commercial research best knowledge largest publicly available english hindi parallel corpus
define novel textual entailment task require inference multiple premise sentence present new dataset task minimize trivial lexical inferences emphasize knowledge everyday events present challenge set textual entailment evaluate several strong neural baselines analyze multiple premise task differ standard textual entailment
recent years retro digitize paper base file become major undertake private public archive well important task electronic mailroom applications first step workflow involve scan optical character recognition ocr document preservation document contexts single page scan major requirement context facilitate workflows involve large amount paper scan page stream segmentation pss task automatically separate stream scan image multi page document digitization project together german federal archive develop novel approach base convolutional neural network cnn combine image text feature achieve optimal document separation result evaluation show pss architecture achieve accuracy ninety-three regard new state art task
surge social media use bring huge demand multilingual sentiment analysis msa unveil cultural difference far traditional methods resort machine translation translate texts languages english adopt methods work english however paradigm condition quality machine translation paper propose new deep learn paradigm assimilate differences languages msa first pre train monolingual word embeddings separately map word embeddings different space share embed space finally train parameter share deep neural network msa experimental result show paradigm effective especially cnn model outperform state art baseline around twenty-one term classification accuracy
attention neural machine translation provide possibility encode relevant part source sentence translation step result attention consider alignment model well however work specifically study attention provide analysis learn attention model thus question still remain attention similar different traditional alignment paper provide detail analysis attention compare traditional alignment answer question whether attention capable model translational equivalent capture information show attention different alignment case capture useful information alignments
propose monoise normalization model focus generalizability efficiency aim easily reusable adaptable normalization task translate texts non canonical domain canonical domain case social media data standard language propose model base modular candidate generation module responsible different type normalization action important generation modules spell correction system word embeddings module depend definition normalization task static lookup list crucial performance train random forest classifier rank candidates generalize well different type normaliza tion action feature rank originate generation modules besides feature n gram feature prove important source information show monoise beat state art different normalization benchmarks english dutch define task normalization slightly different
speech language technologies train massive amount speech text information however world languages resources stable orthography systems construct almost zero resource condition promise speech technology also computational language documentation goal computational language documentation help field linguists semi automatically analyze annotate audio record endanger unwritten languages example task automatic phoneme discovery lexicon discovery speech signal paper present speech corpus collect realistic language documentation process make 5k speech utterances mboshi bantu c25 align french text translations speech transcriptions also make available correspond non standard graphemic form close language phonology present data collect clean process illustrate use zero resource task speak term discovery dataset make available community reproducible computational language documentation experiment evaluation
attention distributions generate translations useful bi product attention base recurrent neural network translation model treat soft alignments input output tokens work use attention distributions confidence metric output translations present two strategies use attention distributions filter bad translations large back translate corpus select best translation hybrid setup two different translation systems manual evaluation indicate weak correlation confidence score human judgments use case show improvements two hundred and twenty-two bleu point filter ninety-nine point hybrid translation test english german english latvian translation
release galactic dependencies ten large set synthetic languages find earth annotate universal dependencies format new resource aim provide train development data nlp methods aim adapt unfamiliar languages synthetic treebank produce real treebank stochastically permute dependents nouns verbs match word order real languages discuss usefulness realism parsability perplexity diversity synthetic languages simple demonstration use galactic dependencies consider single source transfer attempt parse real target language use parser train nearby source language find include synthetic source languages somewhat increase diversity source pool significantly improve result target languages
show predict basic word order facts novel language give corpus part speech pos sequence predict often direct object follow verbs often adjectives follow nouns general directionalities dependency relations typological properties could helpful grammar induction problem usually regard unsupervised learn innovation treat supervise learn use large collection realistic synthetic languages train data supervise learner must identify surface feature language pos sequence hand engineer neural feature correlate language deeper structure latent tree experiment show one give small set real languages help add many synthetic languages train data two system robust even pos sequence include noise three system task outperform grammar induction baseline large margin
company disclosures greatly aid process financial decision make therefore consult financial investors automate traders exercise ownership stock humans usually able correctly interpret content rarely true computerize decision support systems struggle complexity ambiguity natural language possible remedy represent deep learn overcome several shortcomings traditional methods text mine instance recurrent neural network long short term memories employ hierarchical structure together large number hide layer automatically extract feature order sequence word capture highly non linear relationships context dependent mean however deep learn recently start receive traction possibly performance largely untested hence paper study use deep neural network financial decision support additionally experiment transfer learn pre train network different corpus length one thousand, three hundred and ninety-one million word result reveal higher directional accuracy compare traditional machine learn predict stock price movements response financial disclosures work thereby help highlight business value deep learn provide recommendations practitioners executives
develop high quality multi turn dialog dataset dailydialog intrigue several aspects language human write less noisy dialogues dataset reflect daily communication way cover various topics daily life also manually label develop dataset communication intention emotion information evaluate exist approach dailydialog dataset hope benefit research field dialog systems
state art methods learn cross lingual word embeddings rely bilingual dictionaries parallel corpora recent study show need parallel data supervision alleviate character level information methods show encourage result par supervise counterparts limit pair languages share common alphabet work show build bilingual dictionary two languages without use parallel corpora align monolingual word embed space unsupervised way without use character information model even outperform exist supervise methods cross lingual task language pair experiment demonstrate method work well also distant language pair like english russian english chinese finally describe experiment english esperanto low resource language pair exist limit amount parallel data show potential impact method fully unsupervised machine translation code embeddings dictionaries publicly available
focus task identify event temporal status find events directly indirectly govern target event dependency tree important contexts therefore extract dependency chain contain context events use input neural network model consistently outperform previous model use local context word input visualization verify dependency chain representation effectively capture context events closely relate target event play key roles predict event temporal status
research trend japanese predicate argument structure pas analysis shift pointwise prediction model local feature global model design search globally optimal solutions however exist global model tend employ relatively simple local feature therefore overall performance gain rather limit importance design local model demonstrate study show performance sophisticate local model considerably improve recent feature embed methods feature combination learn base neural network outperform state art global model f1 common benchmark dataset
thesis introduce sequence sequence model luong attention mechanism end end asr also describe various neural network algorithms include batch normalization dropout residual network constitute convolutional attention base seq2seq neural network finally propose model prove effectiveness speech recognition achieve one hundred and fifty-eight phoneme error rate timit dataset
analyze customer feedback best way channelize data new market strategies benefit entrepreneurs well customers therefore automate system analyze customer behavior great demand users may write feedbacks language hence mine appropriate information often become intractable especially traditional feature base supervise model difficult build generic system one understand concern language find relevant feature order overcome propose deep convolutional neural network cnn recurrent neural network rnn base approach require handcraft feature evaluate techniques analyze customer feedback sentence four languages namely english french japanese spanish empirical analysis show model perform well four languages setups ijcnlp share task customer feedback analysis model achieve second rank french accuracy seven thousand, one hundred and seventy-five third rank languages
propose end end neural network predict geolocation tweet network take input number raw twitter metadata tweet message associate user account information model language independent despite minimal feature engineer interpretable capable learn location indicative word time pattern compare state art systems model outperform two six additionally propose extensions model compress representation learn network binary cod experiment show produce compact cod compare benchmark hash algorithms implementation model release publicly
paper revisit problem complex word identification cwi follow semeval cwi share task use ensemble classifiers investigate well computational methods discriminate complex non complex word furthermore analyze classification performance understand make lexical complexity challenge find show systems perform poorly semeval cwi dataset one reason way human annotation perform
learn phrase representations widely explore many natural language process nlp task eg sentiment analysis machine translation show promise improvements previous study either learn non compositional phrase representations general word embed learn techniques learn compositional phrase representations base syntactic structure either require huge amount human annotations easily generalize phrase work propose take advantage large scale paraphrase database present pair wise gate recurrent units pairwise gru framework generate compositional phrase representations framework use generate representations phrase experimental result show framework achieve state art result several phrase similarity task
clickbait detection tweet remain elusive challenge paper describe solution zingel clickbait detector clickbait challenge two thousand and seventeen capable evaluate tweet level click bait first reformat regression problem multi classification problem base annotation scheme perform multi classification apply token level self attentive mechanism hide state bi directional gate recurrent units bigru enable model generate tweet task specific vector representations attend important tokens self attentive neural network train end end without involve manual feature engineer detector rank first final evaluation clickbait challenge two thousand and seventeen
paper present norwegian review corpus norec create train evaluate model document level sentiment analysis full text review collect major norwegian news source cover range different domains include literature movies video game restaurants music theater addition product review across range categories review label manually assign score one six provide rat original author first release corpus comprise thirty-five thousand review distribute use conll format pre process use udpipe along rich set metadata work report paper form part sant initiative sentiment analysis norwegian text project seek provide resources tool sentiment analysis opinion mine norwegian resources sentiment analysis far unavailable norwegian norec represent highly valuable seek addition norwegian language technology
rise social media millions people routinely express moods feel daily struggle mental health issue social media platforms like twitter unlike traditional observational cohort study conduct questionnaires self report survey explore reliable detection clinical depression tweet obtain unobtrusively base analysis tweet crawl users self report depressive symptoms twitter profile demonstrate potential detect clinical depression symptoms emulate phq nine questionnaire clinicians use today study use semi supervise statistical model evaluate duration symptoms expression twitter term word usage pattern topical preferences align medical find report via phq nine proactive automatic screen tool able identify clinical depressive symptoms accuracy sixty-eight precision seventy-two
dependency treebank important resource language paper present work build bktreebank dependency treebank vietnamese important point design pos tagset dependency relations annotation guidelines discuss describe experiment pos tag dependency parse treebank experimental result show treebank useful resource vietnamese language process
script knowledge play central role text understand relevant variety downstream task paper consider two recent datasets provide rich general representation script events term paraphrase set introduce task map event mention narrative texts script event type present model task exploit rich linguistic representations well information temporal order result experiment demonstrate complex task indeed feasible
recently convolutional neural network cnns model prove remarkable result text classification sentiment analysis paper present approach task classify business review use word embeddings large scale dataset provide yelp yelp two thousand and seventeen challenge dataset compare word base cnn use several pre train word embeddings end end vector representations text review classification conduct several experiment capture semantic relationship business review use deep learn techniques prove obtain result competitive traditional methods
participate mlws two thousand and seventeen tibetan word segmentation task system train unrestricted way introduce baseline system 76w tibetan segment sentence system character sequence process baseline system word sequence subword unit bpe algorithm split rare word subwords correspond feature neural network classifier adopt token subword bmes label decode step simple rule use recover final word sequence candidate system submition select evaluate f score dev set pre extract 76w sentence experiment show method fix segmentation errors baseline system result significant performance gain
process multi word expressions mwes know problem natural language process task even neural machine translation nmt struggle overcome paper present result experiment investigate nmt attention allocation mwes improve automate translation sentence contain mwes english latvian english czech nmt systems two improvement strategies explore one bilingual pair automatically extract mwe candidates add parallel corpus use train nmt system two full sentence contain automatically extract mwe candidates add parallel corpus approach allow increase automate evaluation result best result ninety-nine bleu point increase reach first approach second approach minimal improvements achieve also provide open source software tool use mwe extraction alignment inspection
present lear lexical entailment attract repel novel post process method transform input word vector space emphasise asymmetric relation lexical entailment le also know hyponymy hypernymy relation inject external linguistic constraints eg wordnet link initial vector space le specialisation procedure bring true hyponymy hypernymy pair closer together transform euclidean space propose asymmetric distance measure adjust norms word vectors reflect actual wordnet style hierarchy concepts simultaneously joint objective enforce semantic similarity use symmetric cosine distance yield vector space specialise lexical relations lear specialisation achieve state art performance task hypernymy directionality hypernymy detection grade lexical entailment demonstrate effectiveness robustness propose asymmetric specialisation model
article present classifiers base svm convolutional neural network cnn tass two thousand and seventeen challenge tweet sentiment analysis classifier best performance general use combination svm cnn use word embeddings particularly useful improve classifiers performance
sentence representation semantic level challenge task natural language process artificial intelligence despite advance word embeddings ie word vector representations capture sentence mean open question due complexities semantic interactions among word paper present embed method aim learn unsupervised sentence representations unlabeled text propose unsupervised method model sentence weight series word embeddings weight word embeddings fit use shannon word entropies provide term frequency inverse document frequency tf idf transform hyperparameters model select accord properties data eg sentence length textual gender hyperparameter selection involve word embed methods dimensionalities well weight schemata method offer advantage exist methods identifiable modules short term train online inference unseen sentence representations well independence domain external knowledge language resources result show model outperform state art well know semantic textual similarity sts benchmarks moreover model reach state art performance compare supervise knowledge base sts systems
subjectivity detection task identify objective subjective sentence objective sentence exhibit sentiment desire sentiment analysis engine find separate objective sentence analysis eg polarity detection subjective sentence opinions often express one multiple topics aspect extraction subtask sentiment analysis consist identify opinion target opinionated text ie detect specific aspects product service opinion holder either praise complain
describe honk open source pytorch reimplementation convolutional neural network keyword spot include examples tensorflow model useful recognize command trigger speech base interfaces eg hey siri serve explicit cue audio record utterances send cloud full speech recognition evaluation google recently release speech command dataset show reimplementation comparable accuracy provide start point future work keyword spot task
lexical ambiguity impede nlp systems accurate understand semantics despite potential benefit integration sense level information nlp systems remain understudy incorporate novel disambiguation algorithm state art classification model create pipeline integrate sense level information downstream nlp applications show simple disambiguation input text lead consistent performance improvement multiple topic categorization polarity detection datasets particularly fine granularity underlie sense inventory reduce document sufficiently large result also point need sense representation research focus vivo evaluations target performance downstream nlp applications rather artificial benchmarks
paper describe complexity build lemmatizer arabic rich complex derivational morphology discuss need fast accurate lammatization enhance arabic information retrieval ir result also introduce new data set use test lemmatization accuracy efficient lemmatization algorithm outperform state art arabic lemmatization term accuracy speed share data set code public
stories vital form communication human culture employ daily persuade elicit sympathy convey message computational understand human narratives especially high level narrative structure remain limit date multiple literary theories narrative structure exist operationalization theories remain challenge develop annotation scheme consolidate extend exist narratological theories include labov waletsky one thousand, nine hundred and sixty-seven functional categorization scheme freytag one thousand, eight hundred and sixty-three pyramid dramatic tension present three hundred and sixty annotate short stories collect online source future research support approach enable systems intelligently sustain complex communications humans
paper describe systems ijcnlp two thousand and seventeen share task customer feedback analysis experiment simple neural architectures give competitive performance certain task include shallow cnn bi directional lstm architectures facebook fasttext baseline model best perform model top five systems use exact accuracy micro average f1 metrics spanish eight thousand, five hundred and twenty-eight french seventy seven thousand, three hundred and seventeen respectively task outperform model comment eight thousand, seven hundred and twenty-eight meaningless five thousand, one hundred and eighty-five tag use micro average f1 tag metric french task
embed base speaker adaptive train sit approach propose investigate paper deep neural network acoustic model approach speaker embed vectors constant give particular speaker map control network layer dependent element wise affine transformations canonicalize internal feature representations output hide layer main network control network generate speaker dependent mappings jointly estimate main network overall speaker adaptive acoustic model experiment large vocabulary continuous speech recognition lvcsr task show propose sit scheme yield superior performance widely use speaker aware train use vectors speaker adapt input feature
describe sling framework parse natural language semantic frame sling support general transition base neural network parse bidirectional lstm input encode transition base recurrent unit tbru output decode parse model train end end use text tokens input transition system design output frame graph directly without intervene symbolic representation sling framework include efficient scalable frame store implementation well neural network jit compiler fast inference parse sling implement c available download github
present unsupervised context sensitive spell correction method clinical free text use word character n gram embeddings method generate misspell replacement candidates rank accord semantic fit calculate weight cosine similarity vectorized representation candidate misspell context tune parameters model generate self induce spell error corpora perform experiment two languages english greatly outperform shelf spell correction tool manually annotate mimic iii test set counter frequency bias noisy channel model show neural embeddings successfully exploit improve upon state art dutch also outperform shelf spell correction tool manually annotate clinical record antwerp university hospital offer empirical evidence method counter frequency bias noisy channel model case well however context sensitive model implementation noisy channel model obtain high score test set establish state art dutch clinical spell correction noisy channel model
multi task learn text classification leverage implicit correlations among relate task extract common feature yield performance gain however previous work treat label task independent meaningless one hot vectors loss potential information make difficult model jointly learn three task paper propose multi task label embed convert label text classification semantic vectors thereby turn original task vector match task implement unsupervised supervise semi supervise model multi task label embed utilize semantic correlations among task make particularly convenient scale transfer task involve extensive experiment five benchmark datasets text classification show model effectively improve performances relate task semantic representations label additional information
build persona base conversation agent challenge owe lack large amount speaker specific conversation data model train paper address problem propose multi task learn approach train neural conversation model leverage conversation data across speakers type data pertain speaker speaker roles model experiment show approach lead significant improvements baseline model quality generate responses capture precisely speakers traits speak style model offer benefit algorithmically simple easy implement rely large quantities data represent specific individual speakers
wake polarize election social media lade hateful content address various limitations supervise hate speech classification methods include corpus bias huge cost annotation propose weakly supervise two path bootstrapping approach online hate speech detection model leverage large scale unlabeled data system significantly outperform hate speech detection systems train supervise manner use manually annotate data apply model large quantity tweet collect election day reveal motivations pattern inflammatory language
wake polarize election cyber world lade hate speech context accompany hate speech text useful identify hate speech however largely overlook exist datasets hate speech detection model paper provide annotate corpus hate speech context information well keep propose two type hate speech detection model incorporate context information logistic regression model context feature neural network model learn components context evaluation show model outperform strong baseline around three four f1 score combine two model improve performance another seven f1 score
rouge one first widely use evaluation metrics text summarization however assessment merely rely surface similarities peer model summaries consequently rouge unable fairly evaluate abstractive summaries include lexical variations paraphrase explore effectiveness lexical resource base model address issue adopt graph base algorithm rouge capture semantic similarities peer model summaries semantically motivate approach compute rouge score base lexical semantic similarities experiment result tac aesop datasets indicate exploit lexico semantic similarity word use summaries would significantly help rouge correlate better human judgments
automate keyphrase extraction fundamental textual information process task concern selection representative phrase document summarize content work present novel unsupervised method keyphrase extraction whose main innovation use local word embeddings particular glove vectors ie embeddings train single document consideration argue local representation word keyphrases able accurately capture semantics context document part therefore help improve keyphrase extraction quality empirical result offer evidence indeed local representations lead better keyphrase extraction result compare embeddings train large third corpora larger corpora consist several document scientific field state art unsupervised keyphrase extraction methods
verbs important semantic understand natural language traditional verb representations framenet propbank verbnet focus verbs roles roles coarse represent verbs semantics paper introduce verb pattern represent verbs semantics pattern correspond single semantic verb first analyze principles verb pattern generality specificity propose nonparametric model base description length experimental result prove high effectiveness verb pattern apply verb pattern context aware conceptualization show verb pattern helpful semantic relate task
paper propose novel deep coherence model dcm use convolutional neural network architecture capture text coherence text coherence problem investigate new perspective learn sentence distributional representation text coherence model simultaneously particular model capture interactions sentence compute similarities distributional representations easily train end end fashion propose model evaluate standard sentence order task experimental result demonstrate effectiveness promise coherence assessment show significant improvement state art wide margin
paper problem disambiguate target word polish approach search relate word know mean relatives use build train corpus unannotated text technique improve propose new rich source replacements substitute traditional requirement monosemy heuristics base wordnet relations nai bayesian classifier modify account unknown distribution sense corpus six hundred million web document five hundred and ninety-four billion tokens gather nekst search engine allow us assess relationship train set size disambiguation accuracy classifier evaluate use wordnet baseline corpus seventeen thousand, three hundred and fourteen manually annotate occurrences fifty-four ambiguous word
internet revolutionize healthcare offer medical information ubiquitously patients via web search healthcare status complex medical information need patients express diversely implicitly medical text query aim better capture focus picture user medical relate information search would insights healthcare information access strategies challenge yet reward detect structure user intentions diversely express medical text query introduce graph base formulation explore structure concept transition effective user intent detection medical query node represent medical concept mention direct edge indicate medical concept transition deep model base multi task learn introduce extract structure semantic transition user query model extract word level medical concept mention well sentence level concept transition collectively customize graph base mutual transfer loss function design impose explicit constraints exploit contribution mention medical concept word implication semantic transition observe eight relative improvement auc twenty-three relative reduction coverage error compare propose model best baseline model concept transition inference task real world medical text query
explore representational space emotions combine methods different academic field cognitive science propose appraisal theory view human emotion previous research show human rat abstract event feature predict fine grain emotions capture similarity space neural pattern mentalizing brain regions time natural language process nlp demonstrate transfer multitask learn use cope scarcity annotate data text model contribution work show appraisal theory combine nlp mutual benefit first fine grain emotion prediction improve human level performance use nlp representations addition appraisal feature second use appraisal feature auxiliary target train improve predictions even text available input third obtain representation similarity matrix better correlate neural activity across regions best result achieve model train simultaneously predict appraisals emotions emojis use share representation result preliminary integration cognitive neuroscience nlp techniques open interest direction future research
semantic similarity two sentence define way determine relate unrelated two sentence task semantic similarity term distribute representations think generate sentence embeddings dense vectors take context mean sentence account embeddings produce multiple methods paper try evaluate lstm auto encoders generate embeddings unsupervised algorithms auto encoders specific try recreate input force learn order inherent mean extent create proper bottleneck try evaluate properly algorithms train plain english sentence learn figure semantic similarity without give sense mean sentence
work relation extraction form prediction look short span text within single sentence contain single entity pair mention however many relation type particularly biomedical text express across sentence require large context disambiguate propose model consider mention entity pair simultaneously order make prediction encode full paper abstract use efficient self attention encoder form pairwise predictions mention bi affine operation entity pair wise pool aggregate mention pair score make final prediction alleviate train noise perform within document multi instance learn improve model performance jointly train model predict name entities add additional corpus weakly label data demonstrate model effectiveness achieve state art biocreative v chemical disease relation dataset model without kb resources outperform ensembles model use hand craft feature additional linguistic resources
importance build sentiment analysis tool arabic social media recognize past couple years especially rapid increase number arabic social media users one main difficulties tackle problem text within social media mostly colloquial many dialects use within social media platforms paper present set feature integrate machine learn base sentiment analysis model apply egyptian saudi levantine msa arabic social media datasets many propose feature derive use arabic sentiment lexicon model also present emoticon base feature well input text relate feature number segment within text length text whether text end question mark etc show present feature result increase accuracy across six seven datasets experiment benchmarked since develop model perform exist arabic sentiment analysis systems publicly available datasets state model present state art arabic sentiment analysis
paper describe two systems use author address arabic sentiment analysis part semeval two thousand and seventeen task four author participate three arabic relate subtasks subtask message polarity classification sub task b topic base message polarity classification subtask tweet quantification use team name niletmrg subtask make use previously develop sentiment analyzer augment score lexicon subtasks b use ensemble three different classifiers first classifier convolutional neural network train word2vec word embeddings second classifier consist multilayer perceptron third classifier logistic regression model take input second classifier vote three classifiers use determine final outcome output task b quantify produce result task three arabic relate task niletmrg participate team rank number one
manual creation gold standards name entity recognition entity link time resource intensive moreover recent work show gold standards contain large proportion mistake addition difficult maintain hence present bengal novel automatic generation gold standards complement manually create benchmarks main advantage benchmarks readily generate time also cost effective guarantee free annotation errors compare performance eleven tool benchmarks english generate bengal 16benchmarks create manually show approach port easily across languages present result achieve four tool brazilian portuguese spanish overall result suggest automatic benchmark generation approach create vary benchmarks characteristics similar exist benchmarks approach open source experimental result available http faturlcom bengalexpinlg code https githubcom dice group bengal
paper present result participation clickbait detection challenge two thousand and seventeen system rely fusion neural network incorporate different type available informations require linguistic preprocessing hence generalize easily new domains languages final combine model achieve mean square error four hundred and twenty-eight accuracy eight hundred and twenty-six f1 score five hundred and sixty-four accord official evaluation metric system rank 6th thirteen participate team
social media platforms grow important medium spread information event publish traditional media news article group diverse source information discuss topic vary perspectives provide new insights gap word usage informal social media content tweet diligently write content eg news article make assemble difficult paper propose transformation framework bridge word usage gap tweet online news article across languages leverage word embeddings use framework word embeddings extract tweet news article align closer across languages thus facilitate identification similarity news article tweet experimental result show notable improvement baselines monolingual tweet news article comparison new find report cross lingual comparison
literary analysis criticism study largely value field dedicate journals researchers remain mostly within humanities scope text analytics computer aid process derive information texts article describe simple generic model perform literary analysis use text analytics method rely statistical measure one token sentence size two wordnet synset feature measure use principal component analysis texts analyze observe shakespeare bible regard reference literature model validate analyze select work jam joyce one thousand, eight hundred and eighty-two one thousand, nine hundred and forty-one one important writers 20th century discuss consistency approach reason use techniques eg part speech tag ways analysis model might adapt enhance
paper investigate application text classification methods support law professionals present several experiment apply machine learn techniques predict high accuracy rule french supreme court law area case belong also investigate influence time period rule make form case description extent need mask information full case rule automatically obtain train test data resemble case descriptions develop mean probability ensemble system combine output multiple svm classifiers report result ninety-eight average f1 score predict case rule ninety-six f1 score predict law area case eight thousand, seven hundred and seven f1 score estimate date rule
present novel transition system base covington non projective parser introduce non local transition directly create arc involve nod leave current focus position avoid need long sequence arc transition create long distance arc thus alleviate error propagation result parser outperform original version achieve best accuracy stanford dependencies conversion penn treebank among greedy transition base algorithms
present one simple model multilingual text classification require parallel data base traditional support vector machine classifier exploit multilingual word embeddings character n grams model simple easily extendable yet effective overall rank 1st twelve team ijcnlp two thousand and seventeen share task customer feedback analysis four languages english french japanese spanish
develop stream keyword spot systems use recurrent neural network transducer rnn model neural end end train sequence sequence model jointly learn acoustic language model components model train predict either phonemes graphemes subword units thus allow us detect arbitrary keyword phrase without vocabulary word order adapt model requirements keyword spot propose novel technique bias rnn system towards specific keyword interest systems compare strong sequence train connectionist temporal classification ctc base keyword filler baseline augment separate phoneme language model overall rnn system propose bias technique significantly improve performance baseline system
paper demonstrate importance coreference resolution natural language process example tac slot fill share task illustrate strengths weaknesses automatic coreference resolution systems provide experimental result show improve performance slot fill end end set finally publish kbpchains resource contain automatically extract coreference chain tac source corpus order support researchers work topic
distant supervision relation extraction use heuristically align text data exist knowledge base train data unsupervised nature technique allow scale web scale relation extraction task expense noise train data previous work explore relationships among instance entity pair reduce noise relationships among instance across entity pair fully exploit explore use inter instance couple base verb phrase entity type similarities propose novel technique candis cast distant supervision use inter instance couple end end neural network model candis incorporate attention module instance level model multi instance nature problem candis outperform exist state art techniques standard benchmark dataset
explore application deep residual learn dilate convolutions keyword spot task use recently release google speech command dataset benchmark best residual network resnet implementation significantly outperform google previous convolutional neural network term accuracy vary model depth width achieve compact model also outperform previous small footprint variants knowledge first examine approach keyword spot result establish open source state art reference support development future speech base interfaces
connectionist temporal classification ctc popular sequence prediction approach automatic speech recognition typically use model base recurrent neural network rnns explore whether deep convolutional neural network cnns use effectively instead rnns encoder ctc cnns lack explicit representation entire sequence advantage much faster train present exploration cnns encoders ctc model context character base lexicon free automatic speech recognition particular explore range one dimensional convolutional layer particularly efficient compare performance cnn base model typical rnnbased model term train time decode time model size word error rate wer switchboard eval2000 corpus find cnn base model close performance lstms match much faster train decode
grammar induction task learn grammar set examples recently neural network show powerful learn machine identify pattern stream data work investigate effectiveness induce regular grammar data without assumptions grammar train recurrent neural network distinguish string outside regular language utilize algorithm extract learn finite state automaton apply method several regular languages find unexpected result regard connections network state may regard evidence generalization
attention model intensively study improve nlp task machine comprehension via question aware passage attention model self match attention model research propose phase conductor phasecond attention model two meaningful ways first phasecond architecture multi layer attention model consist multiple phase implement stack attention layer produce passage representations stack inner outer fusion layer regulate information flow second extend improve dot product attention function phasecond simultaneously encode multiple question passage embed layer different perspectives demonstrate effectiveness propose model phasecond squad dataset show model significantly outperform state art single layer multiple layer attention model deepen result new find via detail qualitative analysis visualize examples show dynamic change multi layer attention model
ever since successful application sequence sequence learn neural machine translation systems interest surge applicability towards language generation problem domains recent work investigate use neural architectures towards model open domain conversational dialogue find although model capable learn good distributional language model dialogue coherence still concern unlike translation conversation much one many map utterance response even press model aware precede flow conversation paper propose tackle problem introduce previous conversational context term latent representations dialogue act time inject latent context representations sequence sequence neural network form dialog act use second encoder enhance quality coherence conversations generate main task research work show add latent variables capture discourse relations indeed result coherent responses compare conventional sequence sequence model
distribute word representations show useful various natural language process nlp application task word vectors learn huge corpora often carry semantic syntactic information word however well know individual user language pattern different factor interest topics friend group social activities word habit etc may imply kind personalize semantics personalize semantics word may imply slightly differently different users example word cappuccino may imply leisure joy excellent user enjoy coffee kind drink someone else personalize semantics course carry standard universal word vectors train huge corpora produce many people paper propose framework train different personalize word vectors different users base successful continuous skip gram model use social network data post many individual users framework universal background word vectors first learn background corpora adapt personalize corpus individual user learn personalize word vectors use two application task evaluate quality personalize word vectors obtain way user prediction task sentence completion task personalize word vectors show carry personalize semantics offer improve performance two evaluation task
fine grain entity type aim assign entity mention free text type arrange hierarchical structure traditional distant supervision base methods employ structure data source weak supervision need hand label data neglect label noise automatically label train corpus although recent study use many feature prune wrong data ahead train suffer error propagation bring much complexity paper propose end end type model call path base attention neural model pan learn noise robust performance leverage hierarchical structure type experiment demonstrate effectiveness
present direct assessment method manually assess quality automatically generate caption video evaluate accuracy video caption particularly difficult give video clip definitive grind truth correct answer measure automatic metrics compare automatic video caption manual caption bleu meteor draw techniques use evaluate machine translation use trecvid video caption task two thousand and sixteen show weaknesses work present bring human assessment evaluation crowdsourcing well caption describe video automatically degrade quality sample caption assess manually able rate quality human assessors factor take account evaluation use data trecvid video text task two thousand and sixteen show direct assessment method replicable robust scale many caption generation techniques evaluate
several dialog frameworks allow manual specification intents rule base dialog flow rule base framework provide good control dialog designers expense time consume laborious job dialog designer reduce could identify pair user intents correspond responses automatically prior conversations users agents paper propose approach find frequent user utterances serve examples intents correspond agent responses propose novel simcluster algorithm extend standard k mean algorithm simultaneously cluster user utterances agent utterances take adjacency information account method also align cluster provide pair intents response group compare result produce use simple kmeans cluster real dataset observe upto ten absolute improvement f1 score experiment synthetic dataset show algorithm gain advantage k mean algorithm data large variance
paper describe japanese english subtitle corpus jesc jesc large japanese english parallel corpus cover underrepresented domain conversational dialogue consist thirty-two million examples make largest freely available dataset kind corpus assemble crawl align subtitle find web assembly process incorporate number novel preprocessing elements ensure high monolingual fluency accurate bilingual alignments summarize content evaluate quality use human experts baseline machine translation mt systems
consider problem adapt neural paragraph level question answer model case entire document give input propose solution train model produce well calibrate confidence score result individual paragraph sample multiple paragraph document train use share normalization train objective encourage model produce globally correct output combine method state art pipeline train model document qa data experiment demonstrate strong performance several document qa datasets overall able achieve score seven hundred and thirteen f1 web portion triviaqa large improvement five hundred and sixty-seven f1 previous best system
goal work design machine translation mt system low resource family dialects collectively know swiss german widely speak switzerland seldom write collect significant number parallel write resources start total 60k word moreover identify several promise data source swiss german design compare three strategies normalize swiss german input order address regional diversity find character base neural mt best solution text normalization combination phrase base statistical mt solution reach thirty-six bleu score translate bernese dialect value however decrease test data become remote train one geographically topically resources normalization techniques first step towards full mt swiss german dialects
paper present new annotate corpus five hundred and thirteen anonymized radiology report write spanish report manually annotate entities negation uncertainty term relations corpus conceive evaluation resource name entity recognition relation extraction algorithms input use supervise methods biomedical annotate resources scarce due confidentiality issue associate cost work provide guidelines could help researchers undertake similar task
recently encoder decoder model widely use social media text summarization however model sometimes select noise word irrelevant sentence part summary error thus decline performance order inhibit irrelevant sentence focus key information propose effective approach learn sentence weight distribution model build multi layer perceptron predict sentence weight train use rouge score alternative estimate sentence weight try minimize gap estimate weight predict weight way encourage model focus key sentence high relevance summary experimental result show approach outperform baselines large scale social media corpus
recent years research devote study subtask complete shallow discourse parse indentifying discourse connective arguments connective need design full discourse parser pull subtasks together develop discourse parser turn free text discourse relations parser include connective identifier arguments identifier sense classifier non explicit identifier connect pipeline component apply maximum entropy model abundant lexical syntax feature extract penn discourse tree bank head base representation pdtb adopt arguments identifier turn problem indentifying arguments discourse connective find head end arguments non explicit identifier contextual type feature like word high frequency reflect discourse relation introduce improve performance non explicit identifier compare methods experimental result achieve considerable performance
study problem response selection multi turn conversation retrieval base chatbots task require match response candidate conversation context whose challenge include recognize important part context model relationships among utterances context exist match methods may lose important information contexts interpret unify framework contexts transform fix length vectors without interaction responses match analysis motivate us propose new match framework sufficiently carry important information contexts match model relationships among utterances time new framework call sequential match framework smf let us utterance context interact response candidate first step transform pair match vector match vectors accumulate follow order utterances context recurrent neural network rnn model relationships among utterances context response match finally calculate hide state rnn smf propose sequential convolutional network sequential attention network conduct experiment two public data set test performance experimental result show model significantly outperform state art match methods also show model interpretable visualizations provide us insights capture leverage important information contexts match
follow technical report present formal approach probabilistic minimalist grammar parameter estimation describe formalization minimalist grammar present algorithm application variational bayesian inference formalization
captchas base read text susceptible machine learn base attack due recent significant advance deep learn dl address paper promote image visual caption base captchas robust machine learn base attack develop image visual caption base captchas paper propose new image caption architecture exploit tensor product representations tpr structure neural symbolic framework develop cognitive science past twenty years aim integrate dl explicit language structure rule call tensor product generation network tpgn key ideas tpgn one unsupervised learn role unbind vectors word via tpr base deep neural network two integration tpr typical dl architectures include long short term memory lstm model novelty approach lie ability generate sentence extract partial grammatical structure sentence use role unbind vectors obtain unsupervised manner experimental result demonstrate effectiveness propose approach
introduce neural method transfer learn two source target classification task aspects domain rather train target label use keywords pertain source target aspects indicate sentence relevance instead document class label document encode learn embed softly select relevant sentence aspect dependent manner share classifier train source encode document label apply target encode document ensure transfer aspect adversarial train encode document set aspect invariant experimental result demonstrate approach outperform different baselines model variants two datasets yield improvement twenty-seven pathology dataset five review dataset
paper describe system create detect stance online discussions goal identify whether author comment favor give target approach base maximum entropy classifier use surface level sentiment domain specific feature system originally develop detect stance english tweet adapt process czech news commentaries
deep stack rnns usually hard train add shortcut connections across different layer common way ease train stack network however extra shortcuts make recurrent step complicate simply stack architecture propose framework call shortcut block marriage gate mechanism shortcuts discard self connect part lstm cell present extensive empirical experiment show design make train easy improve generalization propose various shortcut block topologies compositions explore effectiveness base architecture obtain six relatively improvement state art ccgbank supertagging dataset also get comparable result pos tag task
work present result systematic study investigate commercial benefit automatic text summarization systems real world scenario specifically define use case context media monitor media response analysis claim even use simple query base extractive approach dramatically save process time employees without significantly reduce quality work
rapid growth social media web emotional polarity computation become flourish frontier text mine community however challenge understand latest trend summarize state general opinions products due big diversity size social media data create need automate real time opinion extraction mine hand bulk current research devote study subjective sentence contain opinion keywords limit work report objective statements imply sentiment paper fuzzy base knowledge engineer model develop sentiment classification special group sentence include change deviation desire range value drug review rich source statements therefore research experiment carry patient review several different cholesterol lower drug determine sentiment polarity main conclusion study order increase accuracy level exist drug opinion mine systems objective sentence imply opinion take account experimental result demonstrate propose model obtain seventy-two percent f1 value
much like sentence compose word word compose smaller units example english word questionably analyze questionablely however structural decomposition word directly give us semantic representation word mean since morphology obey principle compositionality semantics word systematically derive mean part work propose novel probabilistic model word formation capture analysis word w constituents segment synthesis mean w mean segment model jointly learn segment word morphemes compose distributional semantic vectors morphemes experiment model english celex data german derivbase zeller et al two thousand and thirteen data show jointly model semantics increase segmentation accuracy morpheme f1 three five additionally investigate different model vector composition show recurrent neural network yield improvement simple additive model finally study degree representations correspond linguist notion morphological productivity
deep learn techniques increasingly popular textual entailment task overcome fragility traditional discrete model hard alignments logics particular recently propose attention model rocktaschel et al two thousand and fifteen wang jiang two thousand and fifteen achieve state art accuracy compute soft word alignments premise hypothesis sentence however remain major limitation line work completely ignore syntax recursion helpful many traditional efforts show beneficial extend attention model tree nod premise hypothesis importantly subtree level attention reveal information entailment relation study recursive composition subtree level entailment relation view soft version natural logic framework maccartney man two thousand and nine experiment show structure attention entailment composition model correctly identify infer entailment relations bottom bring significant improvements accuracy
classification crime discrete categories entail massive loss information crimes emerge complex mix behaviors situations yet detail capture singular crime type label information loss impact ability understand cause crime also develop optimal crime prevention strategies apply machine learn methods short narrative text descriptions accompany crime record goal discover ecologically meaningful latent crime class term latent class crime topics reference text base topic model methods produce use topic distributions measure cluster among formally recognize crime type crime topics replicate broad distinctions violent property crime also reveal nuances link target characteristics situational condition tool methods attack formal crime type discrete topic space rather crime type distribute across range crime topics similarly individual crime topics distribute across range formal crime type key ecological group include identity theft shoplift burglary theft car crimes vandalism criminal threats confidence crimes violent crimes though replacement formal legal crime classifications crime topics provide unique window heterogeneous causal process underlie crime
reproduce experiment important instrument validate previous work build upon exist approach tackle numerous time different areas science paper introduce empirical replicability study three well know algorithms syntactic centric aspect base opinion mine show reproduce result continue difficult endeavor mainly due lack detail regard preprocessing parameter set well due absence available implementations clarify detail consider important threats validity research field specifically compare problems nlp public datasets code availability critical validity components conclude encourage code base research think key role help researchers understand mean state art better generate continuous advance
previous research show learn multiple representations polysemous word improve performance word embeddings many task however lead another problem several vectors word may actually point mean namely pseudo multi sense paper introduce concept pseudo multi sense propose algorithm detect case consideration detect pseudo multi sense case try refine exist word embeddings eliminate influence pseudo multi sense moreover apply algorithm previous release multi sense word embeddings test artificial word similarity task analogy task result experiment show diminish pseudo multi sense improve quality word representations thus method actually efficient way reduce linguistic complexity
analyze limitations future directions extractive summarization paradigm paper propose integer linear program ilp formulation obtain extractive oracle summaries term rouge n also propose algorithm enumerate oracle summaries set reference summaries exploit f measure evaluate system summaries contain many sentence extract oracle summary experimental result obtain document understand conference duc corpora demonstrate follow one room still exist improve performance extractive summarization two f measure derive enumerate oracle summaries significantly stronger correlations human judgment derive single oracle summaries
cross lingual dependency annotation projection information often lose transfer early decode present end end graph base neural network dependency parser train reproduce matrices edge score directly project across word alignments show approach cross lingual dependency parse simpler also achieve absolute improvement two hundred and twenty-five average across ten languages compare previous state art
neural machine translation nmt new approach machine translation mt due success absorb attention many researchers field paper study nmt model persian english language pair analyze model investigate appropriateness model scarce resourced scenarios situation exist persian center translation systems adjust model persian language find best parameters hyper parameters two task translation transliteration also apply preprocessing task persian dataset yield increase one point term bleu score also modify loss function enhance word alignment model new loss function yield total one hundred and eighty-seven point improvements term bleu score translation quality
identify different varieties language challenge unrelated languages identification paper propose approach discriminate language varieties dialects mandarin chinese mainland china hong kong taiwan macao malaysia singapore aka greater china region gcr apply dialects identification gcr find commonly use character level word level uni gram feature efficient since exist several specific problems ambiguity context dependent characteristic word dialects gcr overcome challenge use general feature like character level n gram also many new word level feature include pmi base word alignment base feature series evaluation result news open domain dataset wikipedia show effectiveness propose approach
paper focus personalize response generation conversational systems base sequence sequence learn especially encoder decoder framework propose two phase approach namely initialization adaptation model respond style human generate personalize responses evaluation propose novel human aid method evaluate performance personalize response generation model online real time conversation offline human judgement moreover lexical divergence responses generate five personalize model indicate propose two phase approach achieve good result model respond style human generate personalize responses conversational systems
work study comparatively two typical sentence match task textual entailment te answer selection observe weaker phrase alignments critical te stronger phrase alignments deserve attention key reach observation lie phrase detection phrase representation phrase alignment importantly connect align phrase different match degrees final classifier prior work limitations phrase generation representation ii conduct alignment word phrase level handcraft feature iii utilize single framework alignment without consider characteristics specific task limit framework effectiveness across task propose architecture base gate recurrent unit support representation learn phrase arbitrary granularity ii task specific attentive pool phrase alignments two sentence experimental result te match observation show effectiveness approach
outline bidirectional translation system convert sentence american sign language asl english vice versa perform machine translation asl english utilize generative approach specifically employ adjustment ibm word alignment model one ibm wam1 define language model english asl well translation model attempt generate translation maximize posterior distribution define model use model able quantify concepts fluency faithfulness translation languages
name entity recognition ner key nlp task challenge web user generate content diverse continuously change language paper aim quantify diversity impact state art ner methods measure name entity ne context variability feature sparsity effect precision recall particular find indicate ner approach struggle generalise diverse genres limit train data unseen nes particular play important role higher incidence diverse genres social media regular genres newswire couple higher incidence unseen feature generally lack large train corpora lead significantly lower f1 score diverse genres compare regular ones also find lead systems rely heavily surface form find train data problems generalise beyond offer explanations observation
aim would light strengths weaknesses newly introduce neural machine translation paradigm end conduct multifaceted evaluation compare output produce state art neural machine translation phrase base machine translation systems nine language directions across number dimension specifically measure similarity output fluency amount reorder effect sentence length performance across different error categories find translations produce neural machine translation systems considerably different fluent accurate term word order compare produce phrase base systems neural machine translation systems also accurate produce inflect form perform poorly translate long sentence
first step process question question answeringqa systems carry detail analysis question purpose determine ask perfectly approach answer question analysis use several techniques analyze question give natural language stanford pos tagger parser arabic language name entity recognizer tokenizerstop word removal question expansion question classification question focus extraction components employ numerous detection rule train classifier use feature analysis detect important elements question include one portion question refer answer focus two different term question identify type entity ask lexical answer type three question expansion four process classify question one several different type describe elements identify evaluate effect accurate detection question answer system use mean reciprocal rankmrr accuracy measure
discourse parse integral part understand information flow argumentative structure document previous research focus induce evaluate model english rst discourse treebank however discourse treebanks languages exist include spanish german basque dutch brazilian portuguese treebanks share underlie linguistic theory differ slightly way document annotate paper present new discourse parser simpler yet competitive significantly better two three metrics state art english b harmonization discourse treebanks across languages enable us present c best knowledge first experiment cross lingual discourse parse
distinguish antonyms synonyms key task achieve high performance nlp systems notoriously difficult distinguish distributional co occurrence model pattern base methods prove effective differentiate relations paper present novel neural network model antsynnet exploit lexico syntactic pattern syntactic parse tree addition lexical syntactic information successfully integrate distance relate word along syntactic path new pattern feature result classification experiment show antsynnet improve performance prior pattern base methods
report propose new application twitter data call textitjob detection identify people job category base tweet preliminary work limit task identify workers job holders use compare simple bag word model document representation base skip gram model result show model base skip gram achieve seventy-six precision eighty-two recall
report effort identify sensitive information subset data items list hipaa health insurance portability accountability medical text use recent advance natural language process machine learn techniques represent word high dimensional continuous vectors learn variant word2vec call continous bag word cbow fee word vectors simple neural network long short term memory lstm architecture without attempt extract manually craft feature consider medical dataset small feed neural network obtain promise result result thrill us think larger scale project precise parameter tune possible improvements
propose udp first train free parser universal dependencies ud algorithm base pagerank small set head attachment rule feature two step decode guarantee function word attach leaf nod parser require train competitive delexicalized transfer system udp offer linguistically sound unsupervised alternative cross lingual parse ud use baseline systems parser parameters distinctly robust domain change across languages
sequence sequence model apply conversation response generation problem source sequence conversation history target sequence response unlike translation conversation respond inherently creative generation long informative coherent diverse responses remain hard task work focus single turn set add self attention decoder maintain coherence longer responses propose practical approach call glimpse model scale large datasets introduce stochastic beam search algorithm segment segment reranking let us us inject diversity earlier generation process train combine data set 23b conversation message mine web human evaluation study method produce longer responses overall higher proportion rat acceptable excellent length increase compare baseline sequence sequence model explicit length promotion back strategy produce better responses overall full spectrum lengths
paper propose novel domain adaptation method name mix fine tune neural machine translation nmt combine two exist approach namely fine tune multi domain nmt first train nmt model domain parallel corpus fine tune parallel corpus mix domain domain corpora corpora augment artificial tag indicate specific domains empirically compare propose method fine tune multi domain methods discuss benefit shortcomings
large amount insight social discovery potential mine crowd source comment leave popular news forums like redditcom tumblrcom facebookcom hacker news unfortunately due overwhelm amount participation vary quality commentary extract value data always obvious timely design efficient single pass adaptive natural language filter quickly prune spam noise copy cat market diversions context post remove third entries return comment higher probability relatedness original article question approach present use adaptive two step filter process first leverage original article post thread start corpus parse comment match intersect word term ratio balance per sentence grow corpus add new word harvest high match comment increase filter accuracy time
consider task predict literary text gold standard human rat aside standard bigram baseline apply rich syntactic tree fragment mine train set series hand pick feature model first distinguish degrees highly less literary novels use variety lexical syntactic feature explain seven hundred and sixty variation literary rat
language identification common first step natural language process want automatically determine language input text monolingual language identification assume give document write one language multilingual language identification document usually two three languages want name aim one step propose method textual language identification languages change arbitrarily goal identify span languages method base bidirectional recurrent neural network perform well monolingual multilingual language identification task six datasets cover one hundred and thirty-one languages method keep accuracy also short document across domains ideal shelf use without preparation train data
consider entity level sentiment analysis arabic morphologically rich language increase resources present system apply complex post write response arabic newspaper article goal identify important entity target within post along polarity express target achieve significant improvements multiple baselines demonstrate use specific morphological representations improve performance identify important target sentiment use distributional semantic cluster boost performances representations especially richer linguistic resources available
paper focus automatic multi label document classification czech text document current approach usually use pre process negative impact loss information additional implementation work etc therefore would like omit use deep neural network learn simple feature choice motivate successful usage many machine learn field two different network compare first one standard multi layer perceptron second one popular convolutional network experiment czech newspaper corpus show network significantly outperform baseline method use rich set feature maximum entropy classifier also show convolutional network give best result
paper describe qcri machine translation systems iwslt two thousand and sixteen evaluation campaign participate arabic english english arabic track build phrase base neural machine translation model effort probe whether newly emerge nmt framework surpass traditional phrase base systems arabic english language pair train strong phrase base system include big language model operation sequence model neural network joint model class base model along different domain adaptation techniques mml filter mixture model use fine tune nnjm model however neural mt system train stack data different genres fine tune apply ensemble eight model beat strong phrase base system significant two bleu point margin arabic english direction obtain similar gain direction still able outperform phrase base system also apply system combination phrase base nmt output
many natural language understand nlu task shallow parse ie text chunk semantic slot fill require assignment representative label meaningful chunk sentence current deep neural network dnn base methods consider task sequence label problem word rather chunk treat basic unit label chunk infer standard iob inside outside begin label paper propose alternative approach investigate use dnn sequence chunk propose three neural model chunk treat complete unit label experimental result show propose neural sequence chunk model achieve start art performance text chunk slot fill task
work propose contextual language model incorporate dialog level discourse information language model previous work contextual language model treat precede utterances sequence input without consider dialog interactions design recurrent neural network rnn base contextual language model specially track interactions speakers dialog experiment result switchboard dialog act corpus show propose model outperform conventional single turn base rnn language model thirty-three perplexity propose model also demonstrate advantageous performance competitive contextual language model
consider task identify attitudes towards give set entities text conventionally task decompose two separate subtasks target detection identify whether entity mention text either explicitly implicitly polarity classification classify exact sentiment towards identify entity target positive negative neutral instead show attitude identification solve end end machine learn architecture two subtasks interleave deep memory network way signal produce target detection provide clue polarity classification reversely predict polarity provide feedback identification target moreover treatments set target also influence learn representations may share semantics target vary others propose deep memory network attnet outperform methods consider interactions subtasks among target include conventional machine learn methods state art deep learn model
study present analysis regard performance state art phrase base statistical machine translation smt multiple indian languages report baseline systems several language pair motivation study promote development smt linguistic resources language pair current state art quite bleak due sparse data resources success smt system contingent availability large parallel corpus data necessary reliably estimate translation probabilities report performance baseline systems translate indian languages bengali guajarati hindi malayalam punjabi tamil telugu urdu english average ten accurate result language pair
identify level expertise users important system since lead better interaction adaptation techniques furthermore information use offline process root analysis however much effort put automatically identify level expertise user especially dialog base interactions paper present approach base specific set task relate feature base distribution feature among two class novice expert use random forest classification approach furthermore use support vector machine classifier order perform result comparison apply approach data real system let us go obtain preliminary result consider positive give difficulty task lack compete approach comparison
argumentation mine several sub task argumentation component type classification relation classification exist research tend solve sub task separately ignore close relation paper present joint framework incorporate logical relation sub task improve performance argumentation structure generation design objective function combine predictions individual model sub task solve problem constraints construct background knowledge evaluate propose model two public corpora experiment result show model outperform baseline use separate model significantly sub task model also show advantage component relate sub task compare state art joint model base evidence graph
paper propose novel mechanism enrich feature vector task sarcasm detection cognitive feature extract eye movement pattern human readers sarcasm detection challenge research problem importance nlp applications review summarization dialog systems sentiment analysis well recognize sarcasm often trace incongruity become apparent full sentence unfold presence incongruity implicit explicit affect way readers eye move text observe difference behaviour eye read sarcastic non sarcastic sentence motivate observation augment traditional linguistic stylistic feature sarcasm detection cognitive feature obtain readers eye movement data perform statistical classification use enhance feature set obtain augment cognitive feature improve sarcasm detection thirty-seven term f score performance best report system
sentiments express user generate short text sentence nuanced subtleties lexical syntactic semantic pragmatic level address propose augment traditional feature use sentiment analysis sarcasm detection cognitive feature derive eye movement pattern readers statistical classification use enhance feature set improve performance f score polarity detection maximum thirty-seven ninety-three two datasets systems use traditional feature perform feature significance analysis experiment hold dataset show cognitive feature indeed empower sentiment analyzers handle complex construct
general analysis name entities receive substantial research attention unstructured well structure data analysis relations among name entities receive limit focus fact review literature reveal deficiency research abstract conceptualization require organize relations believe abstract conceptualization benefit various communities applications natural language process information extraction machine learn ontology engineer paper present comprehensive event ontology cevo build levin conceptual hierarchy english verbs categorize verbs share mean syntactic behavior present fundamental concepts requirements ontology furthermore present three use case employ cevo ontology annotation task annotate relations plain text ii annotate ontological properties iii link textual relations ontological properties use case demonstrate benefit use cevo annotation annotate english verbs abstract conceptualization ii play role upper ontology organize ontological properties iii facilitate annotation text relations use underlie vocabulary resource available https shekarpourgithubio cevoio use https w3idorg cevo namespace
introduce multi modal attention base neural machine translation nmt model incorporate visual feature different part encoder decoder utilise global image feature extract use pre train convolutional neural network incorporate word source sentence ii initialise encoder hide state iii additional data initialise decoder hide state experiment evaluate different strategies incorporate global image feature compare ones perform best also study impact add synthetic multi modal multilingual data bring find additional data positive impact multi modal model report new state art result best model also significantly improve comparable phrase base statistical mt pbsmt model train multi30k data set accord metrics evaluate best knowledge first time purely neural model significantly improve pbsmt model metrics evaluate data set
paper draw intuition turing test propose use adversarial train open domain dialogue generation system train produce sequence indistinguishable human generate dialogue utterances cast task reinforcement learn rl problem jointly train two systems generative model produce response sequence discriminator analagous human evaluator turing test distinguish human generate dialogues machine generate ones output discriminator use reward generative model push system generate dialogues mostly resemble human dialogues addition adversarial train describe model adversarial evaluation use success fool adversary dialogue evaluation metric avoid number potential pitfalls experimental result several metrics include adversarial evaluation demonstrate adversarially train system generate higher quality responses previous baselines
introduce simple general strategy manipulate behavior neural decoder enable generate output specific properties interest eg sequence pre specify length model think simple version actor critic model use interpolation actor mle base token generation policy critic value function estimate future value desire property decision make demonstrate approach able incorporate variety properties handle standard neural sequence decoders sequence length backward probability probability source give target addition yield consistent improvements abstractive summarization machine translation property optimize bleu rouge score
study multi turn response generation chatbots response generate accord conversation context exist work model hierarchy context pay enough attention fact word utterances context differentially important result may lose important information context generate irrelevant responses propose hierarchical recurrent attention network hran model aspects unify framework hran hierarchical attention mechanism attend important part within among utterances word level attention utterance level attention respectively word level attention hide vectors word level encoder synthesize utterance vectors feed utterance level encoder construct hide representations context hide vectors context process utterance level attention form context vectors decode response empirical study automatic evaluation human judgment show hran significantly outperform state art model multi turn response generation
paper aim make lack document baselines hungarian language model various approach evaluate three publicly available hungarian corpora perplexity value comparable model similar size english corpora report new freely downloadable hungar ian benchmark corpus introduce
paper task emotion recognition speech consider propose approach use deep recurrent neural network train sequence acoustic feature calculate small speech intervals time special probabilistic nature ctc loss function allow consider long utterances contain emotional neutral part effectiveness approach show two ways firstly comparison recent advance field carry secondly human performance task measure criteria show high quality propose method
users social media spread racist sexist otherwise hateful content purpose train hate speech detection system reliability annotations crucial universally agree upon definition collect potentially hateful message ask two group internet users determine whether hate speech whether ban rate degree offensiveness one group show definition prior complete survey aim assess whether hate speech annotate reliably extent exist definitions accordance subjective rat result indicate show users definition cause partially align opinion definition improve reliability low overall conclude presence hate speech perhaps consider binary yes decision raters need detail instructions annotation
recent application rnn encoder decoder model result substantial progress fully data drive dialogue systems evaluation remain challenge adversarial loss could way directly evaluate extent generate dialogue responses sound like come human could reduce need human evaluation directly evaluate generative task work investigate idea train rnn discriminate dialogue model sample human generate sample although find evidence setup could viable also note many issue remain practical application discuss aspects conclude future work warrant
simultaneous administration multiple drug synergistic antagonistic effect one drug affect activities drug synergistic effect lead improve therapeutic outcomes whereas antagonistic effect life threaten may lead increase healthcare cost may even death thus identification unknown drug drug interaction ddi important concern efficient effective healthcare although multiple resources ddi exist often unable keep pace rich amount information available fast grow biomedical texts exist methods model ddi extraction text classification problem mainly rely handcraft feature feature depend domain specific tool recently neural network model use latent feature show give similar better performance exist model dependent handcraft feature paper present three model namely b lstm ab lstm joint ab lstm base long short term memory lstm network three model utilize word position embed latent feature thus rely explicit feature engineer use bidirectional long short term memory bi lstm network allow implicit feature extraction whole sentence two model ab lstm joint ab lstm also use attentive pool output bi lstm layer assign weight feature experimental result semeval two thousand and thirteen ddi extraction dataset show joint ab lstm model outperform exist methods include rely handcraft feature two propose lstm model also perform competitively state art methods
effectiveness statistical machine translation system smt dependent upon amount parallel corpus use train phase low resource language pair enough parallel corpora build accurate smt paper novel approach present extract bilingual persian italian parallel sentence non parallel comparable corpus study english use pivot language compute match score source target sentence candidate selection phase additionally new monolingual sentence similarity metric normalize google distance ngd propose improve match process moreover extensions baseline system apply improve quality extract sentence measure bleu experimental result show use new pivot base extraction increase quality bilingual corpus significantly consequently improve performance persian italian smt system
bilingual dictionaries important various field natural language process recent years research extract new bilingual lexicons non parallel comparable corpora propose almost use small exist dictionary resources make initial list call seed dictionary paper discuss use different type dictionaries initial start list create bilingual persian italian lexicon comparable corpus experiment apply state art techniques three different seed dictionaries exist dictionary dictionary create pivot base schema dictionary extract small persian italian parallel text interest challenge approach find way combine different dictionaries together order produce better accurate lexicon order combine seed dictionaries propose two different combination model examine effect novel combination model various comparable corpora differ degrees comparability conclude proposal new weight system improve extract lexicon experimental result produce implementation show efficiency propose model
experiment graph base semi supervise learn ssl conditional random field crf application speak language understand slu unaligned data align label examples obtain use ibm model adapt baseline semi supervise crf define new feature set alter label propagation algorithm result demonstrate propose approach significantly improve performance supervise model utilize knowledge gain graph
automatic speech recognition asr text speech tts two prominent area research human computer interaction nowadays set phonetically rich sentence matter importance order develop two interactive modules hci essentially set phonetically rich sentence cover possible phone units distribute uniformly select set big corpus maintain phonetic characteristic base similarity still challenge problem major objective paper devise criteria order select set sentence encompass phonetic aspects corpus size minimum possible first paper present statistical analysis hindi phonetics observe structural characteristics two stage algorithm propose extract phonetically rich sentence high variety triphones emille hindi corpus algorithm consist distance measure criteria select sentence order improve triphone distribution moreover special preprocessing method propose score triphone term inverse probability order fasten algorithm result show approach efficiently build uniformly distribute phonetically rich corpus optimum number sentence
paper describe research method generate bangla word cluster basis relate mean language contextual similarity importance word cluster part speech pos tag word sense disambiguation text classification recommender system spell checker grammar checker knowledge discover many others natural language process nlp applications history word cluster english languages already implement methods word cluster efficiently due lack resources word cluster bangla still implement efficiently presently implementation begin stage research word cluster english base precede next five word key word find efficient result try implement tri gram four gram five gram model word cluster bangla observe one best among start research quite large corpus approximate one lakh bangla word use machine learn technique research generate word cluster analyze cluster test different threshold value
paper introduce novel approach generate synthetic data train neural machine translation systems propose approach transform give parallel corpus write language target language parallel corpus speak dialect variant target language approach language independent use generate data variant source language slang speak dialect even different language closely relate source language propose approach base local embed projection distribute representations utilize monolingual embeddings transform parallel data across language variants report experimental result levantine english translation use neural machine translation show generate data improve large scale system twenty-eight bleu point use synthetic speak data show use provide reliable translation system speak dialect sufficient parallel data
standard content base attention mechanism typically use sequence sequence model computationally expensive require comparison large encoder decoder state time step work propose alternative attention mechanism base fix size memory representation efficient technique predict compact set k attention contexts encode let us decoder compute efficient lookup need consult memory show approach perform par standard attention mechanism yield inference speedups twenty real world translation task task longer sequence visualize attention score demonstrate model learn distinct meaningful alignments
relation extraction fundamental task information extraction exist methods heavy reliance annotations label human experts costly time consume overcome drawback propose novel framework rehession conduct relation extractor learn use annotations heterogeneous information source eg knowledge base domain heuristics annotations refer heterogeneous supervision often conflict bring new challenge original relation extraction task infer true label noisy label give instance identify context information backbone relation extraction true label discovery adopt embed techniques learn distribute representations context bridge components mutual enhancement iterative fashion extensive experimental result demonstrate superiority rehession state art
neural word segmentation attract research interest ability alleviate effort feature engineer utilize external resource pre train character word embeddings paper propose new neural model incorporate word level information chinese word segmentation unlike previous word base model model still adopt framework character base sequence label advantage effectiveness efficiency inference stage utilize word level information also propose new long short term memory lstm architecture direct acyclic graph dag experimental result demonstrate model lead better performances baseline model
propose neural encoder decoder model reinforcement learn nrl grammatical error correction gec unlike conventional maximum likelihood estimation mle model directly optimize towards objective consider sentence level task specific evaluation metric avoid exposure bias issue mle demonstrate nrl outperform mle human automate evaluation metrics achieve state art fluency orient gec corpus
paper present computational approach author profile take gender language variety account apply ensemble system output multiple linear svm classifiers train character word n grams evaluate system use dataset provide organizers two thousand and seventeen pan lab author profile approach achieve seventy-five average accuracy gender identification tweet write four languages ninety-seven accuracy language variety identification portuguese
paper address observe performance gap automatic speech recognition asr systems base long short term memory lstm neural network train connectionist temporal classification ctc loss function systems base hybrid deep neural network dnns train cross entropy ce loss function domains limit data step number experiment show incremental improvements baseline eesen toolkit base lstm ctc asr system train librispeech 100hr train clean one hundred corpus result show effective combination data augmentation regularization lstm ctc base system exceed performance strong kaldi base baseline train data
hierarchical attention network recently achieve remarkable performance document classification give language however multilingual document collections consider train model separately language entail linear parameter growth lack cross language transfer learn single multilingual model fewer parameters therefore challenge potentially beneficial objective end propose multilingual hierarchical attention network learn document structure share encoders share attention mechanisms across languages use multi task learn align semantic space input evaluate propose model multilingual document classification disjoint label set large dataset provide 600k news document eight languages 5k label multilingual model outperform monolingual ones low resource well full resource settings use fewer parameters thus confirm computational efficiency utility cross language transfer
state art neural machine translation nmt attention mechanism use decode enhance translation every step decoder use mechanism focus different part source sentence gather useful information output target word recently effectiveness attention mechanism also explore multimodal task become possible focus sentence part image regions describe paper compare several attention mechanism multimodal translation task english image german evaluate ability model make use image improve translation surpass state art score multi30k data set nevertheless identify report different misbehavior machine translate
multimodal neural machine translation mnmt neural model generate translate sentence describe image give image one source descriptions english consider multimodal image caption translation task image process convolutional neural network cnn extract visual feature exploitable translation model far cnns use pre train object detection localization task hypothesize richer architecture dense caption model may suitable mnmt could lead improve translations extend intuition word embeddings compute linguistic visual representation corpus vocabulary combine compare different confi
previous event extraction study rely heavily feature derive annotate event mention thus apply new event type without annotation effort work take fresh look event extraction model ground problem design transferable neural architecture map event mention type jointly share semantic space use structural compositional neural network type event mention determine closest candidate type leverage 1available manual annotations small set exist event type 2existing event ontologies framework apply new event type without require additional annotation experiment exist event type eg ace ere new event type eg framenet demonstrate effectiveness approach textitwithout manual annotations twenty-three new event type zero shoot framework achieve performance comparable state art supervise model train annotations five hundred event mention
slot fill sf aim extract value certain type attribute slot personcitiesofresidence give entity large collection source document paper propose effective dnn architecture sf follow new strategies one take regularize dependency graph instead raw sentence input dnn compress wide contexts query candidate filler two incorporate two attention mechanisms local attention learn query candidate filler global attention learn external knowledge base guide model better select indicative contexts determine slot type experiment show framework outperform state art relation extraction sixteen absolute f score gain slot fill validation individual system eighty-five absolute f score gain
variations write style commonly use adapt content specific context audience purpose however apply stylistic variations still large manual process little efforts towards automate paper explore automate methods transform text modern english shakespearean english use end end trainable neural model pointers enable copy action tackle limit amount parallel data pre train embeddings word leverage external dictionaries map shakespearean word modern english word well additional text methods able get bleu score thirty-one improvement six point strongest baseline publicly release code foster research area
portmanteaus word formation phenomenon two word combine form new word propose character level neural sequence sequence s2s methods task portmanteau generation end end trainable language independent explicitly use additional phonetic information propose noisy channel style model allow incorporation unsupervised word list improve performance standard source target model model make possible exhaustive candidate generation strategy specifically enable feature portmanteau task experiment find approach superior state art fst base baseline respect grind truth accuracy human evaluation
neural approach relation classification focus find short pattern represent semantic relation use convolutional neural network cnns approach generally achieve better performances use recurrent neural network rnns similar intuition cnn model propose novel rnn base model strongly focus important part sentence use multiple range restrict bidirectional layer attention relation classification experimental result semeval two thousand and ten relation classification task show model comparable state art cnn base rnn base model use additional linguistic information
paper perform comparative analysis three model feature representation text document context document classification particular consider often use family model bag word recently propose continuous space model word2vec doc2vec model base representation text document language network bag word model extensively use document classification task performance two model task well understand especially true network base model rarely consider representation text document classification study measure performance document classifiers train use method random forest feature generate three model variants result empirical comparison show commonly use bag word model performance comparable one obtain emerge continuous space model doc2vec particular low dimensional variants doc2vec generate seventy-five feature among top perform document representation model result finally point doc2vec show superior performance task classify large document
paper present submissions university zurich sigmorphon two thousand and seventeen share task morphological reinflection task predict inflect form give lemma set morpho syntactic feature focus neural network approach tackle task limit resource set transduction lemma inflect form dominate copy lemma character propose two recurrent neural network architectures hard monotonic attention strong copy yet substantially different achieve first approach encoder decoder model copy mechanism second approach neural state transition system set explicit edit action include designate copy action experiment character alignment find naive greedy alignment consistently produce strong result languages best system combination overall winner sigmorphon two thousand and seventeen share task one without external resources set one hundred train sample approach ensembles model outperform next best competitor
propose new attention mechanism neural base question answer depend vary granularities input previous work focus augment recurrent neural network simple attention mechanisms function similarity question embed answer embeddings across time extend make attention mechanism dependent global embed answer attain use separate network evaluate system insuranceqa large question answer dataset model outperform current state art result insuranceqa visualize section text attention mechanism focus explore performance across different parameter settings
recently doc2vec achieve excellent result different task paper present context aware variant doc2vec introduce novel weight estimate mechanism generate weight word occurrence accord contribution context use deep neural network context aware model achieve similar result compare doc2vec initialize bywikipedia train vectors much efficient free heavy external corpus analysis context aware weight show kind enhance idf weight capture sub topic level keywords document might result deep neural network learn hide representations least entropy
current approach cross lingual sentiment analysis try leverage wealth label english data use bilingual lexicons bilingual vector space embeddings machine translation systems show possible use single linear transformation two thousand word pair capture fine grain sentiment relationships word cross lingual set apply cross lingual sentiment model diverse set task demonstrate functionality non english context effectively leverage english sentiment knowledge without need accurate translation analyze extract feature languages scarce data low cost thus make sentiment relate analyse many languages inexpensive
recent developments deep learn application language model lead success task text process summarize machine translation however deploy huge language model mobile device device keyboards pose computation bottle neck due puny computation capacities work propose embed deep learn base word prediction method optimize run time memory also provide real time prediction environment model size 740mb average prediction time six hundred and forty-seven ms improve exist methods word prediction term key stroke save word prediction rate
many nlp applications require disambiguate polysemous word exist methods learn polysemous word vector representations involve first detect various sense optimize sense specific embeddings separately invariably involve single sense learn methods word2vec evaluate methods also problematic rigorous quantitative evaluations space limit especially compare single sense embeddings paper propose simple method learn word representation give context method require learn usual single sense representation coefficients learn via single pass data propose several new test set evaluate word sense induction relevance detection contextual word similarity significantly supplement currently available test result test show method embarrassingly simple achieve excellent result compare state art model unsupervised polysemous word representation learn
neural machine translation model rely beam search algorithm decode practice find quality hypotheses search space negatively affect owe fix beam size mitigate problem store hypotheses single priority queue use universal score function hypothesis selection propose algorithm flexible discard hypotheses revisit later step design penalty function punish hypotheses tend produce final translation much longer shorter expect despite simplicity show propose decode algorithm able select hypotheses better qualities improve translation performance
grammatical error correction gec systems strive correct global errors word order usage local errors spell inflection develop upon recent work neural machine translation propose new hybrid neural model nest attention layer gec experiment show new model effectively correct errors type incorporate word character level informationand model significantly outperform previous neural model gec measure standard conll fourteen benchmark dataset analysis also show superiority propose model largely attribute use nest attention mechanism prove particularly effective correct local errors involve small edit orthography
paper evaluate impact various event extraction systems automatic pathway curation use popular mtor pathway quantify impact train data set well different machine learn classifiers show improve quality automatically extract pathways
paper investigate role tutor feedback language learn use computational model compare two dominant paradigms language learn interactive learn cross situational learn differ primarily role social feedback gaze point analyze relationship two paradigms propose new mix paradigm combine two paradigms allow test algorithms experiment combine feedback social feedback deal mix feedback experiment develop new algorithms show perform respect traditional knn prototype approach
recent years explosion amount text data variety source volume text invaluable source information knowledge need effectively summarize useful review main approach automatic text summarization describe review different process summarization describe effectiveness shortcomings different methods
work neural natural language generation nnlg focus control content generate text experiment control several stylistic aspects generate text addition content method base condition rnn language model desire content well stylistic parameters serve condition contexts demonstrate approach movie review domain show successful generate coherent sentence correspond require linguistic style content
years recursive neural network rvnns show suitable represent text fix length vectors achieve good performance several natural language process task however main drawback rvnns require structure input make data preparation model implementation hard paper propose gumbel tree lstm novel tree structure long short term memory architecture learn compose task specific tree structure plain text data efficiently model use straight gumbel softmax estimator decide parent node among candidates dynamically calculate gradients discrete decision evaluate propose model natural language inference sentiment analysis show model outperform least comparable previous model also find model converge significantly faster model
multi task learn leverage potential correlations among relate task extract common feature yield performance gain however previous work consider simple weak interactions thereby fail model complex correlations among three task paper propose multi task learn architecture four type recurrent neural layer fuse information across multiple relate task architecture structurally flexible consider various interactions among task regard generalize case many previous work extensive experiment five benchmark datasets text classification show model significantly improve performances relate task additional information others
recent work propose several generative neural model constituency parse achieve state art result since direct search generative model difficult primarily use rescore candidate output base parsers decode straightforward first present algorithm direct search generative model demonstrate rescoring result least partly due implicit model combination rather reranking effect finally show explicit model combination improve performance even result new state art number ptb nine thousand, four hundred and twenty-five f1 train gold data nine thousand, four hundred and sixty-six f1 use external data
paper present model use team rivercorners two thousand and seventeen repeval share task first model separately encode pair sentence variable length representations use bidirectional lstm later create fix length raw representations mean simple aggregation function refine use attention mechanism finally combine refine representations sentence single vector use classification model obtain test accuracies seventy-two thousand and fifty-seven seventy-two thousand and fifty-five match mismatch evaluation track respectively outperform lstm baseline obtain performances similar model rely share information sentence esim use ensemble accuracies increase seventy-two thousand, two hundred and forty-seven seventy-two thousand, eight hundred and twenty-seven respectively
progress natural language interfaces databases nlidb slow mainly due linguistic issue language ambiguity domain portability moreover lack large corpus use standard benchmark make data drive approach difficult develop compare paper revisit problem nlidbs recast sequence translation problem end introduce large dataset extract stack exchange data explorer website use train neural natural language interfaces databases also report encourage baseline result smaller manually annotate test corpus obtain use attention base sequence sequence neural network
lys fastparse team present bist covington neural implementation covington two thousand and one algorithm non projective dependency parse bidirectional lstm approach kipperwasser goldberg two thousand and sixteen use train greedy parser dynamic oracle mitigate error propagation model participate conll two thousand and seventeen ud share task spite use ensemble methods use baseline segmentation pos tag parser obtain good result macro average las uas big treebanks category fifty-five languages rank 7th thirty-three team treebanks category las uas rank 16th 12th gap big categories mainly due poor performance four parallel pud treebanks suggest suffix treebanks eg spanish ancora perform poorly cross treebank settings occur correspond unsuffixed treebank eg spanish change obtain 11th best las among run official unofficial code make available https githubcom conll ud two thousand and seventeen lys fastparse
paper present leipzig corpus miner technical infrastructure support qualitative quantitative content analysis infrastructure aim integration close read procedures individual document procedures distant read eg lexical characteristics large document collections therefore information retrieval systems lexicometric statistics machine learn procedures combine coherent framework enable qualitative data analysts make use state art natural language process techniques large document collections applicability framework range social sciences media study market research example introduce usage framework political science study post democracy neoliberalism
terminology work natural language process digital humanities several study address analysis variations context mean term order detect semantic change evolution term distinguish three different approach describe contextual variations methods base analysis pattern linguistic clue methods explore latent semantic space single word methods analysis topic membership paper present notion context volatility new measure detect semantic change apply key term extraction political science case study measure quantify dynamics term contextual variation within diachronic corpus identify periods time characterise intense controversial debate substantial semantic transformations
identify public misinformation complicate challenge task important part check veracity specific claim evaluate stance different news source take towards assertion automatic stance evaluation ie stance detection would arguably facilitate process fact check paper present stance detection system claim third place stage one fake news challenge despite straightforward approach system perform competitive level complex ensembles top two win team therefore propose system simple tough beat baseline fake news challenge stance detection task
geospatial semantics broad field involve variety research areas term semantics refer mean things contrast term syntactics accordingly study geospatial semantics usually focus understand mean geographic entities well counterparts cognitive digital world cognitive geographic concepts digital gazetteers geospatial semantics also facilitate design geographic information systems gi enhance interoperability distribute systems develop intelligent interfaces user interactions past years lot research conduct approach geospatial semantics different perspectives use variety methods target different problems meanwhile arrival big geo data especially large amount unstructured text data web fast development natural language process methods enable new research directions geospatial semantics chapter therefore provide systematic review exist geospatial semantic research six major research areas identify discuss include semantic interoperability digital gazetteers geographic information retrieval geospatial semantic web place semantics cognitive geographic concepts
users post online expect remain anonymous unless log often need able discuss freely various topics preserve anonymity text writer also important contexts eg case witness protection anonymity program however person style write analyze use stylometry result true identity author piece text reveal even try hide thus could helpful design automatic tool help person obfuscate identity write text particular propose approach change text push towards average value general stylometric characteristics thus make use characteristics less discriminative approach consist three main step first calculate value popular stylometric metrics indicate authorship apply various transformations text metrics adjust towards average level preserve semantics soundness text finally add random noise approach turn efficient yield best performance author obfuscation task pan two thousand and sixteen competition
describe participation pan two thousand and seventeen share task author profile identify author gender language variety english spanish arabic portuguese describe final submit system series negative result aim create single model gender language language varieties best perform system cross validate result linear support vector machine svm word unigrams character three five grams feature set additional feature include pos tag additional datasets geographic entities twitter handle hurt rather improve performance result cross validation indicate high performance overall result test set confirm eighty-six average accuracy performance sub task range sixty-eight ninety-eight
critical evaluation word similarity datasets important computational lexical semantics short report concern sanity check propose batchkarov et al two thousand and sixteen evaluate several popular datasets mc rg men first two reportedly fail argue test unstable offer add insight need major revision order fulfill purport goal
present new approach extraction hypernyms base projection learn word embeddings contrast classification base approach projection base methods require candidate hyponym hypernym pair natural use positive negative train examples supervise relation extraction impact negative examples hypernym prediction study far paper show explicit negative examples use regularization model significantly improve performance compare state art approach fu et al two thousand and fourteen three datasets different languages
intelligent assistants ias siri cortana conversationally interact users execute wide range action eg search web set alarm chat ias support action combination various components automatic speech recognition natural language understand language generation however complexity components hinder developers determine component cause error remove hindrance focus reformulation useful signal user dissatisfaction propose method predict reformulation cause evaluate method use user log commercial ia experimental result demonstrate feature design detect error specific component improve performance reformulation detection
problem detect scientific fraud use machine learn recently introduce initial positive result model take account various general indicators result seem suggest write style predictive scientific fraud revisit initial experiment show leave one test procedure use likely lead slight estimate predictability also simple model outperform propose model margin go explore abstract linguistic feature linguistic complexity discourse structure obtain negative result upon analyze model see interest pattern though scientific fraud examples contain less comparison well different type hedge ways present logical reason
study work importance depth convolutional model text classification either character word input consider show five standard text classification sentiment analysis task deep model indeed give better performances shallow network text input represent sequence character however simple shallow wide network outperform deep model densenet word input shallow word model establish new state art performances two datasets yelp binary nine hundred and fifty-nine yelp full six hundred and forty-nine
paper present singlish sentiment lexicon concept level knowledge base sentiment analysis associate multiword expressions set emotion label polarity value unlike many sentiment analysis resources lexicon build manually label piece knowledge come general nlp resources wordnet dbpedia instead automatically construct apply graph mine multi dimensional scale techniques affective common sense knowledge collect three different source knowledge represent redundantly three level semantic network matrix vector space subsequently concepts label emotions polarity ensemble application spread activation neural network emotion categorization model
semantic parse shin analyze complex natural language involve composition computation multiple piece evidence however datasets semantic parse contain many factoid question answer single web document paper propose evaluate semantic parse base question answer model compare question answer baseline query web extract answer web snippets without access target knowledge base investigate approach complexquestions dataset design focus compositional language find model obtain reasonable performance thirty-five f1 compare forty-one f1 state art find analysis model perform well complex question involve conjunctions struggle question involve relation composition superlatives
paper describe monomodal multimodal neural machine translation systems develop lium cvc wmt17 share task multimodal translation mainly explore two multimodal architectures either global visual feature convolutional feature map integrate order benefit visual context final systems rank first en de en fr language pair accord automatic evaluation metrics meteor bleu
paper describe lium submissions wmt17 news translation task english german english turkish english czech english latvian language pair train bpe base attentive neural machine translation systems without factor output use open source nmtpy framework competitive score obtain ensembling various systems exploit availability target monolingual corpora back translation impact back translation quantity quality also analyze english turkish post deadline submission surpass best entry sixteen bleu
paper challenge cross genre document retrieval task query formal write target document conversational write task query sentence extract either summary plot episode tv show target document consist transcripts correspond episode establish strong baseline employ current state art search engine perform document retrieval dataset collect work introduce structure reranking approach improve initial rank utilize syntactic semantic structure generate nlp tool evaluation show improvement four structure reranking apply promise
consider continuous word embed model usually cosines word vectors use measure similarity word cosines change orthogonal transformations embed space demonstrate use canonical orthogonal transformations svd possible increase mean components make components stable learn study interpretability components publicly available model russian language rusvectores fasttext rdt
present first open set language identification experiment use one class classification first highlight shortcomings traditional feature extraction methods propose hash base feature vectorization approach solution use dataset ten languages different write systems train one class support vector machine use monolingual corpus language model evaluate test set data ten languages achieve average f score ninety-nine highlight effectiveness approach open set language identification
performance deep learn natural language process spectacular reason success remain unclear inherent complexity deep learn paper provide empirical evidence effectiveness limitation neural network language engineer precisely demonstrate neural language model base long short term memory lstm effectively reproduce zipf law heap law two representative statistical properties underlie natural language discuss quality reproducibility emergence zipf law heap law train progress also point neural language model limitation reproduce long range correlation another statistical property natural language understand could provide direction improve architectures neural network
study consider problem automate detection non relevant post web forums discuss approach resolve problem approximation task detection semantic relatedness give post open post forum discussion thread approximate task could resolve learn supervise classifier compose word embeddings two post consider success task could quite sensitive choice word representations propose comparison performance different word embed model train seven model word2vec glove word2vec f wang2vec adagram fasttext swivel evaluate embeddings produce dataset human judgements compare performance task non relevant post detection make comparison propose dataset semantic relatedness post one popular russian web forums imageboard 2ch challenge lexical grammatical feature
state art information extraction approach rely token level label find areas interest text unfortunately label time consume costly create consequently available many real life ie task make matter worse token level label usually desire output intermediary step end end e2e model take raw text input produce desire output directly need depend token level label propose e2e model base pointer network train directly pair raw input output text evaluate model atis data set mit restaurant corpus mit movie corpus compare neural baselines use token level label achieve competitive result within percentage point baselines show feasibility e2e information extraction without need token level label open new possibilities many task currently address human extractors raw input output data available token level label
bottom top strategies use neural transition base constituent parse parse strategies differ term order recognize productions derivation tree bottom strategies top strategies take post order pre order traversal tree respectively bottom parsers benefit rich feature readily build partial parse lack lookahead guidance parse process top parsers benefit non local guidance local decisions rely strong encoder input predict constituent hierarchy constructionto mitigate issue propose novel parse system base order traversal syntactic tree design set transition action find compromise bottom constituent information top lookahead information base stack lstm psycholinguistically motivate constituent parse system achieve nine hundred and eighteen f1 wsj benchmark furthermore system achieve nine hundred and thirty-six f1 supervise reranking nine hundred and forty-two f1 semi supervise reranking best result wsj benchmark
paper propose hierarchical attentional neural translation model focus enhance source side hierarchical representations cover local global semantic information use bidirectional tree base encoder maximize predictive likelihood target word weight variant attention mechanism use balance attentive information lexical phrase vectors use tree base rare word encode propose model extend sub word level alleviate vocabulary oov problem empirical result reveal propose model significantly outperform sequence sequence attention base tree base neural translation model english chinese translation task
normalization help part speech pos tag accuracy noisy non canonical data best knowledge little know actual impact normalization real world scenario gold error detection available investigate effect automatic normalization pos tag tweet also compare normalization strategies leverage large amount unlabeled data keep raw form result show normalization help add consistently beyond word embed layer initialization latter approach yield tag model competitive twitter state art tagger
paper present lig cristal submission share automatic post edit task wmt two thousand and seventeen propose two neural post edit model monosource model task specific attention mechanism perform particularly well low resource scenario chain architecture make use source sentence provide extra context latter architecture manage slightly improve result train data available present discuss result two datasets en de de en make available task
propose neural reranking system name entity recognition ner basic idea leverage recurrent neural network model learn sentence level pattern involve name entity mention particular give output sentence produce baseline ner model replace entity mention textitbarack obama entity type textitper result sentence pattern contain direct output information yet less sparse without specific name entities example per bear loc pattern lstm cnn structure utilise learn deep representations sentence reranking result show system significantly improve ner accuracies two different baselines give best report result standard benchmark
work new dataset important first explore familiarize oneself apply advance machine learn algorithms however best knowledge tool exist quickly reliably give insight content selection document respect distinguish document belong different categories paper propose extract relevant word collection texts summarize content document belong certain class discover cluster case unlabeled datasets visualize word cloud allow survey salient feature glance compare three methods extract relevant word demonstrate usefulness result word cloud provide overview class contain dataset scientific publications well discover trend topics recent new york time article snippets
study introduce new approach learn language model train estimate word context pointwise mutual information pmi derive desire conditional probabilities pmi test time specifically show minor modifications word2vec algorithm get principled language model closely relate well establish noise contrastive estimation nce base language model compel aspect approach model train simple negative sample objective function commonly use word2vec learn word embeddings
entity link recently subject significant body research currently best perform approach rely train mono lingual model port approach languages consequently difficult endeavor require correspond train data retrain model address drawback present novel multilingual knowledge base agnostic deterministic approach entity link dub mag mag base combination context base retrieval structure knowledge base graph algorithms evaluate mag twenty-three data set seven languages result show best approach train english datasets pboh achieve micro f measure four time worse datasets languages mag hand achieve state art performance english datasets reach micro f measure six higher pboh non english languages
paper aim automatically discover high quality frame level speech feature acoustic tokens directly unlabeled speech data multi granular acoustic tokenizer mat propose automatic discovery multiple set acoustic tokens give corpus acoustic token set specify set hyperparameters describe model configuration different set acoustic tokens carry different characteristics give corpus language behind thus mutually reinforce multiple set token label use target multi target deep neural network mdnn train frame level acoustic feature bottleneck feature extract mdnn use feedback input mat mdnn next iteration multi granular acoustic token set frame level speech feature iteratively optimize iterative deep learn framework call framework multi granular acoustic tokenizing deep neural network matdnn result evaluate use metrics corpora define zero resource speech challenge organize interspeech two thousand and fifteen improve performance obtain set experiment query example speak term detection corpora visualization discover tokens english phonemes also show
neural machine translation nmt model base sequential encoder decoder framework make use syntactic information paper improve model explicitly incorporate source side syntactic tree specifically propose one bidirectional tree encoder learn sequential tree structure representations two tree coverage model let us attention depend source side syntax experiment chinese english translation demonstrate propose model outperform sequential attentional model well stronger baseline bottom tree encoder word coverage
pairwise rank methods basis many widely use discriminative train approach structure prediction problems natural language processingnlp decompose problem rank hypotheses pairwise comparisons enable simple efficient solutions however neglect global order hypothesis list may hinder learn propose listwise learn framework structure prediction problems machine translation framework directly model entire translation list order learn parameters may better fit give listwise sample furthermore propose top rank enhance loss function sensitive rank errors higher position experiment large scale chinese english translation task show listwise learn framework top rank enhance listwise losses lead significant improvements translation quality
article describe model automatic analysis pun word intentionally use two mean time target word employ roget thesaurus discover two group word pun form around two abstract bits mean semes become semantic vector base svm classifier learn recognize pun reach score seventy-three f measure apply several rule base methods locate intentionally ambiguous target word base structural semantic criteria appear structural criterion effective although possibly characterize test dataset result get correlate result team semeval two thousand and seventeen competition task seven detection interpretation english pun consider effect use supervise learn model word statistics
article describe model automatic interpretation english pun base roget thesaurus implementation punfields pun algorithm discover two group word belong two main semantic field field become semantic vector base svm classifier learn recognize pun rule base model apply recognition intentionally ambiguous target word definitions semeval task seven punfields show considerably good result pun classification require improvement search target word definition
exist natural language generation nlg systems weak ai systems exhibit limit capabilities language generation task demand higher level creativity originality brevity effective solutions least evaluations modern nlg paradigms creative task elusive unfortunately paper introduce address task coherent story generation independent descriptions describe scene event towards explore along two popular text generation paradigms one statistical machine translation smt pose story generation translation problem two deep learn pose story generation sequence sequence learn problem smt choose two popular methods phrase base smt pb smt syntax base smt syntax smt translate incoherent input text stories implement deep recurrent neural network rnn architecture encode sequence variable length input descriptions correspond latent representations decode produce well form comprehensive story like summaries efficacy suggest approach demonstrate publicly available dataset help popular machine translation summarization evaluation metrics
ongoing innovations recurrent neural network architectures provide steady influx apparently state art result language model benchmarks however evaluate use differ code base limit computational resources represent uncontrolled source experimental variation reevaluate several popular architectures regularisation methods large scale automatic black box hyperparameter tune arrive somewhat surprise conclusion standard lstm architectures properly regularise outperform recent model establish new state art penn treebank wikitext two corpora well strong baselines hutter prize dataset
represent texts fix length vectors central many language process task traditional methods build text representations base simple bag word bow representation lose rich semantic relations word recent advance natural language process show semantically meaningful representations word efficiently acquire distribute model make possible build text representations base better foundation call bag word embed bowe representation however exist text representation methods use bowe often lack sound probabilistic foundations well capture semantic relatedness encode word vectors address problems introduce spherical paragraph model spm probabilistic generative model base bowe text representation spm good probabilistic interpretability fully leverage rich semantics word word co occurrence information well corpus wide information help representation learn texts experimental result topical classification sentiment analysis demonstrate spm achieve new state art performances several benchmark datasets
biomedical information grow rapidly recent years retrieve useful data information extraction system get attention current research focus different aspects relation extraction techniques biomedical domain briefly describe state art relation extraction variety biological elements
paper present novel method encode word confusion network represent rich hypothesis space automatic speech recognition systems via recurrent neural network demonstrate utility approach task dialog state track speak dialog systems rely automatic speech recognition output encode confusion network outperform encode best hypothesis automatic speech recognition neural system dialog state track well know second dialog state track challenge dataset
deep learn yield state art performance many natural language process task include name entity recognition ner however typically require large amount label data work demonstrate amount label train data drastically reduce deep learn combine active learn active learn sample efficient computationally expensive since require iterative retrain speed introduce lightweight architecture ner viz cnn cnn lstm model consist convolutional character word encoders long short term memory lstm tag decoder model achieve nearly state art performance standard datasets task computationally much efficient best perform model carry incremental active learn train process able nearly match state art performance twenty-five original train data
paper introduce new distributional method model predicate argument thematic fit judgments use syntax base dsm build prototypical representation verb specific roles every verb extract salient second order contexts roles ie salient dimension typical role fillers compute thematic fit weight overlap top feature candidate fillers role prototypes experiment show method consistently outperform baseline implement state art system achieve better comparable result report literature unsupervised systems moreover provide explicit representation feature characterize verb specific semantic roles
important skill critical think argumentation ability spot recognize fallacies fallacious arguments omnipresent argumentative discourse deceptive manipulative simply lead wrong move discussion despite importance argumentation scholars nlp researchers focus argumentation quality yet investigate fallacies empirically nonexistence resources deal fallacious argumentation call scalable approach data acquisition annotation serious game methodology offer appeal yet unexplored alternative present argotario serious game deal fallacies everyday argumentation argotario multilingual open source platform independent application strong educational aspects accessible wwwargotarionet
nmt systems problems large vocabulary size byte pair encode bpe popular approach solve problem bpe allow system generate target side word enable effective generalization rich vocabulary morphologically rich languages strong inflectional phenomena introduce simple approach overcome problem train system produce lemma word morphologically rich pos tag follow deterministic generation step apply strategy english czech english german translation scenarios obtain improvements settings furthermore show improvement due add explicit morphological information
deal large collections document imperative quickly get overview texts content paper show achieve use cluster algorithm identify topics dataset select visualize relevant word distinguish group document rest texts summarize content document belong topic demonstrate approach discover trend topics collection new york time article snippets
paper introduce novel concept densely connect layer recurrent neural network evaluate propose architecture penn treebank language model task show obtain similar perplexity score six time fewer parameters compare standard stack two layer lstm model train dropout zaremba et al two thousand and fourteen contrast current usage skip connections show densely connect stack layer skip connections already yield significant perplexity reductions
trigram love expect follow positive word happy sarcastic sentence however word ignore may observe expect observe word thus incongruous model sarcasm detection task detect incongruity observe expect word order obtain expect word use context2vec sentence completion library base bidirectional lstm however since exact word incongruity occur may know advance present two approach word approach consult sentence completion every content word incongruous word approach consult sentence completion fifty incongruous content word approach outperform report value tweet discussion forum post likely redundant consultation sentence completion discussion forum post therefore consider oracle case exact incongruous word manually label corpus report past work case performance higher word approach set promise use sentence completion sarcasm detection
submission investigate alternative machine learn model predict hter score sentence level instead directly predict hter score suggest model jointly predict amount four distinct post edit operations use calculate hter score also give possibility correct invalid eg negative predict value prior calculation hter score without feature exploration multi layer perceptron four output yield small significant improvements baseline
work propose novel approach vocabulary oov keyword search kws task propose approach base use high level feature automatic speech recognition asr system call phoneme posterior base ppb feature decode feature obtain calculate time dependent phoneme posterior probabilities word lattices follow smooth ppb feature develop special novel fast simple efficient oov decoder experimental result present georgian language iarpa babel program test language openkws two thousand and sixteen evaluation campaign result show term maximum term weight value mtwv metric computational speed single asr systems propose approach significantly outperform state art approach base use vocabulary proxies oov keywords index database comparison two oov kws approach fusion result nine different asr systems demonstrate propose oov decoder outperform proxy base approach term mtwv metric give comparable process speed important advantage oov decoder include extremely low memory consumption simplicity implementation parameter optimization
introduce novel sub character architecture exploit unique compositional structure korean language method decompose character small set primitive phonetic units call jamo letter character word level representations induce jamo letter divulge syntactic semantic information difficult access conventional character level units greatly alleviate data sparsity problem reduce observation space sixteen original increase accuracy experiment apply architecture dependency parse achieve dramatic improvement strong lexical baselines
naive approach annotation projection effective project discourse annotations one language another implicit discourse relations often change explicit ones vice versa translation paper propose novel approach base intersection statistical word alignment model identify unsupported discourse annotations approach identify sixty-five unsupported annotations english french parallel sentence europarl filter unsupported annotations induce first pdtb style discourse annotate corpus french europarl use corpus train classifier identify discourse usage french discourse connectives show fifteen improvement f1 score compare classifier train non filter annotations
transfer key idea field sentiment analysis new domain community question answer cqa cqa task interest follow give question thread comment want rank comment ones good answer question would rank higher bad ones notice good vs bad comment use specific vocabulary one often predict goodness badness comment even ignore question base comment content lead us idea build good bad polarity lexicon analogy positive negative sentiment polarity lexicons commonly use sentiment analysis particular use pointwise mutual information order build large scale goodness polarity lexicons semi supervise manner start small number initial seed evaluation result show improvement seven map point absolute strong baseline state art performance semeval two thousand and sixteen task three
selectional preferences long claim essential coreference resolution however mainly model implicitly current coreference resolvers propose dependency base embed model selectional preferences allow fine grain compatibility judgments high coverage show incorporation model improve coreference resolution performance conll dataset match state art result complex system however come cost make debatable worthwhile improvements
select optimal parameters neural network architecture often make difference mediocre state art performance however little publish parameters design choices evaluate select make correct hyperparameter optimization often black art require expert experience snoek et al two thousand and twelve paper evaluate importance different network design choices hyperparameters five common linguistic sequence tag task pos chunk ner entity recognition event detection evaluate fifty thousand different setups find parameters like pre train word embeddings last layer network large impact performance parameters example number lstm layer number recurrent units minor importance give recommendation configuration perform well among different task
ever decrease attention span contemporary internet users title online content news article video major factor determine popularity take advantage phenomenon propose new method base bidirectional long short term memory lstm neural network design predict popularity online content use title evaluate propose architecture two distinct datasets news article news videos distribute social media contain forty thousand sample total datasets approach improve performance traditional shallow approach margin fifteen additionally show use pre train word vectors embed layer improve result lstm model especially train set small knowledge first attempt apply popularity prediction use textual information title
majority nlg evaluation rely automatic metrics bleu paper motivate need novel system data independent automatic evaluation methods investigate wide range metrics include state art word base novel grammar base ones demonstrate weakly reflect human judgements system output generate data drive end end nlg also show metric performance data system specific nevertheless result also suggest automatic metrics perform reliably system level support system development find case system perform poorly
interpretability predictive model powerful feature gain trust users correctness predictions word sense disambiguation wsd knowledge base systems tend much interpretable knowledge free counterparts rely wealth manually encode elements represent word sense hypernyms usage examples image present wsd system bridge gap two far disconnect group methods namely system provide access several state art wsd model aim interpretable knowledge base system remain completely unsupervised knowledge free present tool feature web interface word disambiguation texts make sense predictions human readable provide interpretable word sense inventory sense representations disambiguation result provide public api enable seamless integration
paper introduce sgnmt experimental platform machine translation research sgnmt provide generic interface neural symbolic score modules predictors leave right semantic translation model like nmt language model translation lattices n best list kinds score constraints predictors combine predictors form complex decode task sgnmt implement number search strategies traverse space span predictors appropriate different predictor constellations add new predictors decode strategies particularly easy make efficient tool prototyping new research ideas sgnmt actively use students mphil program machine learn speech language technology university cambridge course work theses well research work group
exist approach automatic verbnet style verb classification heavily dependent feature engineer therefore limit languages mature nlp pipelines work propose novel cross lingual transfer method induce verbnets multiple languages best knowledge first study demonstrate architectures learn word embeddings apply challenge syntactic semantic task method use cross lingual translation pair tie six target languages bilingual vector space english jointly specialise representations encode relational information english verbnet standard cluster algorithm run top verbnet specialise representations use vector dimension feature learn verb class result show propose cross lingual transfer approach set new state art verb classification performance across six target languages explore work
pre train word embeddings improve performance neural model cost increase model size propose benefit resource without pay cost operate strictly sub lexical level approach quite simple task specific train first optimize sub word parameters reconstruct pre train word embeddings use various distance measure report interest result variety task word similarity word analogy part speech tag
word embeddings improve generalization lexical feature place word lower dimensional space use distributional information obtain unlabeled data however effectiveness word embeddings downstream nlp task limit vocabulary oov word embeddings exist paper present mimick approach generate oov word embeddings compositionally learn function spell distributional embeddings unlike prior work mimick require train original word embed corpus instead learn perform type level intrinsic extrinsic evaluations demonstrate power simple approach twenty-three languages mimick improve performance word base baseline tag part speech morphosyntactic attribute competitive complementary supervise character base model low resource settings
propose new sentence simplification task split rephrase aim split complex sentence mean preserve sequence shorter sentence like sentence simplification split rephrase potential benefit natural language process societal applications shorter sentence generally better process nlp systems could use preprocessing step facilitate improve performance parsers semantic role labellers machine translation systems also use people read disabilities allow conversion longer sentence shorter ones paper make two contributions towards new task first create make available benchmark consist one million, sixty-six thousand, one hundred and fifteen tuples map single complex sentence sequence sentence express mean second propose five model vanilla sequence sequence semantically motivate model understand difficulty propose task
emotions physiological state generate humans reaction internal external events complex study across numerous field include computer science humans read ever text either interpret sad angry emotion ambiguity exist machine lack facial expressions voice modulations make detect emotions text challenge problem however humans increasingly communicate use text message applications digital agents gain popularity society essential digital agents emotion aware respond accordingly paper propose novel approach detect emotions like happy sad angry textual conversations use lstm base deep learn model approach consist semi automate techniques gather train data model exploit advantage semantic sentiment base embeddings propose solution combine work evaluate real world conversations significantly outperform traditional machine learn baselines well shelf deep learn model
introduce first end end coreference resolution model show significantly outperform previous work without use syntactic parser hand engineer mention detector key idea directly consider span document potential mention learn distributions possible antecedents model compute span embeddings combine context dependent boundary representations head find attention mechanism train maximize marginal likelihood gold antecedent span coreference cluster factor enable aggressive prune potential mention experiment demonstrate state art performance gain fifteen f1 ontonotes benchmark thirty-one f1 use five model ensemble despite fact first approach successfully train external resources
study problem domain adaptation neural abstractive summarization make initial efforts investigate information transfer new domain experimental result news stories opinion article indicate neural summarization model benefit pre train base extractive summaries also find combination domain domain setup yield better summaries domain data insufficient analysis show model capable select salient content even train domain data require domain data capture style target domain
propose new socially impactful task natural language process news corpus extract name persons kill police present newly collect police fatality corpus release publicly present model solve problem use base distant supervision logistic regression convolutional neural network classifiers model outperform two shelf event extractor systems suggest candidate victim name case faster one major manually collect police fatality databases
investigate way predict gender name use character level long short term memory char lstm compare method conventional machine learn methods namely naive bay logistic regression xgboost n grams feature evaluate model dataset consist name indonesian people common use family name surname indonesian culture except ethnicities therefore infer gender full name first name result show achieve nine thousand, two hundred and twenty-five accuracy full name use first name yield nine thousand and sixty-five accuracy result better ones apply classical machine learn algorithms n grams
paper present ensemble system combine output multiple svm classifiers native language identification nli system submit nli share task two thousand and seventeen fusion track feature students essay speak responses form audio transcriptions ivectors non native english speakers eleven native languages system compete challenge team name zcd base ensemble svm classifiers train character n grams achieve eight thousand, three hundred and fifty-eight accuracy rank 3rd share task
social media users often make explicit predictions upcoming events statements vary degree certainty author express toward outcomeleonardo dicaprio win best actor vs leonardo dicaprio may win way leonardo win popular beliefs social media predict win answer question build corpus tweet annotate veridicality train log linear classifier detect positive veridicality high precision forecast uncertain outcomes use wisdom crowd aggregate users explicit predictions method forecast winners fully automate rely set contenders input require train data past outcomes outperform sentiment tweet volume baselines broad range contest prediction task demonstrate approach use measure reliability individual account predictions retrospectively identify surprise outcomes
multimodal sentiment analysis increasingly popular research area extend conventional language base definition sentiment analysis multimodal setup relevant modalities accompany language paper pose problem multimodal sentiment analysis model intra modality inter modality dynamics introduce novel model term tensor fusion network learn dynamics end end propose approach tailor volatile nature speak language online videos well accompany gesture voice experiment model outperform state art approach multimodal unimodal sentiment analysis
learn distribute representations relation instance central technique downstream nlp applications order address semantic model relational pattern paper construct new dataset provide multiple similarity rat every pair relational pattern exist dataset addition conduct comparative study different encoders include additive composition rnn lstm gru compose distribute representations relational pattern also present gate additive composition enhancement additive composition gate mechanism experiment show new dataset enable detail analyse different encoders also provide gauge predict successes distribute representations relational pattern relation classification task
present novel neural model hypervec learn hierarchical embeddings hypernymy detection directionality previous embeddings show limitations prototypical hypernyms hypervec represent unsupervised measure embeddings learn specific order capture hypernym hyponym distributional hierarchy moreover model able generalize unseen hypernymy pair use small set train data map languages result benchmark datasets show hypervec outperform state art unsupervised measure embed model hypernymy detection directionality predict grade lexical entailment
emphverifiability one core edit principles wikipedia editors encourage provide citations add content wikipedia article determine emphcitation span citation ie content cover citation important help decide content citations still miss first address problem determine emphcitation span wikipedia article approach problem classify textual fragment article cover citation propose sequence classification approach paragraph citation determine citation span fine grain level provide thorough experimental evaluation compare approach baselines adopt scientific domain show improvement evaluation metrics
study helpful product review identification problem paper observe evidence conclusion discourse relations also know arguments often appear product review hypothesise argument base feature eg percentage argumentative sentence evidence conclusions ratios good indicators helpful review validate hypothesis manually annotate arguments one hundred and ten hotel review investigate effectiveness several combinations argument base feature experiment suggest use together argument base feature state art baseline feature enjoy performance boost term f1 one thousand, one hundred and one average
preprocessing tool automate text analysis become widely available major languages non english tool often still limit functionality work spanish language text researchers easily find tool tokenization stem may mean extract complex word feature like verb tense mood yet spanish morphologically rich language feature often identifiable word form conjugation rule consistent many special verbs nouns take different rule build complete dictionary know word morphological rule would labor intensive resources already exist spell checker design generate valid form know word paper introduce set tool spanish language morphological analysis build use coes spell check tool label person mood tense gender number derive word root noun verb infinitive convert verbs nominal form
present sequential model temporal relation classification intra sentence events key observation overall syntactic structure compositional mean multi word context events important distinguish among fine grain temporal relations specifically approach first extract sequence context word indicate temporal relation two events well align dependency path two event mention context word sequence together part speech tag sequence dependency relation sequence generate correspond word sequence provide input bidirectional recurrent neural network lstm model neural net learn compositional syntactic semantic representations contexts surround two events predict temporal relation evaluation propose approach timebank corpus show sequential model capable accurately recognize temporal relations events outperform neural net model use various discrete feature input imitate previous feature base model
introduce novel iterative approach event coreference resolution gradually build event cluster exploit inter dependencies among event mention within chain well across event chain among event mention chain distinguish within cross document event coreference link use two distinct pairwise classifiers train separately capture differences feature distributions within cross document event cluster event coreference approach alternate wd cd cluster combine arguments event cluster every merge continue till merge make perform merge event chain closely relate set chain events experiment ecb corpus show model outperform state art methods joint task wd cd event coreference resolution
report result benchmarking open information extraction oie systems use relvis toolkit benchmarking open information extraction systems comprehensive benchmark contain three data set news domain one data set wikipedia overall four thousand, five hundred and twenty-two label sentence eleven thousand, two hundred and forty-three binary n ary oie relations analysis data set compare performance four popular oie systems clausie openie forty-two stanford openie predpatt addition evaluate impact five common error class subset seven hundred and forty-nine n ary tuples deep analysis unreveal important research directions next generation oie systems
paper describe cap two thousand and seventeen challenge challenge concern problem name entity recognition ner tweet write french first present data preparation step follow construct dataset release framework challenge begin demonstrate ner tweet challenge problem especially number entities increase detail annotation process necessary decisions make provide statistics inter annotator agreement conclude data description part examples statistics data describe participation challenge eight team participate focus methods employ challenge participants score achieve term f1 measure importantly construct dataset comprise sim6000 tweet annotate thirteen type entities best knowledge first dataset french publicly available urlhttp cap2017imagfr competitionhtml
work address task generate english sentence abstract mean representation amr graph cope task transform input amr graph structure similar dependency tree annotate syntactic information apply various predefined action subsequently sentence obtain tree structure visit nod specific order train maximum entropy model estimate probability individual action devise algorithm efficiently approximate best sequence action apply use substandard language model generator achieve bleu score two hundred and seventy-four ldc2014t12 test set best result report far without use silver standard annotations another corpus additional train data
work learn lexicon together corpus improve word embeddings however either model lexicon separately update neural network corpus lexicon likelihood minimize distance synonym pair lexicon methods consider relatedness difference corpus lexicon may best optimize paper propose novel method consider relatedness difference corpus lexicon train word embeddings learn corpus predicate word correspond synonym context time polysemous word use word sense disambiguation filter eliminate synonyms different mean context evaluate propose method compare performance word embeddings train propose model control group without filter lexicon prior work word similarity task text classification task experimental result show propose model provide better embeddings polysemous word improve performance text classification
show increase model depth improve quality neural machine translation however different architectural variants increase model depth propose far thorough comparative study work describe evaluate several exist approach introduce depth neural machine translation additionally explore novel architectural variants include deep transition rnns vary attention use deep decoder introduce novel bideep rnn architecture combine deep transition rnns stack rnns evaluation carry english german wmt news translation dataset use single gpu machine train inference find several propose architectures improve upon exist approach term speed translation quality obtain best improvements bideep rnn combine depth eight obtain average improvement fifteen bleu strong shallow baseline release code ease adoption
introduce globally normalize convolutional neural network joint entity classification relation extraction particular propose way utilize linear chain conditional random field output layer predict entity type relations entities time experiment show global normalization outperform locally normalize softmax layer benchmark dataset
present transition base amr parser directly generate amr parse plain text use stack lstms represent parser state make decisions greedily experiment show parser achieve competitive score english use amr train data add additional information pos tag dependency tree improve result
learn semantic parser denotations learn algorithm must search combinatorially large space logical form ones consistent annotate denotations propose new online learn algorithm search faster train progress two key ideas use macro grammars cache abstract pattern useful logical form find thus far holistic trigger efficiently retrieve relevant pattern base sentence similarity wikitablequestions dataset first expand search space exist model improve state art accuracy three hundred and eighty-seven four hundred and twenty-seven use macro grammars holistic trigger achieve 11x speedup accuracy four hundred and thirty-seven
describe recently develop neural machine translation nmt system benchmark statistical machine translation smt system well two general purpose online engines statistical neural present automatic human evaluation result translation output provide system also analyze effect sentence length quality output smt nmt systems
question answer task require build model capable provide answer question express human language full question answer involve form reason ability introduce neural network architecture task form memory network recognize entities relations answer focus attention mechanism model name question dependent recurrent entity network extend recurrent entity network exploit aspects question memorization process validate model synthetic real datasets babi question answer dataset cnn daily news read comprehension dataset experiment model achieve state art former competitive result latter
present sign language model approach allow build grammars create linguistic input sign synthesis avatars comment type grammar allow build observe resemblance result expressions traditional semantic representations compare ways paradigms design name contrast two essentially different strategies build higher level linguistic input source forward vs target back conclude favour latter acknowledge power able automatically generate output semantically relevant input straight articulations target language
recent neural model show significant progress problem generate short descriptive texts condition small number database record work suggest slightly difficult data text generation task investigate effective current approach task particular introduce new large scale corpus data record pair descriptive document propose series extractive evaluation methods analyze performance obtain baseline result use current neural generation methods experiment show model produce fluent text fail convincingly approximate human generate document moreover even templated baselines exceed performance neural model metrics though copy reconstruction base extensions lead noticeable improvements
search systems often focus provide relevant result assume corpora user need focus present however many corpora today reflect significant longitudinal collections range twenty years web hundreds years digitize newspapers book understand temporal intent user retrieve relevant historical content become significant challenge common search feature query expansion leverage relationship term function well across time relationships vary temporally work introduce temporal relationship model extract longitudinal data collections model support task identify give two word relate present algorithmic framework task show application task query expansion achieve high gain
paper present novel unsupervised algorithm word sense disambiguation wsd document level algorithm inspire widely use approach field genetics whole genome sequence know shotgun sequence technique propose wsd algorithm base three main step first brute force wsd algorithm apply short context windows ten word select document order generate short list likely sense configurations window second step local sense configurations assemble longer composite configurations base suffix prefix match result configurations rank length sense word choose base vote scheme consider top k configurations word appear compare algorithm state art unsupervised wsd algorithms demonstrate better performance sometimes large margin also show algorithm yield better performance common sense mcs baseline one data set moreover algorithm small number parameters robust parameter tune unlike bio inspire methods give deterministic solution involve random choices
paper propose novel approach text classification base cluster word embeddings inspire bag visual word model widely use computer vision word collection document represent word vector use pre train word embeddings model k mean algorithm apply word vectors order obtain fix size set cluster centroid cluster interpret super word embed embody semantically relate word vectors certain region embed space every embed word collection document assign nearest cluster centroid end document represent bag super word embeddings compute frequency super word embed respective document also diverge idea build single vocabulary entire collection document propose build class specific vocabularies better performance use kind representation report result two text mine task namely text categorization topic polarity classification task model yield better performance standard bag word
paper present result repeval two thousand and seventeen share task evaluate neural network sentence representation learn model multi genre natural language inference corpus multinli recently introduce williams et al two thousand and seventeen five participate team beat bidirectional lstm bilstm continuous bag word baselines report williams et al best single model use stack bilstms residual connections extract sentence feature reach seven hundred and forty-five accuracy genre match test set surprisingly result competition fairly consistent across genre match genre mismatch test set across subsets test data represent variety linguistic phenomena suggest submit systems learn reasonably domain independent representations sentence mean
entropy fundamental property repertoire present efficient algorithm estimate entropy type help zhang estimator algorithm take advantage fact number different frequencies text general much smaller number type justify convenience algorithm mean analysis statistical properties texts one thousand languages work open various possibilities future research
describe machine learn approach two thousand and seventeen share task native language identification nli propose approach combine several kernels use multiple kernel learn kernels base character p grams also know n grams extract essay speech transcripts also use kernel base vectors low dimensional representation audio record provide share task organizers learn stage choose kernel discriminant analysis kda kernel ridge regression krr former classifier obtain better result latter one development set previous work use similar machine learn approach achieve state art nli result goal paper demonstrate shallow simple approach base string kernels minor improvements pass test time reach state art performance two thousand and seventeen nli share task despite recent advance natural language process participate three track competitors allow use essay essay track speech transcripts speech track fusion track use data provide organizers train model reach macro f1 score eight thousand, six hundred and ninety-five close essay track macro f1 score eight thousand, seven hundred and fifty-five close speech track macro f1 score nine thousand, three hundred and nineteen close fusion track score team unibuckernel rank first group team three track attain best score speech fusion track
paper present augmentation mscoco dataset speech add image text speech caption generate use text speech tts synthesis result six hundred and sixteen thousand, seven hundred and sixty-seven speak caption 600h pair image disfluencies speed perturbation add signal order sound natural speech signal wav pair json file contain exact timecode word syllable phoneme speak caption corpus could use language vision lavi task include speech input output instead text investigate multimodal learn scheme unsupervised speech pattern discovery also possible corpus demonstrate preliminary study conduct subset corpus 10h 10k speak caption dataset available zenodo https zenodoorg record four million, two hundred and eighty-two thousand, two hundred and sixty-seven
paper present set computational methods identify likeliness word borrow base signal social media term spearman correlation coefficient value methods perform two time better nearly sixty-two predict borrow likeliness compare best perform baseline nearly twenty-six report literature base likeliness estimate ask annotators annotate language tag foreign word predominantly native contexts eighty-eight percent case annotators felt foreign language tag replace native language tag thus indicate huge scope improvement automatic language identification systems
present quantitative analysis human word association pair study type relations present associations put main focus correlation response type respondent characteristics occupation gender contrast syntagmatic paradigmatic associations finally propose personalise distribute word association model show importance incorporate demographic factor model commonly use natural language process
practitioners apply neural network increasingly complex problems natural language process syntactic parse semantic role label rich output structure many structure prediction problems require deterministic constraints output value example sequence sequence syntactic parse require sequential output encode valid tree hide units might capture properties network always able learn constraints train data alone practitioners must resort post process paper present inference method neural network enforce deterministic constraints output without perform rule base post process expensive discrete search instead spirit gradient base train enforce constraints gradient base inference gbi input test time nudge continuous model weight network unconstrained inference procedure generate output satisfy constraints study efficacy gbi three task hard constraints semantic role label syntactic parse sequence transduction case algorithm satisfy constraints improve accuracy even underlie network state art
paper deal use word embed model trace temporal dynamics semantic relations pair word set similar well know analogies task expand time dimension end apply incremental update model new train texts include incremental vocabulary expansion couple learn transformation matrices let us map members relation propose approach evaluate task predict insurgent arm group base geographical locations gold standard data time span one thousand, nine hundred and ninety-four two thousand and ten extract ucdp arm conflict dataset result show method feasible outperform baselines also important work still remain do
determine semantic textual similarity core research subject natural language process since vector base model sentence representation often use shallow information capture accurate semantics difficult contrast logical semantic representations capture deeper level sentence semantics symbolic nature offer grade notions textual similarity propose method determine semantic textual similarity combine shallow feature feature extract natural deduction proof bidirectional entailment relations sentence pair natural deduction proof use ccg2lambda higher order automatic inference system convert combinatory categorial grammar ccg derivation tree semantic representations conduct natural deduction proof experiment show system able outperform logic base systems feature derive proof effective learn textual similarity
paper describe builder entry name strawman sentence level sentiment analysis task build break share task first workshop build linguistically generalizable nlp systems goal builder provide automate sentiment analyzer would serve target breakers whose goal find pair minimally differ sentence break analyzer
generative neural model recently achieve state art result constituency parse however without feasible search procedure use far limit reranking output external parsers decode tractable describe alternative conventional action level beam search use discriminative neural model enable us decode directly generative model show improve basic candidate selection strategy use coarse prune function improve accuracy explore significantly less search space apply model choe charniak two thousand and sixteen inference procedure obtain nine thousand, two hundred and fifty-six f1 section twenty-three penn treebank surpass prior state art result single model systems
opinion mine sentiment analysis social media research issue great interest scientific community however begin analysis face set problems particular problem richness languages dialects within media address problem propose paper approach construction implementation syntactic analyzer name asda tool represent parser algerian dialect label term give corpus thus construct label table contain term stem different prefix suffix allow us determine different grammatical part sort pos tag label serve us later semantic process algerian dialect like automatic translation dialect sentiment analysis
control experiment sequence sequence approach task sentence correction find character base model generally effective word base model model encode subword information via convolutions model output data series diffs improve effectiveness standard approach strongest sequence sequence model improve strongest phrase base statistical machine translation model access data six m2 five gleu point additionally data environment standard conll two thousand and fourteen setup demonstrate model tune diffs yield similar better m2 score simpler model significantly less data previous sequence sequence approach
charge prediction task determine appropriate charge give case helpful legal assistant systems user input fact description argue relevant law article play important role task therefore propose attention base neural network method jointly model charge prediction task relevant article extraction task unify framework experimental result show besides provide legal basis relevant article also clearly improve charge prediction result full model effectively predict appropriate charge case different expression style
add manually annotate prosodic information specifically pitch accent phrase typical text base feature set coreference resolution previously show positive effect german data practical applications speak language however would rely automatically predict prosodic information paper predict pitch accent phrase boundaries use convolutional neural network cnn model acoustic feature extract speech signal assessment quality automatic prosodic annotations show also significantly improve coreference resolution
lack large realistic datasets present bottleneck online deception detection study paper apply data collection method base social network analysis quickly identify high quality deceptive truthful online review amazon dataset contain ten thousand deceptive review diverse product domains reviewers use dataset explore effective general feature online deception detection perform well across domains demonstrate generalize feature advertise speak write complexity score deception detection performance improve add additional deceptive review assort domains train finally reviewer level evaluation give interest insight different deceptive reviewers write style
capabilities detect temporal relations two events benefit many applications exist temporal relation classifiers train supervise manner instead explore observation regular event pair show consistent temporal relation despite various contexts rich contexts use train contextual temporal relation classifier recognize new temporal relation contexts identify new regular event pair focus detect temporal relations design weakly supervise learn approach extract thousands regular event pair learn contextual temporal relation classifier simultaneously evaluation show acquire regular event pair high quality contain rich commonsense knowledge domain specific knowledge addition weakly supervise train temporal relation classifier achieve comparable performance state art supervise systems
apply cross lingual latent semantic index bilingual document alignment task wmt16 reduce rank singular value decomposition bilingual term document matrix derive know english french page pair train data allow us map monolingual document joint semantic space two variants cosine similarity vectors place document joint semantic space combine measure string similarity correspond urls produce eleven alignments english french web page variety domains system achieve recall ca eighty-eight domain data use build latent semantic model ninety-three data include analyse system errors train data argue evaluate aligner performance base exact url match estimate true performance propose alternative able account duplicate near duplicate underlie data
paper discuss approach take uwaterloo team arrive solution fine grain sentiment analysis problem pose task five semeval two thousand and seventeen paper describe document vectorization sentiment score prediction techniques use well design implementation decisions take build system task system use text vectorization model n gram tf idf paragraph embeddings couple regression model variants predict sentiment score amongst methods examine unigrams bigrams couple simple linear regression obtain best baseline accuracy paper also explore data augmentation methods supplement train dataset system design subtask two news statements headline
examine effect particular order sentence pair line train neural machine translation nmt focus two type order one ensure minibatch contain sentence similar aspect two gradual inclusion sentence type train progress call curriculum learn english czech experiment internal homogeneity minibatches effect train curricula achieve small improvement baseline
one central mystery neural nlp neural model know subject matter neural machine translation system learn translate one language another learn syntax semantics languages knowledge extract system fill hole human scientific knowledge exist typological databases contain relatively full feature specifications hundred languages exploit existence parallel texts thousand languages build massive many one neural machine translation nmt system one thousand and seventeen languages english use predict information miss typological databases experiment show propose method able infer syntactic also phonological phonetic inventory feature improve baseline access information languages geographic phylogenetic neighbor
name entity recognition ner well establish task information extraction study decades recently study report ner experiment social media texts emerge hand stance detection considerably new research topic usually consider within scope sentiment analysis stance detection study mostly apply texts online debate stance text owner particular target either explicitly implicitly mention text explore study investigate possible contribution name entities stance detection task tweet report evaluation result ner experiment well subsequent stance detection experiment use name entities publicly available stance annotate data set tweet result indicate name entities obtain high performance ner system contribute stance detection performance tweet
unsupervise learn word embeddings see tremendous success numerous natural language process nlp task recent years main contribution paper develop technique call skill2vec apply machine learn techniques recruitment enhance search strategy find candidates possess appropriate skills skill2vec neural network architecture inspire word2vec develop mikolov et al two thousand and thirteen transform skills new vector space characteristics calculation present skills relationships conduct experiment evaluation manually recruitment company domain experts demonstrate effectiveness approach
recent neural headline generation model show great result generally train large datasets focus efforts improve headline quality smaller datasets mean pretraining propose new methods enable pre train parameters model utilize available text result improvements three hundred and twenty-four relative perplexity two hundred and eighty-four point rouge
paper present approach introduce thesaurus knowledge probabilistic topic model main idea approach base assumption frequencies semantically relate word phrase meet texts enhance action lead larger contribution topics find texts conduct experiment several thesauri find improve topic model useful utilize domain specific knowledge general thesaurus wordnet use thesaurus base improvement topic model achieve exclude hyponymy relations combine topic model
necessity use fix size word vocabulary order control model complexity state art neural machine translation nmt systems important bottleneck performance especially morphologically rich languages conventional methods aim overcome problem use sub word character level representations solely rely statistics disregard linguistic properties word lead interruptions word structure cause semantic syntactic losses paper propose new vocabulary reduction method nmt reduce vocabulary give input corpus rate also consider morphological properties language method base unsupervised morphology learn principle use pre process language pair also present alternative word segmentation method base supervise morphological analysis aid us measure accuracy model evaluate method turkish english nmt task input language morphologically rich agglutinative analyze different representation methods term translation accuracy well semantic syntactic properties generate output method obtain significant improvement twenty-three bleu point conventional vocabulary reduction technique show provide better accuracy open vocabulary translation morphologically rich languages
investigate techniques supervise domain adaptation neural machine translation exist model train large domain dataset adapt small domain dataset scenario overfitting major challenge investigate number techniques reduce overfitting improve transfer learn include regularization techniques dropout l2 regularization towards domain prior addition introduce tuneout novel regularization technique inspire dropout apply techniques alone combination neural machine translation obtain improvements iwslt datasets english german english russian also investigate amount domain train data need domain adaptation nmt find logarithmic relationship amount train data gain bleu score
online argumentative dialog rich source information popular beliefs opinions could useful company well governmental public policy agencies compact easy read summaries dialogues would thus highly valuable priori even clear form summary take previous work summarization primarily focus summarize write texts notion abstract text well define collect gold standard train data consist five human summaries one hundred and sixty-one dialogues topics gay marriage gun control abortion present several different computational model aim identify segment dialogues whose content use summary use linguistic feature word2vec feature svms bidirectional lstms show identify important arguments use dialog context best f measure seventy-four gun control seventy-one gay marriage sixty-seven abortion
people interact semantic web data directly unless expertise understand underlie technology need textual visual interfaces help make sense explore problem generate natural language summaries semantic web data non trivial especially open domain context address problem explore use neural network system encode information set triple vector fix dimensionality generate textual summary condition output encode vector train evaluate model two corpora loosely align wikipedia snippets dbpedia wikidata triple promise result
paper consider problem machine read task question form keywords rather natural language recent years researchers achieve significant success machine read comprehension task squad triviaqa datasets provide natural language question sentence pre select passage goal answer question accord passage however situation interact machine mean text people likely raise query form several keywords rather complete sentence keyword base query comprehension new challenge small variations question may completely change semantical information thus yield different answer paper propose novel neural network system consist demand optimization model base passage attention neural machine translation reader model find answer give optimize question demand optimization model optimize original query output multiple reconstruct question reader model take new question input locate answer passage make predictions robust evaluation mechanism score reconstruct question final answer strike good balance quality demand optimization model reader model experimental result several datasets show framework significantly improve multiple strong baselines challenge task
virtual assistants text chatbots recently gain popularity give short message nature text base chat interactions language identification systems bots might fifteen twenty character make prediction however accurate text language identification important especially early stag many multilingual natural language process pipelines paper investigate use naive bay classifier accurately predict language family piece text belong combine lexicon base classifier distinguish specific south african language text write approach lead thirty-one reduction language detection error spirit reproducible research train test datasets well code publish github hopefully useful create text language identification share task south african languages
automatic generation paraphrase give sentence important yet challenge task natural language process nlp play key role number applications question answer search dialogue paper present deep reinforcement learn approach paraphrase generation specifically propose new framework task consist textitgenerator textitevaluator learn data generator build sequence sequence learn model produce paraphrase give sentence evaluator construct deep match model judge whether two sentence paraphrase generator first train deep learn fine tune reinforcement learn reward give evaluator learn evaluator propose two methods base supervise learn inverse reinforcement learn respectively depend type available train data empirical study show learn evaluator guide generator produce accurate paraphrase experimental result demonstrate propose model generators outperform state art methods paraphrase generation automatic evaluation human evaluation
crosstalk also know chinese name xiangsheng traditional chinese comedic perform art feature joke funny dialogues one china popular cultural elements typically form dialogue two performers purpose bring laughter audience one person act lead comedian support role though general dialogue generation widely explore previous study unknown whether entertain dialogues automatically generate paper first time investigate possibility automatic generation entertain dialogues chinese crosstalks give utterance lead comedian dialogue task aim generate reply utterance support role propose humor enhance translation model address task human evaluation result demonstrate efficacy propose model feasibility automatic entertain dialogue generation also verify
compare traditional statistical machine translation smt neural machine translation nmt often sacrifice adequacy sake fluency propose method combine advantage traditional smt nmt exploit exist phrase base smt model compute phrase base decode cost nmt output use cost rerank n best nmt output main challenge implement approach nmt output may search space standard phrase base decode algorithm search space phrase base smt limit phrase base translation rule table propose soft force decode algorithm always successfully find decode path nmt output show use force decode cost rerank nmt output successfully improve translation quality four different language pair
dense word embeddings encode semantic mean word low dimensional vector space become popular natural language process nlp research due state art performances many nlp task word embeddings substantially successful capture semantic relations among word meaningful semantic structure must present respective vector space however many case semantic structure broadly heterogeneously distribute across embed dimension make interpretation big challenge study propose statistical method uncover latent semantic structure dense word embeddings perform analysis introduce new dataset semcat contain six thousand, five hundred word semantically group one hundred and ten categories propose method quantify interpretability word embeddings propose method practical alternative classical word intrusion test require human intervention
thank improvements machine learn techniques include deep learn free large scale speech corpus share academic institutions commercial company important role however corpus japanese speech synthesis exist paper design novel japanese speech corpus name jsut corpus aim achieve end end speech synthesis corpus consist ten hours read style speech data transcription cover main pronunciations daily use japanese character paper describe design analyze corpus corpus freely available online
machine translation tackle discourse phenomena model must access extra sentential linguistic context recent interest model context neural machine translation nmt model principally evaluate standard automatic metrics poorly adapt evaluate discourse phenomena article present hand craft discourse test set design test model ability exploit previous source target sentence investigate performance recently propose multi encoder nmt model train subtitle english french also explore novel way exploit context previous sentence despite gain use bleu multi encoder model give limit improvement handle discourse phenomena fifty accuracy coreference test set five hundred and thirty-five coherence cohesion compare non contextual baseline fifty simple strategy decode concatenation previous current sentence lead good performance novel strategy multi encode decode two sentence lead best performance seven hundred and twenty-five coreference fifty-seven coherence cohesion highlight importance target side context
paper introduce new web base software tool annotate text text annotation graph tag provide functionality represent complex relationships word word phrase available software tool include ability define visualize relationships relationships semantic hypergraphs additionally include approach represent text annotations annotation subgraphs semantic summaries use show relationships outside sequential context text users use subgraphs quickly find similar structure within current document external annotate document initially tag develop support information extraction task large database biomedical article however software flexible enough support wide range annotation task domain examples provide showcase tag capabilities morphological parse event extraction task tag software available https githubcom creativecodinglab textannotationgraphs
decade machine learn use extract opinion holder target structure text answer question express kind sentiment towards recent neural approach outperform state art feature base model opinion role label orl suspect due scarcity label train data address issue use different multi task learn mtl techniques relate task substantially data ie semantic role label srl show two mtl model improve significantly single task model label holders target development test set find vanilla mtl model make predictions use share orl srl feature perform best deeper analysis determine work might do make improvements orl
read comprehension challenge task especially execute across longer across multiple evidence document answer likely reoccur exist neural architectures typically scale entire evidence hence resort select single passage document either via truncation mean carefully search answer within passage however case strategy suboptimal since focus specific passage become difficult leverage multiple mention answer throughout document work take different approach construct lightweight model combine cascade find answer submodel consist fee forward network equip attention mechanism make trivially parallelizable show approach scale approximately order magnitude larger evidence document aggregate information representation level multiple mention answer candidate across document empirically approach achieve state art performance wikipedia web domains triviaqa dataset outperform complex recurrent architectures
automatic analysis poetic rhythm challenge task involve linguistics literature computer science language analyze know rule base systems data drive methods use paper analyze poetic rhythm english spanish show representations data learn character base neural model informative ones hand craft feature bi lstmcrf model produce state art accuracy scansion poetry two languages result also show information whole word structure independent syllables highly informative perform scansion
neural machine translation nmt become new paradigm parameter optimization require large scale parallel data scarce many domains language pair paper address new translation scenario exist monolingual corpora phrase pair propose new method towards translation partially align sentence pair derive phrase pair monolingual corpora make full use partially align corpora adapt conventional nmt train method two aspects one hand different generation strategies design align unaligned target word hand different objective function design model partially align part experiment demonstrate method achieve relatively good result translation scenario tiny bitexts boost translation quality large extent
work present simple elegant approach language model bilingual code switch text since code switch blend two different languages standard bilingual language model improve upon use structure monolingual language model propose novel technique call dual language model involve build two complementary monolingual language model combine use probabilistic model switch two evaluate efficacy approach use conversational mandarin english speech corpus prove robustness model show significant improvements perplexity measure standard bilingual language model without use external information similar consistent improvements also reflect automatic speech recognition error rat
natural language process nlp model often require massive number parameters word embeddings result large storage memory footprint deploy neural nlp model mobile devices require compress word embeddings without significant sacrifice performance purpose propose construct embeddings basis vectors word composition basis vectors determine hash code maximize compression rate adopt multi codebook quantization approach instead binary cod scheme code compose multiple discrete number three two one eight value component limit fix range propose directly learn discrete cod end end neural network apply gumbel softmax trick experiment show compression rate achieve ninety-eight sentiment analysis task ninety-four ninety-nine machine translation task without performance loss task propose method improve model performance slightly lower compression rate compare approach character level segmentation propose method language independent require modifications network architecture
learn new skill take advantage preexist skills knowledge instance skilled violinist likely easier time learn play cello similarly learn new language take advantage languages already speak instance native language norwegian decide learn dutch lexical overlap two languages likely benefit rate language acquisition thesis deal intersection learn multiple task learn multiple languages context natural language process nlp define study computational process human language although two type learn may seem different surface see share many similarities traditional approach nlp consider single task single language time however recent advance allow broaden approach consider data multiple task languages simultaneously important approach explore key improve reliability nlp especially low resource languages take advantage relevant data whenever possible hope long term low resource languages benefit advance make nlp currently large extent reserve high resource languages turn may positive consequences eg language preservation speakers minority languages lower degree pressure use high resource languages short term answer specific research question pose use nlp researchers work towards goal
train bank complex filter operate raw waveform feed convolutional neural network end end phone recognition time domain filterbanks td filterbanks initialize approximation mel filterbanks fine tune jointly remain convolutional architecture perform phone recognition experiment timit show several architectures model train td filterbanks consistently outperform counterparts train comparable mel filterbanks get best performance learn front end step pre emphasis average finally observe filter convergence asymmetric impulse response remain almost analytic
unreliable news piece information false mislead deliberately spread promote political ideological financial agendas recently problem unreliable news get lot attention number instance use news social media outlets propaganda increase rapidly pose serious threat society call technology automatically reliably identify unreliable news source paper effort make direction build systems detect unreliable news article paper various nlp algorithms build evaluate unreliable news data two thousand and seventeen dataset variants hierarchical attention network han present encode classify news article achieve best result nine hundred and forty-four roc auc finally attention layer weight visualize understand give insight decisions make hans result obtain promise encourage deploy use systems real world mitigate problem unreliable news
predict discharge medications right patient admit important clinical decision provide physicians guidance type medication regimen plan possible change initial medication may occur inpatient stay also facilitate medication reconciliation process easy detection medication discrepancy discharge time improve patient safety however since information available upon admission limit patients condition may evolve inpatient stay predictions could difficult decision physicians make work investigate leverage deep learn technologies assist physicians predict discharge medications base information document admission note build convolutional neural network take admission note input predict medications place patient discharge time method able distill semantic pattern unstructured noisy texts capable capture pharmacological correlations among medications evaluate method 25k patient visit compare four strong baselines methods demonstrate twenty increase macro average f1 score best baseline
recent years neural network prove effective chinese word segmentation however promise performance rely large scale train data neural network conventional architectures achieve desire result low resource datasets due lack label train data paper propose deep stack framework improve performance word segmentation task insufficient data integrate datasets diverse domains framework consist two part domain base model deep stack network domain base model use learn knowledge different datasets deep stack network design integrate domain base model reduce model conflict innovatively add communication paths among model design various structure deep stack network include gaussian base stack network concatenate base stack network sequence base stack network tree base stack network conduct experiment six low resource datasets various domains propose framework show significant performance improvements datasets compare several strong baselines
paper present summary first workshop build linguistically generalizable natural language process systems associate build break language edition share task goal workshop bring together researchers nlp linguistics share task aim test generalizability nlp systems beyond distributions train data describe motivation setup participation share task provide discussion highlight result discuss lessons learn
paper propose novel deep neural network architecture sequence sequence audio2vec unsupervised learn fix length vector representations audio segment excise speech corpus vectors contain semantic information pertain segment close vectors embed space correspond segment semantically similar design propose model base rnn encoder decoder framework borrow methodology continuous skip grams train learn vector representations evaluate thirteen widely use word similarity benchmarks achieve competitive result glove biggest advantage propose model capability extract semantic information audio segment take directly raw speech without rely modalities text image challenge expensive collect annotate
past several decades many authorship attribution study use computational methods determine author dispute texts dispute authorship common problem classics since little information ancient document survive centuries many scholars question authenticity final chapter xenophon cyropaedia 4th century bc historical text study use n grams frequency vectors cosine similarity function word frequency vectors naive bay classifiers nbc support vector machine svm analyze authorship cyropaedia although n gram analysis show epilogue cyropaedia differ slightly rest work compare analysis xenophon analyse aristotle plato suggest difference significant nbc svm analyse word frequencies show final chapter cyropaedia closely relate chapters cyropaedia therefore analysis suggest dispute chapter write xenophon information help scholars better understand cyropaedia also demonstrate usefulness apply modern authorship analysis techniques classical literature
traditional chinese medicine tcm accumulate big amount precious resource long history development tcm prescriptions consist tcm herbs important form tcm treatment similar natural language document weakly order fashion directly adapt language model style methods learn embeddings herbs problematic herbs strictly order herbs front prescription connect last ones paper propose represent tcm herbs distribute representations via prescription level language model pllm one experiment correlation calculate similarity medicine judgment professionals achieve spearman score five thousand, five hundred and thirty-five indicate strong correlation surpass human beginners tcm relate field bachelor student big margin ten
dialogue systems attract attention recent advance dialogue systems overwhelmingly contribute deep learn techniques employ enhance wide range big data applications computer vision natural language process recommender systems dialogue systems deep learn leverage massive amount data learn meaningful feature representations response generation strategies require minimum amount hand craft article give overview recent advance dialogue systems various perspectives discuss possible research directions particular generally divide exist dialogue systems task orient non task orient model detail deep learn techniques help representative algorithms finally discuss appeal research directions bring dialogue system research new frontier
croatian poorly resourced highly inflect language slavic language family nowadays research focus mostly english create new word analogy corpus base original english word2vec word analogy corpus add specific linguistic aspects croatian language next create croatian wordsim353 rg65 corpora basic evaluation word similarities compare create corpora two popular word representation model base word2vec tool fasttext tool model train 137b tokens train data corpus test new robust croatian word analogy corpus result show model able create meaningful word representation research show free word order higher morphological complexity croatian language influence quality result word embeddings
describe variant child sum tree lstm deep neural network tai et al two thousand and fifteen fine tune work dependency tree morphologically rich languages use example polish fine tune include apply custom regularization technique zoneout describe krueger et al two thousand and sixteen adapt tree lstms well use pre train word embeddings enhance sub word information bojanowski et al two thousand and sixteen system implement pytorch evaluate phrase level sentiment label task part poleval competition
inspire principles speed read introduce skim rnn recurrent neural network rnn dynamically decide update small fraction hide state relatively unimportant input tokens skim rnn give computational advantage rnn always update entire hide state skim rnn use input output interfaces standard rnn easily use instead rnns exist model experiment show skim rnn achieve significantly reduce computational cost without lose accuracy compare standard rnns across five different natural language task addition demonstrate trade accuracy speed skim rnn dynamically control inference time stable manner analysis also show skim rnn run single cpu offer lower latency compare standard rnns gpus
paper describe tamu system submit tac kbp two thousand and seventeen event nugget detection coreference resolution task system build statistical empirical observations make train development data find modifiers event nuggets tend unique syntactic distribution part speech tag dependency relations provide essential characteristics useful identify span also define type realis status find joint model event span detection realis status identification perform better individual model task simple system design use minimal feature achieve micro average f1 score five thousand, seven hundred and seventy-two four thousand, four hundred and twenty-seven four thousand, two hundred and forty-seven event span detection type identification realis status classification task respectively also system achieve conll f1 score two thousand, seven hundred and twenty event coreference resolution task
build speech recognizers multiple languages typically involve replicate monolingual train recipe language utilize multi task learn approach model different languages separate output label share internal parameters work exploit recent progress end end speech recognition create single multilingual speech recognition system capable recognize languages see train propose use universal character set share among languages also create language specific gate mechanism within network modulate network internal representations language specific way evaluate propose approach microsoft cortana task across three languages show system outperform individual monolingual systems systems build multi task learn approach also show model use initialize monolingual speech recognizer use create bilingual model use code switch scenarios
achieve high accuracy end end speech recognizers require careful parameter initialization prior train otherwise network may fail find good local optimum particularly true online network unidirectional lstms currently best strategy train systems bootstrap train tie triphone system however time consume importantly impossible languages without high quality pronunciation lexicon work propose initialization strategy use teacher student learn transfer knowledge large well train offline end end speech recognition model online end end model eliminate need lexicon linguistic resources also explore curriculum learn label smooth show combine propose teacher student learn improvements evaluate methods microsoft cortana personal assistant task show propose method result nineteen relative improvement word error rate compare randomly initialize baseline system
relation classification important semantic process task field natural language process nlp paper present novel model structure regularize bidirectional recurrent convolutional neural networksr brcnn classify relation two entities sentence new dataset chinese sanwen name entity recognition relation classification state art systems concentrate model shortest dependency path sdp two entities leverage convolutional recurrent neural network explore make full use dependency relations information sdp improve model method structure regularization propose structure regularize model learn relation representations along sdp extract forest form structure regularize dependency tree benefit reduce complexity whole model help improve f1 score one hundred and three experimental result show method outperform state art approach chinese sanwen task perform well semeval two thousand and ten task eight datasetfootnotethe chinese sanwen corpus paper develop use release
huge volumes textual information produce every single day order organize understand large datasets recent years summarization techniques become popular techniques aim find relevant concise non redundant content big data network methods adopt model texts scenarios systematic evaluation multilayer network model multi document summarization task limit study evaluate performance multilayer base method select relevant sentence context extractive multi document summarization mds task adopt model nod represent sentence edge create base number share word sentence differently previous study multi document summarization make distinction edge link sentence different document inter layer connect sentence document intra layer proof principle result reveal discrimination intra inter layer multilayered representation able improve quality generate summaries piece information could use improve current statistical methods relate textual model
rubystar dialog system design create human like conversation combine different response generation strategies rubystar conduct non task orient conversation general topics use ensemble rule base retrieval base generative methods topic detection engagement monitor context track use manage interaction predictable elements conversation bot backstory simple question answer handle separate modules describe rat scheme develop evaluate response generation find character level rnn effective generation model general responses proper parameter settings however kinds conversation topics might benefit use model
paper show distributionally induce semantic class helpful extract hypernyms present methods induce sense aware semantic class use distributional semantics use induce semantic class filter noisy hypernymy relations denoising hypernyms perform label semantic class hypernyms one hand allow us filter wrong extractions use global structure distributionally similar sense hand infer miss hypernyms via label propagation cluster term conduct large scale crowdsourcing study show process automatically extract hypernyms use approach improve quality hypernymy extraction term precision recall furthermore show utility method domain taxonomy induction task achieve state art result semeval sixteen task taxonomy induction
extract relations text corpora important task text mine become particularly challenge focus weakly supervise relation extraction utilize relation instance ie pair entities relation seed extract instance corpora exist distributional approach leverage corpus level co occurrence statistics entities predict relations require large number label instance learn effective relation classifiers alternatively pattern base approach perform bootstrapping apply neural network model local contexts still rely large number label instance build reliable model paper study integrate distributional pattern base methods weakly supervise set two type methods provide complementary supervision build effective unify model propose novel co train framework distributional module pattern module train distributional module help pattern module discriminate informative pattern pattern pattern module generate highly confident instance improve distributional module whole framework effectively optimize iterate improve pattern module update distributional module conduct experiment two task knowledge base completion text corpora corpus level relation extraction experimental result prove effectiveness framework weakly supervise set
read comprehension rc challenge task require synthesis information across sentence multiple turn reason use state art rc model empirically investigate performance single turn multiple turn reason squad ms marco datasets rc model end end neural network iterative attention use reinforcement learn dynamically control number turn find multiple turn reason outperform single turn reason question answer type observe enable flexible number turn generally improve upon fix multiple turn strategy across question type particularly beneficial question lengthy descriptive answer achieve result competitive state art two datasets
dialog state track dst crucial component task orient dialog system conversational information access common practice current dialog systems define dialog state set slot value pair representation dialog state slot fill base dst widely employ suffer three drawbacks one dialog state contain single value slot two contain users affirmative preference value slot three current task base dialog systems mainly focus search task enquire task also common practice observations motivate us enrich current representation dialog state collect brand new dialog dataset movies base upon build new dst call enrich dst edst flexible access movie information edst support search task enquire task mix task show new edst method achieve good result iqiyi dataset also outperform state art dst methods traditional dialog datasets woz20 dstc2
lately problem code switch gain lot attention emerge active area research bilingual communities speakers commonly embed word phrase non native language syntax native language day day communications code switch global phenomenon among multilingual communities still limit acoustic linguistic resources available yet develop effective speech base applications ability exist language technologies deal code switch data emphasize code switch broadly classify two modes inter sentential intra sentential code switch work study intra sentential problem context code switch language model task salient contributions paper include creation hindi english code switch text corpus crawl blogging sit educate usage internet ii exploration part speech feature towards effective model hindi english code switch data monolingual language model lm train native hindi language data iii proposal novel textual factor refer code switch factor cs factor allow lm predict code switch instance context recognition code switch data substantial reduction ppl achieve use pos factor also propose cs factor provide independent well additive gain ppl
tree structure neural network architectures sentence encode draw inspiration approach semantic composition generally see formal linguistics show empirical improvements comparable sequence model moreover add multiplicative interaction term composition function model yield significant improvements however exist compositional approach adopt powerful composition function scale poorly parameter count explode model dimension vocabulary size grow introduce lift matrix space model use global transformation map vector word embeddings matrices compose via operation base matrix matrix multiplication composition function effectively transmit larger number activations across layer relatively model parameters evaluate model stanford nli corpus multi genre nli corpus stanford sentiment treebank find consistently outperform treelstm tai et al two thousand and fifteen previous best know composition function tree structure model
present document level neural machine translation model take source target document context account use memory network model problem structure prediction problem interdependencies among observe hide variables ie source sentence unobserved target translations document result structure prediction problem tackle neural translation model equip two memory components one source target side capture documental interdependencies train model end end propose iterative decode algorithm base block coordinate descent experimental result english translations french german estonian document show model effective exploit source target document context statistically significantly outperform previous work term bleu meteor
task orient dialogue systems efficiently serve large number customers relieve people tedious work however exist task orient dialogue systems depend handcraft action state extra semantic label sometimes degrade user experience despite intensive human intervention moreover current user simulators limit expressive ability deep reinforcement seq2seq model rely selfplay work special case address problems propose user agent model integration samia framework inspire observation roles user agent model asymmetric firstly samia framework model user model seq2seq learn problem instead rank design rule build user model use leverage train agent model deep reinforcement learn test phase output agent model filter user model enhance stability robustness experiment real world coffee order dataset verify effectiveness propose samia framework
read comprehension challenge task natural language process require set skills solve current approach focus solve task whole paper propose use neural network skill transfer approach transfer knowledge several lower level language task skills include textual entailment name entity recognition paraphrase detection question type classification read comprehension model conduct empirical evaluation show transfer language skill knowledge lead significant improvements task much fewer step compare baseline model also show skill transfer approach effective even small amount train data another find work use token wise deep label supervision text classification improve performance transfer learn
paper introduce textscyedda lightweight efficient comprehensive open source tool text span annotation textscyedda provide systematic solution text span annotation range collaborative user annotation administrator evaluation analysis overcome low efficiency traditional text annotation tool annotate entities command line shortcut key configurable custom label textscyedda also give intelligent recommendations learn date annotate text administrator client develop evaluate annotation quality multiple annotators generate detail comparison report annotator pair experiment show propose system reduce annotation time half compare exist annotation tool annotation time compress one thousand, six hundred and forty-seven intelligent recommendation
supervise approach text summarisation suffer problem mismatch target label score individual sentence evaluation score final summary reinforcement learn solve problem provide learn mechanism use score final summary guide determine decisions make time selection sentence paper present proof concept approach apply policy gradient algorithm learn stochastic policy use undiscounted reward method apply policy consist simple neural network simple feature result deep reinforcement learn system able learn global policy obtain encourage result
international classification diseasesicd authoritative health care classification system different diseases condition clinical management purpose consider complicate dedicate process assign correct cod patient admission base overall diagnosis propose hierarchical deep learn model attention mechanism automatically assign icd diagnostic cod give write diagnosis utilize character aware neural language model generate hide representations write diagnosis descriptions icd cod design attention mechanism address mismatch number descriptions correspond cod experimental result show strong potential automate icd cod diagnosis descriptions best model achieve fifty-three ninety f1 score area curve receiver operate characteristic respectively result outperform achieve use character unaware encode method without attention mechanism indicate propose deep learn model code automatically reasonable way provide framework computer auxiliary icd cod
social media platforms contain great wealth information provide us opportunities explore hide pattern unknown correlations understand people satisfaction discuss one showcase paper summarize data set twitter message relate recent demonetization rs five hundred rs one thousand note india explore insights twitter data propose system automatically extract popular latent topics conversations regard demonetization discuss twitter via latent dirichlet allocation lda base topic model also identify correlate topics across different categories additionally also discover people opinions express tweet relate event consideration via emotion analyzer system also employ intuitive informative visualization show uncover insight furthermore use evaluation measure normalize mutual information nmi select best lda model obtain lda result show tool effectively use extract discussion topics summarize manual analysis
consider probabilistic topic model recent word embed techniques perspective learn hide semantic representations inspire strike similarity two approach merge learn probabilistic embeddings online algorithm word co occurrence data result embeddings perform par skip gram negative sample sgns word similarity task benefit interpretability components next learn probabilistic document embeddings outperform paragraph2vec document similarity task require less memory time train finally employ multimodal additive regularization topic model artm obtain high sparsity learn embeddings modalities timestamps categories observe improvement word similarity performance meaningful inter modality similarities
attention mechanism include global attention local attention play key role neural machine translation nmt global attention attend source word word prediction comparison local attention selectively look fix window source word however alignment weight current target word often decrease leave right linear distance center align source position neglect syntax direct distance constraints paper extend local attention syntax distance constraint focus syntactically relate source word predict target word thus learn effective context vector word prediction moreover propose double context nmt architecture consist global context vector syntax direct context vector global attention provide translation performance nmt source representation experiment large scale chinese english english germen translation task show propose approach achieve substantial significant improvement baseline system
model natural language inference challenge task availability large annotate data recently become feasible train complex model neural network base inference model show achieve state art performance although exist relatively large annotate data machine learn knowledge need perform natural language inference nli data neural network base nli model benefit external knowledge build nli model leverage paper enrich state art neural natural language inference model external knowledge demonstrate propose model improve neural nli model achieve state art performance snli multinli datasets
state art deep read comprehension model dominate recurrent neural net sequential nature natural fit language also preclude parallelization within instance often become bottleneck deploy model latency critical scenarios particularly problematic longer texts present convolutional architecture alternative recurrent architectures use simple dilate convolutional units place recurrent ones achieve result comparable state art two question answer task time achieve two order magnitude speedups question answer
character base sequence label framework flexible efficient chinese word segmentation cws recently many character base neural model apply cws obtain good performance two obvious weaknesses first heavily rely manually design bigram feature ie good capture n gram feature automatically second make use full word information first weakness propose convolutional neural model able capture rich n gram feature without feature engineer second one propose effective approach integrate propose model word embeddings evaluate model two benchmark datasets pku msr without feature engineer model obtain competitive performance nine hundred and fifty-seven pku nine hundred and seventy-three msr arm word embeddings model achieve state art performance datasets nine hundred and sixty-five pku nine hundred and eighty msr without use external label resource
neural machine translation nmt new approach machine translation prove outperform conventional statistical machine translation smt across variety language pair translation open vocabulary problem exist nmt systems operate fix vocabulary cause incapability translate rare word problem alleviate use different translation granularities character subword hybrid word character translation involve chinese one difficult task machine translation however best knowledge work explore translation granularity suitable chinese nmt paper conduct extensive comparison use chinese english nmt case study furthermore discuss advantage disadvantage various translation granularities detail experiment show subword model perform best chinese english translation vocabulary big hybrid word character model suitable english chinese translation moreover experiment different granularities show hybridbpe method achieve best result chinese english translation task
prose style transfer task system provide text input target prose style produce output preserve mean input text alter style systems require parallel data evaluation result usually make use parallel data train currently publicly available corpora task work identify high quality source align stylistically distinct text different versions bible provide standardize split train development test data public domain versions corpus corpus highly parallel since many bible versions include sentence align due presence chapter verse number within versions text addition corpus present result measure bleu pinc metrics several model train data serve baselines future research present data style transfer corpus believe unmatched quality may useful natural language task well
propose framework computer assist text edit apply translation post edit paraphrase proposal rely simple interactions human editor modify sentence mark tokens would like system change model generate new sentence reformulate initial sentence avoid mark word approach build upon neural sequence sequence model introduce neural network take input sentence along change markers model train translation bitext simulate post edit demonstrate advantage approach translation post edit simulate post edit also evaluate model paraphrase user study
paper present empirical comparison two strategies vietnamese part speech pos tag unsegmented text pipeline strategy consider output word segmenter input pos tagger ii joint strategy predict combine segmentation pos tag syllable also make comparison state art sota feature base neural network base model benchmark vietnamese treebank nguyen et al two thousand and nine experimental result show pipeline strategy produce better score pos tag unsegmented text joint strategy highest accuracy obtain use feature base model
much recent work train neural attention model sequence level use either reinforcement learn style methods optimize beam paper survey range classical objective function widely use train linear model structure prediction apply neural sequence sequence model experiment show losses perform surprisingly well slightly outperform beam search optimization like like setup also report new state art result iwslt fourteen german english translation well gigaword abstractive summarization larger wmt fourteen english french translation task sequence level train achieve four hundred and fifteen bleu par state art
paper present novel neural model dynamic fusion network dfn machine read comprehension mrc dfns differ state art model use dynamic multi strategy attention process passages question answer candidates jointly fuse attention vectors along dynamic multi step reason module generate answer use reinforcement learn input sample consist question passage list candidate answer instance dfn sample specific network architecture dynamically construct determine attention strategy apply many reason step take experiment show dfns achieve best result report race challenge mrc dataset contain real human read question wide variety type detail empirical analysis also demonstrate dfns produce attention vectors summarize information question passages answer candidates effectively popular mrc model
show explicit pragmatic inference aid correctly generate follow natural language instructions complex sequential task pragmatics enable model reason speakers produce certain instructions listeners react upon hear like previous pragmatic model use learn base listener speaker model build pragmatic speaker use base listener simulate interpretation candidate descriptions pragmatic listener reason counterfactually alternative descriptions extend model task sequential structure evaluation language generation interpretation show pragmatic inference improve state art listener model correctly interpret human instructions speaker model produce instructions correctly interpret humans diverse settings
paper describe neural semantic parser map natural language utterances onto logical form execute task specific environment knowledge base database produce response parser generate tree structure logical form transition base approach combine generic tree generation algorithm domain general operations define logical language generation process model structure recurrent neural network provide rich encode sentential context generation history make predictions tackle mismatch natural language logical form tokens various attention mechanisms explore finally consider different train settings neural semantic parser include fully supervise train annotate logical form give weakly supervise train denotations provide distant supervision unlabeled sentence knowledge base available experiment across wide range datasets demonstrate effectiveness parser
paper introduce dureader new large scale open domain chinese chine read comprehension mrc dataset design address real world mrc dureader three advantage previous mrc datasets one data source question document base baidu search baidu zhidao answer manually generate two question type provide rich annotations question type especially yes opinion question leave opportunity research community three scale contain 200k question 420k answer 1m document largest chinese mrc dataset far experiment show human performance well current state art baseline systems leave plenty room community make improvements help community make improvements dureader baseline systems post online also organize share competition encourage exploration model since release task significant improvements baselines
distant supervision ds well establish method relation extraction text base assumption knowledge base contain relation term pair sentence contain pair likely express relation paper use result crowdsourcing relation extraction task identify two problems ds data quality widely vary degree false positives across different relations observe causal connection relations consider ds method crowdsourcing data aggregation perform use ambiguity aware crowdtruth metrics use capture interpret inter annotator disagreement also present preliminary result use crowd enhance ds train data relation classification model without require crowd annotate entire set
two main contributions work one explore usage stack denoising autoencoder paragraph vector model learn task independent dense patient representations directly clinical note evaluate representations use feature multiple supervise setups compare performance sparse representations two understand interpret representations explore best encode feature within patient representations obtain autoencoder model calculate significance input feature train classifiers use pretrained representations input
current model document summarization disregard user preferences desire length style entities user might interest much document user already read present neural summarization model simple effective mechanism enable users specify high level attribute order control shape final summaries better suit need user input system produce high quality summaries follow user preferences without user input set control variables automatically full text cnn dailymail dataset outperform state art abstractive systems term f1 rouge1 four thousand and thirty-eight vs three thousand, nine hundred and fifty-three human evaluation
word embed model glove rely co occurrence statistics large corpus learn vector representations word mean vectors prove capture surprisingly fine grain semantic syntactic information may similarly expect co occurrence statistics use capture rich information relationships different word exist approach model relationships mostly rely manipulate pre train word vectors paper introduce novel method directly learn relation vectors co occurrence statistics end first introduce variant glove explicit connection word vectors pmi weight co occurrence vectors show relation vectors naturally embed result vector space
understand procedural language require anticipate causal effect action even explicitly state work introduce neural process network understand procedural text neural simulation action dynamics model complement exist memory architectures dynamic entity track explicitly model action state transformers model update state entities execute learn action operators empirical result demonstrate propose model reason unstated causal effect action allow provide accurate contextual information understand generate procedural text offer interpretable internal representations exist alternatives
although transfer learn show successful task like object speech recognition applicability question answer qa yet well study paper conduct extensive experiment investigate transferability knowledge learn source qa dataset target dataset use two qa model performance model toefl listen comprehension test tseng et al two thousand and sixteen mctest richardson et al two thousand and thirteen significantly improve via simple transfer learn technique movieqa tapaswi et al two thousand and sixteen particular one model achieve state art target datasets toefl listen comprehension test outperform previous best model seven finally show transfer learn helpful even unsupervised scenarios correct answer target qa dataset examples available
paper describe effective convolutional neural network framework identify expert question answer community approach use convolutional neural network combine user feature representations question feature representations compute score user get highest score expert question unlike prior work method measure expert base measure answer content quality identify expert require question sentence user embed feature identify expert remarkably model apply different languages different domains propose framework train two datasets first dataset stack overflow second one zhihu top one accuracy result experiment show framework outperform best baseline framework expert identification
neural machine translation source sequence word encode vector target sequence generate decode phase differently statistical machine translation associations source word possible target counterparts explicitly store source target word two end long information process procedure mediate hide state source encode target decode phase make possible source word incorrectly translate target word admissible equivalent counterparts target language paper seek somewhat shorten distance source target word procedure thus strengthen association mean method term bridge source target word embeddings experiment three strategies one source side bridge model source word embeddings move one step closer output target sequence two target side bridge model explore relevant source word embeddings prediction target sequence three direct bridge model directly connect source target word embeddings seek minimize errors translation ones others experiment analysis present paper demonstrate propose bridge model able significantly improve quality sentence translation general alignment translation individual source word target word particular
paper propose sequential neural encoder latent structure description snelsd model sentence model introduce latent chunk level representations conventional sequential neural encoders ie recurrent neural network rnns long short term memory lstm units consider compositionality languages semantic model snelsd model hierarchical structure include detection layer description layer detection layer predict boundaries latent word chunk input sentence derive chunk level vector word description layer utilize modify lstm units process chunk level vectors recurrent manner produce sequential encode output output vectors concatenate word vectors output chain lstm encoder obtain final sentence representation model parameters learn end end manner without dependency additional text chunk syntax parse natural language inference nli task sentiment analysis sa task adopt evaluate performance propose model experimental result demonstrate effectiveness propose snelsd model explore task dependent chunk pattern semantic model sentence furthermore propose method achieve better performance conventional chain lstms tree structure lstms task
paper present aicyber system nlpcc two thousand and seventeen share task two form vote three deep learn base system train character enhance word vectors well know bag word model
although linguistic typology long history computational approach recently gain popularity use distribute representations computational linguistics also become increasingly popular recent development learn distribute representations language typologically similar languages spatially close one another although empirical successes show language representations subject much typological probe paper first look whether type language representations empirically useful model transfer uralic languages deep neural network investigate typological feature encode representations attempt predict feature world atlas language structure various stag fine tune representations focus uralic languages find typological traits automatically infer accuracies well strong baseline
multimodal model prove outperform text base approach learn semantic representations however still remain unclear properties encode multimodal representations aspects outperform single modality representations happen process semantic compositionality different input modalities consider multimodal model originally motivate human concept representations assume correlate multimodal representations brain base semantics would interpret inner properties answer question end propose simple interpretation methods base brain base componential semantics first investigate inner properties multimodal representations correlate correspond brain base property vectors map distribute vector space interpretable brain base componential space explore inner properties semantic compositionality ultimately present paper shed light fundamental question natural language understand represent mean word combine word mean larger units
term diachronic text corpora may exhibit high degree semantic dynamics partially capture common notion semantic change new measure context volatility propose model degree term change context text collection time computation context volatility word rely significance value co occurrent term correspond co occurrence rank sequential time span define baseline present efficient computational approach order overcome problems relate computational issue data structure result evaluate synthetic document use simulate contextual change real example base british newspaper texts
dialogue act recognition dar challenge problem dialogue interpretation aim attach semantic label utterances characterize speaker intention currently many exist approach formulate dar problem range multi classification structure prediction suffer handcraft feature extensions attentive contextual structural dependencies paper consider problem dar viewpoint extend richer conditional random field crf structural dependencies without abandon end end train incorporate hierarchical semantic inference memory mechanism utterance model extend structure attention network linear chain conditional random field layer take account contextual utterances correspond dialogue act extensive experiment two major benchmark datasets switchboard dialogue act swda meet recorder dialogue act mrda datasets show method achieve better performance state art solutions problem remarkable fact method nearly close human annotator performance swda within two gap
recently researchers start pay attention detection temporal shift mean word however approach restrict efforts uncover change time thus neglect valuable dimension social political variability propose approach detect semantic shift different viewpoints broadly define set texts share specific metadata feature time period also social entity political party viewpoint learn semantic space word represent low dimensional neural embed vector challenge compare mean word one space mean another space measure size semantic shift compare effectiveness measure base optimal transformations two space measure base similarity neighbor word respective space experiment demonstrate combination two perform best show semantic shift occur time also along different viewpoints short period time evaluation demonstrate approach capture meaningful semantic shift help improve task contrastive viewpoint summarization ideology detection measure classification accuracy political texts also show two laws semantic change empirically show hold temporal shift also hold shift across viewpoints laws state frequent word less likely shift mean word many sense likely
present language independent unsupervised method build word embeddings use morphological expansion text model handle problem data sparsity yield improve word embeddings rely train word embeddings artificially generate sentence evaluate method use small size train set eleven test set word similarity task across seven languages english evaluate impact approach use large train set three standard test set method improve result across languages
present language independent unsupervised approach transform word embeddings source language target language use transformation matrix model handle problem data scarcity face many languages world yield improve word embeddings word target language rely transform embeddings word source language initially evaluate approach via word similarity task similar language pair hindi source urdu target language also evaluate method french german target languages english source language approach improve current state art result thirteen french nineteen german urdu saw increment sixteen initial baseline score explore prospect approach apply multiple model language transfer word two model thus solve problem miss word model evaluate word similarity word analogy task
describe paranmt 50m dataset fifty million english english sentential paraphrase pair generate pair automatically use neural machine translation translate non english side large parallel corpus follow wieting et al two thousand and seventeen hope paranmt 50m valuable resource paraphrase generation provide rich source semantic knowledge improve downstream natural language understand task show utility use paranmt 50m train paraphrastic sentence embeddings outperform supervise systems every semeval semantic textual similarity competition addition show use paraphrase generation
virtual agents become prominent channel interaction customer service customer interactions smooth however become almost comically bad instance human agent might need step salvage conversation detect bad conversations important since disappoint customer service may threaten customer loyalty impact revenue paper outline approach detect egregious conversations use behavioral cue user pattern agent responses user agent interaction use log two commercial systems show use feature improve detection f1 score around twenty use textual feature alone addition show feature common across two quite different domains arguably universal
introduce question answer mean representations qamrs represent predicate argument structure sentence set question answer pair also develop crowdsourcing scheme show qamrs label little train gather dataset five thousand sentence one hundred thousand question detail qualitative analysis demonstrate crowd generate question answer pair cover vast majority predicate argument relationships exist datasets include propbank nombank qa srl amr along many previously resourced ones include implicit arguments relations qamr data annotation code make publicly available enable future work best model complex phenomena
machine translation go radical revolution drive explosive development deep learn techniques use convolutional neural network cnn recurrent neural network rnn paper consider special case machine translation problems target convert natural language structure query language sql data retrieval relational database although generic cnn rnn learn grammar structure sql train sufficient sample accuracy train efficiency model could dramatically improve translation model deeply integrate grammar rule sql present new encoder decoder framework suite new approach include new semantic feature feed encoder grammar aware state inject memory decoder well recursive state management sub query techniques help neural network better focus understand semantics operations natural language save efforts sql grammar learn empirical evaluation real world database query show approach outperform state art solution significant margin
convolutional neural network cnn recently achieve remarkable performance wide range applications research equip convolutional sequence sequence seq2seq model efficient graph linearization technique abstract mean representation parse linearization method better prior method signal turn graph travel additionally convolutional seq2seq model appropriate considerably faster recurrent neural network model task method outperform previous methods large margin standard dataset ldc2014t12 result indicate future work still room improve parse model use graph linearization approach
question answer come long way answer sentence selection relational qa read comprehension shift attention generative question answer gqa facilitate machine read passages answer question learn generate answer frame problem generative task encoder network model relationship question passage encode vector thus facilitate decoder directly form abstraction answer able retain facts make repetitions common mistake affect overall legibility answer counter issue employ copy mechanism maintenance coverage vector model respectively result ms marco demonstrate superiority baselines also show qualitative examples improve term correctness readability
speak word recognition involve least two basic computations first match acoustic input phonological categories eg b p second activate word consistent phonological categories test hypothesis listener probability distribution lexical items weight outcome computations uncertainty phonological discretisation frequency select word test record neural responses auditory cortex use magnetoencephalography model activity function size relative activation lexical candidates find indicate towards begin word process system indeed weight lexical candidates phonological certainty lexical frequency however later word activation weight frequency alone
anchor word algorithm perform provably efficient topic model inference find approximate convex hull high dimensional word co occurrence space however exist greedy algorithm often select poor anchor word reduce topic quality interpretability rather find approximate convex hull high dimensional space propose find exact convex hull visualizable two three dimensional space low dimensional embeddings improve topics clearly show users algorithm select certain word
style transfer important problem natural language process nlp however progress language style transfer lag behind domains computer vision mainly lack parallel data principle evaluation metrics paper propose learn style transfer non parallel data explore two model achieve goal key idea behind propose model learn separate content representations style representations use adversarial network also propose novel evaluation metrics measure two aspects style transfer transfer strength content preservation access model evaluation metrics two task paper news title transfer positive negative review transfer result show propose content preservation metric highly correlate human judgments propose model able generate sentence higher style transfer strength similar content preservation score compare auto encoder
computational synthesis plan approach achieve recent success organic chemistry tabulate synthesis procedures readily available supervise learn syntheses inorganic materials however exist primarily natural language narratives contain within scientific journal article synthesis information must first extract text order enable analogous synthesis plan methods inorganic materials work present system automatically extract structure representations synthesis procedures texts materials science journal article describe explicit experimental syntheses inorganic compound define structure representation set link events make extract scientific entities evaluate two unsupervised approach extract structure expert annotate article strong heuristic baseline generative model procedural text also evaluate variety supervise model extract scientific entities result provide insight nature data directions work excite new area research
one first study quantitatively examine usage english acronyms eg wto chinese texts use newspaper corpora try answer one instance concept english acronym eg world trade organization percentage express english acronym wto percentage chinese translation shijie maoyi zuzhi two factor play language users choice english chinese form result show different concepts different percentage english acronyms percentofen range two ninety-eight linear model show percentofen individual concepts predict language economy long chinese translation concept frequency whether first appearance concept chinese newspapers english acronym chinese translation p five
name entity recognition relation extraction chinese literature text regard highly difficult problem partially lack tag set paper build discourse level dataset hundreds chinese literature article improve task build high quality dataset propose two tag methods solve problem data inconsistency include heuristic tag method machine auxiliary tag method base corpus also introduce several widely use model conduct experiment experimental result show usefulness propose dataset also provide baselines research dataset available https githubcom lancopku chinese literature ner dataset
incorporate syntactic information neural machine translation model method compensate requirement large amount parallel train text especially low resource language pair previous work use syntactic information provide inevitably error prone parsers promise paper propose forest sequence attentional neural machine translation model make use exponentially many parse tree source sentence compensate parser errors method represent collection parse tree pack forest learn neural attentional transduction model forest target sentence experiment english german chinese persian translation show superiority method tree sequence vanilla sequence sequence neural translation model
paper propose novel btg forest base alignment method base fast unsupervised initialization parameters use variational ibm model synchronously parse parallel sentence top align hierarchically constraint btg two step method achieve run time comparable translation performance fastalign yield smaller phrase table final smt result show method even outperform experiment distantly relate languages eg english japanese
one crucial components natural human robot interaction artificial intuition influence dialog systems intuitive capability humans undeniably extraordinary remain one greatest challenge natural communicative dialogue humans robots paper introduce novel probabilistic model framework identify classify learn feature sarcastic text via train neural network human inform sarcastic benchmarks necessary establish comprehensive sentiment analysis schema sensitive nuances sarcasm ride text train linguistic cue show model provide good fit type real world inform data potential achieve accurate alternatives though implementation benchmarking extensive task extend via method present capture different form nuances communication make much natural engage dialogue systems
robust flexible event representations important many core areas language understand script propose early way represent sequence events understand recently attract renew attention however obtain effective representations model script like event sequence challenge require representations capture event level scenario level semantics propose new tensor base composition method create event representations method capture subtle semantic interactions event entities yield representations effective multiple event relate task continuous representations also devise simple schema generation method produce better schemas compare prior discrete representation base method analysis show tensors capture distinct usages predicate even subtle differences surface realizations
idiom translation challenge problem machine translation mean idioms non compositional literal word word translation likely wrong paper focus evaluate quality idiom translation mt systems introduce new evaluation method base idiom specific blacklist literal translations base insight occurrence blacklist word translation output indicate likely translation error introduce dataset cibb chinese idioms blacklist bank perform evaluation state art chinese english neural mt system evaluation confirm sizable number idioms test set mistranslate four hundred and sixty-one literal translation error common error type blacklist method effective identify literal translation errors
paper propose two strategies apply multilingual neural machine translation system order better tackle zero shoot scenarios despite parallel corpus experiment show effective term performance compute resources especially multilingual translation unbalance data real zero resourced condition alleviate language bias problem
biomedical name entity recognition ner fundamental task text mine medical document many applications deep learn base approach task gain increase attention recent years parameters learn end end without need hand engineer feature however approach rely high quality label data expensive obtain address issue investigate use unlabeled text data improve performance ner model specifically train bidirectional language model bilm unlabeled data transfer weight pretrain ner model architecture bilm result better parameter initialization ner model evaluate approach four benchmark datasets biomedical ner show lead substantial improvement f1 score compare state art approach also show bilm weight transfer lead faster model train pretrained model require fewer train examples achieve particular f1 score
sentiment analysis become important tool analysis social media data several methods develop research field many work differently cover distinct aspects problem disparate strategies despite large number existent techniques single one fit well case data source supervise approach may able adapt specific situations require manually label train cumbersome expensive acquire mainly new application context propose combine several popular effective state practice sentiment analysis methods mean unsupervised bootstrapped strategy polarity classification one main goals reduce large variability lack stability unsupervised methods across different domains datasets solution thoroughly test consider thirteen different datasets several domains opinions comment social media experimental result demonstrate combine method aka 10sent improve effectiveness classification task importantly solve key problem field consistently among best methods many data type mean produce best close best result almost consider contexts without additional cost eg manual label self learn approach also independent base methods mean highly extensible incorporate new additional method envision future finally also investigate transfer learn approach sentiment analysis mean gather additional unsupervised information propose approach show potential technique improve result
contrary natural language process research make use static datasets humans learn language interactively ground environment work propose interactive learn procedure call mechanical turker descent mtd use train agents execute natural language command ground fantasy text adventure game mtd turkers compete train better agents short term collaborate share agents skills long term result gamified engage experience turkers better quality teach signal agents compare static datasets turkers naturally adapt train data agent abilities
research customer satisfaction increase substantially recent years however relative importance relationships different determinants satisfaction remain uncertain moreover quantitative study date tend test significance pre determine factor think influence scalable mean identify cause user satisfaction gap knowledge make difficult use available knowledge user preference public service improvement meanwhile digital technology development enable new methods collect user feedback example online forums users comment freely experience new tool need analyze large volumes feedback use topic model propose feasible solution aggregate open end user opinions easily deploy public sector generate insights contribute inclusive decision make process public service provision novel methodological approach apply case service review publicly fund primary care practice england find analysis one hundred and forty-five thousand review cover almost seven thousand, seven hundred primary care center indicate quality interactions staff bureaucratic exigencies key issue drive user satisfaction across england
exist neural model usually predict tag current token independent neighbor tag popular lstm crf model consider tag dependencies every two consecutive tag however hard exist neural model take longer distance dependencies tag consideration scalability mainly limit complex model structure cost dynamic program train work first design new model call high order lstm predict multiple tag current token contain current tag also previous several tag call number tag one prediction order propose new method call multi order bilstm mo bilstm combine low order high order lstms together mo bilstm keep scalability high order model prune technique evaluate mo bilstm phrase chunk ner datasets experiment result show mo bilstm achieve state art result chunk highly competitive result two ner datasets
dialog response selection important step towards natural response generation conversational agents exist work neural conversational model mainly focus offline supervise learn use large set context response pair paper focus online learn response selection retrieval base dialog systems propose contextual multi arm bandit model nonlinear reward function use distribute representation text online response selection bidirectional lstm use produce distribute representations dialog context responses serve input contextual bandit learn bandit propose customize thompson sample method apply polynomial feature space approximate reward experimental result ubuntu dialogue corpus demonstrate significant performance gain propose method conventional linear contextual bandits moreover report encourage response selection performance propose neural bandit model use recallk metric small set online train sample
paper study transfer learn pi nli problems aim propose general framework effectively efficiently adapt share knowledge learn resource rich source domain resource poor target domain specifically since exist transfer learn methods focus learn share feature space across domains ignore relationship source target domains propose simultaneously learn share representations domain relationships unify framework furthermore propose efficient effective hybrid model combine sentence encode base method sentence interaction base method base model extensive experiment paraphrase identification natural language inference demonstrate base model efficient promise performance compare compete model transfer learn method help significantly boost performance analysis show inter domain intra domain relationship capture model insightful last least deploy transfer learn model pi online chatbot system bring significant improvements exist system finally launch new system chatbot platform eva e commerce site aliexpress
prediction without justification limit utility much success neural model attribute ability learn rich dense expressive representations representations capture underlie complexity latent trend data far interpretable propose novel variant denoising k sparse autoencoders generate highly efficient interpretable distribute word representations word embeddings begin exist word representations state art methods like glove word2vec large scale human evaluation report result word embedddings much interpretable original glove word2vec embeddings moreover embeddings outperform exist popular word embeddings diverse suite benchmark downstream task
use dialogue systems medium human machine interaction increasingly prevalent paradigm grow number dialogue systems use conversation strategies learn large datasets well document instance interactions system result bias even offensive conversations due data drive train process highlight potential ethical issue arise dialogue systems research include implicit bias data drive systems rise adversarial examples potential source privacy violations safety concern special considerations reinforcement learn systems reproducibility concern also suggest areas stem issue deserve investigation initial survey hope spur research lead robust safe ethically sound dialogue systems
word embeddings interface world discrete units text process continuous differentiable world neural network work examine various random pretrained initialization methods embeddings use deep network effect performance four nlp task recurrent convolutional architectures confirm pretrained embeddings little better random initialization especially consider speed learn hand see significant difference various methods random initialization long variance keep reasonably low high variance initialization prevent network use space embeddings force use free parameters accomplish task support hypothesis observe performance learn lexical relations fact network learn perform reasonably task even fix random embeddings
nowadays deep learn widely use natural language learn analysis complex semantics achieve high degree flexibility deceptive opinions detection important application area deep learn model relate mechanisms give attention research line opinions quite short vary type content order effectively identify deceptive opinions need comprehensively study characteristics deceptive opinions explore novel characteristics besides textual semantics emotional polarity widely use text analysis detection mechanism base deep learn better self adaptability effectively identify kinds deceptive opinions paper optimize convolution neural network model embed word order characteristics convolution layer pool layer make convolution neural network suitable various text classification deceptive opinions detection tensorflow base experiment demonstrate detection mechanism propose paper achieve accurate deceptive opinion detection result
acronyms omnipresent usually express information repetitive well know acronyms also ambiguous multiple expansions acronym paper propose general system acronym disambiguation work acronym give context information present methods retrieve possible expansions acronym wikipedia acronymsfindercom propose use expansions collect possible contexts acronyms use score use paragraph embed technique call doc2vec method collectively lead achieve accuracy nine hundred and nine select correct expansion give acronym dataset scrap wikipedia seven hundred and seven distinct acronyms fourteen thousand, eight hundred and seventy-six disambiguations
evaluate eight different word embed model usefulness predict neural activation pattern associate concrete nouns model consider include experiential model base crowd source association data several popular neural distributional model model reflect syntactic context word base dependency parse goal assess cognitive plausibility various embed model understand improve methods interpret brain image data show neural word embed model exhibit superior performance task consider beat experiential word representation model syntactically inform model give overall best performance predict brain activation pattern word embeddings whereas glove distributional method give overall best performance predict reverse direction word vectors brain image interestingly however error pattern different model markedly different may support idea brain use different systems process different kinds word moreover suggest take relative strengths different embed model account lead better model brain activity associate word
exist neural machine translation nmt model generally translate sentence isolation miss opportunity take advantage document level information work propose augment nmt model light weight cache like memory network store recent hide representations translation history probability distribution generate word update online depend translation history retrieve memory endow nmt model capability dynamically adapt time experiment multiple domains different topics style show effectiveness propose approach negligible impact computational cost
large number machine translation approach recently develop facilitate fluid migration content across languages however literature suggest many obstacles must still deal achieve better automatic translations one obstacles lexical syntactic ambiguity promise way overcome problem use semantic web technologies article present result systematic review machine translation approach rely semantic web technologies translate texts overall survey suggest semantic web technologies enhance quality machine translation output various problems combination still infancy
exist neural machine translation systems explicitly model translate decode phase address problem propose novel mechanism separate source information two part translate past content untranslated future content model two additional recurrent layer past future content feed attention model decoder state offer nmt systems knowledge translate untranslated content experimental result show propose approach significantly improve translation performance chinese english german english english german translation task specifically propose model outperform conventional coverage model translation quality alignment error rate
paper aim reveal impact lexical semantic resources use particular word sense disambiguation sense level semantic categorization automatic personality classification task stylistic feature eg part speech count show power task impact semantics beyond target word list relatively unexplored propose extract three type lexical semantic feature capture high level concepts emotions overcome lexical gap word n grams experimental result comparable state art methods personality specific resources require
recurrent neural language model state art model language model vocabulary size large space take store model parameters become bottleneck use recurrent neural language model paper introduce simple space compression method randomly share structure parameters input output embed layer recurrent neural language model significantly reduce size model parameters still compactly represent original input output embed layer method easy implement tune experiment several data set show new method get similar perplexity bleu score result use tiny fraction parameters
social media platforms recently see increase occurrence hate speech discourse lead call improve detection methods rely annotate data keywords classification technique approach provide good coverage fall short deal new term produce online extremist communities act original source word alternate hate speech mean code word create adopt word design evade automatic detection often benign mean regular discourse example skypes google yahoos instance word alternate mean use hate speech overlap introduce additional challenge rely keywords collection data specific hate speech downstream classification work develop community detection approach find extremist hate speech communities collect data members also develop word embed model learn alternate hate speech mean word demonstrate candidacy code word several annotation experiment design determine possible recognize word use hate speech without know alternate mean report inter annotator agreement rate k0871 k0676 data draw extremist community keyword approach respectively support claim hate speech detection contextual task depend fix list keywords goal advance domain provide high quality hate speech dataset addition learn code word feed exist classification approach thus improve accuracy automate detection
paper present new adversarial learn method generative conversational agents gca besides new model gca similar previous work adversarial learn dialogue generation method assume gca generator aim fool discriminator label dialogues human generate machine generate however approach discriminator perform token level classification ie indicate whether current token generate humans machine discriminator also receive context utterances dialogue history incomplete answer current token input new approach make possible end end train backpropagation self conversation process enable produce set generate data diversity adversarial train approach improve performance question relate train data experimental result human adversarial evaluations show adversarial method yield significant performance gain usual teacher force train
paper study semantic role label srl subtask semantic parse natural language sentence application vietnamese language present effort build vietnamese propbank first vietnamese srl corpus software system label semantic roles vietnamese texts particular present novel constituent extraction algorithm argument candidate identification step suitable accurate common node map method machine learn part system integrate distribute word feature produce two recent unsupervised learn model two learn statistical classifiers make use integer linear program inference procedure improve accuracy system evaluate series experiment achieve good result f1 score seven thousand, four hundred and seventy-seven system include corpus software available open source project free research believe good baseline development future vietnamese srl systems
paper compare two paradigms unsupervised discovery structure acoustic tokens directly speech corpora without human annotation multigranular paradigm seek capture available information corpora multiple set tokens different model granularities hierarchical paradigm attempt jointly learn several level signal representations hierarchical structure two paradigms unify within theoretical framework paper query example speak term detection qbe std experiment quesst dataset mediaeval two thousand and fifteen verify competitiveness acoustic tokens enhance relevance score ers propose work improve paradigms task qbe std also list result abx evaluation task zero resource challenge two thousand and fifteen comparison paradigms
recently acoustic word model base connectionist temporal classification ctc criterion show natural end end model directly target word output units however type word base ctc model suffer vocabulary oov issue model limit number word output layer map remain word oov output node therefore word base ctc model recognize frequent word model network output nod also easily handle hot word emerge model train study improve acoustic word model hybrid ctc model predict word character time share hide layer structure modular design alignments word generate word base ctc character base ctc synchronize whenever acoustic word model emit oov token back oov segment word output generate character base ctc hence solve oov hot word issue evaluate microsoft cortana voice assistant task propose model reduce errors introduce oov output token acoustic word model thirty
train transition base dependency parsers oracle use predict transition sequence sentence gold tree however transition system may exhibit ambiguity multiple correct transition sequence form gold tree propose make use property train neural dependency parsers present hybrid oracle new oracle give correct transition parse state use cross entropy loss function provide better supervisory signal also use generate different transition sequence sentence better explore train data improve generalization ability parser evaluations show parsers train use hybrid oracle outperform parsers use traditional oracle chinese dependency parse provide analysis linguistic view code available https githubcom lancopku nndep
investigate neural network learn process languages hierarchical compositional semantics end define artificial task process nest arithmetic expressions study whether different type neural network learn compute mean find recursive neural network find generalise solution problem visualise solution break three step project sum squash next step investigate recurrent neural network show gate recurrent unit process input incrementally also perform well task develop understand recurrent network encode visualisation techniques alone suffice therefore develop approach formulate test multiple hypotheses information encode process network hypothesis derive predictions feature hide state representations time step train diagnostic classifiers test predictions result indicate network follow strategy similar hypothesise cumulative strategy explain high accuracy network novel expressions generalisation longer expressions see train mild deterioration increase length turn show diagnostic classifiers useful technique open black box neural network argue diagnostic classification unlike visualisation techniques scale small network toy domain larger deeper recurrent network deal real life data may therefore contribute better understand internal dynamics current state art model natural language process
multi turn dialogs natural language understand model introduce obvious errors blind contextual information incorporate dialog history present neural architecture speaker sensitive dual memory network encode utterances differently depend speaker address different extents information available system system know surface form user utterances exact semantics system output perform experiment real user data microsoft cortana commercial personal assistant result show significant performance improvement state art slot tag model use contextual information
paper present neural network base task orient dialogue system optimize end end deep reinforcement learn rl system able track dialogue state interface knowledge base incorporate query result agent responses successfully complete task orient dialogues dialogue policy learn conduct hybrid supervise deep rl methods first train dialogue agent supervise manner learn directly task orient dialogue corpora optimize deep rl interaction users experiment two different dialogue task domains model demonstrate robust performance track dialogue state produce reasonable system responses show deep rl base optimization lead significant improvement task success rate reduction dialogue length compare supervise train model show benefit train task orient dialogue model end end compare component wise optimization experiment result dialogue simulations human evaluations
structure curriculum play vital role learn process children adults present material ascend order difficulty also exploit prior knowledge significant impact rate learn however notion difficulty prior knowledge differ person person motivate need personalise curriculum present novel method curriculum learn vocabulary word form visual prompt employ reinforcement learn model ground pedagogical theories emulate action tutor simulate three students different level vocabulary knowledge order evaluate well model adapt environment result simulation reveal interaction model able identify areas weakness well push students edge zpd hypothesise methods also effective train agents learn language representations simulate environment previously show order word prior knowledge play important role efficacy language learn
multiple adverse health condition co occur patient typically associate poor prognosis increase office hospital visit develop methods identify pattern co occur condition assist diagnosis thus identify pattern associations among co occur condition grow interest paper report preliminary result data drive study apply machine learn method namely topic model electronic medical record aim identify pattern associate condition specifically use well establish latent dirichlet allocation method base idea document model mixture latent topics topic distribution word study adapt lda model identify latent topics patients emrs evaluate performance method qualitatively show obtain topics indeed align well distinct medical phenomena characterize co occur condition
last couple decades social network service like twitter generate large volumes data users interest provide meaningful business intelligence organizations better understand engage customers businesses want know promote products complain opinions bring diminish value company company want able identify high value customers quantify value user bring many businesses use social media metrics calculate user contribution score enable quantify value influential users bring social media businesses offer differentiate service however score calculation refine provide better illustration user contribution use microsoft azure case study conduct twitter sentiment analysis develop machine learn classification model identify tweet content sentiments illustrative positive value user contribution use data mine ai power cognitive tool analyze factor social influence specifically promotional language developer community predictive model combination traditional supervise machine learn algorithm custom develop natural language model identify promotional tweet identify product specific promotion twitter ninety accuracy rate
broad goal information extraction derive structure information unstructured data however exist methods focus solely text ignore type unstructured data image video audio comprise increase portion information web address shortcoming propose task multimodal attribute extraction give collection unstructured semi structure contextual information entity textual description visual depictions task extract entity underlie attribute paper provide dataset contain mix media data two million product items along seven million attribute value pair describe items use train attribute extractors weakly supervise manner provide variety baselines demonstrate relative effectiveness individual modes information towards solve task well study human performance
recent work attempt characterize structure semantic memory search algorithms together best approximate human pattern search reveal semantic fluency task number model seek capture semantic search process network vary cognitive plausibility implementation exist work also neglect consider constraints incremental process language acquisition must place structure semantic memory present model incrementally update semantic network limit computational step replicate many pattern find human semantic fluency use simple random walk also perform thorough analyse show combination structural semantic feature correlate human performance pattern
study response generation open domain conversation chatbots exist methods assume word responses generate identical vocabulary regardless input make vulnerable generic pattern irrelevant noise also cause high cost decode propose dynamic vocabulary sequence sequence dvs2s model allow input possess vocabulary decode train vocabulary construction response generation jointly learn maximize lower bind true objective monte carlo sample method inference model dynamically allocate small vocabulary input word prediction model conduct decode small vocabulary dynamic vocabulary mechanism dvs2s elude many generic pattern irrelevant word generation enjoy efficient decode time experimental result automatic metrics human annotations show dvs2s significantly outperform state art methods term response quality require sixty decode time compare efficient baseline
sentence well form text connect via various link form cohesive structure text current neural machine translation nmt systems translate text conventional sentence sentence fashion ignore cross sentence link dependencies may lead generate incoherent target text coherent source text order handle issue propose cache base approach model coherence neural machine translation capture contextual information either recently translate sentence entire document particularly explore two type cache dynamic cache store word best translation hypotheses precede sentence topic cache maintain set target side topical word semantically relate document translate basis build new layer score target word two cache cache base neural model estimate probabilities cache base neural model combine nmt probabilities final word prediction probabilities via gate mechanism finally propose cache base neural model train jointly nmt system end end manner experiment analysis present paper demonstrate propose cache base model achieve substantial improvements several state art smt nmt baselines
goal paper learn cross domain representations slot fill task speak language understand slu recently publish slu model domain specific ones work individual task domains annotate data individual task domain financially costly non scalable work propose adversarial train method learn common feature representations share across multiple domains model produce share representations combine model train individual domain slu data reduce amount train sample require develop new domain experiment use data set multiple domains show adversarial train help learn better domain general slu model lead improve slot fill f1 score show apply adversarial learn domain general model also help achieve higher slot fill performance model jointly optimize domain specific model
sadrzadeh et al two thousand and thirteen present compositional distributional analysis relative clauses english term frobenius algebraic structure finite dimensional vector space analysis rely distinct type assignments lexical recipes subject vs object relativisation situation dutch different verb final nature dutch relative clauses ambiguous subject vs object relativisation read use extend version lambek calculus present compositional distributional framework account derivational ambiguity allow us give single mean recipe relative pronoun reconcile frobenius semantics demand dutch derivational syntax
data set identify alzheimer disease ad often relatively sparse limit ability train generalizable model augment data set dementiabank two normative data set wisconsin longitudinal study talk2me employ speech base picture description assessment minority class oversampling adasyn outperform state art result binary classification people without ad dementiabank work highlight effectiveness combine sparse difficult acquire patient data relatively large easily accessible normative datasets
paper propose model use generative adversarial net gin generate realistic text instead use standard gin combine variational autoencoder vae generative adversarial net use high level latent random variables helpful learn data distribution solve problem generative adversarial net always emit similar data propose vgan model generative model compose recurrent neural network vae discriminative model convolutional neural network train model via policy gradient apply propose model task text generation compare recent neural network base model recurrent neural network language model seqgan evaluate performance model calculate negative log likelihood bleu score conduct experiment three benchmark datasets result show model outperform previous model
sentence representation model train language could potentially suffer ground problem recent work show promise result improve qualities sentence representations jointly train associate image feature however ground capability limit due distant connection input sentence image feature design architecture order close gap propose apply self attention mechanism sentence encoder deepen ground effect result transfer task show self attentive encoders better visual ground exploit specific word strong visual associations
recent years word embeddings surprisingly effective capture intuitive characteristics word represent vectors achieve best result train corpora extremely large sometimes billions word clinical natural language process datasets however tend much smaller even largest publicly available dataset medical note three order magnitude smaller dataset oft use google news word vectors order make limit train data size encode expert domain knowledge embeddings build previous extension word2vec show generalize notion word context include arbitrary feature create avenue encode domain knowledge word embeddings show word vectors produce method outperform text counterparts across board correlation clinical experts
drill activities oil gas industry report decades thousands well daily basis yet analysis text large scale information retrieval sequence mine pattern analysis challenge drill report contain interpretations write drillers note measurements downhole sensors surface equipment use operation optimization accident mitigation initial work methodology propose automatic classification sentence write drill report three relevant label event symptom action hundreds well actual field main challenge text corpus overcome include high frequency technical symbols mistyping abbreviation technical term presence incomplete sentence drill report obtain state art classification accuracy within technical language illustrate advance query enable tool
semantic role label srl believe crucial step towards natural language understand widely study recent years end end srl recurrent neural network rnn gain increase attention however remain major challenge rnns handle structural information long range dependencies paper present simple effective architecture srl aim address problems model base self attention directly capture relationships two tokens regardless distance single model achieve f1834 conll two thousand and five share task dataset f1827 conll two thousand and twelve share task dataset outperform previous state art result eighteen ten f1 score respectively besides model computationally efficient parse speed 50k tokens per second single titan x gpu
use phylogenetic algebraic geometry analyze computationally phylogenetic tree subfamilies indo european language family use data syntactic structure two main source syntactic data sswl database longobardi recent data syntactic parameters compute phylogenetic invariants likelihood function two set germanic languages set romance languages set slavic languages set early indo european languages compare result know historical linguistics
access word sentiment associations useful many applications include sentiment analysis stance detection linguistic analysis however manually assign fine grain sentiment association score word many challenge respect keep annotations consistent apply annotation technique best worst scale obtain real value sentiment association score word phrase three different domains general english english twitter arabic twitter show three domains rank word sentiment remain remarkably consistent even annotation process repeat different set annotators also first time determine minimum difference sentiment association perceptible native speakers language
rat scale widely use method data annotation however present several challenge difficulty maintain inter intra annotator consistency best worst scale bws alternative method annotation claim produce high quality annotations keep require number annotations similar rat scale however veracity claim never systematically establish first time set experiment directly compare rat scale method bws show total number annotations bws produce significantly reliable result rat scale
negators modals degree adverbs significantly affect sentiment word modify often impact model simple heuristics although recent work show heuristics capture true sentiment multi word phrase create dataset phrase include various negators modals degree adverbs well combinations phrase constituent content word annotate real value score sentiment association use phrasal term create dataset analyze impact individual modifiers average effect group modifiers overall sentiment find effect modifiers vary substantially among members group furthermore individual modifier affect sentiment word different ways therefore solutions base statistical learn seem promise fix hand craft rule task automatic sentiment prediction
entity link el task disambiguate mention text associate entries predefined database mention persons organizations etc previous el research focus mainly one language english less attention pay languages spanish chinese paper introduce liel language independent entity link system provide el framework train one language work remarkably well number different languages without change liel make joint global prediction entire document employ discriminative reranking framework many domain language independent feature function experiment numerous benchmark datasets show propose system train one language english outperform several state art systems english four point train model also work well spanish fourteen point better competitor system demonstrate viability approach
major challenge entity link el make effective use contextual information disambiguate mention wikipedia might refer different entities different contexts problem exacerbate cross lingual el involve link mention write non english document entries english wikipedia compare textual clue across languages need compute similarity textual fragment across languages paper propose neural el model train fine grain similarities dissimilarities query candidate document multiple perspectives combine convolution tensor network show english train system apply zero shoot learn languages make surprisingly effective use multi lingual embeddings propose system strong empirical evidence yield state art result english well cross lingual spanish chinese tac two thousand and fifteen datasets
factor neural machine translation fnmt found idea use morphological grammatical decomposition word factor output side neural network architecture address two well know problems occur mt namely size target language vocabulary number unknown tokens produce translation fnmt system design manage larger vocabulary reduce train time systems equivalent target language vocabulary size moreover produce grammatically correct word part vocabulary fnmt model evaluate iwslt fifteen english french task compare baseline word base bpe base nmt systems promise qualitative quantitative result term bleu meteor report
examine problem question answer knowledge graph focus simple question answer lookup single fact adopt straightforward decomposition problem entity detection entity link relation prediction evidence combination explore simple yet strong baselines popular simplequestions dataset find basic lstms grus plus heuristics yield accuracies approach state art techniques use neural network also perform reasonably well result show gain sophisticate deep learn techniques propose literature quite modest previous model exhibit unnecessary complexity
product compatibility functionality utmost importance customers purchase products sellers manufacturers sell products due huge number products available online infeasible enumerate test compatibility functionality every product paper address two closely relate problems product compatibility analysis function satisfiability analysis second problem generalization first problem eg whether product work another product consider special function first identify novel question answer corpus date regard product compatibility functionality information allow automatic discovery product compatibility functionality propose deep learn model call dual attention network dan give qa pair purchase product dan learn one discover complementary products function two accurately predict actual compatibility satisfiability discover products function challenge address model include briefness qas linguistic pattern indicate compatibility appropriate fusion question answer conduct experiment quantitatively qualitatively show identify products function high coverage accuracy compare wide spectrum baselines
attention base encoder decoder effective architecture neural machine translation nmt typically rely recurrent neural network rnn build block lately call attentive reader decode process design encoder yield relatively uniform composition source sentence despite gate mechanism employ encode rnn hand often hope decoder take piece source sentence vary level suit linguistic structure example may want take entity name raw form take idiom perfectly compose unit motivate demand propose multi channel encoder mce enhance encode components different level composition specifically addition hide state encode rnn mce take one original word embed raw encode composition two particular design external memory neural turing machine ntm complex composition three encode strategies properly blend decode empirical study chinese english translation show model improve six hundred and fifty-two bleu point upon strong open source nmt system dl4mt1 wmt14 english french task single shallow system achieve bleu388 comparable state art deep model
paper propose novel embed model name convkb knowledge base completion model convkb advance state art model employ convolutional neural network capture global relationships transitional characteristics entities relations knowledge base convkb triple head entity relation tail entity represent three column matrix column vector represent triple element three column matrix feed convolution layer multiple filter operate matrix generate different feature map feature map concatenate single feature vector represent input triple feature vector multiply weight vector via dot product return score score use predict whether triple valid experiment show convkb achieve better link prediction performance previous state art embed model two benchmark datasets wn18rr fb15k two hundred and thirty-seven
functionality utmost importance customers purchase products however unclear customers whether product really satisfy need function miss function may intentionally hide manufacturers sellers result customer need spend fair amount time purchase purchase product risk paper first identify novel qa corpus dense product functionality information footnotethe annotate corpus find urlhttps wwwcsuicedu hxu design neural network call semi supervise attention network san discover product function question model leverage unlabeled data contextual information perform semi supervise sequence label conduct experiment show extract function high coverage accuracy compare wide spectrum baselines
paper compose new task deep argumentative structure analysis go beyond shallow discourse structure analysis idea argumentative relations reasonably represent small set predefined pattern example use value judgment bipolar causality explain support relation two argumentative segment follow segment one state something good segment two state good promote something good happen motivate follow question formulate task ii reasonable pattern set create iii pattern work examine task feasibility conduct three stage detail annotation study use three hundred and fifty-seven argumentative relations argumentative microtext corpus small highly reliable corpus report coverage explanations capture pattern test set compose two hundred and seventy relations coverage result seven hundred and forty-six indicate argumentative relations reasonably explain small pattern set agreement result eight hundred and fifty-nine show reasonable inter annotator agreement achieve assist future work computational argumentation annotate corpus make publicly available
neural architecture purely numeric framework fit data continuous function however lack logic flow eg textitif traditional algorithms eg textithungarian algorithm search decision tress algorithm could embed paradigm limit theories applications paper reform calculus graph dynamic process guide logic flow within novel methodology traditional algorithms could empower numerical neural network specifically regard subject sentence match reformulate issue form task assignment solve hungarian algorithm first model apply bilstm parse sentence hungarian layer align match position last transform match result soft max regression another bilstm extensive experiment show model outperform state art baselines substantially
textbfobjective develop automatic diagnostic system use textual admission information electronic health record ehrs assist clinicians timely statistically prove decision tool hope tool use reduce mis diagnosis textbfmaterials methods use real world clinical note mimic iii freely available dataset consist clinical data forty thousand patients stay intensive care units beth israel deaconess medical center two thousand and one two thousand and twelve propose convolutional neural network model learn semantic feature unstructured textual input automatically predict primary discharge diagnosis textbfresults propose model achieve overall nine thousand, six hundred and eleven accuracy eight thousand and forty-eight weight f1 score value ten frequent disease class significantly outperform four strong baseline model least one hundred and twenty-seven weight f1 score textbfdiscussion experimental result imply cnn model suitable support diagnosis decision make presence complex noisy unstructured clinical data time use fewer layer parameters traditional deep network model textbfconclusion model demonstrate capability represent complex medical meaningful feature unstructured clinical note prediction power commonly misdiagnosed frequent diseases use easily adopt clinical set provide timely statistically prove decision support textbfkeywords convolutional neural network text classification discharge diagnosis prediction admission information ehrs
present simple yet elegant solution train single joint model multi criteria corpora chinese word segmentation cws novel design require private layer model architecture instead introduce two artificial tokens begin end input sentence specify require target criteria rest model include long short term memory lstm layer conditional random field crfs layer remain unchanged share across datasets keep size parameter collection minimal constant bakeoff two thousand and five bakeoff two thousand and eight datasets innovative design surpass single criterion multi criteria state art learn result best knowledge design first one achieve latest high performance large scale datasets source cod corpora paper available github
neural machine translation model replace conventional phrase base statistical translation methods since former take generic scalable data drive approach rather rely manual hand craft feature neural machine translation system base one neural network compose two part one responsible input language sentence part handle desire output language sentence model base encoder decoder architecture also take input distribute representations source language enrich learn dependencies give warm start network work transform roman urdu urdu transliteration sequence sequence learn problem end make follow contributions create first ever parallel corpora roman urdu urdu create first ever distribute representation roman urdu present first neural machine translation model transliterate text roman urdu urdu language model achieve state art result use bleu evaluation metric precisely model able correctly predict sentence length ten achieve bleu score four hundred and eighty-six test set hopeful model result shall serve baseline work domain neural machine translation roman urdu urdu use distribute representation
apply state art deep learn model novel real world datasets give practical evaluation generalizability model importance process sensitive hyper parameters model novel datasets would affect reproducibility model present work characterize hyper parameter space lstm language model code mix corpus observe evaluate model show minimal sensitivity novel dataset bar hyper parameters
recently yuan et al two thousand and sixteen show effectiveness use long short term memory lstm perform word sense disambiguation wsd propose technique outperform previous state art several benchmarks neither train data source code release paper present result reproduction study technique use openly available datasets gigaword semcore omsti software tensorflow emerge state art result obtain much less data hint yuan et al code train model make freely available
understand customer sentiment useful product development top priorities development order know development procedure become simpler work try address issue mobile app domain along aspect opinion extraction work also categorize extract aspects ac cord importance help developers focus time energy right place
propose new fully end end approach multimodal translation source text encoder modulate entire visual input process use conditional batch normalization order compute informative image feature task additionally propose new attention mechanism derive original idea attention model visual input condition source text encoder representations paper detail model well image analysis pipeline finally report experimental result far know new state art three different test set
paper study problem map natural language instructions complex spatial action 3d block world first introduce new dataset pair complex 3d spatial operations rich natural language descriptions require complex spatial pragmatic interpretations mirror twist balance dataset build simulation environment bisk yuret marcu two thousand and sixteen attain language significantly richer complex also double size original dataset 2d environment one hundred new world configurations two hundred and fifty thousand tokens addition propose new neural architecture achieve competitive result automatically discover inventory interpretable spatial operations figure five
introduce initial groundwork estimate suicide risk mental health deep learn framework model multiple condition system learn make predictions suicide risk mental health low false positive rate condition model task multi task learn mtl framework gender prediction additional auxiliary task demonstrate effectiveness multi task learn comparison well tune single task baseline number parameters best mtl model predict potential suicide attempt well presence atypical mental health auc eight also find additional large improvements use multi task learn mental health task limit train data
study problem induce interpretability kg embeddings specifically explore universal schema riedel et al two thousand and thirteen propose method induce interpretability many vector space model propose problem however methods address interpretability semantics individual dimension work study problem propose method induce interpretability kg embeddings use entity co occurrence statistics propose method significantly improve interpretability maintain comparable performance kg task
propose simple yet robust stochastic answer network san simulate multi step reason machine read comprehension compare previous work reasonet use reinforcement learn determine number step unique feature use kind stochastic prediction dropout answer module final layer neural network train show simple trick improve robustness achieve result competitive state art stanford question answer dataset squad adversarial squad microsoft machine read comprehension dataset ms marco
read document extract answer question content attract substantial attention recently work focus interaction question document work evaluate importance context question document process independently take standard neural architecture task show provide rich contextualized word representations large pre train language model well allow model choose context dependent context independent word representations obtain dramatic improvements reach performance comparable state art competitive squad dataset
task event extraction long investigate supervise learn paradigm bind number quality train instance exist train data must manually generate combination expert domain knowledge extensive human involvement however due drastic efforts require annotate text resultant datasets usually small severally affect quality learn model make hard generalize work develop automatic approach generate train data event extraction approach allow us scale event extraction train instance thousands hundreds thousands much lower cost manual approach achieve employ distant supervision automatically create event annotations unlabelled text use exist structure knowledge base tableswe develop neural network model post inference transfer knowledge extract structure knowledge base automatically annotate type events correspond arguments textwe evaluate approach use knowledge extract freebase label texts wikipedia article experimental result show approach generate large number high quality train instance show large volume train data lead better event extractor also allow us detect multiple type events
identify veracity news article interest problem automate process challenge task detection news article fake still open question contingent many factor current state art model fail incorporate paper explore subtask fake news identification stance detection give news article task determine relevance body claim present novel idea combine neural statistical external feature provide efficient solution problem compute neural embed deep recurrent model statistical feature weight n gram bag word model handcraft external feature help feature engineer heuristics finally use deep neural layer feature combine thereby classify headline body news pair agree disagree discuss unrelated compare propose technique current state art model fake news challenge dataset extensive experiment find propose model outperform state art techniques include submissions fake news challenge
chinese input methods use convert pinyin sequence latin encode systems chinese character sentence effective pinyin character conversion typical input method engines imes rely predefined vocabulary demand manually maintenance schedule purpose remove inconvenient vocabulary set work focus automatic wordhood acquisition fully consider chinese inputting free human computer interaction procedure instead strictly define word loose word likelihood introduce measure likely character sequence user recognize word respect use ime online algorithm propose adjust word likelihood generate new word compare user true choice inputting algorithm prediction experimental result show propose solution agilely adapt diverse type demonstrate performance approach highly optimize ime fix vocabulary
describe new challenge aim discover subword word units raw speech challenge followup zero resource speech challenge two thousand and fifteen aim construct systems generalize across languages adapt new speakers design feature evaluation metrics challenge present result seventeen model discuss
present approach computer aid social media text authorship attribution base recent advance short text authorship verification use various natural language techniques create word level character level model act hide layer simulate simple neural network choice word level character level model layer inform validation performance output layer system use unweighted majority vote vector arrive conclusion also consider write bias social media post collect train dataset increase system robustness system achieve precision recall f measure eighty-two nine hundred and twenty-six eight hundred and sixty-nine respectively
advantage neural machine translation nmt extensively validate offline translation several language pair different domains speak write language however research interactive learn nmt adaptation human post edit far confine simulation experiment present first user study online adaptation nmt user post edit domain patent translation study involve twenty-nine human subject translation students whose post edit effort translation quality measure four thousand, five hundred interactions human post editor machine translation system integrate online adaptive learn algorithm experimental result show significant reduction human post edit effort due online adaptation nmt accord several evaluation metrics include hter hbleu ksmr furthermore find significant improvements bleu ter nmt output professional translations grant patent provide evidence advantage online adaptive nmt interactive setup
brazil legal professionals must demonstrate knowledge law application pass oab exams national bar exams oab exams therefore provide excellent benchmark performance legal information systems since pass exam would arguably signal system acquire capacity legal reason comparable human lawyer article describe construction new data set preliminary experiment treat problem find justification answer question result provide baseline performance measure evaluate future improvements discuss reason poor performance propose next step
many recent advance deep learn natural language process come increase computational cost power state art model need every example dataset demonstrate two approach reduce unnecessary computation case fast weak baseline classier stronger slower model available apply auc base metric task sentiment classification find significant efficiency gain probability threshold method reduce computational cost one use secondary decision network
retrieval base conversation systems generally tend highly rank responses semantically similar even identical give conversation context system goal find appropriate response rather semantically similar one tendency result low quality responses refer challenge echo problem mitigate problem utilize hard negative mine approach train stage evaluation show result model reduce echo achieve better result term average precision recalln metrics compare model train without propose approach
end end model goal orientate dialogue challenge train linguistic strategic aspects entangle latent state vectors introduce approach learn representations message dialogues maximize likelihood subsequent sentence action decouple semantics dialogue utterance linguistic realization use latent sentence representations hierarchical language generation plan reinforcement learn experiment show approach increase end task reward achieve model improve effectiveness long term plan use rollouts allow self play reinforcement learn improve decision make without diverge human language hierarchical latent variable model outperform previous work linguistically strategically
paper describe tacotron two neural network architecture speech synthesis directly text system compose recurrent sequence sequence feature prediction network map character embeddings mel scale spectrograms follow modify wavenet model act vocoder synthesize timedomain waveforms spectrograms model achieve mean opinion score mos four hundred and fifty-three comparable mos four hundred and fifty-eight professionally record speech validate design choices present ablation study key components system evaluate impact use mel spectrograms input wavenet instead linguistic duration f0 feature demonstrate use compact acoustic intermediate representation enable significant simplification wavenet architecture
negative uncertain medical find frequent radiology report discriminate positive find remain challenge information extraction propose new algorithm negbio detect negative uncertain find radiology report unlike previous rule base methods negbio utilize pattern universal dependencies identify scope trigger indicative negation uncertainty evaluate negbio four datasets include two public benchmarking corpora radiology report new radiology corpus annotate work public corpus general clinical texts evaluation datasets demonstrate negbio highly accurate detect negative uncertain find compare favorably widely use state art system negex average ninety-five improvement precision fifty-one f1 score
zero shoot learners model capable predict unseen class work propose zero shoot learn approach text categorization method involve train model large corpus sentence learn relationship sentence embed sentence tag learn relationship make model generalize unseen sentence tag even new datasets provide put embed space model learn predict whether give sentence relate tag unlike classifiers learn classify sentence one possible class propose three different neural network task report accuracy test set dataset use train well two standard datasets retrain do show model generalize well across new unseen class case although model achieve accuracy level state art supervise model yet evidently step forward towards general intelligence natural language process
paper present model generate summaries text document respect query know query base summarization adapt exist dataset news article summaries task train pointer generator model use dataset generate summaries evaluate measure similarity reference summaries result show neural network summarization model similar exist neural network model abstractive summarization construct make use query produce target summaries
give grow assortment sentiment measure instrument imperative understand aspects sentiment dictionaries contribute classification accuracy ability provide richer understand texts perform detail quantitative test qualitative assessments six dictionary base methods apply briefly examine twenty methods show inappropriate sentence dictionary base methods generally robust classification accuracy longer texts stories often follow distinct emotional trajectories form pattern meaningful us classify emotional arc filter subset four thousand, eight hundred and three stories project gutenberg fiction collection find set six core trajectories form build block complex narratives profound scientific interest degree eventually understand full landscape human stories data drive approach play crucial role finally utilize web scale data twitter study limit social data tell us public health mental illness discourse around protest movement blacklivesmatter discourse around climate change hide network conclude review publish work complex systems separately analyze charitable donations happiness word ten languages one hundred years daily temperature data across unite state australian rule football game
present second ever evaluate arabic dialect dialect machine translation effort first leverage external resources beyond small parallel corpus subject previously receive serious attention due lack naturally occur parallel data yet importance evidence dialectal arabic wide usage breadth inter dialect variation comparable romance languages result suggest model morphology syntax significantly improve dialect dialect translation though optimize data sparse model require consideration linguistic differences dialects nature available data resources single reference blind test set untranslated input score sixty-five bleu model train parallel data reach one hundred and forty-six pivot techniques morphosyntactic model significantly improve performance one hundred and seventy-five
abbreviation common phenomenon across languages especially chinese case expression abbreviate abbreviation use often fully expand form since people tend convey information concise way various language process task abbreviation obstacle improve performance textual form abbreviation express useful information unless expand full form abbreviation prediction mean associate fully expand form abbreviations however due deficiency abbreviation corpora task limit current study especially consider general abbreviation prediction also include full form expressions valid abbreviations namely negative full form nffs corpora incorporate negative full form general abbreviation prediction number order promote research area build dataset general chinese abbreviation prediction need preprocessing step evaluate several different model build dataset dataset available https githubcom lancopku chinese abbreviation dataset
paper examine methods detect hate speech social media distinguish general profanity aim establish lexical baselines task apply supervise classification methods use recently release dataset annotate purpose feature system use character n grams word n grams word skip grams obtain result seventy-eight accuracy identify post across three class result demonstrate main challenge lie discriminate profanity hate speech number directions future work discuss
text process one sub branch natural language process recently use machine learn neural network methods give greater consideration reason representation word become important article word representation convert word vectors persian text research glove cbow skip gram methods update produce embed vectors persian word order train neural network bijankhan corpus hamshahri corpus upec corpus compound use finally three hundred and forty-two thousand, three hundred and sixty-two word obtain vectors three model word vectors many usage persian natural language process
paper propose novel approach create unit set ctc base speech recognition systems use byte pair encode learn unit set arbitrary size give train text contrast use character word units allow us find good trade size unit set available train data evaluate crossword units may span multiple word subword units combine approach decode methods use separate language model able achieve state art result grapheme base ctc systems
find analogical inspirations distant domains powerful way solve problems however number inspirations could match dimension match could occur grow become challenge designers find inspirations relevant need furthermore designers often interest explore specific aspects product example one designer might interest improve brew capability outdoor coffee maker another might wish optimize portability paper introduce novel system target analogical search specific need specifically contribute novel analogical search engine express abstract specific design need return distant yet relevant inspirations alternate approach
exist methods automatic bilingual dictionary induction rely prior alignments source target languages parallel corpora seed dictionaries many language pair supervise alignments readily available propose unsupervised approach learn bilingual dictionary pair languages give independently learn monolingual word embeddings propose method exploit local global structure monolingual vector space align similar word map show empirically performance bilingual correspondents learn use propose unsupervised method comparable use supervise bilingual correspondents seed dictionary
paper present simple yet sophisticate approach challenge sproat jaitly two thousand and sixteen give large corpus write text align normalize speak form train rnn learn correct normalization function text normalization token seem straightforward without context give context use token normalize become tricky class present novel approach prediction classification algorithm use sequence sequence model predict normalize text input token approach take less time learn perform well unlike report google five days gpu cluster achieve accuracy nine thousand, seven hundred and sixty-two impressive give resources use approach use best worlds gradient boost state art classification task sequence sequence learn state art machine translation present experiment report result various parameter settings
recent advance conversational systems change search paradigm traditionally user pose query search engine return answer base index possibly leverage external knowledge base condition response earlier interactions search session natural conversation additional source information take account utterances produce earlier conversation also refer conversational ir system keep track information convey user conversation even implicit argue process build representation conversation frame machine read task automate system present number statements answer question question answer solely refer statements provide without consult external knowledge time right information retrieval community embrace task stand alone task integrate broader conversational search set paper focus machine read stand alone task present attentive memory network amn end end trainable machine read algorithm key contribution efficiency achieve hierarchical input encoder iterate input speed important requirement set conversational search gap conversational turn detrimental effect naturalness twenty datasets commonly use evaluate machine read algorithms show amn achieve performance comparable state art model use considerably fewer computations
open domain social dialogue one long stand goals artificial intelligence year amazon alexa prize challenge announce first time real customers get rate systems develop lead universities worldwide aim challenge converse coherently engagingly humans popular topics twenty minutes describe alexa prize system call alana consist ensemble bots combine rule base machine learn systems use contextual rank mechanism choose system response ranker train real user feedback receive competition address problem train noisy sparse feedback obtain competition
discuss control output deep learn model text corpora create contemporary poetic work assess whether control successful immediate sense create stylo metric distinctiveness specific context piece character think ahead two thousand and sixteen seventeen potential applications broad
variational encoder decoder ved encode source information set random variables use neural network turn decode target data use another neural network natural language process sequence sequence seq2seq model typically serve encoder decoder network combine traditional deterministic attention mechanism variational latent space may bypass attention model thus become ineffective paper propose variational attention mechanism ved attention vector also model gaussian distribute random variables result two experiment show without loss quality propose method alleviate bypass phenomenon increase diversity generate sentence
slang ubiquitous internet emergence new social contexts like micro blog question answer forums social network enable slang non standard expressions abound web despite slang traditionally view form non standard language form language focus linguistic analysis largely neglect work use urbandictionary conduct first large scale linguistic analysis slang social aspects internet yield insights variety language increasingly use world online begin computationally analyze phonological morphological syntactic properties slang study linguistic pattern four specific categories slang namely alphabetisms blend clippings reduplicatives analysis reveal slang demonstrate extra grammatical rule phonological morphological formation markedly distinguish standard form shed insight generative pattern next analyze social aspects slang study subject restriction stereotype slang usage analyze tens thousands slang word reveal majority slang internet belong two major categories sex drug also note slang usage immune prevalent social bias prejudice also reflect bias stereotype intensely standard variety
encoder decoder model widely use natural language generation task however model sometimes suffer repeat redundant generation miss important phrase include irrelevant entities toward solve problems propose novel source side token prediction module method jointly estimate probability distributions source target vocabularies capture correspondence source target tokens experiment show propose model outperform current state art method headline generation task additionally show method ability learn reasonable token wise correspondence without know true alignments
distributional semantics model derive word space linguistic items context mean obtain define distance measure vectors correspond lexical entities vectors present several problems paper provide guideline post process improvements baseline vectors focus refine similarity aspect address imperfections model apply hubness reduction method implement relational knowledge model provide new rank similarity definition give maximum weight top one component value feature rank similar one use information retrieval enrichments outperform current literature result joint esl toef set comparison since single word embed basic element semantic task one expect significant improvement result task moreover improve method text process translate continuous distribute representation biological sequence deep proteomics genomics
investigate whether infant direct speech ids could facilitate word form learn compare adult direct speech ads study examine distribution word form two level acoustic phonological use large database spontaneous speech japanese acoustic level show document phonemes realizations word variable less discriminable ids ads phonological level find effect opposite direction ids lexicon contain distinctive word onomatopoeias ads counterpart combine acoustic phonological metrics together global discriminability score reveal bigger separation lexical categories phonological space compensate opposite effect observe acoustic level result ids word form still globally less discriminable ads word form even though effect numerically small discuss implication find view functional role ids improve language learnability
present approach combine distributional semantic representations induce text corpora manually construct lexical semantic network kinds semantic resources available high lexical coverage align resource combine domain specificity availability contextual information distributional model conciseness high quality manually craft lexical network start distributional representation induce sense vocabulary term accompany rich context information give relate lexical items automatically disambiguate representations obtain full fledge proto conceptualization ie type graph induce word sense final step proto conceptualization align lexical ontology result hybrid align resource moreover unmapped induce sense associate semantic type order connect core resource manual evaluations grind truth judgments different stag method well extrinsic evaluation knowledge base word sense disambiguation benchmark indicate high quality new hybrid resource additionally show benefit enrich top lexical knowledge resources bottom distributional information text address high end knowledge acquisition task clean hypernym graph learn taxonomies scratch
character commonly regard minimal process unit natural language process nlp many non latin languages hieroglyphic write systems involve big alphabet thousands millions character character compose even smaller part often ignore previous work paper propose novel architecture employ two stack long short term memory network lstms learn sub character level representation capture deeper level semantic mean build concrete study substantiate efficiency neural architecture take chinese word segmentation research case example among languages chinese typical case every character contain several components call radicals network employ share radical level embed solve simplify traditional chinese word segmentation without extra traditional simplify chinese conversion highly end end way word segmentation significantly simplify compare previous work radical level embeddings also capture deeper semantic mean character level improve system performance learn tie radical character embeddings together parameter count reduce whereas semantic knowledge share transfer two level boost performance largely three four bakeoff two thousand and five datasets method surpass state art result four result reproducible source cod corpora available github
large amount data available social media forums websites motivate research several areas natural language process sentiment analysis popularity area due subjective semantic characteristics motivate research novel methods approach classification hence high demand datasets different domains different languages paper introduce tweetsentbr sentiment corpora brazilian portuguese manually annotate fifteen thousand sentence tv show domain sentence label three class positive neutral negative seven annotators follow literature guidelines ensure reliability annotation also run baseline experiment polarity classification use three machine learn methods reach eight thousand and ninety-nine f measure eight thousand, two hundred and six accuracy binary classification five thousand, nine hundred and eighty-five f measure six thousand, four hundred and sixty-two accuracy three point classification
study refer expression generation reg often make use corpora definite descriptions produce human subject control experiment experiment kind essential study reference phenomena many others may however include considerable amount noise human subject may easily lack attention may simply misunderstand task hand result elicit data may include large proportion ambiguous ill form descriptions addition reg corpora usually collect study semantics relate phenomena often case elicit descriptions input contexts need annotate correspond semantic properties many field may require considerable time skilled annotators mean tackle kinds difficulties poor data quality high annotation cost work discuss semi automatic method annotation definite descriptions produce human subject reg data collection experiment method make use simple rule establish associations word mean intend facilitate design experiment produce reg corpora
generative adversarial net gans successfully apply artificial generation image data term text data much do artificial generation natural language single corpus consider multiple text corpora input data two applications gans one creation consistent cross corpus word embeddings give different word embeddings per corpus two generation robust bag word document embeddings corpora demonstrate gin model real world text data set different corpora show embeddings model lead improvements supervise learn problems
email workplace often intentional call action recipients propose annotate email action recipient take argue approach action base annotation scalable theory agnostic traditional speech act base email intent annotation still carry important semantic pragmatic information show action base annotation scheme achieve good inter annotator agreement also show leverage thread message domains exhibit comparable intents conversation domain adaptive rainbow recurrently attentive neural bag word collection datasets consist irc reddit email reparametrized rnns outperform common multitask multidomain approach several speech act relate task also experiment minimally supervise scenario email recipient action classification find reparametrized rnns learn useful representation
math word problems form natural abstraction range quantitative reason problems understand financial news sport result casualties war solve problems require understand several mathematical concepts dimensional analysis subset relationships etc paper develop declarative rule govern translation natural language description concepts math expressions present framework incorporate declarative knowledge word problem solve method learn map arithmetic word problem text math expressions learn select relevant declarative knowledge operation solution expression provide way handle multiple concepts problem time support interpretability answer expression method model map declarative knowledge latent variable thus remove need expensive annotations experimental evaluation suggest domain knowledge base solver outperform systems generalize better realistic case train data expose bias different way test data
many natural language process applications nowadays rely pre train word representations estimate large text corpora news collections wikipedia web crawl paper show train high quality word vector representations use combination know trick however rarely use together main result work new set publicly available pre train model outperform current state art large margin number task
previous approach chinese word segmentation roughly classify character base word base methods former regard task sequence label problem latter directly segment character sequence word however consider segment give sentence intuitive idea predict whether segment gap two consecutive character comparison make previous approach seem complex therefore paper propose gap base framework implement intuitive idea moreover deep convolutional neural network namely resnets densenets exploit experiment result show approach outperform best character base word base methods five benchmarks without post process module eg conditional random field beam search
text normalization essential task process analysis social media dominate informal write aim map informal word intend standard form previously propose text normalization approach typically require manual selection parameters improve performance paper present automatic optimizationbased nearest neighbor match approach text normalization approach motivate observation text normalization essentially match problem nearest neighbor match adaptive similarity function direct procedure similarity function incorporate weight contributions contextual string phonetic similarity nearest neighbor match involve minimum similarity threshold four parameters tune efficiently use grid search evaluate performance approach two benchmark datasets result demonstrate parameter tune small size label datasets produce state art text normalization performances thus approach allow practically easy construction evolve domain specific normalization lexicons
factoid question question require short fact base answer automatic generation aqg factoid question give text contribute educational activities interactive question answer systems search engines applications goal research generate factoid source question answer triplets base specific domain propose four component pipeline obtain input train corpus domain specific document along set declarative sentence domain generate output set factoid question refer source sentence slightly different question answer system person ask question require deeper understand knowledge simple word match contrary exist domain specific aqg systems utilize template base approach question generation propose transform source sentence set question apply series domain independent rule syntactic base approach pipeline evaluate domain cyber security use series experiment component pipeline separately end end system propose approach generate higher percentage acceptable question prior state art aqg system
ability change arbitrary aspects text leave core message intact could strong impact field like market politics enable eg automatic optimization message impact personalize language adapt receiver profile paper take first step towards system present algorithm manipulate sentiment text preserve semantics use disentangle representations validation perform examine trajectories embed space analyze transform sentence semantic preservation expression desire sentiment shift
dialogue state track dst key component task orient dialogue systems dst estimate user goal user turn give interaction state art approach state track rely deep learn methods represent dialogue state distribution possible slot value slot present ontology representation scalable set possible value unbounded eg date time location dynamic eg movies usernames furthermore train model require label data user turn annotate dialogue state make build model new domains challenge paper present scalable multi domain deep learn base approach dst introduce novel framework state track independent slot value set represent dialogue state distribution set value interest candidate set derive dialogue history knowledge restrict candidate set bound size address problem slot scalability furthermore leverage slot independent architecture transfer learn show propose approach facilitate quick adaptation new domains
paper present approach extract order timelines events participants locations time set multilingual cross lingual data source base assumption event relate information recover different document write different languages extend cross document event order task present semeval two thousand and fifteen specify two new task respectively multilingual cross lingual timeline extraction develop three deterministic algorithms timeline extraction base two main ideas first address implicit temporal relations document level since explicit time anchor scarce build wide coverage timeline extraction system second leverage several multilingual resources obtain single inter operable semantic representation events across document across languages result highly competitive system strongly outperform current state art nonetheless analysis result reveal link event mention target entities time anchor remain difficult challenge systems resources scorers freely available facilitate use guarantee reproducibility result
korea university intelligent signal process lab ku ispl develop speaker recognition system sre16 fix train condition data evaluation trials collect outside north america speak tagalog cantonese train data speak english thus main issue sre16 compensate discrepancy different languages development dataset speak cebuano mandarin could prepare evaluation trials preliminary experiment compensate language mismatch condition team develop four different approach extract vectors apply state art techniques backend compensate language mismatch investigate endeavor unique method unsupervised language cluster inter language variability compensation gender language dependent score normalization
work web archive raise number issue cause temporal characteristics depend age content additional knowledge might need find understand older texts especially facts entities subject change severe term information retrieval name change order find entities change name time search engines need aware evolution tackle problem analyze wikipedia term entity evolutions mention article regardless structural elements gather statistics automatically extract minimum excerpt cover name change incorporate list dedicate subject future work excerpt go use discover pattern detect change source work investigate whether wikipedia suitable source extract require knowledge
access web archive raise number issue cause temporal characteristics additional knowledge need find understand older texts especially entities mention texts subject change severe term information retrieval name change order find entities change name time search engines need aware evolution tackle problem analyze wikipedia term entity evolutions mention article present statistical data excerpt cover name change use discover similar text passages extract evolution knowledge future work
evolution name entities affect exploration retrieval task digital libraries information retrieval system aware name change actively support users find former occurrences evolve entities however current structure knowledge base dbpedia freebase provide enough information evolutions even though data available resources like wikipedia emphevolution base prototype demonstrate excerpt describe name evolutions identify websites promise precision descriptions classify mean model train base recent analysis name entity evolutions wikipedia
advancements technology culture lead change language change create gap language know users language store digital archive affect user possibility firstly find content secondly interpret content previous work introduce approach name entity evolution recognitionnever newspaper collections lately increase efforts web preservation lead increase availability web archive cover longer time span however language web dynamic traditional media many basic assumptions newspaper domain hold web data paper discuss limitations exist methodology never approach adapt exist never method work noisy data like web blogosphere particular develop novel filter reduce noise make use semantic web resources obtain information term evaluation show potentials propose approach
real value word representations transform nlp applications popular examples word2vec glove recognize ability capture linguistic regularities paper demonstrate simple yet counter intuitive postprocessing technique eliminate common mean vector top dominate directions word vectors render shelf representations even stronger postprocessing empirically validate variety lexical level intrinsic task word similarity concept categorization word analogy sentence level task semantic textural similarity text classification multiple datasets variety representation methods hyperparameter choices multiple languages case process representations consistently better original ones
show discourse structure define rhetorical structure theory provide exist discourse parser benefit text categorization approach use recursive neural network newly propose attention mechanism compute representation text focus salient content perspective rst task experiment consider variants approach illustrate strengths weaknesses
effectiveness three stop word list arabic information retrieval general stoplist corpus base stoplist combine stoplist investigate study three popular weight scheme examine inverse document frequency weight probabilistic weight statistical language model idea combine statistical approach linguistic approach reach optimal performance compare effect retrieval ldc linguistic data consortium arabic newswire data set use lemur toolkit best match weight scheme use okapi retrieval system best overall performance three weight algorithms use study stoplists improve retrieval effectiveness especially use bm25 weight overall performance general stoplist better two list
domain adaptation crucial many real world applications distribution train data differ distribution test data previous deep learn base approach domain adaptation need train jointly source target domain data therefore unappealing scenarios model need adapt large number domains domain evolve eg spam detection attackers continuously change tactics fill gap propose knowledge adaptation extension knowledge distillation bucilua et al two thousand and six hinton et al two thousand and fifteen domain adaptation scenario show student model achieve state art result unsupervised domain adaptation multiple source standard sentiment analysis benchmark take account domain specific expertise multiple teachers similarities domains learn single teacher use domain similarity gauge trustworthiness inadequate end propose simple metric correlate well teacher accuracy target domain demonstrate incorporate high confidence examples select metric enable student model achieve state art performance single source scenario
study problem semi supervise question answer utilize unlabeled text boost performance question answer model propose novel train framework generative domain adaptive net framework train generative model generate question base unlabeled text combine model generate question human generate question train question answer model develop novel domain adaptation algorithms base reinforcement learn alleviate discrepancy model generate data distribution human generate data distribution experiment show propose framework obtain substantial improvement unlabeled text
domain adaptation important sentiment analysis sentiment indicate word vary domains recently multi domain adaptation become pervasive exist approach train available source domains include dissimilar ones however selection appropriate train data important choice algorithm undertake knowledge first time extensive study domain similarity metrics context sentiment analysis propose novel representations metrics new scope data selection evaluate propose methods two large scale multi domain adaptation settings tweet review demonstrate consistently outperform strong random balance baselines propose selection strategy outperform instance level selection yield best score large review corpus
recent research neural machine translation largely focus two aspects neural network architectures end end learn algorithms problem decode however receive relatively little attention research community paper solely focus problem decode give train neural machine translation model instead try build new decode algorithm specific decode objective propose idea trainable decode algorithm train decode algorithm find translation maximize arbitrary decode objective specifically design actor observe manipulate hide state neural machine translation decoder propose train use variant deterministic policy gradient extensively evaluate propose algorithm use four language pair two decode objectives show indeed train trainable greedy decoder generate better translation term target decode objective minimal computational overhead
connect different text attribute associate entity conflation important business data analytics since could help merge two different table database provide comprehensive profile entity however conflation task challenge two text string describe entity could quite different reason misspell therefore critical develop conflation model able truly understand semantic mean string match semantic level end develop character level deep conflation model encode input text string character level finite dimension feature vectors use compute cosine similarity text string model train end end manner use back propagation stochastic gradient descent maximize likelihood correct association specifically propose two variants deep conflation model base long short term memory lstm recurrent neural network rnn convolutional neural network cnn respectively model perform well real world business analytics dataset significantly outperform baseline bag character boc model
instant message one major channel computer mediate communication however humans know limit understand others emotions via text base communication aim introduce emotion sense technologies instant message develop emotionpush system automatically detect emotions message end users receive facebook messenger provide color cue smartphones accordingly conduct deployment study twenty participants time span two weeks paper reveal five challenge along examples observe study base user feedback chat log include ithe continuum emotions iimulti user conversations iiidifferent dynamics different users ivmisclassification emotions vunconventional content believe discussion benefit future exploration affective compute instant message also would light research conversational emotion sense
work present approach mine user preferences recommendation base review various study work recommendation problem however study beyond one aspect user generate content user rat user feedback state user preferences prob lem one aspect mine lack state user preferences demonstration collaborative filter recommendation try figure preference trend crowd users use trend predict current user preference therefore gap real user preferences trend crowd people additionally user preferences address mine user review since user often comment various aspects products solve problem mainly focus mine product aspects user aspects inside user review directly state user preferences also take account social network analysis cold start item problem cold start user problem collaborative filter algorithm employ work framework general enough apply different recommendation domains theoretically method would achieve significant enhancement
end end learn recurrent neural network rnns attractive solution dialog systems however current techniques data intensive require thousands dialogs learn simple behaviors introduce hybrid code network hcns combine rnn domain specific knowledge encode software system action templates compare exist end end approach hcns considerably reduce amount train data require retain key benefit infer latent representation dialog state addition hcns optimize supervise learn reinforcement learn mixture hcns attain state art performance babi dialog dataset outperform two commercially deploy customer face dialog systems
recently machine learn methods provide broad spectrum original efficient algorithms base deep neural network dnn automatically predict outcome respect sequence input recurrent hide cells allow dnn base model manage long term dependencies recurrent neural network rnn long short term memory lstm nevertheless rnns process single input stream one lstm two bidirectional lstm directions information available nowadays multistreams multimedia document require rnns process information synchronously train paper present original lstm base architecture name parallel lstm plstm carry multiple parallel synchronize input sequence order predict common output propose plstm method could use parallel sequence classification purpose plstm approach evaluate automatic telecast genre sequence classification task compare different state art architectures result show propose plstm method outperform baseline n gram model well state art lstm approach
natural language sentence match fundamental technology variety task previous approach either match sentence single direction apply single granular word word sentence sentence match work propose bilateral multi perspective match bimpm model match aggregation framework give two sentence p q model first encode bilstm encoder next match two encode sentence two directions p rightarrow q p leftarrow q match direction time step one sentence match time step sentence multiple perspectives another bilstm layer utilize aggregate match result fix length match vector finally base match vector decision make fully connect layer evaluate model three task paraphrase identification natural language inference answer sentence selection experimental result standard benchmark datasets show model achieve state art performance task
like upvoting post mobile app easy reply write note much difficult due cognitive load come meaningful response well mechanics enter text present novel textual reply generation model go beyond current auto reply predictive text entry model take account content preferences user idiosyncrasies conversational style even structure social graph specifically develop two type model personalize user interactions content base conversation model make use location together user information social graph base conversation model combine content base conversation model social graph
two thousand and thirteen efi de fouille de textes deft campaign interest two type language analysis task document classification information extraction specialize domain cuisine recipes present systems lia use deft two thousand and thirteen systems show interest result even though complexity propose task
year deft campaign efi fouilles de textes incorporate task aim identify session article previous taln conferences present describe three statistical systems develop lia adoc task fusion systems enable us obtain interest result micro precision score seventy-six measure test corpus
real time monitor responses emerge public health threats rely availability timely surveillance data early stag epidemic ready availability line list detail tabular information laboratory confirm case assist epidemiologists make reliable inferences forecast inferences crucial understand epidemiology specific disease early enough stop control outbreak however construction line list require considerable human supervision therefore difficult generate real time paper motivate guide deep list first tool build automate line list near real time open source report emerge disease outbreaks specifically focus derive epidemiological characteristics emerge disease affect population report illness guide deep list use distribute vector representations ala word2vec discover set indicators line list feature discovery indicators follow use dependency parse base techniques final extraction tabular form evaluate performance guide deep list human annotate line list provide healthmap correspond mers outbreaks saudi arabia demonstrate guide deep list extract line list feature increase accuracy compare baseline method show automatically extract line list feature use make epidemiological inferences infer demographics symptoms hospitalization period affect individuals
vocal joystick vowel corpus washington university use study monophthongs pronounce native english speakers objective study quantitatively measure extent speech recognition methods distinguish similar sound vowels particular phonemes textipa ae textipaa textipa2 analyse seven hundred and forty-eight sound file corpus use subject linear predictive cod lpc compute formants mel frequency cepstral coefficients mfcc algorithm compute cepstral coefficients decision tree classifier use build predictive model learn pattern two first formants measure data set well pattern thirteen cepstral coefficients accuracy seventy achieve use formants mention phonemes mfcc analysis accuracy fifty-two achieve accuracy seventy-one textipa ignore result obtain show study algorithms far mimic ability distinguish subtle differences sound like human hear
medical errors lead cause death us prevention errors paramount promote health care patient safety event report narratives describe potential adverse events patients important identify prevent medical errors present neural network architecture identify type safety events first step understand narratives propose model base soft neural attention model improve effectiveness encode long sequence empirical result two large scale real world datasets patient safety report demonstrate effectiveness method significant improvements exist methods
social media data mine increasingly use analyse political societal issue undertake classification social media users support oppose ongoing independence movements territories independence movements occur territories whose citizens conflict national identities users oppose national identities support oppose sense part independent nation differ officially recognise country describe methodology rely users self report location build large scale datasets three territories catalonia basque country scotland analysis datasets show homophily play important role determine people connect users predominantly choose follow interact others national identity show classifier rely users follow network achieve accurate language independent classification performances range eighty-five ninety-seven three territories
generally believe linguistic item acquire new mean overall frequency use language rise time shape growth curve yet claim support limit number case study paper provide first corpus base quantitative confirmation genericity curve language change moreover uncover another generic pattern latency phase variable duration precede growth frequency use semantically expand word remain low less constant also propose usage base model language change support cognitive considerations predict phase latency fast growth take place drive mechanism stochastic dynamics random walk space frequency use underlie deterministic dynamics highlight role control parameter strength cognitive impetus govern onset change tune system vicinity saddle node bifurcation neighborhood critical point latency phase correspond diffusion time critical region growth fast convergence follow duration two phase compute specific first passage time random walk process lead distributions fit well ones extract dataset argue result specific study corpus apply semantic change general
linguistic relations oral conversations present opinions construct develop restrict time relations bond ideas arguments thoughts feel shape speech finally build knowledge information provide conversation speakers share common interest discuss expect speaker reply include duplicate form word previous speakers however linguistic adaptation observe evolve complex path transfer slightly modify versions common concepts conversation aim benefit end show emergent cooperation induce adaptation cooperation also competition drive adaptation opposite scenario one capture dynamic process track concepts linguistically link uncover salient complex dynamic events verbal communications attempt discover self organize linguistic relations hide conversation explicitly state winners losers examine open access data unite state supreme court understand crucial big data research guide transition state opinion mine decision make model require knowledge guide model pinpoint filter large amount data
scattertext open source tool visualize linguistic variation document categories language independent way tool present scatterplot axis correspond rank frequency term occur category document tie break strategy tool able display thousands visible term represent point find space legibly label hundreds scattertext also lend query base visualization use term similar embeddings differ document categories well visualization compare importance score bag word feature univariate metrics
word evolution refer change mean associations word throughout time byproduct human language evolution study word evolution infer social trend language construct different periods human history however traditional techniques word representation learn adequately capture evolve language structure vocabulary paper develop dynamic statistical model learn time aware word vector representation propose model simultaneously learn time aware embeddings solve result alignment problem model train crawl nytimes dataset additionally develop multiple intuitive evaluation strategies temporal word embeddings qualitative quantitative test indicate method reliably capture evolution time also consistently outperform state art temporal embed approach semantic accuracy alignment quality
speed train process many exist systems use parallel technology online learn algorithms however research mainly focus stochastic gradient descent sgd instead algorithms propose generic online parallel learn framework large margin model also analyze framework popular large margin algorithms include mira structure perceptron framework lock free easy implement exist systems experiment show systems framework gain near linear speed increase run thread loss accuracy
one major drawbacks modularized task completion dialogue systems module train individually present several challenge example downstream modules affect earlier modules performance entire system robust accumulate errors paper present novel end end learn framework task completion dialogue systems tackle issue neural dialogue system directly interact structure database assist users access information accomplish certain task reinforcement learn base dialogue manager offer robust capabilities handle noise cause components dialogue system experiment movie ticket book domain show end end system outperform modularized dialogue system baselines objective subjective evaluation also robust noise demonstrate several systematic experiment different error granularity rat specific language understand module
statistical classifiers become integrate real world applications important consider accuracy also robustness change data distribution paper consider case unobserved confound variable z influence feature mathbfx class variable influence z change train test data find classifier accuracy degrade rapidly approach assume predict value z train time error prediction z feed pearl back door adjustment build model attenuation bias cause measurement error z standard approach control z ineffective response propose method properly control influence z first estimate relationship class variable update predictions z match estimate relationship adjust influence z show build model exceed compete baselines accuracy well robustness range confound relationships
data noise effective technique regularize neural network model noise widely adopt application domains vision speech commonly use noise primitives develop discrete sequence level settings language model paper derive connection input noise neural network language model smooth n gram model use connection draw upon ideas smooth develop effective noise scheme demonstrate performance gain apply propose scheme language model machine translation finally provide empirical analysis validate relationship noise smooth
emojis new way convey nonverbal cue widely adopt computer mediate communications paper first message sender perspective focus people motives use four type emojis positive neutral negative non facial compare willingness level use emoji type seven typical intentions people usually apply nonverbal cue communication result extensive statistical hypothesis test report popularities intentions also uncover subtle differences emoji type term intend use second perspective message recipients study sentiment effect emojis well duplications verbal message different previous study emoji sentiment study sentiments emojis contexts whole experiment result indicate power convey sentiment different four emoji type sentiment effect emojis vary contexts different valences
extract useful entities attribute value illicit domains human traffic challenge problem potential widespread social impact domains employ atypical language model long tail suffer problem concept drift paper propose lightweight feature agnostic information extraction ie paradigm specifically design domains approach use raw unlabeled text initial corpus twelve one hundred and twenty seed annotations per domain specific attribute learn robust ie model unobserved page websites empirically demonstrate approach outperform feature centric conditional random field baselines eighteen f measure five annotate set real world human traffic datasets low supervision high supervision settings also show approach demonstrably robust concept drift efficiently bootstrapped even serial compute environment
loyalty essential component multi community engagement users choice engage variety different communities often become loyal one focus community expense others however unclear loyalty manifest user behavior whether loyalty encourage certain community characteristics paper operationalize loyalty user community relation users loyal community consistently prefer others loyal communities retain loyal users time explore relation use large dataset discussion communities reddit reveal loyalty manifest remarkably consistent behaviors across wide spectrum communities loyal users employ language signal collective identity engage esoteric less popular content indicate may play curational role surface new material loyal communities denser user user interaction network lower rat triadic closure suggest community level loyalty associate cohesive interactions less fragmentation subgroups exploit general pattern predict future rat loyalty result show user propensity become loyal apparent first interactions community suggest users intrinsically loyal begin
autonomous agents must often detect affordances set behaviors enable situation affordance detection particularly helpful domains large action space allow agent prune search space avoid futile behaviors paper present method affordance extraction via word embeddings train wikipedia corpus result word vectors treat common knowledge database query use linear algebra apply method reinforcement learn agent text environment show affordance base action selection improve performance time method increase computational complexity learn step significantly reduce total number step need addition agent action selections begin resemble human would choose
regularization occur output learner produce less variable linguistic data observe artificial language learn experiment show exist least two independent source regularization bias cognition domain general source base cognitive load domain specific source trigger linguistic stimuli factor modulate frequency information encode produce production side modulations result regularization ie learners eliminate variation observe input formalize definition regularization reduction entropy find entropy measure better identify regularization behavior frequency base analyse use experimental data model cultural transmission generate predictions amount regularity would develop experimental condition artificial language transmit several generations learners find effect cognitive constraints become complex put context cultural evolution although learn bias certainly carry information course language evolution expect one one correspondence micro level process regularize linguistic datasets macro level evolution linguistic regularity
text similarity detection aim measure degree similarity pair texts corpora available text similarity detection design evaluate algorithms assess paraphrase level among document paper present textual german corpus similarity detection purpose corpus automatically assess similarity pair texts evaluate different similarity measure whole document individual sentence therefore calculate several simple measure corpus base library similarity function
quora one popular community qanda sit recent time however many question post qanda site often get answer paper quantify various linguistic activities discriminate answer question unanswered one central find way users use language write question text effective mean characterize answerability characterization help us predict early question remain unanswered specific time period eventually answer achieve accuracy seven thousand, six hundred and twenty-six one month six thousand, eight hundred and thirty-three three months notably feature represent language use pattern users discriminative alone account accuracy seven thousand, four hundred and eighteen also compare method similar work dror et al yang et al achieve maximum improvement thirty-nine term accuracy
learn sophisticate feature interactions behind user behaviors critical maximize ctr recommender systems despite great progress exist methods seem strong bias towards low high order interactions require expertise feature engineer paper show possible derive end end learn model emphasize low high order feature interactions propose model deepfm combine power factorization machine recommendation deep learn feature learn new neural network architecture compare latest wide deep model google deepfm share input wide deep part need feature engineer besides raw feature comprehensive experiment conduct demonstrate effectiveness efficiency deepfm exist model ctr prediction benchmark data commercial data
paper present data visualization method together potential usefulness digital humanities philosophy language compile multilingual parallel corpus different versions wittgenstein tractatus logico philosophicus include original german translations english spanish french russian use corpus compute similarity measure proposition render visual network relations different languages
formal languages theory useful study natural language particular interest study adequacy grammatical formalisms express syntactic phenomena present natural language first help draw hypothesis nature complexity speaker hearer linguistic competence fundamental question linguistics cognitive sciences moreover engineer point view allow knowledge practical limitations applications base formalisms article introduce adequacy problem grammatical formalisms natural language also introduce formal language theory concepts require discussion review formalisms propose history arguments give support reject adequacy la teor ia de lenguajes formales es util para el estudio de los lenguajes naturales en particular resulta de inter es estudiar la adecuaci de los formalismos gramaticales para expresar los fen omenos sint acticos present en el lenguaje natural primero ayuda trazar hip otesis acerca de la naturaleza complejidad de las competencias lingyou isticas de los hablantes oyentes del lenguaje un interrogante fundamental de la lingyou istica otras ciencias cognitivas adem desde el punto de vista de la ingenier ia permite conocer limitaciones pr acticas de las aplicaciones basadas en dichos formalismos en este art iculo hago una introducci al problema de la adecuaci de los formalismos gramaticales para el lenguaje natural introduciendo tambi en algunos conceptos de la teor ia de lenguajes formales necesarios para esta discusi luego hago un repaso de los formalismos que han sido propuestos lo largo de la historia de los argumentos que se han dado para sostener refutar su adecuaci
paper describe application reinforcement learn mention detection task define novel action base formulation mention detection task model flexibly revise past label decisions group together tokens assign partial mention label devise method create mention level episodes train model reward correctly label complete mention irrespective inner structure create model yield result par competitive supervise counterpart flexible term achieve target behavior reward model generate internal mention structure especially longer mention
arabic language write face resurgence international normative solutions challenge local network base operate principles even multilingual digital cod solutions especially propose unicode solve many difficulties arabic write linguistic aspect still search adapt solutions terminology one sectors arabic language require deep modernization classical productivity model normative approach particular iso tc37 propose one solutions would allow combine international standards better integrate knowledge society construction la langue et lecriture arabe sont aujourdhui confrontees une recrudescence de solutions normatives internationales qui remettent en la plupart de leurs principes de fonctionnement en site ou sur les reseaux meme si les solutions du codage numerique multilingue notamment celles propose par unicode ont resolu beaucoup de difficultes de lecriture arabe le volet linguistique est encore en quete de solutions plus adaptees la terminologie est lun des secteurs dans lequel la langue arabe necessite une modernisation profonde de ses model classiques de production la voie normative notamment celle du tc37 de iso est proposee comme une des solutions qui lui permettrait de se mettre en synergie avec les referentiels internationaux pour mieux integrer la societe du savoir en voie de construction
present computational evaluation three hypotheses source deficit sentence comprehension aphasia slow process intermittent deficiency resource reduction act r base lewis vasishth two thousand and five model use implement three proposals slow process implement slow default production rule fire time intermittent deficiency increase random noise activation chunk memory resource reduction reduce goal activation data consider subject vs object rela tives whose matrix clause contain either np reflexive present self pace listen modality fifty-six individuals aphasia iwa forty-six match control participants hear sentence carry picture verification task decide interpretation sentence response accuracies use identify best parameters participant correspond three hypotheses mention show control tightly cluster less variable parameter value iwa specifically compare control among iwa individuals low goal activations high noise slow default action time suggest individual patients show differential amount deficit along three dimension slow process intermittent deficient resource reduction ii overall evidence three source deficit play role iii iwa variable range parameter value control sum study contribute proof concept quantitative implementation evidence three account comprehension deficits aphasia
field speech recognition midst paradigm shift end end neural network challenge dominance hide markov model core technology use attention mechanism recurrent encoder decoder architecture solve dynamic time alignment problem allow joint end end train acoustic language model components paper extend end end framework encompass microphone array signal process noise suppression speech enhancement within acoustic encode network allow beamforming components optimize jointly within recognition architecture improve end end speech recognition objective experiment noisy speech benchmarks chime four ami show multichannel end end system outperform attention base baseline input conventional adaptive beamformer
semantic role label srl task identify predicate argument structure sentence typically regard important step standard nlp pipeline semantic representations closely relate syntactic ones exploit syntactic information model propose version graph convolutional network gcns recent class neural network operate graph suit model syntactic dependency graph gcns syntactic dependency tree use sentence encoders produce latent feature representations word sentence observe gcn layer complementary lstm ones stack gcn lstm layer obtain substantial improvement already state art lstm srl model result best report score standard benchmark conll two thousand and nine chinese english
collaborative filter cf aim build model users past behaviors similar decisions make users use model recommend items users despite success previous collaborative filter approach base assumption sufficient rat score available build high quality recommendation model real world applications however often difficult collect sufficient rat score especially new items introduce system make recommendation task challenge find often short texts describe feature items base approximate similarity items make recommendation together rat score paper borrow idea vector representation word capture information short texts embed matrix factorization framework empirically show approach effective compare state art approach
capture statistical pattern large corpora machine learn enable significant advance natural language process include machine translation question answer sentiment analysis however agents intelligently interact humans simply capture statistical pattern insufficient paper investigate ground compositional language emerge mean achieve goals multi agent populations towards end propose multi agent learn environment learn methods bring emergence basic compositional language language represent stream abstract discrete symbols utter agents time nonetheless coherent structure possess define vocabulary syntax also observe emergence non verbal communication point guide language communication unavailable
paper describe approach triple score task wsdm cup two thousand and seventeen task require participants assign relevance score pair entities type knowledge base order enhance rank result entity retrieval task propose approach wherein output multiple neural network classifiers combine use supervise machine learn model experimental result show propose method achieve best performance one three measure ie kendall tau perform competitively two measure ie accuracy average score difference
paper show performance tweet cluster improve leverage character base neural network propose approach overcome limitations relate vocabulary explosion word base model allow seamless process multilingual content evaluation result code available line https githubcom vendi12 tweet2vecclustering
paper present inscript corpus narrative texts instantiate script structure inscript corpus one thousand stories center around ten different scenarios verbs noun phrase annotate event participant type respectively additionally text annotate coreference information corpus show rich lexical variation serve unique resource study role script knowledge natural language process
paper present study employ rank svm convolutional neural network two missions legal information retrieval question answer competition legal information extraction entailment first task propose model use triple feature lsi manhattan jaccard base paragraph level instead article level previous study fact single paragraph article correspond particular paragraph huge multiple paragraph article legal question answer task additional statistical feature information retrieval task integrate convolutional neural network contribute higher accuracy
technical document contain fair amount unnatural language table formulas pseudo cod etc unnatural language important factor confuse exist nlp tool paper present effective method distinguish unnatural language natural language evaluate impact unnatural language detection nlp task document cluster view problem information extraction task build multiclass classification model identify unnatural language components four categories first create new annotate corpus collect slide paper various format ppt pdf html unnatural language components annotate four categories explore feature available plain text build statistical model handle format long convert plain text experiment show remove unnatural language components give absolute improvement document cluster fifteen corpus tool publicly available
paper propose use set simple uniform architecture lstm base model recover different kinds temporal relations text use shortest dependency path entities input architecture use extract intra sentence cross sentence document creation time relations double check technique reverse entity pair classification boost recall positive case reduce misclassifications opposite class efficient prune algorithm resolve conflict globally evaluate qa tempeval semeval2015 task five propose technique outperform state art methods large margin
recent paper show neural network obtain state art performance several different sequence tag task one appeal property systems generality excellent performance achieve unify architecture without task specific feature engineer however unclear systems use task without large amount train data paper explore problem transfer learn neural sequence taggers source task plentiful annotations eg pos tag penn treebank use improve performance target task fewer available annotations eg pos tag microblogs examine effect transfer learn deep hierarchical recurrent network across domains applications languages show significant improvement often obtain improvements lead improvements current state art several well study task
take image question input method output text base answer query question give image call visual question answer vqa two main modules algorithm give natural language question image first module take question input output basic question main give question second module take main question image basic question input output text base answer main question formulate basic question generation problem lasso optimization problem also propose criterion exploit basic question help answer main question method evaluate challenge vqa dataset yield state art accuracy six thousand and thirty-four open end task
number document available internet move day reason process amount information effectively expressibly become major concern company scientists methods represent textual document topic representation widely use information retrieval ir process big data wikipedia article one main difficulty use topic model huge data collection relate material resources cpu time memory require model estimate deal issue propose build topic space summarize document paper present study topic space representation context big data topic space representation behavior analyze different languages experiment show topic space estimate text summaries relevant estimate complete document real advantage approach process time gain show process time drastically reduce use summarize document sixty general study finally point differences thematic representations document depend target languages english latin languages
translate information text image fundamental problem artificial intelligence connect natural language process computer vision past years performance image caption generation see significant improvement adoption recurrent neural network rnn meanwhile text image generation begin generate plausible image use datasets specific categories like bird flower even see image generation multi category datasets microsoft common object context mscoco use generative adversarial network gans synthesize object complex shape however still challenge example animals humans many degrees freedom mean take many complex shape propose new train method call image text image i2t2i integrate text image image text image caption synthesis improve performance text image synthesis demonstrate capability method understand sentence descriptions i2t2i generate better multi categories image use mscoco state art also demonstrate i2t2i achieve transfer learn use pre train image caption module generate human image mpii human pose
propose supervise algorithm generate type embeddings semantic vector space give set entity embeddings algorithm agnostic derivation underlie entity embeddings require manual feature engineer generalize well hundreds type achieve near linear scale big graph contain many millions triple instance virtue incremental execution demonstrate utility embeddings type recommendation task outperform non parametric feature agnostic baseline achieve 15x speedup near constant memory usage full partition dbpedia use state art visualization illustrate agreement extensionally derive dbpedia type embeddings manually curated domain ontology finally use embeddings probabilistically cluster four million dbpedia instance four hundred and fifteen type dbpedia ontology
despite remarkable progress recently make distant speech recognition state art technology still suffer lack robustness especially adverse acoustic condition characterize non stationary noise reverberation meet prominent limitation current systems lie lack match communication various technologies involve distant speech recognition process speech enhancement speech recognition modules instance often train independently moreover speech enhancement normally help speech recognizer output latter commonly use turn improve speech enhancement address concern propose novel architecture base network deep neural network components jointly train better cooperate thank full communication scheme experiment conduct use different datasets task acoustic condition reveal propose framework overtake competitive solutions include recent joint train approach
word embeddings powerful approach unsupervised analysis language recently rudolph et al two thousand and sixteen develop exponential family embeddings cast word embeddings probabilistic framework develop dynamic embeddings build exponential family embeddings capture mean word change time use dynamic embeddings analyze three large collections historical texts yous senate speeches one thousand, eight hundred and fifty-eight two thousand and nine history computer science acm abstract one thousand, nine hundred and fifty-one two thousand and fourteen machine learn paper arxiv two thousand and seven two thousand and fifteen find dynamic embeddings provide better fit classical embeddings capture interest pattern language change
propose series recurrent contextual neural network model multiple choice visual question answer visual7w dataset motivate divergent trend model complexities literature explore balance model expressiveness simplicity study incrementally complex architectures start lstm encode input question answer build context generation lstm encode neural image question representations attention image evaluate diversity predictive power model ensemble thereof model evaluate simple baseline inspire current state art consist involve simple concatenation bag word cnn representations text image respectively generally observe mark variation image reason performance model obvious overall performance well evidence dataset bias standalone model achieve accuracies six hundred and forty-six ensemble model achieve best accuracy six thousand, six hundred and sixty-seven within five current state art visual7w
unsupervised segmentation cluster unlabelled speech core problems zero resource speech process approach lie methodological extremes use probabilistic bayesian model convergence guarantee others opt efficient heuristic techniques despite competitive performance previous work full bayesian approach difficult scale large speech corpora introduce approximation recent bayesian model still clear objective function improve efficiency use hard cluster segmentation rather full bayesian inference like bayesian counterpart embed segmental k mean model es kmeans represent arbitrary length word segment fix dimensional acoustic word embeddings first compare es kmeans previous approach common english xitsonga data set five twenty-five hours speech es kmeans outperform lead heuristic method word segmentation give similar score bayesian model five time faster fewer hyperparameters however cluster less pure model show es kmeans scale larger corpora apply five languages zero resource speech challenge two thousand and seventeen forty-five hours perform competitively compare challenge baseline
language acquisition infants benefit visual cue grind speak language robots similarly access audio visual sensors recent work show image speak caption map meaningful common space allow image retrieve use speech vice versa set image pair untranscribed speak caption consider whether computer vision systems use obtain textual label speech concretely use image word multi label visual classifier tag image soft textual label train neural network map speech soft target show result speech system able predict word occur utterance act speak bag word classifier without see parallel speech text find model often confuse semantically relate word eg man person make even effective semantic keyword spotter
categorical compositional approach mean successfully apply natural language process outperform model mainstream empirical language process task show approach generalize conceptual space model cognition order first introduce category convex relations new set categorical compositional semantics emphasize convex structure important conceptual space applications show construct conceptual space various type nouns adjectives verbs finally show mean examples concepts systematically combine establish mean composite phrase mean constituent part provide mathematical underpinnings new compositional approach cognition
improve distant speech recognition crucial step towards flexible human machine interfaces current technology however still exhibit lack robustness especially adverse acoustic condition meet despite significant progress make last years speech enhancement speech recognition one potential limitation state art technology lie compose modules well match train jointly address concern promise approach consist concatenate speech enhancement speech recognition deep neural network jointly update parameters within single bigger network unfortunately joint train difficult output distribution speech enhancement system may change substantially optimization procedure speech recognition module would deal input distribution non stationary unnormalized mitigate issue propose joint train approach base fully batch normalize architecture experiment conduct use different datasets task acoustic condition reveal propose framework significantly overtake competitive solutions especially challenge environments
complex human brain enable us communicate natural language gather good understand principles underlie language acquisition process knowledge socio cultural condition insights activity pattern brain however yet able understand behavioural mechanistic characteristics natural language mechanisms brain allow acquire process language bridge insights behavioural psychology neuroscience goal paper contribute computational understand appropriate characteristics favour language acquisition accordingly provide concepts refinements cognitive model regard principles mechanisms brain propose neurocognitively plausible model embody language acquisition real world interaction humanoid robot environment particular architecture consist continuous time recurrent neural network part different leakage characteristics thus operate multiple timescales every modality association higher level nod modalities cell assemblies model capable learn language production ground temporal dynamic somatosensation vision feature hierarchical concept abstraction concept decomposition multi modal integration self organisation latent representations
important yet largely unstudied problem student data analysis detect misconceptions students responses open response question misconception detection enable instructors deliver target feedback misconceptions exhibit many students class thus improve quality instruction paper propose new natural language process base framework detect common misconceptions among students textual responses short answer question propose probabilistic model students textual responses involve misconceptions experimentally validate real world student response dataset experimental result show propose framework excel classify whether response exhibit one misconceptions importantly also automatically detect common misconceptions exhibit across responses multiple students multiple question property especially important large scale since instructors longer need manually specify possible misconceptions students might exhibit
emotional arousal increase activation performance may also lead burnout software development present first version software engineer arousal lexicon sea specifically design address problem emotional arousal software developer ecosystem sea build use bootstrapping approach combine word embed model train issue track data manual score items lexicon show lexicon able differentiate issue priorities source emotional activation act proxy arousal best performance obtain combine sea four hundred and twenty-eight word previously create general purpose lexicon warriner et al thirteen thousand, nine hundred and fifteen word achieve cohen effect size five
problem fake news gain lot attention claim significant impact two thousand and sixteen us presidential elections fake news new problem spread social network well study often underlie assumption fake news discussion write look like real news fool reader check reliability source arguments content unique study three data set feature capture style language article show assumption true fake news case similar satire real news lead us conclude persuasion fake news achieve heuristics rather strength arguments show overall title structure use proper nouns title significant differentiate fake real lead us conclude fake news target audiences likely read beyond title aim create mental associations entities claim
use allure headline clickbait tempt readers become grow practice nowadays sake existence highly competitive media industry line media include mainstream ones start follow practice although wide spread practice clickbait make reader reliability media vulnerable large scale analysis reveal fact still absent paper analyze one hundred and sixty-seven million facebook post create one hundred and fifty-three media organizations understand extent clickbait practice impact user engagement use develop clickbait detection model model use distribute sub word embeddings learn large corpus accuracy model nine hundred and eighty-three power model study distribution topics clickbait non clickbait content
describe prototype dialogue response generation model customer service domain amazon model train weakly supervise fashion measure similarity customer question agent answer use dual encoder network siamese like neural network architecture answer templates extract embeddings derive past agent answer without turn turn annotations responses customer inquiries generate select best template final set templates show close domain like customer service select templates cover seventy past customer inquiries furthermore relevance model select templates significantly higher templates select standard tf idf baseline
humor historically study psychological cognitive linguistic standpoint study computational perspective area yet explore computational linguistics exist previous work characterization humor allow automatic recognition generation far specify work build crowdsourced corpus label tweet annotate accord humor value let annotators subjectively decide humorous humor classifier spanish tweet assemble base supervise learn reach precision eighty-four recall sixty-nine
package cleannlp provide set fast tool convert textual corpus set normalize table underlie natural language process pipeline utilize stanford corenlp library expose number annotation task text write english french german spanish annotators include tokenization part speech tag name entity recognition entity link sentiment analysis dependency parse coreference resolution information extraction
tackle task agent learn navigate 2d maze like environment call xworld session agent perceive sequence raw pixel frame natural language command issue teacher set reward agent learn teacher language scratch ground compositional manner train able correctly execute zero shoot command one combination word command never appear two command contain new object concepts learn another task never learn navigation deep framework agent train end end learn simultaneously visual representations environment syntax semantics language action module output action zero shoot learn capability framework result compositionality modularity parameter tie visualize intermediate output framework demonstrate agent truly understand solve problem believe result provide preliminary insights train agent similar abilities 3d environment
clinical nlp immense potential contribute clinical practice revolutionize advent large scale process clinical record however potential remain largely untapped due slow progress primarily cause strict data access policies researchers paper discuss concern privacy measure entail also suggest source less sensitive data finally draw attention bias compromise validity empirical research lead socially harmful applications
test neutral model evolution english word frequency vocabulary population scale record annual word frequencies three centuries english language book data test static dynamic predictions two neutral model include relation corpus size vocabulary size frequency distributions turnover within frequency distributions although commonly use neutral model fail replicate emergent properties find modify two stage neutral model replicate static dynamic properties corpus data two stage model mean represent relatively small corpus population english book analogous canon sample exponentially increase corpus book wider population author broadly mode smaller neutral model within larger neutral model could represent broadly situations mass attention focus small subset cultural variants
sentence simplification aim make sentence easier read understand recent approach draw insights machine translation learn simplification rewrite monolingual corpora complex simple sentence address simplification problem encoder decoder model couple deep reinforcement learn framework model call sc dress shorthand bf deep bf reinforcement bf sentence bf simplification explore space possible simplifications learn optimize reward function encourage output simple fluent preserve mean input experiment three datasets demonstrate model outperform competitive simplification systems
recent neural encoder decoder model show great promise model open domain conversations often generate dull generic responses unlike past work focus diversify output decoder word level alleviate problem present novel framework base conditional variational autoencoders capture discourse level diversity encoder model use latent variables learn distribution potential conversational intents generate diverse responses use greedy decoders develop novel variant integrate linguistic prior knowledge better performance finally train procedure improve introduce bag word loss propose model validate generate significantly diverse responses baseline approach exhibit competence discourse level decision make
study automatic question generation sentence text passages read comprehension introduce attention base sequence learn model task investigate effect encode sentence vs paragraph level information contrast previous work model rely hand craft rule sophisticate nlp pipeline instead trainable end end via sequence sequence learn automatic evaluation result show system significantly outperform state art rule base system human evaluations question generate system also rat natural ie grammaticality fluency difficult answer term syntactic lexical divergence original text reason need answer
work present unsupervised approach improve wordnet build upon recent advance document sense representation via distributional semantics apply methods construct wordnets french russian languages lack good manual constructions1 evaluate two new six hundred word test set word synset match find improve greatly upon synset recall outperform best automate wordnets f score methods require linguistic resources thus applicable wordnet construction low resources languages may apply sense cluster wordnet improvements
paper introduce speech base visual question answer vqa task generate answer give image speak question two methods study end end deep neural network directly use audio waveforms input versus pipelined approach perform asr automatic speech recognition question follow text base visual question answer furthermore investigate robustness methods inject various level noise speak question find methods tolerate noise similar level
vast amount data increase computational capacity allow analysis texts several perspectives include representation texts complex network nod network represent word edge represent relationship usually word co occurrence even though network representations apply study task approach usually combine traditional model rely upon statistical paradigms network model able grasp textual pattern devise hybrid classifier call label subgraphs combine frequency common word small structure find topology network know motifs approach illustrate two contexts authorship attribution translationese identification former set novels write different author analyze identify translationese texts canadian hansard european parliament classify original translate instance result suggest label subgraphs able represent texts explore task analysis text complexity language proficiency machine translation
paper make simple observation question image often contain premise object relationships imply question reason premise help visual question answer vqa model respond intelligently irrelevant previously unseen question present question irrelevant image state art vqa model still answer purely base learn language bias result non sensical even mislead answer note visual question irrelevant image least one premise false ie depict image leverage observation construct dataset question relevance prediction explanation qrpe search false premise train novel question relevance detection model show model reason premise consistently outperform model also find force standard vqa model reason premise train lead improvements task require compositional reason
automatic fake news detection challenge problem deception detection tremendous real world political social impact however statistical approach combat fake news dramatically limit lack label benchmark datasets paper present liar new publicly available dataset fake news detection collect decade long 128k manually label short statements various contexts politifactcom provide detail analysis report link source document case dataset use fact check research well notably new dataset order magnitude larger previously largest public fake news datasets similar type empirically investigate automatic fake news detection base surface level linguistic pattern design novel hybrid convolutional neural network integrate meta data text show hybrid approach improve text deep learn model
recent years automatic generation image descriptions caption image caption attract great deal attention paper particularly consider generate japanese caption image since available caption datasets construct english language datasets japanese tackle problem construct large scale japanese image caption dataset base image ms coco call stair caption stair caption consist eight hundred and twenty thousand, three hundred and ten japanese caption one hundred and sixty-four thousand and sixty-two image experiment show neural network train use stair caption generate natural better japanese caption compare generate use english japanese machine translation generate english caption
deep neural network dnns provably enhance state art neural machine translation nmt capability model complex function capture complex linguistic structure however nmt systems deep architecture encoder decoder rnns often suffer severe gradient diffusion due non linear recurrent activations often make optimization much difficult address problem propose novel linear associative units lau reduce gradient propagation length inside recurrent unit different conventional approach lstm unit gru laus utilize linear associative connections input output recurrent unit allow unimpeded information flow space time direction model quite simple surprisingly effective empirical study chinese english translation show model proper configuration improve one hundred and seventeen bleu upon groundhog best report result set wmt14 english german task larger wmt14 english french task model achieve comparable result state art
number visual question answer approach propose recently aim understand visual scenes answer natural language question image question answer draw significant attention video question answer largely unexplored video qa different image qa since information events scatter among multiple frame order better utilize temporal structure videos phrasal structure answer propose two mechanisms watch read mechanisms combine forgettable watcher model propose tgif qa dataset video question answer help automatic question generation finally evaluate model dataset experimental result show effectiveness propose model
paper describe amobee sentiment analysis system adapt compete semeval two thousand and seventeen task four system consist two part supervise train rnn model base twitter sentiment treebank use feedforward nn naive bay logistic regression classifiers produce predictions different sub task algorithm reach 3rd place five label classification task sub task c
paper propose neural network model novel sequential attention layer extend soft attention assign weight word input sequence way take account well word match query well surround word match evaluate approach task read comprehension cnn datasets show dramatically improve strong baseline stanford reader competitive state art
chest x ray one commonly accessible radiological examinations screen diagnosis many lung diseases tremendous number x ray image study accompany radiological report accumulate store many modern hospitals picture archive communication systems pacs side still open question type hospital size knowledge database contain invaluable image informatics ie loosely label use facilitate data hungry deep learn paradigms build truly large scale high precision computer aid diagnosis cad systems paper present new chest x ray database namely chestx ray8 comprise one hundred and eight thousand, nine hundred and forty-eight frontal view x ray image thirty-two thousand, seven hundred and seventeen unique patients text mine eight disease image label image multi label associate radiological report use natural language process importantly demonstrate commonly occur thoracic diseases detect even spatially locate via unify weakly supervise multi label image classification disease localization framework validate use propose dataset although initial quantitative result promise report deep convolutional neural network base read chest x ray ie recognize locate common disease pattern train image level label remain strenuous task fully automate high precision cad systems data download link https nihccappboxcom v chestxray nihcc
describe neural network model jointly learn distribute representations texts knowledge base kb entities give text kb train propose model predict entities relevant text model design generic ability address various nlp task ease train model use large corpus texts entity annotations extract wikipedia evaluate model three important nlp task ie sentence textual similarity entity link factoid question answer involve unsupervised supervise settings result achieve state art result three task code train model publicly available academic research
human traffic global epidemic affect millions people across planet sex traffic dominant form human traffic see significant rise mostly due abundance escort websites human traffickers openly advertise among escort advertisements paper take major step automatic detection advertisements suspect pertain human traffic present novel dataset call traffic 10k ten thousand advertisements annotate task dataset contain two source information per advertisement text image accurate detection traffic advertisements design train deep multimodal model call human traffic deep network htdn
word phrase table key input machine translations costly produce new unsupervised learn methods represent word phrase high dimensional vector space monolingual embeddings show encode syntactic semantic relationships language elements information capture embeddings exploit bilingual translation learn transformation matrix allow match relative position across two monolingual vector space method aim identify high quality candidates word phrase translation cost effectively unlabeled data paper expand scope previous attempt bilingual translation four languages english german spanish french show process source data train neural network learn high dimensional embeddings individual languages expand framework test quality beyond english language furthermore show learn bilingual transformation matrices obtain candidates word phrase translation assess quality
today artificial assistants typically prompt perform task direct imperative command emphset timer emphpick box however progress toward natural exchange humans assistants important understand way non imperative utterances indirectly elicit action addressee paper investigate command type set ground collaborative game focus less understand family utterances elicit agent action locatives like emphthe chair room demonstrate utterances indirectly command specific game state contexts work show model domain specific ground effectively realize pragmatic reason necessary robust natural language interaction
propose novel system transform recipe select regional style eg japanese mediterranean italian system two characteristics first system identify degree regional cuisine style mixture select recipe visualize regional cuisine style mixtures use barycentric newton diagram second system suggest ingredient substitutions extend word2vec model recipe become authentic select regional cuisine style draw large number recipes yummly example show propose system transform traditional japanese recipe sukiyaki french style
deeptingle text prediction classification system train collect work renowned fantastic gay erotica author chuck tingle whereas write assistance tool use everyday form predictive text translation grammar check train generic purportedly neutral datasets deeptingle train specific internally consistent externally arguably eccentric dataset allow us foreground confront norms embed data drive creativity productivity assistance tool tool effectively function extensions cognition technology important identify norms embed within extension us deeptingle realize web application base lstm network glove word embed implement javascript keras js
relation extraction refer task populate database tuples form re1 e2 r relation e1 e2 entities distant supervision one technique try automatically generate train examples base exist kb freebase paper survey techniques distant supervision primarily rely probabilistic graphical model pgms
show chinese poems successfully generate sequence sequence neural model particularly attention mechanism potential problem approach however neural model learn abstract rule poem generation highly creative process involve rule also innovations pure statistical model appropriate principle work propose memory augment neural model chinese poem generation neural model augment memory work together balance requirements linguistic accordance aesthetic innovation lead innovative generations still rule compliant addition find memory mechanism provide interest flexibility use generate poems different style
well establish automatic analyse texts mainly consider frequencies linguistic units eg letter word bigrams methods base co occurrence network consider structure texts regardless nod label ie word semantics paper reconcile distinct viewpoints introduce generalize similarity measure compare texts account network structure texts role individual word network use similarity measure authorship attribution three collections book compose eight author ten book per author high accuracy rat obtain typical value ninety nine thousand, eight hundred and seventy-five much higher traditional tf idf approach collections accuracies also higher take topology network account conclude different properties specific word macroscopic scale structure whole text relevant frequency appearance conversely consider identity nod bring knowledge piece text represent network
decompose multimodal translation two sub task learn translate learn visually ground representations multitask learn framework translations learn attention base encoder decoder ground representations learn image representation prediction approach improve translation performance compare state art multi30k dataset furthermore equally effective train image prediction task external ms coco dataset find improvements train translation model external news commentary parallel text
significant part largest knowledge graph today link open data cloud consist metadata document publications news report media article widespread access document metadata tremendous advancement yet easy assign semantic annotations organize document along semantic concepts provide semantic annotations like concepts skos thesauri classical research topic typically conduct full text document first time offer systematic comparison classification approach investigate far semantic annotations conduct use metadata document title publish label link open data cloud compare classifications obtain analyze document title semantic annotations obtain analyze full text apart prominent text classification baselines knn svm also compare recent techniques learn rank neural network revisit traditional methods logistic regression rocchio naive bay result show across three four datasets performance classifications use title reach ninety quality compare classification performance use full text thus conduct document classification use title reasonable approach automate semantic annotation open new possibilities enrich knowledge graph
accuracy one basic principles journalism however increasingly hard manage due diversity news media editors online news tend use catchy headline trick readers click headline either ambiguous mislead degrade read experience audience thus identify inaccurate news headline task worth study previous work name headline clickbaits mainly focus feature extract headline limit performance since consistency headline news body underappreciated paper clearly redefine problem identify ambiguous mislead headline separately utilize class sequential rule exploit structure information detect ambiguous headline identification mislead headline extract feature base congruence headline body make use large unlabeled data set apply co train method gain increase performance experiment result show effectiveness methods use classifiers detect inaccurate headline crawl different source conduct data analysis
conventional text classification model make bag word assumption reduce text word occurrence count per document recent algorithms word2vec capable learn semantic mean similarity word entirely unsupervised manner use contextual window much faster previous methods word project vector space similar mean word strong powerful project general euclidean space open question embeddings include utility across classification task optimal properties source document construct broadly functional embeddings work demonstrate usefulness pre train embeddings classification task demonstrate custom word embeddings build domain task improve performance word embeddings learn general data include news article wikipedia
project rather complete proof theoretical formalization lambek calculus non associative arbitrary extensions port coq proof assistent hol4 theorem prover improvements new theorems three deduction systems syntactic calculus natural deduction sequent calculus lambek calculus define many relate theorems prove equivalance systems formally prove finally formalization sequent calculus proof coq build support design implement hol4 basic result include sub formula properties call cut free proof formally prove work consider preliminary work towards language parser base category grammars multimodal still ability support context sensitive languages customize extensions
assess degree semantic relatedness word important task variety semantic applications ontology learn semantic web semantic search query expansion accomplish automate fashion many relatedness measure propose however metrics encode information contain underlie corpus thus directly model human intuition solve propose utilize metric learn approach improve exist semantic relatedness measure learn additional information explicit human feedback argue use word embeddings instead traditional high dimensional vector representations order leverage semantic density reduce computational cost rigorously test approach several domains include tag data well publicly available embeddings base wikipedia texts navigation human feedback semantic relatedness learn evaluation extract publicly available datasets men ws three hundred and fifty-three find method significantly improve semantic relatedness measure learn additional information explicit human feedback tag data first generate study embeddings result special interest ontology recommendation engineer also researchers practitioners semantic web techniques
frame question answer qa reinforcement learn task approach call active question answer propose agent sit user black box qa system learn reformulate question elicit best possible answer agent probe system potentially many natural language reformulations initial question aggregate return evidence yield best answer reformulation system train end end maximize answer quality use policy gradient evaluate searchqa dataset complex question extract jeopardy agent outperform state art base model play role environment benchmarks also analyze language agent learn interact question answer system find successful question reformulations look quite different natural language paraphrase agent able discover non trivial reformulation strategies resemble classic information retrieval techniques term weight tf idf stem
citation texts sometimes informative case inaccurate need appropriate context reference paper reflect exact contributions address problem propose unsupervised model use distribute representation word well domain knowledge extract appropriate context reference paper evaluation result show effectiveness model significantly outperform state art furthermore demonstrate effective contextualization method result improve citation base summarization scientific article
social media platforms contain great wealth information provide opportunities us explore hide pattern unknown correlations understand people satisfaction discuss one showcase paper present system twiinsight explore insight twitter data different twitter analysis systems twiinsight automatically extract popular topics different categories eg healthcare food technology sport transport discuss twitter via topic model also identify correlate topics across different categories additionally also discover people opinions tweet topics via sentiment analysis system also employ intuitive informative visualization show uncover insight furthermore also develop compare six popular algorithms three sentiment analysis three topic model
number publish find biomedicine increase continually time specifics domain terminology complicate task relevant publications retrieval current research investigate influence term variability ambiguity paper likelihood retrieve obtain statistics demonstrate significance issue challenge follow present sciai platform allow precise term label resolution
introduce second order vector representations word induce nearest neighborhood topological feature pre train contextual word embeddings analyze effect use second order embeddings input feature two deep natural language process model name entity recognition recognize textual entailment well linear model paraphrase recognition surprisingly find nearest neighbor information alone sufficient capture performance benefit derive use pre train word embeddings furthermore second order embeddings able handle highly heterogeneous data better first order representations though cost specificity additionally augment contextual embeddings second order information improve model performance case due variance random initializations word embeddings utilize nearest neighbor feature multiple first order embed sample also contribute downstream performance gain finally identify intrigue characteristics second order embed space research include much higher density different semantic interpretations cosine similarity
paper focus learn structure aware document representations data without recourse discourse parser additional annotations draw inspiration recent efforts empower neural network structural bias propose model encode document automatically induce rich structural dependencies specifically embed differentiable non projective parse algorithm neural model use attention mechanisms incorporate structural bias experimental evaluation across different task datasets show propose model achieve state art result document model task induce intermediate structure interpretable meaningful
applications use human speech input require speech interface high recognition accuracy word phrase recognise text annotate machine understandable mean link knowledge graph process target application semantic annotations recognise word represent subject predicate object triple collectively form graph often refer knowledge graph type knowledge representation facilitate use speech interfaces speak input application since information represent logical semantic form retrieve store follow use web standard query languages work develop methodology link speech input knowledge graph study impact recognition errors overall process show corpus lower wer annotation link entities dbpedia knowledge graph considerable dbpedia spotlight tool interlink text document link open data use link speech recognition output dbpedia knowledge graph knowledge base speech recognition interface useful applications question answer speak dialog systems
real world document collections involve various type metadata author source date yet commonly use approach model text corpora ignore information specialize model develop particular applications widely use practice customization typically require derivation custom inference algorithm paper build recent advance variational inference methods propose general neural framework base topic model enable flexible incorporation metadata allow rapid exploration alternative model approach achieve strong performance manageable tradeoff perplexity coherence sparsity finally demonstrate potential framework exploration corpus article us immigration
paper focus style transfer basis non parallel text instance broad family problems include machine translation decipherment sentiment modification key challenge separate content aspects style assume share latent content distribution across different text corpora propose method leverage refine alignment latent representations perform style transfer transfer sentence one style match example sentence style population demonstrate effectiveness cross alignment method three task sentiment modification decipherment word substitution cipher recovery word order
show skip gram formulation word2vec train negative sample equivalent weight logistic pca connection allow us better understand objective compare word embed methods extend higher dimensional model
cities thrive place citizens centuries due complex infrastructure emergence cyber physical social systems cpss context aware technologies boost grow interest analyse extract eventually understand city events subsequently utilise leverage citizen observations cities paper investigate feasibility use twitter textual stream extract city events propose hierarchical multi view deep learn approach contextualise citizen observations various city systems service goal build flexible architecture learn representations useful task thus avoid excessive task specific feature engineer apply approach real world dataset consist event report tweet four months san francisco bay area dataset additional datasets collect london result evaluations show propose solution outperform exist model use extract city relate events average accuracy eighty-one class evaluate impact twitter event extraction model use two source authorise report collect road traffic disruptions data transport london api parse time london website sociocultural events analysis show four hundred and ninety-five twitter traffic comment report approximately five hours prior authorities official record moreover discover amongst schedule sociocultural event topics tweet report transportation cultural social events three thousand, one hundred and seventy-five likely influence distribution twitter comment sport weather crime topics
experiment new dataset 16m user comment greek news portal exist datasets english wikipedia comment show rnn outperform previous state art moderation deep classification specific attention mechanism improve overall performance rnn also compare cnn word list baseline consider fully automatic semi automatic moderation
despite success deep learn many front especially image speech application text classification often still good simple linear svm n gram tf idf representation especially smaller datasets deep learn tend emphasize sentence level semantics learn representation model like recurrent neural network recursive neural network however success tf idf representation seem bag word type representation strength take advantage representions present model know tdsm top semantic model extract sentence representation consider word level semantics linearly combine word attention weight sentence level semantics bilstm use text classification apply model character result show model better character base word base convolutional neural network model citezhang15 across seven different datasets one parameters also demonstrate model beat traditional linear model tf idf vectors small polish datasets like news article typically deep learn model surrender
generative adversarial network gans great successes synthesize data however exist gans restrict discriminator binary classifier thus limit learn capacity task need synthesize output rich structure natural language descriptions paper propose novel generative adversarial network rankgan generate high quality language descriptions rather train discriminator learn assign absolute binary predicate individual data sample propose rankgan able analyze rank collection human write machine write sentence give reference group view set data sample collectively evaluate quality relative rank score discriminator able make better assessment turn help learn better generator propose rankgan optimize policy gradient technique experimental result multiple public datasets clearly demonstrate effectiveness propose approach
recent years researchers achieve considerable success apply neural network methods question answer qa approach achieve state art result simplify close domain settings squad rajpurkar et al two thousand and sixteen dataset provide pre select passage answer give question may extract recently researchers begin tackle open domain qa model give question access large corpus eg wikipedia instead pre select passage chen et al 2017a set complex require large scale search relevant passages information retrieval component combine read comprehension model read passages generate answer question performance set lag considerably behind close domain performance paper present novel open domain qa system call reinforce ranker reader r3 base two algorithmic innovations first propose new pipeline open domain qa ranker component learn rank retrieve passages term likelihood generate grind truth answer give question second propose novel method jointly train ranker along answer generation reader model base reinforcement learn report extensive experimental result show method significantly improve state art multiple open domain qa datasets
give advantage recent success english character level subword unit model several nlp task consider equivalent model problem chinese chinese script logographic many chinese logograms compose common substructures provide semantic phonetic syntactic hint work propose explicitly incorporate visual appearance character glyph representation result novel glyph aware embed chinese character inspire success convolutional neural network computer vision use incorporate spatio structural pattern chinese glyphs render raw pixels context two basic chinese nlp task language model word segmentation model learn represent character task relevant semantic syntactic information character level embed
conduct largest ever investigation relationship meteorological condition sentiment human expressions employ three half billion social media post tens millions individuals facebook twitter two thousand and nine two thousand and sixteen find cold temperatures hot temperatures precipitation narrower daily temperature range humidity cloud cover associate worsen expressions sentiment even exclude weather relate post compare magnitude estimate effect size associate notable historical events occur within data
significant amount world knowledge store relational databases however ability users retrieve facts database limit due lack understand query languages sql propose seq2sql deep neural network translate natural language question correspond sql query model leverage structure sql query significantly reduce output space generate query moreover use reward loop query execution database learn policy generate unordered part query show less suitable optimization via cross entropy loss addition publish wikisql dataset eighty thousand, six hundred and fifty-four hand annotate examples question sql query distribute across twenty-four thousand, two hundred and forty-one table wikipedia dataset require train model order magnitude larger comparable datasets apply policy base reinforcement learn query execution environment wikisql model seq2sql outperform attentional sequence sequence model improve execution accuracy three hundred and fifty-nine five hundred and ninety-four logical form accuracy two hundred and thirty-four four hundred and eighty-three
retrieve speak content speak query query example speak term detection std attractive make possible match signal directly acoustic level without transcribe text propose end end query example std model base attention base multi hop network whose input speak query audio segment contain several utterances output state whether audio segment include query model train either supervise scenario use label data unsupervised fashion supervise scenario find attention mechanism multiple hop improve performance attention weight indicate time span detect term unsupervised set model mimic behavior exist query example std system yield performance comparable exist system lower search time complexity
effectively make sense short texts critical task many real world applications search engines social media service recommender systems task particularly challenge short text contain sparse information often sparse machine learn algorithm pick useful signal common practice analyze short text first expand external information usually harvest large collection longer texts literature short text expansion do kinds heuristics propose end end solution automatically learn expand short text optimize give learn task novel deep memory network propose automatically find relevant information collection longer document reformulate short text gate mechanism use short text classification demonstrate task show deep memory network significantly outperform classical text expansion methods comprehensive experiment real world data set
word natural languages composite structure elements structure include root could also composite prefix suffix various nuances relations word express thus order build proper word representation one must take account internal structure corpus texts extract set frequent subwords latter set select pattern ie subwords encapsulate information character n gram regularities selection make use pattern base conditional random field model l1 regularization every word construct new sequence alphabet pattern new alphabet symbols confine local statistical context stronger character therefore allow better representations mathbbrn better build block word representation task subword aware language model pattern base model outperform character base analogues two twenty perplexity point also recurrent neural network word represent sum embeddings pattern par competitive significantly sophisticate character base convolutional architecture
research structure dialogue hamper years large dialogue corpora available impact dialogue research community ability develop better theories well good shelf tool dialogue process happily increase amount information opinion exchange occur natural dialogue online forums people share opinions vast range topics particular interest rejection dialogue also call disagreement denial size available dialogue corpora first time offer opportunity empirically test theoretical account expression inference rejection dialogue paper test whether topic independent feature motivate theoretical predictions use recognize rejection online forums topic independent way result show theoretically motivate feature achieve sixty-six accuracy improvement unigram baseline absolute six
information available web dialogic significant portion take place online forum conversations current social political topics aim develop tool summarize conversations central proposition associate different stances issue abstract object discussion central speaker argument recognize two central proposition realize facet argument hypothesize central proposition exactly arguments people find salient use human summarization probe discover describe corpus human summaries opinionated dialogs show identify similar repeat arguments group facets across many discussions topic define new task argument facet similarity afs show predict afs fifty-four correlation score versus ngram system baseline thirty-nine semantic textual similarity system baseline forty-five
recent years interest use proof assistants formalise reason mathematics program languages grow type logical grammars closely relate type theories systems use functional program perfect candidate next apply curiosity advantage use proof assistants allow one write formally verify proof one type logical systems theory implement immediately compute downside many case formal proof write afterthought incomplete use obtuse syntax make verify proof often much difficult read pen paper proof almost never directly publish paper try remedy example concretely use agda model lambek grishin calculus grammar logic rich vocabulary type form operations present verify procedure cut elimination system briefly outline cps translation proof lambek grishin calculus program agda finally put system use analysis simple example sentence
aspect level sentiment classification aim identify sentiment polarity specific target context previous approach realize importance target sentiment classification develop various methods goal precisely model contexts via generate target specific representations however study always ignore separate model target paper argue target contexts deserve special treatment need learn representations via interactive learn propose interactive attention network ian interactively learn attentions contexts target generate representations target contexts separately design ian model well represent target collocative context helpful sentiment classification experimental result semeval two thousand and fourteen datasets demonstrate effectiveness model
supervise speech separation use supervise learn algorithms learn map input noisy signal output target fast development deep learn supervise separation become important direction speech separation area recent years supervise algorithm train target significant impact performance ideal ratio mask commonly use train target improve speech intelligibility quality separate speech however take account correlation noise clean speech paper use optimal ratio mask train target deep neural network dnn speech separation experiment carry various noise environments signal noise ratio snr condition result show optimal ratio mask outperform train target general
paper describe preliminary study produce distribute large scale database embeddings portuguese twitter stream start experiment relatively small sample focus three challenge volume train data vocabulary size intrinsic evaluation metrics use single gpu able scale vocabulary size two thousand and forty-eight word embed 500k train examples thirty-two thousand, seven hundred and sixty-eight word 10m train examples keep stable validation loss approximately linear trend train time per epoch also observe use less fifty available train examples vocabulary size might result overfitting result intrinsic evaluation show promise performance vocabulary size thirty-two thousand, seven hundred and sixty-eight word nevertheless intrinsic evaluation metrics suffer sensitivity correspond cosine similarity thresholds indicate wider range metrics need develop track progress
explore expression personality adaptivity gesture virtual agents storytelling task conduct two experiment use four different dialogic stories manipulate agent personality extraversion scale whether agents adapt one another gestural performance agent gender result show subject able perceive intend variation extraversion different virtual agents independently story tell gender agent second study show subject also prefer adaptive nonadaptive virtual agents
paper model document revision detection problem minimum cost branch problem rely compute document distance furthermore propose two new document distance measure word vector base dynamic time warp wdtw word vector base tree edit distance wted revision detection system design large scale corpus implement apache spark demonstrate system precisely detect revisions state art methods utilize wikipedia revision dump https snapstanfordedu data wiki metahtml simulate data set
social media useful platform share health relate information due vast reach make good candidate public health monitor task specifically pharmacovigilance study problem extraction adverse drug reaction adr mention social media particularly twitter medical information extraction social media challenge mainly due short highly information nature text compare technical formal medical report current methods adr mention extraction rely supervise learn methods suffer label data scarcity problem state art method use deep neural network specifically class recurrent neural network rnn long short term memory network lstms citehochreiter1997long deep neural network due large number free parameters rely heavily large annotate corpora learn end task real world hard get large label data mainly due heavy cost associate manual annotation towards end propose novel semi supervise learn base rnn model leverage unlabeled data also present abundance social media experiment demonstrate effectiveness method achieve state art performance adr mention extraction
use automatic speech recognition assess speak english learner pronunciation base authentic intelligibility learners speak responses determine support vector machine svm classifier deep learn neural network model predictions transcription correctness use numeric feature produce pocketsphinx alignment mode many recognition pass search substitution deletion expect phoneme insertion unexpected phonemes sequence svm model achieve eighty-two percent agreement accuracy amazon mechanical turk crowdworker transcriptions seventy-five percent report multiple independent researchers use feature svm classifier probability prediction model help computer aid pronunciation teach capt systems provide intelligibility remediation
people converse social political topics similar arguments often paraphrase different speakers across many different conversations debate websites produce curated summaries arguments topics summaries typically consist list sentence represent frequently paraphrase proposition label capture essence one particular aspect argument eg morality second amendment call frequently paraphrase proposition argument facets like curated sit goal induce identify argument facets across multiple conversations produce summaries however aim automatically frame problem consist two step first extract sentence express argument raw social media dialogs rank extract arguments term similarity one another set similar arguments use represent argument facets show predict argument facet similarity correlation average sixty-three compare human topline average sixty-eight three debate topics easily beat several reasonable baselines
present cluster base language model use word embeddings text readability prediction presumably euclidean semantic space hypothesis hold true word embeddings whose train do observe word co occurrences argue cluster word embeddings metric space yield feature representations higher semantic space appropriate text regression also represent feature term histograms approach naturally address document vary lengths empirical evaluation use common core standards corpus reveal feature form cluster base language model significantly improve previously know result corpus readability prediction also evaluate task sentence match base semantic relatedness use wiki simplewiki corpus find feature lead superior match performance
build model take advantage hierarchical structure language without priori annotation longstanding goal natural language process introduce model task machine translation pair recurrent neural network grammar encoder novel attentional rnng decoder apply policy gradient reinforcement learn induce unsupervised tree structure source target train character level datasets explicit segmentation parse annotation model learn plausible segmentation shallow parse obtain performance close attentional baseline
ontologies provide feature like common vocabulary reusability machine readable content also allow semantic search facilitate agent interaction order structure knowledge semantic web web thirty application however challenge ontology engineer automatic learn ie still lack fully automatic approach text corpus dataset various topics form ontology use machine learn techniques paper two topic model algorithms explore namely lsi svd mrlda learn topic ontology objective determine statistical relationship document term build topic ontology ontology graph minimum human intervention experimental analysis build topic ontology semantic retrieve correspond topic ontology user query demonstrate effectiveness propose approach
common recurrent neural architectures scale poorly due intrinsic difficulty parallelize state computations work propose simple recurrent unit sru light recurrent unit balance model capacity scalability sru design provide expressive recurrence enable highly parallelize implementation come careful initialization facilitate train deep model demonstrate effectiveness sru multiple nlp task sru achieve five 9x speed cudnn optimize lstm classification question answer datasets deliver stronger result lstm convolutional model also obtain average seven bleu improvement transformer model translation incorporate sru architecture
role sentiment analysis increasingly emerge study software developers emotions mine crowd generate content within social software engineer tool however shelf sentiment analysis tool train non technical domains general purpose social media thus result misclassifications technical jargon problem report present senti4sd classifier specifically train support sentiment analysis developers communication channel senti4sd train validate use gold standard stack overflow question answer comment manually annotate sentiment polarity exploit suite lexicon keyword base feature well semantic feature base word embed respect mainstream shelf tool use baseline senti4sd reduce misclassifications neutral positive post emotionally negative encourage replications release lab package include classifier word embed space gold standard annotation guidelines
study question answer semi structure data introduce new way apply technique semantic parse apply machine learn provide annotations system infer miss parse logic form manually author rule effect machine learn use provide non syntactic match step ill suit manual rule advantage approach debuggability transparency end user demonstrate effectiveness approach achieve state art performance four thousand and forty-two accuracy standard benchmark dataset table wikipedia
build machine understand text like humans ai complete problem great deal research already go astound result allow everyday people discuss telephone read materials analyse classify computers prerequisite process text semantics common examples computational representation text abstract object operations representation practically correspond make semantic inferences extension simulate understand text complexity granularity semantic process realise constrain mathematical computational robustness expressiveness rigour tool use dissertation contribute series tool diverse mathematical formulation common application model semantic inferences machine process text tool principally express nine distinct model capture aspects semantic dependence highly interpretable non complex ways dissertation reflect present future problems current research paradigm area make recommendations overcome amalgamation body work present dissertation advance complexity granularity semantic inferences make automatically machine
linguistic sequence label general model approach encompass variety problems part speech tag name entity recognition recent advance neural network nns make possible build reliable model without handcraft feature however many case hard obtain sufficient annotations train model study develop novel neural framework extract abundant knowledge hide raw texts empower sequence label task besides word level knowledge contain pre train word embeddings character aware neural language model incorporate extract character level knowledge transfer learn techniques adopt mediate different components guide language model towards key knowledge compare previous methods task specific knowledge allow us adopt concise model conduct efficient train different transfer learn methods propose framework rely additional supervision extract knowledge self contain order information train sequence extensive experiment benchmark datasets demonstrate effectiveness leverage character level knowledge efficiency co train example conll03 ner task model train complete six hours single gpu reach f1 score 9171pm010 without use extra annotation
good amount progress sentiment analysis past ten years include proposal new methods creation benchmark datasets paper however tendency compare model one two datasets either time restraints model tailor specific task accordingly hard understand well certain model generalize across different task datasets paper contribute situation compare several model six different benchmarks belong different domains additionally different level granularity binary three class four class five class show bi lstms perform well across datasets lstms bi lstms particularly good fine grain sentiment task e two class incorporate sentiment information word embeddings train give good result datasets lexically similar train data experiment contribute better understand performance different model architectures different data set consequently detect novel state art result sentube datasets
next step human machine interaction artificial intelligence ai interact predominantly use natural language work would fastest way communicate facebook toy task babi provide useful benchmark compare implementations conversational ai publish experiment far base exploit distributional hypothesis machine learn model exploit natural language understand nlu decomposition language base role reference grammar rrg brain base patom theory combinatorial system conversational ai base linguistics many advantage pass babi task test without parse statistics increase scalability model validate train test data find garbage input output gigo rule base use part speech instead rely mean deep learn difficult debug fix every step model understand change like non statistical computer program deep learn lack explicable reason raise opposition ai partly due fear unknown support goals ai propose extend task use human level statements tense aspect voice embed clauses junctures answer natural language generation nlg instead keywords machine learn permit invalid train data produce incorrect test responses system context track would need intentionally break believe exist learn systems currently solve extend natural language test appear knowledge gap nlp researchers linguists ongoing competitive result promise narrow gap
visual question answer vqa model high robustness accuracy unfortunately current vqa research focus accuracy lack proper methods measure robustness vqa model two main modules algorithm give natural language question image first module take question input output rank basic question similarity score main give question second module take main question image basic question input output text base answer main question give image claim robust vqa model one whose performance change much relate basic question also make available input formulate basic question generation problem lasso optimization also propose large scale basic question dataset bqd rscore novel robustness measure analyze robustness vqa model hope bqd use benchmark evaluate robustness vqa model help community build robust accurate vqa model
recurrent neural net rnn convolutional neural net cnn widely use nlp task capture long term local dependencies respectively attention mechanisms recently attract enormous interest due highly parallelizable computation significantly less train time flexibility model dependencies propose novel attention mechanism attention elements input sequence directional multi dimensional ie feature wise light weight neural net directional self attention network disan propose learn sentence embed base solely propose attention without rnn cnn structure disan compose directional self attention temporal order encode follow multi dimensional attention compress sequence vector representation despite simple form disan outperform complicate rnn model prediction quality time efficiency achieve best test accuracy among sentence encode methods improve recent best result one hundred and two stanford natural language inference snli dataset show state art test accuracy stanford sentiment treebank sst multi genre natural language inference multinli sentence involve compositional knowledge sick customer review mpqa trec question type classification subjectivity subj datasets
paper propose new type graph denote embed graph theory employ distribute representation describe relations graph edge embed graph express linguistic complicate relations express exist edge graph weight graph introduce mathematical definition embed graph translation edge distance graph similarity transform embed graph weight graph weight graph edge graph translation method threshold calculation respectively edge distance embed graph distance base components target vector calculate cosine similarity target vector graph similarity obtain consider relations linguistic complexity addition provide examples data structure embed graph paper
formal semantics distributional semantics two important semantic frameworks natural language process nlp cognitive semantics belong movement cognitive linguistics base contemporary cognitive science framework could deal mean phenomena none fulfill requirements propose applications unify semantic theory characterize important language phenomena theoretical practical significance however although many attempt make recent years exist theory achieve goal yet article introduce new semantic theory potential characterize important mean phenomena natural language fulfill necessary requirements philosophical analysis nlp applications theory base unify representation information construct kind mathematical model call cognitive model interpret natural language expressions compositional manner accept empirical assumption cognitive semantics overcome shortcomings formal semantics distributional semantics theory however simple combination exist theories extensive generalization classic logic formal semantics inherit nearly advantage formal semantics also provide descriptive content object events fine gram possible descriptive content represent result human cognition
present paper present weight ontology approximation heuristic woah novel zero shoot approach ontology estimation conversational agents development environments methodology extract verbs nouns separately data distil dependencies obtain apply similarity sparsity metrics generate ontology estimation configurable term level generalization
paper introduce query base attention cnnqacnn text similarity map end end neural network question answer network compose compare mechanism two stag cnn architecture attention mechanism prediction layer first compare mechanism compare give passage query multiple answer choices build similarity map two stag cnn architecture extract feature word level sentence level time attention mechanism help cnn focus important part passage base query information finally prediction layer find possible answer choice conduct model movieqa dataset use plot synopses achieve seven thousand, nine hundred and ninety-nine accuracy state art dataset
build dialog agents converse naturally humans challenge yet intrigue problem artificial intelligence open domain human computer conversation conversational agent expect respond human responses interest engage way commonsense knowledge integrate model effectively paper investigate impact provide commonsense knowledge concepts cover dialog model represent first attempt integrate large commonsense knowledge base end end conversational model retrieval base scenario propose tri lstm model jointly take account message commonsense select appropriate response experiment suggest knowledge augment model superior knowledge free counterparts automatic evaluation
paper address problem detect expressions moral value tweet use content analysis particularly challenge problem moral value often implicitly signal language tweet contain little contextual information due length constraints address obstacles present novel approach automatically acquire background knowledge external knowledge base enrich input texts thus improve moral value prediction combine basic text feature background knowledge overall context aware framework achieve performance comparable single human annotator best knowledge first attempt incorporate background knowledge prediction implicit psychological variables area computational social science
collect fourteen representative corpora major periods chinese history study corpora include poetic work produce several dynasties novels ming qing dynasties essay news report write modern chinese time span corpora range one thousand and forty-six bce two thousand and seven ce analyze character word distributions viewpoint zipf law look factor affect deviations similarities zipfian curve genres epochs demonstrate influence analyse specifically character distributions poetic work six hundred and eighteen ce one thousand, six hundred and forty-four ce exhibit strike similarity addition although texts dynasty may tend use set character character distributions still deviate
rule base techniques extract relational entities document allow users specify desire entities natural language question finite state automata regular expressions structure query language require linguistic program expertise lack support arabic morphological analysis present morphology base entity relational entity extraction framework arabic merf merf require basic knowledge linguistic feature regular expressions provide ability interactively specify arabic morphological synonymity feature tag type associate regular expressions relations code action define match subexpressions merf construct entities relational entities match specifications evaluate merf several case study result show merf require shorter development time effort compare exist application specific techniques produce reasonably accurate result within reasonable overhead run time
collect nine corpora representative chinese poetry time span one thousand and forty-six bce one thousand, six hundred and forty-four ce study history chinese word collocations pattern flexibly integrate tool able provide new perspectives approach goals illustrate ideas two examples first example show new way compare word preferences poets second example demonstrate utilize corpora historical study chinese word show viability tool academic research wish make helpful enrich exist chinese dictionary well
address problem extract structure representations economic events large corpus news article use combination natural language process machine learn techniques develop techniques allow semi automatic population financial knowledge base turn may use support range data mine exploration task key challenge face domain event often report multiple time vary correctness detail address challenge first collect information pertinent give event entire corpus consider possible representations event finally use supervise learn method rank representations associate confidence score main innovative element approach jointly extract store attribute event single representation quintuple use purpose build test set demonstrate supervise learn approach achieve twenty-five improvement f1 score baseline methods consider earliest latest frequent report event
bag word model standard representation text many linear classifier learners many problem domains linear classifiers prefer complex model due efficiency robustness interpretability bag word text representation capture sufficient information linear classifiers make highly accurate predictions however settings large vocabulary large variance frequency term train corpus many class short text eg single sentence document title bag word representation become extremely sparse reduce accuracy classifiers particular issue settings short texts tend contain infrequently occur rare term lack class conditional evidence work introduce method enrich bag word model complement rare term information relate term general domain specific word vector model reduce sparseness bag word model enrichment approach achieve improve classification several baseline classifiers variety text classification problems approach also efficient require change linear classifier train since bag word enrichment apply text classify
besides text content document associate word usually come rich set meta informa tion categories document semantic syntactic feature word like encode word embeddings incorporate meta information directly generative process topic model improve model accuracy topic quality especially case word occurrence information train data insufficient paper present topic model call metalda able leverage either document word meta information jointly two data argumentation techniques derive efficient gibbs sample algorithm benefit fully local conjugacy model moreover algorithm favour sparsity meta information extensive experiment several real world datasets demonstrate model achieve comparable improve performance term perplexity topic quality particularly handle sparse texts addition compare model use meta information model run significantly faster
despite ubiquity mobile wearable text message applications problem keyboard text decode tackle sufficiently light enormous success deep learn recurrent neural network rnn convolutional neural network cnn natural language understand particular consider keyboard decoders operate devices memory processor resource constraints make challenge deploy industrial scale deep neural network dnn model paper propose sequence sequence neural attention network system automatic text correction completion give erroneous sequence model encode character level hide representations decode revise sequence thus enable auto correction completion achieve combination character level cnn gate recurrent unit gru encoder along word level gate recurrent unit gru attention decoder unlike traditional language model learn billions word corpus size twelve million word order magnitude smaller memory footprint learn model inference prediction also order magnitude smaller conventional language model base text decoders report baseline performance neural keyboard decoders limit domain model achieve word level accuracy ninety character error rate cer twenty-four twitter typo dataset present novel dataset noisy correct mappings induce noise distribution twitter data opensubtitles two thousand and nine dataset model predict word level accuracy ninety-eight sequence accuracy six hundred and eighty-nine user study model achieve average cer twenty-six state art non neural touch screen keyboard decoder cer sixteen
interest individual internet users fall hierarchical structure useful regard build personalize search recommendations study subject construct interest hierarchy single person document perspective study construct user interest hierarchy via user profile organize four hundred and thirty-three thousand, three hundred and ninety-seven user interest refer attentions user attention network uan two hundred million user profile apply louvain algorithm detect hierarchical cluster attentions finally twenty-six level hierarchy thirty-four thousand, six hundred and seventy-six cluster obtain find attention cluster aggregate accord certain topics oppose hyponymy relation base conceptual ontologies topics entities concepts relations restrain hyponymy concept relativity encapsulate user interest capture label attention cluster correspond concepts
present methodology use dynamic evaluation improve neural sequence model model adapt recent history via gradient descent base mechanism cause assign higher probabilities occur sequential pattern dynamic evaluation outperform exist adaptation approach comparisons dynamic evaluation improve state art word level perplexities penn treebank wikitext two datasets five hundred and eleven four hundred and forty-three respectively state art character level cross entropies text8 hutter prize datasets one hundred and nineteen bits char one hundred and eight bits char respectively
appropriate comment code snippets provide insight code functionality helpful program comprehension however due great cost author comment many code project contain adequate comment automatic comment generation techniques propose generate comment piece code order alleviate human efforts annotate code exist approach attempt exploit certain correlations usually manually give code generate comment could easily violate cod pattern change hence performance comment generation decline paper first build c2cgit large dataset open project github 20times larger exist datasets propose new attention module call code attention translate code comment able utilize domain feature code snippets symbols identifiers make ablation study determine effect different part code attention experimental result demonstrate propose module better performance exist approach bleu meteor
persuasivenes creative art aim make people believe certain set beliefs many time creativity adapt richness one domain another strike chord target audience research present persuaide persuasive system base linguistic creativity transform give sentence generate various form persuade sentence various form cover multiple focus persuasion memorability sentiment give simple product line algorithm compose several step include select appropriate well know expression target domain add memorability ii identify keywords entities give sentence expression transform produce creative persuade sentence iii add positive negative sentiment persuasion persuasive conversion manually verify use qualitative result effectiveness propose approach empirically discuss
author owl dl ontologies intellectually challenge make process simpler many systems accept natural language text input text base ontology author approach successful combine effective method extract ontological axioms text extract axioms unrestricted english input substantially challenge task due richness language control natural languages cnls propose context tend highly restrictive paper propose new cnl call tedei textual description identifier whose grammar inspire different ways owl dl construct express english build system transform tedei sentence correspond owl dl axioms ambiguity due different possible lexicalizations sentence semantic ambiguity present sentence challenge context find best way handle challenge construct axioms correspond alternative formalizations sentence end user make appropriate choice output compare human author axioms substantial number case human author axiom indeed one alternatives give system propose system substantially enhance type sentence structure use ontology author
many real world applications require automate data annotation identify tissue origins base gene expressions classify image semantic categories annotation class often numerous subject change time annotate examples become major bottleneck supervise learn methods science high value domains large repositories data sample often available together two source organic supervision lexicon annotation class text descriptions accompany data sample distant supervision emerge promise paradigm exploit indirect supervision automatically annotate examples text description contain class mention lexicon however due linguistic variations ambiguities train data inherently noisy limit accuracy approach paper introduce auxiliary natural language process system text modality incorporate co train reduce noise augment signal distant supervision without use manually label data ezlearn system learn accurately annotate data sample functional genomics scientific figure comprehension substantially outperform state art supervise methods train tens thousands annotate examples
paper present methodology result obtain team dub blue man group assin portuguese avaliaccao de similaridade semantica e inferencia textual competition hold propor 2016footnoteinternational conference computational process portuguese language http propor2016difculpt team strategy consist evaluate methods base semantic word vectors follow two distinct directions one make use low dimensional compact feature set two deep learn base strategies deal high dimensional feature vectors evaluation result demonstrate first strategy promise result second strategy discard result consider best run six team able achieve best accuracy f1 value entailment recognition brazilian portuguese set best f1 score overall semantic similarity task team rank second brazilian portuguese set third consider set
paper propose statistical test determine whether give word use polysemic word statistic word test roughly correspond fluctuation sense neighbor word nd word even though sense word correspond single vector discuss polysemy word affect position vectors finally also explain method detect effect
present new approach design deep network natural language process nlp base general technique tensor product representations tprs encode process symbol structure distribute neural network network architecture tensor product generation network tpgn propose capable principle carry tpr computation use unconstrained deep learn design internal representations instantiate model image caption generation tpgn outperform lstm baselines evaluate coco dataset tpr capable structure enable interpretation internal representations operations prove contain considerable grammatical content caption generation model interpret generate sequence grammatical categories retrieve word categories plan encode distribute representation
someone look certain publication field computer science search person likely use dblp find desire publication dblp data set continuously extend new publications rather metadata example name involve author title publication date size data set already remarkable specific areas still improve dblp offer huge collection english paper paper concern computer science publish english nevertheless official publications languages suppose add data set one kind japanese paper diploma thesis show way automatically process publication list japanese paper make ready import dblp data set especially important problems along way process transcription handle personal name match japanese name
question process fundamental step question answer qa application quality impact performance qa application major challenge issue process question extract semantic natural language question nlqs human language ambiguous ambiguity may occur two level lexical syntactic paper propose new approach resolve lexical ambiguity problem integrate context knowledge concepts knowledge domain shallow natural language process snlp techniques concepts knowledge model use ontology context knowledge obtain wordnet determine base neighborhood word question approach apply university qa system
propose novel memory network model name read write memory network rwmn perform question answer task large scale multimodal movie story understand key focus rwmn model design read network write network consist multiple convolutional layer enable memory read write operations high capacity flexibility exist memory augment network model treat memory slot independent block use multi layer cnns allow model read write sequential memory cells chunk reasonable represent sequential story adjacent memory block often strong correlations evaluation apply model six task movieqa benchmark achieve best accuracies several task especially visual qa task model show potential better understand content story also abstract information relationships character reason action
development electronic media heterogeneity arabic data web idea build clean corpus certain applications natural language process include machine translation information retrieval question answer become press manuscript seek create develop corpus pair question texts constitution provide better base experimentation step thus try model constitution method arabic insofar recover texts web could prove answer factual question develop java script extract give query list html page clean page extent data base texts corpus pair question texts addition give preliminary result proposal method investigations construction arabic corpus also present document
context electronic health record automate diagnosis cod patient note useful task challenge one due large number cod length patient note investigate four model assign multiple icd cod discharge summaries take mimic ii iii present hierarchical attention gru ha gru hierarchical approach tag document identify sentence relevant label ha gru achieve state art result furthermore learn sentence level attention layer highlight model decision process allow easier error analysis suggest future directions improvement
present edina university edinburgh social bot amazon alexa prize competition edina conversational agent whose responses utilize data harvest amazon mechanical turk amt innovative new technique call self dialogues conversations single amt worker play participants dialogue dialogues surprisingly natural efficient collect reflective relevant trend topics self dialogues provide train data generative neural network well basis soft rule use match score component match soft rule user utterance associate confidence score show strongly indicative reply quality allow component self censor effectively integrate components edina full architecture feature rule base system back match score back generative neural network hybrid data drive methodology thus address coverage limitations strictly rule base approach lack guarantee strictly machine learn approach
propose novel approach learn word embeddings base extend version distributional hypothesis model derive word embed vectors use etymological composition word rather context appear strength require large text corpus instead require reliable access etymological root word make specially fit languages logographic write systems model consist three step one build etymological graph bipartite network word etymological root two obtain biadjacency matrix etymological graph reduce dimensionality three use columns row result matrices embed vectors test model chinese sino korean vocabularies graph form set one hundred and seventeen thousand chinese word set one hundred and thirty-five thousand sino korean word case show model perform well task synonym discovery
paper present new deep learn architecture natural language inference nli firstly introduce new architecture alignment pair compare compress propagate upper layer enhance representation learn secondly adopt factorization layer efficient expressive compression alignment vectors scalar feature use augment base word representations design approach aim conceptually simple compact yet powerful conduct experiment three popular benchmarks snli multinli scitail achieve competitive performance lightweight parameterization model also enjoy approx three time reduction parameter size compare exist state art model eg esim diin maintain competitive performance additionally visual analysis show propagate feature highly interpretable
type number venues increase automate analysis sentiment textual resources become essential data mine task paper investigate problem mine opinions collection informal short texts positive negative sentiment strength texts detect focus non english language resources text mine approach would help enhance sentiment analysis languages list opinionated word exist propose new method project text dense low dimensional feature vectors accord sentiment strength word detect mixture positive negative sentiments multi variant scale empirical evaluation propose framework turkish tweet show approach get good result opinion mine
program languages limit number reserve keywords character base tokens define language specification however programmers rich use natural language within code comment text literals name entities programmer define name find source code rich source information build high level understand project goal paper apply topic model name use one hundred and thirty-six million repositories perceive infer topics one problems study occurrence duplicate repositories officially mark fork obscure fork show address use identifiers extract topic model open discussion name source code elaborate approach remove exact duplicate fuzzy duplicate repositories use locality sensitive hash bag word model discuss work topic model finally present result data analysis together open access source code tool datasets
recurrent neural network model attention mechanism prove extremely effective wide variety sequence sequence problems however fact soft attention mechanisms perform pass entire input sequence produce element output sequence preclude use online settings result quadratic time complexity base insight alignment input output sequence elements monotonic many problems interest propose end end differentiable method learn monotonic alignments test time enable compute attention online linear time validate approach sentence summarization machine translation online speech recognition problems achieve result competitive exist sequence sequence model
paper describe methodology infer bullish bearish sentiment towards company brand specifically approach leverage affective lexica word embeddings combination convolutional neural network infer sentiment financial news headline towards target company architecture use evaluate context semeval two thousand and seventeen challenge task five subtask two obtain best performance
perception expression emotion key factor success dialogue systems conversational agents however problem study large scale conversation generation far paper propose emotional chat machine ecm generate appropriate responses content relevant grammatical also emotion emotionally consistent best knowledge first work address emotion factor large scale conversation generation ecm address factor use three new mechanisms respectively one model high level abstraction emotion expressions embed emotion categories two capture change implicit internal emotion state three use explicit emotion expressions external emotion vocabulary experiment show propose model generate responses appropriate content also emotion
typically every part coherent text plausible reason presence function perform overall semantics text rhetorical relations eg contrast explanation describe part text link knowledge socalled discourse structure apply successfully several natural language process task work study use rhetorical relations information retrieval ir correlation certain rhetorical relations retrieval performance knowledge document rhetorical relations useful ir present language model modification consider rhetorical relations estimate relevance document query empirical evaluation different versions model trec settings show certain rhetorical relations benefit retrieval effectiveness notably ten mean average precision state art baseline
end end train deep learn base model allow implicit learn intermediate representations base final task loss however end end approach ignore useful domain knowledge encode explicit intermediate level supervision hypothesize use intermediate representations auxiliary supervision lower level deep network may good way combine advantage end end train traditional pipeline approach present experiment conversational speech recognition use lower level task phoneme recognition multitask train approach encoder decoder model direct character transcription compare multiple type lower level task analyze effect auxiliary task result switchboard corpus show approach improve recognition accuracy standard encoder decoder model eval2000 test set
label sequence transduction task transform one sequence another sequence satisfy desiderata specify set label paper propose multi space variational encoder decoders new model label sequence transduction semi supervise learn generative model use neural network handle discrete continuous latent variables exploit various feature data experiment show model provide powerful supervise framework also effectively take advantage unlabeled data sigmorphon morphological inflection benchmark model outperform single model state art result large margin majority languages
recently topic model widely use discover abstract topics text corpora exist topic model base assumption three layer hierarchical bayesian structure ie document model probability distribution topics topic probability distribution word however assumption optimal intuitively reasonable assume topic probability distribution concepts concept probability distribution word ie add latent concept layer topic layer word layer traditional three layer assumption paper verify propose assumption incorporate new assumption two representative topic model obtain two novel topic model extensive experiment conduct among propose model correspond baselines result show propose model significantly outperform baselines term case study perplexity mean new assumption reasonable traditional one
paper describe multi view ensemble approach semeval two thousand and seventeen task four sentiment analysis twitter specifically message polarity classification subtask english subtask system vote ensemble base classifier train different feature space first space bag word model linear svm base classifier second third space two different strategies combine word embeddings represent sentence use linear svm logistic regressor base classifiers propose system rank 18th thirty-eight systems consider f1 score 20th consider recall
paper deal classify ambiguities multimodal languages evolve classifications methods literature ambiguities natural language visual language empirically define original classification ambiguities multimodal interaction use linguistic perspective classification distinguish semantic syntactic multimodal ambiguities subclasses intercept use rule base method implement software module experimental result achieve accuracy obtain classification compare expect one define human judgment nine hundred and forty-six semantic ambiguities class nine hundred and twenty-one syntactic ambiguities class
text normalization techniques base rule lexicons supervise train require large corpora scalable domain interchangeable make unsuitable normalize user generate content ugc current tool available brazilian portuguese make use techniques work propose technique base distribute representation word word embeddings generate continuous numeric vectors high dimensionality represent word vectors explicitly encode many linguistic regularities pattern well syntactic semantic word relationships word share semantic similarity represent similar vectors base feature present totally unsupervised expandable language domain independent method learn normalization lexicons word embeddings approach obtain high correction rate orthographic errors internet slang product review outperform current available tool brazilian portuguese
first step model emotional state person build sentiment analysis model exist deep neural network algorithms compare model psychological measurements enlighten relationship experiment first examine psychological state sixty-four participants ask summarize story book chronicle death foretell marquez one thousand, nine hundred and eighty-one secondly train model use crawl three hundred and sixty-five thousand, eight hundred and two movie review data evaluate participants summaries use pretrained model concept transfer learn background emotion affect memories investigate relationship evaluation score summaries computational model examine psychological measurements result show although cnn perform best among deep neural network algorithms lstm gru result relate psychological state rather gru show explainable result depend psychological state contribution paper summarize follow one enlighten relationship computational model psychological measurements two suggest framework objective methods evaluate emotion real sentiment analysis person
although neural network well suit sequential transfer learn task catastrophic forget problem hinder proper integration prior knowledge work propose solution problem use multi task objective base idea distillation mechanism directly penalize forget share representation layer knowledge integration phase train demonstrate approach twitter domain sentiment analysis task sequential knowledge transfer four relate task show technique outperform network fine tune target task additionally show empirical evidence examples forget useful knowledge source task forget standard fine tune surprisingly find first distil human make rule base sentiment engine recurrent neural network integrate knowledge target task data lead substantial gain generalization performance experiment demonstrate power multi source transfer techniques practical text analytics problems pair distillation particular semeval two thousand and sixteen task four subtask nakov et al two thousand and sixteen dataset surpass state art establish competition comparatively simple model architecture even competitive train label task specific data
order adopt deep learn information retrieval model need capture relevant information require assess relevance document give user query previous work successfully capture unigram term match fully employ position dependent information proximity term dependencies insufficiently explore work propose novel neural ir model name pacrr aim better model position dependent interactions query document extensive experiment six years trec web track data confirm propose model yield better result multiple benchmarks
paper present approach develop faculty engineer university porto participate semeval two thousand and seventeen task five fine grain sentiment analysis financial microblogs news task consist predict real continuous variable ten ten represent polarity intensity sentiment concern company stock mention short texts model task regression analysis problem combine traditional techniques pre process short texts bag word representations lexical base feature enhance financial specific bag embeddings use external collection tweet news headline mention company stock sandp five hundred create financial word embeddings able capture domain specific syntactic semantic similarities result approach obtain cosine similarity score sixty-nine sub task fifty-one microblogs sixty-eight sub task fifty-two news headline
recurrent neural network rnn widely use solve variety problems quantity data amount available compute increase model size number parameters recent state art network make hard deploy especially mobile phone embed devices challenge due size model time take evaluate order deploy rnns efficiently propose technique reduce parameters network prune weight initial train network end train parameters network sparse accuracy still close original dense neural network network size reduce 8x time require train model remain constant additionally prune larger dense network achieve better baseline performance still reduce total number parameters significantly prune rnns reduce size model also help achieve significant inference time speed use sparse matrix multiply benchmarks show use technique model size reduce ninety speed around 2x 7x
artificial intelligence federate numerous scientific field aim develop machine able assist human operators perform complex treatments demand high cognitive skills eg learn decision process central quest give machine ability estimate liken similarity things way human be estimate similarity stimuli context book focus semantic measure approach design compare semantic entities units language eg word sentence concepts instance define knowledge base aim measure assess similarity relatedness semantic entities take account semantics ie mean intuitively word tea coffee refer stimulate beverage estimate semantically similar word toffee confection coffee despite last pair higher syntactic similarity two state art approach estimate quantify semantic similarities relatedness semantic entities present detail first one rely corpora analysis base natural language process techniques semantic model second base less formal computer readable workable form knowledge semantic network thesaurus ontologies beyond simple inventory categorization exist measure aim monograph convey novices well researchers domains towards better understand semantic similarity estimation generally semantic measure
due promise alleviate information overload text summarization attract attention many researchers however remain serious challenge first prove empirical limit recall f1 score extractive summarizers duc datasets rouge evaluation single document multi document summarization task next define concept compressibility document present new model summarization generalize exist model literature integrate several dimension summarization viz abstractive versus extractive single versus multi document syntactic versus semantic finally examine new exist single document summarization algorithms single framework compare state art summarizers duc data
substantial progress factoid question answer qa answer complex question remain challenge typically require large body knowledge inference techniques open information extraction open ie provide way generate semi structure knowledge qa date knowledge use answer simple question retrieval base methods overcome limitation present method reason open ie knowledge allow complex question handle use recently propose support graph optimization framework qa develop new inference model open ie particular one work effectively multiple short facts noise relational structure tuples model significantly outperform state art structure solver complex question vary difficulty also remove reliance manually curated knowledge
linguistically diverse datasets critical train evaluate robust machine learn systems data collection costly process often require experts crowdsourcing process paraphrase generation effective mean expand natural language datasets limit analysis trade off arise design task paper present first systematic study key factor crowdsourcing paraphrase collection consider variations instructions incentives data domains workflows manually analyze paraphrase correctness grammaticality linguistic diversity observations provide new insight trade off accuracy diversity crowd responses arise result task design provide guidance future paraphrase generation procedures
media full false claim even oxford dictionaries name post truth word two thousand and sixteen make important ever build systems identify veracity story kind discourse around rumoureval semeval share task aim identify handle rumour reactions text present annotation scheme large dataset cover multiple topics families claim reply use pose two concrete challenge well result achieve participants challenge
proliferation social media communication information dissemination make ideal platform spread rumor automatically debunk rumor stage diffusion know textitearly rumor detection refer deal sequential post regard dispute factual claim certain variations highly textual duplication time thus identify trend rumor demand efficient yet flexible model able capture long range dependencies among post produce distinct representations accurate early detection however challenge task apply conventional classification algorithms rumor detection earliness since rely hand craft feature require intensive manual efforts case large amount post paper present deep attention model basis recurrent neural network rnn learn textitselectively temporal hide representations sequential post identify rumor propose model delve soft attention recurrence simultaneously pool distinct feature particular focus produce hide representations capture contextual variations relevant post time extensive experiment real datasets collect social media websites demonstrate one deep attention base rnn model outperform state arts rely hand craft feature two introduction soft attention mechanism effectively distill relevant part rumor original post advance three propose method detect rumor quickly accurately competitors
paper describe attempt produce state art twitter sentiment classifier use convolutional neural network cnns long short term memory lstms network system leverage large amount unlabeled data pre train word embeddings use subset unlabeled data fine tune embeddings use distant supervision final cnns lstms train semeval two thousand and seventeen twitter dataset embeddings fin tune boost performances ensemble several cnns lstms together approach achieve first rank five english subtasks amongst forty team
many natural language process computational linguistics applications involve generation new texts base exist texts summarization text simplification machine translation however serious problem haunt applications decades automatically accurately assess quality applications paper present preliminary result one especially useful challenge problem nlp system evaluation pinpoint content differences two text passages especially large pas sag article book idea intuitive different exist approach treat one text passage small knowledge base ask large number question exhaustively identify content point compare correctly answer question two text passages able compare content precisely experiment use two thousand and seven duc summarization corpus clearly show promise result
speak languages speakers divide space phonetic possibilities different regions correspond different phonemes consider simple exemplar model division phonetic space vary time among population language users particular model consider show system initialize give set phonemes phonemes become extinct phonemes maintain system time contrast observe complex model furthermore show boundaries phonemes fluctuate quantitatively study fluctuations simple instance model result prepare grind sophisticate model phonemes go extinct new phonemes emerge process
address personalization issue image caption discuss yet previous research query image aim generate descriptive sentence account prior knowledge user active vocabularies previous document applications personalize image caption tackle two post automation task hashtag prediction post generation newly collect instagram dataset consist 11m post 63k users propose novel caption model name context sequence memory network csmn unique update previous memory network model include exploit memory repository multiple type context information ii append previously generate word memory capture long term information without suffer vanish gradient problem iii adopt cnn memory structure jointly represent nearby order memory slot better context understand quantitative evaluation user study via amazon mechanical turk show effectiveness three novel feature csmn performance enhancement personalize image caption state art caption model
model attention neural multi source sequence sequence learn remain relatively unexplored area despite usefulness task incorporate multiple source languages modalities propose two novel approach combine output attention mechanisms source sequence flat hierarchical compare propose methods exist techniques present result systematic evaluation methods wmt16 multimodal translation automatic post edit task show propose methods achieve competitive result task
propose summarization approach scientific article take advantage citation context document discourse model citations previously use generate scientific summaries lack relate context reference article therefore accurately reflect article content method overcome problem inconsistency citation summary article content provide context citation also leverage inherent scientific article discourse produce better summaries show propose method effectively improve exist summarization approach greater thirty improvement best perform baseline term textscrouge score tac2014 scientific summarization dataset dataset use evaluation biomedical domain approach general therefore adaptable domains
recurrent neural network show much promise many sub areas natural language process range document classification machine translation automatic question answer despite promise many recurrent model read whole text word word make slow handle long document example difficult use recurrent network read book answer question paper present approach read text skip irrelevant information need underlie model recurrent network learn far jump read word input text employ standard policy gradient method train model make discrete jump decisions benchmarks four different task include number prediction sentiment analysis news article classification automatic qanda propose model modify lstm jump six time faster standard sequential lstm maintain even better accuracy
recent work explore deep architectures learn multimodal speech representation eg audio image articulation audio supervise way investigate role combine different speech modalities ie audio visual information represent lips movements weakly supervise way use siamese network lexical different side information particular ask whether one modality benefit provide richer representation phone recognition weakly supervise set introduce mono task multi task methods merge speech visual modalities phone recognition mono task learn consist apply siamese network concatenation two modalities multi task learn receive several different combinations modalities train time show multi task learn enhance discriminability visual multimodal input minimally impact auditory input furthermore present qualitative analysis obtain phone embeddings show cross modal visual input improve discriminability phonological feature visually discernable round open close labial place articulation result representations closer abstract linguistic feature base audio
several approach recently propose learn decentralize deep multiagent policies coordinate via differentiable communication channel policies effective many task interpretation induce communication strategies remain challenge propose interpret agents message translate unlike typical machine translation problems parallel data learn instead develop translation model base insight agent message natural language string mean thing induce belief world listener present theoretical guarantee empirical evidence approach preserve semantics pragmatics message ensure players communicate translation layer suffer substantial loss reward relative players common language
large amount recent research focus task combine language vision result proliferation datasets methods one task action recognition whose applications include image annotation scene stand image retrieval survey categorize exist ap proaches base conceptualize problem provide detail review exist datasets highlight di versity well advantage disad vantages focus recently devel oped datasets link visual informa tion linguistic resources provide fine grain syntactic semantic anal ysis action image
paper describe team turing submission semeval two thousand and seventeen rumoureval determine rumour veracity support rumour semeval two thousand and seventeen task eight subtask subtask address challenge rumour stance classification involve identify attitude twitter users towards truthfulness rumour discuss stance classification consider important step towards rumour verification therefore perform well task expect useful debunk false rumour work classify set twitter post discuss rumour either support deny question comment underlie rumour propose lstm base sequential model model conversational structure tweet achieve accuracy seven hundred and eighty-four rumoureval test set outperform systems subtask
set understand effect differ language ability cybercriminals navigate webmail account locate sensitive information end configure thirty gmail honeypot account english romanian greek language settings populate account email message languages subscribe select online newsletters hide email message fake bank account fifteen account mimic real world webmail users sometimes store sensitive information account leak credentials honey account via paste sit surface web dark web collect data fifteen days statistical analyse data show cybercriminals likely discover sensitive information bank account information greek account remain account contrary expectation greek ought constitute barrier understand non greek visitors greek account also extract important word among email cybercriminals access approximation keywords search within honey account find financial term feature among top word summary show language play significant role ability cybercriminals access sensitive information hide compromise webmail account
automatic affect recognition challenge task due various modalities emotions express applications find many domains include multimedia retrieval human computer interaction recent years deep neural network use great success determine emotional state inspire success propose emotion recognition system use auditory visual modalities capture emotional content various style speak robust feature need extract purpose utilize convolutional neural network cnn extract feature speech visual modality deep residual network resnet fifty layer addition importance feature extraction machine learn algorithm need also insensitive outliers able model context tackle problem long short term memory lstm network utilize system train end end fashion also take advantage correlations stream manage significantly outperform traditional approach base auditory visual handcraft feature prediction spontaneous natural emotions recola database avec two thousand and sixteen research challenge emotion recognition
neural conversational model require substantial amount dialogue data parameter estimation therefore usually learn large corpora chat forums movie subtitle corpora however often challenge work notably due frequent lack turn segmentation presence multiple reference external dialogue paper show challenge mitigate add weight model architecture weight model estimate dialogue data associate train example numerical weight reflect intrinsic quality dialogue model train time sample weight include empirical loss minimise evaluation result retrieval base model train movie tv subtitle demonstrate inclusion weight model improve model performance unsupervised metrics
hate speech detection twitter critical applications like controversial event extraction build ai chatterbots content recommendation sentiment analysis define task able classify tweet racist sexist neither complexity natural language construct make task challenge perform extensive experiment multiple deep learn architectures learn semantic word embeddings handle complexity experiment benchmark dataset 16k annotate tweet show deep learn methods outperform state art char word n gram methods eighteen f1 point
word natural language follow zipfian distribution whereby word frequent rare learn representations word long tail distribution require enormous amount data representations rare word train directly end task usually poor require us pre train embeddings external data treat rare word vocabulary word unique representation provide method predict embeddings rare word fly small amount auxiliary data network train end end downstream task show improve result baselines embeddings train end task read comprehension recognize textual entailment language model
legal professionals worldwide currently try get pace explosive growth legal document availability digital mean drive need high efficiency legal information retrieval ir question answer qa methods ir task particular set unique challenge invite use semantic motivate nlp techniques work two stage method legal information retrieval propose combine lexical statistics distributional sentence representations context competition legal information extraction entailment coliee combination do use disambiguation rule apply rank obtain n gram statistics rank do result evaluate ambiguity disambiguation do result decide unreliable give query competition experimental result indicate small gain overall retrieval performance use propose approach additionally analysis error improvement case present better understand contributions
relational reason central component generally intelligent behavior prove difficult neural network learn paper describe use relation network rns simple plug play module solve problems fundamentally hinge relational reason test rn augment network three task visual question answer use challenge dataset call clevr achieve state art super human performance text base question answer use babi suite task complex reason dynamic physical systems use curated dataset call sort clevr show powerful convolutional network general capacity solve relational question gain capacity augment rns work show deep learn architecture equip rn module implicitly discover learn reason entities relations
paper explore sppim base text classification method experiment reveal sppim method equal even superior sgns method text classification task three international standard text datasets namely 20newsgroups reuters52 webkb compare sgns although sppmi provide better solution necessarily better sgns text classification task base analysis sgns take consideration weight calculation decomposition process better performance sppim standard datasets inspire propose wl sppim semantic model base sppim model experiment show wl sppim approach better classification higher scalability text classification task compare lda sgns sppim approach
describe marmara turkish coreference corpus annotation whole metu sabanci turkish treebank mention coreference chain collect eight independent annotations document allow fully automatic adjudication provide baseline system turkish mention detection coreference resolution evaluate corpus
syntactic parse process obtain internal structure sentence natural languages crucial task artificial intelligence applications need extract mean natural language text speech sentiment analysis one example application parse recently prove useful recent years significant advance accuracy parse algorithms article perform empirical task orient evaluation determine parse accuracy influence performance state art rule base sentiment analysis system determine polarity sentence parse tree particular evaluate system use four well know dependency parsers include current model state art accuracy innacurate model however require less computational resources experiment show parsers produce similarly good result sentiment analysis task without accuracy relevant influence result since parse currently task relatively high computational cost vary strongly algorithms suggest sentiment analysis researchers users prioritize speed accuracy choose parser parse researchers investigate model improve speed even cost accuracy
resolve abstract anaphora important difficult task text understand yet recent advance representation learn task become tangible aim central property abstract anaphora establish relation anaphor embed anaphoric sentence typically non nominal antecedent propose mention rank model learn abstract anaphors relate antecedents lstm siamese net overcome lack train data generate artificial anaphoric sentence antecedent pair model outperform state art result noun resolution also report first benchmark result abstract anaphora subset arrau corpus corpus present greater challenge due mixture nominal pronominal anaphors greater range confounders find model variants outperform baselines nominal anaphors without train individual anaphor data still lag behind pronominal anaphors model select syntactically plausible candidates disregard syntax discriminate candidates use deeper feature
depthwise separable convolutions reduce number parameters computation use convolutional operations increase representational efficiency show successful image classification model obtain better model previously possible give parameter count xception architecture considerably reduce number parameters require perform give level mobilenets family architectures recently convolutional sequence sequence network apply machine translation task good result work study depthwise separable convolutions apply neural machine translation introduce new architecture inspire xception bytenet call slicenet enable significant reduction parameter count amount computation need obtain result like bytenet similar parameter count achieve new state art result addition show depthwise separable convolutions perform well machine translation investigate architectural change enable observe thank depthwise separability increase length convolution windows remove need filter dilation also introduce new super separable convolution operation reduce number parameters computational cost obtain state art result
neural machine translation mean revolution field nevertheless post edit output system mandatory task require high translation quality post edit offer unique opportunity improve neural machine translation systems use online learn techniques treat post edit translations new fresh train data review classical learn methods propose new optimization algorithm thoroughly compare online learn algorithms post edit scenario result show significant improvements translation quality effort reduction
present simple encode unlabeled noncrossing graph show latent counterpart help us represent several families direct undirected graph use syntactic semantic parse natural language context free languages families separate purely basis forbid pattern latent encode eliminate need differentiate families non cross graph inference algorithms one algorithm work search space control parser input
rapid growth scientific literature make difficult researchers quickly learn developments respective field scientific document summarization address challenge provide summaries important contributions scientific paper present framework scientific summarization take advantage citations scientific discourse structure citation texts often lack evidence context support content cite paper even sometimes inaccurate first address problem inaccuracy citation texts find relevant context cite paper propose three approach contextualizing citations base query reformulation word embeddings supervise learn train model identify discourse facets citation finally propose method summarize scientific paper leverage faceted citations correspond contexts evaluate propose method two scientific summarization datasets biomedical computational linguistics domains extensive evaluation result show methods improve state art large margins
dominant sequence transduction model base complex recurrent convolutional neural network encoder decoder configuration best perform model also connect encoder decoder attention mechanism propose new simple network architecture transformer base solely attention mechanisms dispense recurrence convolutions entirely experiment two machine translation task show model superior quality parallelizable require significantly less time train model achieve two hundred and eighty-four bleu wmt two thousand and fourteen english german translation task improve exist best result include ensembles two bleu wmt two thousand and fourteen english french translation task model establish new single model state art bleu score four hundred and eighteen train thirty-five days eight gpus small fraction train cost best model literature show transformer generalize well task apply successfully english constituency parse large limit train data
paper advance state art text understand medical guidelines release two new annotate clinical guidelines datasets establish baselines use machine learn extract condition action pair contrast prior work rely manually create rule report experiment several supervise machine learn techniques classify sentence whether express condition action show limitations possible extensions work text mine medical guidelines
goal semantic parse map natural language machine interpretable mean representation language mrl one constraints limit full exploration deep learn technologies semantic parse lack sufficient annotation train data paper propose use sequence sequence multi task setup semantic parse focus transfer learn explore three multi task architectures sequence sequence model compare performance independently train model experiment show multi task setup aid transfer learn auxiliary task large label data target task smaller label data see absolute accuracy gain range ten forty-four house data set also see good gain range twenty-five seventy atis semantic parse task syntactic semantic auxiliary task
comment dependency distance new perspective syntactic pattern natural language haitao liu et al
cross lingual representations word enable us reason word mean multilingual contexts key facilitator cross lingual transfer develop natural language process model low resource languages survey provide comprehensive typology cross lingual word embed model compare data requirements objective function recur theme survey many model present literature optimize objectives seemingly different model often equivalent modulo optimization strategies hyper parameters also discuss different ways cross lingual word embeddings evaluate well future challenge research horizons
state art solutions vocabulary mismatch information retrieval ir mainly aim leverage either relational semantics provide external resources distributional semantics recently investigate deep neural approach guide intuition relational semantics might improve effectiveness deep neural approach propose deep semantic resource inference model dsrim rely one representation raw data model relational semantics text jointly consider object relations express knowledge resource two end end neural architecture learn query document relevance leverage distributional relational semantics document query experimental evaluation carry two trec datasets trec terabyte trec cds track rely respectively wordnet mesh resources indicate model outperform state art semantic deep neural ir model
investigate integration plan mechanism encoder decoder architecture explicit alignment character level machine translation develop model plan ahead compute alignments source target sequence construct matrix propose future alignments commitment vector govern whether follow recompute plan mechanism inspire strategic attentive reader writer straw model propose model end end trainable fully differentiable operations show outperform strong baseline three character level decoder neural machine translation wmt fifteen corpus analysis demonstrate model compute qualitatively intuitive alignments achieve superior performance fewer parameters
clarin common language resources technology infrastructure regard one important european research infrastructures offer promote wide array useful service digital research linguistics humanities however assessment users core technical development highly limit therefore unclear community thoroughly aware status quo grow infrastructure addition clarin seem fully materialise market business plan strategies despite strong technical assets article analyse web traffic virtual language observatory one main web applications clarin symbol pan european search cooperation evaluate users performance service transparent scientific way envisage paper raise awareness press issue objective transparent operation infrastructure though open evaluation synergy market technical development also investigate science web analytics attempt document research process purpose reusability reproducibility thus find universal lessons use web analytics rather merely produce statistical report particular website lose value outside context
much human dialogue occur semi cooperative settings agents different goals attempt agree common decisions negotiations require complex communication reason skills success easy measure make interest task ai gather large dataset human human negotiations multi issue bargain task agents observe reward function must reach agreement deal via natural language dialogue first time show possible train end end model negotiation must learn linguistic reason skills annotate dialogue state also introduce dialogue rollouts model plan ahead simulate possible complete continuations conversation find technique dramatically improve performance code dataset publicly available https githubcom facebookresearch end end negotiator
elections unleash strong political view twitter people really think politics opinion trend mine micro blog deal politics recently attract researchers several field include information retrieval machine learn ml since performance ml natural language process nlp approach limit amount quality data available one promise alternative task automatic propagation expert annotations paper intend develop call active learn process automatically annotate french language tweet deal image ie representation web reputation politicians main focus methodology follow build original annotate dataset express opinion two french politicians time therefore review state art nlp base ml algorithms automatically annotate tweet use manual initiation step bootstrap paper focus key issue active learn build large annotate data set noise introduce human annotators abundance data label distribution across data entities turn show twitter characteristics author name hashtags consider bear point improve automatic systems opinion mine om topic classification also reduce noise human annotations however later thorough analysis show reduce noise might induce loss crucial information
reveal adverse drug reactions adr essential part post market drug surveillance data health relate forums medical communities great significance estimate effect paper propose end end cnn base method predict drug safety user comment healthcare discussion forums present architecture base vast ensemble cnns vary structural parameters prediction determine majority vote evaluate performance propose solution present large scale dataset collect medical website consist fifty thousand review four thousand drug result demonstrate model significantly outperform conventional approach predict medicine safety accuracy eight thousand, seven hundred and seventeen binary six thousand, two hundred and eighty-eight multi classification task
paper present neural phrase base machine translation npmt method explicitly model phrase structure output sequence use sleep wake network swan recently propose segmentation base sequence model method mitigate monotonic alignment requirement swan introduce new layer perform soft local reorder input sequence different exist neural machine translation nmt approach npmt use attention base decode mechanisms instead directly output phrase sequential order decode linear time experiment show npmt achieve superior performances iwslt two thousand and fourteen german english english german iwslt two thousand and fifteen english vietnamese machine translation task compare strong nmt baselines also observe method produce meaningful phrase output languages
result recent neuroimaging study speak sentence comprehension interpret evidence cortical entrainment hierarchical syntactic structure present simple computational model predict power spectra study even though model linguistic knowledge restrict lexical level word level representations combine higher level units phrase sentence hence cortical entrainment result also explain lexical properties stimuli without recourse hierarchical syntax
master thesis describe algorithm automate categorization scientific document use deep learn techniques compare result result exist classification algorithms additional goal reusable api develop allow automation classification task exist software design propose use convolutional neural network classifier integrate rest base api use basis actual proof concept implementation present well thesis show deep learn classifier provide good result context multi class document categorization feasible integrate classifiers larger ecosystem use rest base service
human conversation inherently complex often span many different topics domains make policy learn dialogue systems challenge standard flat reinforcement learn methods provide efficient framework model dialogues paper focus explore problem multi domain dialogue management first propose new method hierarchical reinforcement learn use option framework next show propose architecture learn faster arrive better policy exist flat ones moreover show pretrained policies adapt complex systems additional set new action show approach potential facilitate policy optimisation sophisticate multi domain dialogue systems
paper propose k nrm kernel base neural model document rank give query set document k nrm use translation matrix model word level similarities via word embeddings new kernel pool technique use kernels extract multi level soft match feature learn rank layer combine feature final rank score whole model train end end rank layer learn desire feature pattern pairwise rank loss kernels transfer feature pattern soft match target similarity level enforce translation matrix word embeddings tune accordingly produce desire soft match experiment commercial search engine query log demonstrate improvements k nrm prior feature base neural base state art explain source k nrm advantage kernel guide embed encode similarity metric tailor match query word document word provide effective multi level soft match
propose neural multi document summarization mds system incorporate sentence relation graph employ graph convolutional network gcn relation graph sentence embeddings obtain recurrent neural network input node feature multiple layer wise propagation gcn generate high level hide sentence feature salience estimation use greedy heuristic extract salient sentence avoid redundancy experiment duc two thousand and four consider three type sentence relation graph demonstrate advantage combine sentence relations graph representation power deep neural network model improve upon traditional graph base extractive approach vanilla gru sequence model graph achieve competitive result state art multi document summarization systems
natural language generation nlg important component speak dialogue systems paper present model call encoder aggregator decoder extension recurrent neural network base encoder decoder architecture propose semantic aggregator consist two components aligner refiner aligner conventional attention calculate encode input information refiner another attention gate mechanism stack attentive aligner order select aggregate semantic elements propose model jointly train sentence plan surface realization produce natural language utterances model extensively assess four different nlg domains experimental result show propose generator consistently outperform previous methods nlg domains
conversations non player character npcs game typically confine dialogue human player virtual agent conversation initiate control player create richer believable environments players need conversational behavior reflect initiative part npcs include conversations include multiple npcs interact one another well player describe generative computational model group conversation agents abstract simulation discussion small group set define conversational interactions term rule turn take interruption well belief change sentiment change emotional response dependent agent personality context relationships evaluate model use parameterized expressive range analysis observe correlations simulation parameters feature result conversations analysis confirm example character personalities predict often speak heterogeneous group character generate belief change
information form basis human behavior include ubiquitous decision make people constantly perform every day live thus mission researchers understand humans process information reach decisions order facilitate task work propose novel method study reception granular expressions natural language approach utilize lasso regularization statistical tool extract decisive word textual content draw statistical inferences base correspondence occurrences word exogenous response variable accordingly method immediately suggest significant implications social sciences information systems research everyone identify text segment word choices statistically relevant author readers base knowledge test hypotheses behavioral research demonstrate contribution method examine author communicate subjective information narrative materials allow us answer question word choose communicate negative information hand show investors trade upon facts financial disclosures distract filler word non informative language practitioners example field investor communications market exploit insights enhance write base true perception word choice
introduce relnet new model relational reason relnet memory augment neural network model entities abstract memory slot equip additional relational memory model relations memory pair model thus build abstract knowledge graph entities relations present document use answer question document train end end supervision model form correct answer question test model twenty babi question answer task 10k examples per task find solve task mean error three achieve zero error eleven twenty task
main goal model human conversation create agents interact people open end goal orient scenarios end end train neural dialog systems important line research generalize dialog model resort situation specific handcraft rule however incorporate personalization systems largely unexplored topic exist corpora facilitate work paper present new dataset goal orient dialogs influence speaker profile attach analyze shortcomings exist end end dialog system base memory network propose modifications architecture enable personalization also investigate personalization dialog multi task learn problem show single model share feature among various profile outperform separate model profile
end end learn treat entire system whole adaptable black box sufficient data available may learn system work well target task principle recently apply several prototype research speaker verification sv feature learn classifier learn together objective function consistent evaluation metric opposite approach end end feature learn firstly train feature learn model construct back end classifier separately perform sv recently approach achieve significant performance gain sv mainly attribute smart utilization deep neural network however two approach carefully compare respective advantage well discuss paper compare end end feature learn approach text independent sv task experiment dataset sample fisher database involve five thousand speakers demonstrate feature learn approach outperform end end approach strong support feature learn approach least data computation resources similar
paper propose speaker recognition sre task trivial speech events cough laugh trivial events ubiquitous conversations less subject intentional change therefore offer valuable particularities discover genuine speaker disguise speech however trivial events often short idiocratic spectral pattern make sre extremely difficult fortunately find powerful deep feature learn structure extract highly speaker sensitive feature employ tool study sre performance three type trivial events cough laugh wei short chinese hello result show rich speaker information within trivial events even cough intuitively less speaker distinguishable deep feature approach ever reach ten fourteen three trivial events despite extremely short durations two ten second
exist speaker verification sv systems often suffer performance degradation language mismatch model train speaker enrollment test major degradation exist sv methods rely probabilistic model infer speaker factor significant change distribution speech signal impact inference recently propose deep learn model learn extract speaker factor deep neural network dnn feature learn sv system construct simple back end model paper investigate robustness feature base sv system situations language mismatch experiment conduct complex cross lingual scenario model train english enrollment test chinese uyghur experiment demonstrate feature base system outperform vector system large margin particularly language mismatch enrollment test
work problem associate imbalanced text corpora address method convert imbalanced text corpus balance one present present method employ cluster algorithm conversion initially avoid curse dimensionality effective representation scheme base term class relevancy measure adapt drastically reduce dimension number class corpus subsequently sample larger size class group number subclasses smaller size make entire corpus balance subclass give single symbolic vector representation use interval value feature symbolic representation addition compact help reduce space requirement also classification time propose model empirically demonstrate superiority bench mark datasets viz reuters twenty-one thousand, five hundred and seventy-eight tdt2 compare several exist contemporary model include model base support vector machine comparative analysis indicate propose model outperform exist model
word embeddings represent word point vector space become ubiquitous several nlp task recent line work use bilingual two languages corpora learn different vector sense word exploit crosslingual signal aid sense identification present multi view bayesian non parametric algorithm improve multi sense word embeddings use multilingual ie two languages corpora significantly improve sense embeddings beyond one achieve bilingual information b use principled approach learn variable number sense per word data drive manner first approach ability leverage multilingual corpora efficiently multi sense representation learn experiment show multilingual train significantly improve performance monolingual bilingual train allow us combine different parallel corpora leverage multilingual context multilingual train yield comparable performance state art mono lingual model train five time train data
generative encoder decoder model offer great promise develop domain general dialog systems however mainly apply open domain conversations paper present practical novel framework build task orient dialog systems base encoder decoder model framework enable encoder decoder model accomplish slot value independent decision make interact external databases moreover paper show flexibility propose method interleave chat capability slot fill system better domain recovery model train real user data bus information system human human chat data result show propose framework achieve good performance offline evaluation metrics task success rate human users
investigate association musical chord lyric analyze large dataset user contribute guitar tablatures motivate idea emotional content chord reflect word use correspond lyric analyze associations lyric chord categories also examine usage pattern chord lyric different musical genres historical eras geographical regions overall result confirm previously know association major chord positive valence also report wide variation association across regions genres eras result suggest possible existence different emotional associations type chord
recent neural ir model demonstrate deep learn utility ad hoc information retrieval however deep model reputation black box roles neural ir model components may obvious first glance work attempt would light inner work recently propose neural ir model namely pacrr model visualize output intermediate layer investigate relationship intermediate weight ultimate relevance score produce highlight several insights hop insights generally applicable
high accuracy speech recognition especially challenge large datasets available possible bridge gap careful knowledge drive parse combine biologically inspire cnn learn guarantee vapnik chervonenkis vc theory work present shallow cnn htsvm hierarchical tree support vector machine classifier architecture use predefined knowledge base set rule statistical machine learn techniques show gross errors present even state art systems avoid accurate acoustic model build hierarchical fashion cnn htsvm acoustic model outperform traditional gmm hmm model htsvm structure outperform mlp multi class classifier importantly isolate performance acoustic model provide result frame phoneme level consider true robustness model show even small amount data accurate robust recognition rat obtain
neural ir model drmm pacrr achieve strong result successfully capture relevance match signal argue context match signal also important intuitively extract model combine match signal one would like consider surround text local context well signal document contribute overall relevance score work highlight three potential shortcomings cause consider context information propose three neural ingredients address disambiguation component cascade k max pool shuffle combination layer incorporate components pacrr model yield co pacrr novel context aware neural ir model extensive comparisons establish model trec web track data confirm propose model achieve superior search result addition ablation analysis conduct gain insights impact interactions different components release code enable future comparisons
show small shallow fee forward neural network achieve near state art result range unstructured structure language process task considerably cheaper memory computational requirements deep recurrent model motivate resource constrain environments like mobile phone showcase simple techniques obtain small neural network model investigate different tradeoffs decide allocate small memory budget
entity population task collect entities belong particular category attract attention vertical domains still high demand create entity dictionaries vertical domains cover exist knowledge base develop lightweight front end tool facilitate interactive entity population implement key components necessary effective interactive entity population one gui base dashboards quickly modify entity dictionary two entity highlight document quickly view current progress aim reduce user cost begin end include package installation maintenance implementation enable users use tool web browsers without additional package users focus missions create entity dictionaries moreover entity expansion module implement external apis design make easy continuously improve interactive entity population pipelines make demo publicly available http bitly luwak demo
recent work learn ontologies hierarchical partially order structure leverage intrinsic geometry space learn representations make predictions automatically obey complex structural constraints explore two extensions one model order embed model hierarchical relation learn aim towards improve performance text data commonsense knowledge representation first model jointly learn order relations non hierarchical knowledge form raw text second extension exploit partial order structure train data find long distance triplet constraints among embeddings poorly enforce pairwise train procedure find incorporate free text augment train constraints improve original order embed model strong baselines
propose new framework abstractive text summarization base sequence sequence orient encoder decoder model equip deep recurrent generative decoder drgn latent structure information imply target summaries learn base recurrent latent random model improve summarization quality neural variational inference employ address intractable posterior inference recurrent latent variables abstractive summaries generate base generative latent variables discriminative deterministic state extensive experiment benchmark datasets different languages show drgn achieve improvements state art methods
paper first attempt learn policy inquiry dialog system ids use deep reinforcement learn drl ids frameworks represent dialog state dialog act logical formulae order make learn inquiry dialog policies effective introduce logical formula embed framework base recursive neural network result experiment evaluate effect one drl two logical formula embed framework show combination two effective even better exist rule base methods inquiry dialog policies
understand long document require track entities introduce evolve time present new type language model entitynlm explicitly model entities dynamically update representations contextually generate mention model generative flexible model arbitrary number entities context generate entity mention arbitrary length addition use several different task language model coreference resolution entity prediction experimental result task demonstrate model consistently outperform strong baselines prior work
recurrent neural network rnns serve fundamental build block many sequence task across natural language process recent research focus recurrent dropout techniques custom rnn cells order improve performance require substantial modifications machine learn model underlie rnn configurations revisit traditional regularization techniques specifically l2 regularization rnn activations slowness regularization successive hide state improve performance rnns task language model techniques require minimal modification exist rnn architectures result performance improvements comparable superior complicate regularization techniques custom cell architectures regularization techniques use without modification optimize lstm implementations nvidia cudnn lstm
investigate problem reader aware multi document summarization ra mds introduce new dataset problem tackle ra mds extend variational auto encode vaes base mds framework jointly consider news document reader comment conduct evaluation summarization performance prepare new dataset describe methods data collection aspect annotation summary write well scrutinize experts experimental result show reader comment improve summarization performance also demonstrate usefulness propose dataset annotate dataset ra mds available online
paper propose new task memexqa give collection photos videos user goal automatically answer question help users recover memory events capture collection towards solve task one present memexqa dataset large realistic multimodal dataset consist real personal photos crowd source question answer two propose memexnet unify end end trainable network architecture image text video question answer experimental result memexqa dataset demonstrate memexnet outperform strong baselines yield state art novel challenge task promise result textqa videoqa suggest memexnet efficacy scalability across various qa task
popularity social media platforms continue rise ever increase amount human communication self expression take place online recent research focus mine social media public user opinion external entities product review sentiment towards political news however less attention pay analyze users internalize thoughts emotions mental health perspective paper quantify semantic difference public tweet private mental health journals use online cognitive behavioral therapy use deep transfer learn techniques analyze semantic gap two domains show task emotional valence prediction social media successfully harness create accurate robust personalize mental health model result suggest semantic gap public private self expression small utilize abundance available social media one way overcome small sample size mental health data commonly limit availability privacy concern
reason crucial part natural language argumentation comprehend argument one must analyze warrant explain claim follow premise arguments highly contextualized warrant usually presuppose leave implicit thus comprehension require language understand logic skills also depend common sense paper develop methodology reconstruct warrant systematically operationalize scalable crowdsourcing process result freely license dataset warrant 2k authentic arguments news comment basis present new challenge task argument reason comprehension task give argument claim premise goal choose correct implicit warrant two options warrant plausible lexically close lead contradict claim solution task define substantial step towards automatic warrant reconstruction however experiment several neural attention language model reveal current approach suffice
present lipread system ie speech recognition system use visual feature use domain adversarial train speaker independence domain adversarial train integrate optimization lipreader base stack feedforward lstm long short term memory recurrent neural network yield end end trainable system require small number frame untranscribed target data substantially improve recognition accuracy target speaker pair different source target speakers achieve relative accuracy improvement around forty fifteen twenty second untranscribed target speech data multi speaker train setups accuracy improvements smaller still substantial
news archive invaluable primary source place current events historical context current search engine tool poor job uncover broad theme narratives across document present rookie practical software system use natural language process nlp help readers reporters editors uncover broad stories news archive unlike prior work rookie design emerge eighteen months iterative development consultation editors computational journalists process lead dramatically different approach previous academic systems similar goals efforts offer generalizable case study others build real world journalism software use nlp
prevalence video share increase demand automatic video digestion highlight detection recently platforms crowdsourced time sync video comment emerge worldwide provide good opportunity highlight detection however task non trivial one time sync comment often lag behind correspond shoot two time sync comment semantically sparse noisy three determine shots highlight highly subjective present paper aim tackle challenge propose framework one use concept map lexical chain lag calibration two model video highlight base comment intensity combination emotion concept concentration shoot three summarize detect highlight use improve sumbasic emotion concept map experiment large real world datasets show highlight detection method summarization method outperform benchmarks considerable margins
article offer empirical study different ways encode chinese japanese korean cjk english languages text classification different encode level study include utf eight bytes character word romanize character romanize word encode level whenever applicable provide comparisons linear model fasttext convolutional network convolutional network compare encode mechanisms use character glyph image one hot one n encode embed total four hundred and seventy-three model use fourteen large scale text classification datasets four languages include chinese english japanese korean conclusions result include byte level one hot encode base utf eight consistently produce competitive result convolutional network word level n grams linear model competitive even without perfect word segmentation fasttext provide best result use character level n gram encode overfit feature overly rich
propose neural vector space model nvsm method learn representations document unsupervised manner news article retrieval nvsm paradigm learn low dimensional representations word document scratch use gradient descent rank document accord similarity query representations compose word representations show nvsm perform better document rank exist latent semantic vector space methods addition nvsm mixture lexical language model state art baseline vector space model yield statistically significant increase retrieval effectiveness consequently nvsm add complementary relevance signal next semantic match find nvsm perform well case lexical match need nvsm learn notion term specificity directly document collection without feature engineer also show nvsm learn regularities relate luhn significance finally give advice deploy nvsm situations model selection eg cross validation infeasible find unsupervised ensemble multiple model train different hyperparameter value perform better single cross validate model therefore nvsm safely use rank document without supervise relevance judgments
paper present state art model visual question answer vqa first place two thousand and seventeen vqa challenge vqa task significant importance research artificial intelligence give multimodal nature clear evaluation protocol potential real world applications performance deep neural network vqa dependent choices architectures hyperparameters help research area describe detail high perform though relatively simple model massive exploration architectures hyperparameters represent three thousand gpu hours identify tip trick lead success namely sigmoid output soft train target image feature bottom attention gate tanh activations output embeddings initialize use glove google image large mini batch smart shuffle train data provide detail analysis impact performance assist others make appropriate selection
since tweet limit one hundred and forty character ambiguous difficult traditional natural language process nlp tool analyse research present keyxtract enhance machine learn base stanford corenlp part speech pos tagger twitter model extract essential keywords tweet system develop use rule base parsers two corpora data research obtain twitter profile telecommunication company system development consist two stag initial stage domain specific corpus compile analyse tweet pos tagger extract noun phrase verb phrase parsers remove noise extract keywords miss pos tagger system evaluate use turing test test compare stanford corenlp second stage system develop address shortcomings first stage enhance use name entity recognition lemmatization second stage also test use turing test pass rate increase five thousand eight thousand, three hundred and thirty-three performance final system output measure use f1 score stanford corenlp twitter model average f1 sixty-nine improve system f1 seventy-seven accuracy system could improve use complete domain specific corpus since system use linguistic feature sentence could apply nlp tool
recent years supervise representation learn provide state art close state art result semantic analysis task include rank information retrieval core idea learn embed items latent space optimize supervise objective latent space dimension latent space clear semantics reduce interpretability system example personalization model hard explain particular item rank high give user profile propose novel model representation learn call supervise explicit semantic analysis sesa train supervise fashion embed items set dimension explicit semantics model learn compare two object represent explicit space dimension correspond concept knowledge base work extend explicit semantic analysis esa supervise model rank problems apply model task job profile relevance linkedin set skills define explicit dimension space every profile job encode set skills similarity calculate space use rnns embed text input space addition interpretability model make use web scale collaborative skills data provide users linkedin profile model provide state art result remain interpretable
many word cloud provide semantics word placement use random layout optimize solely aesthetic purpose propose novel approach model word significance word affinity within document comparison large background corpus demonstrate usefulness generate meaningful word cloud visual summary give document select keywords base significance construct word cloud base derive affinity base modify distribute stochastic neighbor embed sne generate semantic word placement word cooccur significantly include edge cluster word accord cooccurrence design scalable memory efficient sketch base approach usable commodity hardware aggregate require corpus statistics need normalization identify keywords well significant cooccurences empirically validate approch use large wikipedia corpus
present emotxt toolkit emotion recognition text train test gold standard 9k question answer comment online interactions provide empirical evidence performance emotxt best knowledge emotxt first open source toolkit support emotion recognition text train custom emotion classification model
word embed low dimensional dense real value vector representation word word embeddings use many nlp task usually gener ated large text corpus embed word cap tures syntactic semantic aspects tweet short noisy unique lexical semantic feature different type text therefore necessary word embeddings learn specifically tweet paper present ten word embed data set addition data set learn tweet data also build embed set general data combination tweet general data general data consist news article wikipedia data web data ten embed model learn four hundred million tweet seven billion word general text paper also present two experiment demonstrate use data set nlp task tweet sentiment analysis tweet topic classification task
information distribution electronic message privilege mean transmission many businesses individuals often form plain text table number grow become necessary use algorithm extract text number instead human usual methods focus regular expressions strict structure data efficient many variations fuzzy structure implicit label paper introduce sc2t totally self supervise model construct vector representations tokens semi structure message use character context level address issue use unsupervised label tokens basis semi supervise information extraction system
dialog natural modality interaction customers businesses service industry customers call service provider interactions may routine extraordinary believe interactions see dialogs analyze obtain better understand customer need efficiently address introduce idea dialog complexity measure characterize multi party interactions propose general data drive method calculate use discover insights public enterprise dialog datasets demonstrate beneficial usage facilitate better handle customer request evaluate service agents
preventable medical errors estimate among lead cause injury death unite state prevent errors healthcare systems implement patient safety incident report systems systems enable clinicians report unsafe condition case patients harm due errors medical care report narratives natural language provide detail information situation non trivial perform large scale analysis identify common cause errors harm patients work present method base attentive convolutional recurrent network identify harm events patient care categorize harm base severity level demonstrate methods significantly improve performance exist methods identify harm clinical care
cross modal data retrieval basis various creative task perform artificial intelligence ai one highly challenge task ai convert book correspond movie creative film makers today research take first step towards visualize content book use correspond movie visuals give set sentence book even fan fiction write universe employ deep learn model visualize input stitch together relevant frame movie study compare three different type set match book movie content dialog model use dialog movie ii visual model use visual content movie iii hybrid model use dialog visual content movie experiment publicly available moviebook dataset show effectiveness propose model
mobile app distribution platforms google play store allow users share feedback download apps form review comment correspond star rat typically star rat range one five star one star denote high sense dissatisfaction app five star denote high sense satisfaction unfortunately due variety reason often star rat provide user inconsistent opinion express review example consider follow review facebook app android awesome app one would reasonably expect rat review five star actual rat one star inconsistent rat lead deflate inflate overall average rat app affect user download typically users look average star rat make decision download app also app developers receive bias feedback application represent grind reality especially significant small apps thousand download even small number mismatch review bring average rat drastically paper conduct study review rat mismatch problem manually examine eight thousand, six hundred review ten popular android apps find twenty rat dataset inconsistent review develop three systems two base traditional machine learn one deep learn automatically identify review whose rat match opinion express review deep learn system perform best accuracy ninety-two identify correct star rat associate give review
paper propose use deep three dimensional convolutional network 3d cnns order address challenge model spectro temporal dynamics speech emotion recognition ser compare hybrid convolutional neural network long short term memory cnn lstm propose 3d cnns simultaneously extract short term long term spectral feature moderate number parameters evaluate propose state art methods speaker independent manner use aggregate corpora give large diverse set speakers find one shallow temporal moderately deep spectral kernels homogeneous architecture optimal task two 3d cnns effective spectro temporal feature learn compare methods finally visualise feature space obtain propose method use distribute stochastic neighbour embed sne could observe distinct cluster emotions
image caption often require large set train image sentence pair practice however acquire sufficient train pair always expensive make recent caption model limit ability describe object outside train corpora ie novel object paper present long short term memory copy mechanism lstm c new architecture incorporate copy convolutional neural network cnn plus recurrent neural network rnn image caption framework describe novel object caption specifically freely available object recognition datasets leverage develop classifiers novel object lstm c nicely integrate standard word word sentence generation decoder rnn copy mechanism may instead select word novel object proper place output sentence extensive experiment conduct mscoco image caption imagenet datasets demonstrate ability propose lstm c architecture describe novel object furthermore superior result report compare state art deep model
surprisingly little know agenda set international development unite nations un despite significant influence process outcomes development efforts paper address shortcoming use novel approach apply natural language process techniques countries annual statements un general debate every year un member state deliver statements general debate governments perspective major issue world politics speeches provide invaluable information state preferences wide range issue include international development largely overlook study global politics paper identify main international development topics state raise speeches one thousand, nine hundred and seventy two thousand and sixteen examine country specific drivers international development rhetoric
train large vocabulary neural network language model nnlms difficult task due explicit requirement output layer normalization typically involve evaluation full softmax function complete vocabulary paper propose batch noise contrastive estimation b nce approach alleviate problem achieve reduce vocabulary time step target word batch replace softmax noise contrastive estimation approach word play role target noise sample time propose approach fully formulate implement use optimal dense matrix operations apply b nce train different nnlms large text compression benchmark ltcb one billion word benchmark obwb show significant reduction train time noticeable degradation model performance paper also present new baseline comparative study different standard nnlms large obwb single titan x gpu
word embeddings find capture surprisingly rich amount syntactic semantic knowledge however yet sufficiently well understand relational knowledge implicitly encode word embeddings extract reliable way paper propose two probabilistic model address issue first model base common relations translations view cast probabilistic set second model base much weaker assumption linear relationship vector representations relate word compare exist approach model lead accurate predictions explicit extract word embed
paper present exploratory find relate extract knowledge experience community senior tourists use tool qualitative analysis well review literature manage verify set hypotheses relate content create senior tourists participate line communities also produce codebook represent various theme one may encounter communities codebook derive qualitative research well literature review serve basis development automate tool knowledge extraction also manage find older adults often poster tourists forums mention age discussion often share experience motivation travel however differ relation describe barriers encounter travel
goal language model techniques capture statistical structural properties natural languages train corpora task typically involve learn short range dependencies generally model syntactic properties language long range dependencies semantic nature propose paper new multi span architecture separately model short long context information dynamically merge perform language model task do novel recurrent long short range context lsrc network explicitly model local short global long context use two separate hide state evolve time new architecture adaptation long short term memory network lstm take account linguistic properties extensive experiment conduct penn treebank ptb large text compression benchmark ltcb corpus show significant reduction perplexity compare state art language model techniques
performance neural network nn base language model steadily improve due emergence new architectures able learn different natural language characteristics paper present novel framework show significant improvement achieve combine different exist heterogeneous model single architecture do one feature layer separately learn different nn base model two mixture layer merge result model feature architecture benefit learn capabilities model noticeable increase number model parameters train time extensive experiment conduct penn treebank ptb large text compression benchmark ltcb corpus show significant reduction perplexity compare state art feedforward well recurrent neural network architectures
exhaustive study neural network language model nnlm perform paper different architectures basic neural network language model describe examine number different improvements basic neural network language model include importance sample word class cache bidirectional recurrent neural network birnn study separately advantage disadvantage every technique evaluate limit neural network language model explore aspects model architecture knowledge representation part statistical information word sequence loss process word word certain order mechanism train neural network update weight matrixes vectors impose severe restrictions significant enhancement nnlm knowledge representation knowledge represent neural network language model approximate probabilistic distribution word sequence certain train data set rather knowledge language information convey word sequence natural language finally directions improve neural network language model discuss
text network analysis receive increase attention consequence wide range applications work extend previous work found study topological feature mesoscopic network geometrical properties visualize network quantify term several image analysis techniques use subsidies authorship attribution find visual feature account performance similar achieve use topological measurements addition combination two type feature improve performance
last years link data cloud achieve size one hundred billion facts pertain multitude domains however access information significantly challenge lay users approach problems question answer link data link discovery notably play role increase information access approach often base handcraft statistical model derive data observation recently deep learn architectures base neural network call seq2seq show achieve state art result translate sequence sequence direction propose neural sparql machine end end deep architectures translate natural language expression sentence encode sparql query preliminary result restrict select dbpedia class show neural sparql machine promise approach question answer link data deal know problems vocabulary mismatch perform graph pattern composition
recent years many deep learn base model propose text classification kind model well fit train set statistical point view however lack capacity utilize instance level information individual instance train set work propose enhance neural network model allow leverage information k nearest neighbor knn input text model employ neural network encode texts text embeddings moreover also utilize k nearest neighbor input text external memory utilize capture instance level information train set final prediction make base feature neural network encoder knn memory experimental result several standard benchmark datasets show model outperform baseline model datasets even beat deep neural network model twenty-nine layer several datasets model also show superior performance train instance scarce train set severely unbalance model also leverage techniques semi supervise train transfer learn quite well
nationality identification unlock important demographic information many applications biomedical sociological research exist name base nationality classifiers use name substrings feature train small unrepresentative set label name typically extract wikipedia result methods achieve limit performance support fine grain classification exploit phenomena homophily communication pattern learn name embeddings new representation encode gender ethnicity nationality readily applicable build classifiers systems analysis 57m contact list major internet company able design fine grain nationality classifier cover thirty-nine group represent ninety world population evaluation publish systems thirteen common class f1 score seven hundred and ninety-five substantial better closest competitor ethnea five hundred and eighty best knowledge accurate fine grain nationality classifier available social media application apply classifiers followers major twitter celebrities six different domains demonstrate stark differences ethnicities followers trump obama sport entertainments favor different group finally identify anomalous political figure whose presumably inflate follow appear largely incapable read language post
social media datasets especially twitter tweet popular field text classification tweet valuable source micro text sometimes refer micro blog study domains sentiment analysis recommendation systems spam detection cluster among others tweet often include keywords refer hashtags use label tweet use tweet encompass fifty label study impact word versus character level feature selection extraction different learners solve multi class classification task show feature extraction simple character level group perform better simple word group pre process methods like normalize use porter stem part speech pos lemmatization
much user generate content social media provide ordinary people tell stories daily live develop test novel method learn fine grain common sense knowledge stories contingent causal conditional relationships everyday events type knowledge useful text story understand information extraction question answer text summarization test compare different methods learn contingency relation compare learn topic sort story collections vs general domain stories experiment show use topic specific datasets enable learn finer grain knowledge events result significant improvement baselines evaluation amazon mechanical turk show eighty-two relations events learn topic sort stories judge contingent
human understand narrative mainly drive reason causal relations events thus recognize key capability computational model language understand computational work area approach via two different rout focus acquire knowledge base common causal relations events attempt understand particular story macro event along storyline position paper focus knowledge acquisition approach claim newswire relatively poor source learn fine grain causal relations everyday events describe experiment use unsupervised method learn causal relations events narrative genres first person narratives film scene descriptions show method learn fine grain causal relations judge humans likely causal eighty time also demonstrate learn event pair exist publicly available event pair datasets extract newswire
commit message valuable resource comprehension software evolution since provide record change feature additions bug repair unfortunately programmers often neglect write good commit message different techniques propose help programmers automatically write message techniques effective describe change often verbose lack context understand rationale behind change contrast humans write message short summarize high level rationale paper adapt neural machine translation nmt automatically translate diffs commit message train nmt algorithm use corpus diffs human write commit message top 1k github project design filter help ensure train algorithm higher quality commit message evaluation uncover pattern message generate tend either high low quality therefore create quality assurance filter detect case unable produce good message return warn instead
generate video descriptions natural language aka video caption challenge task image caption videos intrinsically complicate image two aspects first videos cover broader range topics news music sport second multiple topics could coexist video paper propose novel caption model topic guide model tgm generate topic orient descriptions videos wild via exploit topic information addition predefined topics ie category tag crawl web also mine topics data drive way base train caption unsupervised topic mine model show data drive topics reflect better topic schema predefined topics test video topic prediction treat topic mine model teacher train student topic prediction model utilize full multi modalities video especially speech modality propose series caption model exploit topic guidance include implicitly use topics input feature generate word relate topic explicitly modify weight decoder topics function ensemble topic aware language decoders comprehensive experimental result current largest video caption dataset msr vtt prove effectiveness topic guide model significantly surpass win performance two thousand and sixteen msr video language challenge
topic diversity open domain videos lead various vocabularies linguistic expressions describe video content therefore make video caption task even challenge paper propose unify caption framework mandm tgm mine multimodal topics unsupervised fashion data guide caption decoder topics compare pre define topics mine multimodal topics semantically visually coherent reflect topic distribution videos better formulate topic aware caption generation multi task learn problem add parallel task topic prediction addition caption task topic prediction task use mine topics teacher train student topic prediction model learn predict latent topics multimodal content videos topic prediction provide intermediate supervision learn process caption task propose novel topic aware decoder generate accurate detail video descriptions guidance latent topics entire learn procedure end end optimize task simultaneously result extensive experiment conduct msr vtt youtube2text datasets demonstrate effectiveness propose model mandm tgm outperform prior state art methods multiple evaluation metrics benchmark datasets also achieve better generalization ability
task generate effective summary give document specific realtime requirements use softplus function enhance keyword rank favor important sentence base present number summarization algorithms use various keyword extraction topic cluster methods show algorithms meet realtime requirements yield best rouge recall score duc two previously know algorithms show algorithms meet realtime requirements yield best rouge recall score duc two previously know algorithms evaluate quality summaries without human generate benchmarks define measure call wesm base word embed use word mover distance show order rouge wesm score algorithms highly comparable suggest wesm may serve viable alternative measure quality summary
study automatic title generation give block text present method call dtatg generate title dtatg first extract small number central sentence convey main mean text suitable structure conversion title dtatg construct dependency tree sentence remove certain branch use dependency tree compression model devise also devise title test determine sentence use title trim sentence pass title test become title candidate dtatg select title candidate highest rank score final title experiment show dtatg generate adequate title also show dtatg generate title higher f1 score generate previous methods
matrix syntax formal model syntactic relations language purpose paper explain mathematical foundations audience formal background make axiomatic presentation motivate axiom linguistic practical ground result mathematical structure resemble aspects quantum mechanics matrix syntax allow us describe number language phenomena otherwise difficult explain linguistic chain arguably economical theory language theories propose context minimalist program linguistics particular sentence naturally model vectors hilbert space tensor product structure build 2x2 matrices belong specific group
machine comprehension mc challenge task natural language process field aim guide machine comprehend passage answer give question many exist approach mc task suffer inefficiency bottleneck insufficient lexical understand complex question passage interaction incorrect answer extraction paper address problems viewpoint humans deal read test scientific way specifically first propose novel lexical gate mechanism dynamically combine word character representations guide machine read interactive way attention mechanism memory network finally add check layer refine answer insurance extensive experiment two popular datasets squad triviaqa show method exceed considerable performance state art solutions time submission
clickbait pejorative term describe web content aim generate online advertise revenue especially expense quality accuracy rely sensationalist headline eye catch thumbnail picture attract click throughs encourage forward material online social network use distribute word representations word title feature identify clickbaits online news media train machine learn model use linear regression predict cickbait score give tweet methods achieve f1 score six thousand, four hundred and ninety-eight mse seven hundred and ninety-one compare methods method simple fast train require extensive feature engineer yet moderately effective
work discuss relate challenge describe approach towards fusion state art technologies speak dialogue systems sds semantic web information retrieval domains envision dialogue system name ld sds support advance expressive engage user request multiple complex rich open domain data source leverage wealth available link data specifically focus improve identification disambiguation link entities occur data source user input b offer advance query service exploit semantics data reason exploratory capabilities c expand typical information seek dialogue model slot fill better reflect real world conversational search scenarios
address problem automatic american sign language fingerspell recognition video prior work largely rely frame level label hand craft feature constraints hamper scarcity data task introduce model fingerspell recognition address issue model consist auto encoder base feature extractor attention base neural encoder decoder train jointly model receive sequence image frame output fingerspell word without rely frame level train label hand craft feature addition auto encoder subcomponent make possible leverage unlabeled data improve feature learn model achieve one hundred and sixteen forty-four absolute letter accuracy improvement respectively signer independent signer adapt fingerspell recognition previous approach require frame level train label
paper propose novel end end neural architecture rank candidate answer adapt hierarchical recurrent neural network latent topic cluster module propose model text encode vector representation word level chunk level effectively capture entire mean particular adapt hierarchical structure model show small performance degradations longer text comprehension state art recurrent neural network model suffer additionally latent topic cluster module extract semantic information target sample cluster module useful text relate task allow data sample find nearest topic cluster thus help neural network model analyze entire data evaluate model ubuntu dialogue corpus consumer electronic domain question answer dataset relate samsung products propose model show state art result rank question answer pair
past years several data set release text image present approach create data set use detect remove gender bias text also include set challenge face create corpora work work movie data wikipedia plot movie trailers youtube bollywood movie corpus contain four thousand movies extract wikipedia eight hundred and eighty trailers extract youtube release one thousand, nine hundred and seventy two thousand and seventeen corpus contain csv file follow data movie wikipedia title movie cast plot text co reference plot text soundtrack information link movie poster caption movie poster number males poster number females poster addition correspond cast member follow data available cast name cast gender cast verbs cast adjectives cast relations cast centrality cast mention present preliminary result task bias removal suggest data set quite useful perform task
sentiment analysis aim uncover emotions convey information simplest form perform polarity basis goal classify information positive negative emotion recent research explore nuanced ways capture emotions go beyond polarity methods work require critical resource lexicon appropriate task hand term range emotions capture diversity past sentiment analysis lexicons create experts linguists behavioural scientists strict rule lexicon evaluation also perform experts gold standards paper propose crowdsourcing method lexicon acquisition scalable cost effective require experts gold standards also compare crowd expert evaluations lexicon assess overall lexicon quality evaluation capabilities crowd
learn effective representations sentence one core missions natural language understand exist model either train vast amount text require costly manually curated sentence relation datasets show dependency parse rule base rubric curate high quality sentence relation task leverage explicit discourse relations show curated dataset provide excellent signal learn vector representations sentence mean represent relations determine mean two sentence combine demonstrate automatically curated corpus allow bidirectional lstm sentence encoder yield high quality sentence embeddings serve supervise fine tune dataset larger model bert fix sentence embeddings achieve high performance variety transfer task include senteval achieve state art result penn discourse treebank implicit relation prediction task
comprehension speak natural language essential component robots communicate human effectively however handle unconstrained speak instructions challenge due one complex structure include wide variety expressions use speak language two inherent ambiguity interpretation human instructions paper propose first comprehensive system handle unconstrained speak language able effectively resolve ambiguity speak instructions specifically integrate deep learn base object detection together natural language process technologies handle unconstrained speak instructions propose method robots resolve instruction ambiguity dialogue experiment simulate environment well physical industrial robot arm demonstrate ability system understand natural instructions human operators effectively higher success rat object pick task achieve interactive clarification process
image wild encapsulate rich knowledge vary abstract concepts sufficiently describe model build use image caption pair contain select object propose handle task guidance knowledge base incorporate many abstract concepts method two step process first build multi entity label image recognition model predict abstract concepts image label leverage second step external semantic attention constrain inference caption generation model describe image depict unseen novel object evaluations show model outperform prior work domain caption mscoco useful integration knowledge vision general
read comprehension methods limit query answer use single sentence paragraph document enable model combine disjoint piece textual evidence would extend scope machine comprehension methods currently exist resources train test capability propose novel task encourage development model text understand across multiple document investigate limit exist methods task model learn seek combine evidence effectively perform multi hop alias multi step inference devise methodology produce datasets task give collection query answer pair thematically link document two datasets different domains induce identify potential pitfalls devise circumvention strategies evaluate two previously propose competitive model find one integrate information across document however model struggle select relevant information provide document guarantee relevant greatly improve performance model outperform several strong baselines best accuracy reach four hundred and twenty-nine compare human performance seven hundred and forty leave ample room improvement
machine translation systems date train large parallel corpora humans learn language different way ground environment interact humans work propose communication game two agents native speakers respective languages jointly learn solve visual referential task find ability understand translate foreign language emerge mean achieve share goals emergent translation interactive multimodal crucially require parallel corpora monolingual independent text correspond image propose translation model achieve ground source target languages share visual modality outperform several baselines word level sentence level translation task furthermore show agents multilingual community learn translate better faster bilingual communication set
speech base natural language question answer interfaces enterprise systems gain lot attention general purpose speech engines integrate nlp systems provide interfaces usually general purpose speech engines train large general corpus however engines use specific domains may recognize domain specific word well may produce erroneous output accent environmental condition speaker speak sentence may induce speech engine inaccurately recognize certain word subsequent natural language question answer produce requisite result question accurately represent speaker intend thus speech engine output may need adapt domain natural language process carry present two mechanisms adaptation one base evolutionary development base machine learn show repair speech output make subsequent natural language question answer better
present result second share task multimodal machine translation multilingual image description nine team submit nineteen systems two task multimodal translation task source sentence supplement image extend new language french two new test set multilingual image description task change test time image give compare last year multimodal systems improve text systems remain competitive
word rank frequency distributions long herald adherence potentially universal phenomenon know zipf law hypothetical form empirical phenomenon refine beniot mandelbrot presently refer zipf mandelbrot law parallel herbet simon propose selection model potentially explain zipf law however significant dispute simon mandelbrot notable empirical exceptions lack strong empirical connection simon model zipf mandelbrot law leave question universality mechanistic generation open offer resolution issue exhibit dark matter word segmentation ie space punctuation etc connect zipf mandelbrot law simon mechanistic process explain mandelbrot refinement fudge factor accommodate effect exclusion rank frequency dark matter thus integrate non word object resolve generalize rank frequency law since rely upon integration space etc find support hypothesis generate common process indicate physical perspective space word
recent advancements information technology huge surge amount data available information retrieval technology able keep pace information generation result spend time retrieve relevant information even though systems exist assist users search database along filter recommend relevant information recommendation system use content document recommendation still long way mature present deep learn base supervise approach recommend similar document base similarity content combine c dssm model word2vec distribute representations word create novel model classify document pair relevant irrelavant assign score use model retrieval document do of1 time memory complexity ofn n number document
health relate social media mine valuable apparatus early recognition diverse antagonistic medicinal condition mostly exist methods base machine learn knowledge base learn work note present recurrent neural network rnn long short term memory lstm base embed automatic health text classification social media mine task two systems build classify tweet tweet level rnn lstm use extract feature non linear activation function last layer facilitate distinguish tweet different categories experiment conduct 2nd social media mine health applications share task amia two thousand and seventeen experiment result considerable however propose method appropriate health text classification primarily due reason rely feature engineer mechanisms
emergence social media news source lead rise clickbait post attempt attract users click article link without inform actual article content paper present efforts create clickbait detector inspire fake news detection algorithms submission clickbait challenge two thousand and seventeen detector base almost exclusively text base feature take previous work clickbait detection work fake post detection feature design specifically challenge use two level classification approach combine output sixty-five first level classifiers second level feature vector present exploratory result individual feature combinations take post text target article title well feature selection blind test dataset lead f score sixty-three final evaluation challenge achieve f score forty-three explore possible cause lay potential future step achieve successful result
plethora diverse approach question answer rdf data develop recent years accuracy systems increase significantly time systems still focus particular type question particular challenge question answer curse single systems bless combination systems show paper machine learn techniques apply create accurate question answer metasystem reuse exist systems particular develop multi label classification base metasystem question answer six exist systems use innovative set fourteen question feature metasystem outperform best single system fourteen f measure recent qald six benchmark furthermore analyze influence correlation underlie feature metasystem quality
substantial amount research carry develop machine learn algorithms account term dependence text classification algorithms offer acceptable performance case associate substantial cost require significantly greater resources operate paper argue justification higher cost algorithms base performance text classification problems order prove conjecture performance one best dependence model compare several well establish algorithms text classification specific collection datasets design would best reflect disparity nature text data present real world applications result show even one best term dependence model perform decent best compare independence model couple substantially greater requirement hardware resources operation make impractical choice use real world scenarios
paper tackle sentiment analysis condition topic twitter data use deep learn propose two tier approach first phase create word embeddings see perform better state art embeddings use standard classifiers perform inference embeddings learn word respect topics consider also top n influence word topic second phase use embeddings predict sentiment tweet respect give topic topics discussion
trans dimensional random field language model trf lms sentence model collection random field show close performance lstm lms speech recognition computationally efficient inference however train efficiency neural trf lms satisfactory limit scalability trf lms large train corpus paper several techniques model formulation parameter estimation propose improve train efficiency performance neural trf lms first trfs reformulate form exponential tilt reference distribution second noise contrastive estimation nce introduce jointly estimate model parameters normalization constants third extend neural trf lms marry deep convolutional neural network cnn bidirectional lstm potential function extract deep hierarchical feature bidirectionally sequential feature utilize techniques enable successful efficient train neural trf lms 40x larger train set one three train time reduce wer relative reduction forty-seven top strong lstm lm baseline
recurrent neural network rnns successfully apply various natural language process nlp task achieve better result conventional methods however lack understand mechanisms behind effectiveness limit improvements architectures paper present visual analytics method understand compare rnn model nlp task propose technique explain function individual hide state units base expect response input texts co cluster hide state units word base expect response visualize co cluster result memory chip word cloud provide structure knowledge rnns hide state also propose glyph base sequence visualization base aggregate information analyze behavior rnn hide state sentence level usability effectiveness method demonstrate case study review domain experts
traditional methods summarization cost effective possible today extractive summarization process help extract important sentence text automatically generate short informative summary work propose unsupervised method summarize persian texts method novel hybrid approach cluster concepts text use deep learn traditional statistical methods first produce word embed base hamshahri2 corpus dictionary word frequencies propose algorithm extract keywords document cluster concepts finally rank sentence produce summary evaluate propose method pasokh single document corpus use rouge evaluation measure without use hand craft feature propose method achieve state art result compare unsupervised method best supervise persian methods achieve overall improvement rouge two recall score seventy-five
name entity recognition ner important subtask information extraction seek locate recognise name entities despite recent achievements still face limitations correctly detect classify entities prominently short noisy text twitter important negative aspect ner approach high dependency hand craft feature domain specific knowledge necessary achieve state art result thus devise model deal linguistically complex contexts still challenge paper propose novel multi level architecture rely specific linguistic resource encode rule unlike traditional approach use feature extract image text classify name entities experimental test state art ner twitter ritter dataset present competitive result fifty-nine f measure indicate approach may lead towards better ner model
automatic relation extraction type interest great importance interpret massive text corpora efficient manner traditional model heavily rely human annotate corpus train costly generate label data become obstacles deal relation type thus extraction systems shift build upon train data automatically acquire link knowledge base distant supervision however due incompleteness knowledge base context agnostic label train data collect via distant supervision ds noisy recent years increase attention bring tackle question answer qa task user feedback datasets task become accessible paper propose novel framework request leverage question answer pair indirect source supervision relation extraction study use supervision reduce noise induce ds model jointly embed relation mention type qa entity mention pair text feature two low dimensional space qa object relation type semantically similar question answer pair similar representations share feature connect two space carry clearer semantic knowledge source request use learn embeddings estimate type test relation mention formulate global objective function adopt novel margin base qa loss reduce noise ds exploit semantic evidence qa dataset experimental result achieve average eleven improvement f1 score two public datasets combine trec qa dataset
technical report present general framework parse variety grammar formalisms develop grammar formalism call abstract grammar general enough represent grammars many level hierarchy include context free grammars minimalist grammars generalize context free grammars develop single parse framework capable parse grammars least gcfgs hierarchy parse framework expose grammar interface parse particular grammar formalism reduce abstract grammar
short text cluster challenge problem due sparseness text representation propose flexible self teach convolutional neural network framework short text cluster dub stc2 flexibly successfully incorporate useful semantic feature learn non bias deep text representation unsupervised manner framework original raw text feature firstly embed compact binary cod use one exist unsupervised dimensionality reduction methods word embeddings explore feed convolutional neural network learn deep feature representations meanwhile output units use fit pre train binary cod train process finally get optimal cluster employ k mean cluster learn representations extensive experimental result demonstrate propose framework effective flexible outperform several popular cluster methods test three public short text datasets
new type end end system text dependent speaker verification present paper previously use phonetically discriminative speaker discriminative dnns feature extractors speaker verification show promise result extract frame level dnn bottleneck posterior vector feature equally weight aggregate compute utterance level speaker representation vector vector work use speaker discriminative cnns extract noise robust frame level feature feature smartly combine form utterance level speaker vector attention mechanism propose attention model take speaker discriminative information phonetic information learn weight whole system include cnn attention model joint optimize use end end criterion train algorithm imitate exactly evaluation process directly map test utterance target speaker utterances single verification score algorithm automatically select similar impostor target speaker train network demonstrate effectiveness propose end end system windows ten hey cortana speaker verification task
introduce pyndri python interface indri search engine pyndri allow access indri index python two level one dictionary tokenized document collection two evaluate query index hope release pyndri stimulate reproducible open fast pace ir research
settings unlabelled speech data available zero resource speech technology need develop without transcriptions pronunciation dictionaries language model text two central problems zero resource speech process find frame level feature representations make easier discriminate linguistic units phone word ii segment cluster unlabelled speech meaningful units thesis argue combination top bottom model advantageous tackle two problems address problem frame level representation learn present correspondence autoencoder cae neural network train weak top supervision unsupervised term discovery system combine top supervision unsupervised bottom initialization cae yield much discriminative feature previous approach present unsupervised segmental bayesian model segment cluster unlabelled speech hypothesize word impose consistent top segmentation also use bottom knowledge detect syllable boundaries system outperform several others multi speaker conversational english xitsonga speech data finally show cluster discover segmental bayesian model make less speaker gender specific use feature cae instead traditional acoustic feature summary different model systems present thesis show top bottom model improve representation learn segmentation cluster unlabelled speech data
among manifold take world literature goal contribute discussion digital point view analyze representation world literature wikipedia millions article hundreds languages preliminary introduce compare three different approach identify writers wikipedia use data dbpedia community project goal extract provide structure information wikipedia equip basic set writers analyze represent throughout fifteen biggest wikipedia language versions combine intrinsic measure mostly examine connectedness article extrinsic ones analyze often article frequent readers develop methods evaluate result better part find seem convey rather conservative old fashion version world literature version derive reproducible facts reveal implicit literary canon base edit read behavior millions people still solve know issue introduce methods help us build observatory world literature investigate representativeness bias
rank function use information retrieval primarily use search engines often adopt various language process applications however feature use construction rank function analyze apply data set paper give guidelines construction generalize rank function application dependent feature paper prescribe specific case generalize function recommendation system use feature engineer guidelines give data set behavior generalize specific function study implement unstructured textual data proximity feature base rank function outperform fifty-two regular bm25
introduce tree structure attention neural network sentence small phrase apply problem sentiment classification model expand current recursive model incorporate structural information around node syntactic tree use bottom top information propagation also model utilize structural attention identify salient representations construction syntactic tree knowledge propose model achieve state art performance stanford sentiment treebank dataset
entities essential elements natural language paper present methods learn multi level representations entities three complementary level character character pattern entity name extract eg neural network word embeddings word entity name entity entity embeddings investigate state art learn methods level find large differences eg deep learn model traditional ngram feature subword model fasttext bojanowski et al two thousand and sixteen character level word2vec mikolov et al two thousand and thirteen word level order aware model wang2vec ling et al 2015a entity level confirm experimentally level representation contribute complementary information joint representation three level improve exist embed base baseline fine grain entity type large margin additionally show add information entity descriptions improve multi level representations entities
cognitive compute systems require human label data evaluation often train standard practice use gather data minimize disagreement annotators find result data fail account ambiguity inherent language propose crowdtruth method collect grind truth crowdsourcing reconsider role people machine learn base observation disagreement annotators provide useful signal phenomena ambiguity text report use method build annotate data set medical relation extraction treat relations data perform supervise train experiment demonstrate model ambiguity label data gather crowd workers one reach level quality domain experts task reduce cost two provide better train data scale distant supervision propose validate new weight measure precision recall f measure account ambiguity human machine performance task
paper propose three novel model enhance word embed implicitly use morphological information experiment word similarity syntactic analogy show implicit model superior traditional explicit ones model outperform state art baselines significantly improve performance task moreover performance smallest corpus similar performance cbow corpus five time size parameter analysis indicate implicit model supplement semantic information word embed train process
introduce simple accurate neural model dependency base semantic role label model predict predicate argument dependencies rely state bidirectional lstm encoder semantic role labeler achieve competitive performance english even without kind syntactic information use local inference however automatically predict part speech tag provide input substantially outperform previous local model approach best report result english conll two thousand and nine dataset also consider chinese czech spanish approach also achieve competitive result syntactic parsers unreliable domain data standard ie syntactically inform srl model hinder test set syntax agnostic model appear robust result best report result standard domain test set
propose novel decode approach neural machine translation nmt base continuous optimisation convert decode basically discrete optimization problem continuous optimization problem result constrain continuous optimisation problem tackle use gradient base methods powerful decode framework enable decode intractable model intersection leave right right leave bidirectional well source target target source bilingual nmt model empirical result show decode framework effective lead substantial improvements translations generate intersect model typical greedy beam search feasible also compare framework reranking analyse advantage disadvantage
weight finite automata transducers include hide markov model conditional random field widely use natural language process nlp perform task morphological analysis part speech tag chunk name entity recognition speech recognition others parallelize finite state algorithms graphics process units gpus would benefit many areas nlp although researchers implement gpu versions basic graph algorithms limit previous work knowledge do gpu algorithms weight finite automata introduce gpu implementation viterbi forward backward algorithm achieve decode speedups 52x serial implementation run different computer architectures 6093x openfst
financial institutions screen transactions ensure affiliate terrorism entities develop appropriate solutions detect affiliations precisely avoid kind interruption large amount legitimate transactions essential paper present build block scalable solution may help financial institutions build software extract terrorism entities structure unstructured financial message real time approximate similarity match approach
paper propose efficient transfer lean methods train personalize language model use recurrent neural network long short term memory architecture propose fast transfer learn scheme general language model update personalize language model small amount user data limit compute resource methods especially useful mobile device environment data prevent transfer device privacy purpose experiment dialogue data drama verify transfer learn methods successfully generate personalize language model whose output similar personal language style qualitative quantitative aspects
increase use microblogging come along rapid growth short linguistic data hand deep learn consider new frontier extract meaningful information large amount raw data automate manner study engage two emerge field come robust language identifier demand namely language identification engine lide result achieve nine thousand, five hundred and twelve accuracy discriminate similar languages dsl share task two thousand and fifteen dataset comparable maximum report accuracy nine thousand, five hundred and fifty-four achieve far
long run high impact events boston marathon bomb often develop many stag involve large number entities unfold timeline summarization event key sentence ease story digestion distinguish user remember might want check work present novel approach timeline summarization high impact events use entities instead sentence summarize event individual point time entity summaries serve one important memory cue retrospective event consideration two pointers personalize event exploration order automatically create summaries crucial identify right entities inclusion propose learn rank function entities dynamically adapt trade document salience entities informativeness entities across document ie level new information associate entity time point consideration furthermore capture collective attention entity use innovative soft label approach base wikipedia experiment real large news datasets confirm effectiveness propose methods
task orient dialogue focus conversational agents participate user initiate dialogues domain specific topics contrast chatbots simply seek sustain open end meaningful discourse exist task orient agents usually explicitly model user intent belief state paper examine bypass explicit representation depend latent neural embed state learn selective attention dialogue history together copy incorporate relevant prior context complement recent work show effectiveness simple sequence sequence neural architectures copy mechanism model outperform complex memory augment model seven per response generation par current state art dstc2
study collective memories form online track entities emerge public discourse online text stream social media news stream incorporate wikipedia argue view online place collective memory track entities emerge public discourse ie temporal pattern first mention online text stream subsequent incorporation collective memory gain insights collective remembrance process happen online specifically analyze nearly eighty thousand entities emerge online text stream incorporate wikipedia online text stream use analysis comprise social media news stream span five hundred and seventy-nine million document timespan eighteen months discover two main emergence pattern entities emerge bursty fashion ie appear public discourse without precedent blast activity transition collective memory entities display delay pattern appear public discourse experience period inactivity resurface transition cultural collective memory
paper propose new document classification method bridge discrepancies call semantic gap train set application set textual data demonstrate superiority classical text classification approach include traditional classifier ensembles method consist combine document categorization technique single classifier classifier ensemble semcom algorithm committee semantic categorizer
paper investigate whether text community question answer qa platform use predict describe real world attribute experiment predict wide range sixty-two demographic attribute neighbourhoods london use text qa platform yahoo answer compare result ones obtain twitter microblogs outcomes show correlation predict demographic attribute use text yahoo answer discussions observe demographic attribute reach average pearson correlation coefficient rho fifty-four slightly higher predictions obtain use twitter data qualitative analysis indicate semantic relatedness highest correlate term extract datasets relative demographic attribute furthermore correlations highlight different natures information contain yahoo answer twitter former seem offer encyclopedic content latter provide information relate current sociocultural aspects phenomena
traffic congestion rapidly increase urban areas particularly mega cities date exist sensor network base systems address problem however techniques suitable enough term monitor entire transportation system deliver emergency service need techniques require real time data intelligent ways quickly determine traffic activity useful information addition exist systems websites city transportation travel rely rat score different factor eg safety low crime rate cleanliness etc rat score efficient enough deliver precise information whereas review tweet significant help travelers transportation administrators know aspect city however difficult travelers read transportation systems process review tweet obtain expressive sentiments regard need city optimum solution kind problem analyze information available social network platforms perform sentiment analysis hand crisp ontology base frameworks extract blur information tweet review therefore produce inadequate result regard paper propose fuzzy ontology base sentiment analysis swrl rule base decision make monitor transportation activities make city feature polarity map travelers system retrieve review tweet relate city feature transportation activities feature opinions extract retrieve data fuzzy ontology use determine transportation city feature polarity fuzzy ontology intelligent system prototype develop use prot eg e owl java respectively
traditional visual speech recognition systems consist two stag feature extraction classification recently several deep learn approach present automatically extract feature mouth image aim replace feature extraction stage however research joint learn feature classification limit work present end end visual speech recognition system base long short memory lstm network best knowledge first model simultaneously learn extract feature directly pixels perform classification also achieve state art performance visual speech classification model consist two stream extract feature directly mouth difference image respectively temporal dynamics stream model lstm fusion two stream take place via bidirectional lstm blstm absolute improvement ninety-seven base line report ouluvs2 database fifteen cuave database compare methods use similar visual front end
give collection image speak audio caption present method discover word like acoustic units continuous speech signal ground semantically relevant image regions example model able detect speak instance word lighthouse within utterance associate image regions contain lighthouses use form conventional automatic speech recognition use text transcriptions conventional linguistic annotations model effectively implement form speak language acquisition computer learn recognize word categories sound also enrich word learn semantics ground image
application deep neural network rank search engines may obviate need extensive feature engineer common current learn rank methods however show combine simple relevance match feature like bm25 exist deep neural net model often substantially improve accuracy model indicate capture essential local relevance match signal describe novel deep recurrent neural net base model call match tensor architecture match tensor model simultaneously account local relevance match global topicality signal allow rich interplay compute relevance document query large hold test set consist social media document demonstrate match tensor outperform bm25 class dnns also largely subsume signal present model
trend topic newspapers indicator understand situation country also way evaluate particular newspaper paper represent model describe techniques select trend topics bangla newspaper topics discuss frequently bangla newspaper mark famous topic lose importance change time another topic take place demonstrate data two popular bangla newspaper date time collect statistical analysis perform data preprocessing popular use keywords extract stream bangla keyword analysis model also cluster category wise news trend list news trend daily weekly basis enough data pattern find news trend comparison among past news trend bangla newspapers give visualization situation bangladesh visualization helpful predict future trend topics bangla newspaper
hide markov model base various phoneme recognition methods bengali language review automatic phoneme recognition bengali language use multilayer neural network review usefulness multilayer neural network single layer neural network discuss bangla phonetic feature table construction enhancement bengali speech recognition also discuss comparison among methods discuss
document categorization technique category document determine paper three well know supervise learn techniques support vector machinesvm nai bayesnb stochastic gradient descentsgd compare bengali document categorization besides classifier classification also depend feature select dataset analyze classifier performances predict document twelve categories several feature selection techniques also apply article namely chi square distribution normalize tfidf term frequency inverse document frequency word analyzer attempt explore efficiency three classification algorithms use two different feature selection techniques article
today kind information get digitize along digitization huge archive various kinds document digitize know optical character recognition method newspapers paper document convert digital resources fact method work texts result try process document contain non textual zone get garbage texts output order digitize document properly prepossess carefully preprocessing segment document different regions accord category properly important optical character recognition process available bangla language algorithm categorize newspaper book page fully work decompose document several part like headline sub headline columns image etc input skew rotate input also deskewed de rotate decompose bangla document find edge input image find horizontal vertical area every pixel lie later input image cut accord areas pick every sub image find height width ratio line height accord value sub image categorize deskew image find skew angle de skew image accord angle de rotate image use line height matra line pixel ratio matra line
present multilingual name entity recognition approach base robust general set feature across languages datasets system combine shallow local information cluster semi supervise feature induce large amount unlabeled text understand via empirical experimentation effectively combine various type cluster feature allow us seamlessly export system datasets languages result simple highly competitive system obtain state art result across five languages twelve datasets result report standard share task evaluation data conll english spanish dutch furthermore despite lack linguistically motivate feature also report best result languages basque german addition demonstrate method also obtain competitive result even amount supervise data cut half alleviate dependency manually annotate data finally result show emphasis cluster feature crucial develop robust domain model system model freely available facilitate use guarantee reproducibility result
highlight important frontier algorithmic fairness disparity quality natural language process algorithms apply language author different social group example current systems sometimes analyze language females minorities poorly white males conduct empirical analysis racial disparity language identification tweet write african american english discuss implications disparity nlp
one challenge neural rank need large amount manually label relevance judgments train contrast prior work examine use weak supervision source train yield pseudo query document pair already exhibit relevance eg newswire headline content pair encyclopedic head paragraph pair also propose filter techniques eliminate train sample far domain use two techniques heuristic base approach novel supervise filter purpose neural ranker use several lead neural rank architectures multiple weak supervision datasets show source train pair effective outperform prior weak supervision techniques filter improve performance
multichannel linear filter multichannel wiener filter mwf generalize eigenvalue gev beamformer popular signal process techniques improve speech recognition performance paper present experimental study linear filter specific speech recognition task namely chime four challenge feature real record multiple noisy environments specifically rank one mwf employ noise reduction new constant residual noise power constraint derive enhance recognition performance fulfill underlie rank one assumption speech covariance matrix reconstruct base eigenvectors generalize eigenvectors rank one constrain mwf evaluate alternative multichannel linear filter framework involve bidirectional long short term memory blstm network mask estimation propose filter outperform alternative ones lead forty relative word error rate wer reduction compare baseline weight delay sum wdas beamformer real test set fifteen relative wer reduction compare gev ban method result also suggest speech recognition accuracy correlate mel frequency cepstral coefficients mfcc feature variance noise reduction speech distortion level
goal contribution use parametric speech synthesis system reduce background noise interferences record speech signal first step hide markov model synthesis system train two adequate train corpora consist text correspond speech file set clear various fault include inaudible utterances incorrect assignments audio text data test compare regard eg flaw synthesize speech naturalness intelligibility thus different voice synthesize whose quality depend less number train sample use much cleanliness signal noise ratio generalize voice model use synthesis result greatly differ two speech corpora test regard adaptation different speakers show resemblance original speaker audible throughout record yet synthesize voice sound robotic unnatural smaller part speak text however usually intelligible show model work well novel approach speech synthesize use side information original audio signal particularly pitch frequency result show increase speech quality intelligibility comparison speech synthesize solely text point nearly indistinguishable original
evaluation metric absolute necessity measure performance system complexity data paper discuss determine level complexity code mix social media texts grow rapidly due multilingual interference general texts write multiple languages often hard comprehend analyze time order meet demand analysis also necessary determine complexity particular document text segment thus present paper discuss exist metrics determine code mix complexity corpus advantage shortcomings well propose several improvements exist metrics new index better reflect variety complexity multilingual document also index apply sentence seamlessly extend paragraph entire document employ two exist code mix corpora suit requirements study
position paper formalise abstract model complex negotiation dialogue model use benchmark optimisation algorithms range reinforcement learn stochastic game transfer learn one shoot learn others
important task recommender system generate explanations accord user preferences current methods explainable recommendations use structure sentence provide descriptions along recommendations produce however methods neglect review orient way write text even though know review strong influence user decision paper propose method automatic generation natural language explanations predict user would write item base user rat different items feature design character level recurrent neural network rnn model generate item review explanations use long short term memories lstm model generate text review give combination review rat score express opinions different factor aspects item network train sub sample large real world dataset beeradvocate empirical evaluation use natural language process metrics show generate text quality close real user write review identify negation misspell domain specific vocabulary
text preprocessing often first step pipeline natural language process nlp system potential impact final performance despite importance text preprocessing receive much attention deep learn literature paper investigate impact simple text preprocessing decisions particularly tokenizing lemmatizing lowercasing multiword group performance standard neural text classifier perform extensive evaluation standard benchmarks text categorization sentiment analysis experiment show simple tokenization input text generally adequate also highlight significant degrees variability across preprocessing techniques reveal importance pay attention usually overlook step pipeline particularly compare different model finally evaluation provide insights best preprocessing practice train word embeddings
relation schema induction rsi problem identify type signatures arguments relations unlabeled text previous work area focus binary rsi ie induce subject object type signatures per relation however practice many relations high order ie two arguments induce type signatures arguments necessary example sport domain induce schema winwinningplayer opponentplayer tournament location informative induce winwinningplayer opponentplayer refer problem higher order relation schema induction hrsi paper propose tensor factorization back aggregation tfba novel framework hrsi problem best knowledge first attempt induce higher order relation schemata unlabeled text use experimental analysis three real world datasets show tfba help deal sparsity induce higher order schemata
question answer important difficult task natural language process domain many basic natural language process task cast question answer task several deep neural network architectures develop recently employ memory inference components memorize reason text information generate answer question however major drawback many model capable generate single word answer addition require large amount train data generate accurate answer paper introduce long term memory network ltmn incorporate external memory module long short term memory lstm module comprehend input data generate multi word answer ltmn model train end end use back propagation require minimal supervision test model two synthetic data set base facebook babi data set real world stanford question answer data set show achieve state art performance
automate documentation program source code automate code generation natural language challenge task practical scientific interest progress areas limit low availability parallel corpora code natural language descriptions tend small constrain specific domains work introduce large diverse parallel corpus hundred thousands python function documentation string docstrings generate scrap open source repositories github describe baseline result code documentation code generation task obtain neural machine translation also experiment data augmentation techniques increase amount train data release datasets process script order stimulate research areas
state art slot fill model goal orient human machine conversational language understand systems rely deep learn methods multi task train model alleviate need large domain annotate datasets bootstrapping semantic parse model new domain use semantic frame back end api knowledge graph schema still one holy grail task language understand dialogue systems paper propose deep learn base approach utilize slot description context without need label unlabeled domain examples quickly bootstrap new domain main idea paper leverage encode slot name descriptions within multi task deep learn slot fill model implicitly align slot across domains propose approach promise solve domain scale problem eliminate need manually annotate data explicit schema alignment furthermore experiment multiple domains show approach result significantly better slot fill performance compare use domain data especially low data regime
present efficient document representation learn framework document vector corruption doc2vecc doc2vecc represent document simple average word embeddings ensure representation generate capture semantic mean document learn corruption model include introduce data dependent regularization favor informative rare word force embeddings common non discriminative ones close zero doc2vecc produce significantly better word embeddings word2vec compare doc2vecc several state art document representation learn algorithms simple model architecture introduce doc2vecc match perform state art generate high quality document representations sentiment analysis document classification well semantic relatedness task simplicity model enable train billions word per hour single machine time model efficient generate representations unseen document test time
state art name entity recognition ner systems supervise machine learn model require large amount manually annotate data achieve high accuracy however annotate ner data human expensive time consume quite difficult new language paper present two weakly supervise approach cross lingual ner human annotation target language first approach create automatically label ner data target language via annotation projection comparable corpora develop heuristic scheme effectively select good quality projection label data noisy data second approach project distribute representations word word embeddings target language source language source language ner system apply target language without train also design two co decode scheme effectively combine output two projection base approach evaluate performance propose approach house open ner data several target languages result show combine systems outperform three weakly supervise approach conll data
important difficult challenge build computational model narratives automatic evaluation narrative quality quality evaluation connect narrative understand generation generation systems need evaluate products circumvent difficulties acquire annotations employ upvotes social media approximate measure story quality collect fifty-four thousand, four hundred and eighty-four answer crowd power question answer website quora use active learn build classifier label twenty-eight thousand, three hundred and twenty answer stories predict number upvotes without use social network feature create neural network model textual regions interdependence among regions serve strong benchmarks future research best knowledge first large scale study automatic evaluation narrative quality
current study apply deep learn herbalism toward goal acquire de identify health insurance reimbursements claim ten year period two thousand and four two thousand and thirteen national health insurance database taiwan total number reimbursement record equal three hundred and forty millions two artificial intelligence techniques apply dataset residual convolutional neural network multitask classifier attention base recurrent neural network former work translate herbal prescriptions diseases latter diseases herbal prescriptions analysis classification result indicate herbal prescriptions specific anatomy pathophysiology sex age patient season year prescription analysis identify temperature gross domestic product meteorological socioeconomic factor associate herbal prescriptions analysis neural machine transitional result indicate recurrent neural network learn syntax also semantics diseases herbal prescriptions
multiple simple context free tree grammars investigate simple mean linear nondeleting every multiple context free tree grammar finitely ambiguous lexicalize ie transform equivalent one generate tree language rule grammar contain lexical symbol due transformation rank nonterminals increase one multiplicity fan grammar increase maximal rank lexical symbols particular multiplicity increase lexical symbols rank zero multiple context free tree grammars tree generate power multi component tree adjoin grammars provide latter use root marker moreover every multi component tree adjoin grammar finitely ambiguous lexicalize multiple context free tree grammars string generate power multiple context free string grammars polynomial time parse algorithms tree language generate multiple context free tree grammar image regular tree language deterministic finite copy macro tree transducer multiple context free tree grammars use synchronous translation device
goal use formal methods analyse normative document write english privacy policies service level agreements require combination number different elements include information extraction natural language formal languages model representation interface property specification verification work collection components task natural language extraction tool suitable formalism represent document interface build model formalism methods answer query ask give model work concern bring together web base tool provide single interface analyse normative texts english use run example describe component demonstrate workflow establish tool
general treebank analyse graph structure parsers typically restrict tree structure efficiency model reason propose new representation algorithm class graph structure flexible enough cover almost treebank structure still admit efficient learn inference particular consider direct acyclic one endpoint cross graph structure cover long distance dislocation share argumentation similar tree violate linguistic phenomena describe convert phrase structure parse include trace new representation reversible manner dynamic program uniquely decompose structure sound complete cover nine hundred and seventy-three penn english treebank also implement proof concept parser recover range null elements trace type
today vocabulary size language model large vocabulary speech recognition typically several hundreds thousands word already sufficient applications vocabulary word still limit usability others agglutinative languages vocabulary conversational speech include millions word form cover spell variations due colloquial pronunciations addition word compound inflections large vocabularies also need example recognition rare proper name important
long stand interest understand social influence social sciences computational linguistics paper present novel approach study measure interpersonal influence daily interactions motivate basic principles influence attempt identify indicative linguistic feature post online knit community present scheme use operationalize label post indicator feature experiment identify feature show improvement classification accuracy influence three hundred and fifteen result illustrate important correlation characteristics language potential influence others
paper describe submissions wmt17 multimodal translation task task one multimodal translation best score system purely textual neural translation source image caption target language main feature system use additional data acquire select similar sentence parallel corpora data synthesis back translation task two cross lingual image caption best submit system generate english caption translate best system use task one also present negative result base ideas believe potential make improvements prove useful particular setup
tag news article blog post relevant tag collection predefined ones coin document tag work accurate tag article benefit several downstream applications recommendation search work propose novel yet simple approach call doctag2vec accomplish task substantially extend word2vec doc2vec two popular model learn distribute representation word document doctag2vec simultaneously learn representation word document tag joint vector space train employ simple k nearest neighbor search predict tag unseen document contrast previous multi label learn methods doctag2vec directly deal raw text instead provide feature vector addition enjoy advantage like learn tag representation ability handle newly create tag demonstrate effectiveness approach conduct experiment several datasets show promise result state art methods
paper present release emojinet largest machine readable emoji sense inventory link unicode emoji representations english mean extract web emojinet dataset consist twelve thousand, nine hundred and four sense label two thousand, three hundred and eighty-nine emoji extract web link machine readable sense definitions see babelnet ii context word associate emoji sense infer word embed model train google news corpus twitter message corpus emoji sense definition iii recognize discrepancies presentation emoji different platforms specification likely platform base emoji sense select set emoji dataset host open service rest api available http emojinetknoesisorg development dataset evaluation quality applications include emoji sense disambiguation emoji sense similarity discuss
emoji grow become one important form communication web widespread use measure similarity emoji become important problem contemporary text process since lie heart sentiment analysis search interface design task paper present comprehensive analysis semantic similarity emoji embed model learn machine readable emoji mean emojinet knowledge base use emoji descriptions emoji sense label emoji sense definitions different train corpora obtain twitter google news develop test multiple embed model measure emoji similarity evaluate work create new dataset call emosim508 assign human annotate semantic similarity score set five hundred and eight carefully select emoji pair validation emosim508 present real world use case emoji embed model use sentiment analysis task show model outperform previous best perform emoji embed model task emosim508 dataset emoji embed model publicly release paper download http emojinetknoesisorg
paper exploit memory augment neural network predict accurate answer visual question even answer occur rarely train set memory network incorporate internal external memory block selectively pay attention train exemplar show memory augment neural network able maintain relatively long term memory scarce train exemplars important visual question answer due heavy tail distribution answer general vqa set experimental result two large scale benchmark datasets show favorable performance propose algorithm comparison state art
today conversational agents restrict simple standalone command paper present iris agent draw human conversational strategies combine command allow perform complex task explicitly design support example compose one command plot histogram another first log transform data enable complexity introduce domain specific language transform command automata iris compose sequence execute dynamically interact user natural language well conversational type system manage kinds command combine design iris help users data science task domain require support command combination evaluation find data scientists complete predictive model task significantly faster twenty-six time speedup iris modern non conversational program environment iris support kinds command today agents empower users weave together command accomplish complex goals
shortage available train data hold back progress area automate error detection paper investigate two alternative methods artificially generate write errors order create additional resources propose treat error generation machine translation task grammatically correct text translate contain errors addition explore system extract textual pattern annotate corpus use insert errors grammatically correct sentence experiment show inclusion artificially generate errors significantly improve error detection accuracy fce conll two thousand and fourteen datasets
domain similarity measure use gauge adaptability select suitable data transfer learn exist approach define ad hoc measure deem suitable respective task inspire work curriculum learn propose emphlearn data selection measure use bayesian optimization evaluate across model domains task learn measure outperform exist domain similarity measure significantly three task sentiment analysis part speech tag parse show importance complement similarity diversity learn measure degree transferable across model domains even task
explainable recommendation important task many methods propose generate explanations content review write items review text unavailable generate explanations still hard problem paper illustrate explanations generate scenario leverage external knowledge form knowledge graph method jointly rank items knowledge graph entities use personalize pagerank procedure produce recommendations together explanations
layer normalization recently introduce technique normalize activities neurons deep neural network improve train speed stability paper introduce new layer normalization technique call dynamic layer normalization dln adaptive neural acoustic model speech recognition dynamically generate scale shift parameters layer normalization dln adapt neural acoustic model acoustic variability arise various factor speakers channel noise environments unlike adaptive acoustic model propose approach require additional adaptation data speaker information vectors moreover model size fix dynamically generate adaptation parameters apply propose dln deep bidirectional lstm acoustic model evaluate two benchmark datasets large vocabulary asr experiment wsj ted lium release two experimental result show dln improve neural acoustic model term transcription accuracy dynamically adapt various speakers environments
domain mismatch train test lead significant degradation performance many machine learn scenarios unfortunately rare situation automatic speech recognition deployments real world applications research robust speech recognition regard try overcome domain mismatch issue paper address unsupervised domain adaptation problem robust speech recognition source target domain speech present word transcripts available source domain speech present novel augmentation base methods transform speech way change transcripts specifically first train variational autoencoder source target domain data without supervision learn latent representation speech transform nuisance attribute speech irrelevant recognition modify latent representations order augment label train data additional data whose distribution similar target domain propose method evaluate chime four dataset reduce absolute word error rate wer much thirty-five compare non adapt baseline
reinforcement learn widely use dialogue policy optimization reward function often consist one component eg dialogue success dialogue length work propose structure method find good balance components search optimal reward component weight render search feasible use multi objective reinforcement learn significantly reduce number train dialogues require apply propose method find optimize component weight six domains compare default baseline
introduce variety model train supervise image caption corpus predict image feature give caption perform sentence representation ground train ground sentence encoder achieve good performance coco caption image retrieval subsequently show encoder successfully transfer various nlp task improve performance text model lastly analyze contribution ground show word embeddings learn system outperform non ground ones
audio word2vec offer vector representations fix dimensionality variable length audio segment use sequence sequence autoencoder sa vector representations show describe sequential phonetic structure audio segment good degree real world applications query example speak term detection std paper examine capability language transfer audio word2vec train sa one language source language use extract vector representation audio segment another language target language find sa still catch phonetic structure audio segment target language source target languages similar query example std obtain vector representations sa learn large amount source language data find surpass representations naive encoder sa directly learn small amount target language data result show possible learn audio word2vec model high resource languages use low resource languages expand usability audio word2vec
distributional semantics model know struggle small data generally accept order learn good vector word model must sufficient examples usage contradict fact humans guess mean word occurrences paper show neural language model word2vec necessitate minor modifications standard architecture learn new term tiny data use background knowledge previously learn semantic space test model word definitions nonce task involve two six sentence worth context show large increase performance state art model definitional task
task selection micro task market support recommender systems help individuals find appropriate task previous work show selection process micro task semantic aspects require action comprehensibility rat important factual aspects payment require completion time work give foundation create similarity measure therefore show automatic classification base task descriptions possible additionally propose similarity measure cluster micro task accord semantic aspects
recent advance neural word embed provide significant benefit various information retrieval task however show recent study adapt embed model need ir task bring considerable improvements embed model general define term relatedness exploit term co occurrences short window contexts alternative well study approach ir relate term query use local information ie set top retrieve document view two methods term relatedness work report study incorporate local information query word embeddings one main challenge direction dense vectors word embeddings estimation term term relatedness remain difficult interpret hard analyze alternative explicit word representations propose vectors whose dimension easily interpretable recent methods show competitive performance dense vectors introduce neural base explicit representation root conceptual ideas word2vec skip gram model method provide interpretable explicit vectors keep effectiveness skip gram model evaluation various explicit representations word association collections show newly propose method perform state art explicit representations task rank highly similar term base introduce ex plicit representation discuss approach integrate local document globally train embed model discuss preliminary result
study problem learn reason large scale knowledge graph kgs specifically describe novel reinforcement learn framework learn multi hop relational paths use policy base agent continuous state base knowledge graph embeddings reason kg vector space sample promise relation extend path contrast prior work approach include reward function take accuracy diversity efficiency consideration experimentally show propose method outperform path rank base algorithm knowledge graph embed methods freebase never end language learn datasets
unsupervised single channel overlap speech recognition one hardest problems automatic speech recognition asr permutation invariant train pit state art model base approach apply single neural network solve single input multiple output model problem propose advance current state art impose modular structure neural network apply progressive pretraining regimen improve objective function transfer learn discriminative train criterion modular structure split problem three sub task frame wise interpret utterance level speaker trace speech recognition pretraining regimen use modules solve progressively harder task transfer learn leverage parallel clean speech improve train target network discriminative train formulation modification standard formulations also penalize compete output system experiment conduct artificial overlap switchboard hub5e swb dataset propose framework achieve thirty relative improvement wer strong jointly train system pit asr separately optimize system pit speech separation clean speech asr model improvement come better model generalization train efficiency sequence level linguistic knowledge integration
generate caption image task recently receive considerable attention work focus caption generation abstract scenes object layouts information provide set object locations propose obj2text sequence sequence model encode set object locations input sequence use lstm network decode representation use lstm language model show model despite encode object layouts sequence represent spatial relationships object generate descriptions globally coherent semantically relevant test approach task object layout caption use object annotations input additionally show model combine state art object detector improve image caption model eight hundred and sixty-three nine hundred and fifty cider score test benchmark standard ms coco caption task
recently grow interest end end speech recognition directly transcribe speech text without predefined alignments paper explore use attention base encoder decoder model mandarin speech recognition voice search task previous attempt show apply attention base encoder decoder mandarin speech recognition quite difficult due logographic orthography mandarin large vocabulary conditional dependency attention model paper use character embed deal large vocabulary several trick use effective model train include l2 regularization gaussian weight noise frame skip compare two attention mechanisms use attention smooth cover long context attention model take together trick allow us finally achieve character error rate cer three hundred and fifty-eight sentence error rate ser seven hundred and forty-three mitv voice search dataset together trigram language model cer ser reach two hundred and eighty-one five hundred and seventy-seven respectively
present moodswipe soft keyboard suggest text message give user specify emotions utilize real dialog data aim moodswipe create convenient user interface enjoy technology emotion classification text suggestion time collect label data automatically develop advance technologies users select moodswipe keyboard type usual sense emotion convey text receive suggestions message benefit moodswipe detect emotions serve medium suggest texts view latter incentive correct former conduct several experiment show superiority emotion classification model train dialog data verify good emotion cue important context text suggestion
recent years deep neural model widely adopt text match task question answer information retrieval show improve performance compare previous methods paper introduce matchzoo toolkit aim facilitate design compare share deep text match model specifically toolkit provide unify data preparation module different text match problems flexible layer base model construction process variety train objectives evaluation metrics addition toolkit implement two school representative deep text match model namely representation focus model interaction focus model finally users easily modify exist model create share model text match matchzoo
standard accuracy metrics indicate read comprehension systems make rapid progress extent systems truly understand language remain unclear reward systems real language understand abilities propose adversarial evaluation scheme stanford question answer dataset squad method test whether systems answer question paragraph contain adversarially insert sentence automatically generate distract computer systems without change correct answer mislead humans adversarial set accuracy sixteen publish model drop average seventy-five f1 score thirty-six adversary allow add ungrammatical sequence word average accuracy four model decrease seven hope insights motivate development new model understand language precisely
work perform empirical comparison among ctc rnn transducer attention base seq2seq model end end speech recognition show without language model seq2seq rnn transducer model outperform best report ctc model language model popular hub5 zero benchmark internal diverse dataset trend continue rnntransducer model rescored language model beam search outperform best ctc model result simplify speech recognition pipeline decode express purely neural network operations also study choice encoder architecture affect performance three model encoder layer forward encoders downsample input representation aggressively
natural language inference nli central problem language understand end end artificial neural network reach state art performance nli field recently paper propose character level intra attention network cian nli task model use character level convolutional network replace standard word embed layer use intra attention capture intra sentence semantics propose cian model provide improve result base newly publish mnli corpus
propose methodology adapt graph embed techniques deepwalk perozzi et al two thousand and fourteen node2vec grover leskovec two thousand and sixteen well cross lingual vector space map approach least square canonical correlation analysis order merge corpus ontological source lexical knowledge also perform comparative analysis use algorithms order identify best combination propose system apply task enhance coverage exist word embed vocabulary rare unseen word show technique provide considerable extra coverage ninety-nine lead consistent performance gain around ten absolute gain achieve w2v gn 500k cfs thirty-three rare word similarity dataset
paper propose model learn multimodal multilingual representations match image sentence different languages aim advance multilingual versions image search image understand model learn common representation image descriptions two different languages need parallel consider image pivot two languages introduce new pairwise rank loss function handle symmetric asymmetric similarity two modalities evaluate model image description rank german english semantic textual similarity image descriptions english case achieve state art performance
discussion forums important source information often use answer specific question user might discover topic interest discussions forums may evolve intricate ways make difficult users follow flow ideas propose novel approach automatically identify underlie thread structure forum discussion approach base neural model compute coherence score possible reconstructions select highest score ie coherent one preliminary experiment demonstrate promise result outperform number strong baseline methods
time important relevance signal search stream social media post distribution document timestamps result initial query leverage infer distribution relevant document use rerank initial result previous experiment show kernel density estimation simple yet effective implementation idea paper explore alternative approach mine temporal signal recurrent neural network intuition neural network provide expressive framework capture temporal coherence neighbor document time knowledge first integrate lexical temporal signal end end neural network architecture exist neural rank model use generate query document similarity vectors fee bidirectional lstm layer temporal model result mix exist neural model document rank alone yield limit improvements simple baselines integration lexical temporal signal yield significant improvements competitive temporal baselines
work natural language question answer today focus answer selection give candidate list sentence determine contain answer although important answer selection one stage standard end end question answer pipeline paper explore effectiveness convolutional neural network cnns answer selection end end context use standard trecqa dataset observe simple idf weight word overlap algorithm form strong baseline despite substantial efforts community apply deep learn tackle answer selection gain modest best dataset furthermore unclear cnn effective baseline end end context base standard retrieval metrics explore find conduct manual user evaluation confirm answer cnn detectably better idf weight word overlap result suggest users sensitive relatively small differences answer selection quality
query segmentation one critical components understand users search intent information retrieval task involve group tokens search query meaningful phrase help downstream task like search relevance query understand paper propose novel approach segment user query use distribute query embeddings key contribution supervise approach segmentation task use low dimensional feature vectors query get rid traditional hand tune heuristic nlp feature quite expensive benchmark fifty thousand human annotate web search engine query corpus achieve comparable accuracy state art techniques advantage technique fast use external knowledge base like wikipedia score boost help us generalize approach domains like ecommerce without fine tune demonstrate effectiveness method another fifty thousand human annotate ecommerce query corpus ebay search log approach easy implement generalize well across different search domains prove power low dimensional embeddings query segmentation task open new direction research problem
dominant neural architectures question answer retrieval base recurrent convolutional encoders configure complex word match layer give recent architectural innovations mostly new word interaction layer attention base match mechanisms seem well establish fact components mandatory good performance unfortunately memory computation cost incur complex mechanisms undesirable practical applications paper tackle question whether possible achieve competitive performance simple neural architectures propose simple novel deep learn architecture fast efficient question answer rank retrieval specifically propose model textschyperqa parameter efficient neural network outperform parameter intensive model attentive pool bilstms multi perspective cnns multiple qa benchmarks novelty behind textschyperqa pairwise rank objective model relationship question answer embeddings hyperbolic space instead euclidean space empower model self organize ability enable automatic discovery latent hierarchies learn embeddings question answer model require feature engineer similarity matrix match complicate attention mechanisms parameterized layer yet outperform remain competitive many model functionalities multiple benchmarks
investigate compositional structure message vectors compute deep network train communication game compare truth conditional representations encoder produce message vectors human produce refer expressions able identify align vector utterance pair mean search structure relationships among align pair discover simple vector space transformations correspond negation conjunction disjunction result suggest neural representations capable spontaneously develop syntax functional analogues qualitative properties natural language
years twitter become one largest communication platforms provide key data various applications brand monitor trend detection among others entity link one major task natural language understand tweet associate entity mention text correspond entries knowledge base order provide unambiguous interpretation additional con text state art techniques focus link explicitly mention entities tweet reasonable success however argue addition explicit mention ie movie gravity ex pensive mar orbiter mission entities movie gravity also mention implicitly ie new space movie crazy must watch paper introduce problem implicit entity link tweet propose approach model entities exploit factual contextual knowledge demonstrate use model perform implicit entity link grind truth dataset three hundred and ninety-seven tweet two domains namely movie book specifically show one importance link implicit entities value addition standard entity link task two importance exploit contextual knowledge associate entity link implicit mention also make grind truth dataset publicly available foster research new research area
propose new self organize hierarchical softmax formulation neural network base language model large vocabularies instead use predefined hierarchical structure approach capable learn word cluster clear syntactical semantic mean language model train process provide experiment standard benchmarks language model sentence compression task find approach fast efficient softmax approximations achieve comparable even better performance relative similar full softmax model
robots operate alongside humans diverse stochastic environments must able accurately interpret natural language command instructions often fall one two categories specify goal condition target state specify explicit action perform give task recent approach use reward function semantic representation goal base command allow use state art planner find policy give task however reward function directly use represent action orient command introduce new hybrid approach deep recurrent action goal ground network draggn task ground execution handle natural language either category input generalize unseen environments robot simulation result demonstrate system successfully interpret goal orient action orient task specifications bring us closer robust natural language understand human robot interaction
work analyze performances two use word embeddings algorithms skip gram continuous bag word italian language algorithms many hyper parameter carefully tune order obtain accurate word representation vectorial space provide accurate analysis evaluation show best configuration parameters specific task
deep residual learn resnet new method train deep neural network use identity map ping shortcut connections resnet imagenet ilsvrc two thousand and fifteen classification task achieve state art performances many computer vision task however effect residual learn noisy natural language process task still well understand paper design novel convolutional neural network cnn residual learn investigate impact task distantly supervise noisy relation extraction contradictory popular beliefs resnet work well deep network find even nine layer cnns use identity map could significantly improve performance distantly supervise relation extraction
introduce describe result novel share task bandit learn machine translation task organize jointly amazon heidelberg university first time second conference machine translation wmt two thousand and seventeen goal task encourage research learn machine translation weak user feedback instead human reference post edit sequence round machine translation system require propose translation input receive real value estimate quality propose translation learn paper describe share task learn evaluation setup use service host amazon web service aws data evaluation metrics result various machine translation architectures learn protocols
machine comprehensionmc style question answer representative problem natural language process previous methods rarely spend time improvement encode layer especially embed syntactic information name entity word crucial quality encode moreover exist attention methods represent query word vector use single vector represent whole query sentence neither handle proper weight key word query sentence paper introduce novel neural network architecture call multi layer embed memory networkmemen machine read task encode layer employ classic skip gram model syntactic semantic information word train new kind embed layer also propose memory network full orientation match query passage catch pivotal information experiment show model competitive result perspectives precision efficiency stanford question answer datasetsquad among publish result achieve state art result triviaqa dataset
paper investigate large scale zero shoot activity recognition model visual linguistic attribute action verbs example verb salute several properties light movement social act short duration use attribute internal map visual textual representations reason previously unseen action contrast much prior work assume access gold standard attribute zero shoot class focus primarily object attribute model uniquely learn infer action attribute dictionary definitions distribute word representations experimental result confirm action attribute infer language provide predictive signal zero shoot prediction previously unseen activities
propose framework multimodal sentiment analysis emotion recognition use convolutional neural network base feature extraction text visual modalities obtain performance improvement ten state art combine visual text audio feature also discuss major issue frequently ignore multimodal sentiment analysis research role speaker independent model importance modalities generalizability paper thus serve new benchmark research multimodal sentiment analysis also demonstrate different facets analysis consider perform task
familia open source toolkit pragmatic topic model industry familia abstract utilities topic model industry two paradigms semantic representation semantic match efficient implementations two paradigms make publicly available first time furthermore provide shelf topic model train large scale industrial corpora include latent dirichlet allocation lda sentencelda topical word embed twe describe typical applications successfully power topic model order ease confusions difficulties software engineer topic model selection utilization
paper show report single performance score insufficient compare non deterministic approach demonstrate common sequence tag task seed value random number generator result statistically significant p ten four differences state art systems two recent systems ner observe absolute difference one percentage point f1 score depend select seed value make systems perceive either state art mediocre instead publish report single performance score propose compare score distributions base multiple executions base evaluation fifty thousand lstm network five sequence tag task present network architectures produce superior performance well stable respect remain hyperparameters
machine translation recently achieve impressive performance thank recent advance deep learn availability large scale parallel corpora numerous attempt extend successes low resource language pair yet require tens thousands parallel sentence work take research direction extreme investigate whether possible learn translate even without parallel data propose model take sentence monolingual corpora two different languages map latent space learn reconstruct languages share feature space model effectively learn translate without use label data demonstrate model two widely use datasets two language pair report bleu score three hundred and twenty-eight one hundred and fifty-one multi30k wmt english french datasets without use even single parallel sentence train time
traditional model question answer optimize use cross entropy loss encourage exact answer cost penalize nearby overlap answer sometimes equally accurate propose mix objective combine cross entropy loss self critical policy learn objective use reward derive word overlap solve misalignment evaluation metric optimization objective addition mix objective improve dynamic coattention network dcn deep residual coattention encoder inspire recent work deep self attention residual network proposals improve model performance across question type input lengths especially long question require ability capture long term dependencies stanford question answer dataset model achieve state art result seven hundred and fifty-one exact match accuracy eight hundred and thirty-one f1 ensemble obtain seven hundred and eighty-nine exact match accuracy eight hundred and sixty f1
name concepts compositional operators present natural language provide rich source information kinds abstractions humans use navigate world linguistic background knowledge improve generality efficiency learn classifiers control policies paper aim show use space natural language string parameter space effective way capture natural task structure pretraining phase learn language interpretation model transform input eg image output eg label give natural language descriptions learn new concept eg classifier search directly space descriptions minimize interpreter loss train examples crucially model require language data learn concepts language use pretraining impose structure subsequent learn result image classification text edit reinforcement learn show settings model linguistic parameterization outperform without
prosodic model core problem speech synthesis key challenge produce desirable prosody textual input contain phonetic information preliminary study introduce concept style tokens tacotron recently propose end end neural speech synthesis model use style tokens aim extract independent prosodic style train data show without annotation data explicit supervision signal approach automatically learn variety prosodic variations purely data drive way importantly style token correspond fix style factor regardless give text sequence result control prosodic style synthetic speech somewhat predictable globally consistent way
parallel data important part reliable statistical machine translation smt system data available better quality smt system however language pair persian english parallel source kind scarce paper bidirectional method propose extract parallel sentence english persian document align wikipedia two machine translation systems employ translate persian english reverse ir system use measure similarity translate sentence add extract sentence train data exist smt systems show improve quality translation furthermore propose method slightly outperform one directional approach extract corpus consist two hundred thousand sentence sort degree similarity calculate ir system freely available public access web
manually label document tedious expensive essential train traditional text classifier recent years dataless text classification techniques propose address problem however exist work mainly center single label classification problems document restrict belong single category paper propose novel seed guide multi label topic model name smtm seed word relevant category smtm conduct multi label classification collection document without label document smtm category associate single category topic cover mean category accommodate multi label document explicitly model category sparsity smtm use spike slab prior weak smooth prior without use threshold tune smtm automatically select relevant categories document incorporate supervision seed word propose seed guide bias gpu ie generalize polya urn sample procedure guide topic inference smtm experiment two public datasets show smtm achieve better classification accuracy state art alternatives even outperform supervise solutions scenarios
paper describe general scalable end end framework use generative adversarial network gin objective enable robust speech recognition encoders train propose approach enjoy improve invariance learn map noisy audio embed space clean audio unlike previous methods new framework rely domain expertise simplify assumptions often need signal process directly encourage robustness data drive way show new approach improve simulate far field speech recognition vanilla sequence sequence model without specialize front end preprocessing
question answer one primary challenge natural language understand realize system provide complex long answer question challenge task oppose factoid answer former need context disambiguation different methods explore literature broadly classify three categories namely one classification base two knowledge graph base three retrieval base individually none address need enterprise wide assistance system support maintenance domain domain variance answer large range factoid structure operate procedures knowledge present across heterogeneous data source like application specific documentation ticket management systems single technique general purpose assistance unable scale landscape address build cognitive platform capabilities adopt domain build general purpose question answer system leverage platform instantiate multiple products technologies support domain system use novel hybrid answer model orchestrate across deep learn classifier knowledge graph base context disambiguation module sophisticate bag word search system orchestration perform context switch provide question also smooth hand question human expert none automate techniques provide confident answer system deploy across six hundred and seventy-five internal enterprise support maintenance project
propose neural language model capable unsupervised syntactic structure induction model leverage structure information form better semantic representations better language model standard recurrent neural network limit structure fail efficiently use syntactic information hand tree structure recursive network usually require additional structural supervision cost human expert annotation paper propose novel neural language model call parse read predict network prpn simultaneously induce syntactic structure unannotated sentence leverage infer structure learn better language model model gradient directly back propagate language model loss neural parse network experiment show propose model discover underlie syntactic structure achieve state art performance word character level language model task
state art result neural machine translation often use attentional sequence sequence model form convolution recursion vaswani et al two thousand and seventeen propose new architecture avoid recurrence convolution completely instead use self attention fee forward layer propose architecture achieve state art result several machine translation task require large number parameters train iterations converge propose weight transformer transformer modify attention layer outperform baseline network bleu score also converge fifteen forty faster specifically replace multi head attention multiple self attention branch model learn combine train process model improve state art performance five bleu point wmt two thousand and fourteen english german translation task four english french translation task
character base neural machine translation nmt model alleviate vocabulary issue learn morphology move us closer completely end end translation systems unfortunately also brittle easily falter present noisy data paper confront nmt model synthetic natural source noise find state art model fail translate even moderately noisy texts humans trouble comprehend explore two approach increase model robustness structure invariant word representations robust train noisy texts find model base character convolutional neural network able simultaneously learn representations robust multiple kinds noise
exist approach neural machine translation condition output word previously generate output introduce model avoid autoregressive property produce output parallel allow order magnitude lower latency inference knowledge distillation use input token fertilities latent variable policy gradient fine tune achieve cost little twenty bleu point relative autoregressive transformer network use teacher demonstrate substantial cumulative improvements associate three aspects train strategy validate approach iwslt two thousand and sixteen english german two wmt language pair sample fertilities parallel inference time non autoregressive model achieve near state art performance two hundred and ninety-eight bleu wmt two thousand and sixteen english romanian
recently continuous cache model propose extensions recurrent neural network language model adapt predictions local change data distribution model capture local context thousands tokens paper propose extension continuous cache model scale larger contexts particular use large scale non parametric memory component store hide activations see past leverage recent advance approximate nearest neighbor search quantization algorithms store millions representations search efficiently conduct extensive experiment show approach significantly improve perplexity pre train language model new distributions scale efficiently much larger contexts previously propose local cache model
cloze test widely adopt language exams evaluate students language proficiency paper propose first large scale human create cloze test dataset cloth contain question use middle school high school language exams miss blank carefully create teachers candidate choices purposely design nuanced cloth require deeper language understand wider attention span previously automatically generate cloze datasets test performance dedicatedly design baseline model include language model train one billion word corpus show humans outperform significant margin investigate source performance gap trace model deficiencies distinct properties cloth identify limit ability comprehend long term context key bottleneck
automatic term extraction deal extraction terminology domain specific corpus long establish research area data knowledge acquisition eat remain challenge task know exist eat methods consistently outperform others domain work adopt refresh perspective problem instead search one size fit solution may never exist propose develop generic methods enhance exist eat methods introduce semre rank first method base principle incorporate semantic relatedness often overlook venue exist eat method improve performance semre rank incorporate word embeddings personalise pagerank process compute semantic importance score candidate term graph semantically relate word nod use revise score candidate term compute base eat algorithm extensively evaluate thirteen state art base eat methods four datasets diverse nature show achieve widespread improvement base methods across datasets fifteen percentage point measure precision top rank k candidate term average set k twenty-eight percentage point f1 measure k equal expect real term candidates f1 short compare alternative approach build well know textrank algorithm semre rank potentially outperform eight point precision top k seventeen point f1
knowledge graph kgs apply many task include web search link prediction recommendation natural language process entity link however kgs far complete grow rapid pace address problems knowledge graph completion kgc propose improve kgs fill miss connections unlike exist methods hold close world assumption ie kgs fix new entities easily add present work relax assumption propose new open world kgc task first attempt solve task introduce open world kgc model call conmask model learn embeddings entity name part text description connect unseen entities kg mitigate presence noisy text descriptions conmask use relationship dependent content mask extract relevant snippets train fully convolutional neural network fuse extract snippets entities kg experiment large data set old new show conmask perform well open world kgc task even outperform exist kgc model standard close world kgc task
formulate language model matrix factorization problem show expressiveness softmax base model include majority neural language model limit softmax bottleneck give natural language highly context dependent imply practice softmax distribute word embeddings enough capacity model natural language propose simple effective method address issue improve state art perplexities penn treebank wikitext two four thousand, seven hundred and sixty-nine four thousand and sixty-eight respectively propose method also excel large scale 1b word dataset outperform baseline fifty-six point perplexity
introduce kbgan adversarial learn framework improve performances wide range exist knowledge graph embed model knowledge graph typically contain positive facts sample useful negative train examples non trivial task replace head tail entity fact uniformly randomly select entity conventional method generate negative facts majority generate negative facts easily discriminate positive facts contribute little towards train inspire generative adversarial network gans use one knowledge graph embed model negative sample generator assist train desire model act discriminator gans framework independent concrete form generator discriminator therefore utilize wide variety knowledge graph embed model build block experiment adversarially train two translation base model transe transd assistance one two probability base model distmult complex evaluate performances kbgan link prediction task use three knowledge base completion datasets fb15k two hundred and thirty-seven wn18 wn18rr experimental result show adversarial train substantially improve performances target embed model various settings
train personalize dialogue system require lot data data collect single user usually insufficient one common practice problem share train dialogues different users train multiple sequence sequence dialogue model together transfer learn however current sequence sequence transfer learn model operate entire sentence might negative transfer different personal information different users mix propose personalize decoder model transfer finer granularity phrase level knowledge different users keep personal preferences user intact novel personal control gate introduce enable personalize decoder switch generate personalize phrase share phrase propose personalize decoder model easily combine various deep model train reinforcement learn real world experimental result demonstrate phrase level personalize decoder improve bleu multiple sentence level transfer baseline model much seventy-five
generate emotional language key step towards build empathetic natural language process agents however major challenge line research lack large scale label train data previous study limit small set human annotate sentiment label additionally explicitly control emotion sentiment generate text also difficult paper take radical approach exploit idea leverage twitter data naturally label emojis specifically collect large corpus twitter conversations include emojis response assume emojis convey underlie emotions sentence introduce reinforce conditional variational encoder approach train deep generative model conversations allow us use emojis control emotion generate text experimentally show quantitative qualitative analyse propose model successfully generate high quality abstractive conversation responses accordance designate emotions
locatednear relation kind commonsense knowledge describe two physical object typically find near real life paper study automatically extract relationship sentence level relation classifier aggregate score entity pair large corpus also release two benchmark datasets evaluation future research
unlike extractive summarization abstractive summarization fuse different part source text incline create fake facts preliminary study reveal nearly thirty output state art neural summarization system suffer problem previous abstractive summarization approach usually focus improvement informativeness argue faithfulness also vital prerequisite practical abstractive summarization system avoid generate fake facts summary leverage open information extraction dependency parse technologies extract actual fact descriptions source text dual attention sequence sequence framework propose force generation condition source text extract fact descriptions experiment gigaword benchmark dataset demonstrate model greatly reduce fake summaries eighty notably fact descriptions also bring significant improvement informativeness since often condense mean source text
adversarial train powerful regularization method neural network aim achieve robustness input perturbations yet specific effect robustness obtain still unclear context natural language process paper propose analyze neural pos tag model exploit experiment penn treebank wsj corpus universal dependencies ud dataset twenty-seven languages find improve overall tag accuracy also one prevent fit well low resource languages two boost tag accuracy rare unseen word also demonstrate three improve tag performance contribute downstream task dependency parse four help model learn cleaner word representations five propose model generally effective different sequence label task positive result motivate use natural language task
deep learn demonstrate tremendous potential automatic text score ats task paper describe new neural architecture enhance vanilla neural network model auxiliary neural coherence feature new method propose new textscskipflow mechanism model relationships snapshots hide representations long short term memory lstm network read subsequently semantic relationships multiple snapshots use auxiliary feature prediction two main benefit firstly essay typically long sequence therefore memorization capability lstm network may insufficient implicit access multiple snapshots alleviate problem act protection vanish gradients parameters textscskipflow mechanism also act auxiliary memory secondly model relationships multiple position allow model learn feature represent approximate textual coherence model call textitneural coherence feature overall present unify deep learn architecture generate neural coherence feature read end end fashion approach demonstrate state art performance benchmark soon possible dataset outperform feature engineer baselines also deep learn model
popular recent approach answer open domain question first search question relate passages apply read comprehension model extract answer exist methods usually extract answer single passages independently question require combination evidence across different source answer correctly paper propose two model make use multiple passages generate answer use answer reranking approach reorder answer candidates generate exist state art qa model propose two methods namely strength base rank coverage base rank make use aggregate evidence different passages better determine answer model achieve state art result three public open domain qa datasets quasar searchqa open domain version triviaqa eight percentage point improvement former two datasets
due pivotal role software engineer considerable effort spend quality assurance software requirements specifications mainly describe natural language relatively mean automate quality assessment exist however find clone detection technique widely apply source code promise assess one important quality aspect automate way namely redundancy stem copyandpaste operations paper describe large scale case study apply clone detection twenty-eight requirements specifications total eight thousand, six hundred and sixty-seven page report amount redundancy find real world specifications discuss nature well consequences evaluate far exist code clone detection approach apply assess quality requirements specifications practice
paper present livemedqa question answer system optimize consumer health question top general qa system pipeline introduce several new feature aim exploit domain specific knowledge entity structure better performance include question type focus analyzer base deep text classification model tree base knowledge graph answer generation complementary structure aware searcher answer retrieval livemedqa system evaluate trec two thousand and seventeen liveqa medical subtask receive average score three hundred and fifty-six three point scale evaluation result reveal three substantial drawbacks current livemedqa system base provide detail discussion propose solutions constitute main focus subsequent work
consider challenge problem entity type extremely fine grain set type wherein single mention entity many simultaneous often hierarchically structure type despite importance problem relative lack resources form fine grain deep type hierarchies align exist knowledge base response introduce typenet dataset entity type consist one thousand, nine hundred and forty-one type organize hierarchy obtain manually annotate map one thousand and eighty-one freebase type wordnet also experiment several model comparable state art systems explore techniques incorporate structure loss hierarchy standard mention type loss first step towards future research dataset
knowledge base kb automatically manually construct often incomplete many valid facts infer kb synthesize exist information popular approach kb completion infer new relations combinatory reason information find along paths connect pair entities give enormous size kbs exponential number paths previous path base model consider problem predict miss relation give two entities evaluate truth propose triple additionally methods traditionally use random paths fix entity pair recently learn pick paths propose new algorithm minerva address much difficult practical task answer question relation know one entity since random walk impractical set combinatorially many destinations start node present neural reinforcement learn approach learn navigate graph condition input query find predictive paths empirically approach obtain state art result several datasets significantly outperform prior methods
depression major debilitate disorder affect people age continuous increase number annual case depression need develop automatic techniques detection presence extent depression avec challenge explore different modalities speech language visual feature extract face design develop automatic methods detection depression psychology literature phq eight questionnaire well establish tool measure severity depression paper aim automatically predict phq eight score feature extract different modalities show visual feature extract facial landmarks obtain best performance term estimate phq eight result mean absolute error mae four hundred and sixty-six development set behavioral characteristics speech provide mae four hundred and seventy-three language feature yield slightly higher mae five hundred and seventeen switch test set turn feature derive audio transcriptions achieve best performance score mae four hundred and eleven correspond rmse four hundred and ninety-four make system winner avec two thousand and seventeen depression sub challenge
explore use unsupervised methods cross lingual word sense disambiguation cl wsd application english persian propose approach target languages scarce resources low density exploit word embed semantic similarity word context evaluate approach recent evaluation benchmark compare state art unsupervised system co graph result show approach outperform standard baseline co graph system task evaluation metrics five best result
deep neural network play essential role many computer vision task include visual question answer vqa recently study accuracy main focus research trend toward assess robustness model adversarial attack evaluate tolerance vary noise level vqa adversarial attack target image propose main question yet lack proper analysis later work propose flexible framework focus language part vqa use semantically relevant question dub basic question act controllable noise evaluate robustness vqa model hypothesize level noise positively correlate similarity basic question main question hence apply noise give main question rank pool basic question base similarity cast rank task lasso optimization problem propose novel robustness measure rscore two large scale basic question datasets bqds order standardize robustness analysis vqa model
though deep neural network great success natural language process limit knowledge intensive ai task open domain question answer qa exist end end deep qa model need process entire text observe question therefore complexity respond question linear text size prohibitive practical task qa wikipedia novel web propose solve scalability issue use symbolic mean representations index retrieve efficiently complexity independent text size apply approach call n gram machine ngm three representative task first proof concept demonstrate ngm successfully solve babi task synthetic text second show ngm scale large corpus experiment life long babi special version babi contain millions sentence lastly wikimovies dataset use ngm induce latent structure ie schema answer question natural language wikipedia text qa pair weak supervision
radiology report rich resource advance deep learn applications medicine leverage large volume data continuously update integrate share however significant challenge well largely due ambiguity subtlety natural language propose hybrid strategy combine semantic dictionary map word2vec model create dense vector embeddings free text radiology report method leverage benefit semantic dictionary map well unsupervised learn use vector representation automatically classify radiology report three class denote confidence diagnosis intracranial hemorrhage interpret radiologist perform experiment vary hyperparameter settings word embeddings range different classifiers best performance achieve weight precision eighty-eight weight recall ninety work offer potential leverage unstructured electronic health record data allow direct analysis narrative clinical note
paper introduce new neural structure call fusionnet extend exist attention approach three perspectives first put forward novel concept history word characterize attention information lowest word level embed highest semantic level representation second introduce improve attention score function better utilize history word concept third propose fully aware multi level attention mechanism capture complete information one text question exploit counterpart context passage layer layer apply fusionnet stanford question answer dataset squad achieve first position single ensemble model official squad leaderboard time write october 4th two thousand and seventeen meanwhile verify generalization fusionnet two adversarial squad datasets set new state art datasets addsent fusionnet increase best f1 metric four hundred and sixty-six five hundred and fourteen addonesent fusionnet boost best f1 metric five hundred and sixty six hundred and seven
medical image widely use clinical practice diagnosis treatment report write error prone unexperienced physicians time consume tedious experience physicians address issue study automatic generation medical image report task present several challenge first complete report contain multiple heterogeneous form information include find tag second abnormal regions medical image difficult identify third port typically long contain multiple sentence cope challenge one build multi task learn framework jointly perform pre diction tag generation para graph two propose co attention mechanism localize regions contain abnormalities generate narrations three develop hierarchical lstm model generate long paragraph demonstrate effectiveness propose methods two publicly available datasets
word embeddings use vectors represent word geometry vectors capture semantic relationship word paper develop framework demonstrate temporal dynamics embed leverage quantify change stereotype attitudes toward women ethnic minorities 20th 21st centuries unite state integrate word embeddings train one hundred years text data yous census show change embed track closely demographic occupation shift time embed capture global social shift eg women movement 1960s asian immigration yous also illuminate specific adjectives occupations become closely associate certain populations time framework temporal analysis word embed open powerful new intersection machine learn quantitative social science
emfet open source flexible tool use extract large number feature email corpus email save eml format extract feature categorize three main group header feature payload body feature attachment feature purpose tool help practitioners researchers build datasets use train machine learn model spam detection far one hundred and forty feature extract use emfet emfet extensible easy use source code emfet publicly available github https githubcom wadeahijjawi emailfeaturesextraction
sentiment analysis one well know task fast grow research areas natural language process nlp text classifications technique become essential part wide range applications include politics business advertise market various techniques sentiment analysis recently word embeddings methods widely use sentiment classification task word2vec glove currently among accurate usable word embed methods convert word meaningful vectors however methods ignore sentiment information texts need huge corpus texts train generate exact vectors use input deep learn model result small size corpuses researcher often use pre train word embeddings train large text corpus google news one hundred billion word increase accuracy pre train word embeddings great impact sentiment analysis research paper propose novel method improve word vectors iwv increase accuracy pre train word embeddings sentiment analysis method base part speech pos tag techniques lexicon base approach word2vec glove methods test accuracy method via different deep learn model sentiment datasets experiment result show improve word vectors iwv effective sentiment analysis
paper propose adversarial process abstractive text summarization simultaneously train generative model g discriminative model particular build generator g agent reinforcement learn take raw text input predict abstractive summarization also build discriminator attempt distinguish generate summary grind truth summary extensive experiment demonstrate model achieve competitive rouge score state art methods cnn daily mail dataset qualitatively show model able generate abstractive readable diverse summaries
intelligent code completion become essential research task accelerate modern software development facilitate effective code completion dynamically type program languages apply neural language model learn large codebases develop tailor attention mechanism code completion however standard neural language model even attention mechanism correctly predict vocabulary oov word restrict code completion performance paper inspire prevalence locally repeat term program source code recently propose pointer copy mechanism propose pointer mixture network better predict oov word code completion base context pointer mixture network learn either generate within vocabulary word rnn component regenerate oov word local context pointer component experiment two benchmarked datasets demonstrate effectiveness attention mechanism pointer mixture network code completion task
paper present hybrid model combine neural conversational model rule base graph dialogue system assist users schedule reminders chat conversation graph base system high precision provide grammatically accurate response low recall neural conversation model cater variety request generate responses word word oppose use can responses hybrid system show significant improvements exist baseline system rule base approach cater complex query domain restrict neural model restrict conversation topic combination graph base retrieval system neural generative model make final system robust enough real world application
table text generation aim generate description factual table view set field value record encode content structure table propose novel structure aware seq2seq architecture consist field gate encoder description generator dual attention encode phase update cell memory lstm unit field gate correspond field value order incorporate field information table representation decode phase dual attention mechanism contain word level attention field level attention propose model semantic relevance generate description table conduct experiment textttwikibio dataset contain 700k biographies correspond infoboxes wikipedia attention visualizations case study show model capable generate coherent informative descriptions base comprehensive understand content structure table automatic evaluations also show model outperform baselines great margin code work available https githubcom tyliupku wiki2bio
review analysis large collections document periodic monitor new additions thereto greatly benefit new developments computer software paper demonstrate use random vectors construct low dimensional euclidean space embed word document enable fast accurate computation semantic similarities technique semantic technology assist review star document select compare classify summarize evaluate quickly minimal expert involvement high quality result
social network main resources gather information people opinion sentiments towards different topics spend hours daily social media share opinion technical paper show application sentimental analysis connect twitter run sentimental analysis query run experiment different query politics humanity show interest result realize neutral sentiments tweet significantly high clearly show limitations current work
visual question answer vqa attract much attention since offer insight relationships multi modal analysis image natural language current algorithms incapable answer open domain question require perform reason beyond image content address issue propose novel framework endow model capabilities answer complex question leverage massive external knowledge dynamic memory network specifically question along correspond image trigger process retrieve relevant information external knowledge base embed continuous vector space preserve entity relation structure afterwards employ dynamic memory network attend large body facts knowledge graph image perform reason facts generate correspond answer extensive experiment demonstrate model achieve state art performance visual question answer task also answer open domain question effectively leverage external knowledge
performance appraisal pa important hr process periodically measure evaluate every employee performance vis vis goals establish organization pa process involve purposeful multi step multi modal communication employees supervisors peer self appraisal supervisor assessment peer feedback analysis structure data text produce pa crucial measure quality appraisals track actual improvements paper apply text mine techniques produce insights pa text first perform sentence classification identify strengths weaknesses suggestions improvements find supervisor assessments use cluster discover broad categories among next use multi class multi label classification techniques match supervisor assessments predefined broad perspectives performance finally propose short text summarization technique produce summary peer feedback comment give employee compare manual summaries techniques illustrate use real life dataset supervisor assessment peer feedback text produce pa four thousand, five hundred and twenty-eight employees large multi national company
many task domains require robots interpret act upon natural language command give people refer robot physical surround interpretation know variously symbol ground problem ground semantics ground language acquisition problem challenge people employ diverse vocabulary grammar robots substantial uncertainty nature content surround make difficult associate constitutive language elements principally noun phrase spatial relations command text elements surround symbolic model capture linguistic structure scale successfully handle diverse language produce untrained users exist statistical approach better handle diversity date model complex linguistic structure limit achievable accuracy recent hybrid approach address limitations scale complexity effectively associate linguistic perceptual feature framework call generalize ground graph g3 address issue define probabilistic graphical model dynamically accord linguistic parse structure natural language command approach scale effectively handle linguistic diversity enable system associate part command specific object place events external world refer show robots learn word mean use learn mean robustly follow natural language command produce untrained users demonstrate approach mobility command mobile manipulation command involve variety semi autonomous robotic platforms include wheelchair micro air vehicle forklift willow garage pr2
information extraction textual document hospital record healthrelated user discussions become topic intense interest task medical concept cod map variable length text medical concepts correspond classification cod external system ontology work utilize recurrent neural network automatically assign icd ten cod fragment death certificate write english develop end end neural architectures directly tailor task include basic encoder decoder architecture statistical translation order incorporate prior knowledge concatenate cosine similarities vector among text dictionary entry encode state apply standard benchmark clef ehealth two thousand and seventeen challenge model achieve f measure eight thousand, five hundred and one full test set significant improvement compare average score six hundred and twenty-two official participants approach
distinctive linguistic practice help communities build solidarity differentiate outsiders online community one practice variation orthography include spell punctuation capitalization use dataset two million instagram post investigate orthographic variation community share pro eat disorder pro ed content find orthographic variation grow frequent time also become profound deep variants become increasingly distant original example anarexyia distant anarexia original spell anorexia change drive newcomers adopt extreme linguistic practice enter community moreover behavior correlate engagement newcomers adopt deeper orthographic variants tend remain active longer community post contain deeper variation receive positive feedback form like previous work link community membership change language change work cast connection new light newcomers drive evolve practice rather adapt also demonstrate utility orthographic variation new lens study sociolinguistic change online communities particularly change result exogenous force content ban
attention mechanism use ancillary mean help rnn cnn however transformer vaswani et al two thousand and seventeen recently record state art performance machine translation dramatic reduction train time solely use attention motivate transformer directional self attention network shen et al two thousand and seventeen fully attention base sentence encoder propose show good performance various data use forward backward directional information sentence study consider distance word important feature learn local dependency help understand context input text propose distance base self attention network consider word distance use simple distance mask order model local dependency without lose ability model global dependency attention inherent model show good performance nli data record new state art result snli data additionally show model strength long sentence document
rumour stance classification define classify stance specific social media post one support deny query comment earlier post become increase interest researchers previous work focus use individual tweet classifier input report performance sequential classifiers exploit discourse feature inherent social media interactions conversational thread test effectiveness four sequential classifiers hawk process linear chain conditional random field linear crf tree structure conditional random field tree crf long short term memory network lstm eight datasets associate break news stories look different type local contextual feature work shed new light development accurate stance classifiers show sequential classifiers exploit use discourse properties social media conversations use local feature outperform non sequential classifiers furthermore show lstm use reduce set feature outperform sequential classifiers performance consistent across datasets across type stances conclude work also analyse different feature study identify best help characterise distinguish stances support tweet likely accompany evidence deny tweet also set forth number directions future research
paper address question neural dialog systems generate short meaningless reply conjecture dialog system utterance may multiple equally plausible reply cause deficiency neural network dialog application propose systematic way mimic dialog scenario machine translation system manage reproduce phenomenon generate short less meaningful sentence translation set show evidence conjecture
digitalization store information hospitals allow exploitation medical data text format electronic health record ehrs initially gather purpose epidemiology manual search analysis operations data become tedious recent years use natural language process nlp tool highlight automatize extraction information contain ehrs structure perform statistical analysis structure information main difficulties exist approach requirement synonyms ontology dictionaries mostly available english include local custom notations work team compose oncologists domain experts data scientists develop custom nlp base system process structure textual clinical report patients suffer breast cancer tool rely combination standard text mine techniques advance synonym detection method allow global analysis retrieval indicators medical history tumor characteristics therapeutic responses recurrences prognosis versatility method allow obtain easily new indicators thus open way retrospective study substantial reduction amount manual work need biomedical annotators pre define ontologies language agnostic method reach good extraction accuracy several concepts interest accord comparison manually structure file without require exist corpus local new notations
propose label propagation base algorithm weakly supervise text classification construct graph document represent node edge weight represent similarities among document additionally discover underlie topics use latent dirichlet allocation lda enrich document graph include topics form additional nod edge weight topic text document represent level affinity approach require document level label instead expect manual label topic nod significantly minimize level supervision need topics observe enough achieve sufficiently high accuracy label propagation algorithm employ enrich graph propagate label among nod approach combine advantage label propagation document document similarities topic model minimal smart supervision demonstrate effectiveness approach various datasets compare state art weakly supervise text classification approach
stories tremendous power useful entertainment activate interest mobilize action degree story resonate audience may part reflect emotional journey take audience upon paper use machine learn methods construct emotional arc movies calculate families arc demonstrate ability certain arc predict audience engagement system apply hollywood film high quality short find web begin use deep convolutional neural network audio visual sentiment analysis model train new exist large scale datasets use compute separate audio visual emotional arc crowdsource annotations thirty second video clip extract highs low arc order assess micro level precision system precision measure term agreement polarity system predictions annotators rat annotations also use combine audio visual predictions next look macro level characterizations movies investigate whether exist universal shape emotional arc particular develop cluster approach discover distinct class emotional arc finally show sample corpus short web videos certain emotional arc statistically significant predictors number comment video receive result suggest emotional arc learn approach successfully represent macroscopic aspects video story drive audience engagement machine understand could use predict audience reactions video stories ultimately improve ability storytellers communicate
long range correlation property time series exhibit long term memory mainly study statistical physics domain report exist natural language use state art method analysis long range correlation first show occur long childes data set understand bayesian generative model language originally propose cognitive scientific domain investigate among representative model simon model find exhibit surprisingly good long range correlation pitman yor model since simon model know correctly reflect vocabulary growth natural language simple new model devise conjunct simon pitman yor model long range correlation hold correct vocabulary growth rate investigation overall suggest uniform sample one long range correlation could thus relation actual linguistic process
recurrent neural network long short term memory cell lstm rnn impressive ability sequence data process particularly language model build text classification research propose combination sentiment analysis new approach sentence vectors lstm rnn novel way sexual predator identification spi lstm rnn language model apply generate sentence vectors last hide state language model sentence vectors feed another lstm rnn classifier capture suspicious conversations hide state enable generate vectors sentence never see fasttext use filter content conversations generate sentiment score identify potential predators experiment achieve record break accuracy precision one hundred recall eight thousand, one hundred and ten exceed top rank result spi competition
modern virtual personal assistants provide convenient interface complete daily task via voice command important consideration assistants ability recover automatic speech recognition asr natural language understand nlu errors paper focus learn robust dialog policies recover errors end develop user simulator interact assistant voice command realistic scenarios noisy audio use learn dialog policies deep reinforcement learn show dialogs generate simulator indistinguishable human generate dialogs determine human evaluators furthermore preliminary experimental result show learn policies noisy environments achieve execution success rate fewer dialog turn compare fix rule base policies
natural language process task performance model often measure non differentiable metric bleu score use efficient gradient base methods optimization common workaround optimize surrogate loss function approach effective optimization loss also result improve target metric correspond problem refer loss evaluation mismatch present work propose method calculation differentiable lower bind expect bleu score involve computationally expensive sample procedure one require use reinforce rule reinforcement learn rl framework
present new workflow create components marytts text speech synthesis platform popular researchers developers extend support new languages custom synthetic voice workflow replace previous toolkit efficient flexible process leverage modern build automation cloud host infrastructure moreover compatible update marytts architecture enable new feature state art paradigms synthesis base deep neural network dnns like marytts new tool free open source software foss promote use open data
phonetic segmentation process split speech distinct phonetic units human experts routinely perform task manually analyze auditory visual cue use analysis software extremely time consume process methods exist automatic segmentation always accurate enough order improve automatic segmentation need model close manual segmentation possible corpus effort capture human segmentation behavior record experts perform segmentation task believe data enable us highlight important aspects manual segmentation use automatic segmentation improve accuracy
sequence sequence model soft attention successfully apply wide variety problems decode process incur quadratic time space cost inapplicable real time sequence transduction address issue propose monotonic chunkwise attention mocha adaptively split input sequence small chunk soft attention compute show model utilize mocha train efficiently standard backpropagation allow online linear time decode test time apply online speech recognition obtain state art result match performance model use offline soft attention mechanism document summarization experiment expect monotonic alignments show significantly improve performance compare baseline monotonic attention base model
universal first letter law fll derive describe predict percentages first letter word novels fll akin benford law bl first digits predict percentages first digits data collection number universal sense fll depend number letter alphabet whereas bl depend number digits base number system existence type universal laws appear counter intuitive nonetheless describe data well relations earlier work give fll predict english author average start sixteen one hundred word english letter corroborate data yet author freely write anything fuller implications applicability fll remain future
present novel framework find complex activities match user describe query clutter surveillance videos wide diversity query couple unavailability annotate activity data limit ability train activity model bridge semantic gap propose let users describe activity semantic graph object attribute inter object relationships associate nod edge respectively learn node edge level visual predictors train test time propose retrieve activity identify likely locations match semantic graph formulate novel crf base probabilistic activity localization objective account mis detections mis classifications track losses output likelihood score candidate ground location query video seek ground maximize overall precision recall handle combinatorial search high probability ground propose highest precision subgraph match algorithm method outperform exist retrieval methods benchmarked datasets
propose efficient method generate white box adversarial examples trick character level neural classifier find manipulations need greatly decrease accuracy method rely atomic flip operation swap one token another base gradients one hot input vectors due efficiency method perform adversarial train make model robust attack test time use semantics preserve constraints demonstrate hotflip adapt attack word level classifier well
ideas forensic linguistics use frequently natural language process nlp use machine learn techniques role forensic linguistics benign earlier use purpose questionable certain methods forensic linguistics employ without consider scientific limitations ethical concern take specific case forensic linguistics example trend nlp machine learn issue larger one present many scientific data drive domains suggest trend indicate apply sciences exceed legal scientific brief highlight carelessly implement practice serve short circuit due process law well breach ethical cod
knowledge base kb completion aim infer miss facts exist ones kb among various approach path rank pr algorithms receive increase attention recent years pr algorithms enumerate paths entity pair kb use paths feature train model miss fact prediction due good performances high model interpretability several methods propose however exist methods suffer scalability high ram consumption feature explosion train exponentially large number feature problems paper propose context aware path rank c pr algorithm solve problems introduce selective path exploration strategy c pr learn global semantics entities kb use word embed leverage knowledge entity semantics enumerate contextually relevant paths use bidirectional random walk experimental result three large kbs show path feature fewer number discover c pr improve predictive performance also interpretable exist baselines
exist study information diffuse across social network thus far concentrate analyse recover spread deterministic innovations urls hashtags group membership however investigate mention real world entities appear spread yet explore largely due computationally intractable nature perform large scale entity extraction paper present best knowledge one first piece work closely examine diffusion name entities social media use reddit case study platform first investigate name entities accurately recognise extract discussion post use extract entities study pattern entity cascade probability user adopt entity ie mention associate exposures entity put piece together present parallelised diffusion model forecast probability entity adoption find influence adoption users characterise prior interactions oppose whether users propagate entity adoptions beforehand find important implications researchers study influence language community analysts wish understand entity level influence dynamics
improve quality conversations online communities attract considerable attention recently engage urbane reactive online conversations critical effect social life internet users study particularly interest identify post multi party conversation unlikely reply therefore kill thread conversation purpose propose deep learn model call convernet convernet attractive due capability model internal structure long conversation appropriate encode contextual information conversation effective integration attention mechanisms empirical experiment real world datasets demonstrate effectiveness proposal model widely concern topic analysis also offer implications improve quality user experience online conversations
traditional neural network approach traffic flow forecast usually single task learn stl model take advantage information provide relate task contrast stl multitask learn mtl potential improve generalization transfer information train signal extra task paper mtl base neural network use traffic flow forecast neural network mtl backpropagation bp network construct incorporate traffic flow several contiguous time instants output layer nod output layer see output different closely relate stl task comprehensive experiment urban vehicular traffic flow data comparisons stl show mtl bp neural network promise effective approach traffic flow forecast
opinion mine refer use natural language process text analysis computational linguistics identify extract subjective information textual material opinion mine also know sentiment analysis receive lot attention recent time provide number tool analyse public opinion number different topics comparative opinion mine subfield opinion mine deal identify extract information express comparative form egpaper x better comparative opinion mine play important role ones try evaluate something provide reference point comparison paper provide review area comparative opinion mine first review cover specifically topic previous review deal mostly general opinion mine survey cover comparative opinion mine two different angle one perspective techniques perspective comparative opinion elements also incorporate preprocessing tool well dataset use past researchers useful future researchers field comparative opinion mine
minimal construct language conlang useful experiment comfortable make tool toki pona tp conlang minimal vocabulary fourteen letter one hundred and twenty-four lemmas ten syntax rule language useful use somewhat establish minimal conlang least hundreds fluent speakers article expose current concepts resources tp make available python vim script routines analysis language synthesis texts syntax highlight scheme achievement preliminary tp wordnet focus analysis basic vocabulary corpus analyse find synthesis base sentence templates relate context keep track use word render larger texts use fix number phonemes eg poems number sentence word letter eg paragraph syntax highlight reflect morphosyntactic class give official dictionary different solutions describe implement well establish vim text editor tentative tp wordnet make available three pattern relations synsets word lemmas summary text hold potentially novel conceptualizations tool result analyze synthesize syntax highlight tp language
recent literature end end speech systems often refer letter base acoustic model train sequence sequence manner either via recurrent model via structure output learn approach ctc contrast traditional phone senone base approach end end approach alleviate need word pronunciation model require force alignment step train time phone base approach remain however state art classical benchmarks paper propose letter base speech recognition system leverage convnet acoustic model key ingredients convnet gate linear units high dropout convnet train map audio sequence correspond letter transcriptions either via classical ctc approach via recent variant call asg couple simple decoder inference time system match best exist letter base systems wsj word error rate show near state art performance librispeech
propose topic compositional neural language model tcnlm novel method design simultaneously capture global semantic mean local word order structure document tcnlm learn global semantic coherence document via neural topic model probability learn latent topic use build mixture experts moe language model expert correspond one topic recurrent neural network rnn account learn local structure word sequence order train moe model efficiently matrix factorization method apply extend weight matrix rnn ensemble topic dependent weight matrices degree member ensemble use tie document dependent probability correspond topics experimental result several corpora show propose approach outperform pure rnn base model topic guide language model model yield sensible topics also capacity generate meaningful sentence condition give topics
study aim identify moments rudeness two individuals particular segment occurrences rudeness conversations three broad distinct categories try identify show machine learn algorithms use identify rudeness base acoustic semantic signal extract conversations furthermore make note shortcomings task highlight make problem inherently difficult finally provide next step need ensure success identify rudeness conversations
latent semantic analysis lsa word2vec widely use word embeddings despite popularity techniques precise mechanisms acquire new semantic relations word remain unclear present article investigate whether lsa word2vec capacity identify relevant semantic dimension increase size corpus one intuitive hypothesis capacity identify relevant dimension increase amount data increase however corpus size grow topics specific domain interest signal noise ratio may weaken set examine distinguish alternative hypothesis investigate effect corpus specificity size word embeddings study two ways progressive elimination document elimination random document vs elimination document unrelated specific task show word2vec take advantage document obtain best performance train whole corpus contrary specialization removal domain document train corpus accompany decrease dimensionality increase lsa word representation quality speed process time furthermore show specialization without decrease lsa dimensionality produce strong performance reduction specific task cognitive model point view point lsa word knowledge acquisitions may efficiently exploit higher order co occurrences global relations whereas word2vec
cross lingual plagiarism clp occur texts write one language translate different language use without acknowledge original source one common methods detect clp require online machine translators google microsoft translate always available give plagiarism detection typically involve large document comparison amount translations require would overwhelm online machine translator especially detect plagiarism web addition translate texts replace synonyms use online machine translators detect clp would result poor performance paper address problem cross lingual plagiarism detection clpd propose model use simulate word embeddings reproduce predictions online machine translator google translate detect clp simulate embeddings comprise translate word different languages map common space replicate increase prediction probability retrieve translations word synonyms model unlike exist model propose model require parallel corpora accommodate multiple languages multi lingual demonstrate effectiveness propose model detect clp standard datasets contain clp case evaluate performance state art baseline rely online machine translator tma model evaluation result reveal propose model effective detect clp outperform baseline result indicate clp could detect state art performances leverage prediction accuracy internet translator word embeddings without rely internet translators
paraphrase plagiarism one difficult challenge face plagiarism detection systems paraphrase occur texts lexically syntactically alter look different retain original mean plagiarism detection systems many commercial base design detect word co occurrences light modifications unable detect severe semantic structural alterations see many academic document hence many paraphrase plagiarism case go undetected paper approach problem paraphrase plagiarism propose methods detect common techniques phenomena use paraphrase texts namely lexical substitution insertion deletion word phrase reorder combine methods paraphrase detection model evaluate propose methods model collections contain paraphrase texts experimental result show significant improvement performance methods combine propose model oppose run individually result also show propose paraphrase detection model outperform standard baseline base greedy string till previous study
number quality user review greatly affect consumer purchase decisions review languages increase still often case especially non english speakers review person first language use online experiment examine value potential purchasers receive interfaces show additional review second language result paint complicate picture positive negative reactions inclusion foreign language review roughly twenty-six twenty-eight subject click see translations foreign language content give opportunity likely select product foreign language review
standardize corpora undeciphered script necessary start point computational epigraphy require laborious human effort preparation raw archaeological record automate process machine learn algorithms significant aid epigraphical research take first step direction present deep learn pipeline take input image undeciphered indus script find archaeological artifacts return output string graphemes suitable inclusion standard corpus image first decompose regions use selective search regions classify contain textual graphical information use convolutional neural network regions classify potentially contain text hierarchically merge trim remove non textual information remain textual part image segment use standard image process techniques isolate individual graphemes set finally pass second convolutional neural network classify graphemes base standard corpus classifier identify presence absence frequent indus grapheme jar sign accuracy ninety-two result demonstrate great potential deep learn approach computational epigraphy generally digital humanities
present case study demonstrate usefulness bayesian hierarchical mixture model investigate cognitive process sentence comprehension widely assume distance linguistic co dependents affect latency dependency resolution longer distance longer retrieval time distance base account alternative theory direct access assume retrieval time mixture two distributions one distribution represent successful retrievals independent dependency distance represent initial failure retrieve correct dependent follow reanalysis lead successful retrieval implement model bayesian hierarchical model show direct access model explain chinese relative clause read time data better distance account
attention network prove effective approach embed categorical inference within deep neural network however many task may want model richer structural dependencies without abandon end end train work experiment incorporate richer structural distributions encode use graphical model within deep network show structure attention network simple extensions basic attention procedure allow extend attention beyond standard soft selection approach attend partial segmentations subtrees experiment two different class structure attention network linear chain conditional random field graph base parse model describe model practically implement neural network layer experiment show approach effective incorporate structural bias structure attention network outperform baseline attention model variety synthetic real task tree transduction neural machine translation question answer natural language inference find model train way learn interest unsupervised hide representations generalize simple attention
show faceted search use combination traditional classification systems mix membership topic model go beyond keyword search inform resource discovery hypothesis formulation argument extraction interdisciplinary research test domain history philosophy scientific work animal mind cognition methods generalize research areas ultimately support system semi automatic identification argument structure provide case study application methods problem identify extract arguments anthropomorphism critical period development comparative psychology show combination classification systems mix membership model train large digital libraries inform resource discovery domain novel approach drill topic model simultaneously reduce size corpus unit analysis able reduce large collection fulltext volumes much smaller set page within six focal volumes contain arguments interest historians philosophers comparative psychology volumes identify way appear among first ten result keyword search hathitrust digital library page bear kind close read need generate original interpretations heart scholarly work humanities zoom back provide way place book onto map science originally construct different data different purpose multilevel approach advance understand intellectual societal contexts write interpret
present visually ground model speech perception project speak utterances image joint semantic space use multi layer recurrent highway network model temporal nature speak speech show learn extract form mean base linguistic knowledge input signal carry depth analysis representations use different components train model show encode semantic aspects tend become richer go hierarchy layer whereas encode form relate aspects language input tend initially increase plateau decrease
report investigations speaker classification larger quantities unlabelled speech data use small set manually phonemically annotate speech kohonen speech typewriter semi supervise method comprise self organise map soms achieve low phoneme error rat som 2d array cells learn vector representations data base neighbourhoods paper report method evaluate pronunciation use multilevel soms hvd single syllable utterances study vowels australian pronunciation
real world dna unique many people share name phenomenon often cause erroneous aggregation document multiple persons namesake one another mistake deteriorate performance document retrieval web search seriously improper attribution credit blame digital forensic resolve issue name disambiguation task design aim partition document associate name reference partition contain document pertain unique real life person exist solutions task substantially rely feature engineer biographical feature extraction construction auxiliary feature wikipedia however many scenarios feature may costly obtain unavailable due risk privacy violation work propose novel name disambiguation method propose method non intrusive privacy instead use attribute pertain real life person method leverage relational data form anonymized graph methodological aspect propose method use novel representation learn model embed document low dimensional vector space name disambiguation solve hierarchical agglomerative cluster algorithm experimental result demonstrate propose method significantly better exist name disambiguation methods work similar set
although deep learn model prove effective solve problems natural language process mechanism come conclusions often unclear result model generally treat black box yield insight underlie learn pattern paper consider long short term memory network lstms demonstrate new approach track importance give input lstm give output identify consistently important pattern word able distill state art lstms sentiment analysis question answer set representative phrase representation quantitatively validate use extract phrase construct simple rule base classifier approximate output lstm
recent research psycholinguistics provide increase evidence humans predict upcoming content prediction also affect perception might key robustness human language process paper investigate factor affect human prediction build computational model predict upcoming discourse referents base linguistic knowledge alone vs linguistic knowledge jointly common sense knowledge form script find script knowledge significantly improve model estimate human predictions second study test highly controversial hypothesis predictability influence refer expression type find evidence effect
social media network phenomenon lead massive amount valuable data available online easy access many users share image videos comment review news opinions different social network sit twitter one popular ones data collect twitter highly unstructured extract useful information tweet challenge task twitter huge number arabic users mostly post write tweet use arabic language lot research sentiment analysis english amount research datasets arabic language limit paper introduce arabic language dataset opinions health service collect twitter paper first detail process collect data twitter also process filter pre process annotate arabic text order build big sentiment analysis dataset arabic several machine learn algorithms naive bay support vector machine logistic regression alongside deep convolutional neural network utilize experiment sentiment analysis health dataset
usually bilingual word vectors train online mikolov et al show also find offline whereby two pre train embeddings align linear transformation use dictionaries compile expert knowledge work prove linear transformation two space orthogonal transformation obtain use singular value decomposition introduce novel invert softmax identify translation pair improve precision one mikolov original map thirty-four forty-three translate test set compose common rare english word italian orthogonal transformations robust noise enable us learn transformation without expert bilingual signal construct pseudo dictionary identical character string appear languages achieve forty precision test set finally extend method retrieve true translations english sentence corpus 200k italian sentence precision one sixty-eight
neural language model predict next token use latent representation immediate token history recently various methods augment neural language model attention mechanism differentiable memory propose predict next token model query information memory recent history facilitate learn mid long range dependencies however conventional attention mechanisms use memory augment neural language model produce single output vector per time step vector use predict next token well key value differentiable memory token history paper propose neural language model key value attention mechanism output separate representations key value differentiable memory well encode next word distribution model outperform exist memory augment neural language model two corpora yet find method mainly utilize memory five recent output representations lead unexpected main find much simpler model base concatenation recent output representations previous time step par sophisticate memory augment neural language model
truncate back propagation time bptt popular approach train recurrent neural network rnns suffer inherently sequential make parallelization difficult truncate gradient flow distant time step investigate whether target propagation tprop style approach address shortcomings unfortunately extensive experiment suggest tprop generally underperform bptt end analysis phenomenon suggestions future work
people refer quantities visual scene use either exact cardinals eg one two three natural language quantifiers eg humans two process underlie fairly different cognitive neural mechanisms inspire evidence present study propose two model learn objective mean cardinals quantifiers visual scenes contain multiple object show model capitalize fuzzy measure similarity effective learn quantifiers whereas learn exact cardinals better accomplish information number provide
since events arab spring increase interest use social media anticipate social unrest efforts make toward automate unrest prediction focus filter vast volume tweet identify tweet relevant unrest provide downstream users analysis train supervise classifier able label arabic language tweet relevant unrest high reliability examine relationship train data size performance investigate ways optimize model build process minimize cost also explore confidence thresholds set achieve desire level performance
paper describe dynamic normalization process apply social network multilingual document facebook twitter improve performance author profile task short texts normalization process n grams character n grams pos tag obtain extract possible stylistic information encode document emoticons character flood capital letter reference users hyperlinks hashtags etc experiment svm show ninety performance
study online social influence demonstrate friends important effect many type behavior wide variety settings however know much less influence work among relative strangers digital public square despite important conversations happen space present result study large public facebook page randomly use two different methods recent social feedback order comment post find social feedback condition result higher quality view comment response comment measure average quality comment write users study find social feedback positive effect response quality low high quality commenters draw theoretical framework social norms explain empirical result order examine influence mechanism measure similarity comment view write study find similarity increase highest quality contributors social feedback condition suggest addition norms individuals may respond increase relevance high quality comment
visual question answer vqa witness great progress since may two thousand and fifteen classic problem unify visual textual data system many enlighten vqa work explore deep image question encode fuse methods attention effective infusive mechanism current attention base methods focus adequate fusion visual textual feature lack attention people focus ask question image traditional attention base methods attach single value feature spatial location losses many useful information remedy problems propose general method perform saliency like pre selection overlap region feature interrelation bidirectional lstm bilstm use novel element wise multiplication base attention method capture competent correlation information visual textual feature conduct experiment large scale coco vqa dataset analyze effectiveness model demonstrate strong empirical result
last years microblogging platforms twitter give rise deluge textual data use analysis informal communication millions individuals work propose information theoretic approach geographic language variation use corpus base twitter test model tens concepts associate keywords detect spanish tweet geolocated spain employ dialectometric measure cosine similarity jensen shannon divergence quantify linguistic distance lexical level cells create uniform grid map do single concept general case take account average consider variants latter permit analysis dialects naturally emerge data interestingly result reveal existence two dialect macrovarieties first group include region specific speech speak small towns rural areas whereas second cluster encompass cities tend use uniform variety since result obtain two different metrics qualitatively agree work suggest social media corpora efficiently use dialectometric analyse
mental health forums online communities people express issue seek help moderators users forums often post severe content indicate user acute distress risk attempt self harm moderators need respond severe post timely manner prevent potential self harm however large volume daily post content make difficult moderators locate respond critical post present framework triaging user content four severity categories define base indications self harm ideation model base feature rich classification framework include lexical psycholinguistic contextual topic model feature approach improve state art triaging content severity mental health forums large margins seventeen improvement f one score use propose model analyze mental state users show overall long term users forum demonstrate decrease severity risk time analysis interaction moderators users indicate without automatic way identify critical content indeed challenge moderators provide timely response users need
topic model provide us insight underlie latent structure large corpus document range methods propose literature include probabilistic topic model techniques base matrix factorization however case standard implementations rely stochastic elements initialization phase potentially lead different result generate corpus use parameter value correspond concept instability previously study context k mean cluster many applications topic model problem instability consider topic model treat definitive even though result may change considerably initialization process alter paper demonstrate inherent instability popular topic model approach use number new measure assess stability address issue context matrix factorization topic model propose use ensemble learn strategies base experiment perform annotate text corpora show k fold ensemble strategy combine ensembles structure initialization significantly reduce instability simultaneously yield accurate topic model
word embed model offer continuous vector representations capture rich contextual semantics base word co occurrence pattern word vectors provide effective feature use many nlp task cluster similar word infer learn relationships many challenge open research question remain paper propose solution align variations model different model joint low dimensional latent space leverage carefully generate synthetic data point generative process inspire observation variety linguistic relationships capture simple linear operations embed space demonstrate approach lead substantial improvements recover embeddings local neighborhoods
present deep voice production quality text speech system construct entirely deep neural network deep voice lay groundwork truly end end neural speech synthesis system comprise five major build block segmentation model locate phoneme boundaries grapheme phoneme conversion model phoneme duration prediction model fundamental frequency prediction model audio synthesis model segmentation model propose novel way perform phoneme boundary detection deep neural network use connectionist temporal classification ctc loss audio synthesis model implement variant wavenet require fewer parameters train faster original use neural network component system simpler flexible traditional text speech systems component require laborious feature engineer extensive domain expertise finally show inference system perform faster real time describe optimize wavenet inference kernels cpu gpu achieve 400x speedups exist implementations
introduce ai rationalization approach generate explanations autonomous system behavior human perform behavior describe rationalization technique use neural machine translation translate internal state action representations autonomous agent natural language evaluate technique frogger game environment train autonomous game play agent rationalize action choices use natural language natural language train corpus collect human players think loud play game motivate use rationalization approach explanation generation show result two experiment evaluate effectiveness rationalization result evaluations show neural machine translation able accurately generate rationalizations describe agent behavior rationalizations satisfy humans alternative methods explanation
despite successes capture continuous distributions application generative adversarial network gans discrete settings like natural language task rather restrict fundamental reason difficulty back propagation discrete random variables combine inherent instability gin train objective address problems propose maximum likelihood augment discrete generative adversarial network instead directly optimize gin objective derive novel low variance objective use discriminator output follow correspond log likelihood compare original new objective prove consistent theory beneficial practice experimental result various discrete datasets demonstrate effectiveness propose approach
recent work generative model text find variational auto encoders vae incorporate lstm decoders perform worse simpler lstm language model bowman et al two thousand and fifteen negative result far poorly understand attribute propensity lstm decoders ignore condition information encoder paper experiment new type decoder vae dilate cnn change decoder dilation architecture control effective context previously generate word experiment find trade contextual capacity decoder amount encode information use show right decoder vae outperform lstm language model demonstrate perplexity gain two datasets represent first positive experimental result use vae generative model text conduct depth investigation use vae new decode architecture semi supervise unsupervised label task demonstrate gain several strong baselines
design 3d scenes currently creative task require significant expertise effort use complex 3d design interfaces effortful design process start stark contrast easiness people use language describe real imaginary environments present sceneseer interactive text 3d scene generation system allow user design 3d scenes use natural language user provide input text extract explicit constraints object appear scene give explicit constraints system use spatial knowledge base learn exist database 3d scenes 3d object model infer arrangement object form natural scene match input description use textual command user iteratively refine create scene add remove replace manipulate object evaluate quality 3d scenes generate sceneseer perceptual evaluation experiment compare manually design scenes simpler baselines 3d scene generation demonstrate generate scenes iteratively refine simple natural language command
exist sequence label model rely fix decomposition target sequence sequence basic units methods suffer two major drawbacks one set basic units fix set word character phonemes speech recognition two decomposition target sequence fix drawbacks usually result sub optimal performance model sequence pa per extend popular ctc loss criterion alleviate limitations propose new loss function call gram ctc preserve advantage ctc gram ctc automatically learn best set basic units grams well suitable decomposition tar get sequence unlike ctc gram ctc allow model output variable number character time step enable model capture longer term dependency improve computational efficiency demonstrate propose gram ctc improve ctc term performance efficiency large vocabulary speech recognition task multiple scale data gram ctc outperform state art standard speech benchmark
task orient dialog systems apply various task automate personal assistants customer service providers tutor systems work well users clear explicit intentions well align systems capabilities however fail users intentions explicit address shortcoming propose framework interleave non task content ie everyday social conversation task conversations task content fail system still keep user engage non task content train policy use reinforcement learn algorithms promote long turn conversation coherence consistency system smooth transition task non task content test effectiveness propose framework develop movie promotion dialog system experiment human users indicate system interleave social task content achieve better task success rate also rat engage compare pure task orient system
work open dawt dataset densely annotate wikipedia texts across multiple languages annotations include label text mention map entities represent freebase machine ids well type entity data set contain total 136m article 50b tokens 138m mention entity co occurrences dawt contain forty-eight time anchor text entity link originally present wikipedia markup moreover span several languages include english spanish italian german french arabic also present methodology use generate dataset enrich wikipedia markup order increase number link addition main dataset open several derive datasets include mention entity co occurrence count entity embeddings well mappings freebase ids wikidata item ids also discuss two applications datasets hope open would prove useful natural language process information retrieval communities well facilitate multi lingual research
generic generation manipulation text challenge limit success compare recent deep generative model visual domain paper aim generate plausible natural language sentence whose attribute dynamically control learn disentangle latent representations designate semantics propose new neural generative model combine variational auto encoders holistic attribute discriminators effective imposition semantic structure differentiable approximation discrete text sample explicit constraints independent attribute control efficient collaborative learn generator discriminators model learn highly interpretable representations even word annotations produce realistic sentence desire attribute quantitative evaluation validate accuracy sentence attribute generation
propose general approach model semi supervise learn ssl algorithms specifically present declarative language model traditional supervise classification task many ssl heuristics include well know heuristics co train novel domain specific heuristics addition represent individual ssl heuristics show multiple heuristics automatically combine use bayesian optimization methods experiment two class task link base text classification relation extraction show modest improvements well study link base classification benchmarks state art result relation extraction task two realistic domains
tutorial introduce new powerful set techniques variously call neural machine translation neural sequence sequence model techniques use number task regard handle human language powerful tool toolbox anyone want model sequential data sort tutorial assume reader know basics math program assume particular experience neural network natural language process attempt explain intuition behind various methods cover delve enough mathematical detail understand concretely culiminates suggestion implementation exercise readers test understand content practice
able interact better humans crucial machine understand sound primary modality human perception previous work use sound learn embeddings improve generic textual similarity assessment work treat sound first class citizen study downstream textual task require aural ground end propose sound word2vec new embed scheme learn specialize word embeddings ground sound example learn two seemingly semantically unrelated concepts like leave paper similar due similar rustle sound make embeddings prove useful textual task require aural reason like text base sound retrieval discover foley sound effect use movies moreover embed space capture interest dependencies word onomatopoeia outperform prior work aurally relevant word relatedness datasets amen aslex
content today social media become rich increasingly mix text image videos audio intrigue research question model interplay different modes attract user attention engagement order pursue study multimodal content must also account context time effect community preferences social factor eg author already popular also affect amount feedback reaction social media post receive work separate influence non content factor several ways first focus rank pair submissions post community quick succession eg within thirty second frame encourage model focus time agnostic community specific content feature within set determine relative performance author vs content feature find victory usually belong cat caption visual textual feature together tend outperform identity base feature moreover experiment show consider isolation simple unigram text feature deep neural network visual feature yield highest accuracy individually combination two modalities generally lead best accuracies overall
empirically characterize performance discriminative generative lstm model text classification find although rnn base generative model powerful bag word ancestors eg account conditional dependencies across word document higher asymptotic error rat discriminatively train rnn model however also find generative model approach asymptotic error rate rapidly discriminative counterparts pattern ng jordan two thousand and one prove hold linear classification model make naive conditional independence assumptions build find hypothesize rnn base generative classification model robust shift data distribution hypothesis confirm series experiment zero shoot continual learn settings show generative model substantially outperform discriminative model
paper present novel approach multi lingual sentiment classification short texts challenge task amount train data languages english limit previously propose multi lingual approach typically require establish correspondence english powerful classifiers already available contrast method require supervision leverage large amount weakly supervise data various languages train multi layer convolutional network demonstrate importance use pre train network thoroughly evaluate approach various multi lingual datasets include recent semeval two thousand and sixteen sentiment prediction benchmark task four achieve state art performance also compare performance model train individually language variant train languages show latter model reach slightly worse still acceptable performance compare single language model benefit better generalization properties across languages
recent tremendous success unsupervised word embeddings multitude applications raise obvious question similar methods could derive improve embeddings ie semantic representations word sequence well present simple efficient unsupervised objective train distribute representations sentence method outperform state art unsupervised model benchmark task highlight robustness produce general purpose sentence embeddings
paper propose new model extract interpretable sentence embed introduce self attention instead use vector use two matrix represent embed row matrix attend different part sentence also propose self attention mechanism special regularization term model side effect embed come easy way visualize specific part sentence encode embed evaluate model three different task author profile sentiment classification textual entailment result show model yield significant performance gain compare sentence embed methods three task
nowadays big part people rely available content social media decisions eg review feedback topic product possibility anybody leave review provide golden opportunity spammers write spam review products service different interest identify spammers spam content hot topic research although considerable number study do recently toward end far methodologies put forth still barely detect spam review none show importance extract feature type study propose novel framework name netspam utilize spam feature model review datasets heterogeneous information network map spam detection procedure classification problem network use importance spam feature help us obtain better result term different metrics experiment real world review datasets yelp amazon websites result show netspam outperform exist methods among four categories feature include review behavioral user behavioral reviewlinguistic user linguistic first type feature perform better categories
overall program objective provide natural ways soldier interact communicate robots much like soldier communicate soldier today describe wizard oz woz method apply multimodal human robot dialogue collaborative exploration task woz method help design robot behaviors traditional approach place burden decisions single wizard work consider two wizards stand robot navigation dialogue management software components scenario use elicit data one human robot team task explore unknown environment human give verbal instructions remote location robot follow clarify possible misunderstand need via dialogue find division labor wizards workable hold promise future software development
examine memory network task question answer qa common real world scenario train examples scarce weakly supervise scenario extrinsic label available train propose extensions dynamic memory network dmn specifically within attention mechanism call result neural architecture dynamic memory tensor network dmtn ultimately see propose extensions result eighty improvement number task pass baselined standard dmn twenty task pass compare state art end end memory network facebook single task weakly train 1k babi dataset
ungrammatical sentence key cabinets table know lead illusion grammaticality discuss meta analysis jaeger et al two thousand and seventeen faster read time observe verb agreement attraction sentence compare equally ungrammatical sentence key cabinet table one explanation facilitation effect feature percolation account plural feature cabinets percolate head noun key lead illusion alternative account term cue base retrieval lewis vasishth two thousand and five assume non subject noun cabinets misretrieved due partial feature match dependency completion process auxiliary initiate memory access subject plural mark present evidence yet another explanation observe facilitation second sentence two nouns identical number possible proportion trials difficult keep distinct lead slower read time verb first sentence feature overwrite account nairne one thousand, nine hundred and ninety show feature overwrite proposal implement finite mixture process reanalysed ten publish data set fit hierarchical bayesian mixture model data assume two mixture distribution show nine ten study mixture distribution correspond feature overwrite furnish superior fit feature percolation cue base retrieval account
entity disambiguation link edl task match entity mention text unique knowledge base kb identifier wikipedia freebase id play critical role construction high quality information network leverage variety information retrieval nlp task text categorization document tag edl complex challenge problem due ambiguity mention real world text multi lingual moreover edl systems need high throughput lightweight order scale large datasets run shelf machine importantly systems need able extract disambiguate dense annotations data order enable information retrieval extraction task run data efficient accurate order address challenge present lithium edl system algorithm high throughput lightweight language agnostic edl system extract correctly disambiguate seventy-five entities state art edl systems significantly faster
recent development large scale question answer qa datasets trigger substantial amount research end end neural architectures qa increasingly complex systems conceive without comparison simpler neural baseline systems would justify complexity work propose simple heuristic guide development neural baseline systems extractive qa task find two ingredients necessary build high perform neural qa system first awareness question word process context second composition function go beyond simple bag word model recurrent neural network result show fastqa system meet two requirements achieve competitive performance compare exist model argue surprise find put result previous systems complexity recent qa datasets perspective
keyword spot kws constitute major component human technology interfaces maximize detection accuracy low false alarm fa rate minimize footprint size latency complexity goals kws towards achieve study convolutional recurrent neural network crnns inspire large scale state art speech recognition systems combine strengths convolutional layer recurrent layer exploit local structure long range context analyze effect architecture parameters propose train strategies improve performance 230k parameters crnn model yield acceptably low latency achieve nine thousand, seven hundred and seventy-one accuracy five fa hour five db signal noise ratio
deep learn model dlms state art techniques speech recognition however train good dlms time consume especially production size model corpora although several parallel train algorithms propose improve train efficiency clear guidance one choose task hand due lack systematic fair comparison among paper aim fill gap compare four popular parallel train algorithms speech recognition namely asynchronous stochastic gradient descent asgd blockwise model update filter bmuf bulk synchronous parallel bsp elastic average stochastic gradient descent easgd one thousand hour librispeech corpora use fee forward deep neural network dnns convolutional long short term memory dnns cldnns base experiment recommend use bmuf top choice train acoustic model since stable scale well number gpus achieve reproducible result many case even outperform single gpu sgd asgd use substitute case
many exist methods learn joint embed image text use supervise information pair image textual attribute take advantage recent success unsupervised learn deep neural network propose end end learn framework able extract robust multi modal representations across domains propose method combine representation learn model ie auto encoders together cross domain learn criteria ie maximum mean discrepancy loss learn joint embeddings semantic visual feature novel technique unsupervised data adaptation inference introduce construct comprehensive embeddings label unlabeled data evaluate method animals attribute caltech ucsd bird two hundred two thousand and eleven dataset wide range applications include zero shoot image recognition retrieval inductive transductive settings empirically show framework improve current state art many consider task
present work build global long tail rank entities across multiple languages use wikipedia freebase knowledge base identify multiple feature build model rank entities use grind truth dataset ten thousand label final system rank twenty-seven million entities seventy-five precision forty-eight f1 score provide performance evaluation empirical evidence quality rank across languages open final rank list future research
introduce first goal drive train visual question answer dialog agents specifically pose cooperative image guess game two agents qbot abot communicate natural language dialog qbot select unseen image lineup image use deep reinforcement learn rl learn policies agents end end pixels multi agent multi round dialog game reward demonstrate two experimental result first sanity check demonstration pure rl scratch show result synthetic world agents communicate ungrounded vocabulary ie symbols pre specify mean x z find two bots invent communication protocol start use certain symbols ask answer certain visual attribute shape color style thus demonstrate emergence ground language communication among visual dialog agents human supervision second conduct large scale real image experiment visdial dataset pretrain supervise dialog data show rl fine tune agents significantly outperform sl agents interestingly rl qbot learn ask question abot good ultimately result informative dialog better team
elaborate quantum model mean associate corpora write document like page form world wide web end guide physicists construct quantum theory microscopic entities unlike classical object fully represent spatial theater suggest similar construction need carry linguists computational scientists capture full mean carry collections documental entities precisely show associate quantum like entity mean language entity form print document consider latter collection trace leave former specific result search action describe measurements word offer perspective collection document like web describe space manifestation complex entity qweb object model draw inspiration previous study operational realistic approach quantum physics quantum model human cognition decision make emphasize consistent qweb model need account observe correlations word appear print document eg co occurrences latter would depend mean connections exist concepts associate word respect show context interference quantum effect require explain probabilities calculate count relative number document contain certain word co ocurrrences word
language understand key component speak dialogue system paper investigate language understand module influence dialogue system performance conduct series systematic experiment task orient neural dialogue system reinforcement learn base set empirical study show among different type language understand errors slot level errors impact overall performance dialogue system compare intent level errors addition experiment demonstrate reinforcement learn base dialogue system able learn confirm order achieve better performance greater robustness
paper analyze gate activation signal inside gate recurrent neural network find temporal structure signal highly correlate phoneme boundaries correlation verify set experiment phoneme segmentation better result compare standard approach obtain
recent work end end automatic speech recognition asr show connectionist temporal classification ctc loss use convert acoustics phone character sequence systems use dictionary separately train language model lm produce word sequence however truly end end sense map acoustics directly word without intermediate phone representation paper present first result employ direct acoustics word ctc model two well know public benchmark task switchboard callhome model require lm even decoder run time hence recognize speech minimal complexity however due large number word output units ctc word model require order magnitude data train reliably compare traditional systems present techniques mitigate issue ctc word model achieve word error rate one hundred and thirty one hundred and eighty-eight hub5 two thousand switchboard callhome test set without lm decoder compare ninety-six one hundred and sixty phone base ctc four gram lm also present rescoring result ctc word model lattices quantify performance benefit lm contrast performance word phone ctc model
knowledge graph kgs real world facts entities relationships useful resources variety natural language process task however knowledge graph typically incomplete useful perform knowledge graph completion link prediction ie predict whether relationship knowledge graph likely true paper serve comprehensive survey embed model entities relationships knowledge graph completion summarize date experimental result standard benchmark datasets point potential future research directions
syntactic structure sentence model tree vertices correspond word edge indicate syntactic dependencies claim recurrently number edge cross real sentence small however baseline null hypothesis lack quantify amount cross real sentence compare predictions series baselines conclude cross really scarce real sentence scarcity unexpected hubiness tree indeed real sentence close linear tree potential number cross maximize
although information workers may complain meet essential part work life consequently busy people spend significant amount time schedule meet present calendarhelp system provide fast efficient schedule structure workflows users interact system via email delegate schedule need system human personal assistant common schedule scenarios break use well define workflows complete series microtasks automate possible execute human otherwise unusual scenarios fall back train human assistant execute unstructured macrotasks describe iterative approach use develop calendarhelp share lessons learn schedule thousands meet year real world deployments find provide insight complex information task break repeatable components execute efficiently improve productivity
present recurrent encoder decoder deep neural network architecture directly translate speech one language text another model explicitly transcribe speech text source language require supervision grind truth source language transcription train apply slightly modify sequence sequence attention architecture previously use speech recognition show repurposed complex task illustrate power attention base model single model train end end obtain state art performance fisher callhome spanish english speech translation task outperform cascade independently train sequence sequence speech recognition machine translation model eighteen bleu point fisher test set addition find make use train data languages multi task train sequence sequence speech translation recognition model share encoder network improve performance fourteen bleu point
objective investigate whether deep learn techniques natural language process nlp use efficiently patient phenotyping patient phenotyping classification task determine whether patient medical condition crucial part secondary analysis healthcare data assess performance deep learn algorithms compare classical nlp approach materials methods compare convolutional neural network cnns n gram model approach base ctakes extract pre define medical concepts clinical note use predict patient phenotypes performance test ten different phenotyping task use one thousand, six hundred and ten discharge summaries extract mimic iii database result cnns outperform phenotyping algorithms ten task average f1 score model seventy-six ppv eighty-three sensitivity seventy-one model f1 score thirty-seven point higher alternative approach additionally assess interpretability model present method extract salient phrase particular prediction conclusion show nlp methods base deep learn improve performance patient phenotyping cnn base algorithm automatically learn phrase associate patient phenotype reduce annotation complexity clinical domain experts normally require develop task specific annotation rule identify relevant phrase method perform well term performance interpretability indicate deep learn effective approach patient phenotyping base clinicians note
recurrent neural network language model use caption generation image information feed neural network either directly incorporate rnn condition language model inject image feature layer follow rnn condition language model merge image feature options attest literature yet systematic comparison two paper empirically show especially detrimental performance whether one architecture use another merge architecture practical advantage condition merge allow rnn hide state vector shrink size four time result suggest visual linguistic modalities caption generation need jointly encode rnn yield large memory intensive model tangible advantage performance rather multimodal integration delay subsequent stage
visual question answer vqa algorithm must answer text base question image multiple datasets vqa create since late two thousand and fourteen flaw content way algorithms evaluate result evaluation score inflate predominantly determine answer easier question make difficult compare different methods paper analyze exist vqa algorithms use new dataset contain sixteen million question organize twelve different categories also introduce question meaningless give image force vqa system reason image content propose new evaluation scheme compensate represent question type make easier study strengths weaknesses algorithms analyze performance baseline state art vqa model include multi modal compact bilinear pool mcb neural module network recurrent answer units experiment establish attention help certain categories others determine model work better others explain simple model eg mlp surpass complex model mcb simply learn answer large easy question categories
large amount information increase complexity applications constrain developers stand alone reusable components libraries component marketsour approach consist develop methods evaluate quality software component libraries one hand moreover optimize financial cost adaptation time select components objective function define metric maximize value software component quality minimize financial cost maintenance time model make possible classify components order order choose optimize mots cles eveloppement de ethode r eutilisation composants logiciels qualit e de composant keywordsmethod development reuse software components component quality
paper survey current state art natural language generation nlg define task generate text speech non linguistic input survey nlg timely view change field undergo past decade especially relation new usually data drive methods well new applications nlg technology survey therefore aim give date synthesis research core task nlg architectures adopt task organise b highlight number relatively recent research topics arise partly result grow synergies nlg areas artificial intelligence c draw attention challenge nlg evaluation relate similar challenge face areas natural language process emphasis different evaluation methods relationships
text speech synthesis system typically consist multiple stag text analysis frontend acoustic model audio synthesis module build components often require extensive domain expertise may contain brittle design choices paper present tacotron end end generative text speech model synthesize speech directly character give pair model train completely scratch random initialization present several key techniques make sequence sequence framework perform well challenge task tacotron achieve three hundred and eighty-two subjective five scale mean opinion score us english outperform production parametric system term naturalness addition since tacotron generate speech frame level substantially faster sample level autoregressive methods
recent research computational linguistics develop algorithms associate matrices adjectives verbs base distribution word corpus text matrices linear operators vector space context word use construct mean composite expressions elementary constituents form part compositional distributional approach semantics propose matrix theory approach data base permutation symmetry along gaussian weight perturbations simple gaussian model test word matrices create large corpus text characterize cubic quartic departures model propose alongside gaussian parameters signatures comparison linguistic corpora propose perturb gaussian model permutation symmetry provide promise framework characterize nature universality statistical properties word matrices matrix theory framework develop exploit view statistics zero dimensional perturbative quantum field theory perceive language physical system realize universality class matrix statistics characterize permutation symmetry
important edit policy wikipedia provide citations add statements wikipedia page statements arbitrary piece text range sentence paragraph many case citations either outdated miss altogether work address problem find update news citations statements entity page propose two stage supervise approach problem first step construct classifier find whether statements need news citation kinds citations web book journal etc second step develop news citation algorithm wikipedia statements recommend appropriate citations give news collection apart ir techniques use statement query news collection also formalize three properties appropriate citation namely citation entail wikipedia statement ii statement central citation iii citation authoritative source perform extensive evaluation step use twenty million article real world news collection result quite promise show perform task high precision scale
wikipedia entity page valuable source information direct consumption knowledge base construction update maintenance facts entity page typically support reference recent study show much twenty reference online news source however many entity page incomplete even relevant information already available exist news article even already present reference often delay news article publication time reference time work therefore look wikipedia lens news propose novel news article suggestion task improve news coverage wikipedia reduce lag newsworthy reference work find direct application precursor wikipedia page generation knowledge base acceleration task rely relevant high quality input source propose two stage supervise approach suggest news article entity page give state wikipedia first suggest news article wikipedia entities article entity placement rely rich set feature take account emphsalience emphrelative authority entities emphnovelty news article entity page second determine exact section entity page input article article section placement guide class base section templates perform extensive evaluation approach base grind truth data extract external reference wikipedia achieve high precision value ninety-three empharticle entity suggestion stage upto eighty-four empharticle section placement finally compare approach competitive baselines show significant improvements
simplify speech recognition system use maximum mutual information mmi criterion consider end end train use gradient descent suggest similarly train connectionist temporal classification ctc use mmi criterion simple language model train stage standard hmm decoder method compare favorably ctc term performance robustness decode time disk footprint quality alignments good alignments enable use straightforward ensemble method obtain simply average predictions several neural network model train separately end end ensemble method yield considerable reduction word error rate
strong progress make image caption last years machine human caption still quite distinct closer look reveal due deficiencies generate word distribution vocabulary size strong bias generators towards frequent caption furthermore humans rightfully generate multiple diverse caption due inherent ambiguity caption task consider today systems address challenge change train objective caption generator reproduce groundtruth caption generate set caption indistinguishable human generate caption instead handcraft learn target employ adversarial train combination approximate gumbel sampler implicitly match generate distribution human one method achieve comparable performance state art term correctness caption generate set diverse caption significantly less bias match word statistics better several aspects
present two simple ways reduce number parameters accelerate train large long short term memory lstm network first one matrix factorization design lstm matrix product two smaller matrices second one partition lstm matrix input state independent group approach allow us train large lstm network significantly faster near state art perplexity use significantly less rnn parameters
paper propose novel rank framework collaborative filter overall aim learn user preferences items minimize pairwise rank loss show minimization problem involve dependent random variables provide theoretical analysis prove consistency empirical risk minimization worst case users choose minimal number positive negative items derive neural network model jointly learn new representation users items embed space well preference relation users pair items learn objective base three scenarios rank losses control ability model maintain order items induce users preferences well capacity dot product define learn embed space produce order propose model nature suitable implicit feedback involve estimation parameters extensive experiment several real world benchmarks implicit data show interest learn preference embed simultaneously compare learn separately also demonstrate approach competitive best state art collaborative filter techniques propose implicit feedback
different sequential data sentence natural language structure linguistic grammars previous generative conversational model chain structure decoder ignore structure human language might generate plausible responses less satisfactory relevance fluency study aim incorporate result linguistic analysis process sentence generation high quality conversation generation specifically use dependency parser transform response sentence dependency tree construct train corpus sentence tree pair tree structure decoder develop learn map sentence tree different type hide state use depict local dependencies internal tree node children train acceleration propose tree canonicalization method transform tree equivalent ternary tree propose tree structure search method model able generate probable responses form dependency tree finally flatten sequence system output experimental result demonstrate propose x2tree framework outperform baseline methods one thousand, one hundred and fifteen increase acceptance ratio
mental illnesses adversely affect significant proportion population worldwide however methods traditionally use estimate characterize prevalence mental health condition time consume expensive consequently best available estimate concern prevalence mental health condition often years date automate approach supplement survey methods broad aggregate information derive social media content provide potential mean near real time estimate scale may turn provide grist support evaluate iteratively improve upon public health program interventions propose novel model automate mental health status quantification incorporate user embeddings build upon recent work explore representation learn methods induce embeddings leverage social media post histories embeddings capture latent characteristics individuals eg political lean encode soft notion homophily paper investigate whether user embeddings learn twitter post histories encode information correlate mental health statuses end estimate user embeddings set users know affect depression post traumatic stress disorder ptsd set demographically match control users evaluate embeddings respect ability capture homophilic relations respect mental health status ii performance downstream mental health prediction model base feature experimental result demonstrate user embeddings capture similarities users respect mental condition predictive mental health
work present novel objective function unsupervised train neural network sentence encoders exploit signal paragraph level discourse coherence train model understand text objective purely discriminative allow us train model many time faster possible prior methods yield model perform well extrinsic evaluations
although problem automatic video summarization recently receive lot attention problem create video summary also highlight elements relevant search query less study address problem pose query relevant summarization video frame subset selection problem let us us optimise summaries simultaneously diverse representative entire video relevant text query quantify relevance measure distance frame query common textual visual semantic embed space induce neural network addition extend model capture query independent properties frame quality compare method previous state art textual visual embeddings thumbnail selection show model outperform relevance prediction furthermore introduce new dataset annotate diversity query specific relevance label dataset train test complete model video summarization show outperform standard baselines maximal marginal relevance
lambek calculus logical foundation categorial grammar linguistic paradigm grammar logic parse deduction pentus two thousand and ten give polynomial time algorithm determ ining provability bound depth formulas lambek calculus empty antecedents allow pentus algorithm base tabularisation proof net lambek calculus bracket conservative extension lambek calculus bracket modalities suitable model syntactical domains paper give algorithm provability lambek calculus bracket allow empty antecedents algorithm run polynomial time formula depth bracket nest depth bound combine pentus style tabularisation proof net automata theoretic treatment bracket
recent study eleven thousand, three hundred and twelve recurrent neural network use generative process surprise performance explain ability create good predictions addition data compression also base predictions problem come whether data compressor could use perform well recurrent neural network natural language process task possiblethen problem come determine compression algorithm even intelligent neural network specific task relate human language journey discover think fundamental difference data compression algorithm recurrent neural network
majority medical document electronic health record ehrs text format pose challenge data process find relevant document look ways automatically retrieve enormous amount health medical knowledge always intrigue topic powerful methods develop recent years make text process automatic one popular approach retrieve information base discover theme health medical corpora topic model however approach still need new perspectives research describe fuzzy latent semantic analysis flsa novel approach topic model use fuzzy perspective flsa handle health medical corpora redundancy issue provide new method estimate number topics quantitative evaluations show flsa produce superior performance feature latent dirichlet allocation lda popular topic model
recurrent neural network rnn widely apply sequence model rnn hide state current step full connect previous step thus influence less relate feature previous step may potentially decrease model learn ability propose simple technique call parallel cells pcs enhance learn ability recurrent neural network rnn layer run multiple small rnn cells rather one single large cell paper evaluate pcs two task language model task ptb penn tree bank model outperform state art model decrease perplexity seven hundred and eighty-six seven hundred and fifty-three chinese english translation task model increase bleu score thirty-nine point baseline model
paper aim understand whether current language vision lavi model truly grasp interaction two modalities end propose extension mscoco dataset foil coco associate image correct foil caption descriptions image highly similar original ones contain one single mistake foil word show current lavi model fall trap data perform badly three task caption classification correct vs foil b foil word detection c foil word correction humans contrast near perfect performance task demonstrate merely utilise language cue enough model foil coco challenge state art require fine grain understand relation text image
paper present result topic model network model topics use international conference computational science corpus contain domain specific computational science paper sixteen years total five thousand, six hundred and ninety-five paper discuss topical structure international conference computational science topics evolve time response topicality various problems technologies methods topics relate one another analysis illustrate multidisciplinary research collaborations among scientific communities construct static dynamic network topic model result keywords author result study give insights past future trend core discussion topics computational science use non negative matrix factorization topic model algorithm discover topics label group result hierarchically
automatically assess emotional valence human speech historically difficult task machine learn algorithms subtle change voice speaker indicative positive negative emotional state often overshadow voice characteristics relate emotional intensity emotional activation work explore representation learn approach automatically derive discriminative representations emotional speech particular investigate two machine learn strategies improve classifier performance one utilization unlabeled data use deep convolutional generative adversarial network dcgan two multitask learn within extensive experiment leverage multitask annotate emotional corpus well large unlabeled meet corpus around one hundred hours speaker independent classification experiment show particular use unlabeled data investigations improve performance classifiers fully supervise baseline approach outperform considerably improve classification emotional valence discrete five point scale four thousand, three hundred and eighty-eight three point scale four thousand, nine hundred and eighty competitive state art performance
abundant data key successful machine learn however supervise learn require annotate data often hard obtain classification task limit resources active learn al promise guide annotators examples bring value classifier al successfully combine self train ie extend train set unlabelled examples classifier certain report experience use al systematic manner train svm classifier stack overflow post discuss performance software components show train examples deem valuable classifier also difficult humans annotate despite carefully evolve annotation criteria report low inter rater agreement also propose mitigation strategies finally base one annotator work show self train improve classification accuracy conclude paper discuss implication future text miners aspire use al self train
propose max pool base loss function train long short term memory lstm network small footprint keyword spot kws low cpu memory latency requirements max pool loss train guide initialize cross entropy loss train network posterior smooth base evaluation approach employ measure keyword spot performance experimental result show lstm model train use cross entropy loss max pool loss outperform cross entropy loss train baseline fee forward deep neural network dnn addition max pool loss train lstm randomly initialize network perform better compare cross entropy loss train lstm finally max pool loss train lstm initialize cross entropy pre train network show best performance yield six hundred and seventy-six relative reduction compare baseline fee forward dnn area curve auc measure
large scale multi relational embed refer task learn latent representations entities relations large knowledge graph effective scalable solution problem crucial true success knowledge base inference broad range applications paper propose novel framework optimize latent representations respect textitanalogical properties embed entities relations formulate learn objective differentiable fashion model enjoy theoretical power computational scalability significantly outperform large number representative baseline methods benchmark datasets furthermore model offer elegant unification several well know methods multi relational embed prove special instantiations framework
deep neural model particularly lstm rnn model show great potential language identification lid however use phonetic information largely overlook exist neural lid methods although information use successfully conventional phonetic lid systems present phonetic temporal neural model lid lstm rnn lid system accept phonetic feature produce phone discriminative dnn input rather raw acoustic feature new model similar traditional phonetic lid methods phonetic knowledge much richer frame level involve compact information phone experiment conduct babel database ap16 olr database demonstrate temporal phonetic neural approach effective significantly outperform exist acoustic neural model also outperform conventional vector approach short utterances noisy condition
pure acoustic neural model particularly lstm rnn model show great potential language identification lid however phonetic information largely overlook exist neural lid model although information use conventional phonetic lid systems great success present phone aware neural lid architecture deep lstm rnn lid system accept output rnn base asr system utilize phonetic knowledge lid performance significantly improve interestingly even test language involve asr train phonetic knowledge still present large contribution experiment conduct four languages within babel corpus demonstrate phone aware approach highly effective
speak language understand slu key component goal orient dialogue systems would parse user utterances semantic frame representations traditionally slu utilize dialogue history beyond previous system turn contextual ambiguities resolve downstream components paper explore novel approach model dialogue context recurrent neural network rnn base language understand system propose sequential dialogue encoder network allow encode context dialogue history chronological order compare performance propose architecture two context model one use previous turn context another encode dialogue context memory network lose order utterances dialogue history experiment multi domain dialogue dataset demonstrate propose architecture result reduce semantic frame error rat
multiple death data provide valuable source information use enhance health standards predict health relate trajectories societies large populations data often available large quantities across yous state require big data techniques uncover complex hide pattern design two different class model suitable large scale analysis mortality data hadoop base ensemble random forest train n grams deepdeath deep classifier base recurrent neural network rnn apply class mortality data provide national center health statistics show perform significantly better random classifier deep model utilize long short term memory network lstms surpass n gram base model capable learn temporal aspect data without need build ad hoc expert drive feature
learn high dimensional dense representation vocabulary term also know word embed recently attract much attention natural language process information retrieval task embed vectors typically learn base term proximity large corpus mean objective well know word embed algorithms eg word2vec accurately predict adjacent word give word context however objective necessarily equivalent goal many information retrieval ir task primary objective various ir task capture relevance instead term proximity syntactic even semantic similarity motivation develop unsupervised relevance base word embed model learn word representations base query document relevance information paper propose two learn model different objective function one learn relevance distribution vocabulary set query classify term belong relevant non relevant class query train model use six million unique query top rank document retrieve response query assume relevant query extrinsically evaluate learn word representation model use two ir task query expansion query classification query expansion experiment four trec collections query classification experiment kdd cup two thousand and five dataset suggest relevance base word embed model significantly outperform state art proximity base embed model word2vec glove
exist methods visual reason attempt directly map input output use black box architectures without explicitly model underlie reason process result black box model often learn exploit bias data rather learn perform visual reason inspire module network paper propose model visual reason consist program generator construct explicit representation reason process perform execution engine execute result program produce answer program generator execution engine implement neural network train use combination backpropagation reinforce use clevr benchmark visual reason show model significantly outperform strong baselines generalize better variety settings
recently deep neural network dnns use learn speaker feature however quality learn feature sufficiently good complex back end model either neural probabilistic use address residual uncertainty apply speaker verification raw feature paper present convolutional time delay deep neural network structure ct dnn speaker feature learn experimental result fisher database demonstrate ct dnn produce high quality speaker feature even single feature three second include context ever low seven hundred and sixty-eight effectively confirm speaker trait largely deterministic short time property rather long time distributional pattern therefore extract dozens frame
visual question answer vqa new excite problem combine natural language process computer vision techniques present survey various datasets model use tackle task first part survey detail various datasets vqa compare along common factor second part survey detail different approach vqa classify four type non deep learn model deep learn model without attention deep learn model attention model fit first three finally compare performances approach provide directions future work
solve algebraic word problems require execute series arithmetic operations program obtain final answer however since program arbitrarily complicate induce directly question answer pair formidable challenge make task feasible solve problems generate answer rationales sequence natural language human readable mathematical expressions derive final answer series small step although rationales explicitly specify program provide scaffold structure via intermediate milestones evaluate approach create new one hundred thousand sample dataset question answer rationales experimental result show indirect supervision program learn via answer rationales promise strategy induce arithmetic program
name entity recognition ner aim identify entities interest text artificial neural network anns recently show outperform exist ner systems however anns remain challenge use non expert users paper present neuroner easy use name entity recognition tool base anns users annotate entities use graphical web base user interface brat annotations use train ann turn predict entities locations categories new texts neuroner make annotation train prediction flow smooth accessible anyone
paper demonstrate state art machine learn text mine techniques use build effective social media base substance use detection systems since substance use grind truth difficult obtain large scale maximize system performance explore different feature learn methods take advantage large amount unsupervised social media data also demonstrate benefit use multi view unsupervised feature learn combine heterogeneous user information facebook like status update enhance system performance base evaluation best model achieve eighty-six auc predict tobacco use eighty-one alcohol use eighty-four drug use significantly outperform exist methods investigation also uncover interest relations user social media behavior eg word usage substance use
availability large scale event data time stamp give rise dynamically evolve knowledge graph contain temporal information edge reason time dynamic knowledge graph yet well understand end present know evolve novel deep evolutionary knowledge network learn non linearly evolve entity representations time occurrence fact edge model multivariate point process whose intensity function modulate score fact compute base learn entity embeddings demonstrate significantly improve performance various relational learn approach two large scale real world datasets method effectively predict occurrence recurrence time fact novel compare prior reason approach multi relational set
zipf law establish word large text order decrease frequency frequency versus rank decrease power law exponent close one previous work stress pattern arise conflict interest participants communication challenge define computational multi agent language game mainly base parameter measure relative participant interest numerical simulations suggest critical value parameter human like vocabulary exhibit scale properties seem appear appearance intermediate distribution frequencies critical value parameter suggest population artificial agents emergence scale partly arise self organize process local interactions agents
recent approach base artificial neural network anns show promise result name entity recognition ner order achieve high performances anns need train large label dataset however label might difficult obtain dataset user want perform ner label scarcity particularly pronounce patient note de identification instance ner work analyze extent transfer learn may address issue particular demonstrate transfer ann model train large label dataset another dataset limit number label improve upon state art result two different datasets patient note de identification
link human whole body motion natural language great interest generation semantic representations observe human behaviors well generation robot behaviors base natural language input large body research area approach exist today require symbolic representation motion eg form motion primitives define priori require complex segmentation algorithms contrast recent advance field neural network especially deep learn demonstrate sub symbolic representations learn end end usually outperform traditional approach applications machine translation paper propose generative model learn bidirectional map human whole body motion natural language use deep recurrent neural network rnns sequence sequence learn approach require segmentation manual feature engineer learn distribute representation share motion descriptions evaluate approach two thousand, eight hundred and forty-six human whole body motion six thousand, one hundred and eighty-seven natural language descriptions thereof kit motion language dataset result clearly demonstrate effectiveness propose model show model generate wide variety realistic motion descriptions thereof form single sentence conversely model also capable generate correct detail natural language descriptions human motion
organization evolution science recently become object scientific quantitative investigation thank wealth information extract scientific document citations paper co authorship researchers however study focus concepts characterize full document extract analyze reveal deeper organization scientific knowledge unfortunately several concepts common across document hinder emergence underlie topical structure document corpus give rise large amount spurious trivial relations among document identify remove common concepts introduce method gauge relevance accord objective information theoretic measure relate statistics occurrence across document corpus progressively remove concepts accord metric consider generic find topic organization display correspondingly refine structure
visual question answer recently propose artificial intelligence task require deep understand image texts deep learn image typically model convolutional neural network texts typically model recurrent neural network requirement model image similar traditional computer vision task object recognition image classification visual question answer raise different need textual representation compare natural language process task work perform detail analysis natural language question visual question answer base analysis propose rely convolutional neural network learn textual representations explore various properties convolutional neural network specialize text data width depth present cnn inception gate model show model improve question representations thus overall accuracy visual question answer model also show text representation requirement visual question answer complicate comprehensive conventional natural language process task make better task evaluate textual representation methods shallow model like fasttext obtain comparable result deep learn model task like text classification suitable visual question answer
reward augment maximum likelihood raml simple effective learn framework directly optimize towards reward function structure prediction task lead number impressive empirical successes raml incorporate task specific reward perform maximum likelihood update candidate output sample accord exponentiated payoff distribution give higher probabilities candidates close reference output raml notable simplicity efficiency impressive empirical successes theoretical properties raml especially behavior exponentiated payoff distribution examine thoroughly work introduce softmax q distribution estimation novel theoretical interpretation raml reveal relation raml bayesian decision theory softmax q distribution regard smooth approximation bay decision boundary bay decision rule achieve decode q distribution show raml equivalent approximately estimate softmax q distribution temperature tau control approximation error perform two experiment one synthetic data multi class classification one real data image caption demonstrate relationship raml propose softmax q distribution estimation method verify theoretical analysis additional experiment three structure prediction task reward define sequential name entity recognition tree base dependency parse irregular machine translation structure show notable improvements maximum likelihood baselines
paper extend attention base neural machine translation nmt model allow access entire train set parallel sentence pair even train propose approach consist two stag first stage retrieval stage shelf black box search engine use retrieve small subset sentence pair train set give source sentence pair filter base fuzzy match score base edit distance second stage translation stage novel translation model call translation memory enhance nmt tm nmt seamlessly use source sentence set retrieve sentence pair perform translation empirical evaluation three language pair en fr en de en es show propose approach significantly outperform baseline approach improvement significant relevant sentence pair retrieve
word embeddings improve performance nlp systems reveal hide structural relationships word despite success many applications word embeddings see little use computational social science nlp task presumably due reliance big data lack interpretability propose probabilistic model base word embed method recover interpretable embeddings without big data key insight leverage mix membership model global representations share individual entities ie dictionary word free use representations uniquely differ degrees show train model use combination state art train techniques word embeddings topic model experimental result show improvement predictive language model sixty-three mrr skip gram demonstrate representations beneficial supervise learn illustrate interpretability model computational social science case study state union address nip article
modern neural network often augment attention mechanism tell network focus within input propose paper new framework sparse structure attention build upon smooth max operator show gradient operator define map real value probabilities suitable attention mechanism framework include softmax slight generalization recently propose sparsemax special case however also show framework incorporate modern structure penalties result interpretable attention mechanisms focus entire segment group input derive efficient algorithms compute forward backward pass attention mechanisms enable use neural network train backpropagation showcase potential drop replacement exist ones evaluate attention mechanisms three large scale task textual entailment machine translation sentence summarization attention mechanisms improve interpretability without sacrifice performance notably textual entailment summarization outperform standard attention mechanisms base softmax sparsemax
dynamic neural network toolkits pytorch dynet chainer offer flexibility implement model cope data vary dimension structure relative toolkits operate statically declare computations eg tensorflow cntk theano however exist toolkits static dynamic require developer organize computations batch necessary exploit high performance algorithms hardware batch task generally difficult become major hurdle architectures become complex paper present algorithm implementation dynet toolkit automatically batch operations developers simply write minibatch computations aggregations single instance computations batch algorithm seamlessly execute fly use computationally efficient batch operations variety task obtain throughput similar obtain manual batch well comparable speedups single instance learn architectures impractical batch manually
generic text embeddings successfully use variety task however often learn capture co occurrence structure pure text corpora result limitations ability generalize paper explore model incorporate visual information text representation base comprehensive ablation study propose conceptually simple yet well perform architecture outperform previous multimodal approach set well establish benchmarks also improve state art result image relate text datasets use order magnitude less data
work present ground recurrent neural network grnn recurrent neural network architecture multi label prediction explicitly tie label specific dimension recurrent hide state call process ground approach particularly well suit extract large number concepts text apply new model address important problem healthcare understand medical concepts discuss clinical text use publicly available dataset derive intensive care units learn label patient diagnose procedures discharge summary evaluation show clear advantage use propose architecture variety strong baselines
paper formulate novel problem graph find minimal subset edge fully connect graph result graph contain span tree set specifed sub graph formulation motivate un supervise grammar induction problem computational linguistics present reduction know problems algorithms graph theory provide computational complexity result describe approximation algorithm
design neural architectures structure object typically guide experimental insights rather formal process work appeal kernels combinatorial structure sequence graph derive appropriate neural operations introduce class deep recurrent neural operations formally characterize associate kernel space recurrent modules compare input virtual reference object cf filter cnn via kernels similar traditional neural operations reference object parameterized directly optimize end end train empirically evaluate propose class neural architectures standard applications language model molecular graph regression achieve state art result across applications
paper address problem automatic speech recognition asr error detection use improve speak language understand slu systems study slu task consist automatically extract asr transcriptions semantic concepts concept value pair eg touristic information system approach propose enrich set semantic label error specific label use recently propose neural approach base word embeddings compute well calibrate asr confidence measure experimental result report show possible decrease significantly concept value error rate state art system outperform previously publish result performance experimental data also show combine slu approach base conditional random field neural encoder decoder attention base architecture possible effectively identify confidence islands uncertain semantic output segment useful decide appropriate error handle action dialogue manager strategy
present software tool employ state art natural language process nlp machine learn techniques help newspaper editors compose effective headline online publication system identify salient keywords news article rank base overall popularity direct relevance article system also use supervise regression model identify headline likely widely share social media user interface design simplify speed editor decision process composition headline tool provide efficient way combine benefit automate predictors engagement search engine optimization seo human judgments overall headline quality
community identity define shape internal dynamics current understand interplay mostly limit glimpse gather isolate study individual communities work provide systematic exploration nature relation across wide variety online communities end introduce quantitative language base typology reflect two key aspects community identity distinctive temporally dynamic map almost three hundred reddit communities landscape induce typology reveal regularities pattern user engagement vary characteristics community result suggest way new exist users engage community depend strongly systematically nature collective identity foster ways highly consequential community maintainers example communities distinctive highly dynamic identities likely retain users however niche communities also exhibit much larger acculturation gap exist users newcomers potentially hinder integration latter generally methodology reveal differences various social phenomena manifest across communities show structure multi community landscape lead better understand systematic nature diversity
word similarities affect language acquisition use multi relational way barely account literature propose multiplex network representation mental lexicon word similarities natural framework investigate large scale cognitive pattern representation account semantic taxonomic phonological interactions identify cluster word use greater frequency identify memorise learn easily mean expect random cluster emerge around age seven explosive transition reproduce null model relate explosive emergence polysemy redundancy word mean result indicate word cluster act core lexicon increase lexical navigability robustness linguistic degradation find provide quantitative confirmation exist conjecture core structure mental lexicon importance integrate multi relational word word interactions psycholinguistic frameworks
minimization length syntactic dependencies well establish principle word order basis mathematical theory word order complete theory perspective information theory add compete word order principle maximization predictability target element two principles conflict maximize predictability head head appear last maximize cost respect dependency length minimization implications broad theoretical framework understand optimality diversity evolution six possible order subject object verb review
show recently propose neural dependency parser improve joint train multiple languages family parser implement deep neural network whose input orthographic representations word order successfully parse network discover linguistically relevant concepts infer word spell analyze representations character word learn network establish properties languages account particular show parser approximately learn associate latin character cyrillic counterparts group polish russian word similar grammatical function finally evaluate parser select languages universal dependencies dataset show competitive recently propose state art methods simple structure
develop dialogue agent capable make autonomous decisions communicate natural language one long term goals machine learn research traditional approach either rely hand craft small state action set apply reinforcement learn scalable construct deterministic model learn dialogue sentence fail capture natural conversational variability paper propose latent intention dialogue model lidm employ discrete latent variable learn underlie dialogue intentions framework neural variational inference goal orient dialogue scenario latent intentions interpret action guide generation machine responses refine autonomously reinforcement learn experimental evaluation lidm show model perform publish benchmarks corpus base human evaluation demonstrate effectiveness discrete latent variable model learn goal orient dialogues
eliminate negative effect non stationary environmental noise long stand research topic automatic speech recognition still remain important challenge data drive supervise approach include ones base deep neural network recently emerge potential alternatives traditional unsupervised approach sufficient train alleviate shortcomings unsupervised methods various real life acoustic environments light review recently develop representative deep learn approach tackle non stationary additive convolutional degradation speech aim provide guidelines involve development environmentally robust speech recognition systems separately discuss single multi channel techniques develop front end back end speech recognition systems well joint front end back end train frameworks
generative adversarial network gans gather lot attention computer vision community yield impressive result image generation advance adversarial generation natural language noise however commensurate progress make generate image still lag far behind likelihood base methods paper take step towards generate natural language gin objective alone introduce simple baseline address discrete output space problem without rely gradient estimators show able achieve state art result chinese poem generation dataset present quantitative result generate sentence context free probabilistic context free grammars qualitative language model result conditional version also describe generate sequence condition sentence characteristics
learn meaningful representations maintain content necessary particular task filter away detrimental variations problem great interest machine learn paper tackle problem learn representations invariant specific factor trait data representation learn process formulate adversarial minimax game analyze optimal equilibrium game find amount maximize uncertainty infer detrimental factor give representation maximize certainty make task specific predictions three benchmark task namely fair bias free classification language independent generation light independent image classification show propose framework induce invariant representation lead better generalization evidence improve performance
learn communicate interaction rather rely explicit supervision often consider prerequisite develop general ai study set two agents engage play referential game scratch develop communication protocol necessary succeed game unlike previous work require message exchange train test time form language ie sequence discrete symbols compare reinforcement learn approach one use differentiable relaxation straight gumbel softmax estimator observe latter much faster converge result effective protocols interestingly also observe protocol induce optimize communication success exhibit degree compositionality variability ie information phrase different ways properties characteristic natural languages ultimate goal ensure communication accomplish natural language also perform experiment inject prior information natural language model study properties result protocol
recent efforts bioinformatics achieve tremendous progress machine read biomedical literature assembly extract biochemical interactions large scale model protein signal pathways however batch machine read literature today scale pubmed alone index one million paper per year unfeasible due cost process overhead work introduce focus read approach guide machine read biomedical literature towards literature read answer biomedical query efficiently possible introduce family algorithms focus read include intuitive strong baseline second approach use reinforcement learn rl framework learn explore widen search exploit narrow demonstrate rl approach capable answer query baseline efficient ie read fewer document
generate texts structure data eg table important various natural language process task question answer dialog systems recent study researchers use neural language model encoder decoder frameworks table text generation however neural network base approach model order content text generation human write summary base give table would probably consider content order word biography example nationality person typically mention occupation biography paper propose order plan text generation model capture relationship different field use relationship make generate text fluent smooth conduct experiment wikibio dataset achieve significantly higher performance previous methods term bleu rouge nist score
online community new word come go today haha may replace tomorrow lol change online write usually study social process innovations diffuse network individuals speech community unlike type innovation language change shape constrain system take part investigate link social structural factor language change undertake large scale analysis nonstandard word growth online community reddit find dissemination across many linguistic contexts sign growth word appear linguistic contexts grow faster survive longer also find social dissemination likely play less important role explain word growth decline previously hypothesize
order successfully annotate arabic speech con tent find open domain media broadcast essential able process diverse set arabic dialects two thousand and seventeen multi genre broadcast challenge mgb three two possible task arabic speech recognition arabic dialect identification adi paper describe efforts create adi system mgb three challenge goal distinguish amongst four major arabic dialects well modern standard arabic research fo cused dialect variability domain mismatch train test domain order achieve robust adi system explore siamese neural network model learn similarity dissimilarities among arabic dialects well vector post process adapt domain mismatch acoustic linguistic feature use final mgb three submissions best primary system achieve seventy-five accuracy official 10hr test set
ubiquity metaphor everyday communication make important problem natural language understand yet majority metaphor process systems date rely hand engineer feature still consensus field feature optimal task paper present first deep learn architecture design capture metaphorical composition result demonstrate outperform exist approach metaphor identification task
current language understand approach focus small document newswire article blog post product review discussion forum entries understand extract information large document like legal brief proposals technical manuals research article still challenge task describe framework analyze large document help people know particular information document aim automatically identify classify semantic section document assign consistent human understandable label similar section across document key contribution research model logical semantic structure electronic document apply machine learn techniques include deep learn prototype system also make available dataset information collection scholarly article arxiv eprints collection include wide range metadata article include table content section label section summarizations hope dataset useful resource machine learn nlp communities information retrieval content base question answer language model
propose information theoretic framework quantitative assessment acoustic model hide markov model hmm base automatic speech recognition asr acoustic model yield probabilities hmm sub word state short temporal window speech acoustic feature cast asr communication channel input sub word probabilities convey information output hmm state sequence quality acoustic model thus quantify term information transmit channel process infer likely hmm state sequence sub word probabilities know decode hmm base decode assume acoustic model yield accurate state level probabilities data distribution give underlie hide state independent state sequence quantify one acoustic model accuracy two robustness mismatch data hmm conditional independence assumption term mutual information quantities context exploit deep neural network dnn posterior probabilities lead simple straightforward analysis framework assess shortcomings acoustic model hmm base decode analysis enable us evaluate gaussian mixture acoustic model gmm importance many hide layer dnns without need explicit speech recognition addition shed light contribution low dimensional model enhance acoustic model better compliance hmm base decode requirements
segment span contiguous part input phonemes speech name entities sentence action videos occur frequently sequence prediction problems segmental model class model explicitly hypothesize segment allow exploration rich segment feature sequence prediction however segmental model suffer slow decode hamper use computationally expensive feature thesis introduce discriminative segmental cascade multi pass inference framework allow us improve accuracy add higher order feature neural segmental feature maintain efficiency also show instead include feature obtain better accuracy segmental cascade use speed train decode segmental model similarly conventional speech recognizers typically train multiple stag first stage frame classifier train manual alignments second stage segmental model train manual alignments put frame classifier however obtain manual alignments time consume expensive explore end end train segmental model various loss function show end end train marginal log loss eliminate need detail manual alignments draw connections marginal log loss popular end end train approach call connectionist temporal classification present unify framework various end end graph search base model hide markov model connectionist temporal classification segmental model finally discuss possible extensions segmental model large vocabulary sequence prediction task
musical program languages develop purely cod virtual instrument algorithmic compositions although work domain musical query languages music information retrieval little attempt unify principles musical program query languages cognitive natural language process model would facilitate activity composition conversation present prototype framework call museci merge domains permit score level algorithmic composition text editor also support connectivity exist natural language process frameworks
biographical databases contain diverse information individuals person name birth information career friends family special achievements possible items record individual relationships individuals kinship friendship provide invaluable insights hide communities directly record databases show simple matrix graph base operations effective infer relationships among individuals illustrate main ideas china biographical database cbdb
rise social media people obtain share information almost instantly twenty-four seven basis many research areas try gain valuable insights large volumes freely available user generate content goal extract knowledge social media stream might useful context intelligent transportation systems smart cities design develop framework provide functionalities parallel collection geo locate tweet multiple pre define bound box cities regions include filter non comply tweet text pre process portuguese english language topic model transportation specific text classifiers well aggregation data visualization perform exploratory data analysis geo locate tweet five different cities rio de janeiro sao paulo new york city london melbourne comprise total forty-three million tweet period three months furthermore perform large scale topic model comparison rio de janeiro sao paulo interestingly topics share cities despite country consider different regard population economy lifestyle take advantage recent developments word embeddings train representations collections geo locate tweet use combination bag embeddings traditional bag word train travel relate classifiers portuguese english filter travel relate content non relate create specific gold standard data perform empirical evaluation result classifiers result line research work application areas show robustness use word embeddings learn word similarities bag word able capture
although neural machine translation nmt encoder decoder framework achieve great success recent time still suffer drawbacks rnns tend forget old information often useful encoder operate word without consider word relationship solve problems introduce relation network rn nmt refine encode representations source method rn first augment representation source word neighbor reason possible pairwise relations source representations relations feed attention module decoder together keep main encoder decoder architecture unchanged experiment two chinese english data set different scale show method outperform competitive baselines significantly
knowledge graph kg know helpful task question answer qa since provide well structure relational information entities allow one infer indirect facts however challenge build qa systems learn reason knowledge graph base question answer pair alone first people ask question expressions noisy example typos texts variations pronunciations non trivial qa system match mention entities knowledge graph second many question require multi hop logic reason knowledge graph retrieve answer address challenge propose novel unify deep learn architecture end end variational learn algorithm handle noise question learn multi hop reason simultaneously method achieve state art performance recent benchmark dataset literature also derive series new benchmark datasets include question multi hop reason question paraphrase neural translation model question human voice method yield promise result challenge datasets
weight finite automata wfa expressively model function define string inherently linear model give recent successes nonlinear model machine learn natural wonder whether ex tend wfa nonlinear set would beneficial paper propose novel model neural network base nonlinearwfa model nl wfa along learn algorithm learn algorithm inspire spectral learn algorithm wfaand rely nonlinear decomposition call hankel matrix mean auto encoder network expressive power nl wfa propose learn algorithm assess synthetic real world data show nl wfa lead smaller model size infer complex grammatical structure data
neural model become ubiquitous automatic speech recognition systems neural network typically use acoustic model complex systems recent study explore end end speech recognition systems base neural network train directly predict text input acoustic feature although systems conceptually elegant simpler traditional systems less obvious interpret train model work analyze speech representations learn deep end end model base convolutional recurrent layer train connectionist temporal classification ctc loss use pre train model generate frame level feature give classifier train frame classification phone evaluate representations different layer deep model compare quality predict phone label experiment would light important aspects end end model layer depth model complexity design choices
model compression significant wide adoption recurrent neural network rnns user devices possess limit resources business cluster require quick responses large scale service request work aim learn structurally sparse long short term memory lstm reduce size basic structure within lstm units include input update gate hide state cell state output independently reduce size basic structure result inconsistent dimension among consequently end invalid lstm units overcome problem propose intrinsic sparse structure iss lstms remove component iss simultaneously decrease size basic structure one thereby always maintain dimension consistency learn iss within lstm units obtain lstms remain regular much smaller basic structure base group lasso regularization method achieve 1059x speedup without lose perplexity language model penn treebank dataset also successfully evaluate compact model 269m weight machine question answer squad dataset approach successfully extend non lstm rnns like recurrent highway network rhns source code publicly available https githubcom wenwei202 iss rnns
paper self guide multimodal lstm sg lstm image caption model propose handle uncontrolled imbalanced real world image sentence dataset collect flickrnyc dataset flickr testbed three hundred and six thousand, one hundred and sixty-five image original text descriptions upload users utilize grind truth train descriptions flickrnyc dataset vary dramatically range short term descriptions long paragraph descriptions describe visual aspects even refer object depict deal imbalanced noisy situation fully explore dataset propose novel guide textual feature extract utilize multimodal lstm lstm model train lstm base portion data image content correspond descriptions strongly bond afterwards train sg lstm rest train data guide information serve additional input network along image representations grind truth descriptions integrate input components multimodal block aim form train scheme textual information tightly couple image content experimental result demonstrate propose sg lstm model outperform traditional state art multimodal rnn caption framework successfully describe key components input image
recommendation systems recognise hugely important industry area well understand news uk requirement able quickly generate recommendations users news items publish however little publish systems generate recommendations response change recommendable items user behaviour short space time paper describe new algorithm update collaborative filter model incrementally demonstrate effectiveness clickstream data time also describe architecture allow recommendations generate fly make component scalable system currently use production news uk
vehicle represent knowledge organization systems koss environment semantic web link data simple knowledge organization system skos skos provide way assign uri concept uri function surrogate concept fact make main concern need clarify uris ontological mean aim study investigate relation ontological substance kos concepts concepts reveal grammatical syntactic formalisms natural language purpose examine dividableness concepts specific koss ie thesaurus subject head system classification scheme apply natural language process nlp techniques ie morphosyntactic analysis lexical representations ie rdf literals skos concepts result comparative analysis reveal despite use multi word units thesauri tend represent concepts way hardly divide conceptually subject head classification scheme certain extent comprise term decompose conceptual constituents consequently skos concepts derive thesauri likely represent atomic conceptual units thus appropriate tool inference reason since identifiers represent mean concept complex concepts neither appropriate efficient way model kos semantic web
distribute word embeddings show superior performances numerous natural language process nlp task however performances vary significantly across different task imply word embeddings learn methods capture complementary aspects lexical semantics therefore believe important combine exist word embeddings produce accurate complete emphmeta embeddings word purpose propose unsupervised locally linear meta embed learn method take pre train word embeddings input produce accurate meta embeddings unlike previously propose meta embed learn methods learn global projection word vocabulary propose method sensitive differences local neighbourhoods individual source word embeddings moreover show vector concatenation previously propose highly competitive baseline approach integrate word embeddings derive special case propose method experimental result semantic similarity word analogy relation classification short text classification task show meta embeddings significantly outperform prior methods several benchmark datasets establish new state art meta embeddings
represent semantic relations exist two give word entities important first step wide range nlp applications analogical reason knowledge base completion relational information retrieval simple yet surprisingly accurate method represent relation two word compute vector offset pairdiff correspond word embeddings despite empirical success remain unclear whether pairdiff best operator obtain relational representation word embeddings conduct theoretical analysis generalise bilinear operators use measure ell2 relational distance two word pair show word embeddings standardise uncorrelated operator independent bilinear term simplify linear form pairdiff special case numerous word embed type empirically verify uncorrelation assumption demonstrate general applicability theoretical result moreover experimentally discover pairdiff bilinear relation composition operator several benchmark analogy datasets
two thousand and ten silent speech challenge benchmark update new result obtain deep learn strategy use input feature decode strategy original article word error rate sixty-four obtain compare publish value one hundred and seventy-four additional result compare new auto encoder base feature original feature reduce dimensionality well decode scenarios two different language model also present silent speech challenge archive update contain original new auto encoder feature addition original raw data
knowledge base wikidata possible assert large set properties entities range generic ones name place birth highly profession specific background specific ones doctoral advisor medical condition determine preference rank large set challenge task prioritisation edit natural language generation previous approach rank knowledge base properties purely data drive show mistake frequency interestingness work develop human annotate dataset three hundred and fifty preference judgments among pair knowledge base properties fix entities set isolate subset pair humans show high level agreement eight hundred and seventy-five average show however baseline state art techniques achieve six hundred and thirteen precision predict human preferences subset analyze contribute one property rat important another one identify least three factor play role namely general frequency ii applicability similar entities iii semantic similarity property entity experimentally analyze contribution factor show combination techniques address three factor achieve seventy-four precision task dataset available wwwkagglecom srazniewski wikidatapropertyranking
textual data compress intelligently without lose accuracy evaluate sentiment study propose novel evolutionary compression algorithm parsec part speech sentiment compression make use part speech tag compress text way sacrifice minimal classification accuracy use conjunction sentiment analysis algorithms analysis parsec eight commercial non commercial sentiment analysis algorithms twelve english sentiment data set reveal accurate compression possible zero thirteen thirty-three loss sentiment classification accuracy twenty fifty seventy-five data compression parsec use lingpipe accurate sentiment algorithms sentiment analysis algorithms severely affect compression conclude significant compression text data possible sentiment analysis depend accuracy demand specific application specific sentiment analysis algorithm use
latent variable model introduce text match infer sentence representations jointly optimize generative discriminative objectives alleviate typical optimization challenge latent variable model text employ deconvolutional network sequence decoder generator provide learn latent cod semantic information better generalization model train unsupervised manner yield stronger empirical predictive performance decoder base long short term memory lstm less parameters considerably faster train apply text sequence match problems propose model significantly outperform several strong sentence encode baselines especially semi supervise set
social media serve unify platform users express thoughts subject range daily live opinion consumer brand products users wield enormous influence shape opinions consumers influence brand perception brand loyalty brand advocacy paper analyze opinion 19m twitter users towards sixty-two popular industries encompass twelve thousand, eight hundred and ninety-eight enterprise consumer brand well associate subject matter topics via sentiment analysis 330m tweet period span month find users tend positive towards manufacture negative towards service industries addition tend positive negative interact brand generally twitter also find sentiment towards brand within industry vary greatly demonstrate use two industries use case addition discover strong correlation topic sentiments different industries demonstrate topic sentiments highly dependent context industry mention demonstrate value analysis order assess impact brand social media hope initial study prove valuable researchers company understand users perception industries brand associate topics encourage research field
conventional automatic speech recognition asr typically perform multi level pattern recognition task map acoustic speech waveform hierarchy speech units widely know information loss earlier stage propagate later stag resurgence deep learn interest emerge possibility develop purely end end asr system raw waveform transcription without predefined alignments hand engineer model however successful attempt end end architecture still use spectral base feature successful attempt use raw waveform still base hybrid deep neural network hide markov model dnn hmm framework paper construct first end end attention base encoder decoder model process directly raw speech waveform text transcription call model attention base wav2text assist train process end end model propose utilize feature transfer learn experimental result also reveal propose attention base wav2text model directly raw waveform could achieve better result comparison attentional encoder decoder model train standard front end filterbank feature
introduce general purpose condition method neural network call film feature wise linear modulation film layer influence neural network computation via simple feature wise affine transformation base condition information show film layer highly effective visual reason answer image relate question require multi step high level process task prove difficult standard deep learn methods explicitly model reason specifically show visual reason task film layer one halve state art error clevr benchmark two modulate feature coherent manner three robust ablations architectural modifications four generalize well challenge new data examples even zero shoot
convolutional neural network cnns recently emerge popular build block natural language process nlp despite success exist cnn model employ nlp share learn static set filter input sentence paper consider approach use small meta network learn context sensitive convolutional filter text process role meta network abstract contextual information sentence document set input aware filter generalize framework model sentence pair bidirectional filter generation mechanism introduce encapsulate co dependent sentence representations benchmarks four different task include ontology classification sentiment analysis answer sentence selection paraphrase identification propose model modify cnn context sensitive filter consistently outperform standard cnn attention base cnn baselines visualize learn context sensitive filter validate rationalize effectiveness propose framework
automatically generate coherent semantically meaningful text many applications machine translation dialogue systems image caption etc recently combine policy gradient generative adversarial net gin use discriminative model guide train generative model reinforcement learn policy show promise result text generation however scalar guide signal available entire text generate lack intermediate information text structure generative process limit success length generate text sample long twenty word paper propose new framework call leakgan address problem long text generation allow discriminative net leak high level extract feature generative net help guidance generator incorporate informative signal generation step additional manager module take extract feature current generate word output latent vector guide worker module next word generation extensive experiment synthetic data various real world task turing test demonstrate leakgan highly effective long text generation also improve performance short text generation scenarios importantly without supervision leakgan would able implicitly learn sentence structure interaction manager worker
propose object orient neural program oonp framework semantically parse document specific domains basically oonp read document parse predesigned object orient data structure refer ontology paper reflect domain specific semantics document oonp parser model semantic parse decision process neural net base reader sequentially go document process build update intermediate ontology summarize partial understand text cover oonp support rich family operations symbolic differentiable compose ontology big variety form symbolic differentiable represent state document oonp parser train supervision different form strength include supervise learn sl reinforcement learn rl hybrid two experiment synthetic real world document parse task show oonp learn handle fairly complicate ontology train data modest size
paper present intertemporal bimodal network analyze evolution semantic content scientific field within framework topic model namely use latent dirichlet allocation lda main contribution conceptualization topic dynamics formalization codification algorithm benchmark effectiveness approach propose three index track transformation topics time rate birth death novelty content apply lda test algorithm control experiment corpus several thousands scientific paper period one hundred years account history economic think
effective collaboration robot person require natural communication robot travel human companion robot able explain navigation behavior natural language paper explain cognitively base autonomous robot navigation system produce informative intuitive explanations decisions language generation base upon robot commonsense qualitative reason learn spatial model approach produce natural explanations real time robot navigate large complex indoor environment
previous study demonstrate empirical success word embeddings various applications paper investigate problem learn distribute representations text document many machine learn algorithms take input number nlp task propose neural network model keyvec learn document representations goal preserve key semantics input text enable learn low dimensional vectors retain topics important information document flow downstream task empirical evaluations show superior quality keyvec representations two different document understand task
use machine learn algorithms include deep learn study prediction personal attribute text tweet gender occupation age group apply word2vec construct word vectors use vectorize tweet block result tweet vectors use input train model prediction accuracy model examine function dimension tweet vectors size tweet black result show machine learn algorithms could predict three personal attribute interest sixty seventy accuracy
paper propose novel neural machine read model open domain question answer scale exist machine comprehension model typically assume short piece relevant text contain answer already identify give model model design extract answer assumption however realistic build large scale open domain question answer system require deep text understand identify relevant text corpus simultaneously paper introduce neural comprehensive ranker ncr integrate passage rank answer extraction one single framework qanda system base framework allow users issue open domain question without need provide piece text must contain answer experiment show unify ncr model able outperform state art retrieval relevant text answer extraction
word embeddings powerful approach analyze language exponential family embeddings efe extend type data develop structure exponential family embeddings efe method discover embeddings vary across relate group data study word usage yous congressional speeches vary across state party affiliation word use differently across section arxiv co purchase pattern groceries vary across season key success method group share statistical information develop two share strategies hierarchical model amortization demonstrate benefit approach empirical study speeches abstract shop baskets show efe enable group specific interpretation word usage outperform efe predict hold data
present optimise multi modal dialogue agent interactive learn visually ground word mean human tutor train real human human tutor data within life long interactive learn period agent train use reinforcement learn rl must able handle natural conversations human users achieve good learn performance accuracy minimise human effort learn process train evaluate system interaction simulate human tutor build burchak corpus human human dialogue dataset visual learn task result show one learn policy coherently interact simulate user achieve goal task ie learn visual attribute object eg colour shape two find better trade classifier accuracy tutor cost hand craft rule base policies include ones dynamic policies
present multi modal dialogue system interactive learn perceptually ground word mean human tutor system integrate incremental semantic parse generation framework dynamic syntax type theory record ds ttr set visual classifiers learn throughout interaction grind mean representations produce use system interaction simulate human tutor study effect different dialogue policies capabilities accuracy learn mean learn rat efforts cost tutor show overall performance learn agent affect one take initiative dialogues two ability express use confidence level visual attribute three ability process elliptical incrementally construct dialogue turn ultimately train adaptive dialogue policy optimise trade classifier accuracy tutor cost
motivate describe new freely available human human dialogue dataset interactive learn visually ground word mean ostensive definition tutor learner data collect use novel character character variant diet chat tool healey et al two thousand and three mill healey submit novel task learner need learn invent visual attribute word burchak square tutor text base interactions closely resemble face face conversation thus contain many linguistic phenomena encounter natural spontaneous dialogue include self correction mid sentence continuations interruptions overlap fillers hedge also present generic n gram framework build user ie tutor simulations type incremental data freely available researchers show simulations produce output similar original data eg seventy-eight turn match similarity finally train evaluate reinforcement learn dialogue control agent learn visually ground word mean train burchak corpus learn policy show comparable performance rule base system build previously
pioneer research g k zipf observe frequent word tend mean show number mean word grow square root frequency derive relationship two assumptions word follow zipf law word frequencies power law dependency frequency rank zipf law mean distribution power law dependency number mean rank show single assumption joint probability word mean suffice infer zipf mean frequency law relax versions interestingly assumption justify outcome bias random walk process mental exploration
players online ad ecosystem struggle acquire user data require precise target audience look alike model potential alleviate issue model performance strongly depend quantity quality available data order maximize predictive performance look alike model algorithms propose two novel hybrid filter techniques utilize recent neural probabilistic language model algorithm doc2vec apply methods data large mobile ad exchange additional app metadata acquire apple app store google play store first model mobile app users app usage histories app descriptions user2vec second introduce context awareness model incorporate additional user app relate metadata model train context2vec find threefold one quality recommendations provide user2vec notably higher current state art techniques two user representations generate hybrid filter use doc2vec prove highly valuable feature supervise machine learn model look alike model represent first application hybrid filter user model use neural probabilistic language model specifically doc2vec look alike model three incorporate context metadata doc2vec model train process introduce context awareness positive effect performance superior directly include data feature downstream supervise model
global recruitment radical islamic movements spur renew interest appeal political extremism appeal rational response material condition expression psychological personality disorder associate aggressive behavior intolerance conspiratorial imagination paranoia empirical answer use survey limit lack access extremist group field study lack psychological measure fail compare extremists contrast group revisit debate appeal extremism yous context compare publicly available twitter message write three hundred and fifty-five thousand political extremist followers message write non extremist yous users analysis text base psychological indicators support moral foundation theory identify emotion critical factor determine political orientation individuals extremist followers also differ others four big five personality traits
implicit discourse relation classification great challenge due lack connectives strong linguistic cue motivate use annotate implicit connectives improve recognition propose feature imitation framework implicit relation network drive learn another neural network access connectives thus encourage extract similarly salient feature accurate classification develop adversarial model enable adaptive imitation scheme competition implicit network rival feature discriminator method effectively transfer discriminability connectives implicit feature achieve state art performance pdtb benchmark
keyphrase boundary classification kbc task detect keyphrases scientific article label respect predefined type although important practice task far underexplored partly due lack label data overcome explore several auxiliary task include semantic super sense tag identification multi word expressions cast task multi task learn problem deep recurrent neural network multi task model perform significantly better previous state art approach two scientific kbc datasets particularly long keyphrases
despite increase use social media platforms information news gather unmoderated nature often lead emergence spread rumour ie piece information unverified time post time openness social media platforms provide opportunities study users share discuss rumour explore natural language process data mine techniques may use find ways determine veracity survey introduce discuss two type rumour circulate social media long stand rumour circulate long periods time newly emerge rumour spawn fast pace events break news report release piecemeal often unverified status early stag provide overview research social media rumour ultimate goal develop rumour classification system consist four components rumour detection rumour track rumour stance classification rumour veracity classification delve approach present scientific literature development four components summarise efforts achievements far towards development rumour classification systems conclude suggestions avenues future research social media mine detection resolution rumour
theory mind ability attribute mental state beliefs intents knowledge perspectives etc others recognize mental state may differ one theory mind critical effective communication team demonstrate higher collective performance effectively leverage progress artificial intelligence ai make live productive important humans ai work well together team traditionally much emphasis research make ai accurate lesser extent better understand human intentions tendencies beliefs contexts latter involve make ai human like develop theory mind work argue human ai team effective humans must also develop theory ai mind toaim get know strengths weaknesses beliefs quirk instantiate ideas within domain visual question answer vqa find use examples fifty lay people train better predict responses oncoming failures complex vqa model evaluate role exist explanation interpretability modalities play help humans build toaim explainable ai receive considerable scientific popular attention recent time surprisingly find access model internal state confidence top k predictions explicit implicit attention map highlight regions image word question model look listen answer question image help people better predict behavior
explore properties byte level recurrent language model give sufficient amount capacity train data compute time representations learn model include disentangle feature correspond high level concepts specifically find single unit perform sentiment analysis representations learn unsupervised manner achieve state art binary subset stanford sentiment treebank also data efficient use handful label examples approach match performance strong baselines train full datasets also demonstrate sentiment unit direct influence generative process model simply fix value positive negative generate sample correspond positive negative sentiment
fifty million scholarly article publish constitute unique repository knowledge particular one may infer relations scientific concepts synonyms hyponyms artificial neural network recently explore relation extraction work continue line work present system base convolutional neural network extract relations model rank first semeval two thousand and seventeen task ten scienceie relation extraction scientific article subtask c
consider problem parse natural language descriptions source code write general purpose program language like python exist data drive methods treat problem language generation task without consider underlie syntax target program language inform previous work semantic parse paper propose novel neural architecture power grammar model explicitly capture target syntax prior knowledge experiment find effective way scale generation complex program natural language descriptions achieve state art result well outperform previous code generation semantic parse approach
great variety text task topic spam identification user profile sentiment analysis pose supervise learn problem tackle use text classifier text classifier consist several subprocesses general enough apply supervise learn problem whereas others specifically design tackle particular task use complex computational expensive process lemmatization syntactic analysis etc contrary traditional approach propose minimalistic wide system able tackle text classification task independent domain language namely microtc compose easy implement text transformations text representations supervise learn algorithm piece produce competitive classifier even domain informally write text provide detail description microtc along extensive experimental comparison relevant state art methods mircotc compare thirty different datasets regard accuracy microtc obtain best performance twenty datasets achieve competitive result remain ten compare datasets include several problems like topic polarity classification spam detection user profile authorship attribution furthermore important state approach allow usage technology even without knowledge machine learn natural language process
recently deep learn methods show improve performance recommender systems traditional methods especially review text available example recent model deepconn use neural net learn one latent representation text review write target user second latent representation text review target item combine latent representations obtain state art performance recommendation task show unsurprisingly much predictive value review text come review target user target item introduce way information use recommendation even target user review target item available model call transnets extend deepconn model introduce additional latent layer represent target user target item pair regularize layer train time similar another latent representation target user review target item show transnets extensions improve substantially previous state art
sentence simplification reduce semantic complexity benefit people language impairments previous simplification study sentence level word level achieve promise result also meet great challenge sentence level study sentence simplification fluent sometimes really simplify word level study word simplify also potential grammar errors due different usages word simplification paper propose two step simplification framework combine word level sentence level simplifications make use correspond advantage base two step framework implement novel constrain neural generation model simplify sentence give simplify word final result wikipedia simple wikipedia align datasets indicate method yield better performance various baselines
voice conversion vc use sequence sequence learn context posterior probabilities propose conventional vc use share context posterior probabilities predict target speech parameters context posterior probabilities estimate source speech parameters although conventional vc build non parallel data difficult convert speaker individuality phonetic property speak rate contain posterior probabilities source posterior probabilities directly use predict target speech parameters work assume train data partly include parallel speech data propose sequence sequence learn source target posterior probabilities conversion model perform non linear variable length transformation source probability sequence target one propose joint train algorithm modules contrast conventional vc separately train speech recognition estimate posterior probabilities speech synthesis predict target speech parameters propose method jointly train modules along propose probability conversion modules experimental result demonstrate approach outperform conventional vc
popular word embed techniques involve implicit explicit factorization word co occurrence base matrix low rank factor paper aim generalize trend use numerical methods factor higher order word co occurrence base array textittensors present four word embeddings use tensor factorization analyze advantage disadvantage one main contributions novel joint symmetric tensor factorization technique relate idea couple tensor factorization show embeddings base tensor factorization use discern various mean polysemous word without explicitly train motivate intuition behind work way exist methods also modify exist word embed evaluation metric know outlier detection camacho collados navigli two thousand and sixteen evaluate quality order n relations word embed capture show tensor base methods outperform exist matrix base methods task experimentally show word embeddings either outperform competitive state art baselines commonly use today variety recent datasets suggest applications tensor factorization base word embeddings give source code pre train vectors publicly available online
describe semeval task extract keyphrases relations scientific document crucial understand publications describe process task materials although new task total twenty-six submissions across three evaluation scenarios expect task find report paper relevant researchers work understand scientific content well broader knowledge base population information extraction communities
major advance recently make merge language vision representations task consider far confine process object lexicalise relations amongst object content word know however humans even pre school children abstract raw data perform certain type higher level reason express natural language function word case point give ability learn quantifiers ie expressions like formal semantics cognitive linguistics know quantifiers relations set simplification see proportion instance fish red encode proportion fish red fish paper study well current language vision strategies model relations show state art attention mechanisms couple traditional linguistic formalisation quantifiers give best performance task additionally provide insights role gist representations quantification logical strategy tackle task would first obtain numerosity estimation two involve set compare cardinalities however argue precisely identify composition set beyond current state art model perhaps even detrimental task efficiently perform refine approximate numerosity estimator system
build dialogue agent fulfill complex task travel plan challenge agent learn collectively complete multiple subtasks example agent need reserve hotel book flight leave enough time commute arrival hotel check paper address challenge formulate task mathematical framework options markov decision process mdps propose hierarchical deep reinforcement learn approach learn dialogue manager operate different temporal scale dialogue manager consist one top level dialogue policy select among subtasks options two low level dialogue policy select primitive action complete subtask give top level policy three global state tracker help ensure cross subtask constraints satisfy experiment travel plan task simulate real users show approach lead significant improvements three baselines two base handcraft rule base flat deep reinforcement learn
paper present automate supervise method persian wordnet construction use persian corpus bi lingual dictionary initial link persian word princeton wordnet synsets generate link discriminate later correct incorrect employ seven feature train classification system whole method classification system train train set contain farsnet set correct instance state art result automatically derive persian wordnet achieve result wordnet precision nine thousand, one hundred and eighteen include sixteen thousand word twenty-two thousand synsets
process mine analyze business process base events store event log however record events may correspond activities low level abstraction events record low level granularity process discovery methods tend generate overgeneralize process model group low level events higher level activities ie event abstraction use discover better process model exist event abstraction methods mainly base common sub sequence cluster techniques paper propose first discover local process model use model lift event log higher level abstraction conjecture process model discover obtain high level event log return process model higher quality fitness precision score balance show preliminary result several real life event log
open domain question answer qa systems prove effective answer simple question struggle complex question goal answer complex question reliably without incur significant cost knowledge resource construction support qa one readily available knowledge resource term bank enumerate key concepts domain develop unsupervised learn approach leverage term bank guide qa system represent terminological knowledge thousands specialize vector space experiment complex science question show approach significantly outperform several state art qa systems demonstrate significant leverage gain continuous vector representations domain terminology
output agreement mechanisms esp game widely use human computation obtain reliable human generate label paper argue time limit output agreement mechanism use create fast robust crowd power component interactive systems particularly dialogue systems extract key information user utterances fly experiment amazon mechanical turk use airline travel information system atis dataset show propose approach achieve high quality result average response time shorter nine second
present new model sing synthesis base modify version wavenet architecture instead model raw waveform model feature produce parametric vocoder separate influence pitch timbre allow conveniently modify pitch match target melody facilitate train modest dataset size significantly reduce train generation time model make frame wise predictions use mixture density output rather categorical output order reduce require parameter count find overfitting issue relatively small datasets use experiment propose method regularize model make autoregressive generation process robust prediction errors use simple multi stream architecture harmonic aperiodic voice unvoiced components predict coherent manner compare method exist parametric statistical state art concatenative methods use quantitative metrics listen test naive implementations autoregressive generation algorithm tend inefficient use smart algorithm greatly speed process obtain system competitive speed quality
ability model generative process learn latent representation speech unsupervised fashion crucial process vast quantities unlabelled speech data recently deep probabilistic generative model variational autoencoders vaes achieve tremendous success model natural image paper apply convolutional vae model generative process natural speech derive latent space arithmetic operations disentangle learn latent representations demonstrate capability model modify phonetic content speaker identity speech segment use derive operations without need parallel supervisory data
coreference evaluation metrics hard optimize directly non differentiable function easily decomposable elementary decisions consequently approach optimize objectives indirectly relate end goal result suboptimal performance instead propose differentiable relaxation lend gradient base optimisation thus bypass need reinforcement learn heuristic modification cross entropy show modify train objective competitive neural coreference system obtain substantial gain performance suggest approach regard viable alternative use reinforcement learn computationally expensive imitation learn
introduce novel framework evaluate multimodal deep learn model respect language understand generalization abilities approach artificial data automatically generate accord experimenter specifications content data train evaluation control detail enable task create require true generalization abilities particular combination previously introduce concepts novel ways demonstrate potential methodology evaluate various visual question answer model four different task show framework give us detail insights capabilities limitations open source framework hope stimulate progress field multimodal language understand
paper propose online learn algorithm base rao blackwellized particle filter spatial concept acquisition map propose nonparametric bayesian spatial concept acquisition model spcoa propose novel method spcoslam integrate spcoa fastslam theoretical framework bayesian generative model propose method simultaneously learn place categories lexicons incrementally generate environmental map furthermore propose method scene image feature language model add spcoa experiment test online learn spatial concepts environmental map novel environment robot map evaluate result online learn spatial concepts lexical acquisition experimental result demonstrate robot able accurately learn relationships word place environmental map incrementally use propose method
present race new dataset benchmark evaluation methods read comprehension task collect english exams middle high school chinese students age range twelve eighteen race consist near twenty-eight thousand passages near one hundred thousand question generate human experts english instructors cover variety topics carefully design evaluate students ability understand reason particular proportion question require reason much larger race benchmark datasets read comprehension significant gap performance state art model forty-three ceiling human performance ninety-five hope new dataset serve valuable resource research evaluation machine comprehension dataset freely available http wwwcscmuedu glai1 data race code available https githubcom qizhex racearbaselines
make distribute stochastic gradient descent faster exchange sparse update instead dense update gradient update positively skew update near zero map ninety-nine smallest update absolute value zero exchange sparse matrices method combine quantization improve compression explore different configurations apply neural machine translation mnist image classification task configurations work mnist whereas different configurations reduce convergence rate complex translation task experiment show achieve forty-nine speed mnist twenty-two nmt without damage final accuracy bleu
propose neural machine translation architecture model surround text addition source sentence model lead better performance term general translation quality pronoun prediction train small corpora although improvement largely disappear train larger corpus also discover attention base neural machine translation well suit pronoun prediction compare favorably approach specifically design task
paper propose novel approach aggregate online review accord opinions express methodology unsupervised due fact rely pre label review agnostic since make assumption domain language review content measure adherence review content domain terminology extract review set first demonstrate informativeness adherence metric respect score associate review exploit metric value group review accord opinions express experimental campaign carry two large datasets collect book amazon respectively
predict personality essential social applications support human center activities yet prior model methods users write text require much input data realistically use context social media work aim drastically reduce data requirement personality model develop model applicable users twitter model integrate word embed feature gaussian process regression base evaluation 13k users twitter find model achieve comparable better accuracy state art techniques eight time fewer data
earlier study collaborative chat intervention massive open online course mooc identify negative effect attrition stem requirement students match exactly one partner prior begin activity study raise question orchestrate collaborative chat intervention mooc context order provide benefit synchronous social engagement without coordination difficulties paper present careful analysis intervention design overcome coordination difficulties welcome students chat roll basis arrive rather require match partner begin result suggest positive impact experience chat exactly one partner rather less qualitative analysis chat data reveal differential experience configurations suggest potential explanation effect raise question future research
introduce self annotate reddit corpus sarc large corpus sarcasm research train evaluate systems sarcasm detection corpus thirteen million sarcastic statements ten time previous dataset many time instance non sarcastic statements allow learn balance unbalance label regimes statement furthermore self annotate sarcasm label author independent annotator provide user topic conversation context evaluate corpus accuracy construct benchmarks sarcasm detection evaluate baseline methods
propose multi view network text classification method automatically create various view input text take form soft attention weight distribute classifier focus among set base feature bag word representation view focus different subset text word aggregate many view result discriminative robust representation novel architecture stack concatenate view produce network emphasize depth width allow train converge quickly use multi view architecture establish new state art accuracies two benchmark task
knowledge base important resources variety natural language process task suffer incompleteness propose novel embed model emphitransf perform knowledge base completion equip sparse attention mechanism itransf discover hide concepts relations transfer statistical strength share concepts moreover learn associations relations concepts represent sparse attention vectors interpret easily evaluate itransf two benchmark datasets wn18 fb15k knowledge base completion obtain improvements mean rank hits10 metrics baselines use additional information
relation detection core component many nlp applications include knowledge base question answer kbqa paper propose hierarchical recurrent neural network enhance residual learn detect kb relations give input question method use deep residual bidirectional lstms compare question relation name via different hierarchies abstraction additionally propose simple kbqa system integrate entity link propose relation detector enable one enhance another experimental result evidence approach achieve outstanding relation detection performance importantly help kbqa system achieve state art accuracy single relation simplequestions multi relation webqsp qa benchmarks
bandit structure prediction describe stochastic optimization framework learn perform partial feedback feedback receive form task loss evaluation predict output structure without access gold standard structure advance framework lift linear bandit learn neural sequence sequence learn problems use attention base recurrent neural network furthermore show incorporate control variates learn algorithms variance reduction improve generalization present evaluation neural machine translation task show improvements five hundred and eighty-nine bleu point domain adaptation simulate bandit feedback
paper study new learn paradigm neural machine translation nmt instead maximize likelihood human translation previous work minimize distinction human translation translation give nmt model achieve goal inspire recent success generative adversarial network gans employ adversarial train architecture name adversarial nmt adversarial nmt train nmt model assist adversary elaborately design convolutional neural network cnn goal adversary differentiate translation result generate nmt model human goal nmt model produce high quality translations cheat adversary policy gradient method leverage co train nmt model adversary experimental result englishrightarrowfrench germanrightarrowenglish translation task show adversarial nmt achieve significantly better translation quality several strong baselines
goal create convenient natural language interface perform well specify complex action analyze data manipulate text query databases however exist natural language interfaces task quite primitive compare power one wield program language bridge gap start core program language allow users naturalize core language incrementally define alternative natural syntax increasingly complex concepts term compositions simpler ones voxel world show community users simultaneously teach common system diverse language use build hundreds complex voxel structure course three days users go use core language use naturalize language eight hundred and fifty-nine last 10k utterances
demonstrate continuous relaxation argmax operation use create differentiable approximation greedy decode sequence sequence seq2seq model incorporate approximation schedule sample train procedure bengio et al two thousand and fifteen well know technique correct exposure bias introduce new train objective continuous differentiable everywhere provide informative gradients near point previous decode decisions change value addition use relate approximation demonstrate similar approach sample base train finally show approach outperform cross entropy train schedule sample procedures two sequence prediction task name entity recognition machine translation
global constraints reranking use cognates detection research date propose methods use global constraints perform rescoring score matrices produce state art cognates detection systems use global constraints perform rescoring complementary state art methods perform cognates detection result significant performance improvements beyond current state art performance publicly available datasets different language pair various condition different level baseline state art performance different data size condition include realistic large data size condition evaluate past
visual question answer visual qa attract lot attention lately see essentially form visual turing test artificial intelligence strive achieve paper study crucial component task design good datasets task focus design multiple choice base datasets learner select right answer set candidate ones include target ie correct one decoy ie incorrect ones careful analysis result attain state art learn model human annotators exist datasets show design decoy answer significant impact learn model learn datasets particular result learner ignore visual information question still well task inspire propose automatic procedures remedy design deficiencies apply procedures construct decoy answer two popular visual qa datasets well create new visual qa dataset visual genome project result largest dataset task extensive empirical study show design deficiencies alleviate remedied datasets performance likely faithful indicator difference among learn model datasets release publicly available via http wwwtedsuscedu websitevqa
propose sequence label framework secondary train objective learn predict surround word every word dataset language model objective incentivises system learn general purpose pattern semantic syntactic composition also useful improve accuracy different sequence label task architecture evaluate range datasets cover task error detection learner texts name entity recognition chunk pos tag novel language model objective provide consistent performance improvements every benchmark without require additional annotate unannotated data
conversational speech acoustic signal provide cue help listeners disambiguate difficult parse automatically parse speak utterances introduce model integrate transcribe text acoustic prosodic feature use convolutional neural network energy pitch trajectories couple attention base recurrent neural network accept text prosodic feature find different type acoustic prosodic feature individually helpful together give statistically significant improvements parse disfluency detection f1 score strong text baseline study know sentence boundaries error analyse show main benefit acoustic prosodic feature sentence disfluencies attachment decisions improve transcription errors obscure gain prosody
video caption task describe content video see promise improvements recent years sequence sequence model accurately learn temporal logical dynamics involve task still remain challenge especially give lack sufficient annotate data improve video caption share knowledge two relate direct generation task temporally direct unsupervised video prediction task learn richer context aware video encoder representations logically direct language entailment generation task learn better video entail caption decoder representations present many many multi task learn model share parameters across encoders decoders three task achieve significant improvements new state art several standard video caption datasets use diverse automatic human evaluations also show mutual multi task improvements entailment generation task
task like code generation semantic parse require map unstructured partially structure input well form executable output introduce abstract syntax network model framework problems output represent abstract syntax tree asts construct decoder dynamically determine modular structure parallel structure output tree benchmark hearthstone dataset code generation model obtain seven hundred and ninety-two bleu two hundred and twenty-seven exact match accuracy compare previous state art value six hundred and seventy-one sixty-one furthermore perform competitively atis job geo semantic parse datasets task specific engineer
propose simple yet effective approach towards induce multilingual taxonomies wikipedia give english taxonomy approach leverage interlanguage link wikipedia follow character level classifiers induce high precision high coverage taxonomies languages experiment demonstrate approach significantly outperform state art heuristics heavy approach six languages consequence work release presumably largest accurate multilingual taxonomic resource span two hundred and eighty languages
propose novel semi supervise approach towards domain taxonomy induction input vocabulary seed term unlike previous approach typically extract direct hypernym edge term approach utilize novel probabilistic framework extract hypernym subsequences taxonomy induction extract subsequences cast instance minimumcost flow problem carefully design direct graph experiment demonstrate approach outperform stateof art taxonomy induction approach across four languages importantly also show approach robust presence noise input vocabulary best knowledge previous approach empirically prove manifest noise robustness input vocabulary
computer program write one language often require port languages support multiple devices environments program use language specific apis application program interfaces challenge migrate apis correspond apis write languages exist approach mine api mappings project correspond versions two languages rely sparse availability bilingual project thus produce limit number api mappings paper propose intelligent system call deepam automatically mine api mappings large scale code corpus without bilingual project key component deepam base multimodal sequence sequence learn architecture aim learn joint semantic representations bilingual api sequence big source code data experimental result indicate deepam significantly increase accuracy api mappings well number api mappings compare state art approach
understand ideas relate fundamental question many domains range intellectual history public communication ideas naturally embed texts propose first framework systematically characterize relations ideas base occurrence corpus document independent ideas represent combine two statistics cooccurrence within document prevalence correlation time approach reveal number different ways ideas cooperate compete instance two ideas closely track prevalence time yet rarely cooccur almost like cold war scenario observe pairwise cooccurrence prevalence correlation exhibit different distributions demonstrate approach able uncover intrigue relations ideas depth case study news article research paper
introduce attention base bi lstm chinese implicit discourse relations demonstrate model argument pair joint sequence outperform word order agnostic approach model benefit partial sample scheme conceptually simple yet achieve state art performance chinese discourse treebank also visualize attention activity illustrate model ability selectively focus relevant part input sequence
wit form rich interaction often ground specific situation eg comment response event work attempt build computational model produce witty descriptions give image inspire cognitive account humor appreciation employ linguistic wordplay specifically pun image descriptions develop two approach involve retrieve witty descriptions give image large corpus sentence generate via encoder decoder neural network architecture compare approach meaningful baseline approach via human study show substantial improvements find human subject similar constraints model regard word usage style people vote image descriptions generate model slightly wittier human write witty descriptions unsurprisingly humans almost always wittier model free choose vocabulary style etc
visual question answer vqa receive lot attention past couple years number deep learn model propose task however show model heavily drive superficial correlations train data lack compositionality ability answer question unseen compositions see concepts compositionality desirable central intelligence paper propose new set visual question answer test question answer pair compositionally novel compare train question answer pair facilitate develop model set present new compositional split vqa v10 dataset call compositional vqa c vqa analyze distribution question answer c vqa split finally evaluate several exist vqa model new set show performances model degrade significant amount compare original vqa set
word embeddings provide point representations word contain useful semantic information introduce multimodal word distributions form gaussian mixtures multiple word mean entailment rich uncertainty information learn distributions propose energy base max margin objective show result approach capture uniquely expressive semantic information outperform alternatives word2vec skip grams gaussian embeddings benchmark datasets word similarity entailment
despite impressive improvements achieve unsupervised deep neural network computer vision nlp task improvements yet observe rank information retrieval reason may complexity rank problem obvious learn query document supervise signal available hence paper propose train neural rank model use weak supervision label obtain automatically without human annotators external resources eg click data aim use output unsupervised rank model bm25 weak supervision signal train set simple yet effective rank model base fee forward neural network study effectiveness various learn scenarios point wise pair wise model use different input representations ie encode query document pair dense sparse vectors use word embed representation train network use tens millions train instance evaluate two standard collections homogeneous news collectionrobust heterogeneous large scale web collection clueweb experiment indicate employ proper objective function let network learn input representation base weakly supervise data lead impressive performance thirteen thirty-five map improvements bm25 model robust clueweb collections find also suggest supervise neural rank model greatly benefit pre train large amount weakly label data easily obtain unsupervised ir model
present superpivot analysis method low resource languages occur superparallel corpus ie corpus contain order magnitude languages parallel corpora currently use show superpivot perform well crosslingual analysis linguistic phenomenon tense produce analysis result one thousand languages conduct best knowledge largest crosslingual computational study perform date extend exist methodology leverage parallel corpora typological analysis overcome limit assumption earlier work require linguistic feature overtly mark thousands languages oppose require mark languages investigation
robots eventually part every household thus critical enable algorithms learn guide non expert users paper bring human loop enable human teacher give feedback learn agent form natural language argue descriptive sentence provide much stronger learn signal numeric reward easily point mistake correct focus problem image caption quality output easily judge non experts propose hierarchical phrase base caption model train policy gradients design feedback network provide reward learner condition human provide feedback show exploit descriptive feedback model learn perform better give independently write human caption
end end train automate speech recognition asr systems require massive data compute resources explore transfer learn base model adaptation approach train asr model constrain gpu memory throughput train data conduct several systematic experiment adapt wav2letter convolutional neural network originally train english asr german language show technique allow faster train consumer grade resources require less train data order achieve accuracy thereby lower cost train asr model languages model introspection reveal small adaptations network weight sufficient good performance especially inner layer
topic model widely explore probabilistic generative model document traditional inference methods seek close form derivations update model however expressiveness model grow difficulty perform fast accurate inference parameters paper present alternative neural approach topic model provide parameterisable distributions topics permit train backpropagation framework neural variational inference addition help stick break construction propose recurrent network able discover notionally unbounded number topics analogous bayesian non parametric topic model experimental result mxm song lyric 20newsgroups reuters news datasets demonstrate effectiveness efficiency neural topic model
present attract repel algorithm improve semantic quality word vectors inject constraints extract lexical resources attract repel facilitate use constraints mono cross lingual resources yield semantically specialise cross lingual vector space evaluation show method make use exist cross lingual lexicons construct high quality vector space plethora different languages facilitate semantic transfer high lower resource ones effectiveness approach demonstrate state art result semantic similarity datasets six languages next show attract repel specialise vectors boost performance downstream task dialogue state track dst across multiple languages finally show cross lingual vector space produce algorithm facilitate train multilingual dst model bring performance improvements
much scientific progress stem previously publish find search vast sea scientific publications difficult often rely metrics scholarly authority find prominent author authority indices differentiate authority base research topics present latent topical authority index ltai jointly model topics citations topical authority corpus academic paper compare previous model ltai differ two main aspects first explicitly model generative process citations rather treat citations give second model author influence citations paper base topics cite paper well cite paper fit ltai four academic corpora cora arxiv physics pnas citeseer compare performance ltai various baselines start latent dirichlet allocation advance model include author link topic model dynamic author citation topic model result show ltai achieve improve accuracy similar model predict word citations author publications
task specific word identification aim choose task relate word best describe short text exist approach require well define seed word lexical dictionaries eg wordnet often unavailable many applications social discrimination detection fake review detection however often set label short texts short text task relate class label eg discriminatory non discriminatory specify users learn classification algorithms paper focus identify task specific word phrase short texts exploit class label rather use seed word lexical dictionaries consider task specific word phrase identification feature learn train convolutional neural network set label texts use score vectors localize task specific word phrase experimental result sentiment word identification show approach significantly outperform exist methods conduct two case study show effectiveness approach one case study crawl tweet dataset demonstrate approach successfully capture discrimination relate word phrase case study fake review detection show approach identify fake review word phrase
wikipedia largest online encyclopedia allow anyone edit article paper propose use deep learn detect vandals base edit history particular develop multi source long short term memory network lstm model user behaviors use variety user edit aspects input include history edit reversion information edit page title categories lstm encode user low dimensional real vector call user embed meanwhile sequential model lstm update user embed time user commit new edit thus predict whether user benign vandal dynamically base date user embed furthermore user embeddings crucial discover collaborative vandals
learn good representation text key many recommendation applications examples include news recommendation texts recommend constantly publish everyday however exist recommendation techniques matrix factorization base methods mainly rely interaction histories learn representations items latent factor items learn effectively user interaction data many case data available especially newly emerge items work aim address problem personalize recommendation completely new items text information available cast problem personalize text rank problem propose general framework combine text embed personalize recommendation users textual content embed latent feature space text embed function learn end end predict user interactions items alleviate sparsity interaction data leverage large amount text data little user interactions propose joint text embed model incorporate unsupervised text embed combination module experimental result show model significantly improve effectiveness recommendation systems real world datasets
discuss problems standard approach evaluation task like visual question answer argue artificial data use address complement current practice demonstrate help exist deep linguistic process technology able create challenge abstract datasets enable us investigate language understand abilities multimodal deep learn model detail compare single performance value static monolithic dataset
automate story generation problem automatically select sequence events action word tell story seek develop system generate stories learn everything need know textual story corpora date recurrent neural network learn language model character word sentence level little success generate coherent stories explore question event representations provide mid level abstraction word sentence order retain semantic information original data minimize event sparsity present technique preprocessing textual story data event sequence present technique automate story generation whereby decompose problem generation successive events event2event generation natural language sentence events event2sentence give empirical result compare different event representations effect event successor generation translation events natural language
propose generative machine comprehension model learn jointly ask answer question base document propose model use sequence sequence framework encode document generate question answer give answer question significant improvement model performance observe empirically squad corpus confirm hypothesis model benefit jointly learn perform task believe joint model novelty offer new perspective machine comprehension beyond architectural engineer serve first step towards autonomous information seek
present novel train framework neural sequence model particularly ground dialog generation standard train paradigm model maximum likelihood estimation mle minimize cross entropy human responses across variety domains recur problem mle train generative neural dialog model g tend produce safe generic responses know tell contrast discriminative dialog model train rank list candidate human responses outperform generative counterparts term automatic metrics diversity informativeness responses however useful practice since deploy real conversations users work aim achieve best worlds practical usefulness g strong performance via knowledge transfer g primary contribution end end trainable generative visual dialog model g receive gradients perceptual adversarial loss sequence sample g leverage recently propose gumbel softmax gs approximation discrete distribution specifically rnn augment sequence gs samplers couple straight gradient estimator enable end end differentiability also introduce stronger encoder visual dialog employ self attention mechanism answer encode along metric learn loss aid better capture semantic similarities answer responses overall propose model outperform state art visdial dataset significant margin two hundred and sixty-seven recall10 source code download https githubcom jiasenlu visdialpytorch
state art methods protein protein interaction ppi extraction primarily feature base kernel base leverage lexical syntactic information incorporate knowledge recent deep learn methods remain open question paper propose multichannel dependency base convolutional neural network model mcdepcnn apply one channel embed vector word sentence another channel embed vector head correspond word therefore model use richer information obtain different channel experiment two public benchmarking datasets aim bioinfer demonstrate mcdepcnn compare favorably state art rich feature single kernel base methods addition mcdepcnn achieve two hundred and forty-four relative improvement f1 score state art methods cross corpus evaluation twelve improvement f1 score kernel base methods difficult instance result suggest mcdepcnn generalize easily different corpora capable capture long distance feature sentence
internet online forums reddit become increasingly popular medium citizens engage political conversations however online disinhibition effect result ability use pseudonymous identities may manifest form offensive speech consequently make political discussions aggressive polarize already environments may result harassment self censorship target paper present preliminary result large scale temporal measurement aim quantify offensiveness online political discussions enable measurements develop evaluate offensive speech classifier use classifier quantify compare offensiveness political general contexts perform study use database 168m reddit comment make 7m pseudonyms january two thousand and fifteen january two thousand and seventeen period cover several divisive political events include two thousand and sixteen us presidential elections
ladder network notable new concept field semi supervise learn show state art result image recognition task compatible many exist neural architectures present recurrent ladder network novel modification ladder network semi supervise learn recurrent neural network evaluate phoneme recognition task timit corpus result show model able consistently outperform baseline achieve fully supervise baseline performance seventy-five label demonstrate model capable use unsupervised data effective regulariser
recurrent neural network rnns powerful scheme model temporal sequential data need capture long term dependencies datasets represent hide layer powerful model capture information input model long term dependencies dataset gate mechanism concept help rnns remember forget previous information represent hide layer rnn expressive operations ie tensor products help learn complex relationship current input previous hide layer information ideas generally improve rnn performances paper propose novel rnn architecture combine concepts gate mechanism tensor product single model combine two concepts single rnn propose model learn long term dependencies model gate units obtain expressive direct interaction input hide layer use tensor product three dimensional array tensor weight parameters use long short term memory lstm rnn gate recurrent unit gru rnn combine tensor product inside formulations propose rnns call long short term memory recurrent neural tensor network lstmrntn gate recurrent unit recurrent neural tensor network grurntn make combine lstm gru rnn model tensor product conduct experiment propose model word level character level language model task reveal propose model significantly improve performance compare baseline model
simple architecture ability learn meaningful word embeddings efficiently texts contain billions word word2vec remain one popular neural language model use today however single embed learn every word vocabulary model fail optimally represent word multiple mean additionally possible create embeddings new vocabulary word spot base intuitive interpretation continuous bag word cbow word2vec model negative sample train objective term predict context base similarities motivate extension model call context encoders conec multiply matrix train word2vec embeddings word average context vector vocabulary oov embeddings representations word multiple mean create base word local contexts benefit approach illustrate use word embeddings feature conll two thousand and three name entity recognition ner task
common sense background knowledge require understand natural language neural natural language understand nlu systems knowledge must acquire train corpora learn static test time introduce new architecture dynamic integration explicit background knowledge nlu model general purpose read module read background knowledge form free text statements together task specific text input yield refine word representations task specific nlu architecture reprocess task input representations experiment document question answer dqa recognize textual entailment rte demonstrate effectiveness flexibility approach analysis show model learn exploit knowledge semantically appropriate way
recognition social signal human facial expressions prosody speech popular research topic human robot interaction study also long line research speak dialogue community investigate user satisfaction relation dialogue characteristics however little research relate combination multimodal social signal language feature detect speak face face human robot interaction result user perception robot paper show different emotional facial expressions human users combination prosodic characteristics human speech feature human robot dialogue correlate users impressions robot conversation find happiness user recognise facial expression strongly correlate likeability robot dialogue relate feature number human turn number sentence per robot utterance correlate perceive robot intelligent addition show facial expression emotional feature prosody better predictors human rat relate perceive robot likeability anthropomorphism linguistic non linguistic feature often predict perceive robot intelligence interpretability characteristics may future use online reward signal situ reinforcement learn base adaptive human robot dialogue systems
state level minimum bay risk smbr train become de facto standard sequence level train speech recognition acoustic model elegant formulation use expectation semiring give large improvements word error rate wer model train solely use cross entropy ce connectionist temporal classification ctc smbr train optimize expect number frame reference hypothesize acoustic state differ may preferable optimize expect wer wer interact well expectation semiring previous approach base compute expect wer exactly involve expand lattices use train paper show perform optimization expect wer sample paths lattices use conventional smbr train gradient expect wer expectation may approximate use monte carlo sample show experimentally optimize wer acoustic model train give five relative improvement wer well tune smbr baseline two channel query recognition task google home
deep convolutional neural network actively investigate wide range speech audio process applications include speech recognition audio event detection computational paralinguistics owe ability reduce factor variations learn speech however study suggest favor certain type convolutional operations build deep convolutional neural network speech applications although promise result use different type convolutional operations work study four type convolutional operations different input feature speech emotion recognition noisy clean condition order derive comprehensive understand since affective behavioral information show reflect temporally vary mental state convolutional operation apply locally time deep neural network share deep recurrent sub network architecture temporal model present detail quantitative module wise performance analysis gain insights information flow within propose architectures particular demonstrate interplay affective information irrelevant information progression one module another finally show deep neural network provide state art performance enterface five corpus
study skip think model neighborhood information weak supervision specifically propose skip think neighbor model consider adjacent sentence neighborhood train skip think neighbor model large corpus continuous sentence evaluate train model seven task include semantic relatedness paraphrase detection classification benchmarks quantitative comparison qualitative investigation conduct empirically show skip think neighbor model perform well skip think model evaluation task addition find incorporate autoencoder path model aid model perform better hurt performance skip think model
factoid question answer qa recently benefit development deep learn dl systems neural network model outperform traditional approach domains large datasets exist squad ca one hundred thousand question wikipedia article however systems yet apply qa specific domains biomedicine datasets generally small train dl system scratch example bioasq dataset biomedical qa comprise less nine hundred factoid single answer list multiple answer qa instance work adapt neural qa system train large open domain dataset squad source biomedical dataset bioasq target employ various transfer learn techniques network architecture base state art qa system extend biomedical word embeddings novel mechanism answer list question contrast exist biomedical qa systems system rely domain specific ontologies parsers entity taggers expensive create despite fact systems achieve state art result factoid question competitive result list question
unsupervised learn low dimensional semantic representations word entities recently gain attention paper describe semantic entity retrieval toolkit sert provide implementations previously publish entity representation model toolkit provide unify interface different representation learn algorithms fine grain parse configuration use transparently gpus addition users easily modify exist model implement model framework model train sert use rank entities accord textual query extract learn entity word representation use downstream algorithms cluster recommendation
study representation encode phonemes recurrent neural network model ground speech use model process image speak descriptions project visual auditory representations semantic space perform number analyse information individual phonemes encode mfcc feature extract speech signal activations layer model via experiment phoneme decode phoneme discrimination show phoneme representations salient lower layer model low level signal process fine grain level although large amount phonological information retain top recurrent layer find attention mechanism follow top recurrent layer significantly attenuate encode phonology make utterance embeddings much invariant synonymy moreover hierarchical cluster phoneme representations learn network show organizational structure phonemes similar propose linguistics
generative adversarial network gin achieve great success generate realistic real value synthetic data however convergence issue difficulties deal discrete data hinder applicability gin text propose framework generate realistic text via adversarial train employ long short term memory network generator convolutional network discriminator instead use standard objective gin propose match high dimensional latent feature distributions real synthetic sentence via kernelized discrepancy metric ease adversarial train alleviate mode collapse problem experiment show superior performance quantitative evaluation demonstrate model generate realistic look sentence
show relation extraction reduce answer simple read comprehension question associate one natural language question relation slot reduction several advantage one learn relation extraction model extend recent neural read comprehension techniques two build large train set model combine relation specific crowd source question distant supervision even three zero shoot learn extract new relation type specify test time label train examples experiment wikipedia slot fill task demonstrate approach generalize new question know relation type high accuracy zero shoot generalization unseen relation type possible lower accuracy level set bar future work task
deep latent variable model train use variational autoencoders generative adversarial network key technique representation learn continuous structure however apply similar methods discrete structure text sequence discretized image prove challenge work propose flexible method train deep latent variable model discrete structure approach base recently propose wasserstein autoencoder wae formalize adversarial autoencoder aae optimal transport problem first extend framework model discrete sequence explore different learn priors target controllable representation adversarially regularize autoencoder arae allow us generate natural textual output well perform manipulations latent space induce change output space finally show latent representation train perform unaligned textual style transfer give improvements automatic human evaluation compare exist methods
discuss stationary stochastic process nonergodic random persistent topic detect infinite random text sample process whereas call process strongly nonergodic infinite sequence independent random bits call probabilistic facts need describe topic completely replace probabilistic facts algorithmically random sequence bits call algorithmic facts adapt property back ergodic process subsequently call process perigraphic number algorithmic facts infer finite text sample process grow like power text length present simple example process moreover demonstrate assertion call theorem facts word proposition state number probabilistic algorithmic facts infer text draw process must roughly smaller number distinct word like string detect text mean ppm compression algorithm also observe number word like string sample play shakespeare follow empirical stepwise power law stark contrast markov process hence suppose natural language consider process non markov also perigraphic
propose two stage neural model tackle question generation document first model estimate probability word sequence document ones human would pick select candidate answer train neural key phrase extractor answer question answer corpus predict key phrase act target answer condition sequence sequence question generation model copy mechanism empirically key phrase extraction model significantly outperform entity tag baseline exist rule base approach demonstrate question generation system formulate fluent answerable question key phrase two stage system could use augment generate read comprehension datasets may leverage improve machine read systems educational settings
joint extraction entities relations important task information extraction tackle problem firstly propose novel tag scheme convert joint extraction task tag problem base tag scheme study different end end model extract entities relations directly without identify entities relations separately conduct experiment public dataset produce distant supervision method experimental result show tag base methods better exist pipelined joint learn methods end end model propose paper achieve best result public dataset
topic model extensively use organize interpret content large unstructured corpora text document although topic model often perform well traditional train vs test set evaluations often case result topic model align human interpretation interpretability fallacy largely due unsupervised nature topic model prohibit user guidance result model paper introduce semi supervise method call topic supervise non negative matrix factorization ts nmf enable user provide label example document promote discovery meaningful semantic structure corpus way result ts nmf better match intuition desire label user core ts nmf rely solve non convex optimization problem derive iterative algorithm show monotonic convergent local optimum demonstrate practical utility ts nmf reuters pubmed corpora find ts nmf especially useful conceptual broad topics topic key term well understand although identify optimal latent structure data primary objective propose approach find ts nmf achieve higher weight jaccard similarity score contemporary methods unsupervised nmf latent dirichlet allocation supervision rat low ten twenty
propose novel embed model represent relationships among several elements bibliographic information high representation ability flexibility base model present novel search system show relationships among elements acl anthology reference corpus evaluation result show model achieve high prediction ability produce reasonable search result
availability large idea repositories eg yous patent database could significantly accelerate innovation discovery provide people inspiration solutions analogous problems however find useful analogies large messy real world repositories remain persistent challenge either human automate methods previous approach include costly hand create databases high relational structure eg predicate calculus representations sparse simpler machine learn information retrieval similarity metrics scale large natural language datasets struggle account structural similarity central analogy paper explore viability value learn simpler structural representations specifically problem schemas specify purpose product mechanisms achieve purpose approach combine crowdsourcing recurrent neural network extract purpose mechanism vector representations product descriptions demonstrate learn vectors allow us find analogies higher precision recall traditional information retrieval methods ideation experiment analogies retrieve model significantly increase people likelihood generate creative ideas compare analogies retrieve traditional methods result suggest promise approach enable computational analogy scale learn leverage weaker structural representations
propose simple yet effective technique neural network learn forward propagation compute usual back propagation small subset full gradient compute update model parameters gradient vectors sparsified way top k elements term magnitude keep result k row columns depend layout weight matrix modify lead linear reduction k divide vector dimension computational cost surprisingly experimental result demonstrate update one four weight back propagation pass result larger number train iterations interestingly accuracy result model actually improve rather degrade detail analysis give code available https githubcom lancopku meprop
generative model long dominant approach speech recognition success model however rely use sophisticate recipes complicate machinery easily accessible non practitioners recent innovations deep learn give rise alternative discriminative model call sequence sequence model almost match accuracy state art generative model model easy train train end end single step practical limitation use offline recognition model require entirety input sequence available begin inference assumption valid instantaneous speech recognition address problem online sequence sequence model recently introduce model able start produce output data arrive model feel confident enough output partial transcripts model like sequence sequence causal output produce model time affect feature compute subsequently make model inherently powerful generative model unable change feature compute data paper highlight two main contributions improvement online sequence sequence model train application noisy settings mix speech two speakers
increasingly surround artificially intelligent technology take decisions execute action behalf create press need general mean communicate instruct guide artificial agents human language compel mean communication achieve scalable fashion agents must able relate language world action understand language must ground embody however learn ground language notoriously challenge problem artificial intelligence research present agent learn interpret language simulate 3d environment reward successful execution write instructions train via combination reinforcement unsupervised learn begin minimal prior knowledge agent learn relate linguistic symbols emergent perceptual representations physical surround pertinent sequence action agent comprehension language extend beyond prior experience enable apply familiar language unfamiliar situations interpret entirely novel instructions moreover speed agent learn new word increase semantic knowledge grow facility generalise bootstrapping semantic knowledge indicate potential present approach reconcile ambiguous natural language complexity physical world
recently technique call layer wise relevance propagation lrp show deliver insightful explanations form input space relevances understand fee forward neural network classification decisions present work extend usage lrp recurrent neural network propose specific propagation rule applicable multiplicative connections arise recurrent network architectures lstms grus apply technique word base bi directional lstm model five class sentiment prediction task evaluate result lrp relevances qualitatively quantitatively obtain better result gradient base relate method use previous work
perform task specify natural language instructions autonomous agents need extract semantically meaningful representations language map visual elements action environment problem call task orient language ground propose end end trainable neural architecture task orient language ground 3d environments assume prior linguistic perceptual knowledge require raw pixels environment natural language instruction input propose model combine image text representations use gate attention mechanism learn policy execute natural language instruction use standard reinforcement imitation learn methods show effectiveness propose model unseen instructions well unseen map quantitatively qualitatively also introduce novel environment base 3d game engine simulate challenge task orient language ground rich set instructions environment state
word embed model skip gram learn vector space representation word base local word collocation pattern observe text corpus latent topic model hand take global view look word distributions across corpus assign topic word occurrence two paradigms complementary represent mean word occurrences previous work already look use word embeddings improve quality latent topics conversely use latent topics improve word embeddings two step methods capture mutual interaction two paradigms paper propose ste framework learn word embeddings latent topics unify manner ste naturally obtain topic specific word embeddings thus address issue polysemy time also learn term distributions topics topic distributions document experimental result demonstrate ste model indeed generate useful topic specific word embeddings coherent latent topics effective efficient way
paper present semi supervise learn algorithm classification text document method label unlabeled text document present present method base principle divide conquer strategy use recursive k mean algorithm partition label unlabeled data collection k mean algorithm apply recursively partition till desire level partition achieve partition contain label document single class desire cluster obtain respective cluster centroids consider representatives cluster nearest neighbor rule use classify unknown text document series experiment conduct bring superiority propose model recent state art model 20newsgroups dataset
number recent work propose techniques end end learn communication protocols among cooperative multi agent populations simultaneously find emergence ground human interpretable language protocols develop agents learn without human supervision paper use task tell reference game two agents testbed present sequence negative result culminate positive one show agent invent languages effective ie achieve near perfect task reward decidedly interpretable compositional essence find natural language emerge naturally despite semblance ease natural language emergence one may gather recent literature discuss possible coax invent languages become human like compositional increase restrictions two agents may communicate
paper describe submission two thousand and seventeen bioasq challenge participate task b phase b concern biomedical question answer qa focus factoid list question use extractive qa model restrict system output substrings provide text snippets core system use fastqa state art neural qa system extend biomedical word embeddings change answer layer able answer list question addition factoid question pre train model large scale open domain qa dataset squad fine tune parameters bioasq train set approach achieve state art result factoid question competitive result list question
paper deal problem audio source separation handle complex ill pose nature problems audio source separation current state art approach employ deep neural network obtain instrumental spectra mixture study propose novel network architecture extend recently develop densely connect convolutional network densenet show excellent result image classification task deal specific problem audio source separation sample layer block skip connection band dedicate dense block incorporate top densenet propose approach take advantage long contextual information outperform state art result sisec two thousand and sixteen competition large margin term signal distortion ratio moreover propose architecture require significantly fewer parameters considerably less train time compare methods
present first approach automate audio caption employ encoder decoder scheme alignment model input encoder sequence log mel band energies calculate audio file output sequence word ie caption encoder multi layer bi directional gate recurrent unit gru decoder multi layer gru classification layer connect last gru decoder classification layer alignment model fully connect layer share weight timesteps propose method evaluate use data draw commercial sound effect library prosound effect result caption rat metrics utilize machine translation image caption field result metrics show propose method predict word appear original caption always correctly order
recurrent neural network show state art result many text analysis task often require lot memory store weight recently propose sparse variational dropout eliminate majority weight fee forward neural network without significant loss quality apply technique sparsify recurrent neural network account recurrent specifics also rely binary variational dropout rnn report nine hundred and ninety-five sparsity level sentiment analysis task without quality drop eighty-seven sparsity level language model task slight loss accuracy
computer vision benefit initialize multiple deep layer weight pretrained large supervise train set like imagenet natural language process nlp typically see initialization lowest layer deep model pretrained word vectors paper use deep lstm encoder attentional sequence sequence model train machine translation mt contextualize word vectors show add context vectors cove improve performance use unsupervised word character vectors wide variety common nlp task sentiment analysis sst imdb question classification trec entailment snli question answer squad fine grain sentiment analysis entailment cove improve performance baseline model state art
beam search desirable choice test time decode algorithm neural sequence model potentially avoid search errors make simpler greedy methods however typical cross entropy train procedures model directly consider behaviour final decode method result cross entropy train model beam decode sometimes yield reduce test performance compare greedy decode order train model effectively make use beam search propose new train procedure focus final loss metric eg ham loss evaluate output beam search well define direct loss objective discontinuous thus difficult optimize hence approach form sub differentiable surrogate objective introduce novel continuous approximation beam search decode procedure experiment show optimize new train objective yield substantially better result two sequence task name entity recognition ccg supertagging compare cross entropy train greedy decode cross entropy train beam decode baselines
knowledge graph versatile framework encode richly structure data relationships challenge combine graph unstructured data methods retrofit pre train entity representations structure knowledge graph typically assume entities embed connect space relations imply similarity however useful knowledge graph often contain diverse entities relations potentially disjoint underlie corpora accord assumptions overcome limitations present functional retrofit framework generalize current retrofit methods explicitly model pairwise relations framework directly incorporate variety pairwise penalty function previously develop knowledge graph completion allow users encode learn extract information relation semantics present linear neural instantiations framework functional retrofit significantly outperform exist retrofit methods complex knowledge graph lose accuracy simpler graph relations imply similarity finally demonstrate utility framework predict new drug disease treatment pair large complex health knowledge graph
paper explore utilization natural language drive transfer reinforcement learn rl despite wide spread application deep rl techniques learn generalize policy representations work across domains remain challenge problem demonstrate textual descriptions environments provide compact intermediate channel facilitate effective policy transfer specifically learn grind mean text dynamics environment transition reward autonomous agent effectively bootstrap policy learn new domain give description employ model base rl approach consist differentiable plan module model free component factorize state representation effectively use entity descriptions model outperform prior work transfer multi task scenarios variety different environments instance achieve fourteen one hundred and fifteen absolute improvement previously exist model term average initial reward respectively
recently e commerce sit launch new interaction box call tip mobile apps users express experience feel provide suggestions use short texts typically several word one sentence essence write tip give numerical rat two facets user product assessment action express user experience feel jointly model two facets helpful design better recommendation system exist model integrate text information item specifications user review user item latent factor improve rat prediction exist work consider tip improve recommendation quality propose deep learn base framework name nrt simultaneously predict precise rat generate abstractive tip good linguistic quality simulate user experience feel abstractive tip generation gate recurrent neural network employ translate user item latent representations concise sentence extensive experiment benchmark datasets different domains show nrt achieve significant improvements state art methods moreover generate tip vividly predict user experience feel
present new topic model generate document sample topic one whole sentence time generate word sentence use rnn decoder condition topic sentence argue novel formalism help us visualize model topical discourse structure document better also potentially lead interpretable topics since illustrate topics sample representative sentence instead bag word phrase present variational auto encoder approach learn use factorize variational encoder independently model posterior topical mixture vectors document use fee forward network posterior topic assignments sentence use rnn preliminary experiment two different datasets indicate early promise also expose many challenge remain address
segmental model alternative frame base model sequence prediction hypothesize path weight base entire segment score rather single frame time neural segmental model segmental model use neural network base weight function neural segmental model achieve competitive result speech recognition end end train explore several study work review neural segmental model view consist neural network base acoustic encoder finite state transducer decoder study end end segmental model different weight function include ones base frame level neural classifiers segmental recurrent neural network study reduce search space size impact performance different weight function also compare several loss function end end train finally explore train approach include multi stage vs end end train multitask train combine segmental frame level losses
describe university maryland machine translation systems submit wmt17 german english bandit learn task task adapt translation system new domain use bandit feedback system receive german sentence translate produce english sentence get scalar score feedback target two challenge adaptation bandit learn build standard neural machine translation system extend two ways one robust reinforcement learn techniques learn effectively bandit feedback two domain adaptation use data selection large corpus parallel data
consider well know facts syntax physics perspective allow us establish equivalences field many consequences mainly observe operation merge put forward n chomsky one thousand, nine hundred and ninety-five interpret physical information coarse grain thus merge linguistics entail information renormalization physics accord different time scale make point mathematically formal term language model set merge amount probability tensor implement coarse grain akin probabilistic context free grammar probability vectors meaningful sentence give stochastic tensor network tn build diagonal tensors mostly loop free tree tensor network matrix product state thus computationally efficient manipulate show imply polynomially decay long range correlations experimentally observe language also provide arguments favour certain type neural network language process moreover show obtain language model quantum state efficiently prepare quantum computer use find bound perplexity probability distribution word sentence implications result discuss across several ambits
one main computational scientific challenge modern age extract useful information unstructured texts topic model one popular machine learn approach infer latent topical structure collection document despite success particular widely use variant call latent dirichlet allocation lda numerous applications sociology history linguistics topic model know suffer severe conceptual practical problems eg lack justification bayesian priors discrepancies statistical properties real texts inability properly choose number topics obtain fresh view problem identify topical structure relate problem find communities complex network achieve represent text corpora bipartite network document word adapt exist community detection methods use stochastic block model sbm non parametric priors obtain versatile principled framework topic model eg automatically detect number topics hierarchically cluster word document analysis artificial real corpora demonstrate sbm approach lead better topic model lda term statistical model selection importantly work show formally relate methods community detection topic model open possibility cross fertilization two field
paper present new dataset user simulator e qraq explainable query reason answer question test agent ability read ambiguous text ask question answer challenge question explain reason behind question answer user simulator provide agent short ambiguous story challenge question story story ambiguous entities replace variables turn agent may ask value variable try answer challenge question response user simulator provide natural language explanation agent query answer useful narrow set possible answer demonstrate one potential application e qraq dataset train new neural architecture base end end memory network successfully generate predictions partial explanations current understand problem observe strong correlation quality prediction explanation
neural image caption systems recurrent neural network rnn typically view primary generation component view suggest image feature inject rnn fact dominant view literature alternatively rnn instead view encode previously generate word view suggest rnn use encode linguistic feature final representation merge image feature later stage paper compare two architectures find general late merge outperform injection suggest rnns better view encoders rather generators
classification social media data important approach understand user behavior web although information social media different modalities texts image audio videos traditional approach classification usually leverage one prominent modality techniques able leverage multiple modalities often complex susceptible absence modalities paper present simple model combine information different modalities classify social media content able handle problems exist techniques model combine information different modalities use pool layer auxiliary learn task use learn common feature space demonstrate performance model robustness miss modalities emotion classification domain approach although simple achieve significantly higher accuracies traditional fusion approach also comparable result one modality available
recurrent neural network rnns long short term memory network lstms serve fundamental build block many sequence learn task include machine translation language model question answer paper consider specific problem word level language model investigate strategies regularize optimize lstm base model propose weight drop lstm use dropconnect hide hide weight form recurrent regularization introduce nt asgd variant average stochastic gradient method wherein average trigger determine use non monotonic condition oppose tune user use regularization strategies achieve state art word level perplexities two data set five hundred and seventy-three penn treebank six hundred and fifty-eight wikitext two explore effectiveness neural cache conjunction propose model achieve even lower state art perplexity five hundred and twenty-eight penn treebank five hundred and twenty wikitext two
present study propose litstoryteller interactive system visually explore semantic structure scientific article demonstrate litstoryteller could use answer fundamental research question new method build top exist methods base theoretical proof experimental evidence importantly litstoryteller assist users understand full interest story scientific paper concise outline important detail propose system borrow metaphor screen play visualize storyline scientific paper arrange character scientific concepts terminologies scenes paragraph sentence progressive interactive storyline storylines help preserve semantic structure logical think process scientific paper semantic structure scientific concepts comparative sentence extract use exist name entity recognition apis supervise classifiers scientific paper automatically two supplementary view rank entity frequency view entity co occurrence network view provide help users identify main plot scientific storylines collective document ready litstoryteller also provide temporal entity evolution view entity community view collection digestion
generative statistical model chord sequence play crucial roles music process capture syntactic similarities among certain chord eg c major key g g7 f dm study hide markov model probabilistic context free grammar model latent variables describe syntactic categories chord symbols unsupervised learn techniques induce latent grammar data surprisingly find model often outperform conventional markov model predictive power self emergent categories often correspond traditional harmonic function imply need chord categories harmony model informatics perspective
sequence sequence model show promise improvements temporal task video caption optimize word level cross entropy loss train first use policy gradient mix loss methods reinforcement learn directly optimize sentence level task base metrics reward achieve significant improvements baseline base automatic metrics human evaluation multiple datasets next propose novel entailment enhance reward cident correct phrase match base metrics cider allow logically imply partial match avoid contradictions achieve significant improvements cider reward model overall cident reward model achieve new state art msr vtt dataset
present simple sequential sentence encoder multi domain natural language inference encoder base stack bidirectional lstm rnns shortcut connections fine tune word embeddings overall supervise model use encoder encode two input sentence two vectors use classifier vector combination label relationship two sentence entailment contradiction neural shortcut stack sentence encoders achieve strong improvements exist encoders match mismatch multi domain natural language inference top non ensemble single model result emnlp repeval two thousand and seventeen share task nangia et al two thousand and seventeen moreover achieve new state art encode result original snli dataset bowman et al two thousand and fifteen
active learn aim select small subset data annotation classifier learn data highly accurate usually do use heuristic selection methods however effectiveness methods limit moreover performance heuristics vary datasets address shortcomings introduce novel formulation reframing active learn reinforcement learn problem explicitly learn data selection policy policy take role active learn heuristic importantly method allow selection policy learn use simulation one language transfer languages demonstrate method use cross lingual name entity recognition observe uniform improvements traditional active learn
address problem end end visual storytelling give photo album model first select representative summary photos compose natural language story album task make use visual storytelling dataset model compose three hierarchically attentive recurrent neural net rnns encode album photos select representative summary photos compose story automatic human evaluations show model achieve better performance selection generation retrieval baselines
intelligent conversational assistants apple siri microsoft cortana amazon echo quickly become part digital life however assistants major limitations prevent users converse would human dialog partner limit ability observe users really want interact underlie system address problem develop crowd power conversational assistant chorus deploy see users workers would interact together mediate system chorus sophisticatedly converse end users time recruit workers demand turn decide might best response user sentence first month deployment fifty-nine users hold conversations chorus three hundred and twenty conversational sessions paper present account chorus deployment focus four challenge identify conversations ii malicious users workers iii demand recruit iv settings consensus enough observations could assist deployment crowd power conversation systems crowd power systems general
embarrassingly communication free parallel markov chain monte carlo mcmc methods commonly use learn graphical model however mcmc directly apply learn topic model quasi ergodicity problem cause multimodal distribution topics paper develop embarrassingly parallel mcmc algorithm slda algorithm work switch order sample topics combination label variable prediction slda overcome quasi ergodicity problem high dimension topics follow multimodal distribution project one dimension document label follow unimodal distribution empirical experiment confirm sample prediction performance use embarrassingly parallel algorithm comparable non parallel slda computation time significantly reduce
users try articulate complex information need search sessions reformulate query make process effective search engines provide relate query help users specify information need search process paper propose customize sequence sequence model session base query suggestion model employ query aware attention mechanism capture structure session context enable us control scope session infer suggest next query help handle noisy data also automatically detect session boundaries furthermore observe base user query reformulation behavior within single session large portion query term retain previously submit query consist mostly infrequent unseen term usually include vocabulary therefore empower decoder model access source word session context decode incorporate copy mechanism moreover propose evaluation metrics assess quality generative model query suggestion conduct extensive set experiment analysis e result suggest model outperform baselines term generate query score candidate query task query suggestion
exist two main approach automatically extract affective orientation lexicon base corpus base work argue two methods compatible show combine improve accuracy emotion classifiers particular introduce novel variant label propagation algorithm tailor distribute word representations apply batch gradient descent accelerate optimization label propagation make optimization feasible large graph propose reproducible method emotion lexicon expansion conclude label propagation expand emotion lexicon meaningful way expand emotion lexicon leverage improve accuracy emotion classifier
neural network one popular approach many natural language process task sentiment analysis often outperform traditional machine learn model achieve state art result task however many exist deep learn model complex difficult train provide limit improvement simpler methods propose simple robust powerful model sentiment classification model outperform many deep learn model achieve comparable result deep learn model complex architectures sentiment analysis datasets publish code online
word embeddings representations individual word text document vector space often use ful perform natural language pro cessing task current state art al gorithms learn word embeddings learn vector representations large corpora text document unsu pervised fashion paper introduce swesa supervise word embeddings sentiment analysis algorithm sentiment analysis via word embeddings swesa leverage document label infor mation learn vector representations word modest corpus text doc uments solve optimization prob lem minimize cost function respect word embeddings well classification accuracy analysis veals swesa provide efficient way estimate dimension word embeddings learn experiment several real world data set show swesa superior per formance compare previously suggest approach word embeddings sentiment analysis task
propose method embed two dimensional locations continuous vector space use neural network base model incorporate mixtures gaussian distributions present two model variants text base geolocation lexical dialectology evaluate twitter data propose model outperform conventional regression base geolocation provide better estimate uncertainty also show effectiveness representation predict word location lexical dialectology evaluate use dare dataset
paper propose text summarization approach factual report use deep learn model approach consist three phase feature extraction feature enhancement summary generation work together assimilate core information generate coherent understandable summary explore various feature improve set sentence select summary use restrict boltzmann machine enhance abstract feature improve resultant accuracy without lose important information sentence score base enhance feature extractive summary construct experimentation carry several article demonstrate effectiveness propose approach source code available https githubcom vagisha nidhi textsummarizer
present database parliamentary debate contain complete record parliamentary speeches ail eireann lower house principal chamber irish parliament one thousand, nine hundred and nineteen two thousand and thirteen addition database contain background information tds teachta ala members parliament party affiliations constituencies office position current version database include close forty-five million speeches one thousand, one hundred and seventy-eight tds speeches download official parliament website process parse python script background information tds collect member database parliament website data cabinet position minister junior minister collect official website government record linkage algorithm human coders use match tds minister
debate summarization one novel challenge research areas automatic text summarization largely unexplored paper develop debate summarization pipeline summarize key topics discuss argue two oppose side online debate view generation debate summaries achieve cluster cluster label visualization work investigate two different cluster approach generation summaries first approach generate summaries apply purely term base cluster cluster label second approach make use x mean cluster mutual information label cluster approach drive ontologies visualize result use bar chart think result smooth entry users aim receive first impression discuss within debate topic contain waste number argumentations
usage online textual media steadily increase daily news stories blog post scientific article add online volumes freely accessible employ extensively multiple research areas eg automatic text summarization information retrieval information extraction etc meanwhile online debate forums recently become popular remain largely unexplored reason sufficient resources annotate debate data available conduct research genre paper collect annotate debate data automatic summarization task similar extractive gold standard summary generation data contain sentence worthy include summary five human annotators perform task inter annotator agreement base semantic similarity thirty-six cohen kappa forty-eight krippendorff alpha moreover also implement extractive summarization system online debate discuss prominent feature task summarize online debate data automatically
rich dense human label datasets among main enable factor recent advance vision language understand many seemingly distant annotations eg semantic segmentation visual question answer vqa inherently connect reveal different level perspectives human understand visual scenes even set image eg coco popularity coco correlate annotations task explicitly link may significantly benefit individual task unify vision language model present preliminary work link instance segmentations provide coco question answer qas vqa dataset name collect link visual question segmentation answer vqs transfer human supervision previously separate task offer effective leverage exist problems also open door new research problems model study two applications vqs data paper supervise attention vqa novel question focus semantic segmentation task former obtain state art result vqa real multiple choice task simply augment multilayer perceptrons attention feature learn use segmentation qa link explicit supervision put latter perspective study two plausible methods compare oracle method assume instance segmentations give test stage
learn latent representations long text sequence important first step many natural language process applications recurrent neural network rnns become cornerstone challenge task however quality sentence rnn base decode reconstruction decrease length text propose sequence sequence purely convolutional deconvolutional autoencoding framework free issue also computationally efficient propose method simple easy implement leverage build block many applications show empirically compare rnns framework better reconstruct correct long paragraph quantitative evaluation semi supervise text classification summarization task demonstrate potential better utilization long unlabeled text data
nowadays cross modal retrieval play indispensable role flexibly find information across different modalities data effectively measure similarity different modalities data key cross modal retrieval different modalities image text imbalanced complementary relationships contain unequal amount information describe semantics example image often contain detail demonstrate textual descriptions vice versa exist work base deep neural network dnn mostly construct one common space different modalities find latent alignments lose exclusive modality specific characteristics different exist work propose modality specific cross modal similarity measurement mcsm approach construct independent semantic space modality adopt end end framework directly generate modality specific cross modal similarity without explicit common representation semantic space modality specific characteristics within one modality fully exploit recurrent attention network data another modality project space attention base joint embed utilize learn attention weight guide fine grain cross modal correlation learn capture imbalanced complementary relationships different modalities finally complementarity semantic space different modalities explore adaptive fusion modality specific cross modal similarities perform cross modal retrieval experiment widely use wikipedia pascal sentence datasets well construct large scale xmedianet dataset verify effectiveness propose approach outperform nine state art methods
entity alignment task find entities two knowledge base kbs represent real world object face kbs different natural languages conventional cross lingual entity alignment methods rely machine translation eliminate language barriers approach often suffer uneven quality translations languages recent embed base techniques encode entities relationships kbs need machine translation cross lingual entity alignment significant number attribute remain largely unexplored paper propose joint attribute preserve embed model cross lingual entity alignment jointly embed structure two kbs unify vector space refine leverage attribute correlations kbs experimental result real world datasets show approach significantly outperform state art embed approach cross lingual entity alignment could complement methods base machine translation
ai continue advance human ai team inevitable however progress ai routinely measure isolation without human loop crucial benchmark progress ai isolation also term translate help humans perform certain task ie performance human ai team work design cooperative game guesswhich measure human ai team performance specific context ai visual conversational agent guesswhich involve live interaction human ai ai call alice provide image unseen human follow brief description image human question alice secret image identify fix pool image measure performance human alice team number guess take human correctly identify secret image fix number dialog round alice compare performance human alice team two versions alice human study suggest counterintuitive trend ai literature show one version outperform pair ai questioner bot find improvement ai ai performance translate improve human ai performance suggest mismatch benchmarking ai isolation context human ai team
present ladder first deep reinforcement learn agent successfully learn control policies large scale real world problems directly raw input compose high level semantic information agent base asynchronous stochastic variant dqn deep q network name dasqn input agent plain text descriptions state game incomplete information ie real time large scale online auction reward auction profit large scale apply agent essential portion jd online rtb real time bid advertise business find easily beat former state art bid policy carefully engineer calibrate human experts jdcom june 18th anniversary sale agent increase company ads revenue portion fifty advertisers roi return investment also improve significantly
long short term memory lstm primary recurrent neural network architecture acoustic model automatic speech recognition systems residual learn efficient method help neural network converge easier faster paper propose several type residual lstm methods acoustic model experiment indicate compare classic lstm architecture show eight relative reduction phone error rate per timit task time residual fast lstm approach show four relative reduction per task besides find architecture could good result thchs thirty librispeech switchboard corpora
paper consider several compression techniques language model problem base recurrent neural network rnns know conventional rnns eg lstm base network language model characterize either high space complexity substantial inference time problem especially crucial mobile applications constant interaction remote server inappropriate use penn treebank ptb dataset compare prune quantization low rank factorization tensor train decomposition lstm network term model size suitability fast inference
recurrent chinese restaurant process rcrp powerful statistical method model evolve cluster large scale social media data rcrp one allow number cluster cluster parameters model change time however application rcrp largely limit due non conjugacy cluster evolutionary priors multinomial likelihood non conjugacy make inference di cult restrict scalability model use rcrp lead rcrp apply simple problems approximate single gaussian emission paper provide novel solution non conjugacy issue rcrp example leverage solution one speci c problem social event discovery problem utilize sequential monte carlo methods inference approach massively parallel highly scalable extent work tens millions document able generate high quality topical location distributions cluster directly interpret real social events experimental result suggest approach propose achieve much better predictive performance techniques report prior work also demonstrate techniques develop use much general ways toward similar problems
era digitization know user sociolect aspects become essential feature build user specific recommendation systems sociolect aspects could find mine user language share form text social media review paper describe experiment perform pan author profile two thousand and seventeen share task objective task find sociolect aspects users tweet sociolect aspects consider experiment user gender native language information user tweet write different language native language represent document term matrix document frequency constraint classification do use support vector machine take gender native language target class experiment attain average accuracy seven thousand, three hundred and forty-two gender prediction seven thousand, six hundred and twenty-six native language identification task
electronic health record ehr contain large amount multi dimensional unstructured clinical data significant operational research value distinguish previous study approach embrace double annotate dataset stray away obscure black box model comprehensive deep learn model paper present novel neural attention mechanism classify clinically important find specifically convolutional neural network cnn attention analysis use classify radiology head compute tomography report base five categories radiologists would account assess acute communicable find daily practice experiment show cnn attention model outperform non neural model especially train larger dataset attention analysis demonstrate intuition behind classifier decision generate heatmap highlight attend term use cnn model valuable potential downstream medical decisions perform human experts classifier information use cohort construction epidemiological study
propose graph contextualization method pairgraphtext study political engagement facebook two thousand and twelve french presidential election spectral algorithm contextualizes graph data text data online discussion thread particular examine facebook post eight lead candidates comment beneath post find evidence candidate center structure citizens primarily comment wall one candidate ii issue center structure ie political topics citizens attention expression primarily direct towards specific set issue eg economics immigration etc identify issue center structure develop pairgraphtext analyze network high dimensional feature interactions ie text technique scale hundreds thousands nod thousands unique word facebook data spectral cluster without contextualizing text information find mixture candidate ii issue cluster contextualized information text data help separate two structure conclude show novel methodology consistent statistical model
automatically evaluate quality dialogue responses unstructured domains challenge problem unfortunately exist automatic evaluation metrics bias correlate poorly human judgements response quality yet accurate automatic evaluation procedure crucial dialogue research allow rapid prototyping test new model fewer expensive human evaluations response challenge formulate automatic dialogue evaluation learn problem present evaluation model adem learn predict human like score input responses use new dataset human response score show adem model predictions correlate significantly level much higher word overlap metrics bleu human judgements utterance system level also show adem generalize evaluate dialogue model unseen train important step automatic dialogue evaluation
speech separation task separate target speech background interference traditionally speech separation study signal process problem recent approach formulate speech separation supervise learn problem discriminative pattern speech speakers background noise learn train data past decade many supervise separation algorithms put forward particular recent introduction deep learn supervise speech separation dramatically accelerate progress boost separation performance article provide comprehensive overview research deep learn base supervise speech separation last several years first introduce background speech separation formulation supervise separation discuss three main components supervise separation learn machine train target acoustic feature much overview separation algorithms review monaural methods include speech enhancement speech nonspeech separation speaker separation multi talker separation speech dereverberation well multi microphone techniques important issue generalization unique supervise learn discuss overview provide historical perspective advance make addition discuss number conceptual issue include constitute target source
comment neurophysiological dynamics phrase structure build sentence process nelson et al two thousand and seventeen proceed national academy sciences usa eleven thousand, four hundred and eighteen e3669 e3678
investigate task cluster deep learn base multi task shoot learn many task set propose new method measure task similarities cross task transfer performance matrix deep learn scenario although matrix provide us critical information regard similarity task asymmetric property unreliable performance score affect conventional cluster methods adversely additionally uncertain task pair ie ones extremely asymmetric transfer score may collectively mislead cluster algorithms output inaccurate task partition overcome limitations propose novel task cluster algorithm use matrix completion technique propose algorithm construct partially observe similarity matrix base certainty cluster membership task pair use matrix completion algorithm complete similarity matrix theoretical analysis show mild constraints propose algorithm perfectly recover underlie true similarity matrix high probability result show new task cluster method discover task cluster train flexible superior neural network model multi task learn setup sentiment classification dialog intent classification task task cluster approach also extend metric base shoot learn methods adapt multiple metrics demonstrate empirical advantage task diverse
address problem generate query suggestions support users complete underlie task motivate search first place give initial query query suggestions provide coverage possible subtasks user might look propose probabilistic model framework obtain keyphrases multiple source generate query suggestions keyphrases use test suit trec task track evaluate analyze component model
today practice return entities knowledge base response search query become widespread one distinctive characteristics entities type ie assign hierarchically organize type system type taxonomy primary objective paper gain better understand entity type information utilize entity retrieval perform investigation idealize oracle set assume know distribution target type relevant entities give query perform thorough analysis three main aspects choice type taxonomy ii representation hierarchical type information iii combination type base term base similarity retrieval model use standard entity search test collection base dbpedia find type information prove useful use large type taxonomies provide specific type provide insights extensional coverage entities utility target type
paper present hierarchical relationbased latent dirichlet allocation hrlda data drive hierarchical topic model extract terminological ontologies large number heterogeneous document contrast traditional topic model hrlda rely noun phrase instead unigrams consider syntax document structure enrich topic hierarchies topic relations series experiment demonstrate superiority hrlda exist topic model especially build hierarchies furthermore illustrate robustness hrlda settings noisy data set likely occur many practical scenarios ontology evaluation result show ontologies extract hrlda competitive ontologies create domain experts
many genres natural language text narratively structure testament predilection organize experience narratives broad consensus understand narrative require identify track goals desire character narrative outcomes however date limit work computational model problem introduce new dataset desiredb include gold standard label identify statements desire textual evidence desire fulfillment annotations whether state desire fulfil give evidence narrative context report experiment track desire fulfillment use different methods show lstm skip think model achieve f measure seven corpus
unseen data condition inflict serious performance degradation systems rely supervise machine learn algorithms data often unseen traditional machine learn algorithms train supervise manner unsupervised adaptation techniques must use adapt model unseen data condition however unsupervised adaptation often challenge one must generate hypothesis give model use hypothesis bootstrap model unseen data condition unfortunately reliability hypotheses often poor give mismatch train test datasets case model hypothesis confidence measure enable perform data selection model adaptation underlie approach fact unseen data condition data variability introduce model model propagate output decision impact decision reliability fully connect network data variability propagate distortions one layer next work aim estimate propagation distortion form network activation entropy measure short time run window activation neuron give hide layer measurements use compute summary entropy work demonstrate entropy measure help select data unsupervised model adaptation result performance gain speech recognition task result standard benchmark speech recognition task show propose approach alleviate performance degradation experience unseen data condition iteratively adapt model unseen datas acoustic condition
speech recognition largely take advantage deep learn show substantial benefit obtain modern recurrent neural network rnns popular rnns long short term memory lstms typically reach state art performance many task thank ability learn long term dependencies robustness vanish gradients nevertheless lstms rather complex design three multiplicative gate might impair efficient implementation attempt simplify lstms recently lead gate recurrent units grus base two multiplicative gate paper build efforts revise grus propose simplify architecture potentially suitable speech recognition contribution work two fold first suggest remove reset gate gru design result efficient single gate architecture second propose replace tanh relu activations state update equations result show implementation revise architecture reduce per epoch train time thirty consistently improve recognition performance across different task input feature noisy condition compare standard gru
dependence frequency distributions due multiple mean word text investigate delete letter cod word fewer letter number mean per cod word increase increase measure use input predictive theory text write english word frequency distribution broad fat tail whereas word represent first letter distribution become exponential distribution well predict theory whole sequence obtain consecutively represent word first l654321 letter comparisons texts write chinese character texts write letter cod make similarity correspond frequency distributions interpret consequence multiple mean chinese character imply difference shape word frequencies english text write letter chinese text write chinese character due cod language per se
previous research traditionally analyze emoji sentiment point view reader content author analyze emoji sentiment point view author present emoji sentiment benchmark build employee happiness dataset emoji happen annotate daily happiness author comment data span three years 4k employees fifty-six company base barcelona compare sentiment writers readers result indicate eighty-two agreement emoji sentiment perceive readers writers finally report author use emoji report higher level happiness emoji use find correlate differences author moodiness
critical assumption current visual speech recognition systems visual speech units call visemes map units acoustic speech phonemes despite number publish map infrequent see effectiveness test particularly visual lip read many work use audio visual speech examine one hundred and twenty mappings consider stable across talkers show method devise map base phoneme confusions automate lip read system present new mappings show improvements individual talkers
machine lip read continue debate research around correct class use recognition paper use structure approach devise speaker dependent viseme class enable creation set phoneme viseme map different quantity visemes range two forty-five viseme class base upon map articulate phonemes confuse phoneme recognition viseme group use map lilir dataset show effect change viseme map size speaker dependent machine lip read measure word recognition correctness demonstrate word recognition phoneme classifiers possible often better word recognition viseme classifiers furthermore intermediate units visemes phonemes better still
online media outlets bid expand reach subsequently increase revenue ad monetisation begin adopt clickbait techniques lure readers click article article fail fulfill promise make headline traditional methods clickbait detection rely heavily feature engineer turn dependent dataset build application neural network task explore partially propose novel approach consider information find social media post train bidirectional lstm attention mechanism learn extent word contribute post clickbait score differential manner also employ siamese net capture similarity source target information information glean image consider previous approach learn image embeddings large amount data use convolutional neural network add another layer complexity model finally concatenate output three separate components serve input fully connect layer conduct experiment test corpus nineteen thousand, five hundred and thirty-eight social media post attain f1 score six thousand, five hundred and thirty-seven dataset better previous state art well propose approach feature engineer otherwise
grow interest model learn unlabelled speech pair visual context set relevant low resource speech process robotics human language acquisition research study visually ground speech model train image scenes pair speak caption capture aspects semantics use external image tagger generate soft text label image serve target neural model map untranscribed speech semantic keyword label introduce newly collect data set human semantic relevance judgements associate task semantic speech retrieval goal search speak utterances semantically relevant give text query without see text model train parallel speech image achieve precision almost sixty top ten semantic retrievals compare supervise model train transcriptions model match human judgements better measure especially retrieve non verbatim semantic match perform extensive analysis model result representations
paper introduce content possible usage dirha english multi microphone corpus recently realize ec dirha project reference scenario domestic environment equip large number microphones microphone array distribute space corpus compose real simulate material include twelve us twelve uk english native speakers speaker utter different set phonetically rich sentence newspaper article conversational speech keywords command material large set one minute sequence generate also include typical domestic background noise well inter intra room reverberation effect dev test set derive represent precious material different study multi microphone speech process distant speech recognition various task correspond kaldi recipes already develop paper report first set baseline result obtain use different techniques include deep neural network dnn align state art international level
current topic model often suffer discover topics match human intuition unnatural switch topics within document high computational demand address concern propose topic model inference algorithm base automatically identify characteristic keywords topics keywords influence topic assignments nearby word algorithm learn keyword topic score self regulate number topics inference simple easily parallelizable qualitative analysis yield comparable result state art model eg lda different strengths weaknesses quantitative analysis use nine datasets show gain term classification accuracy pmi score computational performance consistency topic assignments within document often use less topics
despite significant progress make last years state art speech recognition technologies provide satisfactory performance close talk condition robustness distant speech recognition adverse acoustic condition hand remain crucial open issue future applications human machine interaction end several advance speech enhancement acoustic scene analysis well acoustic model recently contribute improve state art field one effective approach derive robust acoustic model base use contaminate speech prove helpful reduce acoustic mismatch train test condition paper revise classical approach context modern dnn hmm systems propose adoption three methods namely asymmetric context windowing close talk base supervision close talk base pre train experimental result obtain use real simulate data show significant advantage use three methods overall provide fifteen error rate reduction compare baseline systems trend performance confirm either use high quality train set small size large one
present web service query embed entities wikidata knowledge graph embed train wikidata dump use gensim word2vec implementation simple graph walk rest api implement together wikidata api web service expose multilingual resource six hundred zero wikidata items properties
propose marve system extract measurement value units relate word natural language text marve use conditional random field crf identify measurement value units follow rule base system find relate entities descriptors modifiers within sentence sentence tokens represent undirected graphical model rule base part speech word dependency pattern connect value units contextual word marve unique focus measurement context early experimentation demonstrate marve ability generate high precision extractions strong recall also discuss marve role refine measurement requirements nasa propose hyspiri mission hyperspectral infrared image satellite study world ecosystems general work hyspiri demonstrate value semantic measurement extractions characterize quantitative discussion contain large corpuses natural language text extractions accelerate broad cross cut research expose scientists new algorithmic approach experimental nuances also facilitate identification scientific opportunities enable hyspiri lead efficient scientific investment research
paper propose generative model learn relationship language human action order generate human action sequence give sentence describe human behavior propose generative model generative adversarial network gin base sequence sequence seq2seq model use propose generative network synthesize various action robot virtual agent use text encoder recurrent neural network rnn action decoder rnn propose generative network train twenty-nine thousand, seven hundred and seventy pair action sentence annotations extract msr video text msr vtt large scale video dataset demonstrate network generate human like action transfer baxter robot robot perform action base provide sentence result show propose generative network correctly model relationship language action generate diverse set action sentence
find semantically rich computer understandable representations textual dialogues utterances word crucial dialogue systems conversational agents performance mostly depend understand context conversations recent research aim find distribute vector representations embeddings word semantically similar word relatively close within vector space encode mean text vectors current trend text range word phrase document actual human human conversations recent research approach responses generate utilize decoder architecture give vector representation current conversation paper utilization embeddings answer retrieval explore use locality sensitive hash forest lsh forest approximate nearest neighbor ann model find similar conversations corpus rank possible candidates experimental result well know ubuntu corpus english customer service chat dataset dutch show combination candidate selection method retrieval base approach outperform generative ones reveal promise future research directions towards usability system
present pubmed 200k rct new dataset base pubmed sequential sentence classification dataset consist approximately two hundred thousand abstract randomize control trials total twenty-three million sentence sentence abstract label role abstract use one follow class background objective method result conclusion purpose release dataset twofold first majority datasets sequential short text classification ie classification short texts appear sequence small hope release new large dataset help develop accurate algorithms task second application perspective researchers need better tool efficiently skim literature automatically classify sentence abstract would help researchers read abstract efficiently especially field abstract may long medical field
paper present result conclusions participation clickbait challenge two thousand and seventeen automatic clickbait detection social media first describe linguistically infuse neural network model identify informative representations predict level clickbaiting present twitter post model allow answer question whether post clickbait extent clickbait post eg slightly considerably heavily clickbaity use score range zero one evaluate predictive power model train vary text image representations extract tweet best perform model rely tweet text linguistic markers bias language extract tweet correspond page yield mean square error mse four mean absolute error mae sixteen r2 forty-three hold test data binary classification setup clickbait vs non clickbait model achieve f1 score sixty-nine find image representations combine text yield significant performance improvement yet nevertheless work first present preliminary analysis object extract use google tensorflow object detection api image clickbait vs non clickbait twitter post finally outline several step improve model performance part future work
describe adaptation refinement graphical user interface design facilitate wizard oz woz approach collect human robot dialogue data data collect use develop dialogue system robot navigation build interface previously use development dialogue systems virtual agents video playback add templates open parameters allow wizard quickly produce wide variety utterances research demonstrate approach data collection viable intermediate step develop dialogue system physical robots remote locations users domain human robot need regularly verify update share understand physical environment show woz interface fix set utterances templates therein provide natural pace dialogue good coverage navigation domain
study develop automate system evaluate speech language feature audio record neuropsychological examinations ninety-two subject framingham heart study total two hundred and sixty-five feature use elastic net regularize binomial logistic regression model classify presence cognitive impairment select predictive feature compare performance demographic model six thousand, two hundred and fifty-eight subject greater study cohort seventy-nine auc find system incorporate audio text feature perform best ninety-two auc true positive rate twenty-nine zero false positive rate good model fit hosmer lemeshow test five also find decrease pitch jitter shorter segment speech responses phrase question positively associate cognitive impairment
create computational framework understand social action demonstrate framework use build open source event detection tool scalable statistical machine learn algorithms subsampled database six hundred million geo tag tweet around world tweet collect april 1st two thousand and fourteen april 30th two thousand and fifteen notably black live matter movement begin demonstrate methods use diagnostically researchers government officials public understand peaceful violent collective action fine grain level time geography
paper present novel deep triphone embed dte representation derive deep neural network dnn encapsulate discriminative information present adjoin speech frame dtes generate use four hide layer dnn three thousand nod hide layer first stage dnn train tie triphone classification accuracy optimization criterion thereafter retain activation vectors three thousand last hide layer speech mfcc frame perform dimension reduction obtain three hundred dimensional representation term dte dtes along mfcc feature feed second stage four hide layer dnn subsequently train task tie triphone classification dnns train use tri phone label generate tie state triphone hmm gmm system perform force alignment transcriptions mfcc feature frame conduct experiment publicly available ted lium speech corpus result show propose dte method provide improvement absolute two hundred and eleven phoneme recognition compare competitive hybrid tie state triphone hmm dnn system
probabilistic methods classify text form rich tradition machine learn natural language process many important problems however class prediction uninteresting class know instead focus shift estimate latent quantities relate text affect ideology focus one problem interest estimate ideological position fifty-five irish legislators one thousand, nine hundred and ninety-one ail confidence vote solve ail scale problem others like develop text model framework allow actors take latent position gray spectrum black white polar opposites able validate result model measure influence exhibit individual word able quantify uncertainty scale estimate use sentence level block bootstrap apply method ail debate able scale legislators extreme pro government pro opposition way reveal nuances speeches capture vote party affiliations
propose evaluate new techniques compress speed dense matrix multiplications find fully connect recurrent layer neural network embed large vocabulary continuous speech recognition lvcsr compression introduce study trace norm regularization technique train low rank factor versions matrix multiplications compare standard low rank train show method lead good accuracy versus number parameter trade off use speed train large model speedup enable faster inference arm processors new open source kernels optimize small batch size result 3x 7x speed up widely use gemmlowp library beyond lvcsr expect techniques kernels generally applicable embed neural network large fully connect recurrent layer
although word popularity base negative sampler show superb performance skip gram model theoretical motivation behind oversampling popular non observe word negative sample still well understand paper start investigation gradient vanish issue skipgram model without proper negative sampler perform insightful analysis stochastic gradient descent sgd learn perspective demonstrate theoretically intuitively negative sample larger inner product score informative lower score sgd learner term convergence rate accuracy understand propose alternative sample algorithm dynamically select informative negative sample sgd update importantly propose sampler account multi dimensional self embed feature sample process essentially make effective original popularity base one dimensional sampler empirical experiment verify observations show fine grain samplers gain significant improvement exist ones without increase computational complexity
neural network base systems learn locate referents word phrase image answer question visual scenes execute symbolic instructions first person actors partially observable worlds achieve call ground language learn model must overcome challenge infants face learn first word notable model meaningful prior knowledge overcome obstacles researchers currently lack clear understand problem attempt address paper maximum control generality focus simple neural network base language learn agent train via policy gradient methods interpret single word instructions simulate 3d world whilst goal explicitly model infant word learn take inspiration experimental paradigms developmental psychology apply artificial agent explore condition establish human bias learn effect emerge propose novel method visualise semantic representations agent
despite remarkable progress achieve automatic speech recognition recognize far field speeches mix various noise source still challenge task paper introduce novel student teacher transfer learn bridgenet provide solution improve distant speech recognition two key feature bridgenet first bridgenet extend traditional student teacher frameworks provide multiple hint teacher network hint limit soft label teacher network teacher intermediate feature representations better guide student network learn denoise dereverberate noisy input second propose recursive architecture bridgenet iteratively improve denoising recognition performance experimental result bridgenet show significant improvements tackle distant speech recognition problem achieve one thousand, three hundred and twenty-four relative wer reductions ami corpus compare baseline neural network without teacher hint
standard deep learn systems require thousands millions examples learn concept integrate new concepts easily contrast humans incredible ability one shoot shoot learn instance hear word use sentence humans infer great deal leverage syntax semantics surround word tell us draw inspiration highlight simple technique deep recurrent network similarly exploit prior knowledge learn useful representation new word little data could make natural language process systems much flexible allow learn continually new word encounter
context play important role human language understand thus may also useful machine learn vector representations language paper explore asymmetric encoder decoder structure unsupervised context base sentence representation learn carefully design experiment show neither autoregressive decoder rnn decoder require design model still keep rnn encoder use non autoregressive convolutional decoder combine suite effective design significantly improve model efficiency also achieve better performance model train two different large unlabelled corpora case transferability evaluate set downstream nlp task empirically show model simple fast produce rich sentence representations excel downstream task
propose method call label embed network learn label representation label embed train process deep network propose method label embed adaptively automatically learn back propagation original one hot represent loss function convert new loss function soft distributions originally unrelated label continuous interactions train process result train model achieve substantially higher accuracy faster convergence speed experimental result base competitive task demonstrate effectiveness propose method learn label embed reasonable interpretable propose method achieve comparable even better result state art systems source code available urlhttps githubcom lancopku labelemb
paper propose new loss function call generalize end end ge2e loss make train speaker verification model efficient previous tuple base end end te2e loss function unlike te2e ge2e loss function update network way emphasize examples difficult verify step train process additionally ge2e loss require initial stage example selection properties model new loss function decrease speaker verification ever ten reduce train time sixty time also introduce multireader technique allow us domain adaptation train accurate model support multiple keywords ie ok google hey google well multiple dialects
despite success sequence sequence approach automatic speech recognition asr systems model still suffer several problems mainly due mismatch train inference condition sequence sequence architecture model train predict grapheme current time step give input speech signal grind truth grapheme history previous time step however remain unclear well model approximate real world speech inference thus generate whole transcription scratch base previous predictions complicate errors propagate time furthermore model optimize maximize likelihood train data instead error rate evaluation metrics actually quantify recognition quality paper present alternative strategy train sequence sequence asr model adopt idea reinforcement learn rl unlike standard train scheme maximum likelihood estimation propose approach utilize policy gradient algorithm one sample whole transcription base model prediction train process two directly optimize model negative levenshtein distance reward experimental result demonstrate significantly improve performance compare model train maximum likelihood estimation
spite recent success neural machine translation nmt standard benchmarks lack large parallel corpora pose major practical problem many language pair several proposals alleviate issue instance triangulation semi supervise learn techniques still require strong cross lingual signal work completely remove need parallel data propose novel method train nmt system completely unsupervised manner rely nothing monolingual corpora model build upon recent work unsupervised embed mappings consist slightly modify attentional encoder decoder model train monolingual corpora alone use combination denoising backtranslation despite simplicity approach system obtain one thousand, five hundred and fifty-six one thousand and twenty-one bleu point wmt two thousand and fourteen french english german english translation model also profit small parallel corpora attain two thousand, one hundred and eighty-one one thousand, five hundred and twenty-four point combine one hundred thousand parallel sentence respectively implementation release open source project
paper present new method adversarial advantage actor critic adversarial a2c significantly improve efficiency dialogue policy learn task completion dialogue systems inspire generative adversarial network gin train discriminator differentiate responses action generate dialogue agents responses action experts incorporate discriminator another critic advantage actor critic a2c framework encourage dialogue agent explore state action within regions agent take action similar experts experimental result movie ticket book domain show propose adversarial a2c accelerate policy exploration efficiently
due complex nature hard characterize ways machine learn model misbehave exploit deploy recent work adversarial examples ie input minor perturbations result substantially different model predictions helpful evaluate robustness model expose adversarial scenarios fail however malicious perturbations often unnatural semantically meaningful applicable complicate domains language paper propose framework generate natural legible adversarial examples lie data manifold search semantic space dense continuous data representation utilize recent advance generative adversarial network present generate adversaries demonstrate potential propose approach black box classifiers wide range applications image classification textual entailment machine translation include experiment show generate adversaries natural legible humans useful evaluate analyze black box classifiers
paper argue crime drama exemplify television program csicrime scene investigation ideal testbed approximate real world natural language understand complex inferences associate propose treat crime drama new inference task capitalize fact episode pose basic question ie commit crime naturally provide answer perpetrator reveal develop new dataset base csi episodes formalize perpetrator identification sequence label problem develop lstm base model learn multi modal data experimental result show incremental inference strategy key make accurate guess well learn representations fuse textual visual acoustic input
examine relationship social structure sentiment analysis large collection tweet irish marriage referendum two thousand and fifteen obtain sentiment every tweet hashtags marref marriageref post days lead referendum construct network aggregate sentiment use study interactions among users result show sentiment mention tweet post users correlate sentiment receive mention significantly connections users similar sentiment score among users opposite score mention follower network combine community structure two network activity level users sentiment score find group users support vote yes referendum numerous conversations users oppose side debate absence follower connections suggest efforts users establish dialogue debate across ideological divisions analysis show social structure integrate successfully sentiment analyse understand disposition social media users result potential applications integration data meta data study opinion dynamics public opinion model poll
investigate notions ambiguity partial information categorical distributional model natural language probabilistic ambiguity previously study use selinger cpm construction construction work well model build upon vector space show quantum computational applications unfortunately seem provide satisfactory method introduce mix compact close categories category set binary relations therefore lack uniform strategy extend category model imprecise linguistic information work adopt different approach analyze different form ambiguous incomplete information without quantitative probabilistic data scheme correspond suitable enrichment category model language view different monads encapsulate informational behaviour interest analogy use model side effect computation previous result jacobs allow us systematically construct suitable base enrichment show freely enrich arbitrary dagger compact close categories order capture phenomena interest whilst retain important dagger compact close structure allow us construct model real convex combination binary relations make non trivial use scalars finally relate various different enrichments show finite subconvex algebra enrichment cover effect consideration
paper propose probabilistic parse model define proper conditional probability distribution non projective dependency tree give sentence use neural representations input neural network architecture base bi directional lstm cnns benefit word character level representations automatically use combination bidirectional lstm cnn top neural network introduce probabilistic structure layer define conditional log linear model non projective tree evaluate model seventeen different datasets across fourteen different languages exploit kirchhoff matrix tree theorem tutte one thousand, nine hundred and eighty-four partition function marginals compute efficiently lead straight forward end end model train procedure via back propagation parser achieve state art parse performance nine datasets
one main problems emerge classic approach semantics difficulty acquisition maintenance ontologies semantic annotations hand internet explosion massive diffusion mobile smart devices lead creation worldwide system information daily check fuel contribution millions users interact collaborative way search engines continually explore web natural source information base modern approach semantic annotation promise idea possible generalize semantic similarity assumption semantically similar term behave similarly define collaborative proximity measure base index information return search engines pming distance proximity measure use data mine information retrieval collaborative information express degree relationship two term use number document return result query search engine work pminig distance update provide novel formal algebraic definition correct previous work novel point view underline feature pming locally normalize linear combination pointwise mutual information normalize google distance analyze measure dynamically reflect collaborative change make web resources
multi task learn mtl involve simultaneous train two relate task share representations work apply mtl audio visual automatic speech recognitionav asr primary task learn map audio visual fuse feature frame label obtain acoustic gmm hmm model combine auxiliary task map visual feature frame label obtain separate visual gmm hmm model mtl model test various level babble noise result compare base line hybrid dnn hmm av asr model result indicate mtl especially useful higher level noise compare base line upto seven relative improvement wer report three snr db
convolutional neural network cnns effective model reduce spectral variations model spectral correlations acoustic feature automatic speech recognition asr hybrid speech recognition systems incorporate cnns hide markov model gaussian mixture model hmms gmms achieve state art various benchmarks meanwhile connectionist temporal classification ctc recurrent neural network rnns propose label unsegmented sequence make feasible train end end speech recognition system instead hybrid settings however rnns computationally expensive sometimes difficult train paper inspire advantage cnns ctc approach propose end end speech framework sequence label combine hierarchical cnns ctc directly without recurrent connections evaluate approach timit phoneme recognition task show propose model computationally efficient also competitive exist baseline systems moreover argue cnns capability model temporal correlations appropriate context information
describe open source toolkit neural machine translation nmt toolkit prioritize efficiency modularity extensibility goal support nmt research model architectures feature representations source modalities maintain competitive performance reasonable train requirements toolkit consist model translation support well detail pedagogical documentation underlie techniques
microblogging service like twitter become influential today globalise world facets like sentiment analysis extensively study longer constrain opinion others opinions sentiments play huge role shape perspective paper build previous work twitter sentiment analysis use distant supervision exist approach require huge computation resource analyse large number tweet paper propose techniques speed computation process sentiment analysis use tweet subjectivity select right train sample also introduce concept efws effective word score tweet derive polarity score frequently use word additional heuristic use speed sentiment classification standard machine learn algorithms perform experiment use sixteen million tweet experimental evaluations show propose technique efficient higher accuracy compare previously propose methods achieve overall accuracies around eighty efws heuristic give accuracy around eighty-five train dataset 100k tweet half size dataset use baseline model accuracy propose model two three higher baseline model model effectively train twice speed baseline model
open domain human computer conversation attract increase attention past years however exist standard automatic evaluation metric open domain dialog systems researchers usually resort human annotation model evaluation time labor intensive paper propose ruber reference metric unreferenced metric blend evaluation routine evaluate reply take consideration groundtruth reply query previous user issue utterance metric learnable train require label human satisfaction hence ruber flexible extensible different datasets languages experiment retrieval generative dialog systems show ruber high correlation human annotation
currently successful methods video description base encoder decoder sentence generation use recur rent neural network rnns recent work show advantage integrate temporal spatial attention mechanisms model decoder net work predict word description selectively give weight encode feature specific time frame temporal attention feature specific spatial regions spatial attention paper propose expand attention model selectively attend specific time spatial regions specific modalities input image feature motion feature audio feature new modality dependent attention mechanism call multimodal attention provide natural way fuse multimodal information video description evaluate method youtube2text dataset achieve result competitive current state art importantly demonstrate model incorporate multimodal attention well temporal attention significantly outperform model use temporal attention alone
latent dirichlet allocation lda model train without stopword removal often produce topics high posterior probabilities uninformative word obscure underlie corpus content even canonical stopwords manually remove uninformative word common corpus still dominate probable word topic work first show standard topic quality measure coherence pointwise mutual information act counter intuitively presence common irrelevant word make difficult even quantitatively identify situations topics may dominate stopwords propose additional topic quality metric target stopword problem show unlike standard measure correctly correlate human judgements quality also propose simple implement strategy generate topics evaluate much higher quality human assessment new metric approach collection informative priors easily introduce lda style inference methods automatically promote term domain relevance demote domain specific stop word demonstrate approach effectiveness three different domains department labor accident report online health forum post nip abstract overall find current practice think solve problem adequately proposal offer substantial improvement interest interpret topics object right
study large scale kernel methods acoustic model speech recognition compare performance deep neural network dnns perform experiment four speech recognition datasets include timit broadcast news benchmark task compare two type model frame level performance metrics accuracy cross entropy well recognition metrics word character error rate order scale kernel methods large datasets use random fourier feature method rahimi recht two thousand and seven propose two novel techniques improve performance kernel acoustic model first order reduce number random feature require kernel model propose simple effective method feature selection method able explore large number non linear feature maintain compact model efficiently exist approach second present number frame level metrics correlate strongly recognition performance compute heldout set take advantage correlations monitor metrics train order decide stop learn technique noticeably improve recognition performance dnn kernel model narrow gap additionally show linear bottleneck method sainath et al two thousand and thirteen improve performance kernel model significantly addition speed train make model compact together three methods dramatically improve performance kernel acoustic model make performance comparable dnns task explore
describe dynet toolkit implement neural network model base dynamic declaration network structure static declaration strategy use toolkits like theano cntk tensorflow user first define computation graph symbolic representation computation examples feed engine execute computation compute derivatives dynet dynamic declaration strategy computation graph construction mostly transparent implicitly construct execute procedural code compute network output user free use different network structure input dynamic declaration thus facilitate implementation complicate network architectures dynet specifically design allow users implement model way idiomatic prefer program language c python one challenge dynamic declaration symbolic computation graph define anew every train example construction must low overhead achieve dynet optimize c backend lightweight graph representation experiment show dynet speed faster comparable static declaration toolkits significantly faster chainer another dynamic declaration toolkit dynet release open source apache twenty license available http githubcom clab dynet
end end e2e systems achieve competitive result compare conventional hybrid hide markov model hmm deep neural network base automatic speech recognition asr systems e2e systems attractive due lack dependence alignments input acoustic output grapheme hmm state sequence train paper explore design asr free end end system text query base keyword search kws speech train minimal supervision e2e kws system consist three sub systems first sub system recurrent neural network rnn base acoustic auto encoder train reconstruct audio finite dimensional representation second sub system character level rnn language model use embeddings learn convolutional neural network since acoustic text query embeddings occupy different representation space input third fee forward neural network predict whether query occur acoustic utterance e2e asr free kws system perform respectably despite lack conventional asr system train much faster
work several semantic approach concept base query expansion reranking scheme study compare different ontology base expansion methods web document search retrieval particular focus concept base query expansion scheme order effectively increase precision web document retrieval decrease users browse time main goal quickly provide users suitable query expansion two key task query expansion web document retrieval find expansion candidates closest concepts web document domain rank expand query properly approach propose aim improve expansion phase better web document retrieval precision basic idea measure distance candidate concepts use pming distance collaborative semantic proximity measure ie measure compute use statistical result web search engine experiment show propose technique provide users satisfy expansion result improve quality web document retrieval
many aspects people live prove deeply connect job paper first investigate distinct characteristics major occupation categories base tweet multiple social media platforms gather several type user information users linkedin webpages learn proficiencies overcome ambiguity self report information soft cluster approach apply extract occupations crowd source data eight job categories extract include market administrator start editor software engineer public relation office clerk designer meanwhile users post twitter provide cue understand linguistic style interest personalities result suggest people different job unique tendencies certain language style interest result also clearly reveal distinctive level term big five traits different job finally classifier build predict job type base feature extract tweet high accuracy indicate strong discrimination power language feature job prediction task
fifth dialog state track challenge dstc5 introduce new cross language dialog state track scenario participants ask build trackers base english train corpus evaluate unlabeled chinese corpus although computer generate translations english chinese corpus provide dataset translations contain errors careless use easily hurt performance build trackers address problem propose multichannel convolutional neural network cnn architecture treat english chinese language different input channel one single cnn model evaluation dstc5 find multichannel architecture effectively improve robustness translation errors additionally method dstc5 purely machine learn base require prior knowledge target language consider desirable property build tracker cross language context every developer familiar languages
one ubiquitous representation long dna sequence divide shorter k mer components unfortunately straightforward vector encode k mer one hot vector vulnerable curse dimensionality worse yet distance pair one hot vectors equidistant particularly problematic apply latest machine learn algorithms solve problems biological sequence analysis paper propose novel method train distribute representations variable length k mers method base popular word embed model word2vec train shallow two layer neural network experiment provide evidence sum dna2vec vectors akin nucleotides concatenation also demonstrate correlation needleman wunsch similarity score cosine similarity dna2vec vectors
capacity neural network absorb information limit number parameters conditional computation part network active per example basis propose theory way dramatically increase model capacity without proportional increase computation practice however significant algorithmic performance challenge work address challenge finally realize promise conditional computation achieve greater 1000x improvements model capacity minor losses computational efficiency modern gpu cluster introduce sparsely gate mixture experts layer moe consist thousands fee forward sub network trainable gate network determine sparse combination experts use example apply moe task language model machine translation model capacity critical absorb vast quantities knowledge available train corpora present model architectures moe one hundred and thirty-seven billion parameters apply convolutionally stack lstm layer large language model machine translation benchmarks model achieve significantly better result state art lower computational cost
utility twitter data medium support population level mental health monitor well understand effort better understand predictive power supervise machine learn classifiers influence feature set efficiently classify depression relate tweet large scale conduct two feature study experiment first experiment assess contribution feature group lexical information eg unigrams emotions eg strongly negative use feature ablation study second experiment determine percentile top rank feature produce optimal classification performance apply three step feature elimination approach first experiment observe lexical feature critical identify depressive symptoms specifically depress mood thirty-five point disturb sleep forty-three point second experiment observe optimal f1 score performance top rank feature percentiles variably range across class eg fatigue loss energy 5th percentile two hundred and eighty-eight feature depress mood 55th percentile three thousand, one hundred and sixty-eight feature suggest consistent count feature predict depressive relate tweet conclude simple lexical feature reduce feature set produce comparable result larger feature set
popularity image share social media engagement create users reflect important role visual context play everyday conversations present novel task image ground conversations igc natural sound conversations generate share image benchmark progress introduce new multiple reference dataset crowd source event centric conversations image igc fall continuum chit chat goal direct conversation model visual ground constrain topic conversation event drive utterances experiment model train social media data show combination visual textual context enhance quality generate conversational turn human evaluation gap human performance neural retrieval architectures suggest multi modal igc present interest challenge dialogue research
natural language facilitate human robot cooperation nlc natural language nl use share knowledge human robot conduct intuitive human robot cooperation hrc continuously develop recent decade currently nlc use several robotic domains manufacture daily assistance health caregiving necessary summarize current nlc base robotic systems discuss future develop trend provide helpful information future nlc research review first analyze drive force behind nlc research regard robot cognition level cooperation nlc implementations categorize four type nl base control nl base robot train nl base task execution nl base social companion comparison discussion last base perspective comprehensive paper review future research trend discuss
chinese societies superstition paramount importance vehicle license plat desirable number fetch high price auction unlike valuable items license plat allocate estimate price auction propose task predict plate price view natural language process nlp task value depend mean individual character plate semantics construct deep recurrent neural network rnn predict price vehicle license plat hong kong base character plate demonstrate importance deep network retrain evaluate thirteen years historical auction price deep rnn predictions explain eighty percent price variations outperform previous model significant margin also demonstrate model extend become search engine plat provide estimate expect price distribution
natural language facilitate human robot cooperation nlc refer use natural language nl facilitate interactive information share task executions common goal constraint robots humans recently nlc research receive increase attention typical nlc scenarios include robotic daily assistance robotic health caregiving intelligent manufacture autonomous navigation robot social accompany however thorough review reveal latest methodologies use nl facilitate human robot cooperation miss review comprehensive summary methodologies nlc present nlc research include three main research focus nl instruction understand nl base execution plan generation knowledge world map depth analyse theoretical methods applications model advantage disadvantage make base paper review perspective potential research directions nlc summarize
item recommendation task predict personalize rank set items individual user one paradigm rat base methods concentrate explicit feedbacks hence face difficulties collect meanwhile rank base methods present rat items rank rat unrated paradigm take advantage widely available implicit feedback however usually ignore kind important information item review item review justify preferences users also help alleviate cold start problem fail collaborative filter paper propose two novel simple model integrate item review bayesian personalize rank model make use text feature extract item review use word embeddings top text feature uncover review dimension explain variation users feedback review factor represent prior preference users experiment six real world data set show benefit leverage item review rank prediction also conduct analyse understand propose model
machine learn successfully apply new daunt problems almost every day general ai start look like attainable goal however current research focus instead important narrow applications image classification machine translation believe largely due lack objective ways measure progress towards broad machine intelligence order fill gap propose set concrete desiderata general ai together platform test machine well satisfy desiderata keep complexities minimum
paper present semantic attribute modulation sam language model style variation semantic attribute modulation include various document attribute title author document categories consider two type attribute title attribute category attribute flexible attribute selection scheme automatically score via attribute attention mechanism semantic attribute embed hide semantic space generation input attribute properly harness propose sam generate interpretable texts regard input attribute qualitative analysis include word semantic analysis attention value show interpretability sam several typical text datasets empirically demonstrate superiority semantic attribute modulate language model different combinations document attribute moreover present style variation lyric generation use sam show strong connection style variation semantic attribute
deep reinforcement learn rl methods significant potential dialogue policy optimisation however suffer poor performance early stag learn especially problematic line learn real users two approach introduce tackle problem firstly speed learn process two sample efficient neural network algorithms trust region actor critic experience replay tracer episodic natural actor critic experience replay enacer present tracer trust region help control learn step size avoid catastrophic model change enacer natural gradient identify steepest ascent direction policy space speed convergence model employ policy learn experience replay improve sample efficiency secondly mitigate cold start issue corpus demonstration data utilise pre train model prior line reinforcement learn combine two approach demonstrate practical approach learn deep rl base dialogue policies demonstrate effectiveness task orient information seek domain
correlate topic model limit small model problem size due high computational cost poor scale paper propose new model learn compact topic embeddings capture topic correlations closeness topic vectors method enable efficient inference low dimensional embed space reduce previous cubic quadratic time complexity linear wrt topic size speedup variational inference fast sampler exploit sparsity topic occurrence extensive experiment show approach capable handle model data scale several order magnitude larger exist correlation result without sacrifice model quality provide competitive superior performance document classification retrieval
commonly assume language refer high level visual concepts leave low level visual process unaffected view dominate current literature computational model language vision task visual linguistic input mostly process independently fuse single representation paper deviate classic pipeline propose modulate emphentire visual process linguistic input specifically condition batch normalization parameters pretrained residual network resnet language embed approach call modulate resnet mrn significantly improve strong baselines two visual question answer task ablation study show modulate early stag visual process beneficial
question answer qa video content significant challenge achieve human level intelligence involve vision language real world settings demonstrate possibility ai agent perform video story qa learn large amount cartoon videos develop video story learn model ie deep embed memory network demn reconstruct stories joint scene dialogue video stream use latent embed space observe data video stories store long term memory component give question lstm base attention model use long term memory recall best question story answer triplet focus specific word contain key information train demn novel qa dataset children cartoon video series pororo dataset contain sixteen thousand and sixty-six scene dialogue pair two hundred and five hour videos twenty-seven thousand, three hundred and twenty-eight fine grain sentence scene description eight thousand, nine hundred and thirteen story relate qa pair experimental result show demn outperform qa model mainly due one reconstruction video stories scene dialogue combine form utilize latent embed two attention demn also achieve state art result movieqa benchmark
sentiment analysis natural language process nlp task deal detection classification sentiments texts task deal identify presence sentiment text subjectivity analysis task aim determine polarity text categorize positive negative neutral whenever presence sentiment text source people group people entity sentiment direct towards entity object event person sentiment analysis task aim determine subject target polarity valence sentiment work try automatically extract sentiment positive negative facebook post use machine learn approachwhile work do code mix social media data sentiment analysis separately work first attempt aim perform sentiment analysis code mix social media text use extensive pre process remove noise raw text multilayer perceptron model use determine polarity sentiment also develop corpus task manually label facebook post associate sentiments
whenever human be interact exchange express opinions emotions sentiments opinions express text speech image analysis sentiments one popular research areas present day researchers sentiment analysis also know opinion mine try identify classify sentiments opinions two broad categories positive negative recent years scientific community take lot interest analyze sentiment textual data available various social media platforms much work do social media conversations blog post newspaper article various narrative texts however come identify emotions scientific paper researchers face difficulties due implicit hide nature opinion default citation instance consider inherently positive emotion popular rank index paradigms often neglect opinion present cite paper try achieve three objectives first try identify major sentiment citation text assign score instance use statistical classifier purpose secondly propose new index shall refer hereafter index take account quantitative qualitative factor score paper thirdly develop rank research paper base index also try explain index impact rank scientific paper
internet become central medium network publics express opinions engage debate offensive comment personal attack inhibit participation space automate content moderation aim overcome problem use machine learn classifiers train large corpora texts manually annotate offence systems could help encourage civil debate must navigate inherently normatively contestable boundaries subject idiosyncratic norms human raters provide train data important objective platforms implement measure might ensure unduly bias towards particular norms offence paper provide exploratory methods normative bias algorithmic content moderation systems measure way case study use exist dataset comment label offence train classifiers comment label different demographic subsets men women understand differences conceptions offence group might affect performance result model various test set conclude discuss ethical choices face implementers algorithmic moderation systems give various desire level diversity viewpoints amongst discussion participants
natural languages compositional state art neural model achieve compositionality still unclear propose deep network achieve competitive accuracy text classification also exhibit compositional behavior create hierarchical representations piece text sentence lower layer network distribute layer specific attention weight individual word contrast higher layer compose meaningful phrase clauses whose lengths increase network get deeper fully compose sentence
automatic image description systems commonly train evaluate large image description datasets recently researchers start collect datasets languages english unexplored question different datasets english differences cause differ paper provide cross linguistic comparison dutch english german image descriptions find descriptions similar many respect familiarity crowd workers subject image noticeable influence description specificity
natural language process nlp systems often make use machine learn techniques unfamiliar end users interest analyze clinical record although nlp widely use extract information clinical text current systems generally support model revision base feedback domain experts present prototype tool allow end users visualize review output nlp system extract binary variables clinical text tool combine multiple visualizations help users understand result make necessary corrections thus form feedback loop help improve accuracy nlp model test prototype formative think aloud user study clinicians researchers involve colonoscopy research result semi structure interview system usability scale sus analysis show users able quickly start refine nlp model despite little experience machine learn observations sessions suggest revisions interface better support review workflow interpretation result
state art name entity recognition ner systems statistical machine learn model strong generalization capability ie recognize unseen entities appear train data base lexical contextual information however model could still make mistake feature favor wrong entity type paper utilize wikipedia open knowledge base improve multilingual ner systems central approach construction high accuracy high coverage multilingual wikipedia entity type mappings mappings build weakly annotate data extend new languages human annotation language dependent knowledge involve base mappings develop several approach improve ner system evaluate performance approach via experiment ner systems train six languages experimental result show propose approach effective improve accuracy systems unseen entities especially system apply new domain train little train data one hundred and eighty-three f1 score improvement
enormous amount texts publish daily internet users foster development methods analyze content several natural language process areas sentiment analysis main goal task classify polarity message even though many approach propose sentiment analysis successful ones rely availability large annotate corpus expensive time consume process recent years distant supervision use obtain larger datasets inspire techniques paper extend approach incorporate popular graphic symbols use electronic message emojis order create large sentiment corpus portuguese train almost one million tweet several model test domain cross domain corpora methods obtain competitive result five annotate corpora mix domains twitter product review prove domain independent property approach addition result suggest combination emoticons emojis able properly capture sentiment message
every year unite nations member state deliver statements general debate discuss major issue world politics speeches provide invaluable information governments perspectives preferences wide range issue largely overlook study international politics paper introduce new dataset consist seven thousand, seven hundred and one english language country statements one thousand, nine hundred and seventy two thousand and sixteen demonstrate un general debate corpus ungdc use derive country position different policy dimension use text analytic methods paper provide applications estimate demonstrate contribution ungdc make study international politics
adversarial sample strategically modify sample craft purpose fool classifier hand attacker introduce specially craft adversarial sample deploy classifier mis classify classifier however sample perceive draw entirely different class thus become hard detect adversarial sample prior work focus synthesize adversarial sample image domain paper propose new method craft adversarial text sample modification original sample modifications original text sample do delete replace important salient word text introduce new word text sample algorithm work best datasets sub categories within class examples craft adversarial sample one key constraint generate meaningful sentence pass legitimate language english viewpoint experimental result imdb movie review dataset sentiment analysis twitter dataset gender detection show efficiency propose method
amount text generate every day increase dramatically tremendous volume mostly unstructured text simply process perceive computers therefore efficient effective techniques algorithms require discover useful pattern text mine task extract meaningful information text gain significant attentions recent years paper describe several fundamental text mine task techniques include text pre process classification cluster additionally briefly explain text mine biomedical health care domains
achieve artificial visual reason ability answer image relate question require multi step high level process important step towards artificial general intelligence multi modal task require learn question dependent structure reason process image language standard deep learn approach tend exploit bias data rather learn underlie structure lead methods learn visually reason successfully hand craft reason show general purpose conditional batch normalization approach achieve state art result clevr visual reason benchmark twenty-four error rate outperform next best end end method forty-five even methods use extra supervision thirty-one probe model would light reason show learn question dependent multi step process previous work operate assumption visual reason call specialize architecture show general architecture proper condition learn visually reason effectively
quantitative methods measure participation parliamentary debate discourse elect members parliament mps party belong lack exploratory study propose development new approach quantitative analysis participation utilize new zealand government digital hansard database construct topic model parliamentary speeches consist nearly forty million word period two thousand and three two thousand and sixteen latent dirichlet allocation topic model implement order reveal thematic structure set document generative statistical model enable detection major theme topics publicly discuss new zealand parliament well permit classification mp information topic proportion subsequently analyze use combination statistical methods observe pattern arise time series analysis topic frequencies relate specific social economic legislative events construct bipartite network representation link mps topics four parliamentary term time frame build project network onto set nod represent mps proceed study dynamical change topology include community structure perform longitudinal network analysis observe evolution new zealand parliamentary topic network main party period study
foreign policy analysis struggle find ways measure policy preferences paradigm shift international political systems paper present novel potential solution challenge application neural word embed word2vec model dataset feature speeches head state government unite nations general debate paper provide three key contributions base output word2vec model first present set policy attention indices synthesize semantic proximity political speeches specific policy theme second introduce country specific semantic centrality indices base topological analyse countries semantic position respect third test hypothesis exist statistical relation semantic content political speeches un vote behavior falsify suggest political speeches contain information different nature one behind vote outcomes paper conclude discussion practical use result consequences foreign policy analysis public accountability transparency
traditional sentiment analysis approach tackle problems like ternary three category fine grain five category classification learn task separately argue classification task correlate propose multitask approach base recurrent neural network benefit jointly learn study demonstrate potential multitask model type problems improve state art result fine grain sentiment classification problem
model execute natural language instructions situate robotic task assembly navigation several useful applications home offices remote scenarios study semantics spatially refer configuration arrangement instructions base challenge bisk two thousand and sixteen blank label block dataset task involve find source block move target position mention via reference block offset block name color refer via spatial location feature present novel model subtasks source block classification target position regression base joint loss language spatial world representation learn well cnn base dual attention model compute alignment world block instruction phrase target position prediction compare two inference approach anneal sample via policy gradient versus expectation inference via supervise regression model achieve new state art task improvement forty-seven source block accuracy twenty-two target position distance
present two new large scale datasets aim evaluate systems design comprehend natural language query extract answer large corpus text quasar dataset consist thirty-seven thousand cloze style fill gap query construct definitions software entity tag popular website stack overflow post comment website serve background corpus answer cloze question quasar dataset consist forty-three thousand open domain trivia question answer obtain various internet source clueweb09 serve background corpus extract answer pose datasets challenge two relate subtasks factoid question answer one search relevant piece text include correct answer query two read retrieve text answer query also describe retrieval system extract relevant sentence document corpus give query include release researchers wish focus two evaluate several baselines datasets range simple heuristics powerful neural model show lag behind human performance one hundred and sixty-four three hundred and twenty-one quasar respectively datasets available https githubcom bdhingra quasar
interpretation spatial reference highly contextual require joint inference language environment consider task spatial reason simulate environment agent act receive reward propose model learn representation world steer instruction text design allow precise alignment local neighborhoods correspond verbalizations also handle global reference instructions train model reinforcement learn use variant generalize value iteration model outperform state art approach several metrics yield forty-five reduction goal localization error
machine learn play role many aspects modern ir systems deep learn apply fast pace modern day research give rise many different approach many different ir problems amount information available overwhelm junior students experience researchers look new research topics directions additionally interest see key insights ir problems new technologies able give us aim full day tutorial give clear overview current try trust neural methods ir benefit ir research cover key architectures well promise future directions
paper describe lithium natural language process nlp system resource constrain high throughput language agnostic system information extraction noisy user generate text social media lithium nlp extract rich set information include entities topics hashtags sentiment text discuss several real world applications system currently incorporate lithium products also compare system exist commercial academic nlp systems term performance information extract languages support show lithium nlp par case outperform state art commercial nlp systems
music genre classification especially use lyric alone remain challenge topic music information retrieval study apply recurrent neural network model classify large dataset intact song lyric lyric exhibit hierarchical layer structure word combine form line line form segment segment form complete song adapt hierarchical attention network han exploit layer addition learn importance word line segment test model one hundred and seventeen genre dataset reduce twenty genre dataset experimental result show han outperform non neural model simpler neural model whilst also classify higher number genres previous research learn process also visualise word line song model believe important classify genre result han provide insights computational perspective lyrical structure language feature differentiate musical genres
memoisation table well know technique yield large improvements performance recursive computations table resolution prologs xsb b prolog transform call leave recursive predicate non terminate computations finite well behave ones functional program literature memoisation usually implement way handle leave recursion require supplementary mechanisms prevent non termination notable exception johnson one thousand, nine hundred and ninety-five continuation pass approach scheme however rely mutation memo table data structure cod explicit continuation pass style show johnson approach implement purely functionally modern strongly type functional language ocaml present via monadic interface hide implementation detail yet provide way return compact represention memo table end computation
despite close relationship speech perception production research automatic speech recognition asr text speech synthesis tts progress less independently without exert much mutual influence human communication hand close loop speech chain mechanism auditory feedback speaker mouth ear crucial paper take step develop close loop speech chain model base deep learn sequence sequence model close loop architecture allow us train model concatenation label unlabeled data asr transcribe unlabeled speech feature tts attempt reconstruct original speech waveform base text asr opposite direction asr also attempt reconstruct original text transcription give synthesize speech best knowledge first deep learn model integrate human speech perception production behaviors experimental result show propose approach significantly improve performance separate systems train label data
paper discuss generation symbols alphabets base specific user requirements medium priorities type information need convey framework generation alphabets propose use generation shorthand write system explore discuss possible use machine learn genetic algorithms gather input generation alphabets optimization already generate ones alphabets generate use methods may use different field creation synthetic languages construct script creation sensible command multimodal interaction human computer interfaces mouse gesture touchpads body gesture eye track cameras brain compute interfaces especially applications elderly care people disabilities
sequential constraint grammar scg karlsson one thousand, nine hundred and ninety extensions lack clear connections formal language theory purpose article lay foundation connections simplify definition string process grammar show nonmonotonic scg undecidable derivations similar generative phonology exist current investigations propose resource bound restrict generative power scg subset context sensitive languages present strong finite state condition grammars wholes show grammar equivalent finite state transducer implement turing machine run ofn log n time condition open new finite state hypotheses avenues deeper analysis scg instance way inspire finite state phonology
investigate utility different auxiliary objectives train strategies within neural sequence label approach error detection learner write auxiliary cost provide model additional linguistic information allow learn general purpose compositional feature exploit objectives experiment show joint learn approach train parallel label domain data improve performance previous best error detection system result model number parameters additional objectives allow optimise efficiently achieve better performance
automate methods essay score make great progress recent years achieve accuracies close human annotators however know weakness automate scorers take account semantic relevance submit text exist work detect answer relevance give textual prompt little previous research do incorporate visual write prompt propose neural architecture several extensions detect topic responses visual prompt evaluate dataset texts write language learners
matter cognition user interest apt classification independent language users social network content interest prove analyze collection english russian twitter vkontakte community page interest followers first create model major interest mais help expert analysis classify set page use machine learn algorithms svm neural network naive bay take three interest domains typical english russian speak communities football rock music vegetarianism result classification show greater correlation russian vkontakte russian twitter page english twitterpages appear provide highest score
present new technique learn visual semantic embeddings cross modal retrieval inspire hard negative mine use hard negative structure prediction rank loss function introduce simple change common loss function use multi modal embeddings combine fine tune use augment data yield significant gain retrieval performance showcase approach vse ms coco flickr30k datasets use ablation study comparisons exist methods ms coco approach outperform state art methods eighty-eight caption retrieval one hundred and thirteen image retrieval r1
human language one natural interfaces humans interact robots paper present robot system retrieve everyday object unconstrained natural language descriptions core issue system semantic spatial ground infer object spatial relationships image natural language expressions introduce two stage neural network ground pipeline map natural language refer expressions directly object image first stage use visual descriptions refer expressions generate candidate set relevant object second stage examine pairwise relationships candidates predict likely refer object accord spatial descriptions refer expressions key feature system leverage large dataset image label text descriptions allow unrestricted object type natural language refer expressions preliminary result indicate system outperform near state art object comprehension system standard benchmark datasets also present robot system follow voice command pick place previously unseen object
goal study automatic detection onsets sing voice polyphonic audio record start hypothesis knowledge current position metrical cycle ie metrical accent improve accuracy vocal note onset detection propose novel probabilistic model jointly track beat vocal note onsets propose model extend state art model beat meter track priori probability note specific metrical accent interact probability observe vocal note onset carry evaluation vary collection multi instrument datasets two music traditions english popular music turkish makam different type metrical cycle sing style result confirm propose model reasonably improve vocal note onset detection accuracy compare baseline model take metrical position account
present novel method obtain high quality domain target multiple choice question crowd workers generate question difficult without trade away originality relevance diversity answer options method address problems leverage large corpus domain specific text small set exist question produce model suggestions document selection answer distractor choice aid human question generation process method assemble sciq dataset 137k multiple choice science exam question dataset available http allenaiorg datahtml demonstrate method produce domain question provide analysis new dataset show humans distinguish crowdsourced question original question use sciq additional train data exist question observe accuracy improvements real science exams
computational model sarcasm detection often rely content utterances isolation however speaker sarcastic intent always obvious without additional context focus social media discussions investigate two issue one model conversation context help sarcasm detection two understand part conversation context trigger sarcastic reply address first issue investigate several type long short term memory lstm network model conversation context sarcastic response show conditional lstm network rocktaschel et al two thousand and fifteen lstm network sentence level attention context response outperform lstm model read response address second issue present qualitative analysis attention weight produce lstm model attention discuss result compare human performance task
video question answer challenge problem visual information retrieval provide answer reference video content accord question however exist visual question answer approach mainly tackle problem static image question may ineffectively video question answer due insufficiency model temporal dynamics video content paper study problem video question answer model temporal dynamics frame level attention mechanism propose attribute augment attention network learn framework enable joint frame level attribute detection unify video representation learn video question answer incorporate multi step reason process propose attention network improve performance construct large scale video question answer dataset conduct experiment multiple choice open end video question answer task show effectiveness propose method
syllabification seem improve word level rnn language model quality compare character base segmentation however best syllable aware language model achieve performance comparable competitive character aware model eighteen thirty-three fewer parameters train twelve twenty-two time faster
although great progress make automatic speech recognition asr significant performance degradation still observe recognize multi talker mix speech paper propose evaluate several architectures address problem assumption single channel mix signal available technique extend permutation invariant train pit introduce front end feature separation module minimum mean square error mse criterion back end recognition module minimum cross entropy ce criterion specifically train compute average mse ce whole utterance possible utterance level output target assignment pick one minimum mse ce optimize assignment strategy elegantly solve label permutation problem observe deep learn base multi talker mix speech separation recognition systems propose architectures evaluate compare artificially mix ami dataset two three talker mix speech experimental result indicate propose architectures cut word error rate wer four hundred and fifty two hundred and fifty relatively state art single talker speech recognition system across speakers energies comparable two three talker mix speech respectively knowledge first work multi talker mix speech recognition challenge speaker independent spontaneous large vocabulary continuous speech task
present new neural text speech tts method able transform text speech voice sample wild unlike systems solution able deal unconstrained voice sample without require align phonemes linguistic feature network architecture simpler exist literature base novel shift buffer work memory buffer use estimate attention compute output audio update buffer input sentence encode use context free lookup table contain one entry per character phoneme speakers similarly represent short vector also fit new identities even sample variability generate speech achieve prim buffer prior generate audio experimental result several datasets demonstrate convince capabilities make tts accessible wider range applications order promote reproducibility release source code model
propose novel word embed pre train approach exploit write errors learners script compare method previous model tune embeddings base script score discrimination correct corrupt word contexts addition generic commonly use embeddings pre train large corpora comparison achieve use aforementioned model bootstrap neural network learn predict holistic score script furthermore investigate augment model error corrections monitor impact performance result show error orient approach outperform comparable ones demonstrate train data additionally extend model corrections provide performance gain data sparsity issue
paper focus online review employ artificial intelligence tool take cognitive compute field help understand relationships textual part review assign numerical score move intuitions one set textual review express different sentiments may feature score vice versa two detect analyze mismatch review content actual score may benefit service providers consumers highlight specific factor satisfaction dissatisfaction texts prove intuitions adopt sentiment analysis techniques concentrate hotel review find polarity mismatch therein particular first train text classifier set annotate hotel review take book website analyze large dataset around 160k hotel review collect tripadvisor aim detect polarity mismatch indicate textual content review line associate score use well establish artificial intelligence techniques analyze depth review feature mismatch text polarity score find scale five star review rank middle score include mixture positive negative aspects approach propose beside act polarity detector provide effective selection review initial large dataset may allow consumers providers focus directly review subset feature text score disagreement conveniently convey user summary positive negative feature review target
creative task ideation question proposal powerful applications crowdsourcing yet quantity workers available address practical problems often insufficient enable scalable crowdsourcing thus require gain possible efficiency information available workers one option text focus task allow assistive technology autocompletion user interface aui help workers input text responses support efficacy auis mix design conduct randomize experiment workers ask provide short text responses give question experimental goal determine aui help workers respond quickly improve consistency mitigate typos misspell surprisingly find neither occur workers assign aui treatment slower assign non aui control responses diverse less control lexical semantic diversities responses higher latter measure use word2vec crowdsourcer interest worker speed may want avoid use aui use aui boost response diversity may valuable crowdsourcers interest receive much novel information workers possible
dynamics govern time series represent appearance word social media data paper investigate elementary dynamics word dependent special effect segregate break news increase decrease concern seasonality elucidate problem investigate approximately three billion japanese blog article period six years analyse correspond solvable mathematical model analysis find word appearance explain random diffusion model base power law forget process type long memory point process relate arfima0050 particular confirm ultraslow diffusion mean square displacement grow logarithmically model predict approximate manner reproduce actual data addition also show model reproduce statistical properties time series fluctuation scale ii spectrum density iii shape probability density function
trans dimensional random field language model trf lms recently introduce sentence model collection random field trf approach show advantage computationally efficient inference lstm lms close performance able flexibly integrate rich feature paper propose neural trfs beyond previous discrete trfs use linear potentials discrete feature idea use nonlinear potentials continuous feature implement neural network nns trf framework neural trfs combine advantage nns trfs benefit word embed nonlinear feature learn larger context model inherit use nns time strength efficient inference avoid expensive softmax preserve number technical contributions include employ deep convolutional neural network cnns define potentials incorporate joint stochastic approximation jsa strategy train algorithm develop work enable us successfully train neural trf lms various lms evaluate term speech recognition wers rescoring one thousand best list wsj ninety-two test data result show neural trf lms improve discrete trf lms also perform slightly better lstm lms one fifth parameters 16x faster inference efficiency
machine translation natural candidate problem reinforcement learn human feedback users provide quick dirty rat candidate translations guide system improve yet current neural machine translation train focus expensive human generate reference translations describe reinforcement learn algorithm improve neural machine translation systems simulate human feedback algorithm combine advantage actor critic algorithm mnih et al two thousand and sixteen attention base neural encoder decoder architecture luong et al two thousand and fifteen algorithm well design problems large action space delay reward b effectively optimize traditional corpus level machine translation metrics c robust skew high variance granular feedback model actual human behaviors
paper propose method stock prediction term feature extraction extract feature stock relate news besides stock price first select seed word base experience symbols good news bad news propose optimization method calculate positive polar word construct feature news base positive polar word consideration sequential stock price continuous news effect propose recurrent neural network model help predict stock price compare svm classifier price feature find propose method five improvement stock prediction accuracy experiment
deep neural network become primary tool solve problems many field also use address information retrieval problems show strong performance several task train model require large representative datasets ir task data contain sensitive information users privacy confidentiality concern prevent many data owners share data thus today research community benefit research large scale datasets limit manner paper discuss privacy preserve mimic learn ie use predictions privacy preserve train model instead label original sensitive train data supervision signal present result preliminary experiment apply idea mimic learn privacy preserve mimic learn task document rank one core ir task research step toward lay grind enable researchers data rich environments share knowledge learn actual users data facilitate research collaborations
number scientific article grow rapidly years sign growth slow near future become increasingly difficult keep latest developments scientific field address problem present approach help researchers learn latest developments find extract normalize form core claim scientific article normalize representation control natural language english sentence call aida propose previous work method formally structure organize scientific find discourse show aida sentence automatically extract detect core claim article check aida compliance necessary transform compliant sentence algorithm still far perfect result indicate different step feasible support claim aida sentence might promise approach improve scientific communication future
entity retrieval task find entities people products response query base solely textual document associate recent semantic entity retrieval algorithms represent query experts finite dimensional vector space construct text sequence investigate entity vector space degree capture structural regularities vector space construct unsupervised manner without explicit information structural aspects concreteness address question specific type entity experts context expert find discover cluster experts correspond committees organizations ability expert representations encode co author graph degree encode academic rank compare latent continuous representations create use methods base distributional semantics lsi topic model lda neural network word2vec doc2vec sert vector space create use neural methods doc2vec sert systematically perform better cluster lsi lda word2vec come encode entity relations sert perform best
carry comprehensive analysis letter frequencies contemporary write marathi determine set letter statistically predominate large generic marathi text use set estimate entropy marathi
paper introduce novel type rectify linear unit relu call dual rectify linear unit drelu drelu come unbounded positive negative image use drop replacement tanh activation function recurrent step quasi recurrent neural network qrnns bradbury et al two thousand and seventeen similar relus drelus less prone vanish gradient problem noise robust induce sparse activations independently reproduce qrnn experiment bradbury et al two thousand and seventeen compare drelu base qrnns original tanh base qrnns long short term memory network lstms sentiment classification word level language model additionally evaluate character level language model show able stack eight qrnn layer drelus thus make possible improve current state art character level language model shallow architectures base lstms
work present technique use natural language help reinforcement learn generalize unseen environments technique use neural machine translation specifically use encoder decoder network learn associations natural language behavior descriptions state action information use learn model guide agent exploration use modify version policy shape make effective learn unseen environments evaluate technique use popular arcade game frogger ideal non ideal condition evaluation show modify policy shape algorithm improve q learn agent well baseline version policy shape
explain underlie cause effect events challenge valuable task define novel problem generate explanations time series event one search effect relationships time series textual data two construct connect chain generate explanation detect causal feature text propose novel method base granger causality time series feature extract text n grams topics sentiments composition generation sequence causal entities require commonsense causative knowledge base efficient reason ensure good interpretability appropriate lexical usage combine symbolic neural representations use neural reason algorithm train commonsense causal tuples predict next step quantitative human analysis show empirical evidence method successfully extract meaningful causality relationships time series textual feature generate appropriate explanation
goal counterfactual learn statistical machine translation smt optimize target smt system log data consist user feedback translations predict another historic smt system challenge arise fact risk averse commercial smt systems deterministically log probable translation lack sufficient exploration smt output space seemingly contradict theoretical requirements counterfactual learn show counterfactual learn deterministic bandit log possible nevertheless smooth deterministic components learn achieve additive multiplicative control variates avoid degenerate behavior empirical risk minimization simulation experiment show improvements two bleu point counterfactual learn deterministic bandit feedback
language increasingly use define rich visual recognition problems support image collections source web structure prediction model use task take advantage correlations co occur label visual input risk inadvertently encode social bias find web corpora work study data model associate multilabel object classification visual semantic role label find datasets task contain significant gender bias b model train datasets amplify exist bias example activity cook thirty-three likely involve females males train set train model amplify disparity sixty-eight test time propose inject corpus level constraints calibrate exist structure prediction model design algorithm base lagrangian relaxation collective inference method result almost performance loss underlie recognition task decrease magnitude bias amplification four hundred and seventy-five four hundred and five multilabel classification visual semantic role label respectively
complex high dimensional unstructured data often difficult extract meaningful pattern especially case deal textual data recent study machine learn information theory network science develop several novel instrument extract semantics unstructured data harness build network relations approach serve efficient tool dimensionality reduction pattern detection paper apply semantic network science extract ideological proximity international arena focus data general debate un general assembly topics high salience international community un general debate corpus ungdc cover high level debate un general assembly one thousand, nine hundred and seventy two thousand and fourteen cover un member state research proceed three main step first latent dirichlet allocation lda use extract topics un speeches therefore semantic information country assign vector specify exposure topics identify intermediate output use construct network countries base information theoretical metrics link capture similar vectorial pattern topic distributions topology network analyze network properties like density path length cluster finally identify specific topological feature network use map equation framework detect communities network countries
current state art image annotation image retrieval task obtain deep neural network combine image representation text representation share embed space paper evaluate impact use full network embed set replace original image representation competitive multimodal embed generation scheme unlike one layer image embeddings typically use approach full network embed provide multi scale representation image result richer characterizations measure influence full network embed evaluate performance three different datasets compare result original multimodal embed generation scheme use one layer image embed rest state art result image annotation image retrieval task indicate full network embed consistently superior one layer embed result motivate integration full network embed multimodal embed generation scheme something feasible thank flexibility approach
train deep neural network require massive amount train data many task limit label data available make weak supervision attractive use weak noisy signal like output heuristic methods user click data train semi supervise set use large set data weak label pretrain neural network fine tune parameters small amount data true label feel intuitively sub optimal two independent stag leave model unaware vary label quality could somehow inform model label quality paper propose semi supervise learn method train two neural network multi task fashion target network confidence network target network optimize perform give task train use large set unlabeled data weakly annotate propose weight gradient update target network use score provide second confidence network train small amount supervise data thus avoid weight update compute noisy label harm quality target network model evaluate learn strategy two different task document rank sentiment classification result demonstrate approach enhance performance compare baselines also speed learn process weak label
humans understand produce new utterances effortlessly thank compositional skills person learn mean new verb dax immediately understand mean dax twice sing dax paper introduce scan domain consist set simple compositional navigation command pair correspond action sequence test zero shoot generalization capabilities variety recurrent neural network rnns train scan sequence sequence methods find rnns make successful zero shoot generalizations differences train test command small apply mix match strategies solve task however generalization require systematic compositional skills dax example rnns fail spectacularly conclude proof concept experiment neural machine translation suggest lack systematicity might partially responsible neural network notorious train data thirst
paper present design machine learn architecture underlie alexa skills kit ask large scale speak language understand slu software development kit sdk enable developers extend capabilities amazon virtual assistant alexa amazon infrastructure power twenty-five thousand skills deploy ask well aws amazon lex slu service ask emphasize flexibility predictability rapid iteration cycle third party developers impose inductive bias allow learn robust slu model extremely small sparse datasets remove significant barriers entry software developers dialogue systems researchers
train conventional automatic speech recognition asr system support multiple languages challenge sub word unit lexicon word inventory typically language specific contrast sequence sequence model well suit multilingual asr encapsulate acoustic pronunciation language model jointly single network work present single sequence sequence asr model train nine different indian languages little overlap script specifically take union language specific grapheme set train grapheme base sequence sequence model jointly data languages find model explicitly give information language identity improve recognition performance twenty-one relative compare analogous sequence sequence model train language individually modify model accept language identifier additional input feature improve performance additional seven relative eliminate confusion different languages
data mine machine learn natural language process powerful techniques use together extract information large texts depend task problem hand many different approach use methods available continuously optimize methods test compare set problems solve use supervise machine learn algorithms question happen quality methods increase train data size say one hundred mb one gb moreover quality gain worth rate data process diminish trade quality time efficiency recover quality loss able process data attempt answer question general way text process task consider trade off involve train data size learn time quality obtain propose performance trade framework apply three important text process problems name entity recognition sentiment analysis document classification problems also choose different level object granularity word paragraph document problem select several supervise machine learn algorithms evaluate trade off large publicly available data set news review patent explore trade off use different data subsets increase size range fifty mb several gb also consider impact data set evaluation technique find result change significantly time best algorithms fastest however also show result small data say less one hundred mb different result big data case best algorithm much harder determine
train deep neural network require many train sample practice train label expensive obtain may vary quality may trust expert labelers others might heuristics source weak supervision crowd source create fundamental quality versus quantity trade learn process learn small amount high quality data potentially large amount weakly label data argue learner could somehow know take label quality account learn data representation could get best worlds end propose fidelity weight learn fwl semi supervise student teacher approach train deep neural network use weakly label data fwl modulate parameter update student network train task care per sample basis accord posterior confidence label quality estimate teacher access high quality label student teacher learn data evaluate fwl two task information retrieval natural language process outperform state art alternative semi supervise methods indicate approach make better use strong weak label lead better task dependent data representations
paper analyse benefit incorporate interval value fuzzy set bousi prolog system syntax declarative semantics i plementation extension present formalise show use potential applications fuzzy logic program frameworks enhance correctly work together lexical resources ontologies order improve capabilities knowledge representation reason
represent semantics word long stand problem natural language process community methods compute word semantics give textual context large corpora recently researchers attempt integrate perceptual visual feature work consider visual appearance object enhance word representations ignore visual environment context object appear propose unify text base techniques vision base techniques simultaneously leverage textual visual context learn multimodal word embeddings explore various choices serve visual context present end end method integrate visual context elements multimodal skip gram model provide experiment extensive analysis obtain result
speech recognition systems achieve high recognition performance several task however performance systems dependent tremendously costly development work prepare vast amount task match transcribe speech data supervise train key problem cost transcribe speech data cost repeatedly require support new languages new task assume broad network service transcribe speech data many users system would become self sufficient useful possess ability learn light feedback users without annoy paper propose general reinforcement learn framework speech recognition systems base policy gradient method particular instance framework also propose hypothesis selection base reinforcement learn method propose framework provide new view several exist train adaptation methods experimental result show propose method improve recognition performance compare unsupervised adaptation
recently development internet web different type social media web blog become immense source text data process data possible discover practical information different topics individuals opinions thorough understand society therefore apply model automatically extract subjective information document would efficient helpful topic model methods also sentiment analysis raise topics natural language process text mine field paper new structure joint sentiment topic model base restrict boltzmann machine rbm type neural network propose modify structure rbm well append layer analogous sentiment text data propose generative structure joint sentiment topic model base neutral network propose method supervise train contrastive divergence algorithm new attach layer propose model layer multinomial probability distribution use text data sentiment classification supervise application propose model compare exist model experiment evaluate generative model sentiment classification information retrieval correspond result demonstrate efficiency method
object refer important applications especially human machine interaction receive great attention task mainly attack write language text input rather speak language speech natural paper investigate object refer speak language orspoken present two datasets one novel approach object annotate locations image text descriptions speech descriptions make datasets ideal multi modality learn approach develop carefully take orspoken problem three sub problems introduce task specific vision language interactions correspond level experiment show method outperform compete methods consistently significantly approach also evaluate presence audio noise show efficacy propose vision language interaction methods counteract background noise
word2vec mikolov et al two thousand and thirteen prove successful natural language process capture semantic relationships different word build top single word embeddings paragraph vectors le mikolov two thousand and fourteen find fix length representations piece text arbitrary lengths document paragraph sentence work propose novel interpretation neural network base paragraph vectors develop unsupervised generative model whose maximum likelihood solution correspond traditional paragraph vectors probabilistic formulation allow us go beyond point estimate parameters perform bayesian posterior inference find entropy paragraph vectors decrease length document information posterior uncertainty improve performance supervise learn task sentiment analysis paraphrase detection
kernel methods produce state art result number nlp task relation extraction suffer poor scalability due high cost compute kernel similarities natural language structure recently propose technique kernelized locality sensitive hash klsh significantly reduce computational cost applicable classifiers operate knn graph propose use random subspaces klsh cod efficiently construct explicit representation nlp structure suitable general classification methods propose approach optimize klsh model classification problems maximize approximation mutual information klsh cod feature vectors class label evaluate propose approach biomedical relation extraction datasets observe significant robust improvements accuracy wrt state art classifiers along drastic order magnitude speedup compare conventional kernel methods
propose new model unsupervised document embed lead exist approach either require complex inference use recurrent neural network rnn difficult parallelize take different route develop convolutional neural network cnn embed model cnn architecture fully parallelizable result 10x speedup inference time rnn model parallelizable architecture enable train deeper model successive layer increasingly larger receptive field model longer range semantic structure within document additionally propose fully unsupervised learn algorithm train model base stochastic forward prediction empirical result two public benchmarks show approach produce comparable state art accuracy fraction computational cost
synthesize sql query natural language long stand open problem attract considerable interest recently toward solve problem de facto approach employ sequence sequence style model approach necessarily require sql query serialize since sql query may multiple equivalent serializations train sequence sequence style model sensitive choice one phenomenon document order matter problem exist state art approach rely reinforcement learn reward decoder generate equivalent serializations however observe improvement reinforcement learn limit paper propose novel approach ie sqlnet fundamentally solve problem avoid sequence sequence structure order matter particular employ sketch base approach sketch contain dependency graph one prediction do take consideration previous predictions depend addition propose sequence set model well column attention mechanism synthesize query base sketch combine novel techniques show sqlnet outperform prior art nine thirteen wikisql task
one major terminological force drive ict integration research today big data phrase sound inclusive integrative big data approach highly selective exclude input effectively structure represent digitise data complex sort precisely kind human activity produce technological imperative enhance signal reduction noise accommodate richness data computational approach facilitate big data acquire perceive objectivity belie curated malleable reactive performative nature input environment anything data enter system data data clean process together metadata information architectures structure facilitate cultural archive acquire capacity delimit data engender process simplification major implications potential future innovation within research environments depend rich material yet increasingly mediate digital technologies paper present preliminary find european fund kplex knowledge complexity project investigate delimit effect digital mediation datafication rich complex cultural data paper present systematic review exist implicit definitions data elaborate implications definitions highlight ways metadata computational technologies restrict interpretative potential data shed light gap analogue augment digital practice fully computational ones strategies researchers develop deal gap paper propose reconceptualisation data functionally employ within digitally mediate research incorporate acknowledge richness complexity source materials
audience interest demography purchase behavior possible classifications ex tremely important factor carefully study target campaign information help advertisers publishers deliver advertisements right audience group ever easy collect information especially online audience limit interaction minimum deterministic knowledge paper pro pose predictive framework estimate online audience demographic attribute base browse histories propose framework first retrieve content websites visit audience represent content website feature vectors second aggregate vectors websites audience visit arrive feature vectors represent users finally support vector machine exploit predict audience demographic attribute key achieve good prediction performance prepare representative feature audience word embed widely use tech nique natural language process task together term frequency inverse document frequency weight scheme use propose method new representation ap proach unsupervised easy implement experimental result demonstrate new audience feature representation method powerful exist baseline methods lead great improvement prediction accuracy
train automatic speech recognition asr systems require large amount data target language order achieve good performance whereas large train corpora readily available languages like english exist long tail languages suffer lack resources one method handle data sparsity use data additional source languages build multilingual system recently asr systems base recurrent neural network rnns train connectionist temporal classification ctc gain substantial research interest work extend previous approach towards train ctc base systems multilingually systems feature global phone set base joint phone set source language evaluate use different language combinations well addition language feature vectors lfvs contrastive experiment build systems base graphemes well systems multilingual phone set know suffer performance compare monolingual counterparts propose approach could reduce gap mono multilingual setups use either graphemes phonemes
work focus multilingual systems base recurrent neural network rnns train use connectionist temporal classification ctc loss function use multilingual set acoustic units pose difficulties address issue propose language feature vectors lfvs train language adaptive multilingual systems language adaptation contrast speaker adaptation need apply feature level also deeper layer network work therefore extend previous approach introduce novel technique call modulation base method modulate hide layer rnns use lfvs evaluate approach full low resource condition well grapheme phone base systems lower error rat throughout different condition could achieve use modulation
ensemble techniques powerful approach combine several weak learners build stronger one meta learn framework ensemble techniques easily apply many machine learn techniques paper propose neural network extend ensemble loss function text classification weight weak loss function tune within train phase gradient propagation optimization method neural network approach evaluate several text classification datasets also evaluate performance various environments several degrees label noise experimental result indicate improvement result strong resilience label noise comparison methods
train semantic parsers weak supervision denotations rather strong supervision program complicate train two ways first large search space potential program need explore train time find correct program second spurious program accidentally lead correct denotation add noise train work propose close worlds clear semantic type one substantially alleviate problems utilize abstract representation tokens language utterance program lift abstract form show abstractions define handful lexical rule result share different examples alleviate difficulties train test approach develop first semantic parser cnlvr challenge visual reason dataset search space large overcome spuriousness critical denotations either true false thus random program likely lead correct denotation method substantially improve performance reach eight hundred and twenty-five accuracy one hundred and forty-seven absolute accuracy improvement compare best report accuracy far
investigate computational complexity various problems simple recurrent neural network rnns formal model recognize weight languages focus single layer relu activation rational weight rnns softmax commonly use natural language process applications show problems rnns undecidable include consistency equivalence minimization determination highest weight string however consistent rnns last problem become decidable although solution length surpass computable bound additionally string limit polynomial length problem become np complete apx hard summary show approximations heuristic algorithms necessary practical applications rnns
trivial events ubiquitous human human conversations eg cough laugh sniff compare regular speech trivial events usually short unclear thus generally regard speaker discriminative largely ignore present speaker recognition research however trivial events highly valuable particular circumstances forensic examination less subject intentional change use discover genuine speaker disguise speech paper collect trivial event speech database involve seventy-five speakers six type events report preliminary speaker recognition result database human listeners machine particularly deep feature learn technique recently propose group utilize analyze recognize trivial events lead acceptable equal error rat eers despite extremely short durations two five second events compare different type events hmm seem speaker discriminative
paper introduce emotional speech synthesizer base recent end end neural model name tacotron despite benefit find original tacotron suffer exposure bias problem irregularity attention alignment later address problem utilization context vector residual connection recurrent neural network rnns experiment show model could successfully train generate speech give emotion label
recurrent neural network rnn language model lms long short term memory lstm lms variant rnn lms show outperform traditional n gram lms speech recognition task however model computationally expensive n gram lms decode thus challenge integrate speech recognizers recent research propose use lattice rescoring algorithms use rnnlms lstmlms efficient strategy integrate model speech recognition system paper evaluate exist lattice rescoring algorithms along new variants youtube speech recognition task lattice rescoring use lstmlms reduce word error rate wer task eight relative wer obtain use n gram lm
automatic generation caption describe content image gain lot research interest recently exist work treat image caption pure sequential data natural language however possess temporal hierarchy structure complex dependencies subsequence paper propose phrase base hierarchical long short term memory phi lstm model generate image description contrast conventional solutions generate caption pure sequential manner propose model decode image caption phrase sentence consist phrase decoder bottom hierarchy decode noun phrase variable length abbreviate sentence decoder upper hierarchy decode abbreviate form image description complete image caption form combine generate phrase sentence inference stage empirically propose model show better competitive result flickr8k flickr30k ms coco datasets comparison state art model also show propose model able generate novel caption see train data richer word content three datasets
dynamic topic model facilitate identification topical trend time temporal collections unstructured document introduce novel unsupervised neural dynamic topic model name recurrent neural network replicate softmax model rnnrsm discover topics time influence topic discovery subsequent time step account temporal order document explicitly model joint distribution latent topical dependencies time use distributional estimators temporal recurrent connections apply rnn rsm nineteen years article nlp research demonstrate compare state art topic model rnnrsm show better generalization topic interpretation evolution trend also introduce metric name span quantify capability dynamic topic model capture word evolution topics time
present new algorithm significantly improve efficiency exploration deep q learn agents dialogue systems agents explore via thompson sample draw monte carlo sample bay backprop neural network algorithm learn much faster common exploration strategies epsilon greedy boltzmann bootstrapping intrinsic reward base ones additionally show spike replay buffer experience successful episodes make q learn feasible might otherwise fail
search engines rely heavily term base approach represent query document bag word text document query represent bag word ignore grammar word order retain word frequency count present search query engine rank document accord relevance score compute among things match degrees query document term term base approach intuitive effective practice base hypothesis document exactly contain query term highly relevant regardless query semantics inversely term base approach assume document contain query term irrelevant however know high match degree term level necessarily mean high relevance vice versa document match null query term may still relevant consequently exist vocabulary gap query document occur use different word describe concepts alleviation effect bring forward vocabulary gap topic dissertation specifically propose one methods formulate effective query complex textual structure two latent vector space model circumvent vocabulary gap information retrieval
investigate problem language base image edit lbie give source image natural language description want generate target image edit source image base description propose generic model framework two sub task lbie language base image segmentation image colorization framework use recurrent attentive model fuse image language feature instead use fix step size introduce region image termination gate dynamically determine inference step whether continue extrapolate additional information textual description effectiveness framework validate three datasets first introduce synthetic dataset call cosal evaluate end end performance lbie system second show framework lead state art performance image segmentation referit dataset third present first language base colorization result oxford one hundred and two flower dataset
hallmark human intelligence ability ask rich creative reveal question introduce cognitive model capable construct human like question approach treat question formal program execute state world output answer model specify probability distribution complex compositional space program favor concise program help agent learn current context evaluate approach model type open end question generate humans attempt learn ambiguous situation game find model predict question people ask creatively produce novel question present train set addition compare number model variants find question informativeness complexity important produce human like question
recently visual question answer vqa task gain increase attention artificial intelligence exist vqa methods mainly adopt visual attention mechanism associate input question correspond image regions effective question answer free form region base detection base visual attention mechanisms mostly investigate former ones attend free form image regions latter ones attend pre specify detection box regions argue two attention mechanisms able provide complementary information effectively integrate better solve vqa problem paper propose novel deep neural network vqa integrate attention mechanisms propose framework effectively fuse feature free form image regions detection box question representations via multi modal multiplicative feature embed scheme jointly attend question relate free form image regions detection box accurate question answer propose method extensively evaluate two publicly available datasets coco qa vqa outperform state art approach source code available https githubcom lupantech dual mfa vqa
spatial understand fundamental problem wide reach real world applications representation spatial knowledge often model spatial templates ie regions acceptability two object explicit spatial relationship eg etc contrast prior work restrict spatial templates explicit spatial prepositions eg glass table extend concept implicit spatial language ie relationships generally action spatial arrangement object implicitly imply eg man rid horse contrast explicit relationships predict spatial arrangements implicit spatial language require significant common sense spatial understand introduce task predict spatial templates two object relationship see spatial question answer task 2d continuous output man wrt horse man walk horse present two simple neural base model leverage annotate image structure text learn task good performance model reveal spatial locations large extent predictable implicit spatial language crucially model attain similar performance challenge generalize set object relation object combinations egman walk dog never see next go one step present model unseen object eg dog scenario show leverage word embeddings enable model output accurate spatial predictions prove model acquire solid common sense spatial knowledge allow generalization
explore ideas infectious disease genetics use uncover pattern cultural inheritance innovation corpus five hundred and ninety-one national constitutions span one thousand, seven hundred and eighty-nine two thousand and eight legal ideas encode topics word statistically link document derive topic model corpus constitutions use topics derive diffusion network borrow ancestral constitutions back us constitution one thousand, seven hundred and eighty-nine reveal constitutions complex cultural recombinants find systematic variation pattern borrow ancestral texts biological like behavior pattern inheritance distribution offspring arise bound preferential attachment process process lead small number highly innovative influential constitutions yet identify current literature find thus would new light critical nod constitution make network constitutional network structure reflect periods intense constitution creation systematic pattern variation constitutional life span temporal influence
spectral topic model algorithms operate matrices tensors word co occurrence statistics learn topic specific word distributions approach remove dependence original document produce substantial gain efficiency provable topic inference cost model longer provide information topic composition individual document recently thresholded linear inverse tli propose map observe word document back topic composition however linear characteristics limit inference quality without consider important prior information topics paper evaluate simple probabilistic inverse spi method novel prior aware dual decomposition padd capable learn document specific topic compositions parallel experiment show padd successfully leverage topic correlations prior notably outperform tli learn quality topic compositions comparable gibbs sample various data
work explore build automatic speech recognition model transcribe doctor patient conversation collect large scale dataset clinical conversations fourteen thousand hr design task represent real word scenario explore several alignment approach iteratively improve data quality explore ctc las systems build speech recognition model las resilient noisy data ctc require data clean detail analysis provide understand performance clinical task analysis show speech recognition model perform well important medical utterances errors occur causal conversations overall believe result model provide reasonable quality practice
robot carry natural language instruction dream since jetsons cartoon series imagine life leisure mediate fleet attentive robot helpers dream remain stubbornly distant however recent advance vision language methods make incredible progress closely relate areas significant robot interpret natural language navigation instruction basis see carry vision language process similar visual question answer task interpret visually ground sequence sequence translation problems many methods applicable enable encourage application vision language methods problem interpret visually ground navigation instructions present matterport3d simulator large scale reinforcement learn environment base real imagery use simulator future support range embody vision language task provide first benchmark dataset visually ground natural language navigation real build room room r2r dataset
visual dialogue task require agent engage conversation image human represent extension visual question answer task agent need answer question image need light previous dialogue take place key challenge visual dialogue thus maintain consistent natural dialogue continue answer question correctly present novel approach combine reinforcement learn generative adversarial network gans generate human like responses question gin help overcome relative paucity train data tendency typical mle base approach generate overly terse answer critically gin tightly integrate attention mechanism generate human interpretable reason answer mean discriminative model gin task assess whether candidate answer generate human give provide reason significant drive generative model produce high quality answer well support associate reason method also generate state art result primary benchmark
despite significant progress variety vision language problems develop method capable ask intelligent goal orient question image prove inscrutable challenge towards end propose deep reinforcement learn framework base three new intermediate reward namely goal achieve progressive informativeness encourage generation succinct question turn uncover valuable information towards overall goal directly optimize question work quickly towards fulfil overall goal avoid tendency exist methods generate long series insane query add little value evaluate model guesswhat dataset show result question help standard guesser identify specific object image much higher success rate
computer poetry generation first step towards computer write write must theme current approach use sequence sequence model attention often produce non thematic poems present novel conditional variational autoencoder hybrid decoder add deconvolutional neural network general recurrent neural network fully learn topic information via latent variables approach significantly improve relevance generate poems represent line poem context sensitive manner also holistic way highly relate give keyword learn topic propose augment word2vec model improve rhythm symmetry test show generate poems approach mostly satisfy regulate rule consistent theme seven thousand, three hundred and forty-two receive overall score less three highest score five
temporal gate play significant role modern recurrent base neural encoders enable fine grain control recursive compositional operations time recurrent model long short term memory lstm temporal gate control amount information retain discard time play important role influence learn representations also serve protection vanish gradients paper explore idea learn temporal gate sequence pair question answer jointly influence learn representations pairwise manner approach temporal gate learn via 1d convolutional layer subsequently cross apply across question answer joint learn empirically show conceptually simple share temporal gate lead competitive performance across multiple benchmarks intuitively network achieve interpret learn representations question answer pair aware remember forget ie pairwise temporal gate via extensive experiment show propose model achieve state art performance two community base qa datasets competitive performance one factoid base qa dataset
sentiment analysis attract attentions become hot research topic due potential applications personalize recommendation opinion mine etc exist methods base either textual visual data achieve satisfactory result hard extract sufficient information one single modality data inspire observation exist strong semantic correlation visual textual data social medias propose end end deep fusion convolutional neural network jointly learn textual visual sentiment representations train examples two modality information fuse together pool layer feed fully connect layer predict sentiment polarity evaluate propose approach two widely use data set result show method achieve promise result compare state art methods clearly demonstrate competency
unsupervised domain adaptation speech signal aim adapt well train source domain acoustic model unlabeled data target domain achieve adversarial train deep neural network dnn acoustic model learn intermediate deep representation senone discriminative domain invariant specifically dnn train jointly optimize primary task senone classification secondary task domain classification adversarial objective function work instead focus learn domain invariant feature ie share component domains also characterize difference source target domain distributions explicitly model private component domain private component extractor dnn private component train orthogonal share component thus implicitly increase degree domain invariance share component reconstructor dnn use reconstruct original speech feature private share components regularization domain separation framework apply unsupervised environment adaptation task achieve one thousand, one hundred and eight relative wer reduction gradient reversal layer train representative adversarial train method automatic speech recognition chime three dataset
far field speech recognition noisy reverberant condition remain challenge problem despite recent deep learn breakthroughs problem commonly address acquire speech signal multiple microphones perform beamforming paper propose use recurrent neural network long short term memory lstm architecture adaptively estimate real time beamforming filter coefficients cope non stationary environmental noise dynamic nature source microphones position result set timevarying room impulse responses lstm adaptive beamformer jointly train deep lstm acoustic model predict senone label use hide units deep lstm acoustic model assist predict beamforming filter coefficients propose system achieve seven hundred and ninety-seven absolute gain baseline systems beamforming chime three real evaluation set
propose use cascade classifiers keyword spot kws task narrow band nb 8khz audio acquire non iid environments challenge task state art kws systems face present model incorporate deep neural network dnns cascade multiple feature representations multiple instance learn cascade classifiers handle task class imbalance reduce power consumption computationally constrain devices via early termination kws system achieve false negative rate six hourly false positive rate seventy-five
counterfactual learn natural scenario improve web base machine translation service offline learn feedback log user interactions order avoid risk show inferior translations users scenarios mostly exploration free deterministic log policies place analyze possible degeneracies inverse reweighted propensity score estimators stochastic deterministic settings relate recently propose techniques counterfactual learn deterministic log
paper propose continuous semantic topic embed model cstem find latent topic variables document use continuous semantic distance function topics word mean variational autoencodervae semantic distance could represent symmetric bell shape geometric distance function euclidean space mahalanobis distance use paper order semantic distance perform properly newly introduce additional model parameter word take global factor distance indicate likely occur regardless topic certainly improve problem gaussian distribution use previous topic model continuous word embed could explain semantic relation correctly help obtain higher topic coherence experiment dataset twenty newsgroup nip paper cnn dailymail corpus performance recent state art model accomplish model well generate topic embed vectors make possible observe topic vectors embed word vectors real euclidean space topics relate semantically
text attribute transfer use non parallel data require methods perform disentanglement content linguistic attribute work propose multiple improvements exist approach enable encoder decoder framework cope text attribute transfer non parallel data perform experiment sentiment transfer task use two datasets datasets propose method outperform strong baseline two three employ evaluation metrics
deep learn methods recently achieve great empirical success machine translation dialogue response generation summarization text generation task high level technique train end end neural network model consist encoder model produce hide representation source text follow decoder model generate target model significantly fewer piece earlier systems significant tune still require achieve good performance text generation model particular decoder behave undesired ways generate truncate repetitive output output bland generic responses case produce ungrammatical gibberish paper intend practical guide resolve undesired behavior text generation model aim help enable real world applications
consider task fine grain sentiment analysis perspective multiple instance learn mil neural model train document sentiment label learn predict sentiment text segment ie sentence elementary discourse units edus without segment level supervision introduce attention base polarity score method identify positive negative text snippets new dataset call spot shorthand segment level polarity annotations evaluate mil style sentiment model like experimental result demonstrate superior performance multiple baselines whereas judgement elicitation study show edu level opinion extraction produce informative summaries sentence base alternatives
address problem bootstrapping language acquisition artificial system similarly observe experiment human infants method work associate mean word manipulation task robot interact object listen verbal descriptions interactions model base affordance network ie map robot action robot perceptions perceive effect action upon object extend affordance model incorporate speak word allow us grind verbal symbols execution action perception environment model take verbal descriptions task input use temporal co occurrence create link speech utterances involve object action effect show robot able form useful word mean associations even without consider grammatical structure learn process presence recognition errors word mean associations embed robot understand action thus directly use instruct robot perform task also allow incorporate context speech recognition task believe encourage result approach may afford robots capacity acquire language descriptors operation environment well would light challenge process develop human infants
learn distribute representations document push state art several natural language process task successfully apply field recommender systems recently paper propose novel content base recommender system base learn representations generative model user interest method work follow first learn representations corpus text document capture user interest generative model space document representations particular model distribution interest user gaussian mixture model gmm recommendations obtain directly sample user generative model use latent semantic analysis lsa comparison compute explore document representations delicious bookmarks dataset standard benchmark recommender systems perform density estimation space show learn representations outperform lsa term predictive performance
recent systems structure prediction focus increase level structural dependencies within model however study suggest complex structure entail high overfitting risk control structure base overfitting propose conduct structure regularization decode sr decode decode complex structure model regularize additionally train simple structure model theoretically analyze quantitative relations structural complexity overfitting risk analysis show complex structure model prone structure base overfitting empirical evaluations show propose method improve performance complex structure model reduce structure base overfitting sequence label task propose method substantially improve performance complex neural network model maximum f1 error rate reduction three hundred and sixty-four third order model propose method also work parse task maximum uas improvement fifty-five tri sibling model result competitive better state art result
dialogue assistants rapidly become indispensable daily aid avoid significant effort need hand craft require dialogue flow dialogue management dm module cast continuous markov decision process mdp train reinforcement learn rl several rl model investigate recent years however lack common benchmarking framework make difficult perform fair comparison different model capability generalise different environments therefore paper propose set challenge simulate environments dialogue model development evaluation provide baselines investigate number representative parametric algorithms namely deep reinforcement learn algorithms dqn a2c natural actor critic compare non parametric model gp sarsa environments policy model implement use publicly available pydial toolkit release line order establish testbed framework experiment facilitate experimental reproducibility
introduce method embed word probability densities low dimensional space rather assume word embed fix across entire text collection standard word embed methods bayesian model generate word specific prior density occurrence give word intuitively word prior density encode distribution potential mean prior densities conceptually similar gaussian embeddings interestingly unlike gaussian embeddings also obtain context specific densities encode uncertainty sense word give context correspond posterior distributions within model context dependent densities many potential applications example show directly use lexical substitution task describe effective estimation method base variational autoencoding framework also demonstrate embeddings achieve competitive result standard benchmarks
video caption task automatically generate textual description action video although previous work eg sequence sequence model show promise result abstract coarse description short video still challenge caption video contain multiple fine grain action detail description paper aim address challenge propose novel hierarchical reinforcement learn framework video caption high level manager module learn design sub goals low level worker module recognize primitive action fulfill sub goal compositional framework reinforce video caption different level approach significantly outperform baseline methods newly introduce large scale dataset fine grain video caption furthermore non ensemble model already achieve state art result widely use msr vtt dataset
paper propose method train neural network large set data weak label small amount data true label propose model train two neural network target network learner confidence network meta learner target network optimize perform give task train use large set unlabeled data weakly annotate propose control magnitude gradient update target network use score provide second confidence network train small amount supervise data thus avoid weight update compute noisy label harm quality target network model
statistical dialogue management dialogue manager learn policy map belief state action system perform efficient exploration key successful policy optimisation current deep reinforcement learn methods promise rely epsilon greedy exploration thus subject user random choice action learn alternative approach gaussian process sarsa gpsarsa estimate uncertainties sample efficient lead better user experience expense greater computational complexity paper examine approach extract uncertainty estimate deep q network dqn context dialogue management perform extensive benchmark deep bayesian methods extract uncertainty estimate namely bay backprop dropout concrete variation bootstrapped ensemble alpha divergences combine dqn algorithm
determine semantic similarity academic document crucial many task plagiarism detection automatic technical survey semantic search current study mostly focus semantic similarity concepts sentence short text fragment however document level semantic match still base statistical information surface level neglect article structure global semantic mean may deviation document understand paper focus document level semantic similarity issue academic literatures novel method represent academic article topic events utilize multiple information profile research purpose methodologies domains integrally describe research work calculate similarity topic events base domain ontology acquire semantic similarity article experiment show approach achieve significant performance compare state art methods
present new ai task embody question answer embodiedqa agent spawn random location 3d environment ask question color car order answer agent must first intelligently navigate explore environment gather information first person egocentric vision answer question orange challenge task require range ai skills active perception language understand goal drive navigation commonsense reason ground language action work develop environments end end train reinforcement learn agents evaluation protocols embodiedqa
many entity link systems use collective graph base methods disambiguate entity mention within document focus graph construction initial weight candidate entities less attention devote compare graph rank algorithms work focus graph base rank algorithms therefore propose apply five centrality measure degree hit pagerank betweenness closeness disambiguation graph candidate entities construct document use popularity method centrality measure apply choose relevant candidate boost result entity popularity method investigate effectiveness centrality measure performance across different domains datasets experiment show simple fast centrality measure degree centrality outperform time consume measure
number study find today visual question answer vqa model heavily drive superficial correlations train data lack sufficient image ground encourage development model gear towards latter propose new set vqa every question type train test set different prior distributions answer specifically present new split vqa v1 vqa v2 datasets call visual question answer change priors vqa cp v1 vqa cp v2 respectively first evaluate several exist vqa model new set show performance degrade significantly compare original vqa set second propose novel ground visual question answer model gvqa contain inductive bias restrictions architecture specifically design prevent model cheat primarily rely priors train data specifically gvqa explicitly disentangle recognition visual concepts present image identification plausible answer space give question enable model robustly generalize across different distributions answer gvqa build exist vqa model stack attention network san experiment demonstrate gvqa significantly outperform san vqa cp v1 vqa cp v2 datasets interestingly also outperform powerful vqa model multimodal compact bilinear pool mcb several case gvqa offer strengths complementary san train evaluate original vqa v1 vqa v2 datasets finally gvqa transparent interpretable exist vqa model
introduce interactive learn framework development test intelligent visual systems call learn ask lba explore lba context visual question answer vqa task lba differ standard vqa train question observe train time learner must ask question want answer thus lba closely mimic natural learn potential data efficient traditional vqa set present model perform lba clevr dataset show automatically discover easy hard curriculum learn interactively oracle lba generate data consistently match outperform clevr train data sample efficient also show model ask question generalize state art vqa model novel test time distributions
derive event storylines effective summarization method succinctly organize extensive information significantly alleviate pain information overload critical challenge lack widely recognize definition storyline metric prior study develop various approach base different assumptions users interest work extract interest pattern assumptions guarantee derive pattern match users preference hand exclusiveness single modality source miss cross modality information paper propose method multimodal imitation learn via generative adversarial networksmil gin directly model users interest reflect various data particular propose model address critical challenge imitate users demonstrate storylines propose model design learn reward pattern give user provide storylines apply learn policy unseen data propose approach demonstrate capable acquire user implicit intent outperform compete methods substantial margin user study
hashtag recommendation problem address recommend suggest one hashtags explicitly tag post make give social network platform base upon content context post work propose novel methodology hashtag recommendation microblog post specifically twitter methodology emtagger build upon train test framework build top concept word embed train phase comprise learn word vectors associate hashtag derive word embed hashtag provide two train procedures one hashtag train separate word embed model applicable context hashtag another hashtag obtain embed global context test phase constitute compute average word embed test post find similarity embed know embeddings hashtags tweet contain similar hashtag extract hashtags appear tweet rank term embed similarity score top k hashtags appear rank list recommend give test post system produce f1 score five thousand and eighty-three improve lda baseline around six hundred and fifty-three time outperform best perform system know literature provide lift six hundred and forty-two time emtagger fast scalable lightweight system make practical deploy real life applications
attention base encoder decoder architectures listen attend spell las subsume acoustic pronunciation language model components traditional automatic speech recognition asr system single neural network previous work show architectures comparable state theart asr systems dictation task clear architectures would practical challenge task voice search work explore variety structural optimization improvements las model significantly improve performance structural side show word piece model use instead graphemes also introduce multi head attention architecture offer improvements commonly use single head attention optimization side explore synchronous train schedule sample label smooth minimum word error rate optimization show improve accuracy present result unidirectional lstm encoder stream recognition twelve five hundred hour voice search task find propose change improve wer ninety-two fifty-six best conventional system achieve sixty-seven dictation task model achieve wer forty-one compare five conventional system
sequence sequence model operate online fashion important stream applications voice search neural transducer stream sequence sequence model show significant degradation performance compare non stream model listen attend spell las paper present various improvements nt specifically look increase window nt compute attention mainly look backwards time model still remain online addition explore initialize nt model las train model guide better alignment finally explore include stronger language model use wordpiece model apply external lm beam search voice search task find improvements get nt match performance las
sequence sequence model attention base model automatic speech recognition asr typically train optimize cross entropy criterion correspond improve log likelihood data however system performance usually measure term word error rate wer log likelihood traditional asr systems benefit discriminative sequence train optimize criteria state level minimum bay risk smbr closely relate wer present work explore techniques train attention base model directly minimize expect word error rate consider two loss function approximate expect number word errors either sample model use n best list decode hypotheses find effective sample base method experimental evaluations find propose train procedure improve performance eighty-two relative baseline system allow us train grapheme base uni directional attention base model match performance traditional state art discriminative sequence train system mobile voice search task
decades context dependent phonemes dominant sub word unit conventional acoustic model systems status quo begin challenge recently end end model seek combine acoustic pronunciation language model components single neural network systems typically predict graphemes word simplify recognition process since remove need separate expert curated pronunciation lexicon map phoneme base units word however little previous work compare phoneme base versus grapheme base sub word units end end model framework determine whether gain approach primarily due new probabilistic model joint learn various components grapheme base units work conduct detail experiment aim quantify value phoneme base pronunciation lexica context end end model examine phoneme base end end model contrast grapheme base ones large vocabulary english voice search task find graphemes indeed outperform phonemes also compare grapheme phoneme base approach multi dialect english task confirm superiority graphemes greatly simplify system recognize multiple dialects
attention base sequence sequence model automatic speech recognition jointly train acoustic model language model alignment mechanism thus language model component train transcribe audio text pair lead use shallow fusion external language model inference time shallow fusion refer log linear interpolation separately train language model step beam search work investigate behavior shallow fusion across range condition different type language model different decode units different task google voice search demonstrate use shallow fusion neural lm wordpieces yield ninety-one relative word error rate reduction werr competitive attention base sequence sequence model obviate need second pass rescoring
chemical databases store information text representations smile format universal standard use many cheminformatics software encode smile string structural information use predict complex chemical properties work develop smiles2vec deep rnn automatically learn feature smile predict chemical properties without need additional explicit feature engineer use bayesian optimization methods tune network architecture show optimize smiles2vec model serve general purpose neural network predict distinct chemical properties include toxicity activity solubility solvation energy also outperform contemporary mlp neural network use engineer feature furthermore demonstrate proof concept interpretability develop explanation mask localize important character use make prediction test solubility dataset identify specific part chemical consistent establish first principles knowledge accuracy eighty-eight work demonstrate neural network learn technically accurate chemical concept provide state art accuracy make interpretable deep neural network useful tool relevance chemical industry
paper concern paraphrase detection ability detect similar sentence write natural language crucial several applications text mine text summarization plagiarism detection authorship authentication question answer give two sentence objective detect whether semantically identical important insight work exist paraphrase systems perform well apply clean texts necessarily deliver good performance noisy texts challenge paraphrase detection user generate short texts twitter include language irregularity noise cope challenge propose novel deep neural network base approach rely coarse grain sentence model use convolutional neural network long short term memory model combine specific fine grain word level similarity match model experimental result show propose approach outperform exist state art approach user generate noisy social media data twitter texts achieve highly competitive performance cleaner corpus
learn goal orient dialog policy generally perform offline supervise learn algorithms online reinforcement learn rl additionally company accumulate massive quantities dialog transcripts customers train human agents encoder decoder methods gain popularity agent utterances directly treat supervision without need utterance level annotations however one potential drawback approach myopically generate next agent utterance without regard dialog level considerations resolve concern paper describe offline rl method learn unannotated corpora optimize goal orient policy utterance dialog level introduce novel reward function use policy policy policy gradient learn policy offline without require online user interaction explicit state space definition
paper describe study indicator mine problem online sex advertise domain present development system flagit flexible adaptive generation indicators text combine benefit lightweight expert system classical semi supervision heuristic label recently release state art unsupervised text embeddings tag millions sentence indicators highly correlate human traffic flagit technology stack open source preliminary evaluations involve five indicators flagit illustrate promise performance compare several alternatives system actively develop refine integrate domain specific search system use two hundred law enforcement agencies combat human traffic aggressively extend mine least six indicators minimal program effort flagit good example system operate limit label settings require creative combinations establish machine learn techniques produce output could use real world non technical analysts
direct acoustics word a2w model end end paradigm receive increase attention compare conventional sub word base automatic speech recognition model use phone character context dependent hide markov model state a2w model recognize word speech without decoder pronunciation lexicon externally train language model make train decode model simple prior work show a2w model require order magnitude train data order perform comparably conventional model work also show accuracy gap use english switchboard fisher data set paper describe recipe train a2w model close gap par state art sub word base model achieve word error rate eighty-eight one hundred and thirty-nine hub5 two thousand switchboard callhome test set without decoder language model find model initialization train data order regularization impact a2w model performance next present joint word character a2w model learn first spell word recognize model provide rich output user instead simple word hypotheses make especially useful case word unseen rarely see train
february two thousand and sixteen facebook allow users express experience emotions post use five call reactions research paper propose evaluate alternative methods predict reactions user post public page firm company like supermarket chain purpose collect post reactions facebook page large supermarket chain construct dataset available research order predict distribution reactions new post neural network architectures convolutional recurrent neural network test use pretrained word embeddings result neural network improve introduce bootstrapping approach sentiment emotion mine comment post final model combination neural network baseline emotion miner able predict reaction distribution facebook post mean square error misclassification rate one hundred and thirty-five
paper describe method nonlinear wavelet thresholding time series ramachandran ranganathan run test use assess quality approximation minimize objective function propose use genetic algorithms one stochastic optimization methods suggest method test model series word frequency series use google book ngram data show method filter use run criterion show significantly better result compare standard wavelet thresholding method use quality filter primary importance speed calculations
paper explore unsupervised learn semantic embed space co occur sensory input specifically focus task learn semantic vector space speak handwritten digits use tidigits mnist datasets current techniques encode image audio textual input directly semantic embeddings contrast technique map input mean log variance vectors diagonal gaussian sample semantic embeddings draw addition encourage semantic similarity co occur inputsour loss function include regularization term borrow variational autoencoders vaes drive posterior distributions embeddings unit gaussian use regularization term filter modality information preserve semantic information speculate technique may broadly applicable areas cross modality domain information retrieval transfer learn
paper approach task handwritten text recognition htr attentional encoder decoder network train sequence character rather word experiment line text popular handwrite datasets compare different activation function attention mechanism use align image pixels target character find softmax attention focus heavily individual character sigmoid attention focus multiple character step decode sequence alignment one one softmax attention able learn precise alignment step decode whereas alignment generate sigmoid attention much less precise linear function use obtain attention weight model predict character look entire sequence character perform poorly lack precise alignment source target future research may explore htr natural scene image since model capable transcribe handwritten text without need produce segmentations bound box text image
recent deep learn dl model move beyond static network architectures dynamic ones handle data network structure change every example sequence variable lengths tree graph exist dataflow base program model dl static dynamic declaration either readily express dynamic model inefficient due repeat dataflow graph construction process difficulties batch execution present cavs vertex centric program interface optimize system implementation dynamic dl model cavs represent dynamic network structure static vertex function mathcalf dynamic instance specific graph mathcalg perform backpropagation schedule execution mathcalf follow dependencies mathcalg cavs bypass expensive graph construction preprocessing overhead allow use static graph optimization techniques pre define operations mathcalf naturally expose batch execution opportunities different graph experiment compare cavs two state art frameworks dynamic nns tensorflow fold dynet demonstrate efficacy approach cavs achieve near one order magnitude speedup train various dynamic nn architectures ablations demonstrate contribution propose batch memory management strategies
propose novel document generation process base hierarchical latent tree model hltms learn data hltm layer observe word variables bottom multiple layer latent variables top document first sample value latent variables layer layer via logic sample draw relative frequencies word condition value latent variables finally generate word document use relative word frequencies motivation work take word count consideration hltms comparison lda base hierarchical document generation process new process achieve drastically better model fit much fewer parameters also yield meaningful topics topic hierarchies new state art hierarchical topic detection
paper discuss formalize approach generate estimate symbols alphabets communicate wide range non verbal mean base specific user requirements medium priorities type information need convey short characterization basic term parameters symbols alphabets approach generate give framework experimental setup machine learn methods estimate usefulness effectiveness nonverbal alphabets systems present previous result demonstrate usage multimodal data source like wearable accelerometer heart monitor muscle movements sensors braincomputer interface along machine learn approach provide deeper understand usefulness effectiveness alphabets systems nonverbal situate communication symbols alphabets generate estimate methods may useful various applications synthetic languages construct script multimodal nonverbal situate interaction people artificial intelligence systems human computer interfaces mouse gesture touchpads body gesture eyetracking cameras wearables brain compute interfaces especially applications elderly care people disabilities
investigate effect usefulness spontaneity ie whether give speech spontaneous speech context emotion recognition hypothesize emotional content speech interrelate spontaneity use spontaneity classification auxiliary task problem emotion recognition propose two supervise learn settings utilize spontaneity improve speech emotion recognition hierarchical model perform spontaneity detection perform emotion recognition multitask learn model jointly learn recognize spontaneity emotion various experiment well know iemocap database show use spontaneity detection additional task significant improvement achieve emotion recognition systems unaware spontaneity achieve state art emotion recognition accuracy four class six hundred and ninety-one iemocap database outperform several relevant competitive baselines
introduce pair tool rasa nlu rasa core open source python libraries build conversational software purpose make machine learn base dialogue management language understand accessible non specialist software developers term design philosophy aim ease use bootstrapping minimal initial train data package extensively document ship comprehensive suite test code available https githubcom rasahq
advent internet large amount digital text generate everyday form news article research publications blog question answer forums social media important develop techniques extract information automatically document lot important information hide within extract information use improve access management knowledge hide large text corpora several applications question answer information retrieval would benefit information entities like persons organizations form basic unit information occurrences entities sentence often link well define relations eg occurrences person organization sentence may link relations employ task relation extraction identify relations automatically paper survey several important supervise semi supervise unsupervised techniques also cover paradigms open information extraction oie distant supervision finally describe recent trend techniques possible future research directions survey would useful three kinds readers newcomers field want quickly learn ii researchers want know various techniques evolve time possible future research directions iii practitioners need know technique work best various settings
aspect base sentiment analysis absa try predict polarity give document respect give aspect entity neural network architectures successful predict overall polarity sentence aspect specific sentiment analysis still remain open problem paper propose novel method integrate aspect information neural model specifically incorporate aspect information neural model model word aspect relationships novel model textitaspect fusion lstm af lstm learn attend base associative relationships sentence word aspect allow model adaptively focus correct word give aspect term ameliorate flaw state art model utilize naive concatenations model word aspect similarity instead model adopt circular convolution circular correlation model similarity aspect word elegantly incorporate within differentiable neural attention framework finally model end end differentiable highly relate convolution correlation holographic like memories propose neural model achieve state art performance benchmark datasets outperform atae lstm four five average across multiple datasets
work propose goal drive collaborative task combine language perception action specifically develop collaborative image draw game two agents call codraw game ground virtual world contain movable clip art object game involve two players teller drawer teller see abstract scene contain multiple clip art piece semantically meaningful configuration drawer try reconstruct scene empty canvas use available clip art piece two players communicate use natural language collect codraw dataset 10k dialogs consist 138k message exchange human players define protocols metrics evaluate learn agents testbed highlight need novel crosstalk evaluation condition pair agents train independently disjoint subsets train data present model task benchmark use fully automate evaluation play game live humans
deep learn base discriminative methods state art machine learn techniques ill suit learn lower amount data paper propose novel framework call simultaneous two sample learn s2sl effectively learn class discriminative characteristics even low amount data s2sl one sample two sample simultaneously consider train test classifier demonstrate approach speech music discrimination emotion classification experiment also show effectiveness s2sl approach classification low resource scenario imbalanced data
describe sockeye version one hundred and twelve open source sequence sequence toolkit neural machine translation nmt sockeye production ready framework train apply model well experimental platform researchers write python build mxnet toolkit offer scalable train inference three prominent encoder decoder architectures attentional recurrent neural network self attentional transformers fully convolutional network sockeye also support wide range optimizers normalization regularization techniques inference improvements current nmt literature users easily run standard train recipes explore different model settings incorporate new ideas paper highlight sockeye feature benchmark nmt toolkits two language arc two thousand and seventeen conference machine translation wmt english german latvian english report competitive bleu score across three architectures include overall best score sockeye transformer implementation facilitate comparison release system output train script use experiment sockeye toolkit free software release apache twenty license
work present find experiment stock market prediction use various textual sentiment analysis tool mood analysis event extraction well prediction model lstms specific convolutional architectures
article present preliminary approach towards characterize political fake news twitter analysis meta data particular focus 15m tweet collect day election donald trump 45th president unite state america use meta data embed within tweet order look differences tweet contain fake news tweet contain specifically perform analysis tweet go viral study proxies users exposure tweet characterize account spread fake news look polarization find significant differences distribution followers number urls tweet verification users
deep learn emerge technology consider one promise directions reach higher level artificial intelligence among achievements build computers understand speech represent crucial leap towards intelligent machine despite great efforts past decades however natural robust human machine speech interaction still appear reach especially users interact distant microphone noisy reverberant environments latter disturbances severely hamper intelligibility speech signal make distant speech recognition dsr one major open challenge field thesis address latter scenario propose novel techniques architectures algorithms improve robustness distant talk acoustic model first elaborate methodologies realistic data contamination particular emphasis dnn train simulate data investigate approach better exploit speech contexts propose original methodologies fee forward recurrent neural network lastly inspire idea cooperation across different dnns could key counteract harmful effect noise reverberation propose novel deep learn paradigm call network deep neural network analysis original concepts base extensive experimental validations conduct real simulate data consider different corpora microphone configurations environments noisy condition asr task
generate novel pair image text problem combine computer vision natural language process paper present strategies generate novel image caption pair base exist caption datasets model take advantage recent advance generative adversarial network sequence sequence model make generalizations generate pair sample multiple domains furthermore study cycle generate image text back image vise versa well connection autoencoders
scientific publications evolve several feature mitigate vocabulary mismatch index retrieve compute similarity article mitigation strategies range simply focus high value article section title abstract assign keywords often control vocabularies either manually automatic annotation various document representation scheme possess different cost benefit tradeoffs paper propose model different representations article translations generate common latent representation multilingual topic model start methodological overview latent variable model parallel document representations could use across many information science task show solve inference problem map diverse representations share topic space allow us evaluate representations base topically similar original article addition propose approach provide mean discover different concept vocabularies require improvement
gram kernels flexible efficient way employ bag n gram feature learn textual data also compatible use word embeddings word similarities account original gram kernels implement top tree kernels propose new approach independent tree kernels efficient also propose effective way make use word embeddings original gram formulation apply task sentiment classification new formulation achieve significantly better performance
read comprehension rc contrast information retrieval require integrate information reason events entities relations across full document question answer conventionally use assess rc ability artificial agents children learn read however exist rc datasets task dominate question solve select answer use superficial information eg local context similarity global term frequency thus fail test essential integrative aspect rc encourage progress deeper comprehension language present new dataset set task reader must answer question stories read entire book movie script task design successfully answer question require understand underlie narrative rather rely shallow pattern match salience show although humans solve task easily standard rc model struggle task present provide analysis dataset challenge present
connectionist temporal classification ctc widely use maximum likelihood learn end end speech recognition model however usually disparity negative maximum likelihood performance metric use speech recognition eg word error rate wer result mismatch objective function metric train show problem mitigate jointly train maximum likelihood policy gradient particular policy learn able directly optimize otherwise non differentiable performance metric show joint train improve relative performance four thirteen end end model compare model learn maximum likelihood model achieve five hundred and fifty-three wer wall street journal dataset five hundred and forty-two one thousand, four hundred and seventy librispeech test clean test set respectively
regularization important end end speech model since model highly flexible easy overfit data augmentation dropout important improve end end model domains however relatively explore end end speech model therefore investigate effectiveness methods end end trainable deep speech recognition model augment audio data random perturbations tempo pitch volume temporal alignment add random noisewe investigate effect dropout apply input layer network show combination data augmentation dropout give relative performance improvement wall street journal wsj librispeech dataset twenty model performance also competitive end end speech model datasets
propose cognitive databases approach transparently enable artificial intelligence ai capabilities relational databases novel aspect design first view structure data source meaningful unstructured text use text build unsupervised neural network model use natural language process nlp technique call word embed model capture hide inter intra column relationships database tokens different type database token model include vector encode contextual semantic relationships seamlessly integrate word embed model exist sql query infrastructure use enable new class sql base analytics query call cognitive intelligence ci query ci query use model vectors enable complex query semantic match inductive reason query analogies predictive query use entities present database generally use knowledge external source demonstrate unique capabilities cognitive databases use apache spark base prototype execute inductive reason ci query multi modal database contain text image believe first kind system exemplify use ai functionality endow relational databases capabilities previously hard realize practice
process design neural architectures require expert knowledge extensive trial error automate architecture search may simplify requirements recurrent neural network rnn architectures generate exist methods limit flexibility components propose domain specific language dsl use automate architecture search produce novel rnns arbitrary depth width dsl flexible enough define standard architectures gate recurrent unit long short term memory allow introduction non standard rnn components trigonometric curve layer normalization use two different candidate generation techniques random search rank function reinforcement learn explore novel architectures produce rnn dsl language model machine translation domains result architectures follow human intuition yet perform well target task suggest space usable rnn architectures far larger previously assume
one big challenge machine learn applications train data different real world data face algorithm language model users language eg private message could change year completely different observe publicly available data time public data use obtain general knowledge ie general model english study approach distribute fine tune general model user private data additional requirements maintain quality general data minimization communication cost propose novel technique significantly improve prediction quality users language compare general model outperform gradient compression methods term communication efficiency propose procedure fast lead almost seventy perplexity reduction eighty-seven percentage point improvement keystroke save rate informal english texts also show range task approach applicable limit language model finally propose experimental framework evaluate differential privacy distribute train language model show approach good privacy guarantee
internet facilitate large scale collaborative project emergence web twenty platforms producers consumers content unify drastically change information market one hand promise wisdom crowd inspire successful project wikipedia become primary source crowd base information many languages hand decentralize often un monitor environment project may make susceptible low quality content work focus urban dictionary crowd source online dictionary combine computational methods qualitative annotation would light overall feature urban dictionary term growth coverage type content measure high presence opinion focus entries oppose mean focus entries expect traditional dictionaries furthermore urban dictionary cover many informal unfamiliar word well proper nouns urban dictionary also contain offensive content highly offensive content tend receive lower score dictionary vote system low threshold include new material urban dictionary enable quick record new word new mean result heterogeneous content pose challenge use urban dictionary source study language innovation
question require count variety object image remain major challenge visual question answer vqa common approach vqa involve either classify answer base fix length representations image question sum fractional count estimate section image contrast treat count sequential decision process force model make discrete choices count specifically model sequentially select detect object learn interactions object influence subsequent selections distinction approach intuitive interpretable output discrete count automatically ground image furthermore method outperform state art architecture vqa multiple metrics evaluate count
problem automatic accent identification important several applications like speaker profile recognition well improve speech recognition systems accent nature speech primarily attribute influence speaker native language give speech record paper propose novel accent identification system whose train exploit speech native languages along accent speech specifically develop deep siamese network base model learn association accent speech record native language speech record siamese network train vector feature extract speech record use either unsupervised gaussian mixture model gmm supervise deep neural network dnn model perform several accent identification experiment use cslu foreign accent english fae corpus experiment propose approach use deep siamese network yield significant relative performance improvements one hundred and fifty-four percent ten class accent identification task baseline dnn base classification system use gmm vectors furthermore present detail error analysis propose accent identification system
convolution neural network cnn demonstrate unique advantage audio image text learn recently also challenge recurrent neural network rnns long short term memory cells lstm sequence sequence learn since computations involve cnn easily parallelizable whereas involve rnn mostly sequential lead performance bottleneck however unlike rnn native cnn lack history sensitivity require sequence transformation therefore enhance sequential order awareness position sensitivity become key make cnn general deep learn model work introduce extend cnn model strengthen position sensitivity call posenet notable feature posenet asymmetric treatment position information encoder decoder experiment show posenet allow us improve accuracy cnn base sequence sequence learn significantly achieve around thirty-three thirty-six bleu score wmt two thousand and fourteen english german translation task around forty-four forty-six bleu score english french translation task
current state art many natural language process automate knowledge base completion task hold representation learn methods learn distribute vector representations symbols via gradient base optimization require little hand craft feature thus avoid need preprocessing step task specific assumptions however many case representation learn require large amount annotate train data generalize well unseen data label train data provide human annotators often use formal logic language specify annotations thesis investigate different combinations representation learn methods logic reduce need annotate train data improve generalization
end end neural conversation model lead promise advance reduce hand craft feature errors induce traditional complex system architecture typically require enormous amount data due lack modularity previous study adopt hybrid approach knowledge base components either abstract domain specific information augment data cover diverse pattern contrary propose directly address problem use recent developments space continual learn neural model specifically adopt domain independent neural conversational model introduce novel neural continual learn algorithm allow conversational agent accumulate skills across different task data efficient way best knowledge first work apply continual learn conversation systems verify efficacy method conversational skill transfer either synthetic dialogs human human dialogs human computer conversations customer support domain
ancient chinese texts present area enormous challenge opportunity humanities scholars interest exploit computational methods assist development new insights interpretations culturally significant materials paper describe collaborative effort indiana university xi jiaotong university support exploration interpretation digital corpus eighteen thousand ancient chinese document refer handian ancient classics corpus han diyouan gyouyou j ie han canon chinese classics contain classics ancient chinese philosophy document historical biographical significance literary work begin describe digital humanities context joint project advance humanities compute make project feasible describe corpus introduce application probabilistic topic model corpus attention particular challenge pose model ancient chinese document give specific example software develop use aid discovery interpretation theme corpus outline advance form computer aid interpretation also make possible program interface provide system general implications methods understand nature mean texts
group discussions way individuals exchange ideas arguments order reach better decisions could one premise productive discussions better solutions prevail idea selection process mediate relative competence individuals involve however since people may know actual competence new task behavior influence self estimate competence confidence misalign actual competence goal work understand effect confidence competence misalignment dynamics outcomes discussions end design large scale natural set form online team base geography game allow us disentangle confidence competence thus separate effect find task orient discussions confident individuals larger impact group decisions even individuals level competence teammates furthermore unjustified role confidence decision make process often lead team perform explore phenomenon investigate effect confidence conversational dynamics
paper tutorial formal concept analysis fca applications fca apply branch lattice theory mathematical discipline enable formalisation concepts basic units human think analyse data object attribute form originate early 80s last three decades become popular human centre tool knowledge representation data analysis numerous applications since tutorial specially prepare russir two thousand and fourteen cover fca topics include information retrieval focus visualisation aspects machine learn data mine knowledge discovery text mine several others
online review provide consumers valuable asset e commerce platforms influence potential consumers make purchase decisions however review vary quality useful ones bury deep within heap non informative review work attempt automatically identify review quality term helpfulness end consumers contrast previous work domain exploit variety syntactic community level feature delve deep semantics review make useful provide interpretable explanation identify set consistency semantic factor text rat timestamps user generate review make approach generalizable across communities domains explore review semantics term several latent factor like expertise author judgment fine grain facets underlie product write style cast hide markov model latent dirichlet allocation hmm lda base model jointly infer reviewer expertise ii item facets iii review helpfulness large scale experiment five real world datasets amazon show significant improvement state art baselines predict rank useful review
current recommender systems exploit user item similarities collaborative filter advance methods also consider temporal evolution item rat global background process however prior methods disregard individual evolution user experience level express user write review community paper model joint evolution user experience interest specific item facets write style rat behavior way generate individual recommendations take account user maturity level eg recommend art movies rather blockbusters cinematography expert item rat review texts observables capture user experience interest latent model learn review vocabulary write style develop generative hmm lda model trace user evolution hide markov model hmm trace latent experience progress time solely user review rat observables time facets user interest draw latent dirichlet allocation lda model derive review function latent experience level experiment five real world datasets show model improve rat prediction state art baselines substantial margin also show use case study model perform well assessment user experience level
online health communities valuable source information patients physicians however user generate resources often plague inaccuracies misinformation work propose method automatically establish credibility user generate medical statements trustworthiness author exploit linguistic cue distant supervision expert source end introduce probabilistic graphical model jointly learn user trustworthiness statement credibility language objectivity apply methodology task extract rare unknown side effect medical drug one problems large scale non expert data potential complement expert medical knowledge show method reliably extract side effect filter false statements identify trustworthy users likely contribute valuable medical information
media seem become partisan often provide bias coverage news cater interest specific group therefore essential identify credible information content provide objective narrative event news communities digg reddit newstrust offer recommendations review quality rat insights journalistic work however complex interaction different factor online communities fairness style report language clarity objectivity topical perspectives like political viewpoint expertise bias community members paper present model systematically analyze different interactions news community users news source develop probabilistic graphical model leverage joint interaction identify one highly credible news article two trustworthy news source three expert users perform role citizen journalists community method extend crf model incorporate real value rat communities fine grain scale easily discretized without lose information best knowledge paper first full fledge analysis credibility trust expertise news communities
online review provide viewpoints strengths shortcomings products service influence potential customers purchase decisions however proliferation non credible review either fake promote demote item incompetent involve irrelevant aspects bias entail problem identify credible review prior work involve classifiers harness rich information items users might readily available several domains provide limit interpretability review deem non credible paper present novel approach address issue utilize latent topic model leverage review texts item rat timestamps derive consistency feature without rely item user histories unavailable long tail items users develop model compute review credibility score provide interpretable evidence non credible review also transferable domains address scarcity label data experiment real world datasets demonstrate improvements state art baselines
online review communities dynamic users join leave adopt new vocabulary adapt evolve trend recent work show recommender systems benefit explicit consideration user experience however prior work assume fix number discrete experience level whereas reality users gain experience mature continuously time paper present new model capture continuous evolution user experience result language model review post model unsupervised combine principles geometric brownian motion brownian motion latent dirichlet allocation trace smooth temporal progression user experience language model respectively develop practical algorithms estimate model parameters data inference model eg recommend items extensive experiment five real world datasets show model fit data better discrete model baselines also outperform state art methods predict item rat
transform graphical user interface screenshot create designer computer code typical task conduct developer order build customize software websites mobile applications paper show deep learn methods leverage train model end end automatically generate code single input image seventy-seven accuracy three different platforms ie ios android web base technologies
multi task learn mtl allow deep neural network learn relate task share parameters network practice however mtl involve search enormous space possible parameter share architectures find layer subspaces benefit share b appropriate amount share c appropriate relative weight different task losses recent work address problems isolation work present approach learn latent multi task architecture jointly address c present experiment synthetic data data ontonotes fifty include four different task seven different domains extension consistently outperform previous approach learn latent architectures multi task problems achieve fifteen average error reductions common approach mtl
present milabot deep reinforcement learn chatbot develop montreal institute learn algorithms mila amazon alexa prize competition milabot capable converse humans popular small talk topics speech text system consist ensemble natural language generation retrieval model include template base model bag word model sequence sequence neural network latent variable neural network model apply reinforcement learn crowdsourced data real world user interactions system train select appropriate response model ensemble system evaluate b test real world users perform significantly better many compete systems due machine learn architecture system likely improve additional data
exist neural conversational model process natural language primarily lexico syntactic level thereby ignore one crucial components human human dialogue affective content take step direction propose three novel ways incorporate affective emotional aspects long short term memory lstm encoder decoder neural conversation model one affective word embeddings cognitively engineer two affect base objective function augment standard cross entropy loss three affectively diverse beam search decode experiment show techniques improve open domain conversational prowess encoder decoder network enable produce emotionally rich responses interest natural
present factorize hierarchical variational autoencoder learn disentangle interpretable representations sequential data without supervision specifically exploit multi scale nature information sequential data formulate explicitly within factorize hierarchical graphical model impose sequence dependent priors sequence independent priors different set latent variables model evaluate two speech corpora demonstrate qualitatively ability transform speakers linguistic content manipulate different set latent variables quantitatively ability outperform vector baseline speaker verification reduce word error rate much thirty-five mismatch train test scenarios automatic speech recognition task
social media base digital epidemiology potential support faster response deeper understand public health relate threats study propose new framework analyze unstructured health relate textual data via twitter users post tweet characterize negative health sentiments non health relate concern relations corpus negative sentiments regard diet diabetes exercise obesity ddeo collection six million tweet one month study identify prominent topics users relate negative sentiments propose framework use two text mine methods sentiment analysis topic model discover negative topics negative sentiments twitter users support literature narratives many morbidity issue associate ddeo linkage obesity diabetes framework offer potential method understand publics opinions sentiments regard ddeo importantly research provide new opportunities computational social scientists medical experts public health professionals collectively address ddeo relate issue
social media provide platform users express opinions share information understand public health opinions social media twitter offer unique approach characterize common health issue diabetes diet exercise obesity ddeo however collect analyze large scale conversational public health data set challenge research task goal research analyze characteristics general public opinions regard diabetes diet exercise obesity ddeo express twitter multi component semantic linguistic framework develop collect twitter data discover topics interest ddeo analyze topics extract forty-five million tweet eight tweet discuss diabetes two hundred and thirty-seven diet one hundred and sixty-six exercise five hundred and seventeen obesity strongest correlation among topics determine exercise obesity notable correlations diabetes obesity diet obesity ddeo term also identify subtopics ddeo topics frequent subtopics discuss along diabetes exclude ddeo term blood pressure heart attack yoga alzheimer non ddeo subtopics diet include vegetarian pregnancy celebrities weight loss religious mental health subtopics exercise include computer game brain fitness daily plan non ddeo subtopics obesity include alzheimer cancer children two hundred and sixty-seven billion social media users two thousand and sixteen publicly available data twitter post utilize support clinical providers public health experts social scientists better understand common public opinions regard diabetes diet exercise obesity
continually increase number document produce year necessitate ever improve information process methods search retrieve organize text central information process methods document classification become important application supervise learn recently performance traditional classifiers degrade number document increase along growth number document come increase number categories paper approach problem differently current document classification methods view problem multi class classification instead perform hierarchical classification use approach call hierarchical deep learn text classification hdltex hdltex employ stack deep learn architectures provide specialize understand level document hierarchy
propose new generative model sentence first sample prototype sentence train corpus edit new sentence compare traditional model generate scratch either leave right first sample latent sentence vector prototype edit model improve perplexity language model generate higher quality output accord human evaluation furthermore model give rise latent edit vector capture interpretable semantics sentence similarity sentence level analogies
string kernel sk techniques especially use gap k mers feature gk obtain great success classify sequence like dna protein text however state art gk sk run extremely slow increase dictionary size sigma allow mismatch current gk sk use trie base algorithm calculate co occurrence mismatch substrings result time cost proportional ofsigmam propose textbffast algorithm calculate underlinegapped k mer underlinekernel use underlinecounting gakco gakco use associative array calculate co occurrence substrings use cumulative count algorithm fast scalable larger sigma naturally parallelizable provide rigorous asymptotic analysis compare gakco state art gk sk theoretically time cost gakco independent sigmam term slow trie base approach experimentally observe gakco achieve accuracy state art outperform speed factor two one hundred four classify sequence dna five datasets protein twelve datasets character base english text two datasets respectively gakco share open source tool urlhttps githubcom qdata gakco svm
entity type systems become richer fine grain expect number type assign give entity increase however fine grain type work focus datasets exhibit low degree type multiplicity paper consider high multiplicity regime inherent data source wikipedia semi open type systems introduce set prediction approach problem show model outperform unstructured baselines new wikipedia base fine grain type corpus
use support backchannel bc cue make human computer interaction social bcs provide feedback listener speaker indicate speaker still listen bcs express different ways depend modality interaction example gesture acoustic cue work consider acoustic cue propose approach towards detect bc opportunities base acoustic input feature like power pitch work field rely use hand write rule set specialize feature make use artificial neural network capable derive higher order feature input feature setup first use fully connect fee forward network establish update baseline comparison previously propose setup also extend setup use long short term memory lstm network show outperform fee forward base setups various task best system achieve f1 score thirty-seven use power pitch feature add linguistic information use word2vec score increase thirty-nine
humans animals constantly expose continuous stream sensory information different modalities time form compress representations like concepts symbols species use language process structure interaction map sensorimotor concepts linguistic elements need establish evidence children might learn language simply disambiguate potential mean base multiple exposures utterances different contexts cross situational learn exist model map modalities usually find single step directly use frequencies referent mean co occurrences paper present extension one step map introduce newly propose sequential map algorithm together publicly available matlab implementation demonstration choose less typical scenario instead learn associate object name focus body representations humanoid robot receive tactile stimulations body time listen utterances body part name eg hand forearm torso goal arrive correct body categories demonstrate sequential map algorithm outperform one step map addition effect data set size noise linguistic input study
automatic summarisation popular approach reduce document main arguments recent research area focus neural approach summarisation data hungry however large datasets exist none traditionally popular domain scientific publications open challenge research avenues center encode large complex document paper introduce new dataset summarisation computer science publications exploit large resource author provide summaries show straightforward ways extend develop model dataset make use neural sentence encode traditionally use summarisation feature show model encode sentence well local global context perform best significantly outperform well establish baseline methods
question play prominent role social interactions perform rhetorical function go beyond simple informational exchange surface form question signal intention background person ask well nature relation interlocutor informational nature question extensively examine context question answer applications rhetorical aspects largely understudy work introduce unsupervised methodology extract surface motifs recur question group accord latent rhetorical role apply framework set question sessions uk parliament show result typology encode key aspects political discourse bifurcation question behavior government opposition party reveal new insights effect legislator tenure political career ambition
present deep voice three fully convolutional attention base neural text speech tts system deep voice three match state art neural speech synthesis systems naturalness train ten time faster scale deep voice three data set size unprecedented tts train eight hundred hours audio two thousand speakers addition identify common error modes attention base speech synthesis network demonstrate mitigate compare several different waveform synthesis methods also describe scale inference ten million query per day one single gpu server
propose new statistical model suitable machine learn systems long distance correlations natural languages model base direct acyclic graph decorate multi linear tensor map vertices vector space edge call tensor network tensor network previously employ effective numerical computation renormalization group flow space effective quantum field theories lattice model statistical mechanics provide explicit algebro geometric analysis parameter moduli space tree graph discuss model properties applications statistical translation
global political preeminence gradually shift unite kingdom unite state capacity culturally influence rest world work analyze world wide varieties write english evolve study spatial temporal variations vocabulary spell english use large corpus geolocated tweet google book datasets correspond book publish us uk advantage approach address standard write language google book colloquial form microblogging message twitter find american english dominant form english outside uk influence felt even within uk border finally analyze trend evolve time impact cultural events shape
represent word co occurrences word context effective way capture mean word however theory behind remain challenge work take example word classification task give theoretical analysis approach represent word x function fpcx c context feature pcx conditional probability estimate text corpus function f map co occurrence measure prediction score investigate impact context feature c function f also explain reason use co occurrences multiple context feature may better use single one addition result would light theory feature learn machine learn general
recent work representation learn graph structure data predominantly focus learn distribute representations graph substructures nod subgraphs however many graph analytics task graph classification cluster require represent entire graph fix length feature vectors aforementioned approach naturally unequipped learn representations graph kernels remain effective way obtain however graph kernels use handcraft feature eg shortest paths graphlets etc hence hamper problems poor generalization address limitation work propose neural embed framework name graph2vec learn data drive distribute representations arbitrary size graph graph2vec embeddings learn unsupervised manner task agnostic hence could use downstream task graph classification cluster even seed supervise representation learn approach experiment several benchmark large real world datasets show graph2vec achieve significant improvements classification cluster accuracies substructure representation learn approach competitive state art graph kernels
one major hurdle prevent full exploitation information online communities widespread concern regard quality credibility user contribute content prior work domain operate static snapshot community make strong assumptions structure data eg relational table consider shallow feature text classification address limitations propose probabilistic graphical model leverage joint interplay multiple factor online communities like user interactions community dynamics textual content automatically assess credibility user contribute online content expertise users evolution user interpretable explanation end devise new model base conditional random field different settings like incorporate partial expert knowledge semi supervise learn handle discrete label well numeric rat fine grain analysis enable applications extract reliable side effect drug user contribute post healthforums identify credible content news communities online communities dynamic users join leave adapt evolve trend mature time capture dynamics propose generative model base hide markov model latent dirichlet allocation brownian motion trace continuous evolution user expertise language model time allow us identify expert users credible content jointly time improve state art recommender systems explicitly consider maturity users also enable applications identify helpful product review detect fake anomalous review limit information
sport channel video portals offer excite domain research multimodal multilingual analysis present methods address problem automatic video highlight prediction base joint visual feature textual analysis real world audience discourse complex slang english traditional chinese present novel dataset base league legends championships record north american taiwanese twitchtv channel release research demonstrate strong result use multimodal character level cnn rnn model architectures
propose statistical model natural language begin consider language monoid represent complex matrices compatible translation invariant probability measure interpret probability measure arise via bear rule translation invariant matrix product state
text base analysis methods allow reveal privacy relevant author attribute gender age identify text author methods compromise privacy anonymous author even author try remove privacy sensitive content paper propose automatic method call adversarial author attribute anonymity neural translation a4nt combat text base adversaries combine sequence sequence language model use machine translation generative adversarial network obfuscate author attribute unlike machine translation techniques need pair data method train unpaired corpora text contain different author importantly propose evaluate techniques impose constraints a4nt preserve semantics input text a4nt learn make minimal change input text successfully fool author attribute classifiers aim maintain mean input show experiment two different datasets three settings propose method effective fool author attribute classifiers thereby improve anonymity author
keyword spot kws critical component enable speech base user interactions smart devices require real time response high accuracy good user experience recently neural network become attractive choice kws architecture superior accuracy compare traditional speech process algorithms due always nature kws application highly constrain power budget typically run tiny microcontrollers limit memory compute capability design neural network architecture kws must consider constraints work perform neural network architecture evaluation exploration run kws resource constrain microcontrollers train various neural network architectures keyword spot publish literature compare accuracy memory compute requirements show possible optimize neural network architectures fit within memory compute constraints microcontrollers without sacrifice accuracy explore depthwise separable convolutional neural network ds cnn compare neural network architectures ds cnn achieve accuracy nine hundred and fifty-four ten higher dnn model similar number parameters
paper present self supervise method visual detection active speaker multi person speak interaction scenario active speaker detection fundamental prerequisite artificial cognitive system attempt acquire language social settings propose method intend complement acoustic detection active speaker thus improve system robustness noisy condition method detect arbitrary number possibly overlap active speakers base exclusively visual information face furthermore method rely external annotations thus comply cognitive development instead method use information auditory modality support learn visual domain paper report extensive evaluation propose method use large multi person face face interaction dataset result show good performance speaker dependent set however speaker independent set propose method yield significantly lower performance believe propose method represent essential component artificial cognitive system robotic platform engage social interactions
grow field robotics artificial intelligence ai research human robot collaboration whose target enable effective teamwork humans robots however many situations human team still superior human robot team primarily human team easily agree common goal language individual members observe effectively leverage share motor repertoire sensorimotor resources paper show cognitive robots possible indeed fruitful combine knowledge acquire interact elements environment affordance exploration probabilistic observation another agent action propose model unite learn robot affordances word descriptions ii statistical recognition human gesture vision sensors discuss theoretical motivations possible implementations show initial result highlight acquire knowledge surround environment humanoid robot generalize knowledge case observe another agent human partner perform motor action previously execute train
present methodology aim cross modal machine learn use multidisciplinary tool methods draw broad range areas discipline include music systematic musicology dance motion capture human computer interaction computational linguistics audio signal process main task include one adapt wisdom crowd approach embodiment music dance performance create dataset music music lyric cover variety emotions two apply audio language inform machine learn techniques dataset identify automatically emotional content music lyric three integrate motion capture data vicon system dancers perform music
automatic transcriptions consumer generate multi media content youtube videos still exhibit high word error rat data typically occupy broad domain record challenge condition cheap hardware focus visual modality may post process edit paper extend earlier work adapt acoustic model dnn base speech recognition system rnn language model show adapt object scenes automatically detect video work corpus videos web idea object see car scene detect kitchen use condition model context record thereby reduce perplexity improve transcription achieve good improvements case compare analyze respective reductions word error rate expect result use type speech process context information available example robotics man machine interaction index large audio visual archive ultimately help bring together video text speech text communities
project analyse much semantic information image carry much value image data add sentiment analysis text associate image better understand contribution image compare model make use image data model make use text data model combine data type also analyse approach could help sentiment classifiers generalize unknown sentiments
work propose blackbox intervention method visual dialog model aim assess contribution individual linguistic visual components concretely conduct structure randomize interventions aim impair individual component model observe change task performance reproduce state art visual dialog model demonstrate methodology yield surprise insights namely dialog image information minimal contributions task performance intervention method present apply sanity check strength robustness component visual dialog systems
bag word bow represent corpus matrix whose elements frequency word however row matrix high dimensional sparse vector dimension reduction dr popular method address sparsity high dimensionality issue among different strategies develop dr method unsupervised feature transformation uft popular strategy map word new basis represent bow recent increase text data challenge imply dr area still need new perspectives although wide range methods base uft strategy develop fuzzy approach consider dr base strategy research investigate application fuzzy cluster dr method base uft strategy collapse bow matrix provide lower dimensional representation document instead word corpus quantitative evaluation show fuzzy cluster produce superior performance feature principal components analysis pca singular value decomposition svd two popular dr methods base uft strategy
political polarization unite state continue rise question whether polarize individuals fruitfully cooperate become press although diversity individual perspectives typically lead superior team performance complex task strong political perspectives associate conflict misinformation reluctance engage people perspectives beyond one echo chamber unclear whether self select team politically diverse individuals create higher lower quality outcomes paper explore effect team political composition performance analysis millions edit wikipedia political social issue science article measure editors political alignments contributions conservative versus liberal article survey editors validate primarily edit liberal article identify strongly democratic party edit conservative ones republican party analysis reveal polarize team consist balance set politically diverse editors create article higher quality politically homogeneous team effect appear strongly wikipedia political article also observe social issue even science article analysis article talk page reveal politically polarize team engage longer constructive competitive substantively focus linguistically diverse debate political moderate intense use wikipedia policies politically diverse team suggest institutional design principles help unleash power politically polarize team
inspire previous work emergent communication referential game propose novel multi modal multi step referential game sender receiver access distinct modalities object information exchange bidirectional arbitrary duration multi modal multi step set allow agents develop internal communication significantly closer natural language share single set message length conversation may vary accord difficulty task examine properties empirically use dataset consist image textual descriptions mammals agents task identify correct object experiment indicate robust efficient communication protocol emerge gradual information exchange inform better predictions higher communication bandwidth improve generalization
introduce home household multimodal environment artificial agents learn vision audio semantics physics interaction object agents within realistic context home integrate forty-five thousand diverse 3d house layouts base suncg dataset scale may facilitate learn generalization transfer home open source openai gym compatible platform extensible task reinforcement learn language ground sound base navigation robotics multi agent learn hope home better enable artificial agents learn humans interactive multimodal richly contextualized set