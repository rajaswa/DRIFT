state art speech recognition systems rely heavily three basic components acoustic model pronunciation lexicon language model build components researcher need linguistic well technical expertise barrier low resource domains techniques construct three components without expert domain knowledge great demand urdu despite millions speakers world low resource language term standard publically available linguistic resources paper present grapheme phoneme conversion tool urdu generate pronunciation lexicon form suitable use speech recognition systems list urdu word tool predict pronunciation word use lstm base model train handcraft expert lexicon around thirty-nine thousand word show accuracy sixty-four upon internal evaluation external evaluation speech recognition task obtain word error rate comparable one achieve use fully handcraft expert lexicon
sanskrit small word morphemes combine form compound word process know sandhi sandhi split process split give compound word constituent morphemes although rule govern word split exist language highly challenge identify location split compound word though exist sandhi split systems incorporate pre define split rule low accuracy compound word might break multiple ways provide syntactically correct split research propose novel deep learn architecture call double decoder rnn dd rnn predict location split ninety-five accuracy ii predict constituent word learn sandhi split rule seven hundred and ninety-five accuracy outperform state art twenty additionally show generalization capability deep learn model show competitive result problem chinese word segmentation well
multimodal model prove outperform text base model learn semantic word representations almost previous multimodal model typically treat representations different modalities equally however obvious information different modalities contribute differently mean word motivate us build multimodal model dynamically fuse semantic representations different modalities accord different type word end propose three novel dynamic fusion methods assign importance weight modality weight learn weak supervision word association pair extensive experiment demonstrate propose methods outperform strong unimodal baselines state art multimodal model
adverse reaction cause drug potentially dangerous problem may lead mortality morbidity patients adverse drug event ade extraction significant problem biomedical research model ade extraction question answer problem take inspiration machine read comprehension mrc literature design model objective design model exploit local linguistic context clinical text enable intra sequence interaction order jointly learn classify drug disease entities extract adverse reactions cause give drug model make use self attention mechanism facilitate intra sequence interaction text sequence enable us visualize understand network make use local wider context classification
classification social media post emergency response important practical problem accurate classification help automate process message help responders public react emergencies timely fashion research focus classify facebook message us police departments randomly select five thousand message use train classifiers distinguish four categories message emergency preparedness response recovery well general engagement message feature represent bag word word2vec model construct use support vector machine svms convolutional cnns recurrent neural network rnns best perform classifier rnn custom train word2vec model represent feature achieve f1 measure eight hundred and thirty-nine
present easy use fast toolkit namely vncorenlp java nlp annotation pipeline vietnamese vncorenlp support key natural language process nlp task include word segmentation part speech pos tag name entity recognition ner dependency parse obtain state art sota result task release vncorenlp provide rich linguistic annotations facilitate research work vietnamese nlp vncorenlp open source available https githubcom vncorenlp vncorenlp
challenge practical research problem obtain effective compression lengthy product title e commerce particularly important users browse mobile e commerce apps merchants make original product title redundant lengthy search engine optimization traditional text summarization approach often require large amount preprocessing cost capture important issue conversion rate e commerce paper propose novel multi task learn approach improve product title compression user search log data particular pointer network base sequence sequence approach utilize title compression attentive mechanism extractive method attentive encoder decoder approach utilize generate user search query encode parameters ie semantic embed original title share among two task attention distributions jointly optimize extensive set experiment human annotate data online deployment demonstrate advantage propose research compression qualities online business value
ability learn optimal control policies systems action space define sentence natural language would allow many interest real world applications automatic optimisation dialogue systems text base game multiple end reward promise platform task since feedback allow us employ reinforcement learn techniques jointly learn text representations control policies present general text game play agent test generalisation transfer learn performance show ability play multiple game also present pyfiction open source library universal access different text game could together agent implement interface serve baseline future research
paper describe corpus three thousand english literary texts two hundred and fifty million word extract gutenberg project span range genres fiction non fiction write one hundred and thirty author eg darwin dickens shakespeare quantitative narrative analysis qna use explore clean subcorpus gutenberg english poetry corpus gepc comprise one hundred poetic texts around two million word fifty author eg keats joyce wordsworth exemplary qna study show author similarities base latent semantic analysis significant topics author various text analytic metrics george eliot poem lisa love king jam joyce chamber music concern eg lexical diversity sentiment analysis gepc particularly suit research digital humanities natural language process neurocognitive poetics eg train test corpus stimulus development control
paper give comprehensive analyse corpora base wikipedia several task question answer four recent corpora collectedwikiqa selqa squad infoqa first analyze intrinsically contextual similarities question type answer categories corpora analyze extrinsically three question answer task answer retrieval selection trigger index base method creation silver standard dataset answer retrieval use entire wikipedia also present analysis show uniqueness corpora suggest better use statistical question answer learn
one major essential task natural language process machine translation highly dependent upon multilingual parallel corpora paper introduce biggest persian english parallel corpus one million sentence pair collect masterpieces literature also present acquisition process statistics corpus experiment base line statistical machine translation system use corpus
multilingual speakers often switch languages express social communication platforms sometimes original script language preserve use common script languages quite popular well due convenience occasion multiple languages mix different rule grammar use script make challenge task natural language process even case accurate sentiment identification paper report result various experiment carry movie review dataset code mix property two languages english bengali type roman script test various machine learn algorithms train english feature code mix data achieve maximum accuracy five thousand, nine hundred use naive bay nb model also test various model train code mix data well english feature highest accuracy seven thousand, two hundred and fifty obtain support vector machine svm model finally analyze misclassified snippets discuss challenge need resolve better accuracy
paper present novel task use real user data obtain human machine conversation task concern denotation extraction answer hint collect interactively dialogue task motivate need large amount train data question answer dialogue system development data often expensive hard collect able collect denotation interactively directly users one could improve example natural understand components line ease collection train data paper also present introductory result evaluation several denotation extraction model include attention base neural network approach
pronouns frequently omit pro drop languages chinese generally lead significant challenge respect production complete translations date little attention pay drop pronoun dp problem within neural machine translation nmt work propose novel reconstruction base approach alleviate dp translation problems nmt model firstly dps within source sentence automatically annotate parallel information extract bilingual train corpus next annotate source sentence reconstruct hide representations nmt model auxiliary train objectives term reconstruction score parameters associate nmt model guide produce enhance hide representations encourage much possible embed annotate dp information experimental result chinese english japanese english dialogue translation task show propose approach significantly consistently improve translation performance strong nmt baseline directly build train data annotate dps
introduce milkqa question answer dataset dairy domain dedicate study consumer question dataset contain two thousand, six hundred and fifty-seven pair question answer write portuguese language originally collect brazilian agricultural research corporation embrapa question motivate real situations write thousands author different background level literacy answer elaborate specialists embrapa customer service dataset filter anonymized three human annotators consumer question challenge kind question usually employ form seek information although several question answer datasets available resources suitable research answer selection model consumer question aim fill gap make milkqa publicly available study behavior four answer selection model milkqa two baseline model two convolutional neural network archictetures result show milkqa pose real challenge computational model particularly due linguistic characteristics question unusually longer lengths one experiment model give reasonable result cost high computational requirements
gradient symbolic computation propose mean solve discrete global optimization problems use neurally plausible continuous stochastic dynamical system gradient symbolic dynamics involve two free parameters must adjust function time obtain global maximizer end computation provide summary know gsc dynamics special case settings parameters also establish schedule two parameters convergence correct answer occur high probability result put empirical result already obtain gsc sound theoretical foot
roles one important concepts understand human sociocognitive behavior group interactions members take different roles within discussion roles distinct pattern behavioral engagement ie active passive lead follow contribution characteristics ie provide new information echo give material social orientation ie individual group different combinations roles produce characteristically different group outcomes either less productive towards collective goals online collaborative learn environments lead better worse learn outcomes individual participants study propose validate novel approach detect emergent roles participants contributions pattern interaction specifically develop group communication analysis gca combine automate computational linguistic techniques analyse sequential interactions online group communication gca apply three large collaborative interaction datasets participant n two thousand, four hundred and twenty-nine group n three thousand, five hundred and ninety-eight cluster analyse linear mix effect model use assess validity gca approach influence learner roles student group performance result indicate participants pattern linguistic coordination cohesion representative roles individuals play collaborative discussions broadly gca provide framework researchers explore micro intra interpersonal pattern associate participants roles sociocognitive process relate successful collaboration
part speech pos tag old fundamental task natural language process supervise pos taggers show promise accuracy always feasible use supervise methods due lack label data project attempt unsurprisingly induce pos tag iteratively look recur pattern word hierarchical agglomerative cluster process approach show promise result compare tag result state art unsupervised pos taggers
distant supervise relation extraction efficient approach scale relation extraction large corpora widely use find novel relational facts plain text recent study neural relation extraction show great progress task via model sentence low dimensional space seldom consider syntax information model entities paper propose learn syntax aware entity embed neural relation extraction first encode context entities dependency tree sentence level entity embed base tree gru utilize intra sentence inter sentence attentions obtain sentence set level entity embed sentence contain focus entity pair finally combine sentence embed entity embed relation classification conduct experiment widely use real world dataset experimental result show model make full use informative instance achieve state art performance relation extraction
neural machine translation nmt suffer performance deficiency limit vocabulary fail cover source target side adequately happen frequently deal morphologically rich languages address problem previous work focus adjust translation granularity expand vocabulary size however morphological information relatively consider nmt architectures may improve translation quality propose novel method reduce data sparsity also model morphology simple effective mechanism predict stem suffix separately decode system achieve improvement one hundred and ninety-eight bleu compare previous work english russian translation method orthogonal different nmt architectures stably gain improvements various domains
william shakespeare believe significant author anonymous play reign king edward iii publish one thousand, five hundred and ninety-six however recently thomas kyd suggest primary author use neurolinguistics approach authorship identification use four feature technique rpas convert nineteen scenes edward iii multi dimensional vector three complementary analytical techniques apply cluster data reduce single technique bias alternate method seriation use measure distance cluster test strength connections find multivariate techniques robust able allocate fourteen scenes thomas kyd question scenes long believe shakespeare
keyphrase extraction task automatically select small set phrase best describe give free text document supervise keyphrase extraction require large amount label train data generalize poorly outside domain train data time unsupervised systems poor accuracy often generalize well require input document belong larger corpus also give input address drawbacks paper tackle keyphrase extraction single document embedrank novel unsupervised method leverage sentence embeddings embedrank achieve higher f score graph base state art systems standard datasets suitable real time process large amount web data embedrank also explicitly increase coverage diversity among select keyphrases introduce embed base maximal marginal relevance mmr new phrase user study include two hundred vote show although reduce phrase semantic overlap lead gain f score high diversity selection prefer humans
multi relation question answer challenge task due requirement elaborate analysis question reason multiple fact triple knowledge base paper present novel model call interpretable reason network employ interpretable hop hop reason process question answer model dynamically decide part input question analyze hop predict relation correspond current parse result utilize predict relation update question representation state reason process drive next hop reason experiment show model yield state art result two datasets interestingly model offer traceable observable intermediate predictions reason analysis failure diagnosis thereby allow manual manipulation predict final answer
give rise new approach mt neural mt nmt promise performance different text type assess translation quality attain perceive greatest challenge mt literary text specifically target novels arguably popular type literary text build literary adapt nmt system english catalan translation direction evaluate system pertain previous dominant paradigm mt statistical phrase base mt pbsmt end first time train mt systems nmt pbsmt large amount literary text one hundred million word evaluate set twelve widely know novels span 1920s present day accord bleu automatic evaluation metric nmt significantly better pbsmt p one novels consider overall nmt result eleven relative improvement three point absolute pbsmt complementary human evaluation three book show seventeen thirty-four translations depend book produce nmt versus eight twenty pbsmt perceive native speakers target language equivalent quality translations produce professional human translator
partially inspire successful applications variational recurrent neural network propose novel variational recurrent neural machine translation vrnmt model paper different variational nmt vrnmt introduce series latent random variables model translation procedure sentence generative way instead single latent variable specifically latent random variables include hide state nmt decoder elements variational autoencoder way variables recurrently generate enable capture strong complex dependencies among output translations different timesteps order deal challenge perform efficient posterior inference large scale train incorporation latent variables build neural posterior approximator equip reparameterization technique estimate variational lower bind experiment chinese english english german translation task demonstrate propose model achieve significant improvements conventional variational nmt model
dominant neural machine translation nmt model apply unify attentional encoder decoder neural network translation traditionally nmt decoders adopt recurrent neural network rnns perform translation leave toright manner leave target side contexts generate right leave unexploited translation paper equip conventional attentional encoder decoder nmt framework backward decoder order explore bidirectional decode nmt attend hide state sequence produce encoder backward decoder first learn generate target side hide state sequence right leave forward decoder perform translation forward direction translation prediction timestep simultaneously apply two attention model consider source side reverse target side hide state respectively new architecture model able fully exploit source target side contexts improve translation quality altogether experimental result nist chinese english wmt english german translation task demonstrate model achieve substantial improvements conventional nmt three hundred and fourteen one hundred and thirty-eight bleu point respectively source code work obtain https githubcom deeplearnxmu abdnmt
quickly obtain new label data choose crowdsourcing alternative way lower cost short time exchange crowd annotations non experts may lower quality experts paper propose approach perform crowd annotation learn chinese name entity recognition ner make full use noisy sequence label multiple annotators inspire adversarial learn approach use common bi lstm private bi lstm represent annotator generic specific information annotator generic information common knowledge entities easily master crowd finally build chinese ne tagger base lstm crf model experiment create two data set chinese ner task two domains experimental result show system achieve better score strong baseline systems
practice speak language understand systems process user input pipelined manner first domain predict intent semantic slot infer accord semantic frame predict domain pipeline approach however disadvantage error propagation lack information share address issue present unify neural network jointly perform domain intent slot predictions approach adopt principled architecture multitask learn fold state art model task ingredients eg orthography sensitive input encode curriculum train model deliver significant improvements three task across domains strong baselines include one use oracle prediction domain detection real user data commercial personal assistant
exist machine learn model achieve great success sentiment classification typically explicitly capture sentiment orient word interaction lead poor result fine grain analysis snippet level phrase sentence factorization machine provide possible approach learn element wise interaction recommender systems directly applicable task due inability model contexts word sequence work develop two position aware factorization machine consider word interaction context position information information jointly encode set sentiment orient word interaction vectors compare traditional word embeddings swi vectors explicitly capture sentiment orient word interaction simplify parameter learn experimental result show comparable performance state art methods document level classification benefit snippet sentence level sentiment analysis
text classification one widely study task natural language process motivate principle compositionality large multilayer neural network model employ task attempt effectively utilize constituent expressions almost report work train large network use discriminative approach come caveat proper capacity control tend latch signal may generalize use various recent state art approach text classification explore whether model actually learn compose mean sentence still focus keywords lexicons classify document test hypothesis carefully construct datasets train test split direct overlap lexicons overall language structure would similar study various text classifiers observe big performance drop datasets finally show even simple model propose regularization techniques disincentivize focus key lexicons substantially improve classification accuracy
paper present distributional word embed model train one largest available russian corpora araneum russicum maximum ten billion word crawl web compare model model train russian national corpus rnc two corpora much different size compilation procedures test differences evaluate train model russian part multilingual simlex999 semantic similarity dataset detect describe numerous issue dataset publish new correct version aside already know fact rnc generally better train corpus web corpora enumerate explain fine differences model process semantic similarity task part evaluation set difficult particular model additionally learn curve model describe show rnc generally robust train material task
behavior deep neural network dnns hard understand make necessary explore post hoc explanation methods conduct first comprehensive evaluation explanation methods nlp end design two novel evaluation paradigms cover two important class nlp problems small context large context problems paradigms require manual annotation therefore broadly applicable also introduce limsse explanation method inspire lime design nlp show empirically limsse lrp deeplift effective explanation methods recommend explain dnns nlp
recognize semantically similar sentence paragraph across languages beneficial many task range cross lingual information retrieval plagiarism detection machine translation recently propose methods predict cross lingual semantic similarity short texts however make use tool resources eg machine translation systems syntactic parsers name entity recognition many languages language pair exist contrast propose unsupervised resource light approach measure semantic similarity texts different languages operate bilingual multilingual space project continuous word vectors ie word embeddings one language vector space language via linear translation model align word accord similarity vectors bilingual embed space investigate different unsupervised measure semantic similarity exploit bilingual embeddings word alignments require limit size set word translation pair languages propose approach applicable virtually pair languages exist sufficiently large corpus require learn monolingual word embeddings experimental result three different datasets measure semantic textual similarity show simple resource light approach reach performance close supervise resource intensive methods display stability across different language pair furthermore evaluate propose method two extrinsic task namely extraction parallel sentence comparable corpora cross lingual plagiarism detection show yield performance comparable complex resource intensive state art model respective task
transfer learn tl play crucial role give dataset insufficient label examples train accurate model scenarios knowledge accumulate within model pre train source dataset transfer target dataset result improvement target model though tl find successful realm image base applications impact practical use natural language process nlp applications still subject research due hierarchical architecture deep neural network dnn provide flexibility customization adjust parameters depth layer thereby form apt area exploit use tl paper report result conclusions obtain extensive empirical experiment use convolutional neural network cnn try uncover thumb rule ensure successful positive transfer addition also highlight flaw mean could lead negative transfer explore transferability various layer describe effect vary hyper parameters transfer performance also present comparison accuracy value model size state art methods finally derive inferences empirical result provide best practice achieve successful positive transfer
novel text data dimension reduction technique call tree structure multi linear principal component anal ysis tmpca propose work different traditional text dimension reduction methods deal word level representation tmpca technique reduce dimension input sequence sentence simplify follow text classification task show mathematically experimentally tmpca tool demand much lower complexity hence less compute power ordinary principal component analysis pca furthermore demon strated experimental result support vector machine svm method apply tmpca process data achieve commensurable better performance state art recurrent neural network rnn approach
web twenty bring numerous user produce data reveal one thoughts experience knowledge great source many task information extraction knowledge base construction however colloquial nature texts pose new challenge current natural language process techniques adapt formal form language ellipsis common linguistic phenomenon word leave understand context especially oral utterance hinder improvement dependency parse great importance task rely mean sentence order promote research area release chinese dependency treebank three hundred and nineteen weibos contain five hundred and seventy-two sentence omissions restore contexts reserve
major challenge problem community question answer lexical semantic gap sentence representations solutions minimize gap include introduction extra parameters deep model augment external handcraft feature paper propose novel attentive recurrent tensor network solve lexical semantic gap community question answer introduce token level phrase level attention strategy map input sequence output use trainable parameters use tensor parameters introduce three way interaction question answer external feature vector space introduce simplify tensor matrices l2 regularization result smooth optimization train propose model achieve state art performance task answer sentence selection trecqa wikiqa datasets outperform current state art task best answer selection yahoo l4 answer trigger task wikiqa
present new method estimate vector space representations word embed learn concept induction test method highly parallel corpus learn semantic representations word one thousand, two hundred and fifty-nine different languages single common space extensive experimental evaluation crosslingual word similarity sentiment analysis indicate concept base multilingual embed learn perform better previous approach
grammatical error detection automate essay score two task area automate assessment traditionally task treat independently different machine learn model feature use task paper develop multi task neural network model jointly optimise task particular show neural automate essay score significantly improve show essay score provide little evidence inform grammatical error detection essay score highly influence error detection
paper describe biographynet digital humanities project two thousand and twelve two thousand and sixteen bring together researchers history computational linguistics computer science project use data biography portal netherlands bpn contain approximately one hundred and twenty-five thousand biographies variety dutch biographical dictionaries eighteenth century describe around seventy-six thousand individuals biographynet aim strengthen value portal comparable biographical datasets historical research improve search options presentation outcome historically justify nlp pipeline work user evaluate demonstrator project main target group professional historians project therefore work two key concepts provenance understand term allow historical source criticism reference data management program interventions digitize source perspective interpret inherent uncertainty concern interpretation historical result
explore methods extract relations name entities free text unsupervised set addition standard feature extraction develop novel method weight word embeddings alleviate problem feature sparsity use individual feature reduction approach exhibit significant improvement fifty-eight state art relation cluster score f1 score four hundred and sixteen nyt fb dataset
determine whether two give question semantically similar fairly challenge task give different structure form question take paper use gate recurrent unitsgru combination highly use machine learn algorithms like random forest adaboost svm similarity prediction task dataset release quora consist 400k label question pair get best result use siamese adaptation bidirectional gru random forest classifier land us among top twenty-four competition quora question pair host kaggle
present assertion base question answer abqa open domain question answer task take question passage input output semi structure assertion consist subject predicate list arguments assertion convey evidence short answer span read comprehension concise tedious passage passage base qa advantage make abqa suitable human computer interaction scenarios voice control speakers progress towards improve abqa require richer supervise dataset powerful model text understand remedy introduce new dataset call webassertions include hand annotate qa label three hundred and fifty-eight thousand, four hundred and twenty-seven assertions fifty-five thousand, nine hundred and sixty web passages address abqa develop generative extractive approach backbone generative approach sequence sequence learn order capture structure output assertion introduce hierarchical decoder first generate structure assertion generate word field extractive approach base learn rank feature different level granularity design measure semantic relevance question assertion experimental result show approach ability infer question aware assertions passage evaluate approach incorporate abqa result additional feature passage base qa result two datasets show abqa feature significantly improve accuracy passage basedqa
describe large high quality benchmark evaluation mention detection tool benchmark contain annotations name entities well type entities annotate different type text range clean text take wikipedia noisy speak data benchmark build highly control crowd source process ensure quality describe benchmark process guidelines use build demonstrate result state art system run benchmark
query focus summarization qfs address mostly use extractive methods methods however produce text suffer low coherence investigate abstractive methods apply qfs overcome limitations recent developments neural attention base sequence sequence model lead state art result task abstractive generic single document summarization model train end end method large amount train data address three aspects make abstractive summarization applicable qfs asince train data incorporate query relevance pre train abstractive model b since exist abstractive model train single document set design iterate method embed abstractive model within multi document requirement qfs c abstractive model adapt train generate text specific length one hundred word aim generate output different size two hundred and fifty word design way adapt target size generate summaries give size ratio compare method relevance sensitive attention qfs extractive baselines various ways combine abstractive model duc qfs datasets demonstrate solid improvements rouge performance
sentiment analysis sa major field study natural language process computational linguistics information retrieval interest sa constantly grow academia industry recent years moreover increase need generate appropriate resources datasets particular low resource languages include persian datasets play important role design develop appropriate opinion mine platforms use supervise semi supervise unsupervised methods paper outline entire process develop manually annotate sentiment corpus sentipers cover formal informal write contemporary persian best knowledge sentipers unique sentiment corpus rich annotation three different level include document level sentence level entity aspect level persian corpus contain twenty-six thousand sentence users opinions digital product domain benefit special characteristics quantify positiveness negativity opinion assign number within specific range give sentence furthermore present statistics various components corpus well study inter annotator agreement among annotators finally challenge face annotation process discuss well
science happiness area positive psychology concern understand behaviors make people happy sustainable fashion recently interest develop technologies help incorporate find science happiness users daily live steer towards behaviors increase happiness goal build technology understand people express happy moments text crowd source happydb corpus one hundred thousand happy moments make publicly available paper describe happydb properties outline several important nlp problems study help corpus also apply several state art analysis techniques analyze happydb result demonstrate need deeper nlp techniques develop make happydb excite resource follow research
neural machine translation nmt model provide improve translation quality elegant end end framework less clear learn language recent work start evaluate quality vector representations learn nmt model morphological syntactic task paper investigate representations learn different layer nmt encoders train nmt systems parallel data use train model extract feature train classifier two task part speech semantic tag measure performance classifier proxy quality original nmt model give task quantitative analysis yield interest insights regard representation learn nmt model instance find higher layer better learn semantics lower layer tend better part speech tag also observe little effect target language source side representations especially higher quality nmt model
open information extraction oie process extract relations arguments automatically textual document without need restrict search predefined relations recent years several oie systems english language create system vietnamese language paper propose method oie vietnamese use clause base approach accordingly exploit vietnamese dependency parse use grammar clauses strive consider possible relations sentence correspond clause type identify proposition extractable relations base grammatical function constituents result system first oie system name vnoie vietnamese language generate open relations arguments vietnamese text highly scalable extraction domain independent experimental result show oie system achieve promise result precision eight thousand, three hundred and seventy-one
neural network model recently propose question answer qa primarily focus capture passage question relation however minimal capability link relevant facts distribute across multiple sentence crucial achieve deeper understand perform multi sentence reason co reference resolution etc also explicitly focus question answer type often play critical role qa paper propose novel end end question focus multi factor attention network answer extraction multi factor attentive encode use tensor base transformation aggregate meaningful facts even locate multiple sentence implicitly infer answer type also propose max attentional question aggregation mechanism encode question vector base important word question prediction incorporate sequence level encode first wh word immediately follow word additional source question type information propose model achieve significant improvements best prior state art result three large scale challenge qa datasets namely newsqa triviaqa searchqa
bilingual sequence model improve phrase base translation reorder overcome phrasal independence assumption handle long range reorder however due data sparsity model often fall back small context size problem previously address learn sequence generalize representations pos tag word cluster paper explore alternative base neural network model concretely train neuralized versions lexicalize reorder operation sequence model use fee forward neural network result show improvements six five bleu point top baseline german english english german systems also observe improvements compare systems use pos tag word cluster train model modify bilingual corpus integrate reorder operations allow us also train sequence sequence neural mt model explicit reorder trigger motivation directly enable reorder information encoder decoder framework otherwise rely solely attention model handle long range reorder try coarser fine grain reorder operations however experiment yield improvements baseline neural mt systems
improve automatic correction grammatical orthographic collocation errors text use multilayer convolutional encoder decoder neural network network initialize embeddings make use character n gram information better suit task evaluate common benchmark test data set conll two thousand and fourteen jfleg model substantially outperform prior neural approach task well strong statistical machine translation base systems neural task specific feature train data analysis show superiority convolutional neural network recurrent neural network long short term memory lstm network capture local context via attention thereby improve coverage correct grammatical errors ensembling multiple model incorporate n gram language model edit feature via rescoring novel method become first neural approach outperform current state art statistical machine translation base approach term grammaticality fluency
research summarization mainly drive empirical approach craft systems perform well standard datasets notion information importance remain latent argue establish theoretical model importance advance understand task help improve summarization systems end propose simple rigorous definitions several concepts previously use intuitively summarization redundancy relevance informativeness importance arise single quantity naturally unify concepts additionally provide intuitions interpret propose quantities experiment demonstrate potential framework inform guide subsequent work
traditional chinese medicine tcm influential form medical treatment china surround areas paper propose tcm prescription generation task aim automatically generate herbal medicine prescription base textual symptom descriptions sequence sequence seq2seq model successful deal sequence generation task explore potential end end solution tcm prescription generation task use seq2seq model however experiment show directly apply seq2seq model lead unfruitful result due repetition problem solve problem propose novel decoder coverage mechanism novel soft loss function experimental result demonstrate effectiveness propose approach judge professors excel tcm generate prescriptions rat seventy-three ten show model indeed help prescribe procedure real life
use low dimensional vector space represent word effective many nlp task however work well face problem rare unseen word paper propose leverage knowledge semantic dictionary combination morphological information build enhance vector space get improvement twenty-three state art heidel time system temporal expression recognition obtain large gain name entity recognition ner task semantic dictionary hownet alone also show promise result compute lexical similarity
introduce new formal model base mathematical construct sheaves represent contradictory information textual source model advantage let us identify cause inconsistency b measure strong c something eg suggest ways reconcile inconsistent advice model naturally represent distinction contradictions disagreements base idea represent natural language sentence formulas parameters sit lattices create partial order base predicate share theories build sheaves partial order products lattices stalk degrees disagreement measure existence global local section limitations sheaf approach connections recent work natural language process well topics contextuality physics data fusion topological data analysis epistemology also discuss
paper address problem sentence level sentiment analysis recent years convolution recursive neural network prove effective network architecture sentence level sentiment analysis nevertheless potential drawbacks alleviate weaknesses combine convolution recursive neural network new network architecture addition employ transfer learn large document level label sentiment dataset improve word embed model result model outperform recent convolution recursive neural network beyond model achieve comparable performance state art systems stanford sentiment treebank
word embeddings real value word representations able capture lexical semantics train natural language corpora model propose representations gain popularity recent years issue adequate evaluation method still remain open paper present extensive overview field word embeddings evaluation highlight main problems propose typology approach evaluation summarize sixteen intrinsic methods twelve extrinsic methods describe widely use experimental methods systematize information evaluation datasets discuss key challenge
crisis responders increasingly use social media data digital source information build situational understand crisis situation order design effective response however increase availability data challenge identify relevant information also increase paper present successful automatic approach handle problem message filter informativeness base definition concept draw prior research crisis response experts informative message tag actionable data example people need threats rescue efforts change environment eight categories actionability identify two components informativeness actionability classification package together openly available tool call emina emergent informativeness actionability
motivate project create system people deaf hard hear would use automatic speech recognition asr produce real time text caption speak english person meet hear individuals augment transcript switchboard conversational dialogue corpus overlay word importance annotations numeric score word indicate importance mean dialogue turn demonstrate utility corpus train automatic word importance label model best perform model f score sixty ordinal six class word importance classification task agreement concordance correlation coefficient eight hundred and thirty-nine human annotators agreement score annotators eighty-nine finally discuss intend future applications resource particularly task evaluate asr performance ie create metrics predict asr output caption text usability dhh users better thanword error rate wer
paper review state art semantic change computation one emerge research field computational linguistics propose framework summarize literature identify expound five essential components field diachronic corpus diachronic word sense characterization change model evaluation data data visualization despite potential field review show current study mainly focus testify hypotheses propose theoretical linguistics several core issue remain solve need diachronic corpora languages english need comprehensive evaluation data evaluation comparison construction approach diachronic word sense characterization change model exploration data visualization techniques hypothesis justification
relation detection play crucial role knowledge base question answer kbqa high variance relation expression question traditional deep learn methods follow encode compare paradigm question candidate relation represent vectors compare semantic similarity max average pool operation compress sequence word fix dimensional vectors become bottleneck information paper propose learn attention base word level interactions question relations alleviate bottleneck issue similar traditional model question relation firstly represent sequence vectors instead merge sequence single vector pool operation soft alignments word question relation learn align word subsequently compare convolutional neural network cnn comparison result merge finally perform comparison low level representations attention base word level interaction model abwim relieve information loss issue cause merge sequence fix dimensional vector comparison experimental result relation detection simplequestions webquestions datasets show abwim achieve state art accuracy demonstrate effectiveness
present report summarize exploratory study carry context cost action is1310 reassemble republic letter one thousand, five hundred one thousand, eight hundred relevant activities work group three texts topics work group two people network study investigate use natural language process nlp network text analysis small sample seventeenth century letter select hartlib paper whose record one catalogue early modern letter online emlo whose online edition available website humanities research institute university sheffield http wwwhrionlineacuk hartlib outline nlp pipeline use automatically process texts network representation order identify texts narrative centrality ie central entities texts relations
goal ner task classify proper nouns text class person location organization important preprocessing step many nlp task question answer summarization although many research study conduct area english state art ner systems reach performances higher ninety percent term f1 measure research study task persian one main important cause may lack standard persian ner dataset train test ner systems research create standard big enough tag persian ner dataset distribute free research purpose order construct standard dataset study standard ner datasets construct english research find almost datasets construct use news texts collect document ten news websites later order provide annotators guidelines tag document study guidelines use construct conll muc standard english datasets set guidelines consider persian linguistic rule
show generate english wikipedia article approach multi document summarization source document use extractive summarization coarsely identify salient information neural abstractive model generate article abstractive model introduce decoder architecture scalably attend long sequence much longer typical encoder decoder architectures use sequence transduction show model generate fluent coherent multi sentence paragraph even whole wikipedia article give reference document show extract relevant factual information reflect perplexity rouge score human evaluations
compositional vector space model mean promise new solutions stubborn language understand problems paper make two contributions toward end use automatically extract paraphrase examples source supervision train compositional model replace previous work rely manual annotations use purpose ii develop context aware model score phrasal compositionality experimental result indicate multiple source information use learn partial semantic supervision match previous techniques intrinsic evaluation task approach also evaluate impact machine translation system show improvements translation quality demonstrate compositionality interpretation correlate compositionality translation
many natural language process task solely rely sparse dependencies tokens sentence soft attention mechanisms show promise performance model local global dependencies soft probabilities every two tokens effective efficient apply long sentence contrast hard attention mechanisms directly select subset tokens difficult inefficient train due combinatorial nature paper integrate soft hard attention one context fusion model reinforce self attention resa mutual benefit resa hard attention trim sequence soft self attention process soft attention feed reward signal back facilitate train hard one purpose develop novel hard attention call reinforce sequence sample rss select tokens parallel train via policy gradient use two rss modules resa efficiently extract sparse dependencies pair select tokens finally propose rnn cnn free sentence encode model reinforce self attention network resan solely base resa achieve state art performance stanford natural language inference snli sentence involve compositional knowledge sick datasets
converse chatbots humans typically tend ask many question significant portion answer refer large scale knowledge graph kg question answer qa dialog systems study independently need study closely evaluate real world scenarios face bots involve task towards end introduce task complex sequential qa combine two task answer factual question complex inferencing realistic size kg millions entities ii learn converse series coherently link qa pair labor intensive semi automatic process involve house crowdsourced workers create dataset contain around 200k dialogs total 16m turn unlike exist large scale qa datasets contain simple question answer single tuple question dialogs require larger subgraph kg specifically dataset question require logical quantitative comparative reason well combinations call model parse complex natural language question ii use conversation context resolve coreferences ellipsis utterances iii ask clarifications ambiguous query finally iv retrieve relevant subgraphs kg answer question however experiment combination state art dialog qa model show clearly achieve objectives inadequate deal complex real world settings believe new dataset couple limitations exist model report paper encourage research complex sequential qa
paper introduce new open source platform end end speech process name espnet espnet mainly focus end end automatic speech recognition asr adopt widely use dynamic neural network toolkits chainer pytorch main deep learn engine espnet also follow kaldi asr toolkit style data process feature extraction format recipes provide complete setup speech recognition speech process experiment paper explain major architecture software platform several important functionalities differentiate espnet open source asr toolkits experimental result major asr benchmarks
lot recent success natural language process nlp drive distribute vector representations word train large amount text unsupervised manner representations typically use general purpose feature word across range nlp problems however extend success learn representations sequence word sentence remain open problem recent work explore unsupervised well supervise learn techniques different train objectives learn general purpose fix length sentence representations work present simple effective multi task learn framework sentence representations combine inductive bias diverse train objectives single model train model several data source multiple train objectives one hundred million sentence extensive experiment demonstrate share single recurrent sentence encoder across weakly relate task lead consistent improvements previous methods present substantial improvements context transfer learn low resource settings use learn general purpose representations
recent statistical approach improve robustness scalability speak dialogue systems however despite recent progress domain adaptation reliance domain data still limit cross domain scalability paper argue problem address extend current model reflect exploit multi dimensional nature human dialogue present multi dimensional statistical dialogue management framework transferable conversational skills learn separate domain independent dimension communication use multi agent reinforcement learn initial experiment simulate user show speed learn process transfer learn policies
article describe experiment neural machine translation use recent tensor2tensor framework transformer sequence sequence model vaswani et al two thousand and seventeen examine critical parameters affect final translation quality memory usage train stability train time conclude experiment set recommendations fellow researchers addition confirm general mantra data larger model address scale multiple gpus provide practical tip improve train regard batch size learn rate warmup step maximum sentence length checkpoint average hope observations allow others get better result give particular hardware data constraints
unsupervised discovery acoustic tokens audio corpora without annotation learn vector representations tokens widely study although techniques show successful applications query example speak term detection std lack map relationships discover tokens real phonemes limit stream applications paper represent probably first attempt towards goal completely unsupervised phoneme recognition map audio signal phoneme sequence without phoneme label audio data basic idea cluster embed acoustic tokens learn map cluster sequence unknown phoneme sequence generative adversarial network gin unsupervised phoneme recognition accuracy thirty-six achieve preliminary experiment
user machine interaction crucial information retrieval especially speak content retrieval speak content difficult browse speech recognition high degree uncertainty interactive retrieval machine take different action interact user obtain better retrieval result critical select efficient action previous work deep q learn techniques propose train interactive retrieval system rely hand craft user simulator build reliable user simulator difficult paper improve interactive speak content retrieval framework propose learnable user simulator jointly train interactive retrieval system make hand craft user simulator unnecessary experimental result show learn simulate users achieve larger reward hand craft ones act like real users
read comprehension widely study one representative read comprehension task stanford question answer dataset squad machine already comparable human hand access large collections multimedia speak content much difficult time consume plain text content humans therefore highly attractive develop machine automatically understand speak content paper propose new listen comprehension task speak squad new task find speech recognition errors catastrophic impact machine comprehension several approach propose mitigate impact
present marian efficient self contain neural machine translation framework integrate automatic differentiation engine base dynamic computation graph marian write entirely c describe design encoder decoder framework demonstrate research friendly toolkit achieve high train translation speed
past years huge growth twitter sentiment analysis already provide fair amount research sentiment detection public opinion among twitter users give fact twitter message generate constantly dizzy rat huge volume stream data create thus imperative need accurate methods knowledge discovery mine information although exist plethora twitter sentiment analysis methods recent literature researchers shift real time sentiment identification twitter stream data expect major challenge deal big data challenge arise twitter stream applications concern volume velocity perspective paper methodological approach base open source tool provide real time detection change sentiment ultra efficient respect memory consumption computational cost achieve iteratively collect tweet real time discard immediately process purpose employ lexicon approach sentiment characterizations change detection achieve appropriate control chart require historical information believe propose methodology provide trigger potential large scale monitor thread attempt discover fake news spread propaganda efforts early stag experimental real time analysis base recent hashtag provide evidence propose approach detect meaningful sentiment change across hashtags lifetime
paper describe nihrio system semeval two thousand and eighteen task three irony detection english tweet propose use simple neural network architecture multilayer perceptron various type input feature include lexical syntactic semantic polarity feature system achieve high performance subtasks binary multi class irony detection tweet particular rank third use accuracy metric fifth use f1 metric code available https githubcom nihrio ironydetectionintwitter
grammar check task detection correction grammatical errors text english dominate language field science technology therefore non native english speakers must able use correct english grammar read write speak generate need automatic grammar check tool far many approach propose implement less efforts make survey literature past decade objective systematic review examine exist literature highlight current issue suggest potential directions future research systematic review result analysis twelve primary study obtain design search strategy select paper find web also present possible scheme classification grammar errors among main observations find lack efficient robust grammar check tool real time applications present several useful illustrations prominent schematic diagram provide approach table summarize approach along different dimension target error type linguistic dataset use strengths limitations approach facilitate better understandability comparison evaluation previous research
distributional data tell us man swallow candy man swallow paintball since never attest however physically plausible events paper introduce task semantic plausibility recognize plausible possibly novel events present new crowdsourced dataset semantic plausibility judgments single events man swallow paintball simple model base distributional representations perform poorly task despite well selection preference inject manually elicit knowledge entity properties provide substantial performance boost error analysis show new dataset great testbed semantic plausibility model sophisticate knowledge representation propagation could address many remain errors
social media platforms twitter facebook become popular multilingual societies trend induce portmanteau south asian languages english blend multiple languages code mix data recently become popular research communities various nlp task code mix data consist anomalies grammatical errors spell variations paper leverage contextual property word different spell variation word share similar context large noisy social media text capture different variations word belong context unsupervised manner use distribute representations word experiment reveal preprocessing code mix dataset base approach improve performance state art part speech tag pos tag sentiment analysis task
machine learn approach sentiment analysis principally rely abundance resources limit dependence propose novel method call siamese network architecture sentiment analysis snasa learn representations resource poor languages jointly train resource rich languages use siamese network snasa model consist twin bi directional long short term memory recurrent neural network bi lstm rnn share parameters join contrastive loss function base similarity metric model learn sentence representations resource poor resource rich language common sentiment space use similarity metric base individual sentiments model hence project sentence similar sentiment closer sentence different sentiment farther experiment large scale datasets resource rich languages english spanish resource poor languages hindi telugu reveal snasa outperform state art sentiment analysis approach base distributional semantics semantic rule lexicon list deep neural network representations without sh
code mix data important challenge natural language process characteristics completely vary traditional structure standard languages paper propose novel approach call sentiment analysis code mix text sacmt classify sentence correspond sentiment positive negative neutral use contrastive learn utilize share parameters siamese network map sentence code mix standard languages common sentiment space also introduce basic cluster base preprocessing method capture variations code mix transliterate word experiment reveal sacmt outperform state art approach sentiment analysis code mix text seventy-six accuracy one hundred and one f score
recently implicit representation model embed deep learn successfully adopt text classification task due outstanding performance however approach limit small moderate scale text classification explicit representation model often use large scale text classification like open directory project odp base text classification however performance model limit associate knowledge base paper incorporate word embeddings odp base large scale classification end first generate category vectors represent semantics odp categories jointly model word embeddings odp base text classification propose novel semantic similarity measure utilize category word vectors obtain joint model word embeddings respectively evaluation result clearly show efficacy methodology large scale text classification propose scheme exhibit significant improvements ten twenty-eight term macro average f1 score precision k respectively state art techniques
yorub widely speak west african language write system rich tonal orthographic diacritics exceptions diacritics omit electronic texts due limit device application support diacritics provide morphological information crucial lexical disambiguation pronunciation vital yorub text speech tts automatic speech recognition asr natural language process nlp task reframing automatic diacritic restoration adr machine translation task experiment two different attentive sequence sequence neural model process undiacritized text evaluation dataset approach produce diacritization error rat less five release pre train model datasets source code open source project advance efforts yorub language technology
convolutional neural network computer vision fairly intuitive typical cnn use image classification first layer learn edge follow layer learn filter identify object cnns natural language process use often completely intuitive good idea convolution filter learn task text classification propose neural network structure able give good result less time use convolutional neural network predict primary broader topic question use separate network predict topics accurately classify sub topics
provide better access inventory buyers better search engine optimization e commerce websites automatically generate millions easily searchable browse page browse page consist set slot name value pair within give category group multiple items share characteristics browse page require title describe content page since number browse page huge manual creation title infeasible previous statistical neural approach depend heavily availability large amount data language research apply sequence sequence model generate title high low resourced languages leverage transfer learn train model multi lingual data thereby create one joint model generate title various different languages performance title generation system evaluate three different languages english german french particular focus low resourced french language
statistical machine translation smt systems perform poorly apply new target domains goal explore domain adaptation approach techniques improve translation quality domain specific smt systems however translate texts specific domain eg medicine full challenge first challenge ambiguity word phrase contain different mean different contexts second one language style due fact texts different genres always present different syntax length structural organization third one vocabulary word oovs problem domain train data often scarce low terminology coverage thesis explore state art domain adaptation approach propose effective solutions address problems
although increase significant tie china portuguese speak countries much parallel corpora chinese portuguese language pair languages populous twelve billion native chinese speakers two hundred and seventy-nine million native portuguese speakers language pair however could consider low resource term available parallel corpora paper describe methods curate chinese portuguese parallel corpora evaluate quality extract bilingual data macao government websites propose hierarchical strategy build large parallel corpus experiment conduct exist corpora use phrase base machine translation pbmt state art neural machine translation nmt model result work use benchmark future chinese portuguese mt systems approach use paper also show good example boost performance mt systems low resource language pair
emergence knowledge graph scholarly communication domain recent advance artificial intelligence natural language process bring us closer scenario intelligent systems assist scientists range knowledge intensive task paper present experimental result generation word embeddings scholarly publications intelligent process scientific texts extract scigraph compare performance domain specific embeddings exist pre train vectors generate large general purpose corpora result suggest trade corpus specificity volume embeddings domain specific scientific corpora effectively capture semantics domain hand obtain comparable result general corpora also achieve presence large corpora well form text furthermore also show degree overlap knowledge areas directly relate performance embeddings domain evaluation task
propose new approach chinese word segmentation problem consider sentence undirected graph whose nod character one use various techniques compute edge weight measure connection strength character spectral graph partition algorithms use group character achieve word segmentation follow graph partition approach design several unsupervised algorithms show inspire segmentation result two corpora one electronic health record chinese two benchmark data second international chinese word segmentation bakeoff
introduction emojis emoticons social media platforms give users increase potential expression propose novel method call classification emojis use siamese network architecture cesna learn emoji base representations resource poor languages jointly train resource rich languages use siamese network cesna model consist twin bi directional long short term memory recurrent neural network bi lstm rnn share parameters join contrastive loss function base similarity metric model learn representations resource poor resource rich language common emoji space use similarity metric base emojis present sentence languages model hence project sentence similar emojis closer sentence different emojis farther one another experiment large scale twitter datasets resource rich languages english spanish resource poor languages hindi telugu reveal cesna outperform state art emoji prediction approach base distributional semantics semantic rule lexicon list deep neural network representations without share parameters
reliably detect relevant relations entities unstructured text valuable resource knowledge extraction awaken significant interest field natural language process paper present system relation classification extraction base ensemble convolutional recurrent neural network rank first three four subtasks semeval two thousand and eighteen task seven provide detail explanations ground design choices behind relevant feature analyze importance
literature around text classification treat supervise learn problem give corpus label document train classifier accurately predict class unseen document industry however uncommon business entire corpora document none classify exist classifications become meaningless web content example poor taxonomy management result label apply indiscriminately make filter label unhelpful work aim make possible classify entire corpus unlabeled document use human loop approach content owner manually classify one two document per category rest automatically classify shoot learn approach require rich representations document manually label treat prototypes automatic classification rest simple case measure distance prototypes approach use pre train word embeddings document represent use simple weight average constituent word embeddings test accuracy approach exist label datasets provide result also make code available reproduce result get twenty newsgroups dataset
paper describe enrichment ontosensenet verb centric lexical resource indian languages resource contain newly develop telugu telugu dictionary important native speakers better annotate sense word mean telugu hence efforts make develop soft copy telugu dictionary resource also manually annotate gold standard corpus consist eight thousand, four hundred and eighty-three verbs two hundred and fifty-three adverbs one thousand, six hundred and seventy-three adjectives annotations do native speakers accord define annotation guidelines paper provide overview annotation procedure present validation resource inter annotator agreement concepts sense class sense type discuss additionally discuss potential lexical sense annotate corpora improve word sense disambiguation wsd task telugu wordnet crowd source annotation individual word synsets compare develop sense annotate lexicon ontosensenet examine improvement also present special categorization spatio temporal classification adjectives
short note describe chart parser multimodal type logical grammars develop conjunction type logical treebank french chart parser present incomplete fast implementation proof search multimodal type logical grammars use deductive parse framework proof find transform natural deduction proof
present two neural model event factuality prediction yield significant performance gain previous model three event factuality datasets factbank uw meantime also present substantial expansion happen portion universal decompositional semantics dataset yield largest event factuality dataset date report model result extend factuality dataset well
conventional seq2seq chatbot model try find sentence highest probabilities condition input sequence without consider sentiment output sentence research work try modify sentiment output sequence report paper propose five model scale adjust sentiment chatbot response persona base model reinforcement learn plug play model sentiment transformation network cyclegan base conventional seq2seq model also develop two evaluation metrics estimate responses reasonable give input metrics together two popularly use metrics use analyze performance five propose model different aspects reinforcement learn cyclegan show attractive evaluation metrics also find well correlate human evaluation
highlight several issue evaluation historical text normalization systems make hard tell well systems would actually work practice ie new datasets languages comparison nai systems preprocessing step downstream nlp tool illustrate issue exemplify propose evaluation practice compare two neural model nai baseline system show neural model generalize well unseen word test five languages nevertheless provide clear benefit nai baseline downstream pos tag english historical collection conclude future work include rigorous evaluation include intrinsic extrinsic measure possible
one difficulties neural machine translation nmt recall appropriate translation low frequency word phrase paper propose simple fast effective method recall previously see translation examples incorporate nmt decode process specifically input sentence use search engine retrieve sentence pair whose source side similar input sentence collect n grams retrieve target sentence align word match source sentence call translation piece compute pseudo probabilities retrieve sentence base similarities input sentence retrieve source sentence use weight retrieve translation piece finally exist nmt model use translate input sentence additional bonus give output contain collect translation piece show method improve nmt translation result six bleu point three narrow domain translation task repetitiveness target sentence particularly salient also cause little increase translation time compare favorably another alternative retrieval base method respect accuracy speed simplicity implementation
describe initial work extension kaldi toolkit support weight finite state transducer wfst decode graphics process units gpus implement token recombination atomic gpu operation order fully parallelize viterbi beam search propose dynamic load balance strategy efficient token pass schedule among gpu thread also redesign exact lattice generation lattice prune algorithms better utilization gpus experiment switchboard corpus show propose method achieve identical one best result lattice quality recognition confidence measure task run three fifteen time faster single process kaldi decoder result report different gpu architectures additionally obtain forty-six fold speedup sequence parallelism multi process service mps gpu
word sense induction wsi address polysemy unsupervised discovery multiple word sense resolve ambiguities downstream nlp task also make word representations interpretable paper propose accurate efficient graph base method wsi build global non negative vector embed basis interpretable like topics cluster basis index ego network polysemous word adopt distributional inclusion vector embeddings basis formation model avoid expensive step nearest neighbor search plague graph base methods without sacrifice quality sense cluster experiment three datasets show propose method produce similar better sense cluster embeddings compare previous state art methods significantly efficient
paper consider online news censorship concentrate censorship identities obfuscate identities may occur disparate reason military judiciary ones majority case happen protect individuals identify persecute hostile people however collaborative web characterise redundancy information unusual fact report multiple source may apply restriction policies term censorship also prove aptitude social network users disclose personal information lead phenomenon comment news reveal data withhold news give us mean figure subject censor news propose adaptation text analysis approach unveil censor identities approach test synthesise scenario however resemble real use case leverage text analysis base context classifier train snippets post comment facebook page achieve promise result despite quite constrain settings operate consider snippets short length system successfully detect censor name choose among ten different candidate name fifty investigate case outperform result two reference baselines find report paper support thorough experimental methodology interest also pave way investigation insidious issue censorship web
today scenario imagine world without negativity something unrealistic bad news spread virally good ones though seem impractical real life could implement build system use machine learn natural language process techniques identify news datum negative shade filter take news positive shade good news end user work around two lakhs datum train test use combination rule base data drive approach vader along filtration method use annotate tool follow statistical machine learn approach use document term matrix representation support vector machine classification deep learn algorithms come picture make system reliable doc2vec finally end convolutional neural networkcnn yield better result experiment modules show train accuracy ninety-six test accuracy internal external news datum eighty-five obtain
neural machine translation significant result still great problem lack absence parallel corpus many languages article suggest method generate considerable amount parallel corpus language pair extract open source materials exist internet parallel corpus content derive video subtitle need set video title attribute like release date rat duration etc process find download subtitle pair desire language pair automate use crawler finally sentence pair extract synchronous dialogues subtitle main problem method unsynchronized subtitle pair therefore subtitle verify download two subtitle synchronize another subtitle video process till find match subtitle use approach give ability make context base parallel corpus filter videos genre context base corpus use complex translators decode sentence different network determine content subject languages many differences formal informal style include word syntax advantage method make corpus informal style languages movies dialogues part conversation informal style feature generate corpus use real time translators accurate conversation translations
express language subjective everyone different style read write apparently boil down way mind understand things specific format language style transfer way preserve mean text change way express progress language style transfer lag behind domains computer vision mainly lack parallel data use case reliable evaluation metrics response challenge lack parallel data explore learn style transfer non parallel data propose model combine seq2seq autoencoders adversarial loss achieve goal key idea behind propose model learn separate content representations style representations use adversarial network consider problem evaluate style transfer task frame problem sentiment transfer evaluation use sentiment classifier calculate many sentiments model able transfer report result several kinds model
investigate whether multi task learn mtl improve performance nlp problems relate argumentation mine particular argument component identification result show mtl perform particularly well better single task learn little train data available main task common scenario find challenge previous assumptions conceptualizations across datasets divergent mtl difficult semantic higher level task
supervise train abstractive language generation model result learn conditional probabilities language sequence base supervise train signal train signal contain variety write style model may end learn average style directly influence train data make control need application describe family model architectures capable capture generic language characteristics via share model parameters well particular style characteristics via private model parameters model able generate language accord specific learn style still take advantage power model generic language phenomena furthermore describe extension use mixture output distributions learn style perform fly style adaptation base textual input alone experimentally find propose model consistently outperform model encapsulate single style average style language generation capabilities
versatility word embeddings various applications attract researchers various field however impact hyper parameters train embed model often poorly understand much hyper parameters vector dimension corpus size affect quality embeddings result translate downstream applications use standard embed evaluation metrics datasets conduct study empirically measure impact hyper parameters
unlike previous unknown nouns tag task first attempt focus vocabulary oov lexical evaluation task require prior knowledge oov word word appear test sample goal task provide solutions oov lexical classification prediction task require annotators conclude attribute oov word base relate contexts utilize unsupervised word embed methods word2vec word2gm perform baseline experiment categorical classification task oov word attribute prediction task
automatic text summarization automate process shorten text reserve main ideas document critical research area natural language process aim literature review survey recent work neural base model automatic text summarization examine detail ten state art neural base summarizers five abstractive model five extractive model addition discuss relate techniques apply summarization task present promise paths future research neural base summarization
keep dialogue state dialogue systems notoriously difficult task introduce ontology base dialogue manageontodm dialogue manager keep state conversation provide basis anaphora resolution drive conversation via domain ontologies bank finance area promise great potential disambiguate context via rich set products specificity proper nouns name entities verbs use ontologies knowledge base basis dialogue manager knowledge base component dialogue manager components coalesce sense domain knowledge use track entities interest ie nod class ontology happen products service way also introduce conversation memory attention sense finely blend linguistic methods domain drive keyword rank domain ontologies create ways domain drive conversation propose framework use house german language bank finance chatbots general challenge german language process finance bank domain chatbot language model lexicons also introduce work still progress hence success metrics introduce yet
clinical name entity recognition cner aim identify classify clinical term diseases symptoms treatments exams body part electronic health record fundamental crucial task clinical translational research recent years deep neural network achieve significant success name entity recognition many natural language process nlp task algorithms train end end automatically learn feature large scale label datasets however data drive methods typically lack capability process rare unseen entities previous statistical methods feature engineer practice demonstrate human knowledge provide valuable information handle rare unseen case paper address problem incorporate dictionaries deep neural network chinese cner task two different architectures extend bi directional long short term memory bi lstm neural network five different feature representation scheme propose handle task computational result ccks two thousand and seventeen task two benchmark dataset show propose method achieve highly competitive performance compare state art deep learn methods
neural machine translation achieve level fluency adequacy would surprise short time ago output quality extremely relevant industry purpose however equally important produce result shortest time possible mainly latency sensitive applications control cloud host cost paper show effectiveness translate eight bite quantization model train use thirty-two bite float point value result show eight bite translation make non negligible impact term speed degradation accuracy adequacy
language identification become prerequisite kinds automate text process systems paper present rule base language identifier tool two closely relate indo aryan languages hindi magahi system currently achieve accuracy approx eight thousand, six hundred and thirty-four hope improve future automatic identification languages significant accuracy output web crawlers
study develop keyword spot kws acoustic model components far field speaker system specifically use teacher student learn adapt close talk well train production far field use parallel close talk simulate far field data also use learn compress large size kws model small size one fit device computational cost without need transcription learn well utilize untranscribed data boost model performance adaptation kws model compression optimize model sequence discriminative train live data reach best performance systems adapt improve baseline seven thousand, two hundred and sixty five thousand, seven hundred and sixteen relative word error rate reduction play back live test data respectively final kws model size reduce twenty-seven time large size kws model without lose accuracy
conversations social media often contain use irony sarcasm users say opposite really mean irony markers meta communicative clue inform reader utterance ironic propose thorough analysis theoretically ground irony markers two social media platforms twitter reddit classification frequency analysis show twitter typographic markers emoticons emojis discriminative markers recognize ironic utterances reddit morphological markers eg interjections tag question discriminative
create accurate meta embeddings pre train source embeddings receive attention lately methods base global locally linear transformation concatenation show produce accurate meta embeddings paper show arithmetic mean two distinct word embed set yield performant meta embed comparable better complex meta embed learn methods result seem counter intuitive give vector space different source embeddings comparable simply average give insight average still produce accurate meta embed despite incomparability source vector space
recent high profile cyber attack exemplify organizations need better cyber defenses cyber threats hard accurately predict attackers usually try mask trace however often discuss exploit techniques hack forums community behavior hackers may provide insights group collective malicious activity propose novel approach predict cyber events use sentiment analysis test approach use cyber attack data two major business organizations consider three type events malicious software installation malicious destination visit malicious email surpass target organizations defenses construct predictive signal apply sentiment analysis hacker forum post better understand hacker behavior analyze 400k post generate january two thousand and sixteen january two thousand and eighteen one hundred hack forums surface dark web find forums significantly predictive power others sentiment base model leverage specific forums outperform state art deep learn time series model forecast cyber attack weeks ahead events
many project apply knowledge pattern kps retrieval specialize information yet terminologists still rely manual analysis concordance line extract semantic information since user friendly publicly available applications enable find knowledge rich contexts krcs fill void create kp base ecolexicon semantic sketchgrammar essg well know corpus query system sketch engine first time essg publicly available insketch engine query ecolexicon english corpus additionally reuse essg english corpus upload user enable sketch engine extract krcs codify generic specific part whole location function relations kps domain independent information display form summary list word sketch contain pair term link give semantic relation paper describe process build kp base sketch grammar special focus last stage namely evaluation refinement purpose conduct initial shallow precision recall evaluation sixty-four english sketch grammar rule create far hyponymy meronymy causality precision measure base random sample concordances extract word sketch type recall assess base random sample concordances know term pair find result necessary improvement refinement essg noise false positives help specify rule whereas silence false negative allow us find useful new pattern
present two novel datasets low resource language vietnamese assess model semantic similarity vicon comprise pair synonyms antonyms across word class thus offer data distinguish similarity dissimilarity visim four hundred provide degrees similarity across five semantic relations rat human judge two datasets verify standard co occurrence neural network model show result comparable respective english datasets
introduce fully differentiable approximation higher order inference coreference resolution approach use antecedent distribution span rank architecture attention mechanism iteratively refine span representations enable model softly consider multiple hop predict cluster alleviate computational cost iterative process introduce coarse fine approach incorporate less accurate efficient bilinear factor enable aggressive prune without hurt accuracy compare exist state art span rank approach model significantly improve accuracy english ontonotes benchmark far computationally efficient
contextual knowledge important element understand language contextual knowledge mean general knowledge discourse knowledge ie knowledge situational context background knowledge co textual context ten paper discuss importance contextual knowledge understand humor present cartoon base amul advertisements indiain process analyze advertisements also see humor effective tool advertise thereby marketingthese bilingual advertisements also expect audience appropriate linguistic knowledge include knowledge english hindi vocabulary morphology syntax different techniques like pun portmanteaus parody popular proverbs expressions acronyms famous dialogues songs etc employ convey message humorous way present study concentrate linguistic cue require context understand wit humor
semeval two thousand and eighteen task seven focus relation ex traction classification scientific literature work present tree base lstm network share task approach place 9th twenty-eight subtask eleven relation classification 5th twenty subtask twelve relation classification noisy entities also provide ablation study feature include input network
evaluate performance state art algorithms automatic cognate detection compare useful automatically infer cognates task phylogenetic inference compare classical manually annotate cognate set find suggest phylogenies infer automate cognate set come close phylogenies infer expert annotate ones although average latter still superior conclude future work phylogenetic reconstruction profit much automatic cognate detection especially scholars merely interest explore bigger picture language family phylogeny algorithms automatic cognate detection useful complement current research language phylogenies
combine neural image captioner rational speech act rsa model make system pragmatically informative objective produce caption merely true also distinguish input similar image previous attempt combine rsa neural image caption require inference normalize entire set possible utterances pose serious problem efficiency previously solve sample small subset possible utterances instead solve problem implement version rsa operate level character abc unroll caption find utterance level effect referential caption obtain character level decisions finally introduce automatic method test performance pragmatic speaker model show model outperform non pragmatic baseline well word level rsa captioner
paper address problem community membership detection use text feature scenario small number positive label examples define community solution introduce unsupervised proxy task learn user embeddings user identification experiment sixteen different communities show result embeddings effective community membership identification common unsupervised representations
recent work show effectiveness word representations feature significantly improve supervise ner english language study investigate whether word representations also boost supervise ner arabic use word representations additional feature conditional random field crf model systematically compare three popular neural word embed algorithms skip gram cbow glove six different approach integrate word representations ner system experimental result show brown cluster achieve best performance among six approach concern word embed feature cluster embed feature outperform embed feature distributional prototypes produce second best result moreover combination brown cluster word embed feature provide additional improvement nearly ten f1 score baseline
neural abstractive summarization model lead promise result summarize relatively short document propose first model abstractive summarization single longer form document eg research paper approach consist new hierarchical encoder model discourse structure document attentive discourse aware decoder generate summary empirical result two large scale datasets scientific paper show model significantly outperform state art model
article present multivariate model determine different syntactic semantic form surface structure process underlie comprehension simple phrase model apply eeg signal record read task result show hierarchical precedence neurolinguistic process form syntactic lastly semantic process also find verbs heart phrase syntax process b interaction syntactic movement within phrase semantic process derive person center reference frame eigenvectors multivariate model provide electrode time profile separate distinctive linguistic process highlight interaction accordance find different linguistic theories discuss
paper describe experiment automatically identify native accent speech sample non native english speakers use low level audio feature n gram feature manual transcriptions use publicly available non native speech corpus simple audio feature representations perform word phoneme recognition show possible achieve close ninety classification accuracy task character n grams perform similar speech feature show speech feature affect prompt variation whereas ngrams since approach follow easily adapt language provide enough train data believe result provide useful insights development accent recognition systems study accent context language learn
self train useful strategy semi supervise learn leverage raw texts enhance model performances traditional self train methods depend heuristics model confidence instance selection manual adjustment expensive address challenge propose deep reinforcement learn method learn self train strategy automatically base neural network representation sentence model automatically learn optimal policy instance selection experimental result show approach outperform baseline solutions term better tag performances stability
paper describe post evaluation result semeval two thousand and eighteen task seven clas sification semantic relations scientific literature clean subtask eleven noisy data subtask twelve extend ver sion workshop paper hettinger et al two thousand and eighteen include technical detail sec tions thirty-two forty-three change make preprocessing step post evaluation phase section twenty-one due change classification relations use embeddings claire achieve improve f1 score seven thousand, five hundred and eleven first subtask eight thousand, one hundred and forty-four second
paper present study neologisms loan word frequently occur facebook user post analyze dataset several million publically available post write two thousand and six two thousand and thirteen russian speak facebook users build vocabulary frequent lemmatized word miss opencorpora dictionary assumption many word enter common use recently assumption certainly true word extract way reason manually filter automatically obtain list order exclude non russian incorrectly lemmatized word well word record dictionaries occur texts russian national corpus result list one hundred and sixty-eight word potentially consider neologisms present attempt etymological classification neologisms unsurprisingly recently borrow english also quite new word compose previously borrow stem identify various derivational pattern also classify word several large thematic areas internet market multimedia among largest number word believe together word base collect process serve start point study neologisms lexical process lead acceptance mainstream language
code switch phenomenon mix grammatical structure two languages vary social constraints code switch data differ radically benchmark corpora use nlp community application standard technologies data degrade performance sharply unlike standard corpora data often need go additional process language identification normalization back transliteration efficient process paper investigate indispensable process problems associate syntactic parse code switch data propose methods mitigate effect particular study dependency parse code switch data hindi english multilingual speakers twitter present treebank hindi english code switch tweet universal dependencies scheme propose neural stack model parse efficiently leverage part speech tag syntactic tree annotations code switch treebank preexist hindi english treebanks also present normalization back transliteration model decode process tailor code switch data result show neural stack parser fifteen las point better augment parse model decode process improve result thirty-eight las point first best normalization back transliteration
argue semantic mean sentence clause interpret independently rest paragraph independently discourse relations overall paragraph level discourse structure goal improve implicit discourse relation classification introduce paragraph level neural network model inter dependencies discourse units well discourse relation continuity pattern predict sequence discourse relations paragraph experimental result show model outperform previous state art systems benchmark corpus pdtb
previously neural methods grammatical error correction gec reach state art result compare phrase base statistical machine translation smt baselines demonstrate parallel neural gec low resource neural mt successfully adapt several methods low resource mt neural gec establish guidelines trustable result neural gec propose set model independent methods neural gec easily apply gec settings propose methods include add source side noise domain adaptation techniques gec specific train objective transfer learn monolingual data ensembling independently train gec model language model combine effect methods result better state art neural gec model outperform previously best neural gec systems ten m2 conll two thousand and fourteen benchmark fifty-nine jfleg test set non neural state art systems outperform two conll two thousand and fourteen benchmark four jfleg
combine two popular approach automate grammatical error correction gec gec base statistical machine translation smt gec base neural machine translation nmt hybrid system achieve new state art result conll two thousand and fourteen jfleg benchmarks gec system preserve accuracy smt output time generate fluent sentence typical nmt analysis show create systems closer reach human level performance gec system report far
investigate effect various dependency base word embeddings distinguish functional domain similarity word similarity rank two downstream task english variations include word embeddings train use context windows stanford universal dependencies several level enhancement range unlabeled enhance dependencies result compare basic linear contexts evaluate several datasets find embeddings train universal stanford dependency contexts excel different task enhance dependencies often improve performance
present new approach learn semantic parsers multiple datasets even target semantic formalisms drastically different underlie corpora overlap handle disjoint data treat annotations unobserved formalisms latent structure variables build state art baselines show improvements frame semantic parse semantic dependency parse model jointly
dependency parse research make significant gain recent years typically focus improve accuracy single tree predictions however ambiguity inherent natural language syntax communicate ambiguity important error analysis better inform downstream applications work propose transition sample algorithm sample full joint distribution parse tree define transition base parse model demonstrate use sample probabilistic dependency analysis first define new task dependency path prediction infer syntactic substructures part sentence provide first analysis performance task second demonstrate usefulness monte carlo syntax marginal method parser error analysis calibration finally use method propagate parse uncertainty two downstream information extraction applications identify persons kill police semantic role assignment
morphological segmentation polysynthetic languages challenge word may consist many individual morphemes train data extremely scarce since neural sequence sequence seq2seq model define state art morphological segmentation high resource settings mostly european languages first show also obtain competitive performance mexican polysynthetic languages minimal resource settings propose two novel multi task train approach one one without need external unlabeled resources two correspond data augmentation methods improve neural baseline languages finally explore cross lingual transfer third way fortify neural model show train one single multi lingual model relate languages maintain comparable even improve performance thus reduce amount parameters close seventy-five provide morphological segmentation datasets mexicanero nahuatl wixarika yorem nokki future research
latent tree learn model learn parse sentence without syntactic supervision use parse build sentence representation exist work model show perform well task like sentence classification learn grammars conform plausible semantic syntactic formalism williams et al 2018a study parse ability model natural language challenge due inherent complexities natural language like several valid parse single sentence paper introduce listops toy dataset create study parse ability latent tree model listops sequence style prefix arithmetic dataset design single correct parse strategy system need learn succeed task show current lead latent tree model unable learn parse succeed listops model achieve accuracies worse purely sequential rnns
co train popular semi supervise learn framework utilize large amount unlabeled data addition small label set co train methods exploit predict label unlabeled data select sample base prediction confidence augment train however selection sample exist co train methods base predetermine policy ignore sample bias unlabeled label subsets fail explore data space paper propose novel method reinforce co train select high quality unlabeled sample better co train specifically approach use q learn learn data selection policy small label dataset exploit policy train co train classifiers automatically experimental result clickbait detection generic text classification task demonstrate propose method obtain accurate text classification result
propose syntactically control paraphrase network scpns use generate adversarial examples give sentence target syntactic form eg constituency parse scpns train produce paraphrase sentence desire syntax show possible create train data task first backtranslation large scale use parser label syntactic transformations naturally occur process data allow us train neural encoder decoder model extra input specify target syntax combination automate human evaluations show scpns generate paraphrase follow target specifications without decrease paraphrase quality compare baseline uncontrolled paraphrase systems furthermore capable generate syntactically adversarial examples one fool pretrained model two improve robustness model syntactic variation use augment train data
paper describe best perform system semeval two thousand and eighteen affect tweet english sub task system focus ordinal classification regression sub task valence emotion ordinal classification valence classify seven different class range three three whereas emotion classify four different class zero three separately emotion namely anger fear joy sadness regression sub task estimate intensity valence emotion system perform domain adaptation four different model create ensemble give final prediction propose system achieve 1st position seventy-five team participate fore mention sub task outperform baseline model margins range four hundred and ninety-two seven hundred and sixty-four thus push state art significantly
prerequisite train corpus base machine translation mt systems either statistical mt smt neural mt nmt availability high quality parallel data arguably important today ever nmt show many study outperform smt mostly large parallel corpora available case data limit smt still outperform nmt recently researchers show back translate monolingual data use create synthetic parallel corpora turn use combination authentic parallel data train high quality nmt system give large collections new parallel text become available quite rarely backtranslation become norm build state art nmt systems especially resource poor scenarios however assert many unknown factor regard actual effect back translate data translation capabilities nmt model accordingly work investigate use back translate data train corpus separate standalone dataset well combine human generate parallel data affect performance nmt model use incrementally larger amount back translate data train range nmt systems german english analyse result translation performance
performance neural machine translation nmt systems often suffer low resource scenarios sufficiently large scale parallel corpora obtain pre train word embeddings prove invaluable improve performance natural language analysis task often suffer paucity data however utility nmt extensively explore work perform five set experiment analyze expect pre train word embeddings help nmt task show embeddings surprisingly effective case provide gain twenty bleu point favorable set
course description provide instructors essential piece information define expect instructor go deliver particular course one key components course description learn objectives section content section use program managers task compare match two different course development transfer agreements various institutions research introduce development semantic similarity algorithms calculate similarity two learn objectives domain present novel methodology deal semantic similarity use previously establish algorithm integrate domain corpus utilize domain statistics disambiguate domain serve supervise learn data algorithm also introduce bloom index calculate similarity action verbs learn objectives refer bloom taxonomy
core step statistical data text generation concern learn correspondences structure data representations eg facts database associate texts paper aim bootstrap generators large scale datasets data eg dbpedia facts relate texts eg wikipedia abstract loosely align tackle challenge task introduce special purpose content selection mechanism use multi instance learn automatically discover correspondences data text pair show use enhance content signal train encoder decoder architecture experimental result demonstrate model train content specific objectives improve upon vanilla encoder decoder solely rely soft attention
consider task text attribute transfer transform sentence alter specific attribute eg sentiment preserve attribute independent content eg change screen right size screen small train data include sentence label attribute eg positive negative pair sentence differ attribute must learn disentangle attribute attribute independent content unsupervised way previous work use adversarial methods struggle produce high quality output paper propose simpler methods motivate observation text attribute often mark distinctive phrase eg small strongest method extract content word delete phrase associate sentence original attribute value retrieve new phrase associate target attribute use neural model fluently combine final output human evaluation best method generate grammatical appropriate responses twenty-two input best previous system average three attribute transfer datasets alter sentiment review yelp alter sentiment review amazon alter image caption romantic humorous
alzheimer disease ad irreversible progressive brain disease stop slow medical treatment language change serve sign patient cognitive function impact potentially lead early diagnosis work use nlp techniques classify analyze linguistic characteristics ad patients use dementiabank dataset apply three neural model base cnns lstm rnns combination distinguish language sample ad control patients achieve new independent benchmark accuracy ad classification task importantly next interpret neural model learn linguistic characteristics ad patients via analysis base activation cluster first derivative saliency techniques perform novel automatic pattern discovery inside activation cluster consolidate ad patients distinctive grammar pattern additionally show first derivative saliency rediscover previous language pattern ad patients also would light limitations neural model lastly also include analysis gender separate ad data
show many publish model stanford question answer dataset rajpurkar et al two thousand and sixteen lack robustness suffer fifty decrease f1 score adversarial evaluation base addsent jia liang two thousand and seventeen algorithm also show retrain model data generate addsent limit effect robustness propose novel alternative adversary generation algorithm addsentdiverse significantly increase variance within adversarial train data provide effective examples punish model make certain superficial assumptions order improve robustness addsent semantic perturbations eg antonyms jointly improve model semantic relationship learn capabilities addition addsentdiverse base adversarial train data augmentation additions show make state art model significantly robust achieve three hundred and sixty-five increase f1 score many different type adversarial evaluation maintain performance regular squad task
recently neural machine translation nmt emerge powerful alternative conventional statistical approach however performance drop considerably presence morphologically rich languages mrls neural engines usually fail tackle large vocabulary high vocabulary oov word rate mrls therefore suitable exploit exist word base model translate set languages paper propose extension state art model chung et al two thousand and sixteen work character level boost decoder target side morphological information architecture additional morphology table plug model time decoder sample target vocabulary table send auxiliary signal relevant affix order enrich decoder current state constrain provide better predictions evaluate model translate english german russian turkish three mrls observe significant improvements
work present hybrid learn method train task orient dialogue systems online user interactions popular methods learn task orient dialogues include apply reinforcement learn user feedback supervise pre train model efficiency learn method may suffer mismatch dialogue state distribution offline train online interactive learn stag address challenge propose hybrid imitation reinforcement learn method dialogue agent effectively learn interaction users learn human teach feedback design neural network base task orient dialogue agent optimize end end propose learn method experimental result show end end dialogue agent learn effectively mistake make via imitation learn user teach apply reinforcement learn user feedback imitation learn stage improve agent capability successfully complete task
propose framework extend synchronic polysemy annotation diachronic change lexical mean counteract lack resources evaluate computational model lexical semantic change framework exploit intuitive notion semantic relatedness distinguish innovative reductive mean change high inter annotator agreement result test set german comprise rat five annotators relatedness one thousand, three hundred and twenty use pair across twenty-two target word
aspect level sentiment classification aim identify sentiment express towards aspects give context sentence paper introduce attention attention aoa neural network aspect level sentiment classification approach model aspects sentence joint way explicitly capture interaction aspects context sentence aoa module model jointly learn representations aspects sentence automatically focus important part sentence experiment laptop restaurant datasets demonstrate approach outperform previous lstm base architectures
end end nature neural machine translation nmt remove many ways manually guide translation process available older paradigms recent work however introduce new capability lexically constrain guide decode modification beam search force inclusion pre specify word phrase output however theoretically sound exist approach computational complexities either linear hokamp liu two thousand and seventeen exponential anderson et al two thousand and seventeen number constraints present algorithm lexically constrain decode complexity of1 number constraints demonstrate algorithms remarkable ability properly place constraints use explore shaky relationship model bleu score implementation available part sockeye
present graph base tree adjoin grammar tag parser use bilstms highway connections character level cnns best end end parser jointly perform supertagging pos tag parse outperform previously report best result twenty-two las uas point graph base parse architecture allow global inference rich feature representations tag parse alleviate fundamental trade transition base graph base parse systems also demonstrate propose parser achieve state art performance downstream task parse evaluation use textual entailments pete unbounded dependency recovery provide support claim tag viable formalism problems require rich structural analysis sentence
common european framework reference cefr guidelines describe language proficiency learners scale six level description cefr guidelines generic across languages development automate proficiency classification systems different languages follow different approach paper explore universal cefr classification use domain specific domain agnostic theory guide well data drive feature report result preliminary experiment monolingual cross lingual multilingual classification three languages german czech italian result show monolingual multilingual model achieve similar performance cross lingual classification yield lower comparable result monolingual classification
paper present deep learn model compete semeval two thousand and eighteen task two multilingual emoji prediction participate subtask call predict likely associate emoji english tweet propose architecture rely long short term memory network augment attention mechanism condition weight word context vector take aggregation tweet mean moreover initialize embed layer model word2vec word embeddings pretrained dataset five hundred and fifty million english tweet finally model rely hand craft feature lexicons train end end back propagation rank 2nd forty-eight team
paper present deep learn model submit semeval two thousand and eighteen task1 competition affect tweet participate subtasks english tweet propose bi lstm architecture equip multi layer self attention mechanism attention mechanism improve model performance allow us identify salient word tweet well gain insight model make interpretable model utilize set word2vec word embeddings train large collection five hundred and fifty million twitter message augment set word affective feature due limit amount task specific train data opt transfer learn approach pretraining bi lstms dataset semeval two thousand and seventeen task 4a propose approach rank 1st subtask e multi label emotion classification 2nd subtask emotion intensity regression achieve competitive result subtasks
paper present two deep learn systems compete semeval two thousand and eighteen task three irony detection english tweet design ensemble two independent model base recurrent neural network bi lstm operate word character level order capture semantic syntactic information tweet model augment self attention mechanism order identify informative word embed layer word level model initialize word2vec word embeddings pretrained collection five hundred and fifty million english tweet utilize handcraft feature lexicons external datasets prior information model train end end use back propagation constrain data furthermore provide visualizations tweet annotations salient tokens attention layer help interpret inner work propose model rank 2nd forty-two team subtask 2nd thirty-one team subtask b however post task completion enhancements model achieve state art result rank 1st subtasks
paper describe new open domain dialogue system alquist develop part alexa prize competition amazon echo line products alquist dialogue system design conduct coherent engage conversation popular topics present hybrid system combine several machine learn rule base approach discuss describe alquist pipeline data acquisition process dialogue manager nlg knowledge aggregation hierarchy sub dialogs present experimental result
demo proposal present phrase base sanskrit hindi sahit statistical machine translation system system develop moses 43k sentence sanskrit hindi parallel corpus 56k sentence monolingual corpus target language hindi use system give fifty-seven bleu score
test hypothesis degree grammaticalization german prepositions correlate corpus base contextual dispersion measure word entropy find indeed moderate correlation entropy stronger correlation frequency number context type
propose context dependent model map utterances within interaction executable formal query incorporate interaction history model maintain interaction level encoder update turn copy sub sequence previously predict query generation approach combine implicit explicit model reference utterances evaluate model atis flight plan interactions demonstrate benefit model context explicit reference
sentence gap paul like coffee mary tea lack overt predicate indicate relation two arguments surface syntax representations sentence often produce poorly parsers even correct well suit downstream natural language understand task relation extraction typically design extract information sentence canonical clause structure paper present two methods parse universal dependencies graph representation explicitly encode elide material additional nod edge find methods reconstruct elide material dependency tree high accuracy parser correctly predict existence gap demonstrate one methods apply languages base case study swedish
relation extraction problem classify relationship two entities give sentence distant supervision ds popular technique develop relation extractors start limit supervision note sentence distant supervision relation extraction set long may benefit word attention better sentence representation contributions paper threefold firstly propose two novel word attention model distantly supervise relation extraction one bi directional gate recurrent unit bi gru base word attention model bgwa two entity centric attention model ea three combination model combine multiple complementary model use weight vote method improve relation extraction secondly introduce gds new distant supervision dataset relation extraction gds remove test data noise present previous distant supervision benchmark datasets make credible automatic evaluation possible thirdly extensive experiment multiple real world datasets demonstrate effectiveness propose methods
propose task quantifiable sequence edit quase edit input sequence generate output sequence satisfy give numerical outcome value measure certain property sequence requirement keep main content input sequence example input sequence could word sequence review sentence advertisement text review sentence outcome could review rat advertisement outcome could click rate major challenge perform quase perceive outcome relate word edit change outcome paper propose framework contain two latent factor namely outcome factor content factor disentangle input sentence allow convenient edit change outcome keep content framework explore pseudo parallel sentence model content similarity outcome differences enable better disentanglement latent factor allow generate output better satisfy desire outcome keep content dual reconstruction structure enhance capability generate expect output exploit couple latent factor pseudo parallel sentence evaluation prepare dataset yelp review sentence rat outcome extensive experimental result report discuss elaborate peculiarities framework
coherence play critical role produce high quality summary document recent years neural extractive summarization become increasingly attractive however ignore coherence summaries extract sentence effort towards extract coherent summaries propose neural coherence model capture cross sentence semantic syntactic coherence pattern propose neural coherence model obviate need feature engineer train end end fashion use unlabeled data empirical result show propose neural coherence model efficiently capture cross sentence coherence pattern use combine output neural coherence model rouge package reward design reinforcement learn method train propose neural extractive summarizer name reinforce neural extractive summarization rnes model rnes model learn optimize coherence informative importance summary simultaneously experimental result show propose rnes outperform exist baselines achieve state art performance term rouge cnn daily mail dataset qualitative evaluation indicate summaries produce rnes coherent readable
formal logic base approach recognize textual entailment rte combinatory categorial grammar ccg parser use parse input premise hypotheses obtain logical formulas important parser process sentence consistently fail recognize similar syntactic structure result inconsistent predicate argument structure among case succeed theorem prove doom failure work present simple method extend exist ccg parser parse set sentence consistently achieve inter sentence model markov random field mrf combine exist logic base systems method always show improvement rte experiment english japanese languages
traditional information retrieval offer web search engines impede users information overload extensive result page need manually locate desire information therein conversely question answer systems change humans interact information systems users ask specific question obtain tailor answer conveniently natural language despite obvious benefit use often limit academic context largely expensive domain customizations mean performance domain specific applications often fail meet expectations paper propose cost efficient remedy leverage metadata filter mechanism increase precision document retrieval ii develop novel fuse oversample approach transfer learn order improve performance answer extraction knowledge inductively transfer relate yet different task domain specific application account potential differences sample size across task result performance demonstrate actual use case finance company film industry fewer four hundred question answer pair annotate order yield significant performance gain direct implication management present promise path better leverage knowledge store information systems
propose method learn disentangle representations texts code distinct complementary aspects aim afford efficient model transfer interpretability induce disentangle embeddings propose adversarial objective base dissimilarity triplets document respect specific aspects motivate application embed biomedical abstract describe clinical trials manner disentangle populations interventions outcomes give trial show method learn representations encode clinically salient aspects effectively use perform aspect specific retrieval demonstrate approach generalize beyond motivate application experiment two multi aspect review corpora
recent years online communities form around suicide self harm prevention communities offer support moment crisis also normalize harmful behavior discourage professional treatment instigate suicidal ideation work focus interaction others community affect mental state users seek support first build dataset conversation thread users distress state community members offer support show construct classifier predict whether distress users help harm interactions thread achieve macro f1 score sixty-nine
present novel approach determine learners second language proficiency utilize behavioral trace eye movements read approach provide stand alone eyetracking base english proficiency score reflect extent learner gaze pattern read similar native english speakers show score correlate strongly standardize english proficiency test also demonstrate gaze information use accurately predict outcomes test approach yield strongest performance test taker present suite sentence eyetracking data readers however remain effective even use eyetracking sentence eye movement data previously collect derive proficiency automatic byproduct eye movements ordinary read approach offer potentially valuable new tool second language proficiency assessment broadly result open door future methods infer reader characteristics behavioral trace read
notional anaphors pronouns disagree antecedents grammatical categories notional reason plural singular agreement government since case rare conflict evidence strictly agree case government present substantial challenge coreference resolution refer expression generation use ontonotes corpus paper take ensemble approach predict english notional anaphora context basis largest empirical data date addition state art prediction accuracy result suggest theoretical approach posit plural construal antecedent utterance insufficient circumstances anaphor utterance location well global factor genre strong effect choice refer expression
sentence simplification aim simplify content structure complex sentence thus make easier interpret human readers easier process downstream nlp applications recent advance neural machine translation pave way novel approach task paper adapt architecture augment memory capacities call neural semantic encoders munkhdalai yu two thousand and seventeen sentence simplification experiment demonstrate effectiveness approach different simplification datasets term automatic evaluation measure human judgments
natural language understand nlu technology maximally useful practically scientific object study must general must able process language way exclusively tailor one specific task dataset pursuit objective introduce general language understand evaluation benchmark glue tool evaluate analyze performance model across diverse range exist nlu task glue model agnostic incentivizes share knowledge across task certain task limit train data provide hand craft diagnostic test suite enable detail linguistic analysis nlu model evaluate baselines base current methods multi task transfer learn find immediately give substantial improvements aggregate performance train separate model per task indicate room improvement develop general robust nlu systems
present novel end end memory network stance detection jointly predict whether document agree disagree discuss unrelated respect give target claim also ii extract snippets evidence prediction network operate paragraph level integrate convolutional recurrent neural network well similarity matrix part overall architecture experimental evaluation fake news challenge dataset show state art performance
present claimrank online system detect check worthy claim originally train political debate system work kind text eg interview regular news article aim facilitate manual fact check efforts prioritize claim fact checker consider first claimrank support arabic english train actual annotations nine reputable fact check organizations politifact factcheck abc cnn npr nyt chicago tribune guardian washington post thus mimic claim selection strategies well union
identify extract use phrasal knowledge crucial problem task recognize textual entailment rte solve problem propose method detect paraphrase via natural deduction proof semantic relations sentence pair solution rely graph reformulation partial variable unifications algorithm induce subgraph alignments mean representations experiment show method automatically detect various paraphrase absent exist paraphrase databases addition detection paraphrase use proof information improve accuracy rte task
often case best perform language model ensemble neural language model n grams work propose method improve two model combine use small network predict mixture weight two model adapt relative importance time step gate network small train quickly small amount hold data add overhead score time experiment carry one billion word benchmark show significant improvement state art ensemble without retrain basic modules
generate abstract mean representation amr underspecified problem many syntactic decisions constrain semantic graph explicitly account underspecification break generate amr two step first generate syntactic structure generate surface form show decompose generation process way lead state art single model performance generate amr without additional unlabelled data also demonstrate generate mean preserve syntactic paraphrase amr graph judge humans
formalize new modular variant current question answer task enforce complete independence document encoder question encoder formulation address key challenge machine comprehension require standalone representation document discourse additionally lead significant scalability advantage since encode answer candidate phrase document pre compute index offline efficient retrieval experiment baseline model new task achieve reasonable accuracy significantly underperform unconstrained qa model invite qa research community engage phrase index question answer piqa pika close gap leaderboard nlpcswashingtonedu piqa
present novel approach learn representations sentence level semantic similarity use conversational data method train unsupervised model predict conversational input response pair result sentence embeddings perform well semantic textual similarity sts benchmark semeval two thousand and seventeen community question answer cqa question similarity subtask performance improve introduce multitask train combine conversational input response prediction task natural language inference task extensive experiment show propose model achieve best performance among neural model sts benchmark competitive state art feature engineer mix systems task
machine translation systems achieve near human level performance languages yet effectiveness strongly rely availability large amount parallel sentence hinder applicability majority language pair work investigate learn translate access large monolingual corpora language propose two model variants neural phrase base model versions leverage careful initialization parameters denoising effect language model automatic generation parallel data iterative back translation model significantly better methods literature simpler fewer hyper parameters widely use wmt fourteen english french wmt sixteen german english benchmarks model respectively obtain two hundred and eighty-one two hundred and fifty-two bleu point without use single parallel sentence outperform state art eleven bleu point low resource languages like english urdu english romanian methods achieve even better result semi supervise supervise approach leverage paucity available bitexts code nmt pbsmt publicly available
one way interpret neural model predictions highlight important input feature example heatmap visualization word input sentence exist interpretation methods nlp word importance determine either input perturbation measure decrease model confidence word remove gradient respect word understand limitations methods use input reduction iteratively remove least important word input expose pathological behaviors neural model remain word appear nonsensical humans ones determine important interpretation methods confirm human experiment reduce examples lack information support prediction label model still make predictions high confidence explain counterintuitive result draw connections adversarial examples confidence calibration pathological behaviors reveal difficulties interpret neural model train maximum likelihood mitigate deficiencies fine tune model encourage high entropy output reduce examples fine tune model become interpretable input reduction without accuracy loss regular examples
many efforts make facilitate natural language process task pre train language model lms bring significant improvements various applications fully leverage nearly unlimited corpora capture linguistic information multifarious level large size lms require specific task part information useful large size lms even inference stage may heavy computation workloads make time consume large scale applications propose compress bulky lms preserve useful information regard specific task different layer model keep different information develop layer selection method model prune use sparsity induce regularization introduce dense connectivity detach layer without affect others stretch shallow wide lms deep narrow model train lms learn layer wise dropouts better robustness experiment two benchmark datasets demonstrate effectiveness method
exist temporal relation temprel annotation scheme often low inter annotator agreements iaa even experts suggest current annotation task need better definition paper propose new multi axis model better capture temporal structure events addition identify event end point major source confusion annotation also propose annotate temprels base start point pilot expert annotation use propose scheme show significant improvement iaa conventional sixty eighty cohen kappa better define annotation scheme enable use crowdsourcing alleviate labor intensity annotator hope work foster interest study towards event understand
sentence encoders produce sentence embeddings use neural network typically evaluate well transfer downstream task include semantic similarity important task natural language understand although much work dedicate build sentence encoders accompany transfer learn techniques receive relatively little attention paper propose transfer learn set specialize semantic similarity refer direct network transfer experiment several standard text similarity datasets show apply direct network transfer exist encoders lead state art performance additionally compare several approach transfer sentence encoders semantic similarity task show choice transfer learn set greatly affect performance many case differ encoder dataset
state art model joint entity recognition relation extraction strongly rely external natural language process nlp tool pos part speech taggers dependency parsers thus performance joint model depend quality feature obtain nlp tool however feature always accurate various languages contexts paper propose joint neural model perform entity recognition relation extraction simultaneously without need manually extract feature use external tool specifically model entity recognition task use crf conditional random field layer relation extraction task multi head selection problem ie potentially identify multiple relations entity present extensive experimental setup demonstrate effectiveness method use datasets various contexts ie news biomedical real estate languages ie english dutch model outperform previous neural model use automatically extract feature perform within reasonable margin feature base neural model even beat
address part speech pos induction maximize mutual information induce label context focus two train objectives amenable stochastic gradient descent sgd novel generalization classical brown cluster objective recently propose variational lower bind objectives subject noise gradient update show analysis experiment variational lower bind robust whereas generalize brown objective vulnerable obtain competitive performance multitude datasets languages simple architecture encode morphology context
number differences emerge modern classic approach constituency parse recent years structural components like grammars feature rich lexicons become less central recurrent neural network representations rise popularity goal work analyze extent information provide directly model structure classical systems still capture neural methods end propose high performance neural model nine thousand, two hundred and eight f1 ptb representative recent work perform series investigative experiment find model implicitly learn encode much information explicitly provide grammars lexicons past indicate scaffold largely subsume powerful general purpose neural machinery
work translation rich resource languages low resource languages main challenge identify lack low resource language data effective methods cross lingual transfer variable bind problem common neural systems build translation system address challenge use eight european language families test grind firstly add source target family label study intra family inter family influence effective cross lingual transfer achieve improvement ninety-nine bleu score english swedish translation use eight families compare single family multi source multi target baseline moreover find train two neighbor families closest low resource language often enough secondly construct ablation study find reasonably good result achieve even considerably less target data thirdly address variable bind problem build order preserve name entity translation model obtain six hundred and six accuracy qualitative evaluation translations akin human translations preliminary study
propose new method event extraction ee task base imitation learn framework specifically inverse reinforcement learn irl via generative adversarial network gin gin estimate proper reward accord difference action commit expert grind truth agent among complicate state environment ee task benefit dynamic reward instance label yield various extents difficulty gain expect diverse eg ambiguous correctly detect trigger argument receive high gain traditional rl model usually neglect differences pay equal attention instance moreover experiment also demonstrate propose framework outperform state art methods without explicit feature engineer
propose stochastic answer network san explore multi step inference strategies natural language inference rather directly predict result give input model maintain state iteratively refine predictions experiment show san achieve state art result three benchmarks stanford natural language inference snli dataset multigenre natural language inference multinli dataset quora question pair dataset
current image caption approach generate descriptions lack specific information name entities involve image paper propose new task aim generate informative image caption give image hashtags input propose simple effective approach tackle problem first train convolutional neural network long short term memory network cnn lstm model generate template caption base input image use knowledge graph base collective inference algorithm fill template specific name entities retrieve via hashtags experiment new benchmark dataset collect flickr show model generate news style image descriptions much richer information model outperform unimodal baselines significantly various evaluation metrics
taylor law describe fluctuation characteristics underlie system variance event within time span grow power law respect mean although taylor law apply many natural social systems application language scarce article describe new quantification taylor law natural language report analysis one thousand, one hundred texts across fourteen languages taylor exponents write natural language texts find exhibit almost value exponent also compare language relate data child direct speech music program language code result show taylor exponent serve quantify fundamental structural complexity underlie linguistic time series article also show applicability find evaluate language model
generate text structure data important various task question answer dialog systems show least one domain without supervision base unlabeled text able build natural language generation nlg system higher performance supervise approach approach interpret structure data corrupt representation desire output use denoising auto encoder reconstruct sentence show introduce noise train examples contain structure data result denoising auto encoder generalize generate correct sentence give structure data
learn distribute sentence representations one key challenge natural language process previous work demonstrate recurrent neural network rnns base sentence encoder train large collection annotate natural language inference data efficient transfer learn facilitate relate task paper show joint learn multiple task result better generalizable sentence representations conduct extensive experiment analysis compare multi task single task learn sentence encoders quantitative analysis use auxiliary task show multi task learn help embed better semantic information sentence representations compare single task learn addition compare multi task sentence encoders contextualized word representations show combine boost performance transfer learn
beam search widely use approximate search strategy neural network decoders generally outperform simple greedy decode task like machine translation however improvement come substantial computational cost paper propose flexible new method allow us reap nearly full benefit beam search nearly additional computational cost method revolve around small neural network actor train observe manipulate hide state previously train decoder train actor network introduce use pseudo parallel corpus build use output beam search base model rank target quality metric like bleu method inspire earlier work problem require reinforcement learn train reliably range model experiment three parallel corpora three architectures show method yield substantial improvements translation quality speed base system
propose duorc novel dataset read comprehension rc motivate several new challenge neural approach language understand beyond offer exist rc datasets duorc contain one hundred and eighty-six thousand and eighty-nine unique question answer pair create collection seven thousand, six hundred and eighty pair movie plot pair collection reflect two versions movie one wikipedia imdb write two different author ask crowdsourced workers create question one version plot different set workers extract synthesize answer version unique characteristic duorc question answer create different versions document narrate underlie story ensure design little lexical overlap question create one version segment contain answer version since two versions different level plot detail narration style vocabulary etc answer question second version require deeper language understand incorporate external background knowledge additionally narrative style passages arise movie plot oppose typical descriptive passages exist datasets exhibit need perform complex reason events across multiple sentence indeed observe state art neural rc model achieve near human performance squad dataset even couple traditional nlp techniques address challenge present duorc exhibit poor performance f1 score three thousand, seven hundred and forty-two duorc v eighty-six squad dataset open several interest research avenues wherein duorc could complement rc datasets explore novel neural approach study language understand
study problem stock relate question answer stockqa automatically generate answer stock relate question like professional stock analysts provide action recommendations stock upon user request stockqa quite different previous qa task since one answer stockqa natural language sentence rather entities value due dynamic nature stockqa scarcely possible get reasonable answer extractive way train data two stockqa require properly analyze relationship keywords qa pair numerical feature stock propose address problem memory augment encoder decoder architecture integrate different mechanisms number understand generation critical component stockqa build large scale dataset contain 180k stockqa instance base various technique combinations extensively study compare experimental result show hybrid word character model separate character components number process achieve best performance analyze result find four hundred and forty-eight answer generate best model still suffer generic answer problem alleviate straightforward hybrid retrieval generation model
work present approach base combine string kernels word embeddings automatic essay score string kernels capture similarity among string base count common character n grams low level yet powerful type feature demonstrate state art result various text classification task arabic dialect identification native language identification best knowledge first apply string kernels automatically score essay also first combine high level semantic feature representation namely bag super word embeddings report best performance automate student assessment prize data set domain cross domain settings surpass recent state art deep learn approach
increasingly wide range artificial intelligence applications rely syntactic information process extract mean natural language text speech constituent tree one widely use syntactic formalisms produce phrase structure representations sentence natural language shift reduce constituent parsers become one efficient approach increase accuracy speed still one main objectives pursue research community artificial intelligence applications make use parse output machine translation voice assistant service improve performance goal mind propose article novel non binary shift reduce algorithm constituent parse parser follow classical bottom strategy unlike others straightforwardly create non binary branch one reduce transition instead require prior binarization sequence binary transition allow direct application language without need resources percolation table result use fewer transition per sentence exist transition base constituent parsers become fastest system consequence speed downstream applications use static oracle train greedy search accuracy novel approach par state art transition base constituent parsers outperform top bottom greedy shift reduce systems wall street journal section english penn treebank penn chinese treebank additionally develop dynamic oracle train propose transition base algorithm achieve improvements benchmarks obtain best accuracy date penn chinese treebank among greedy shift reduce parsers
paper study recent neural generative model text generation relate variational autoencoders previous work employ various techniques control prior distribution latent cod model important sample performance little attention pay reconstruction error study follow rigorous evaluation protocol use large set previously use novel automatic human evaluation metrics apply generate sample reconstructions hope become new evaluation standard compare neural generative model text
present model semantic proto role label sprl use adapt bidirectional lstm encode strategy call neural davidsonian predicate argument structure represent pair hide state correspond predicate argument head tokens input sequence demonstrate one state art result sprl two network naturally share parameters attribute allow learn new attribute type limit add supervision
one first step many nlp systems select pre train word embeddings use argue step better leave neural network figure end introduce dynamic meta embeddings simple yet effective method supervise learn embed ensembles lead state art performance within model class variety task subsequently show technique use would new light usage word embeddings nlp systems
deep neural network dnns vulnerable adversarial examples perturbations correctly classify examples model misclassify image domain perturbations often virtually indistinguishable human perception cause humans state art model disagree however natural language domain small perturbations clearly perceptible replacement single word drastically alter semantics document give challenge use black box population base optimization algorithm generate semantically syntactically similar adversarial examples fool well train sentiment analysis textual entailment model success rat ninety-seven seventy respectively additionally demonstrate nine hundred and twenty-three successful sentiment analysis adversarial examples classify original label twenty human annotators examples perceptibly quite similar finally discuss attempt use adversarial train defense fail yield improvement demonstrate strength diversity adversarial examples hope find encourage researchers pursue improve robustness dnns natural language domain
fine grain entity type task assign fine grain semantic type entity mention propose neural architecture learn distributional semantic representation leverage greater amount semantic context document sentence level information prior work find additional context improve performance improvements gain utilize adaptive classification thresholds experiment show approach without reliance hand craft feature achieve state art result three benchmark datasets
reasonable approach fact check claim involve retrieve potentially relevant document different source eg news websites social media etc determine stance document respect claim finally make prediction claim factuality aggregate strength stances take reliability source account moreover fact check system able explain decision provide relevant extract rationales document yet setup directly support exist datasets treat fact check document retrieval source credibility stance detection rationale extraction independent task paper support interdependencies task annotations corpus implement setup arabic fact check corpus first kind
introduce task cross lingual semantic parse map content provide source language mean representation base target language present one mean representation design allow systems target vary level structural complexity shallow deep analysis two evaluation metric measure similarity system output reference mean representations three end end model novel copy mechanism support intrasentential coreference four evaluation dataset experiment show model outperform strong baselines least one hundred and eighteen f1 score
social media user geolocation vital many applications event detection paper propose gcn multiview geolocation model base graph convolutional network use text network context compare gcn state art two baselines propose show model achieve competitive state art three benchmark geolocation datasets sufficient supervision available also evaluate gcn minimal supervision scenario show outperform baselines find highway network gate essential control amount useful neighbourhood expansion gcn
paper present new network architecture call multi head decoder end end speech recognition extension multi head attention model multi head attention model multiple attentions calculate integrate single attention hand instead integration attention level propose method use multiple decoders attention integrate output generate final output furthermore order make head capture different modalities different attention function use head lead improvement recognition performance ensemble effect evaluate effectiveness propose method conduct experimental evaluation use corpus spontaneous japanese experimental result demonstrate propose method outperform conventional methods location base multi head attention model capture different speech linguistic contexts within attention base encoder decoder framework
present novel effective technique perform text coherence task facilitate deeper insights data despite obtain ever increase task performance modern deep learn approach nlp task often provide users final network decision additional understand data work show new type sentence embed learn self supervision apply effectively text coherence task serve window deeper understand data obtain produce sentence embeddings train recurrent neural network take individual sentence predict location document form distribution locations demonstrate embeddings combine simple visual heuristics use achieve performance competitive state art multiple text coherence task outperform complex specialize approach additionally demonstrate embeddings provide insights useful writers improve write quality inform document structure assist readers summarize locate information
community base question answer cqa websites represent important source information result problem match valuable answer correspond question become increasingly popular research topic frame task binary relevant irrelevant classification problem present adversarial train framework alleviate label imbalance issue employ generative model iteratively sample subset challenge negative sample fool classification model model alternatively optimize use reinforce algorithm propose method completely different previous ones negative sample train set directly use uniformly sample propose use multi scale match explicitly inspect correlation word ngrams different level granularity evaluate propose method semeval two thousand and sixteen semeval two thousand and seventeen datasets achieve state art similar performance
intelligent personal digital assistants ipdas popular real life application speak language understand capabilities cover potentially thousands overlap domains natural language understand task find best domain handle utterance become challenge problem large scale paper propose set efficient scalable neural shortlist reranking model large scale domain classification ipdas shortlist stage focus efficiently trim domains list k best candidate domains reranking stage perform list wise reranking initial k best domains additional contextual information show effectiveness approach extensive experiment one thousand, five hundred ipda domains
paper explore task map speak language utterances one thousands natural language understand domains intelligent personal digital assistants ipdas scenario observe many mainstream ipdas industry allow third party develop thousands new domains augment build ones rapidly increase domain coverage overall ipda capabilities propose scalable neural model architecture share encoder novel attention mechanism incorporate personalization information domain specific classifiers solve problem efficiently architecture design efficiently accommodate new domains appear full model retrain cycle rapid bootstrapping mechanism two order magnitude faster retrain account practical constraints real time production systems design minimize memory footprint runtime latency demonstrate incorporate personalization result significantly accurate domain classification set thousands overlap domains
methods learn word sense embeddings represent single word multiple sense specific vectors methods produce interpretable sense embeddings also learn select sense use give context propose unsupervised model learn sense embeddings use modify gumbel softmax function allow differentiable discrete sense selection model produce sense embeddings competitive sometimes state art multiple similarity base downstream evaluations however performance downstream evaluations task correlate interpretability sense embeddings discover interpretability comparison compete multi sense embeddings many previous approach perform well downstream evaluations produce interpretable embeddings learn duplicate sense group method achieve best worlds
paper introduce system task irony detection english tweet part semeval two thousand and eighteen propose representation learn approach rely multi layer bidirectional lstm without use external feature provide additional semantic information although model able outperform baseline validation set result show limit generalization power test set give limit size dataset think usage pre train scheme would greatly improve obtain result
investigate apply repurposed generic qa data model recently propose relation extraction task find train squad produce better zero shoot performance robust generalisation compare task specific train set also show standard qa architectures eg fastqa bidaf apply slot fill query without need model modification
technique report aim mitigate overfitting problem natural language apply data augmentation methods specifically attempt several type noise perturb input word embed gaussian noise bernoulli noise adversarial noise etc also apply several constraints different type noise implement propose data augmentation methods baseline model gain improvements several sentence classification task
language identification li problem determine natural language document part thereof write automatic li extensively research fifty years today li key part many text process pipelines text process techniques generally assume language input text know research area recently especially active article provide brief history li research extensive survey feature methods use far li literature describe feature methods introduce unify notation discuss evaluation methods applications li well shelf li systems require train end user finally identify open issue survey work date issue propose future directions research li
incorporate explicit neural interlingua multilingual encoder decoder neural machine translation nmt architecture demonstrate model learn language independent representation perform direct zero shoot translation without use pivot translation use source sentence embeddings create english yelp review classifier mediation neural interlingua also classify french german review furthermore show despite use smaller number parameters pairwise collection bilingual nmt model approach produce comparable bleu score language pair wmt15
current state art semantic role label srl use deep neural network explicit linguistic feature however prior work show gold syntax tree dramatically improve srl decode suggest possibility increase accuracy explicit model syntax work present linguistically inform self attention lisa neural network model combine multi head self attention multi task learn across dependency parse part speech tag predicate detection srl unlike previous model require significant pre process prepare linguistic feature lisa incorporate syntax use merely raw tokens input encode sequence simultaneously perform parse predicate detection role label predicate syntax incorporate train one attention head attend syntactic parent token moreover high quality syntactic parse already available beneficially inject test time without train srl model experiment conll two thousand and five srl lisa achieve new state art performance model use predict predicate standard word embeddings attain twenty-five f1 absolute higher previous state art newswire thirty-five f1 domain data nearly ten reduction error conll two thousand and twelve english srl also show improvement twenty-five f1 lisa also perform state art contextually encode elmo word representations nearly ten f1 news twenty f1 domain text
show spell know word help us deal unknown word open vocabulary nlp task method propose use extend close vocabulary generative model paper specifically consider case neural language model bayesian generative story combine standard rnn language model generate word tokens sentence rnn base spell model generate letter word type two rnns respectively capture sentence structure word structure keep separate linguistics invoke second rnn generate spell novel word context obtain open vocabulary language model know word embeddings naturally infer combine evidence type spell token context compare baselines include novel strong baseline beat previous work establish state art result multiple datasets
present large scale collection diverse natural language inference nli datasets help provide insight well sentence representation capture distinct type reason collection result recast thirteen exist datasets seven semantic phenomena common nli structure result half million label context hypothesis pair total refer collection dnc diverse natural language inference collection dnc available online https wwwdecompnet grow time additional resources recast add novel source
end end task orient dialog systems usually suffer challenge incorporate knowledge base paper propose novel yet simple end end differentiable model call memory sequence mem2seq address issue mem2seq first neural generative model combine multi hop attention memories idea pointer network empirically show mem2seq control generation step multi hop attention mechanism help learn correlations memories addition model quite general without complicate task specific design result show mem2seq train faster attain state art performance three different task orient dialog datasets
study problem analyze tweet universal dependencies extend ud guidelines cover special constructions tweet affect tokenization part speech tag label dependencies use extend guidelines create new tweet treebank english tweebank v2 four time larger unlabeled tweebank v1 introduce kong et al two thousand and fourteen characterize disagreements annotators show challenge deliver consistent annotation due ambiguity understand explain tweet nonetheless use new treebank build pipeline system parse raw tweet ud overcome annotation noise without sacrifice computational efficiency propose new method distill ensemble twenty transition base parsers single one parser achieve improvement twenty-two las un ensembled baseline outperform parsers state art treebanks accuracy speed
many languages inflectional morphological systems replete irregulars ie word seem follow standard inflectional rule work quantitatively investigate condition irregulars survive language course time use recurrent neural network simulate language learners test diachronic relation frequency word irregularity
argument reason comprehension task require significant language understand complex reason world knowledge focus transfer sentence encoder bootstrap complicate model give small size dataset best model use pre train bilstm encode input sentence learn task specific feature argument warrant perform independent argument warrant match model achieve mean test set accuracy six thousand, four hundred and forty-three encoder transfer yield significant gain best model random initialization independent warrant match effectively double size dataset provide additional regularization demonstrate regularization come ignore statistical correlations warrant feature position also report experiment best model match warrant reason ignore claim relatively low performance degradation suggest model necessarily learn intend task
paper describe system submit semeval two thousand and eighteen task one affect tweet ait solve five subtasks focus model sentence word level representations emotion inside texts large distantly label corpora emojis hashtags transfer emotional knowledge exploit neural network model feature extractors use representations traditional machine learn model support vector regression svr logistic regression solve competition task system place among top3 subtasks participate
semantic representations long argue potentially useful enforce mean preservation improve generalization performance machine translation methods work first incorporate information predicate argument structure source sentence namely semantic role representations neural machine translation use graph convolutional network gcns inject semantic bias sentence encoders achieve improvements bleu score linguistic agnostic syntax aware versions english german language pair
present generative model map natural language question sql query exist neural network base approach typically generate sql query word word however large portion generate result incorrect executable due mismatch question word table content approach address problem consider structure table syntax sql language quality generate sql query significantly improve one learn replicate content column name cells sql keywords two improve generation clause leverage column cell relation experiment conduct wikisql recently release dataset largest question sql pair approach significantly improve state art execution accuracy six hundred and ninety seven hundred and forty-four
first stage every knowledge base question answer approach link entities input question investigate entity link context question answer task present jointly optimize neural architecture entity mention detection entity disambiguation model surround context different level granularity use wikidata knowledge base available question answer datasets create benchmarks entity link question answer data approach outperform previous state art system data result average eight improvement final score demonstrate model deliver strong performance across different entity categories
paper address relatively new task prediction asr performance unseen broadcast program first propose heterogenous french corpus dedicate task two prediction approach compare state art performance prediction base regression engineer feature new strategy base convolutional neural network learn feature particularly focus combination textual asr transcription signal input joint use textual signal feature work regression baseline combination input cnns lead best wer prediction performance also show cnn prediction remarkably predict wer distribution collection speech record
review products service internet marketplace websites contain rich amount information users often wish survey review review snippets perspective certain aspect result large body work aspect identification extraction corpora work evaluate newly propose neural model aspect extraction two practical task first extract canonical sentence various aspects review judge human evaluators alternatives k mean baseline remarkably well set second experiment focus suitability recover aspect distributions represent users review write set review reranking experiment find aspect base profile largely capture notions user preferences show divergent users generate markedly different review rank
follow paper explore possibility use machine learn algorithms detect case corruption malpractice governments dataset use author contain information several government contract colombia year two thousand and seven two thousand and twelve author begin explore clean data follow perform feature engineer finally implement machine learn model detect anomalies give dataset
recent years certain success task model lexical semantics obtain distributional semantic model nevertheless scientific community still unaware reliable evaluation method model researchers argue possible gold standard could obtain neuro cognitive resources store information human cognition one resources eye movement data silent read goal work test hypothesis whether data could use evaluate distributional semantic model different languages propose experiment english russian eye movement datasets provo corpus geco russian sentence corpus word vectors skip gram model train national corpora web corpora word similarity datasets russian english assess humans order find existence correlation embeddings eye movement data test hypothesis correlation language independent result find validity hypothesis test could question
present machine learn approach distinguish texts translate chinese humans texts originally write chinese focus wide range syntactic feature use support vector machine svms classifier genre balance corpus translation study chinese find constituent parse tree dependency triple feature without lexical information perform well task f measure ninety close result lexical n gram feature without risk learn topic information rather translation feature thus claim syntactic feature alone accurately distinguish translate original chinese translate chinese exhibit increase use determiners subject position pronouns np de np modifiers multiple nps vps conjoin chinese specific punctuation among structure also interpret syntactic feature reference previous translation study chinese particularly usage pronouns
field machine translation face recognize problem inconsistency report score dominant metric although people refer bleu score bleu fact parameterized metric whose value vary wildly change parameters parameters often report hard find consequently bleu score paper directly compare quantify variation find differences high eighteen commonly use configurations main culprit different tokenization normalization scheme apply reference point success parse community suggest machine translation researchers settle upon bleu scheme use annual conference machine translation wmt allow user supply reference process provide new tool sacrebleu facilitate
work deal scitail natural entailment challenge derive multi choice question answer problem premise hypotheses scitail generate awareness specifically aim entailment task make challenge entailment data set directly useful end task question answer propose deiste deep explorations inter sentence interactions textual entailment entailment task give word word interactions premise hypothesis pair p h deiste consist parameter dynamic convolution make important word p h play dominant role learn representations ii position aware attentive convolution encode representation position information align word pair experiment show deiste get approx5 improvement prior state art pretrained deiste scitail generalize well rte five
supervise distributional methods apply successfully lexical entailment recent work question whether methods actually learn relation two word specifically levy et al two thousand and fifteen claim linear classifiers learn separate properties word suggest cheap easy way boost performance methods integrate multiplicative feature commonly use representations provide extensive evaluation different classifiers evaluation setups suggest suitable evaluation setup task eliminate bias exist previous ones
data drive approach sequence sequence model successfully apply short text summarization news article model typically train input summary pair consist single sentence partially due limit availability multi sentence train data propose use scientific article new milestone text summarization large scale train data come almost free two type high quality summaries different level title abstract generate two novel multi sentence summarization datasets scientific article test suitability wide range exist extractive abstractive neural network base summarization approach analysis demonstrate scientific paper suitable data drive text summarization result could serve valuable benchmarks scale sequence sequence model long sequence
language model primarily evaluate perplexity perplexity quantify comprehensible prediction performance provide qualitative information success failure model another approach evaluate language model thus propose use scale properties natural language five test consider first two account vocabulary population three long memory natural language follow model evaluate test n grams probabilistic context free grammar pcfg simon pitman yor py process hierarchical py neural language model neural language model exhibit long memory properties natural language limit degree effectiveness every test model also discuss
article present sirius ltg uio system semeval two thousand and eighteen task seven semantic relation extraction classification scientific paper first extract shortest dependency path sdp two entities introduce convolutional neural network cnn take shortest dependency path embeddings input perform relation classification differ objectives subtask share task approach achieve overall f1 score seven hundred and sixty-seven eight hundred and thirty-two relation classification clean noisy data respectively furthermore combine relation extraction classification clean data obtain f1 score three hundred and seventy-four three hundred and thirty-six phase system rank 3rd three sub task share task
neural encoder decoder model machine translation achieve impressive result learn linguistic knowledge source target languages implicit end end manner propose framework model begin learn syntax translation interleave gradually put focus translation use approach achieve considerable improvements term bleu score relatively large parallel corpus wmt14 english german low resource wit german english setup
style transfer task rephrase text contain specific stylistic properties without change intent affect within context paper introduce new method automatic style transfer first learn latent representation input sentence ground language translation model order better preserve mean sentence reduce stylistic properties adversarial generation techniques use make output match desire style evaluate technique three different style transformations sentiment gender political slant compare two state art style transfer model techniques show improvements automatic evaluation style transfer manual evaluation mean preservation fluency
till neural abstractive summarization methods achieve great success single document summarization sds however due lack large scale multi document summaries methods hardly apply multi document summarization mds paper investigate neural abstractive methods mds adapt state art neural abstractive summarization model sds propose approach extend neural abstractive model train large scale sds data mds task approach make use small number multi document summaries fine tune experimental result two benchmark duc datasets demonstrate approach outperform variety baseline neural model
unsupervised neural machine translation nmt recently propose approach machine translation aim train model without use label data model propose unsupervised nmt often use one share encoder map pair sentence different languages share latent space weak keep unique internal characteristics language style terminology sentence structure address issue introduce extension utilize two independent encoders share partial weight responsible extract high level representations input sentence besides two different generative adversarial network gans namely local gin global gin propose enhance cross language translation new approach achieve significant improvements english german english french chinese english translation task
report find second complex word identification cwi share task organize part bea workshop co locate naacl hlt two thousand and eighteen second cwi share task feature multilingual multi genre datasets divide four track english monolingual german monolingual spanish monolingual multilingual track french test set two task binary classification probabilistic classification total twelve team submit result different task track combinations eleven write system description paper refer report appear bea workshop proceed
commonsense knowledge base conceptnet represent knowledge form relational triple inspire recent work li et al analyse knowledge base completion model use mine commonsense knowledge raw text propose novelty predict triple respect train set important factor interpret result critically analyse difficulty mine novel commonsense knowledge show simple baseline method outperform previous state art predict novel
present empirical study gender bias coreference resolution systems first introduce novel winograd schema style set minimal pair sentence differ pronoun gender winogender schemas evaluate confirm systematic gender bias three publicly available coreference resolution systems correlate bias real world textual gender statistics
every lawsuit document contain information party claim court analysis decision others information helpful understand case better predict judge decision similar case future however extraction information document difficult language complicate sentence vary length treat problem task sequence label paper present first research extract relevant information civil lawsuit document china hierarchical rnn framework
despite recent popularity word embed methods small body work explore limitations representations paper consider one aspect embed space namely stability show even relatively high frequency word one hundred two hundred occurrences often unstable provide empirical evidence various factor contribute stability word embeddings analyze effect stability downstream task
interact relational databases natural language help users background easily query analyze vast amount data require system understand users question convert sql query automatically paper present novel approach typesql view problem slot fill task additionally typesql utilize type information better understand rare entities number natural language question test idea wikisql dataset outperform prior state art fifty-five much less time also show access content databases significantly improve performance users query well form typesql get eight hundred and twenty-six accuracy one hundred and seventy-five absolute improvement compare previous content sensitive model
propose process investigate extent sentence representations arise neural machine translation nmt systems encode distinct semantic phenomena use representations feature train natural language inference nli classifier base datasets recast exist semantic annotations apply process representative nmt system find encoder appear suit support inferences syntax semantics interface compare anaphora resolution require world knowledge conclude discussion merit potential deficiencies exist process may improve extend broader framework evaluate semantic coverage
extract entities relations text important task understand massive text corpora open information extraction ie systems mine relation tuples ie entity arguments predicate string describe relation sentence relation tuples confine predefined schema relations interest however current open ie systems focus model local context information sentence extract relation tuples ignore fact global statistics large corpus collectively leverage identify high quality sentence level extractions paper propose novel open ie system call remine integrate local context signal global structural signal unify distant supervision framework leverage facts external knowledge base supervision new system apply many different domains facilitate sentence level tuple extractions use corpus level statistics system operate solve joint optimization problem unify one segment entity relation phrase individual sentence base local context two measure quality tuples extract individual sentence translate base objective learn two subtasks jointly help correct errors produce subtask mutually enhance experiment two real world corpora different domains demonstrate effectiveness generality robustness remine compare state art open ie systems
multilingual topic model enable document analysis across languages coherent multilingual summaries data however standard effective metric evaluate quality multilingual topics introduce new intrinsic evaluation multilingual topic model correlate well human judgments multilingual topic coherence well performance downstream applications importantly also study evaluation low resource languages standard metrics fail accurately measure topic quality robust external resources unavailable propose adaptation model improve accuracy reliability metrics low resource settings
current methods mine parallel texts web assume web page web sit share structure across languages believe still exist non negligible amount parallel data spread across source satisfy assumption propose approach base combination bivec bilingual extension word2vec locality sensitive hash allow us efficiently identify pair parallel segment locate anywhere page give web domain regardless structure validate method realign segment large parallel corpus another experiment real world data provide common crawl foundation confirm solution scale hundreds terabytes large set web crawl data
paper aim improve machine answer question directly text focus model answer correctly multiple type question various type texts document even large collections end introduce weaver model use new way relate question textual context weave layer recurrent network goal make assumptions possible information question context combine form answer show empirically six datasets weaver perform well multiple condition instance produce solid result popular squad dataset rajpurkar et al two thousand and sixteen solve almost babi task weston et al two thousand and fifteen greatly outperform state art methods open domain question answer text chen et al two thousand and seventeen
generalize cohen g omez rodr iguez satta two thousand and eleven parser family non projective transition base dependency parsers allow polynomial time exact inference include novel parsers better coverage cohen et al two thousand and eleven even variant reduce time complexity ofn6 improve know bound exact inference non projective transition base parse hope piece theoretical work inspire design novel transition systems better coverage better run time guarantee code available https githubcom tzshi nonproj dp variants naacl2018
entity link involve align textual mention name entities correspond entries knowledge base entity link systems often exploit relations textual mention document eg coreference decide link decisions compatible unlike previous approach rely supervise systems heuristics predict relations treat relations latent variables neural entity link model induce relations without supervision optimize entity link system end end fashion multi relational model achieve best report score standard benchmark aida conll substantially outperform relation agnostic version train also converge much faster suggest inject structural bias help explain regularities train data
paper present watasense unsupervised system word sense disambiguation give sentence system choose relevant sense input word respect semantic similarity give sentence synset constitute sense target word watasense two modes operation sparse mode use traditional vector space model estimate similar word sense correspond context dense mode instead use synset embeddings cope sparsity problem describe architecture present system also conduct evaluation three different lexical semantic resources russian find dense mode substantially outperform sparse one datasets accord adjust rand index
end end learn framework useful build dialog systems simplicity train efficiency model update however current end end approach consider user semantic input learn utilize user information therefore propose include user sentiment obtain multimodal information acoustic dialogic textual end end learn framework make systems user adaptive effective incorporate user sentiment information supervise reinforcement learn settings settings add sentiment information reduce dialog length improve task success rate bus information search task work first attempt incorporate multimodal user information adaptive end end dialog system train framework attain state art performance
introduce neural particle smooth sequential monte carlo method sample annotations input string give probability model contrast conventional particle filter algorithms train proposal distribution look ahead end input string mean right leave lstm demonstrate innovation improve quality sample motivate formal choices explain neural model neural sampler view low dimensional nonlinear approximations work hmms large state space
subword units effective way alleviate open vocabulary problems neural machine translation nmt sentence usually convert unique subword sequence subword segmentation potentially ambiguous multiple segmentations possible even vocabulary question address paper whether possible harness segmentation ambiguity noise improve robustness nmt present simple regularization method subword regularization train model multiple subword segmentations probabilistically sample train addition better subword sample propose new subword segmentation algorithm base unigram language model experiment multiple corpora report consistent improvements especially low resource domain settings
neural network show achieve impressive result sentence level sentiment analysis target aspect base sentiment analysis tabsa extraction fine grain opinion polarity wrt pre define set aspects remain difficult task motivate recent advance memory augment model machine read propose novel architecture utilise external memory chain delay memory update mechanism track entities tabsa task propose model demonstrate substantial improvements state art approach include use external knowledge base
metric validation grammatical error correction gec currently do observe correlation human metric induce rank however correlation study costly methodologically troublesome suffer low inter rater agreement propose maege automatic methodology gec metric validation overcome many difficulties exist practice experiment maege would new light metric quality show example standard m2 metric fare poorly corpus level rank moreover use maege perform detail analysis metric behavior show correct type errors consistently penalize exist metrics
paper describe bomji supervise system capture discriminative attribute word pair eg yellow discriminative banana watermelon system rely xgb classifier train carefully engineer graph pattern word embed base feature participate semeval two thousand and eighteen task ten capture discriminative attribute achieve f1 score seventy-three rank 2nd twenty-six participant systems
prevalent use reference evaluate text text generation know bias estimate quality low coverage bias lcb paper show overcome lcb grammatical error correction gec evaluation attain scale increase number reference feasible range contrary previous suggestions due long tail distribution valid corrections sentence concretely show lcb incentivizes gec systems avoid correct even generate valid correction consequently exist systems obtain comparable superior performance compare humans make target change input similar effect text simplification support claim
present newsroom summarization dataset thirteen million article summaries write author editors newsrooms thirty-eight major news publications extract search social media metadata one thousand, nine hundred and ninety-eight two thousand and seventeen high quality summaries demonstrate high diversity summarization style particular summaries combine abstractive extractive strategies borrow word phrase article vary rat analyze extraction strategies use newsroom summaries datasets quantify diversity difficulty new data train exist methods data evaluate utility challenge
describe batch beam decode algorithm nmt lmbr n gram posteriors show lmbr techniques still yield gain top best recently report result transformers also discuss acceleration strategies deployment effect beam size batch memory speed
paper present nli pt first portuguese dataset compile native language identification nli task identify author first language base second language write dataset include one thousand, eight hundred and sixty-eight student essay write learners european portuguese native speakers follow l1s chinese english spanish german russian french japanese italian dutch tetum arabic polish korean romanian swedish nli pt include original student text four different type annotation pos fine grain pos constituency parse dependency parse nli pt use nli also research several topics field second language acquisition educational nlp discuss possible applications dataset present result obtain first lexical baseline system portuguese nli
humans read make sequence fixations saccades often skip word without apparent detriment understand offer novel explanation skip readers optimize tradeoff perform language relate task fixate word possible propose neural architecture combine attention module decide whether skip word task module memorize input show model predict human skip behavior also model read time well even though skip forty input key prediction model different read task result different skip behaviors confirm prediction eye track experiment participants answer question text able capture experimental result use model replace memorization module task module perform neural question answer
introduce task zero shoot style transfer different languages train data include multilingual parallel corpora contain parallel sentence style similarly recent previous work propose unify multilingual multi style machine translation system design allow perform zero shoot style conversions inference moreover monolingually cross lingually model allow increase presence dissimilar style corpus three time easily learn operate various contractions provide reasonable lexicon swap see manual evaluation
development neural machine translation quality machine translation systems improve significantly exploit advancements deep learn systems able better approximate complex map source sentence target sentence ability new challenge also arise example translation partial sentence low latency speech translation since model see complete sentence train always try generate complete sentence though input may partial sentence show nmt systems adapt scenarios task specific train data available furthermore possible without lose performance original train data achieve create artificial data use multi task learn adaptation able reduce number corrections display incremental output construction forty-five without decrease translation quality
paper investigate code switch detection performance code switch cs automatic speech recognition asr system data augment acoustic language model focus recognition frisian dutch radio broadcast one mix languages namely frisian resourced recently explore acoustic model benefit monolingual speech data belong high resourced mix language purpose train state art ams significantly increase amount cs speech apply automatic transcription monolingual dutch speech moreover improve language model lm create cs text various ways include text generation use recurrent lms train exist cs text motivate significantly improve cs asr performance delve cs detection performance asr system work report cs detection accuracies together detail detection error analysis
speech recognition sequence prediction problem besides employ various deep learn approach framelevel classification sequence level discriminative train prove indispensable achieve state art performance large vocabulary continuous speech recognition lvcsr however keyword spot kws one common speech recognition task almost benefit frame level deep learn due difficulty get compete sequence hypotheses study sequence discriminative train kws limit fix vocabulary lvcsr base methods compare state art deep learn base kws approach paper sequence discriminative train framework propose fix vocabulary unrestricted acoustic kws sequence discriminative train sequence level generative discriminative model systematically investigate introduce word independent phone lattices non keyword blank symbols construct compete hypotheses feasible efficient sequence discriminative train approach propose acoustic kws experiment show propose approach obtain consistent significant improvement fix vocabulary unrestricted kws task compare previous frame level deep learn base acoustic kws methods
recent advance deep learn base large vocabulary con tinuous speech recognition lvcsr invoke grow demand large scale speech transcription inference process speech recognizer find sequence label whose correspond acoustic language model best match input feature one main computation include two stag acoustic model inference linguistic search weight finite state transducer wfst large computational overheads stag hamper wide application lvcsr benefit stronger classifiers deep learn powerful compute devices propose general ideas initial trials solve fundamental problems
follow approach understand lexical mean develop yaska patanjali bhartrihari indian linguistic traditions extend approach develop leibniz brentano modern time framework formal ontology language develop framework propose mean word form intrinsic extrinsic ontological structure paper aim capture intrinsic extrinsic mean word two major indian languages namely hindi telugu part speech render sense type sense class use develop gold standard annotate lexical resource support semantic understand language resource collection hindi telugu lexicons manually annotate native speakers languages follow annotation guidelines resource utilise derive adverbial sense class distribution verbs karaka verb sense type distribution different corpora news novels compare use verb sense type distribution word embed use aid enrichment resource work progress aim lexical coverage language extensively
research describe paper concern automatic cyberbullying detection social media two goals achieve build gold standard cyberbullying detection dataset measure performance samurai cyberbullying detection system formspring dataset provide kaggle competition annotate part research annotation procedure describe detail unlike many recent data annotation initiatives use mechanical turk find people will perform annotation new annotation compare old one seem coherent since test cyberbullying detection system perform better former performance samurai system compare five commercial systems one well know machine learn algorithm use classify textual content namely fasttext turn samurai score best measure accuracy precision recall fasttext second best perform algorithm
work focus lightweight convolutional architecture create fix size vector embeddings sentence representations useful build nlp systems include conversational agents work derive recently propose recursive convolutional architecture auto encode text paragraph byte level propose alternations significantly reduce train time number parameters improve auto encode accuracy finally evaluate representations create model task senteval benchmark suite show serve better yet fairly low resource alternative popular bag word embeddings
paper multi task ensemble framework address three problems emotion sentiment analysis ie emotion classification intensity valence arousal dominance emotion valence arousal sentiment underlie problems cover two granularities ie coarse grain fine grain diverse range domains ie tweet facebook post news headline blog letter etc ensemble model aim leverage learn representations three deep learn model ie cnn lstm gru hand craft feature representation predictions experimental result benchmark datasets show efficacy propose multi task ensemble frameworks obtain performance improvement two three point average single task systems problems domains
recently seq2seq abstractive summarization model achieve good result cnn daily mail dataset still improve abstractive methods extractive methods good research direction since extractive methods potentials exploit various efficient feature extract important sentence one text paper order improve semantic relevance abstractive summaries adopt wordnet base sentence rank algorithm extract sentence semantically one text design dual attentional seq2seq framework generate summaries consideration extract information time combine pointer generator coverage mechanisms solve problems vocabulary oov word duplicate word exist abstractive model experiment cnn daily mail dataset show model achieve competitive performance state art rouge score human evaluations also show summaries generate model high semantic relevance original text
computational linguistics large body work exist distribute model lexical relations focus largely lexical relations hypernymy scientist person hold two categories express common nouns contrast computational linguistics pay little attention entities denote proper nouns marie curie mumbai investigate detail knowledge representation semantic web communities generally regard linguistic properties paper close gap investigate model lexical relation instantiation hold entity denote category denote expression marie curie scientist mumbai city present new principled dataset task instantiation detection well experiment analyse dataset obtain follow result entities belong one category form region distributional space embed category word typically locate outside subspace b easy learn distinguish entities categories distributional evidence due instantiation proper much harder learn use common nouns representations categories c problem alleviate use category representations base entity rather category word embeddings
mine social media message tweet article facebook post health drug relate information receive significant interest pharmacovigilance research social media sit eg twitter use monitor drug abuse adverse reactions drug usage analyze expression sentiments relate drug study base aggregate result large population rather specific set individuals order conduct study individual level specific cohorts identify post mention intake medicine user necessary towards objective develop classifier identify mention personal intake medicine tweet train stack ensemble shallow convolutional neural network cnn model annotate dataset use random search tune hyper parameters cnn model present ensemble best model prediction task system produce state art result micro average f score six hundred and ninety-three believe develop classifier direct use areas psychology health informatics pharmacovigilance affective compute track moods emotions sentiments patients express intake medicine social media
exist speech recognition systems typically build sentence level although know dialog context eg higher level knowledge span across sentence speakers help process long conversations recent progress end end speech recognition systems promise integrate available information eg acoustic language resources single model jointly optimize seem natural dialog context information thus also integrate end end model improve recognition accuracy work present dialog context aware speech recognition model explicitly use context information beyond sentence level information end end fashion dialog context model capture history sentence level context whole system train dialog context information end end manner evaluate propose approach switchboard conversational speech corpus show system outperform comparable sentence level end end speech recognition system
word2vec represent word text vectors carry semantic information audio word2vec show able represent signal segment speak word vectors carry phonetic structure information audio word2vec train unsupervised way unlabeled corpus except word boundaries need paper extend audio word2vec word level utterance level propose new segmental audio word2vec unsupervised speak word boundary segmentation audio word2vec jointly learn mutually enhance utterance directly represent sequence vectors carry phonetic structure information achieve segmental sequence sequence autoencoder ssae segmentation gate train reinforcement learn insert encoder experiment english czech french german show good performance unsupervised speak word segmentation speak term detection applications significantly better frame base dtw
read comprehension machine widely study machine comprehension speak content still less investigate problem paper release open domain speak question answer dataset odsqa three thousand question best knowledge largest real sqa dataset dataset find asr errors catastrophic impact sqa mitigate effect asr errors subword units involve bring consistent improvements model find data augmentation text base qa train examples improve sqa
unsupervised pre train word embeddings use effectively many task natural language process leverage unlabeled textual data often embeddings either use initializations fix word representations task specific classification model work extend classification model task loss unsupervised auxiliary loss word embed level model ensure learn word representations contain task specific feature learn supervise loss component general feature learn unsupervised loss component evaluate approach task temporal relation extraction particular narrative containment relation extraction clinical record show continue train embeddings unsupervised objective together task objective give better task specific embeddings result improvement state art thyme dataset use general domain part speech tagger linguistic resource
analyze fundamental design challenge impact development multilingual state art name entity transliteration system include curating bilingual name entity datasets evaluation multiple transliteration methods empirically evaluate transliteration task use traditional weight finite state transducer wfst approach two neural approach encoder decoder recurrent neural network method recent non sequential transformer method order improve availability bilingual name entity transliteration datasets release personal name bilingual dictionaries mind wikidata english russian hebrew arabic japanese katakana code dictionaries publicly available
domain adaptation arise aim learn source domain model per form acceptably well different target domain especially crucial natural language generation nlg speak dialogue systems sufficient annotate data source domain limit label data target domain effectively utilize much exist abilities source domains crucial issue domain adaptation paper propose adversarial train procedure train variational encoder decoder base language generator via multiple adaptation step procedure model first train source domain data fine tune small set target domain utterances guidance two propose critics experimental result show propose method effec tively leverage exist knowledge source domain adapt another relate domain use small amount domain data
clinicians spend significant amount time inputting free form textual note electronic health record ehr systems much documentation work see burden reduce time spend patients contribute clinician burnout aspiration ai assist note write propose new language model task predict content note condition past data patient medical record include patient demographics labs medications past note train generative model use public de identify mimic iii dataset compare generate note dataset multiple measure find much content predict many common templates find note learn discuss model useful support assistive note write feature error detection auto complete
one main challenge rank embed query document pair joint feature space feed learn rank algorithm achieve representation conventional state art approach perform extensive feature engineer encode similarity query answer pair recently deep learn solutions show possible achieve comparable performance settings learn similarity representation directly data unfortunately previous model perform poorly longer texts texts significant portion irrelevant information grammatically incorrect overcome limitations propose novel rank algorithm question answer qarat use attention mechanism learn word phrase focus build mutual representation demonstrate superior rank performance several real world question answer rank datasets provide visualization attention mechanism otter insights model attention could benefit rank difficult question answer challenge
paper describe tool debug output attention weight neural machine translation nmt systems improve estimations confidence output base attention purpose tool help researchers developers find weak faulty example translations nmt systems produce without need reference translations tool also include option directly compare translation output two different nmt engines experiment addition present demo website tool examples good bad translations http attentionlielakedalv
natural language generation nlg critical component speak dialogue systems classic nlg divide two phase one sentence plan decide overall sentence structure two surface realization determine specific word form flatten sentence structure string many simple nlg model base recurrent neural network rnn sequence sequence seq2seq model basically contain encoder decoder structure nlg model generate sentence scratch jointly optimize sentence plan surface realization use simple cross entropy loss train criterion however simple encoder decoder architecture usually suffer generate complex long sentence decoder learn grammar diction knowledge paper introduce hierarchical decode nlg model base linguistic pattern different level show propose method outperform traditional one smaller model size furthermore design hierarchical decode flexible easily extensible various nlg systems
machine read comprehension task model relationship passage query term deep learn framework state art model simply concatenate word character level representations show suboptimal concern task paper empirically explore different integration strategies word character embeddings propose character augment reader attend character level representation augment word embed short list improve word representations especially rare word experimental result show propose approach help baseline model significantly outperform state art baselines various public benchmarks
identify stance news article body respect certain headline first step automate fake news detection paper introduce two stage ensemble model solve stance detection task use hand craft feature input gradient boost classifier able achieve score ninety-one thousand, six hundred and fifteen one million, one hundred and sixty-five thousand, one hundred and twenty-five seven thousand, eight hundred and sixty-three official fake news challenge stage one dataset identify useful feature detect fake news discuss sample techniques use improve recall accuracy highly imbalanced dataset
effective representation text critical various natural language process task particular task chinese sentiment analysis important understand choose effective representation text different form chinese representations word character pinyin paper present systematic study effect representations chinese sentiment analysis propose multi channel convolutional neural network mccnn channel correspond representation experimental result show one word win dataset low oov rate character win otherwise two use representations combination generally improve performance three representations base mccnn outperform conventional ngram feature use svm four propose mccnn model achieve competitive performance state art model fasttext chinese sentiment analysis
automatic word problem solve always pose great challenge nlp community usually word problem narrative comprise sentence question ask quantity refer sentence solve word problem involve reason across sentence identification operations order relevant quantities discard irrelevant quantities paper present novel approach automatic arithmetic word problem solve approach start frame identification frame either classify state action frame frame identification dependent verb sentence every frame unique identify slot slot fill use dependency parse output sentence slot entity holder entity quantity entity recipient additional information like place time slot frame help identify type question ask entity refer action frame act state frame cause change quantities state frame frame use build graph change quantities propagate neighbor nod current solvers answer question relate quantity system answer different kinds question like quantity relate question many three major contributions paper one frame annotate corpus frame annotation tool two frame identification module three new easily understandable framework word problem solve
emotions crucial part compel narratives literature tell us people goals desire passions intentions emotion analysis part broader larger field sentiment analysis receive increase attention literary study past affective dimension literature mainly study context literary hermeneutics however emergence research field know digital humanities dh study emotions literary context take computational turn give fact dh still form field direction research render relatively new survey offer overview exist body research emotion analysis apply literature research review deal variety topics include track dramatic change plot development network analysis literary text understand emotionality texts among topics
pos tag serve preliminary task many nlp applications kannada relatively poor indian language limit number quality nlp tool available use accurate reliable pos tagger essential many nlp task like shallow parse dependency parse sentiment analysis name entity recognition present statistical pos tagger kannada use different machine learn neural network model kannada pos tagger outperform state art kannada pos tagger six contribution paper three fold build generic pos tagger compare performances different model techniques explore use character word embeddings together kannada pos tag
sentiment analysis indian languages sail code mix tool contest aim identify sentence level sentiment polarity code mix dataset indian languages pair hi en ben hi en hi en dataset henceforth refer hi en ben hi en dataset bn en respectively submit four model sentiment analysis code mix hi en bn en datasets first model ensemble vote classifier consist three classifiers linear svm logistic regression random forest second one linear svm model use tf idf feature vectors character n grams n range two six use scikit learn sklearn machine learn library implement approach run1 obtain vote classifier run2 use linear svm model produce result four submit output run2 outperform run1 datasets finish first contest hi en f score five hundred and sixty-nine bn en f score five hundred and twenty-six
maintain efficient semantic representations environment major challenge humans machine human languages represent useful solutions problem yet clear computational principle could give rise similar solutions machine work propose answer open question suggest languages compress percepts word optimize information bottleneck ib tradeoff complexity accuracy lexicons present empirical evidence principle may give rise human like semantic representations explore human languages categorize color show color name systems across languages near optimal ib sense natural systems similar artificial ib color name systems single tradeoff parameter control cross language variability addition ib systems evolve sequence structural phase transition demonstrate possible adaptation process work thus identify computational principle characterize human semantic systems could usefully inform semantic representations machine
traditional chatbots usually need mass human dialogue data especially use supervise machine learn method though easily deal single turn question answer multi turn performance usually unsatisfactory paper present lingke information retrieval augment chatbot able answer question base give product introduction document deal multi turn conversations introduce fine grain pipeline process distill responses base unstructured document attentive sequential context response match multi turn conversations
hybrid approach transliteration algerian arabizi primary study paper present hybrid approach transliteration algerian arabizi define set rule enable us passage arabizi arabic rule generate set candidates transliteration arabizi word arabic extract best candidate approach evaluate use three test corpora obtain result show improvement precision score equal seven thousand, five hundred and eleven best result result allow us verify approach competitive compare others work treat arabizi transliteration general keywords arabizi dialecte alg erien arabizi alg erien translit eration
determine whether give claim support evidence fundamental nlp problem best model textual entailment however give large collection text find evidence could support refute give claim challenge amplify fact different evidence might need support refute claim nevertheless prior work decouple evidence identification determine truth value claim give evidence propose consider two aspects jointly develop twowingos two wing optimization strategy system identify appropriate evidence claim also determine whether claim support evidence give claim twowingos attempt identify subset evidence candidates give predict evidence attempt determine truth value correspond claim treat challenge couple optimization problems train joint model twowingos offer two advantage unlike pipeline systems facilitate flexible size evidence set ii joint train improve claim entailment evidence identification experiment benchmark dataset show state art performance code https githubcom yinwenpeng fever
paper present latest investigation densely connect convolutional network densenets acoustic model automatic speech recognition densen ets deep compact convolutional neural network demonstrate incredible improvements state art result several data set computer vision experimental result show densenet use significantly outperform neural base model dnns cnns vggs furthermore result wall street journal reveal half train data densenet able outperform model train full data set large margin
propose novel unsupervised keyphrase extraction approach filter candidate keywords use outlier detection start train word embeddings target document capture semantic regularities among word use minimum covariance determinant estimator model distribution non keyphrase word vectors assumption vectors come distribution indicative irrelevance semantics express dimension learn vector representation candidate keyphrases consist word detect outliers dominant distribution empirical result show approach outperform state art recent unsupervised keyphrase extraction methods
attention mechanisms sequence sequence model show great ability wonderful performance various natural language process nlp task sentence embed text generation machine translation machine read comprehension etc unfortunately exist attention mechanisms learn either high level low level feature paper think lack hierarchical mechanisms bottleneck improve performance attention mechanisms propose novel hierarchical attention mechanism ham base weight sum different layer multi level attention ham achieve state art bleu score twenty-six chinese poem generation task nearly sixty-five average improvement compare exist machine read comprehension model bidaf match lstm furthermore experiment theorems reveal ham greater generalization representation ability exist attention mechanisms
background give importance relation event extraction biomedical research publications support knowledge capture synthesis strong dependency approach information extraction task syntactic information valuable understand approach syntactic process biomedical text highest performance result perform empirical study compare state art traditional feature base neural network base model two core natural language process task part speech pos tag dependency parse two benchmark biomedical corpora genia craft best knowledge recent work make comparisons biomedical context specifically detail analysis neural model data available experimental result show general neural model outperform feature base model two benchmark biomedical corpora genia craft also perform task orient evaluation investigate influence model downstream application biomedical event extraction show better intrinsic parse performance always imply better extrinsic event extraction performance conclusion present detail empirical study compare traditional feature base neural network base model pos tag dependency parse biomedical context also investigate influence parser selection biomedical event extraction downstream task availability data material make retrain model available https githubcom datquocnguyen bioposdep
ancient chinese bring wisdom spirit culture chinese nation automatic translation ancient chinese modern chinese help inherit carry forward quintessence ancients however lack large scale parallel corpus limit study machine translation ancient modern chinese paper propose ancient modern chinese clause alignment approach base characteristics two languages method combine lexical base information statistical base information achieve nine hundred and forty-two f1 score manual annotation test set use method create new large scale ancient modern chinese parallel corpus contain 124m bilingual pair best knowledge first large high quality ancient modern chinese dataset furthermore analyze compare performance smt various nmt model dataset provide strong baseline task
replicate variation image caption architecture vinyals et al two thousand and fifteen introduce dropout inference mode simulate effect neurodegenerative diseases like alzheimer disease ad wernicke aphasia wa evaluate effect dropout language production measure kl divergence word frequency distributions linguistic metrics dropout add find generate sentence closely approximate word frequency distribution train corpus use moderate dropout four inference
objective annotation expensive essential clinical note review clinical natural language process cnlp however extent computer generate pre annotation beneficial human annotation still open question study introduce clean clinical note review annotation pre annotation base cnlp annotation system improve clinical note annotation data elements comprehensively compare clean widely use annotation system brat rapid annotation tool brat materials methods clean include ensemble pipeline clean ep newly develop annotation tool clean domain expert novice user annotator participate comparative usability test tag eighty-seven data elements relate congestive heart failure chf kawasaki disease kd cohorts eighty-four public note result clean achieve higher note level f1 score eight hundred and ninety-six brat eight hundred and twenty significant difference correctness p value one mostly relate factor system software p value one significant difference p value one hundred and eighty-eight annotation time observe clean seven thousand, two hundred and sixty-two minutes note brat eight thousand, two hundred and eighty-six minutes note difference mostly associate note length p value one system software p value thirteen expert report clean useful satisfactory novice report slight improvements discussion clean improve correctness annotation increase usefulness satisfaction level efficiency limitations include untested impact pre annotation correctness rate small sample size small user size restrictedly validate gold standard conclusion clean pre annotation beneficial expert deal complex annotation task involve numerous diverse target data elements
semantic role label srl recognize predicate argument structure sentence include subtasks predicate disambiguation argument label previous study usually formulate entire srl problem two subtasks first time paper introduce end end neural model unifiedly tackle predicate disambiguation argument label one shoot use biaffine scorer model directly predict semantic role label give word pair sentence without rely syntactic parse information specifically augment bilstm encoder non linear transformation distinguish predicate argument give sentence model semantic role label process word pair classification task employ biaffine attentional mechanism though propose model syntax agnostic local decoder outperform state art syntax aware srl systems conll two thousand and eight two thousand and nine benchmarks english chinese best knowledge report first syntax agnostic srl model surpass know syntax aware model
sentence encoders typically train language model task large unlabeled datasets encoders achieve state art result many sentence level task difficult train long train cycle introduce fake sentence detection new train task learn sentence encoders automatically generate fake sentence corrupt original sentence source collection train encoders produce representations effective detect fake sentence binary classification task turn quite efficient train sentence encoders compare basic bilstm encoder train task strong sentence encode model skipthought fastsent train language model task find bilstm train much faster fake sentence detection twenty hours instead weeks use smaller amount data 1m instead 64m sentence analysis show learn representations capture many syntactic semantic properties expect good sentence representations
current state art machine translation systems base encoder decoder architectures first encode input sequence generate output sequence base input encode interfaced attention mechanism recombine fix encode source tokens base decoder state propose alternative approach instead rely single 2d convolutional neural network across sequence layer network cod source tokens basis output sequence produce far attention like properties therefore pervasive throughout network model yield excellent result outperform state art encoder decoder systems conceptually simpler fewer parameters
develop conversational systems converse many languages interest challenge natural language process paper introduce multilingual addressee response selection task conversational system predict appropriate addressee response input message multiple languages key develop multilingual respond systems utilize high resource language data compensate low resource language data present several knowledge transfer methods conversational systems evaluate methods create new multilingual conversation dataset experiment dataset demonstrate effectiveness methods
paper present empirical exploration use capsule network text classification show capsule network effective image classification validity domain text explore paper show capsule network indeed potential text classification several advantage convolutional neural network suggest simple rout method effectively reduce computational complexity dynamic rout utilize seven benchmark datasets demonstrate capsule network along propose rout method provide comparable result
name entity recognition ner use identify relevant entities text bidirectional lstm long short term memory encoder neural conditional random field crf decoder bilstm crf state art methodology work do analysis several methods intend optimize performance network base architecture case encourage overfitting avoidance methods target exploration parameter space regularization lstms penalization confident output distributions result show optimization methods improve performance bilstm crf ner baseline system set new state art performance conll two thousand and three spanish set f1 eight thousand, seven hundred and eighteen
although neural machine translation nmt achieve remarkable progress past several years nmt systems still suffer fundamental shortcoming sequence generation task errors make early generation process feed input model quickly amplify harm subsequent sequence generation address issue propose novel model regularization method nmt train aim improve agreement translations generate leave right l2r right leave r2l nmt decoders goal achieve introduce two kullback leibler divergence regularization term nmt train objective reduce mismatch output probabilities l2r r2l model addition also employ joint train strategy allow l2r r2l model improve interactive update process experimental result show propose method significantly outperform state art baselines chinese english english german translation task
language style transfer problem migrate content source sentence target style many applications parallel train data available source sentence transfer may arbitrary unknown style first sentence encode content style latent representations recombine content target style decode sentence align target domain adequately constrain encode decode function couple two loss function first style discrepancy loss enforce style representation accurately encode style information guide discrepancy sentence style target style second cycle consistency loss ensure transfer sentence preserve content original sentence disentangle style validate effectiveness model three task sentiment modification restaurant review dialog response revision romantic style sentence rewrite shakespearean style
paper propose task live comment generation live comment new form comment videos regard mixture comment chat high quality live comment relevant video also interactive users work first construct new dataset live comment generation propose novel end end model generate human like live comment refer video users comment finally evaluate model construct dataset experimental result show method significantly outperform baselines
approach knowledge base question answer base semantic parse paper address problem learn vector representations complex semantic parse consist multiple entities relations previous work largely focus select correct semantic relations question disregard structure semantic parse connections entities directions relations propose use gate graph neural network encode graph structure semantic parse show two data set graph network outperform baseline model explicitly model structure error analysis confirm approach successfully process complex semantic parse
study three general multi task learn mtl approach eleven sequence tag task extensive empirical result show fifty case jointly learn eleven task improve upon either independent pairwise learn task also show pairwise mtl inform us task benefit others task benefit learn jointly particular identify task always benefit others well task always harm others interestingly one mtl approach yield embeddings task reveal natural cluster semantic syntactic task inquiries open doors utilization mtl nlp
compare performance apt autoprf metrics pronoun translation manually annotate dataset comprise human judgements correctness translations protest test suite although correlation human judgements range issue limit performance automate metrics instead recommend use semi automatic metrics test suit place fully automatic metrics
paper examine problem adapt neural machine translation systems new low resourced languages lrls effectively rapidly possible propose methods base start massively multilingual seed model train ahead time continue train data relate lrl contrast number strategies lead novel simple yet effective method similar language regularization jointly train lrl interest similar high resourced language prevent fit small lrl data experiment demonstrate massively multilingual model even without explicit adaptation surprisingly effective achieve bleu score one hundred and fifty-five data lrl propose similar language regularization method improve adaptation methods seventeen bleu point average four lrl settings code reproduce experiment https githubcom neubig rapid adaptation
character level model tokens show effective deal within token noise vocabulary word model still rely correct token boundaries paper propose novel end end character level model demonstrate effectiveness multilingual settings token boundaries noisy model semi markov conditional random field neural network character segment representation require tokenizer model match state art baselines various languages significantly outperform noisy english version part speech tag benchmark dataset code noisy dataset publicly available http cisterncislmude semicrf
use two small parallel corpora compare morphological complexity spanish otomi nahuatl languages belong different linguistic families latter low resourced take account two quantitative criteria one hand distribution type tokens corpus perplexity entropy indicators word structure predictability show language complex term many different morphological word form produce however may less complex term predictability internal structure word
paper tackle problem disentangle latent variables style content language model propose simple yet effective approach incorporate auxiliary multi task adversarial objectives label prediction bag word prediction respectively show qualitatively quantitatively style content indeed disentangle latent space disentangle latent representation learn method apply style transfer non parallel corpora achieve substantially better result term transfer accuracy content preservation language fluency comparison previous state art approach
deep learn methods employ multiple process layer learn hierarchial representations data already deploy humongous number applications produce state art result recently growth process power computers able high dimensional tensor calculations natural language process nlp applications give significant boost term efficiency well accuracy paper take look various signal process techniques application produce speech text system use deep recurrent neural network
structural plan important produce long sentence miss part current language generation model work add plan phase neural machine translation control coarse structure output sentence model first generate planner cod predict real output word condition cod learn capture coarse structure target sentence order obtain cod design end end neural network discretization bottleneck predict simplify part speech tag target sentence experiment show translation performance generally improve plan ahead also find translations different structure obtain manipulate planner cod
polysemy common phenomenon modern languages many circumstances exist primal mean expression define primal mean expression frequently use sense expression frequent sense deduce many new appear mean expressions either originate primal mean merely literal reference original expression eg apple fruit apple inc apple movie construct knowledge base line encyclopedia data would efficient aware information importance sense paper would like explore way automatically recommend primal mean expression base textual descriptions multiple sense expression line encyclopedia websites propose hybrid model capture pattern description relationship different descriptions weakly supervise unsupervised model experiment result show method yield good result p1 precision score eight hundred and thirty-three per cent map mean average precision nine hundred and five per cent surpass umfs baseline big margin p1 six hundred and eleven per cent map seven hundred and sixty-three per cent
study cross lingual sequence tag little label data target language adversarial train previously show effective train cross lingual sentence classifiers however clear language agnostic representations enforce adversarial language discriminator also enable effective transfer token level prediction task therefore experiment different type adversarial train two task dependency parse sentence compression show adversarial train consistently lead improve cross lingual performance task compare conventionally train baseline
sequence generation model dialogue know several problems tend produce short generic sentence uninformative unengaging retrieval model hand surface interest responses restrict give retrieval set lead erroneous reply tune specific context work develop model combine two approach avoid deficiencies first retrieve response refine final sequence generator treat retrieval additional context show recent convai2 challenge task approach produce responses superior standard retrieval generation model human evaluations
paper present ensemble base systems dialect language variety identification use datasets make available organizers vardial evaluation campaign two thousand and eighteen present system develop discriminate flemish dutch subtitle system train discriminate four arabic dialects egyptian levantine gulf north african modern standard arabic speech broadcast finally compare performance two systems systems submit discriminate dutch flemish subtitle dfs arabic dialect identification adi share task vardial two thousand and eighteen
kgcleaner framework identify correct errors data produce deliver information extraction system task understudy kgcleaner first address introduce multi task model jointly learn predict extract relation credible repair evaluate approach model instance framework two collections wikidata corpus nearly 700k facts 5m fact relevant sentence collection 30k facts two thousand and fifteen tac knowledge base population task credibility classification parameter efficient simple shallow neural network achieve absolute performance gain thirty f1 point wikidata comparable performance tac repair task significant performance twice gain obtain depend nature dataset model
non local feature exploit syntactic parsers capture dependencies sub output structure feature key success state art statistical parsers rise deep learn however show local output decisions give highly competitive accuracies thank power dense neural input representations embody global syntactic information investigate two conceptually simple local neural model constituent parse make local decisions constituent span cfg rule respectively consistent previous find along line best model give highly competitive result achieve label bracket f1 score nine hundred and twenty-four ptb eight hundred and seventy-three ctb fifty-one
text generation fundamental build block natural language process task exist sequential model perform autoregression directly text sequence difficulty generate long sentence complex structure paper advocate simple approach treat sentence generation tree generation task explicitly model syntactic structure constituent syntactic tree perform top breadth first tree generation model fix dependencies appropriately perform implicit global plan contrast transition base depth first generation process difficulty deal incomplete texts parse also incorporate future contexts plan preliminary result two generation task one parse task demonstrate effective strategy
classic grammars regular expressions use variety purpose include parse intent detection match however comparisons perform structural level constituent elements word character match exactly recent advance word embeddings show semantically relate word share common feature vector space representation suggest possibility hybrid grammar word embed paper blend structure standard context free grammars semantic generalization capabilities word embeddings create hybrid semantic grammars semantic grammars generalize specific terminals use programmer word phrase relate mean allow construction compact grammars match entire region vector space rather match specific elements
folksonomy movies cover wide range heterogeneous information movies like genre plot structure visual experience soundtracks metadata emotional experience watch movie able automatically generate predict tag movies help recommendation engines improve retrieval similar movies help viewers know expect movie advance work explore problem create tag movies plot synopses propose novel neural network model merge information synopses emotion flow throughout plot predict set tag movies compare system multiple baselines find addition emotion flow boost performance network learn eighteen tag traditional machine learn system
automatic question generation qg useful yet challenge task nlp recent neural network base approach represent state art task work attempt strengthen significantly adopt holistic novel generator evaluator framework directly optimize objectives reward semantics structure generator sequence sequence model incorporate structure semantics question generate generator predict answer passage question pivot employ copy coverage mechanisms also acknowledge contextually important possibly rare keywords passage question need conform redundantly repeat word evaluator model evaluate assign reward predict question base conformity structure grind truth question propose two novel qg specific reward function text conformity answer conformity generate question evaluator also employ structure sensitive reward base evaluation measure bleu gleu rouge l suitable qg contrast previous work optimize cross entropy loss induce inconsistencies train objective test evaluation measure evaluation show approach significantly outperform state art systems widely use squad benchmark per automatic human evaluation
chinese word segmentation cws often regard character base sequence label task current work achieve great success help powerful neural network however work neglect important clue chinese character incorporate semantic phonetic mean paper introduce multiple character embeddings include pinyin romanization wubi input easily accessible effective depict semantics character propose novel share bi lstm crf model fuse linguistic feature efficiently share lstm network train procedure extensive experiment five corpora show extra embeddings help obtain significant improvement label accuracy specifically achieve state art performance cityu corpora f1 score nine hundred and sixty-nine nine hundred and seventy-three respectively without leverage external lexical resources
rise social media enable people freely express opinions products service aim sentiment analysis automatically determine subject sentiment eg positive negative neutral towards particular aspect topic product movie news etc deep learn recently emerge powerful machine learn technique tackle grow demand accurate sentiment analysis however limit work conduct apply deep learn algorithms languages english persian work two deep learn model deep autoencoders deep convolutional neural network cnns develop apply novel persian movie review dataset propose deep learn model analyze compare state art shallow multilayer perceptron mlp base machine learn model simulation result demonstrate enhance performance deep learn state art mlp
data annotation important time consume costly procedure sort text two class first thing need good annotation guideline establish require qualify class literature difficulties associate appropriate data annotation underestimate paper present novel approach automatically construct annotate sentiment corpus algerian dialect maghrebi arabic dialect construction corpus base algerian sentiment lexicon also construct automatically present work deal two widely use script arabic social media arabic arabizi propose approach automatically construct sentiment corpus contain eight thousand message four thousand dedicate arabic four thousand arabizi achieve f1 score seventy-two seventy-eight arabic arabizi test set respectively ongoing work aim integrate transliteration process arabizi message improve obtain result
recent neural model data document generation achieve remarkable progress produce fluent informative texts however large proportion generate texts actually conform input data address issue propose new train framework attempt verify consistency generate texts input data guide train process measure consistency relation extraction model apply check information overlap input data generate texts non differentiable consistency signal optimize via reinforcement learn experimental result recently release challenge dataset rotowire show improvements framework various metrics
give partial description like open hood car humans reason situation anticipate might come next examine engine paper introduce task ground commonsense inference unify natural language inference commonsense reason present swag new dataset 113k multiple choice question rich spectrum ground situations address recur challenge annotation artifacts human bias find many exist datasets propose adversarial filter af novel procedure construct de bias dataset iteratively train ensemble stylistic classifiers use filter data account aggressive adversarial filter use state art language model massively oversample diverse set potential counterfactuals empirical result demonstrate humans solve result inference problems high accuracy eighty-eight various competitive model struggle task provide comprehensive analysis indicate significant opportunities future research
cluster lexicon word well study problem natural language process nlp word cluster use deal sparse data statistical language process well feature solve various nlp task text categorization question answer name entity recognition others spectral cluster widely use technique field image process speech recognition however scarcely explore context nlp specifically method use meila shi two thousand and one never use cluster general word lexicon apply spectral cluster lexicon word evaluate result cluster use feature solve two classical nlp task semantic role label dependency parse compare performance brown cluster widely use technique word cluster well cluster methods show spectral cluster produce similar result brown cluster outperform cluster methods addition quantify overlap spectral brown cluster show model capture information uncaptured
huge number new word emerge every day lead great need represent semantic mean understandable nlp systems sememes define minimum semantic units human languages combination represent mean word manual construction sememe base knowledge base time consume labor intensive fortunately communities devote compose descriptions word wiki websites paper explore automatically predict lexical sememes base descriptions word wiki websites view problem weakly order multi label task propose label distribute seq2seq model ld seq2seq novel soft loss function solve problem experiment take real world sememe knowledge base hownet correspond descriptions word baidu wiki train evaluation result show ld seq2seq model beat baselines significantly test set also outperform amateur human annotators random subset test set
sentence embed important research topic natural language process essential generate good embed vector fully reflect semantic mean sentence order achieve enhance performance various natural language process task machine translation document classification thus far various sentence embed model propose feasibility demonstrate good performances task follow embed sentiment analysis sentence classification however performances sentence classification sentiment analysis enhance use simple sentence representation method sufficient claim model fully reflect mean sentence base good performances task paper inspire human language recognition propose follow concept semantic coherence satisfy good sentence embed method similar sentence locate close embed space propose paraphrase think p think model pursue semantic coherence much possible experimental result two paraphrase identification datasets ms coco sts benchmark show p think model outperform benchmarked sentence embed methods
present overview clef two thousand and eighteen checkthat lab automatic identification verification political claim focus task one check worthiness task ask predict claim political debate prioritize fact check particular give debate political speech goal produce rank list sentence base worthiness fact check offer task english arabic base debate two thousand and sixteen us presidential campaign well speeches campaign total thirty team register participate lab seven team actually submit systems task1 successful approach use participants rely recurrent multi layer neural network well combinations distributional representations match claim vocabulary lexicons measure syntactic dependency best systems achieve mean average precision eighteen fifteen english arabic test datasets respectively leave large room improvement thus release datasets score script enable research check worthiness estimation
present path2vec new approach learn graph embeddings rely structural measure pairwise node similarities model learn representations nod dense space approximate give user define graph distance measure eg shortest path distance distance measure take information beyond graph structure account evaluation propose model semantic similarity word sense disambiguation task use various wordnet base similarity measure show approach yield competitive result outperform strong graph embed baselines model computationally efficient order magnitude faster direct computation graph base distance
trustfulness one general tendency confidence unknown people situations predict many important real world outcomes mental health likelihood cooperate others clinicians data drive measure interpersonal trust previously introduce develop first language base assessment personality trait trustfulness fit one language accept questionnaire base trust score use trustfulness type case study explore role questionnaire size well word count develop language base predictive model users psychological traits find leverage longer questionnaire yield greater test set accuracy train find beneficial include users take smaller questionnaires offer observations train similarly note decrease individual prediction error word count increase find word count weight train scheme helpful users first place
statistical machine translation systems translate word unseen train data however humans translate many class vocabulary oov word eg novel morphological variants misspell compound without context use orthographic clue follow observation describe evaluate several general methods oov translation use subword information pose oov translation problem standalone task intrinsically evaluate approach fourteen typologically diverse languages across vary resource level add oov translators statistical machine translation system yield consistent bleu gain five point average twenty fourteen languages especially low resource scenarios
machine read comprehension unanswerable question aim abstain answer answer infer addition extract answer previous work usually predict additional answer probability detect unanswerable case however fail validate answerability question verify legitimacy predict answer address problem propose novel read verify system utilize neural reader extract candidate answer produce answer probabilities also leverage answer verifier decide whether predict answer entail input snippets moreover introduce two auxiliary losses help reader better handle answer extraction well answer detection investigate three different architectures answer verifier experiment squad twenty dataset show system achieve score seven hundred and forty-two f1 test set achieve state art result time submission august 28th two thousand and eighteen
word embeddings aim map sense word lower dimensional vector space order reason train embeddings domain specific data help express concepts relevant use case come cost accuracy data less effort minimise infuse syntactic knowledge embeddings propose graph base embed algorithm inspire node2vec experimental result show algorithm improve syntactic strength give robust performance meagre data
present seven semantic vector network hybrid resource encode relationships word form graph different traditional semantic network relations represent vectors continuous vector space propose simple pipeline learn relation vectors base word vector average combination ad hoc autoencoder show explicitly encode relational information dedicate vector space capture aspects word mean complementary capture word embeddings example examine cluster relation vectors observe relational similarities identify abstract level traditional word vector differences finally test effectiveness semantic vector network two task measure word similarity neural text categorization seven available bitbucketorg luisespinosa seven
recursive neural network recnn type model compose word phrase recursively syntactic tree structure prove superior ability obtain sentence representation variety nlp task however recnn bear thorny problem share compositional function node tree capture complex semantic compositionality expressive power model limit paper order address problem propose tag guide hyperrecnn treelstm tg hrecnn treelstm introduce hypernetwork recnns take input part speech pos tag word phrase generate semantic composition parameters dynamically experimental result five datasets two typical nlp task show propose model obtain significant improvement compare recnn treelstm consistently tg htreelstm outperform exist recnn base model achieve competitive state art four sentence classification benchmarks effectiveness model also demonstrate qualitative analysis
sentiment give emoji traditionally calculate average rat one zero one give various users give context emoji appear however use formula complicate statistical significance analysis particularly low sample size provide sentiment score use odds sentiment map four icon scale show odds ratio statistics lead simpler sentiment analysis finally provide list sentiment score often miss exact p value ci common emoji
paper present recipe build good arabic english neural machine translation compare neural systems traditional phrase base systems use various parallel corpora include un isi ummah also investigate importance special preprocessing arabic script present result base test set nist mt two thousand and five two thousand and twelve best neural system produce gain thirteen bleu point compare equivalent simple phrase base system nist mt12 test set unexpectedly find tune model train whole data use small high quality corpus like ummah give substantial improvement three bleu point also find train neural system small arabic english corpus competitive traditional phrase base system
prevalent model base artificial neural network ann sentence classification often classify sentence isolation without consider context sentence appear hamper traditional sentence classification approach problem sequential sentence classification structure prediction need better overall classification performance work present hierarchical sequential label network make use contextual information within surround sentence help classify current sentence model outperform state art result two three two benchmarking datasets sequential sentence classification medical scientific abstract
deploy speak language understand slu model new language language transfer desire avoid trouble acquire label new big slu corpus translate original slu corpus target language attractive strategy however slu corpora consist plenty semantic label slot general purpose translators handle well mention additional culture differences paper focus language transfer task give tiny domain parallel slu corpus domain parallel corpus use first adaptation general translator importantly show use reinforcement learn rl finetune adapt translator translate sentence proper slot tag receive higher reward evaluate approach chinese english language transfer slu systems experimental result show generate english slu corpus via adaptation reinforcement learn give us ninety-seven slot f1 score eighty-four accuracy domain classification demonstrate effectiveness propose language transfer method compare naive translation propose method improve domain classification accuracy relatively twenty-two slot fill f1 score relatively seventy-one
generate text abstract set document remain challenge task neural encoder decoder framework recently exploit summarize single document success part attribute availability large parallel data automatically acquire web contrast parallel data multi document summarization scarce costly obtain press need adapt encoder decoder model train single document summarization data work multiple document input paper present initial investigation novel adaptation method exploit maximal marginal relevance method select representative sentence multi document input leverage abstractive encoder decoder model fuse disparate sentence abstractive summary adaptation method robust require train data system compare favorably state art extractive abstractive approach judge automatic metrics human assessors
website privacy policies represent single important source information users gauge personal data collect use share company however privacy policies often vague people struggle understand content opaqueness pose significant challenge users policy regulators paper seek identify vague content privacy policies construct first corpus human annotate vague word sentence present empirical study automatic vagueness detection particular investigate context aware context agnostic model predict vague word explore auxiliary classifier generative adversarial network characterize sentence vagueness experimental result demonstrate effectiveness propose approach finally provide suggestions resolve vagueness improve usability privacy policies
paper describe sentencepiece language independent subword tokenizer detokenizer design neural base text process include neural machine translation provide open source c python implementations subword units exist subword segmentation tool assume input pre tokenized word sequence sentencepiece train subword model directly raw sentence allow us make purely end end language independent system perform validation experiment nmt english japanese machine translation find possible achieve comparable accuracy direct subword train raw sentence also compare performance subword train segmentation various configurations sentencepiece available apache two license https githubcom google sentencepiece
investigate neural model ability capture lexicosyntactic inferences inferences trigger interaction lexical syntactic information take task event factuality prediction case study build factuality judgment dataset english clause embed verbs various syntactic contexts use dataset make publicly available probe behavior current state art neural systems show systems make certain systematic errors clearly visible lens factuality prediction
neural machine translation nmt systems know degrade confront noisy data especially system train clean data paper show augment train data sentence contain artificially introduce grammatical errors make system robust errors combination automatic grammar error correction system recover fifteen bleu twenty-four bleu lose due grammatical errors also present set spanish translations jfleg grammar error correction corpus allow test nmt robustness real grammatical errors
cloze style read comprehension popular task measure progress natural language understand recent years paper design novel multi perspective framework see joint train heterogeneous experts aggregate context information different perspectives perspective model simple aggregation module output multiple aggregation modules feed one timestep pointer network get final answer time tackle problem insufficient label data propose efficient sample mechanism automatically generate train examples match distribution candidates label unlabeled data conduct experiment recently release cloze test dataset cloth xie et al two thousand and seventeen consist nearly 100k question design professional teachers result show method achieve new state art performance previous strong baselines
study learn semantic parser state art accuracy less supervise train data conduct study wikisql largest hand annotate semantic parse dataset date first demonstrate question generation effective method empower us learn state art neural network base semantic parser thirty percent supervise train data second show apply question generation full supervise train data improve state art model addition observe logarithmic relationship accuracy semantic parser amount train data
although embed vector representations word offer impressive performance many natural language process nlp applications information order input sequence lose extent context base sample use train performance improvement two new post process techniques call post process via variance normalization pvn post process via dynamic embed pde propose work pvn method normalize variance principal components word vectors pde method learn orthogonal latent variables order input sequence pvn pde methods integrate achieve better performance apply post process techniques two popular word embed methods ie word2vec glove yield post process representations extensive experiment conduct demonstrate effectiveness propose post process techniques
state art systems deep question answer proceed follow one initial document retrieval select relevant document two process neural network order extract final answer yet exact interplay components poorly understand especially concern number candidate document retrieve show choose static number document use prior research suffer noise information trade yield suboptimal result remedy propose adaptive document retrieval model learn optimal candidate number document retrieval conditional size corpus query report extensive experimental result show adaptive approach outperform state art methods multiple benchmark datasets well context corpora variable size
present detail theoretical computational analysis watset meta algorithm fuzzy graph cluster find widely applicable variety domains algorithm create intermediate representation input graph reflect ambiguity nod use hard cluster discover cluster disambiguate intermediate graph outline approach analyze computational complexity demonstrate watset show competitive result three applications unsupervised synset induction synonymy graph unsupervised semantic frame induction dependency triple unsupervised semantic class induction distributional thesaurus algorithm generic also apply network linguistic data
identification frequent sense polysemous word important semantic task introduce two concepts benefit mfs detection companion frequently co occur word frequent translation bitext present two novel methods incorporate new concepts show advance state art mfs detection
extract relations critical knowledge base completion construction distant supervise methods widely use extract relational facts automatically exist knowledge base however automatically construct datasets comprise amount low quality sentence contain noisy word neglect current distant supervise methods result unacceptable precisions mitigate problem propose novel word level distant supervise approach relation extraction first build sub tree parsestp remove noisy word irrelevant relations construct neural network inputting sub tree apply entity wise attention identify important semantic feature relational word instance make model robust noisy word initialize network priori knowledge learn relevant task entity classification transfer learn conduct extensive experiment use corpora new york timesnyt freebase experiment show approach effective improve area precision recallpr thirty-five thirty-nine state art work
state art model use deep neural network become good learn accurate map input output however still lack generalization capabilities condition differ ones encounter train even challenge specialize knowledge intensive domains train data limit address gap introduce mednli dataset annotate doctor perform natural language inference task nli ground medical history patients present strategies one leverage transfer learn use datasets open domain eg snli two incorporate domain knowledge external data lexical source eg medical terminologies result demonstrate performance gain use strategies
study problem generate keyphrases summarize key point give document sequence sequence seq2seq model achieve remarkable performance task meng et al two thousand and seventeen model train often rely large amount label data applicable resource rich domains paper propose semi supervise keyphrase generation methods leverage label data large scale unlabeled sample learn two strategies propose first unlabeled document first tag synthetic keyphrases obtain unsupervised keyphrase extraction methods selflearning algorithm combine label sample train furthermore investigate multi task learn framework jointly learn generate keyphrases well title article experimental result show semi supervise learn base methods outperform state art model train label data
stability word embed algorithms ie consistency word representations reveal train repeatedly data set recently raise concern compare word embed algorithms three corpora different size evaluate stability accuracy find strong evidence sample strategies use part train procedures particularly influential stability svdppmi type embeddings find seem explain diverge report stability lead us simple modification provide superior stability well accuracy par skip gram embeddings
paper investigate whether multilingual neural translation model learn stronger semantic abstractions sentence bilingual ones test hypotheses measure perplexity model apply paraphrase source language intuition encoder produce better representations decoder capable recognize synonymous sentence language even though model never train task setup add sixteen different auxiliary languages bidirectional bilingual baseline model english french test domain domain paraphrase english result show perplexity significantly reduce case indicate mean ground translation support study paraphrase generation also include end paper
increase usage internet data digitize include parliamentary debate unstructured format need convert structure format linguistic analysis much work do parliamentary data hansard american congressional floor debate data various aspects less pragmatics paper provide dataset synopsis indian parliamentary debate perform stance classification speeches ie identify speaker support bill issue also analyze intention speeches beyond mere sentence ie pragmatics parliament base thorough manual analysis debate develop annotation scheme four mutually exclusive categories analyze purpose speeches find issue blame appreciate call action annotate dataset provide four categories conduct preliminary experiment automatic detection categories automate classification approach give us promise result
paper present par4sem semantic write aid tool base adaptive paraphrase unlike many annotation tool primarily use collect train examples par4sem integrate real word application case write aid tool order collect train examples usage data par4sem tool support adaptive iterative interactive process underlie machine learn model update iteration use new train examples usage data motivate use ever learn tool nlp applications evaluate par4sem adopt text simplification task mere usage
adversarial train regularization method use improve robustness neural network methods add small perturbations train data show use task entity recognition relation extraction particular demonstrate apply general purpose baseline model jointly extract entities relations allow improve state art effectiveness several datasets different contexts ie news biomedical real estate data different languages english dutch
narrative story generation challenge problem demand generate sentence tight semantic connections well study exist generative model address problem propose skeleton base model promote coherence generate stories different traditional model generate complete sentence stroke propose model first generate critical phrase call skeleton expand skeleton complete fluent sentence skeleton manually define learn reinforcement learn method compare state art model skeleton base model generate significantly coherent text accord human evaluation automatic evaluation g score improve two hundred and one human evaluation code available https githubcom lancopku skeleton base generation model
compare word embed base point representation distribution base word embed show flexibility express uncertainty therefore embed richer semantic information represent word wasserstein distance provide natural notion dissimilarity probability measure close form solution measure distance two gaussian distributions therefore aim represent word highly efficient way propose operate gaussian word embed model loss function base wasserstein distance also external information conceptnet use semi supervise result gaussian word embed thirteen datasets word similarity task together one word entailment task six datasets downstream document classification task evaluate paper test hypothesis
news agencies publish news websites world moreover create novel corpuses necessary bring natural process new domains textual process online news challenge term strategy collect data complex structure news websites select design suitable algorithms process type data despite previous work focus create corpuses iran news persian paper introduce new corpus english news national news agency isna set new dataset english news iranian students news agency isna one famous news agencies iran statistically analyze data sentiments news also extract entities part speech tag
recent research suggest neural machine translation achieve parity professional human translation wmt chinese english news translation task empirically test claim alternative evaluation protocols contrast evaluation single sentence entire document pairwise rank experiment human raters assess adequacy fluency show stronger preference human machine translation evaluate document compare isolate sentence find emphasise need shift towards document level evaluation machine translation improve degree errors hard impossible spot sentence level become decisive discriminate quality different translation output
accurate language identification tool absolute necessity build complex nlp systems use code mix data lot work recently do still room improvement inspire recent advancements neural network architectures computer vision task implement multichannel neural network combine cnn lstm word level language identification code mix data combine bi lstm crf context capture module accuracies nine thousand, three hundred and twenty-eight nine thousand, three hundred and thirty-two achieve two test set
paper study automatic keyphrase generation although conventional approach task show promise result neglect correlation among keyphrases result duplication coverage issue solve problems propose new sequence sequence architecture keyphrase generation name corrrnn capture correlation among multiple keyphrases two ways first employ coverage vector indicate whether word source document summarize previous phrase improve coverage keyphrases second precede phrase take account eliminate duplicate phrase improve result coherence experiment result show model significantly outperform state art method benchmark datasets term accuracy diversity
paper present novel approach segmentation orthographic word form contemporary hebrew focus purely split without carry morphological analysis disambiguation cast analysis task character wise binary classification use adjacent character word base lexicon lookup feature approach achieve ninety-eight accuracy benchmark spmrl share task data hebrew ninety-seven accuracy new domain wikipedia dataset improvement four five previous state art performance
abusive language detection model tend problem bias toward identity word certain group people imbalanced train datasets example good woman consider sexist train exist dataset model bias obstacle model robust enough practical use work measure gender bias model train different abusive language datasets analyze effect different pre train word embeddings model architectures also experiment three bias mitigation methods one debiased word embeddings two gender swap data augmentation three fine tune larger corpus methods effectively reduce gender bias ninety ninety-eight extend correct model bias scenarios
important machine interpret human emotions properly better human machine communications emotion essential part human human communications one aspect emotion reflect language use represent emotions texts challenge natural language process nlp although continuous vector representations like word2vec become new norm nlp problems limitations take emotions consideration unintentionally contain bias toward certain identities like different genders thesis focus improve exist representations word sentence level explicitly take emotions inside text model bias account train process improve representations help build robust machine learn model affect relate text classification like sentiment emotion analysis abusive language detection first propose representations call emotional word vectors evec learn convolutional neural network model emotion label corpus construct use hashtags secondly extend learn sentence level representations huge corpus texts pseudo task recognize emojis result show representations train millions tweet weakly supervise label hashtags emojis solve sentiment emotion analysis task effectively lastly examples model bias representations exist approach explore specific problem automatic detection abusive language address issue gender bias various neural network model conduct experiment measure reduce bias representations order build robust classification model
paper study context response match pre train contextualized representations multi turn response selection retrieval base chatbots exist model cove elmo train limit context often single sentence paragraph may work well multi turn conversations due hierarchical nature informal language domain specific word address challenge propose pre train hierarchical contextualized representations include contextual word level sentence level representations learn dialogue generation model large scale conversations hierarchical encoder decoder architecture two level representations blend input output layer match model respectively experimental result two benchmark conversation datasets indicate propose hierarchical contextualized representations bring significantly consistently improvement exist match model response selection
solve mathematical word problems mwps automatically challenge primarily due semantic gap human readable word machine understandable logics despite long history date back the1960s mwps regain intensive attention past years advancement artificial intelligence ai solve mwps successfully consider milestone towards general ai many systems claim promise result self craft small scale datasets however apply large diverse datasets none propose methods literature achieve high precision reveal current mwp solvers still much room improvement motivate us present comprehensive survey deliver clear complete picture automatic math problem solvers survey emphasize algebraic word problems summarize extract feature propose techniques bridge semantic gap compare performance publicly accessible datasets also cover automatic solvers type math problems geometric problems require understand diagram finally identify several emerge research directions readers interest mwps
task sentiment modification require reverse sentiment input preserve sentiment independent content however align sentence content different sentiments usually unavailable due lack parallel data hard extract sentiment independent content reverse sentiment unsupervised way previous work usually reconcile sentiment transformation content preservation paper motivate fact non emotional context eg staff provide strong cue occurrence emotional word eg friendly propose novel method automatically extract appropriate sentiment information learn sentiment memories accord specific context experiment show method substantially improve content preservation degree achieve state art performance
retrofit techniques inject external resources word representations compensate weakness distribute representations semantic relational knowledge word implicitly retrofit word vectors expansional technique outperform retrofit word similarity task word vector generalization paper propose unsupervised extrofitting expansional retrofit extrofitting without external semantic lexicons also propose deep extrofitting depth stack extrofitting combinations extrofitting retrofit experiment glove show methods outperform previous methods word similarity task require synonyms external resource lastly show effect word vector enrichment text classification task downstream task
name entity recognition ner vital task speak language understand aim identify mention name entities text eg transcribe speech exist neural model ner rely mostly dedicate word level representations suffer two main shortcomings first vocabulary size large yield large memory requirements train time second model able learn morphological phonological representations remedy shortcomings adopt neural solution base bidirectional lstms conditional random field rely subword units namely character phonemes bytes word utterance model learn representation subword units conduct experiment real world large scale set use case voice control device cover four languages 55m utterances per language experiment show one increase train data performance model train solely subword units become closer model dedicate word level embeddings nine thousand, one hundred and thirty-five vs nine thousand, three hundred and ninety-two f1 english use much smaller vocabulary size three hundred and thirty-two vs 74k two subword units enhance model dedicate word level embeddings three combine different subword units improve performance
work examine methods data augmentation text base task neural machine translation nmt formulate design data augmentation policy desirable properties optimization problem derive generic analytic solution solution subsume exist augmentation scheme also lead extremely simple data augmentation strategy nmt randomly replace word source sentence target sentence random word correspond vocabularies name method switchout experiment three translation datasets different scale show switchout yield consistent improvements five bleu achieve better comparable performances strong alternatives word dropout sennrich et al 2016a code implement method include appendix
computational model sarcasm detection often rely content utterances isolation however speaker sarcastic intent always apparent without additional context focus social media discussions investigate three issue one model conversation context help sarcasm detection two identify part conversation context trigger sarcastic reply three give sarcastic post contain multiple sentence identify specific sentence sarcastic address first issue investigate several type long short term memory lstm network model conversation context current turn show lstm network sentence level attention context current turn well conditional lstm network rocktaschel et al two thousand and sixteen outperform lstm model read current turn conversation context consider prior turn succeed turn computational model test two type social media platforms twitter discussion forums discuss several differences datasets range size nature gold label annotations address last two issue present qualitative analysis attention weight produce lstm model attention discuss result compare human performance two task
temporal relations events time expressions document often model unstructured manner relations individual pair time expressions events consider isolation often result inconsistent incomplete annotation computational model propose novel annotation approach events time expressions document form dependency tree dependency relation correspond instance temporal anaphora antecedent parent anaphor child annotate corpus two hundred and thirty-five document use approach two genres news narratives forty-eight document doubly annotate report stable high inter annotator agreement doubly annotate subset validate approach perform quantitative comparison two genres entire corpus make corpus publicly available
paper explore new natural language process task review drive multi label music style classification task require system identify multiple style music base review websites biggest challenge lie complicate relations music style bring failure many multi label classification methods tackle problem propose novel deep learn approach automatically learn exploit style correlations propose method consist two part label graph base neural network soft train mechanism correlation base continuous label representation experimental result show approach achieve large improvements baselines propose dataset especially micro f1 improve five hundred and thirty-nine six hundred and forty-five one error reduce three hundred and five two hundred and twenty-six furthermore visualize analysis show approach perform well capture style correlations
weakly supervise semantic parsers train utterance denotation pair treat logical form latent task challenge due large search space spuriousness logical form paper introduce neural parser ranker system weakly supervise semantic parse parser generate candidate tree structure logical form utterances use clue denotations candidates rank base two criterion likelihood execute correct denotation agreement utterance semantics present schedule train procedure balance contribution two objectives furthermore propose use neurally encode lexicon inject prior domain knowledge model experiment three freebase datasets demonstrate effectiveness semantic parser achieve result within state art range
despite current read comprehension systems achieve significant advancements promise performances often obtain cost make ensemble numerous model besides exist approach also vulnerable adversarial attack paper tackle problems leverage knowledge distillation aim transfer knowledge ensemble model single model first demonstrate vanilla knowledge distillation apply answer span prediction effective read comprehension systems propose two novel approach penalize prediction confuse answer also guide train alignment information distil ensemble experiment show best student model slight drop four f1 squad test set compare ensemble teacher run 12x faster inference even outperform teacher adversarial squad datasets narrativeqa benchmark
paper present arap tweet large scale multi dialectal corpus tweet eleven regions sixteen countries arab world represent major arabic dialectal varieties build corpus collect data twitter provide team experience annotators annotation guidelines use annotate corpus age categories gender dialectal variety data collection effort base search distinctive keywords specific different arabic dialects also validate location use twitter api paper report corpus data collection annotation efforts also present issue encounter phase present result evaluation perform ensure consistency annotation provide corpus enrich limit set available language resources arabic invaluable enabler develop author profile tool nlp tool arabic
paper present annotation pipeline guidelines write part effort create large manually annotate arabic author profile dataset various social media source cover sixteen arabic countries eleven dialectal regions target size annotate arap tweet corpus twenty-four million word illustrate summarize general dialect specific guidelines dialectal regions select also present annotation framework logistics control annotation quality frequently compute inter annotator agreement annotation process finally describe issue encounter annotation phase especially relate peculiarities arabic dialectal varieties use social media
paper introduce evaluate intonation base feature score english speech nonnative english speakers indian context create automate speak english score engine learn manual evaluation speak english involve use exist automatic speech recognition asr engine convert speech text thereafter macro feature like accuracy fluency prosodic feature use build score model process introduce simintonation short similarity speak intonation pattern ideal ie train intonation pattern result show highly predictive feature control environment also categorize interword pause four distinct type granular evaluation pause impact speech evaluation moreover take step moderate test difficulty evaluation across parameters like difficult word count average sentence readability lexical density result show macro feature like accuracy intonation micro feature like pause topography strongly predictive score speak english within purview paper
entity link el essential task semantic text understand information extraction popular methods separately address mention detection md entity disambiguation ed stag el without leverage mutual dependency propose first neural end end el system jointly discover link entities text document main idea consider possible span potential mention learn contextual similarity score entity candidates useful md ed decisions key components context aware mention embeddings entity embeddings probabilistic mention entity map without demand engineer feature empirically show end end method significantly outperform popular systems gerbil platform enough train data available conversely test datasets follow different annotation conventions compare train set eg query tweet vs news document ed model couple traditional ner system offer best second best el accuracy
analyze performance different sentiment classification model syntactically complex input like b sentence first contribution analysis address reproducible research meaningfully compare different model accuracies must average far random seed traditionally report proper average place notice distillation model describe arxiv160306318v4 cslg incorporate explicit logic rule sentiment classification ineffective contrast use contextualized elmo embeddings arxiv180205365v2 cscl instead logic rule yield significantly better performance additionally provide analysis visualizations demonstrate elmo ability implicitly learn logic rule finally crowdsourced analysis reveal elmo outperform baseline model even sentence ambiguous sentiment label
sentiment index measure average emotional level corpus introduce four index use gauge average positiveness population period base post social network article first time present text rather word base sentiment index furthermore study present first large scale study sentiment index russian speak facebook result consistent prior experiment english language
language style transfer rephrase text specific stylistic attribute preserve original attribute independent content one main challenge learn style transfer system lack parallel data source sentence one style target sentence another style constraint paper adapt unsupervised machine translation methods task automatic style transfer first take advantage style preference information word embed similarity produce pseudo parallel data statistical machine translation smt framework iterative back translation approach employ jointly train two neural machine translation nmt base transfer systems control noise generate joint train style classifier introduce guarantee accuracy style transfer penalize bad candidates generate pseudo data experiment benchmark datasets show propose method outperform previous state art model term accuracy style transfer quality input output correspondence
abstractive text summarization aim shorten long text document human readable form contain important facts original document however level actual abstraction measure novel phrase appear source document remain low exist approach propose two techniques improve level abstraction generate summaries first decompose decoder contextual network retrieve relevant part source document pretrained language model incorporate prior knowledge language generation second propose novelty metric optimize directly policy learn encourage generation novel phrase model achieve result comparable state art model determine rouge score human evaluations achieve significantly higher level abstraction measure n gram overlap source document
topic aspect base sentiment analysis absa explore variety industries still remain much unexplored finance recent release data open challenge fiqa companion proceed www eighteen provide valuable finance specific annotations fiqa contain high quality label still lack data quantity apply traditional absa deep learn architecture paper employ high level semantic representations methods inductive transfer learn nlp experiment extensions recently develop domain adaptation methods target task fine tune significantly improve performance small dataset result show eighty-seven improvement f1 score classification eleven improvement mse regression current state art result
theoretical note compare different type computational model word similarity association ability predict set nine hundred rat data use regression predictive model tool neural net decision tree performance total twenty-eight model use different combinations surface semantic word feature evaluate result present evidence hypothesis word similarity rat base semantic relatedness limit cross validate performance model ask development psychological process model word similarity rat task
predict structure discourse challenge relations discourse segment often implicit thus hard distinguish computationally extend previous work classify implicit discourse relations introduce novel set feature level semantic roles result demonstrate feature helpful yield result competitive feature rich approach pdtb main contribution analysis improvements trace back role base feature provide insights role semantics helpful
background unstructured textual data increase rapidly latent dirichlet allocation lda topic model popular data analysis methods past work suggest instability lda topics may lead systematic errors aim propose method rely replicate lda run cluster provide stability metric topics method generate k lda topics replicate process n time result nk topics use k medioids cluster nk topics k cluster k cluster represent original lda topics present like normal lda topics show ten probable word cluster try multiple stability metrics recommend rank bias overlap show stability topics inside cluster result provide initial validation method use two hundred and seventy thousand mozilla firefox commit message k20 n20 show topic stability metrics relate content topics conclusions advance text mine enable us analyze large mass text software engineer non deterministic algorithms lda may lead unreplicable conclusions approach make lda stability transparent also complementary rather alternative many prior work focus lda parameter tune
introduce novel multimodal machine translation model utilize parallel visual textual information model jointly optimize learn share visual language embed translator model leverage visual attention ground mechanism link visual semantics correspond textual semantics approach achieve competitive state art result multi30k ambiguous coco datasets also collect new multilingual multimodal product description dataset simulate real world international online shop scenario dataset visual attention ground model outperform methods large margin
paper introduce madari joint morphological annotation spell correction system texts standard dialectal arabic madari framework provide intuitive interfaces annotate text manage annotation process large number sizable document morphological annotation include indicate word context baseword clitics part speech lemma gloss dialect identification madari suite utilities help annotator productivity example annotators provide pre compute analyse assist task reduce amount work need complete madari also allow annotators query morphological analyzer list possible analyse multiple dialects look previously submit analyse madari management interface enable lead annotator easily manage organize whole annotation process remotely concurrently describe motivation design implementation interface present detail user study work system
recently string kernels obtain state art result various text classification task arabic dialect identification native language identification paper apply two simple yet effective transductive learn approach improve result string kernels first approach base interpret pairwise string kernel similarities sample train set sample test set feature second approach simple self train method base two learn iterations first iteration classifier train train set test test set usual second iteration number test sample classifier associate higher confidence score add train set another round train however grind truth label add test sample necessary instead use label predict classifier first train iteration adapt string kernels test set report significantly better accuracy rat english polarity classification arabic dialect identification
propose new method detect users express intent leave service also know churn previous work focus solely social media show intent detect chatbot conversations company increasingly rely chatbots need overview potentially churny users end crowdsource publish dataset churn intent expressions chatbot interactions german english show classifiers train social media data detect intent context chatbots introduce classification architecture outperform exist work churn intent detection social media moreover show use bilingual word embeddings system train combine english german data outperform monolingual approach exist dataset english crowdsource publish novel dataset german tweet thus underline universal aspect problem examples churn intent english help us identify churn german tweet chatbot conversations
paraphrase reword semantic mean useful improve generalization translation however prior work explore paraphrase word phrase level sentence corpus level unlike previous work explore paraphrase word phrase level use different translations whole train data consistent structure paraphrase corpus level train parallel paraphrase multiple languages various source treat paraphrase foreign languages tag source sentence paraphrase label train parallel paraphrase style multilingual neural machine translation nmt multi paraphrase nmt train two languages outperform multilingual baselines add paraphrase improve rare word translation increase entropy diversity lexical choice add source paraphrase boost performance better add target ones combine source target paraphrase lift performance combine paraphrase multilingual data help mix performance achieve bleu score five hundred and seventy-two french english translation use twenty-four corpus level paraphrase bible outperform multilingual baselines three hundred and forty-seven single source single target nmt baseline
compare use lstm base cnn base character level word embeddings bilstm crf model approach chemical disease name entity recognition ner task empirical result biocreative v cdr corpus show use either type character level word embeddings conjunction bilstm crf model lead comparable state art performance however model use cnn base character level word embeddings computational performance advantage increase train time word base model twenty-five lstm base character level word embeddings double require train time
neural machine translation nmt decoder capture feature entire prediction history neural connections representations mean partial hypotheses different prefix regard differently matter similar however might inefficient since partial hypotheses contain local differences influence future predictions work introduce recombination nmt decode base concept equivalence partial hypotheses heuristically use simple n gram suffix base equivalence function adapt beam search decode experiment large scale chinese english english germen translation task show propose method obtain similar translation quality smaller beam size make nmt decode efficient
deep learn emerge versatile tool wide range nlp task due superior capacity representation learn applicability limit reliance annotate examples difficult produce scale indirect supervision emerge promise direction address bottleneck either introduce label function automatically generate noisy examples unlabeled text impose constraints interdependent label decisions plethora methods propose respective strengths limitations probabilistic logic offer unify language represent indirect supervision end end model probabilistic logic often infeasible due intractable inference learn paper propose deep probabilistic logic dpl general framework indirect supervision compose probabilistic logic deep learn dpl model label decisions latent variables represent prior knowledge relations use weight first order logical formulas alternate learn deep neural network end task refine uncertain formula weight indirect supervision use variational framework subsume prior indirect supervision methods special case enable novel combination via infusion rich domain linguistic knowledge experiment biomedical machine read demonstrate promise approach
detect events classify predefined type important step knowledge extraction natural language texts neural network model generally lead state art differences performance different architectures rigorously study paper present novel gru base model combine syntactic information along temporal structure attention mechanism show competitive neural network architectures empirical evaluations different random initializations train validation test split ace2005 dataset
establish method word sense induction wsi use language model predict probable substitute target word induce sense cluster result substitute vectors replace ngram base language model lm recurrent one beyond accurate use recurrent lm allow us effectively query creative way use call dynamic symmetric pattern combination rnn lm dynamic symmetric pattern result strong substitute vectors wsi allow surpass current state art semeval two thousand and thirteen wsi share task large margin
propose novel model multi label text classification base sequence sequence learn model generate higher level semantic unit representations multi level dilate convolution well correspond hybrid attention mechanism extract information word level level semantic unit design dilate convolution effectively reduce dimension support exponential expansion receptive field without loss local information attention attention mechanism able capture summary relevant information source context result experiment show propose model significant advantage baseline model dataset rcv1 v2 ren cecps analysis demonstrate model competitive deterministic hierarchical model robust classify low frequency label
paper address relatively new task prediction asr performance unseen broadcast program previous paper present asr performance prediction system use cnns encode text asr transcript speech order predict word error rate work dedicate analysis speech signal embeddings text embeddings learn cnn train prediction model try better understand information capture deep model relation different condition factor show hide layer convey clear signal speech style accent broadcast type try leverage three type information train time multi task learn experiment show allow train slightly efficient asr performance prediction systems addition simultaneously tag analyze utterances accord speech style accent broadcast program origin
keyphrase generation kg aim generate set keyphrases give document fundamental task natural language process nlp previous methods solve problem extractive manner recently several attempt make generative set use deep neural network however state art generative methods simply treat document title document main body equally ignore lead role title overall document solve problem introduce new model call title guide network tg net automatic keyphrase generation task base encoder decoder architecture two new feature title additionally employ query like input ii title guide encoder gather relevant information title word document experiment range kg datasets demonstrate model outperform state art model large margin especially document either low high title length ratios
exist approach neural machine translation typically autoregressive model model attain state art translation quality suffer low parallelizability thus slow decode long sequence paper propose novel model fast sequence generation semi autoregressive transformer sit sit keep autoregressive property global relieve local thus able produce multiple successive word parallel time step experiment conduct english german chinese english translation task show sit achieve good balance translation quality decode speed wmt fourteen english german translation sit achieve 558times speedup maintain eighty-eight translation quality significantly better previous non autoregressive methods produce two word time step sit almost lossless one degeneration bleu score
supervise event extraction systems limit accuracy due lack available train data present method self train event extraction systems bootstrapping additional train data do take advantage occurrence multiple mention event instance across newswire article multiple source system make highconfidence extraction mention cluster acquire diverse train examples add mention well experiment show significant performance improvements multiple event extractors ace two thousand and five tac kbp two thousand and fifteen datasets
semantics sentence representable semantic parser output schema parse inevitably fail detection instance commonly treat domain classification problem however also subtle scenario test data draw domain addition formalize problem domain adjacency present comparison various baselines could use solve also propose new simple sentence representation emphasize word unexpected approach improve performance downstream semantic parser run domain domain adjacent instance
semantic graph wordnet resources curate natural language two distinguishable layer local level individual relations synsets semantic build block hypernymy meronymy enhance understand word use express mean globally analysis graph theoretic properties entire net shed light structure human language whole paper combine global local properties semantic graph framework max margin markov graph model m3gm novel extension exponential random graph model ergm scale large multi relational graph demonstrate global model improve performance local task predict semantic relations synsets yield new state art result wn18rr dataset challenge version wordnet link prediction easy reciprocal case remove addition m3gm model identify multirelational motifs characteristic well form lexical semantic ontologies
clinical name entity recognition cner aim identify classify clinical term diseases symptoms treatments exams body part electronic health record fundamental crucial task clinical translation research recent years deep learn methods achieve significant success cner task however methods depend greatly recurrent neural network rnns maintain vector hide activations propagate time thus cause much time train model paper propose residual dilate convolutional neural network conditional random field rd cnn crf solve specifically chinese character dictionary feature first project dense vector representations feed residual dilate convolutional neural network capture contextual feature finally conditional random field employ capture dependencies neighbor tag computational result ccks two thousand and seventeen task two benchmark dataset show propose rd cnn crf method compete favorably state art rnn base methods term computational performance train time
paper describe system design wassa two thousand and eighteen implicit emotion share task iest obtain 2textnd place twenty-six team test macro f1 score seven hundred and ten system compose single pre train elmo layer encode word bidirectional long short memory network bilstm enrich word representations context max pool operation create sentence representations say word vectors dense layer project sentence representations label space official submission obtain ensembling six model initialize different random seed code replicate paper available https githubcom jabalazs implicitemotion
encode decoder framework show recent success image caption visual attention good detailedness semantic attention good comprehensiveness separately propose grind caption image paper propose stepwise image topic merge network simnet make use two kinds attention time time step generate caption decoder adaptively merge attentive information extract topics image accord generate context visual information semantic information effectively combine propose approach evaluate two benchmark datasets reach state art performancesthe code available https githubcom lancopku simnet
propose machine read comprehension model base compare aggregate framework two stag attention achieve state art result movieqa question answer dataset investigate limitations model well behavioral difference convolutional recurrent neural network generate adversarial examples confuse model compare human performance furthermore assess generalizability model analyze differences human inference
introduce extreme summarization new single document summarization task favor extractive strategies call abstractive model approach idea create short one sentence news summary answer question article collect real world large scale dataset task harvest online article british broadcast corporation bbc propose novel abstractive model condition article topics base entirely convolutional neural network demonstrate experimentally architecture capture long range dependencies document recognize pertinent content outperform oracle extractive system state art abstractive approach evaluate automatically humans
cross lingual word embeddings become increasingly important multilingual nlp recently show embeddings effectively learn align two disjoint monolingual vector space linear transformations use small bilingual dictionary supervision work propose apply additional transformation initial alignment step move cross lingual synonyms towards middle point apply transformation aim obtain better cross lingual integration vector space addition perhaps surprisingly monolingual space also improve transformation contrast original alignment typically learn structure monolingual space preserve experiment confirm result cross lingual embeddings outperform state art model monolingual cross lingual evaluation task
generate semantically coherent responses still major challenge dialogue generation different conventional text generation task map input responses conversations complicate highly demand understand utterance level semantic dependency relation whole mean input output address problem propose auto encoder match aem model learn dependency model contain two auto encoders one map module auto encoders learn semantic representations input responses map module learn connect utterance level representations experimental result automatic human evaluations demonstrate model capable generate responses high coherence fluency compare baseline model code available https githubcom lancopku amm
sentence boundary detection sbd major research topic since automatic speech recognition transcripts use natural language process task like part speech tag question answer automatic summarization evaluation standard evaluation metrics like precision recall f score classification error important evaluate automatic system unique reference enough conclude well sbd system perform give final application transcript paper propose window base sentence boundary evaluation wisebe semi supervise metric evaluate sentence boundary detection systems base multi reference disagreement evaluate compare performance different sbd systems set youtube transcripts use wisebe standard metrics double evaluation give understand wisebe reliable metric sbd task
order extract best possible performance asynchronous stochastic gradient descent one must increase mini batch size scale learn rate accordingly order achieve speedup introduce technique delay gradient update effectively increase mini batch size unfortunately increase mini batch size worsen stale gradient problem asynchronous stochastic gradient descent sgd make model convergence poor introduce local optimizers mitigate stale gradient problem together fine tune momentum able train shallow machine translation system twenty-seven faster optimize baseline negligible penalty bleu
paper present rusentrel corpus include analytical texts sphere international relations document annotate sentiments author mention name entities sentiments relations mention entities current experiment consider problem extract sentiment relations entities whole document three class machine learn task experiment conventional machine learn methods naive bay svm random forest
multilingual word embeddings mwes represent word multiple languages single distributional vector space unsupervised mwe umwe methods acquire multilingual embeddings without cross lingual supervision significant advantage traditional supervise approach open many new possibilities low resource languages prior art learn umwes however merely rely number independently train unsupervised bilingual word embeddings ubwes obtain multilingual embeddings methods fail leverage interdependencies exist among many languages address shortcoming propose fully unsupervised framework learn mwes directly exploit relations language pair model substantially outperform previous approach experiment multilingual word translation cross lingual word similarity addition model even beat supervise approach train cross lingual resources
recently non recurrent architectures convolutional self attentional outperform rnns neural machine translation cnns self attentional network connect distant word via shorter network paths rnns speculate improve ability model long range dependencies however theoretical argument test empirically alternative explanations strong performance explore depth hypothesize strong performance cnns self attentional network could also due ability extract semantic feature source text evaluate rnns cnns self attention network two task subject verb agreement capture long range dependencies require word sense disambiguation semantic feature extraction require experimental result show one self attentional network cnns outperform rnns model subject verb agreement long distance two self attentional network perform distinctly better rnns cnns word sense disambiguation
contextual word representations derive pre train bidirectional language model bilms recently show provide significant improvements state art wide range nlp task however many question remain model effective paper present detail empirical study choice neural architecture eg lstm cnn self attention influence end task accuracy qualitative properties representations learn show tradeoff speed accuracy architectures learn high quality contextual representations outperform word embeddings four challenge nlp task additionally architectures learn representations vary network depth exclusively morphological base word embed layer local syntax base lower contextual layer longer range semantics coreference upper layer together result suggest unsupervised bilms independent architecture learn much structure language previously appreciate
propose large margin criterion train neural language model conventionally neural language model train minimize perplexity ppl grammatical sentence however demonstrate ppl may best metric optimize task propose large margin formulation propose method aim enlarge margin good bad sentence task specific sense train end end widely apply task involve score generate text compare minimum ppl train method gain eleven wer reduction speech recognition ten bleu increase machine translation
neural machine translation achieve state art performance several language pair use combination parallel synthetic data synthetic data often generate back translate sentence randomly sample monolingual data use reverse translation model back translation show effective many case entirely clear work explore different aspects back translation show word high prediction loss train benefit addition synthetic data introduce several variations sample strategies target difficult predict word use prediction losses frequencies word addition also target contexts difficult word sample sentence similar context experimental result wmt news translation task show method improve translation quality seventeen twelve bleu point back translation use random sample german english english german respectively
thesis explore use deep neural network generation natural language specifically implement two sequence sequence neural variational model variational autoencoders vae variational encoder decoders ved vaes text generation difficult train due issue associate kullback leibler kl divergence term loss function vanish zero successfully train vaes implement optimization heuristics kl weight anneal word dropout also demonstrate effectiveness continuous latent space experiment random sample linear interpolation sample neighborhood input argue vaes design appropriately may lead bypass connections result latent space ignore train show experimentally example decoder hide state initialization bypass connections degrade vae deterministic model thereby reduce diversity generate sentence discover traditional attention mechanism use sequence sequence ved model serve bypass connection thereby deteriorate model latent space order circumvent issue propose variational attention mechanism attention context vector model random variable sample distribution show empirically use automatic evaluation metrics namely entropy distinct measure variational attention model generate diverse output sentence deterministic attention model qualitative analysis human evaluation study prove model simultaneously produce sentence high quality equally fluent ones generate deterministic attention counterpart
lstms powerful tool model contextual information evidence success task language model however model contexts high dimensional space lead poor generalizability introduce pyramidal recurrent unit pru enable learn representations high dimensional space generalization power fewer parameters prus replace linear transformation lstms sophisticate interactions include pyramidal group linear transformations architecture give strong result word level language model reduce number parameters significantly particular pru improve perplexity recent state art language model merity et al two thousand and eighteen thirteen point learn fifteen twenty fewer parameters similar number model parameters pru outperform previous rnn model exploit different gate mechanisms transformations provide detail examination pru behavior language model task code open source available https sacmehtagithubio pru
present dataset evaluate grammaticality predictions language model automatically construct large number minimally different pair english sentence consist grammatical ungrammatical sentence sentence pair represent different variations structure sensitive phenomena subject verb agreement reflexive anaphora negative polarity items expect language model assign higher probability grammatical sentence ungrammatical one experiment use data set lstm language model perform poorly many constructions multi task train syntactic objective ccg supertagging improve lstm accuracy large gap remain performance accuracy human participants recruit online suggest considerable room improvement lstms capture syntax language model
knowledge graph kgs key components various natural language process applications expand kgs coverage previous study knowledge graph completion usually require large number train instance relation however observe long tail relations actually common kgs newly add relations often many know triple train work aim predict new facts challenge set one train instance available propose one shoot relational learn framework utilize knowledge extract embed model learn match metric consider learn embeddings one hop graph structure empirically model yield considerable performance improvements exist embed model also eliminate need train embed model deal newly add relations
paper present method adversarial decomposition text representation method use decompose representation input sentence several independent vectors responsible specific aspect input sentence evaluate propose method two case study conversion different social register diachronic language change show propose method capable fine grain control change aspects input sentence also learn continuous rather categorical representation style sentence linguistically realistic model use adversarial motivational train include special motivational loss act opposite discriminator encourage better decomposition furthermore evaluate obtain mean embeddings downstream task paraphrase detection show significantly outperform embeddings regular autoencoder
previous work suggest parameter share transition base neural dependency parsers relate languages lead better performance consensus parameters share present evaluation twenty-seven different parameter share strategies across ten languages represent five pair relate languages pair different language family find share transition classifier parameters always help whereas usefulness share word character lstm parameters vary base result propose architecture transition classifier share share word character parameters control parameter tune validation data model linguistically motivate obtain significant improvements monolingually train baseline also find share transition classifier parameters help train parser unrelated language pair find case unrelated languages share many parameters help
provide comprehensive analysis interactions pre train word embeddings character model pos tag transition base dependency parser previous study show pos information less important presence character model show fact complex interactions three techniques isolation produce large improvements baseline system use randomly initialise word embeddings combine quickly lead diminish return categorise word frequency pos tag language order systematically investigate techniques affect parse quality many word categories apply two three techniques almost good full combine system character model tend important low frequency open class word especially morphologically rich languages pos tag help disambiguate high frequency function word also show large character embed size help even languages small character set especially morphologically rich languages
conventional wisdom hand craft feature redundant deep learn model already learn adequate representations text automatically corpora work test claim propose new method exploit handcraft feature part novel hybrid learn approach incorporate feature auto encoder loss component evaluate task name entity recognition ner show include manual feature part speech word shape gazetteers improve performance neural crf model obtain f1 nine thousand, one hundred and eighty-nine conll two thousand and three english share task significantly outperform collection highly competitive baseline model also present ablation study show importance auto encode use feature either input output alone moreover show include autoencoder components reduce train requirements sixty retain predictive accuracy
paper present model disfluency detection spontaneous speech transcripts call lstm noisy channel model model use noisy channel model ncm generate n best candidate disfluency analyse long short term memory lstm language model score underlie fluent sentence analysis lstm language model score along feature use maxent reranker identify plausible analysis show use lstm language model reranking process noisy channel disfluency model improve state art disfluency detection
recent years natural language process community move away task specific feature engineer ie researchers discover ad hoc feature representations various task favor general purpose methods learn input representation however state art approach disfluency detection spontaneous speech transcripts currently still depend array hand craft feature representations derive output pre exist systems language model dependency parsers alternative paper propose simple yet effective model automatic disfluency detection call auto correlational neural network acnn model use convolutional neural network cnn augment new auto correlation operator lowest layer capture kinds rough copy dependencies characteristic repair disfluencies speech experiment acnn model outperform baseline cnn disfluency detection task five increase f score close previous best result task
cross sentence n ary relation extraction detect relations among n entities across multiple sentence typical methods formulate input textitdocument graph integrate various intra sentential inter sentential dependencies current state art method split input graph two dags adopt dag structure lstm though able model rich linguistic knowledge leverage graph edge important information lose split procedure propose graph state lstm model use parallel state model word recurrently enrich state value via message pass compare dag lstms graph lstm keep original graph structure speed computation allow parallelization standard benchmark model show best result literature
spread social network unfortunate use hate speech automatic detection latter become press problem paper reproduce seven state art hate speech detection model prior work show perform well test type data train base result argue successful hate speech detection model architecture less important type data label criteria show propose detection techniques brittle adversaries automatically insert typos change word boundaries add innocuous word original hate speech combination methods also effective google perspective cut edge solution industry experiment demonstrate adversarial train completely mitigate attack use character level feature make model systematically attack resistant use word level feature
design word embeddings unable model dynamic nature word semantics ie property word correspond potentially different mean address limitation dozens specialize mean representation techniques sense contextualized embeddings propose however despite popularity research topic evaluation benchmarks exist specifically focus dynamic semantics word paper show exist model surpass performance ceiling standard evaluation dataset purpose ie stanford contextual word similarity highlight shortcomings address lack suitable benchmark put forward large scale word context dataset call wic base annotations curated experts generic evaluation context sensitive representations wic release https pilehvargithubio wic
web provide rich open domain environment textual structural spatial properties propose new task ground language environment give natural language command eg click second article choose correct element web page eg hyperlink text box collect dataset fifty thousand command capture various phenomena functional reference eg find make site relational reason eg article john visual reason eg top article also implement analyze three baseline model capture different phenomena present dataset
discourse segmentation segment texts elementary discourse units fundamental step discourse analysis previous discourse segmenters rely complicate hand craft feature practical actual use paper propose end end neural segmenter base bilstm crf framework improve accuracy address problem data insufficiency transfer word representation model train large corpus also propose restrict self attention mechanism order capture useful information within neighborhood experiment rst dt corpus show model significantly faster previous methods achieve new state art performance
recent work abstractive summarization make progress neural encoder decoder architectures however model often challenge due lack explicit semantic model source document summary paper extend previous work abstractive summarization use abstract mean representation amr neural language generation stage guide use source document demonstrate guidance improve summarization result seventy-four one hundred and five point rouge two use gold standard amr parse parse obtain shelf parser respectively also find summarization performance use latter two rouge two point higher well establish neural encoder decoder approach train larger dataset code available urlhttps githubcom sheffieldnlp amr2text summ
investigate encoder decoder model train synthetic dataset task orient dialogues process disfluencies hesitations self corrections find contrary earlier result disfluencies little impact task success seq seq model attention use visualisation diagnostic classifiers analyse representations incrementally build model discover model develop little awareness structure disfluencies however add disfluencies data appear help model create clearer representations overall evidence attention pattern different model exhibit
parse morphologically rich languages neural model beneficial model input character level claim character level model learn morphology test claim compare character level model oracle access explicit morphological analysis twelve languages vary morphological typologies result highlight many strengths character level model also show poor disambiguate word particularly face case syncretism demonstrate explicitly model morphological case improve best model show character level model benefit target form explicit morphological model
recent advance sequence sequence learn reveal purely data drive approach response generation task despite diverse applications exist neural model prone produce short generic reply make infeasible tackle open domain challenge research analyze critical issue light model optimization goal specific characteristics human human dialog corpus decompose black box part detail analysis probability limit conduct reveal reason behind universal reply base analyse propose max margin rank regularization term avoid model lean reply finally empirical experiment case study benchmarks several metrics validate approach
work propose new model aspect base sentiment analysis contrast previous approach jointly model detection aspects classification polarity end end trainable neural network conduct experiment different neural architectures word representations recent germeval two thousand and seventeen dataset able show considerable performance gain use joint model approach settings compare pipeline approach combination convolutional neural network fasttext embeddings outperform best submission share task two thousand and seventeen establish new state art
rare word representation recently enjoy surge interest owe crucial role effective handle infrequent word play accurate semantic understand however paucity reliable benchmarks evaluation comparison techniques show paper exist benchmark stanford rare word dataset suffer low confidence annotations limit vocabulary hence constitute solid comparison framework order fill evaluation gap propose cambridge rare word dataset card six hundred and sixty expert annotate word similarity dataset provide highly reliable yet challenge benchmark rare word representation techniques set experiment show even best mainstream word embeddings millions word vocabularies unable achieve performances higher forty-three pearson correlation dataset compare human level upperbound ninety release dataset annotation materials https pilehvargithubio card six hundred and sixty
introduce class convolutional neural network cnns utilize recurrent neural network rnns convolution filter convolution filter typically implement linear affine transformation follow non linear function fail account language compositionality result limit use high order filter often warrant natural language process task work model convolution filter rnns naturally capture compositionality long term dependencies language show simple cnn architectures equip recurrent neural filter rnfs achieve result par best publish ones stanford sentiment treebank two answer sentence selection datasets
propose new dataset evaluate question answer model respect capacity reason beliefs task inspire theory mind experiment examine whether children able reason beliefs others particular beliefs differ reality evaluate number recent neural model memory augmentation find fail task require keep track inconsistent state world moreover model accuracy decrease notably random sentence introduce task test
paper present experiment apply tupa conll two thousand and eighteen ud share task tupa general neural transition base dag parser use present first experiment recover enhance dependencies part general parse task tupa design parse ucca cross linguistic semantic annotation scheme exhibit reentrancy discontinuity non terminal nod convert ud tree graph ucca like dag format train tupa almost without modification ud parse task generic nature approach lend naturally multitask learn code available https githubcom conll ud two thousand and eighteen huji
despite tremendous empirical success neural model natural language process many lack strong intuitions accompany classical machine learn approach recently connections show convolutional neural network cnns weight finite state automata wfsas lead new interpretations insights work show recurrent neural network also share connection wfsas characterize connection formally define rational recurrences recurrent hide state update function write forward calculation finite set wfsas show several recent neural model use rational recurrences analysis provide fresh view model facilitate devise new neural architectures draw inspiration wfsas present one model perform better two recent baselines language model text classification result demonstrate transfer intuitions classical model like wfsas effective approach design understand neural model
attention base model successful train large amount data paper demonstrate even low resource scenario attention learn effectively end start discrete human annotate rationales map continuous attention central hypothesis map general across domains thus transfer resource rich domains low resource ones model jointly learn domain invariant representation induce desire map rationales attention empirical result validate hypothesis show approach deliver significant gain state art baselines yield fifteen average error reduction benchmark datasets
recent advance neural machine translation nmt show add syntactic information nmt systems improve quality translations exist work utilize specific type linguistically inspire tree structure like constituency dependency parse tree often do via standard rnn decoder operate linearize target tree structure however open question specific linguistic formalism best structural representation nmt paper one propose nmt model naturally generate topology arbitrary tree structure target side two experiment various target tree structure experiment show surprise result model deliver best improvements balance binary tree construct without linguistic knowledge model outperform standard seq2seq model twenty-one bleu point methods incorporate target side syntax seven bleu
effective method improve neural machine translation monolingual data augment parallel train corpus back translations target language sentence work broaden understand back translation investigate number methods generate synthetic source sentence find resource poor settings back translations obtain via sample noise beam output effective analysis show sample noisy synthetic data give much stronger train signal data generate beam greedy search also compare synthetic data compare genuine bitext study various domain effect finally scale hundreds millions monolingual sentence achieve new state art thirty-five bleu wmt fourteen english german test set
amidst grow concern media manipulation nlp attention focus overt strategies like censorship fake news draw two concepts political science literature explore subtler strategies government media manipulation agenda set select topics cover frame decide topics cover analyze thirteen years 100k article russian newspaper izvestia identify strategy distraction article mention yous frequently month directly follow economic downturn russia introduce embed base methods cross lingually project english frame russian discover article emphasize yous moral fail threats yous work offer new ways identify subtle media manipulation strategies intersection agenda set frame
current lead paradigm temporal information extraction text consist three phase one recognition events temporal expressions two recognition temporal relations among three time line construction temporal relations contrast first two phase last phase time line construction receive little attention focus work paper propose new method construct linear time line set extract temporal relations importantly propose novel paradigm directly predict start end point events text constitute time line without go intermediate step prediction temporal relations earlier work within paradigm propose two model predict linear complexity new train loss use timeml style annotations yield promise result
article deal adversarial attack towards deep learn systems natural language process nlp context privacy protection study specific type attack attacker eavesdrop hide representations neural text classifier try recover information input text scenario may arise situations computation neural network share across multiple devices eg hide representation compute user device send cloud base model measure privacy hide representation ability attacker predict accurately specific private information characterize tradeoff privacy utility neural representations finally propose several defense methods base modify train objectives show improve privacy neural representations
paper study semantic parse interlanguage l2 take semantic role label srl case task learner chinese case language first manually annotate semantic roles set learner texts derive gold standard automatic srl base new data evaluate three shelf srl systems ie pcfgla parser base neural parser base neural syntax agnostic systems gauge successful srl learner chinese find two non obvious facts one l1 sentence train systems perform rather badly l2 data two performance drop l1 data l2 data two parser base systems much smaller indicate importance syntactic parse srl interlanguages finally paper introduce new agreement base model explore semantic coherency information large scale l2 l1 parallel data show information effective enhance srl learner texts model achieve f score seven thousand, two hundred and six two hundred and two point improvement best baseline
understand search query hard problem involve deal word salad text ubiquitously issue users however query resemble well form question natural language process pipeline able perform accurate interpretation thus reduce downstream compound errors hence identify whether query well form enhance query understand introduce new task identify well form natural language question construct release dataset twenty-five thousand, one hundred publicly available question classify well form non wellformed categories report accuracy seven hundred and seven test set also show classifier use improve performance neural sequence sequence model generate question read comprehension
release corpus forty-three million atomic edit across eight languages edit mine wikipedia edit history consist instance human editor insert single contiguous phrase delete single contiguous phrase exist sentence use collect data show language generate edit differ language observe standard corpora model train edit encode different aspects semantics discourse model train raw unstructured text release full corpus resource aid ongoing research semantics discourse representation learn
introduce graphene open ie system whose goal generate accurate meaningful complete proposition may facilitate variety downstream semantic applications purpose transform syntactically complex input sentence clean compact structure form core facts accompany contexts identify rhetorical relations hold order maintain semantic relationship way preserve context relational tuples extract source sentence generate novel lightweight semantic representation open ie enhance expressiveness extract proposition
university cambridge submission wmt18 news translation task focus combination diverse model translation compare recurrent convolutional self attention base neural model german english english german chinese english final system combine neural model together phrase base smt system mbr base scheme report small consistent gain top strong transformer ensembles
split rephrase task break sentence shorter ones together convey mean extract rich new dataset task mine wikipedia edit history wikisplit contain one million naturally occur sentence rewrite provide sixty time distinct split examples ninety time larger vocabulary websplit corpus introduce narayan et al two thousand and seventeen benchmark task incorporate wikisplit train data produce model qualitatively better predictions score thirty-two bleu point prior best result websplit benchmark
predictive model social media language show promise capture community outcomes approach thus far largely neglect socio demographic context eg age education rat race community language originate example may inaccurate assume people mobile alabama population relatively older use word way san francisco median age younger higher rate college education paper present residualized factor adaptation novel approach community prediction task effectively integrate community attribute well b adapt linguistic feature community attribute factor use eleven demographic socioeconomic attribute evaluate approach five different community level predictive task span health heart disease mortality percent fair poor health psychology life satisfaction economics percent house price increase foreclosure rate evaluation show residualized factor adaptation significantly improve four five community level outcome predictions prior state art incorporate socio demographic contexts
open domain question answer remain challenge task require model capable understand question answer collect useful information reason evidence previous work typically formulate task read comprehension entailment problem give evidence retrieve search engines however exist techniques struggle retrieve indirectly relate evidence directly relate evidence provide especially complex question hard parse precisely question ask paper propose retriever reader model learn attend essential term question answer process build one essential term selector first identify important word question reformulate query search relate evidence two enhance reader distinguish essential term distract word predict answer evaluate model multiple open domain multiple choice qa datasets notably perform level state art ai2 reason challenge arc dataset
much work natural language process nlp resource rich languages make generalization new less resourced languages challenge present two approach improve generalization low resourced languages adapt continuous word representations use linguistically motivate subword units phonemes morphemes graphemes method require neither parallel corpora bilingual dictionaries provide significant gain performance previous methods rely resources demonstrate effectiveness approach name entity recognition four languages namely uyghur turkish bengali hindi uyghur bengali low resource languages also perform experiment machine translation exploit subwords transfer learn give us boost one hundred and fifty-two ner f1 uyghur ninety-seven f1 bengali also show improvements monolingual set achieve avg three f1 avg one hundred and thirty-five bleu
consider case domain expert wish explore extent particular idea express text collection propose task semantically match idea express natural language proposition corpus create two preliminary task derive exist datasets introduce realistic one disaster recovery design emergency managers engage user study latter find new model build natural language entailment data produce higher quality match simple word vector average expert craft query ones produce subject work provide proof concept applications semantic match illustrate key challenge
popular stack lstm layer get better model power especially large amount train data available however lstm rnn many vanilla lstm layer hard train still exist gradient vanish issue network go deep issue partially solve add skip connections layer residual lstm paper propose layer trajectory lstm ltlstm build layer lstm use layer output standard multi layer time lstm layer lstm scan output time lstms use summarize layer trajectory information final senone classification forward propagation time lstm layer lstm handle two separate thread parallel network computation time standard time lstm layer lstm run layer gate path provide output layer bottom layer alleviate gradient vanish issue train thirty thousand hours en us microsoft internal data propose ltlstm perform significantly better standard multi layer lstm residual lstm ninety relative word error rate reduction across different task
script define knowledge everyday scenarios go restaurant expect unfold one challenge learn script hierarchical nature knowledge example suspect arrest might plead innocent guilty different track events expect happen capture type information propose autoencoder model latent space define hierarchy categorical variables utilize recently propose vector quantization base approach allow continuous embeddings associate latent variable value permit decoder softly decide portion latent hierarchy condition attend value embeddings give set model effectively encode generate script outperform recent language model base method several standard task allow autoencoder model achieve substantially lower perplexity score compare previous language model base method
neural model show several state art performances semantic role label srl however neural model require immense amount semantic role corpora thus well suit low resource languages domains paper propose semi supervise semantic role label method outperform state art limit srl train corpora method base explicitly enforce syntactic constraints augment train objective syntactic inconsistency loss component use srl unlabeled instance train joint objective lstm conll two thousand and twelve english section propose semi supervise train one ten srl label data vary amount srl unlabeled data achieve one hundred and fifty-eight seventy-eight f1 respectively pre train model train sota architecture elmo srl label data additionally use syntactic inconsistency loss inference time propose model achieve three hundred and sixty-seven twenty-one f1 pre train model one ten srl label data respectively
neural text generation include neural machine translation image caption summarization quite successful recently however train time typically one reference consider example even though often multiple reference available eg four reference nist mt evaluations five reference image caption data first investigate several different ways utilize multiple human reference train importantly propose algorithm generate exponentially many pseudo reference first compress exist human reference lattices traverse generate new pseudo reference approach lead substantial improvements strong baselines machine translation fifteen bleu image caption thirty-one bleu one hundred and seventeen cider
beam search widely use neural machine translation usually improve translation quality compare greedy search widely observe however beam size larger five hurt translation quality explain happen propose several methods address problem furthermore discuss optimal stop criteria methods result show hyperparameter free methods outperform widely use hyperparameter free heuristic length normalization twenty bleu achieve best result among methods chinese english translation
source code rarely write isolation depend significantly programmatic context class code would reside study phenomenon introduce task generate class member function give english documentation programmatic context provide rest class task challenge desire code vary greatly depend functionality class provide eg sort function may may available ask return smallest element particular member variable list introduce concode new large dataset one hundred thousand examples consist java class online code repositories develop new encoder decoder architecture model interaction method documentation class environment also present detail error analysis suggest significant room future work task
introduce multi task setup identify classify entities relations coreference cluster scientific article create scierc dataset include annotations three task develop unify framework call scientific information extractor sciie share span representations multi task setup reduce cascade errors task leverage cross sentence relations coreference link experiment show multi task model outperform previous model scientific information extraction without use domain specific feature show framework support construction scientific knowledge graph use analyze information scientific literature
consider negotiation settings two agents use natural language bargain goods agents need decide high level strategy eg propose fifty execution strategy eg generate bike brand new sell fifty recent work negotiation train neural model end end nature make hard control strategy reinforcement learn tend lead degenerate solutions paper propose modular approach base coarse di alogue act eg proposeprice50 decouple strategy generation show flexibly set strategy use supervise learn reinforcement learn domain specific knowledge without degeneracy retrieval base generation maintain context awareness produce diverse utterances test approach recently propose dealornodeal game also collect richer dataset base real items craigslist human evaluation show systems achieve higher task success rate human like negotiation behavior previous approach
neural network tree base sentence encoders show better result many downstream task exist tree base encoders adopt syntactic parse tree explicit structure prior study effectiveness different tree structure replace parse tree trivial tree ie binary balance tree leave branch tree right branch tree encoders though trivial tree contain syntactic information encoders get competitive even better result ten downstream task investigate surprise result indicate explicit syntax guidance may main contributor superior performances tree base neural sentence model analysis show tree model give better result crucial word closer final representation additional experiment give clue design effective tree base encoder code open source available https githubcom explorerfreda treeenc
present end end neural model detect metaphorical word use context show relatively standard bilstm model operate complete sentence work well set comparison previous work use restrict form linguistic context model establish new state art exist verb metaphor detection benchmarks show strong performance jointly predict metaphoricity word run text
propose achieve explainable neural machine translation nmt change output representation explain present novel approach nmt generate target sentence monotonically walk source sentence word reorder model operations allow set markers target sentence move target side write head markers contrast many modern neural model system emit explicit word alignment information often crucial practical machine translation improve explainability technique outperform plain text system term bleu score recent transformer architecture japanese english portuguese english within five bleu difference spanish english
investigate effect multi task learn use recently introduce task semantic tag employ semantic tag auxiliary task three different nlp task part speech tag universal dependency parse natural language inference compare full neural network share partial neural network share term learn share set negative transfer task less likely find show considerable improvements task particularly learn share set show consistent gain across task
recent years number people study english second language esl surpass number native speakers recent work demonstrate success provide personalize content base read difficulty information retrieval summarization however almost prior study read difficulty design native speakers rather non native readers study investigate various feature esl readers conduct linear regression estimate read level english language source estimation base complexity lexical syntactic feature also several novel concepts include age word grammar acquisition several source word sense wordnet implicit relation sentence employ bayesian information criterion bic select optimal model find combination number word age word acquisition height parse tree generate better result alternative compete model thus result show propose second language read difficulty estimation outperform first language read difficulty estimations
vlogs provide rich public source data novel set paper examine continuous sentiment style employ twenty-seven thousand, three hundred and thirty-three vlogs use dynamic intra textual approach sentiment analysis use unsupervised cluster identify seven distinct continuous sentiment trajectories characterize fluctuations sentiment throughout vlog narrative time provide taxonomy seven continuous sentiment style find vlogs whose sentiment build towards positive end prevalent sample gender associate preferences different continuous sentiment trajectories paper discuss find respect previous work conclude outlook towards possible use corpus method find paper relate areas research
introduce dsds cross lingual neural part speech tagger learn disparate source distant supervision realistically scale hundreds low resource languages model exploit annotation projection instance selection tag dictionaries morphological lexicons distribute representations uniform framework approach simple yet surprisingly effective result new state art without access gold annotate data
note deep learn nlp
languages annotate resources unsupervised transfer natural language process model name entity recognition ner resource rich languages would appeal capability however differences word word order across languages make challenge problem improve map lexical items across languages propose method find translations base bilingual word embeddings improve robustness word order differences propose use self attention allow degree flexibility respect word order demonstrate methods achieve state art competitive ner performance commonly test languages cross lingual set much lower resource requirements past approach also evaluate challenge apply methods uyghur low resource language
recently propose quantum language model qlm aim principled approach model term dependency apply quantum probability theory latest development effective qlm adopt word embeddings kind global dependency information integrate quantum inspire idea neural network architecture quantum inspire lms theoretically general also practically effective two major limitations first take account interaction among word multiple mean common important understand natural language text second integration quantum inspire lm neural network mainly effective train parameters yet lack theoretical foundation account integration address two issue paper propose quantum many body wave function qmwf inspire language model approach qmwf inspire lm adopt tensor product model aforesaid interaction among word also enable us reveal inherent necessity use convolutional neural network cnn qmwf language model furthermore approach deliver simple algorithm represent match text sentence pair systematic evaluation show effectiveness propose qmwf lm algorithm comparison state art quantum inspire lms couple cnn base methods three typical question answer qa datasets
product review form texts dominantly significantly help consumers finalize purchase decisions thus important e commerce company predict review helpfulness present recommend review informative manner work introduce convolutional neural network model able extract abstract feature multi granularity representations inspire fact different word contribute mean sentence differently consider learn word level embed gate representations furthermore common product domains categories rich user review domains help domains less sufficient data integrate model cross domain relationship learn framework effectively transfer knowledge domains extensive experiment show model yield better performance exist methods
argue humans rapidly adapt lexical syntactic expectations match statistics current linguistic context provide support claim show addition simple adaptation mechanism neural language model improve predictions human read time compare non adaptive model analyze performance model control materials psycholinguistic experiment show adapt lexical items also abstract syntactic structure
translate character instead word word fragment potential simplify process pipeline neural machine translation nmt improve result eliminate hyper parameters manual feature engineer however result longer sequence symbol contain less information create model computational challenge paper show model problem solve standard sequence sequence architectures sufficient depth deep model operate character level outperform identical model operate word fragment result imply alternative architectures handle character input better view methods reduce computation time improve ways model longer sequence perspective evaluate several techniques character level nmt verify match performance deep character baseline model evaluate performance versus computation time tradeoffs offer within framework also perform first evaluation nmt conditional computation time model learn timesteps skip rather dictate fix schedule specify train begin
substantial thread recent work latent tree learn attempt develop neural network model parse value latent variables train non parse task hope discover interpretable tree structure recent paper shen et al two thousand and eighteen introduce model report near state art result target task language model first strong latent tree learn result constituency parse attempt reproduce result discover issue make original result hard trust include tune even train effectively test set attempt reproduce result fair experiment extend two new datasets find result work robust variants model study outperform latent tree learn baselines perform competitively symbolic grammar induction systems find model represent first empirical success latent tree learn neural network language model warrant study set grammar induction
study two problems neural machine translation nmt first beam search whereas wider beam principle help translation often hurt nmt second nmt tendency produce translations short argue problems closely relate root label bias show correct brevity problem almost eliminate beam problem compare commonly use methods find simple per word reward work well introduce simple quick way tune reward use perceptron algorithm
character level string string transduction important component various nlp task goal map input string output string string may different lengths character take different alphabets recent approach use sequence sequence model attention mechanism learn part input string model focus generation output string soft attention hard monotonic attention use hard non monotonic attention use sequence model task image caption require stochastic approximation compute gradient work introduce exact polynomial time algorithm marginalize exponential number non monotonic alignments two string show hard attention model view neural reparameterizations classical ibm model one compare soft hard non monotonic attention experimentally find exact algorithm significantly improve performance stochastic approximation outperform soft attention
model generate program source code natural language represent code tree structure common approach however exist methods often fail generate complex code correctly due lack ability memorize large complex structure introduce recode method base subtree retrieval make possible explicitly reference exist code examples within neural code generation model first retrieve sentence similar input sentence use dynamic program base sentence similarity score method next extract n grams action sequence build associate abstract syntax tree finally increase probability action retrieve n gram action subtree predict code show approach improve performance two code generation task twenty-six bleu
conversational agents alexa google assistant constantly need increase language understand capabilities add new domains massive amount label data require train new domain domain adaptation approach alleviate annotation cost prior approach suffer increase train time suboptimal concept alignments tackle introduce novel zero shoot adaptive transfer method slot tag utilize slot description transfer reusable concepts across domains enjoy efficient train without explicit concept alignments extensive experimentation dataset ten domains relevant commercial personal digital assistant show model outperform previous state art systems large margin achieve even higher improvement low data regime
generate reasonable end give story context ie story end generation strong indication story comprehension task require understand context clue play important role plan plot also handle implicit knowledge make reasonable coherent story paper devise novel model story end generation model adopt incremental encode scheme represent context clue span story context addition commonsense knowledge apply multi source attention facilitate story comprehension thus help generate coherent reasonable end build context clue use implicit knowledge model able produce reasonable story end context clue imply post make inference base automatic manual evaluation show model generate reasonable story end state art baselines
paper propose state art recurrent neural network rnn language model combine probability distributions compute final rnn layer also middle layer propose method raise expressive power language model base matrix factorization interpretation language model introduce yang et al two thousand and eighteen propose method improve current state art language model achieve best score penn treebank wikitext two standard benchmark datasets moreover indicate propose method contribute two application task machine translation headline generation code publicly available https githubcom nttcslab nlp doclm
always criticism use n gram base similarity metrics bleu nist etc evaluate performance nlg systems however metrics continue remain popular recently use evaluate performance systems automatically generate question document knowledge graph image etc give rise interest automatic question generation aqg systems important objectively examine whether metrics suitable task particular important verify whether metrics use evaluate aqg systems focus answerability generate question prefer question contain relevant information question type wh type entities relations etc work show current automatic evaluation metrics base n gram similarity always correlate well human judgments answerability question alleviate problem first step towards better evaluation metrics aqg introduce score function capture answerability show score function integrate exist metrics correlate significantly better human judgments script data develop part work make publicly available https githubcom prekshanema25 answerability metric
pronouns long stand challenge machine translation present study performance range rule base statistical neural mt systems pronoun translation base extensive manual evaluation use protest test suite enable fine grain analysis different pronoun type shed light difficulties task find rule base approach corpus perform poorly result oversimplification whereas smt early nmt systems exhibit significant shortcomings due lack awareness functional referential properties pronouns recent transformer base nmt system cross sentence context show promise result non anaphoric pronouns intra sentential anaphora still considerable room improvement examples cross sentence dependencies
performance automatic speech recognition systems improve adapt acoustic model compensate mismatch train test condition example adapt unseen speakers success speaker adaptation methods rely select weight suitable adaptation use good adaptation schedule update weight order overfit adaptation data paper investigate principled way adapt weight acoustic model use meta learn show meta learner learn perform supervise unsupervised speaker adaptation outperform strong baseline adapt lhuc parameters adapt dnn 15m parameters also report initial experiment adapt tdnn ams meta learner achieve comparable performance lhuc
context dependent nature online aggression make annotate large collections data extremely difficult previously study datasets abusive language detection insufficient size efficiently train deep learn model recently hate abusive speech twitter dataset much greater size reliability release however dataset comprehensively study potential paper conduct first comparative study various learn model hate abusive speech twitter discuss possibility use additional feature context data improvements experimental result show bidirectional gru network train word level feature latent topic cluster modules accurate model score eight hundred and five f1
introduce novel multi source technique incorporate source syntax neural machine translation use linearize parse achieve employ separate encoders sequential parse versions source sentence result representations combine use hierarchical attention mechanism propose model improve seq2seq parse baselines one bleu wmt17 english german task analysis show multi source syntactic model able translate successfully without parse input unlike standard parse methods addition performance deteriorate much long sentence baselines
implicit discourse relation classification one challenge important task discourse parse due lack connective strong linguistic cue principle bottleneck improvement shortage train data ca16k instance pdtb shi et al two thousand and seventeen propose acquire additional data exploit connectives translation human translators mark discourse relations implicit source language explicitly translation use back translations explicitated connectives improve discourse relation parse performance paper address open question whether choice translation language matter whether multiple translations different languages effectively use improve quality additional data
neural network dominate state art result wide range nlp task attract considerable attention improve performance neural model integrate symbolic knowledge different exist work paper investigate combination two powerful paradigms knowledge drive side propose neural rule engine nre learn knowledge explicitly logic rule generalize implicitly neural network nre implement neural module network module represent action logic rule experiment show nre could greatly improve generalization abilities logic rule significant increase recall meanwhile precision still maintain high level
computational detection understand empathy important factor advance human computer interaction yet date text base empathy prediction follow major limitations underestimate psychological complexity phenomenon adhere weak notion grind truth empathic state ascribe third party lack share corpus contrast contribution present first publicly available gold standard empathy prediction construct use novel annotation methodology reliably capture empathy assessments writer statement use multi item scale also first computational work distinguish multiple form empathy empathic concern personal distress recognize throughout psychology finally present experimental result three different predictive model cnn perform best
reassess recent study hassan et al two thousand and eighteen claim machine translation mt reach human parity translation news chinese english use pairwise rank consider three variables take account previous study language source side test set originally write translation proficiency evaluators provision inter sentential context consider original source text ie translate another language translationese find evidence show human parity achieve compare judgments professional translators non experts discover experts result higher inter annotator agreement better discrimination human machine translations addition analyse human translations test set identify important translation issue finally base find provide set recommendations future human evaluations mt
introduce syntactic scaffold approach incorporate syntactic information semantic task syntactic scaffold avoid expensive syntactic process runtime make use treebank train multitask objective improve strong baselines propbank semantics frame semantics coreference resolution achieve competitive performance three task
natural language process greatly benefit introduction attention mechanism however standard attention model limit interpretability task involve series inference step describe iterative recursive attention model construct incremental representations input data reuse result previously compute query train model sentiment classification datasets demonstrate capacity identify combine different aspects input easily interpretable manner obtain performance close state art
aishell one far largest open source speech corpus available mandarin speech recognition research release baseline system contain solid train test pipelines mandarin asr aishell two one thousand hours clean read speech data ios publish free academic usage top aishell two corpus improve recipe develop release contain key components industrial applications chinese word segmentation flexible vocabulary expension phone set transformation etc pipelines support various state art techniques time delay neural network lattic free mmi objective funciton addition also release dev test data channelsandroid mic research community hope aishell two corpus solid resource topics like transfer learn robust asr industry hope aishell two recipe helpful reference build meaningful industrial systems products
paper describe multimodal machine translation systems develop jointly oregon state university baidu research wmt two thousand and eighteen share task multimodal translation paper introduce simple approach incorporate image information feed image feature decoder side also explore different sequence level train methods include schedule sample reinforcement learn lead substantial improvements systems ensemble several model use different architectures train methods achieve best performance three subtasks en de en cs task one endefr cs task 1b
task dialogue generation aim automatically provide responses give previous utterances track dialogue state important ingredient dialogue generation estimate users intention however emphexpensive nature state label emphweak interpretability make dialogue state track challenge problem task orient non task orient dialogue generation generate responses task orient dialogues state track usually learn manually annotate corpora human annotation expensive train generate responses non task orient dialogues exist work neglect explicit state track due unlimited number dialogue state paper propose emphsemi supervise explicit dialogue state tracker sedst neural dialogue generation end approach two core ingredients emphcopyflownet emphposterior regularization specifically propose encoder decoder architecture name emphcopyflownet represent explicit dialogue state probabilistic distribution vocabulary space optimize train procedure apply posterior regularization strategy integrate indirect supervision extensive experiment conduct task orient non task orient dialogue corpora demonstrate effectiveness propose model moreover find propose semi supervise dialogue state tracker achieve comparable performance state art supervise learn baselines state track procedure
paper attempt link inner work neural language model linguistic theory focus complex phenomenon well discuss formal linguis tics negative polarity items briefly discuss lead hypotheses license contexts allow negative polarity items evaluate extent neural language model ability correctly process subset constructions show model find relation license context negative polarity item appear aware scope context extract parse tree sentence research hope pave way study link formal linguistics deep learn
effect amplifiers downtoners negations study general particularly context sentiment analysis however limit work aim transfer result methods discrete class emotions e g joy anger fear sadness surprise disgust instance straight forward interpret emotion phrase happy express paper aim obtain better understand modifiers context emotion bear word impact document level emotion classification namely microposts twitter select appropriate scope detection method modifiers emotion word incorporate document level emotion classification model additional bag word show approach improve performance emotion classification addition build term weight approach base different modifiers lexical model analysis semantics modifiers impact emotion mean show amplifiers separate emotions express emotion bear word clearly secondary connotations downtoners opposite effect addition discuss mean negations emotion bear word instance show empirically happy closer sadness anger fear express word scope downtoners often express surprise
tie weight target word embeddings target word classifiers neural machine translation model lead faster train often better translation quality give success parameter share investigate form share share hard equality parameters particular propose structure aware output layer capture semantic structure output space word within joint input output embed model generalize form weight tie share parameters allow learn flexible relationship input word embeddings allow effective capacity output layer control addition model share weight across output classifiers translation contexts allow better leverage prior knowledge evaluation english finnish english german datasets show effectiveness method strong encoder decoder baselines train without weight tie
open end survey data constitute important basis research well make business decisions collect manually analyse free text survey data generally costly collect analyse survey data consist answer multiple choice question yet free text data allow new content express beyond predefined categories valuable source new insights people opinions time survey always make ontological assumptions nature entities research vital ethical consequences human interpretations opinions properly ascertain richness use textual data source source analyze appropriately essential linguistic nature humans social entities safeguard natural language process nlp offer possibilities meet ethical business challenge automate analysis natural language thus allow insightful investigations human judgements present computational pipeline analyse large amount responses open end question survey extract keywords appropriately represent people opinions pipeline address need perform task outside scope commercial software bespeak analysis exceed performance state art systems perform task transparent way allow scrutinise expose potential bias analysis follow principle open data science code open source generalizable datasets
employ imitation learn train neural transition base string transducer morphological task inflection generation lemmatization previous approach train type model either rely external character aligner production gold action sequence result suboptimal model due unwarranted dependence single gold action sequence despite spurious ambiguity require warm start mle model approach require simple expert policy eliminate need character aligner warm start also address familiar mle train bias lead strong state art performance several benchmarks
human conversational interactions turn take exchange coordinate use cue multiple modalities design speak dialog systems conduct fluid interactions desirable incorporate cue separate modalities turn take model propose appropriate temporal granularity modalities model design multiscale rnn architecture model modalities separate timescales continuous manner result show model linguistic acoustic feature separate temporal rat beneficial turn take model also show approach use incorporate gaze feature turn take model
article describe aalto university entry wmt18 news translation share task participate multilingual subtrack system train constrain condition translate english finnish estonian system base transformer model focus improve consistency morphological segmentation word similar orthographically semantically distributionally word include etymological cognates loan word proper name introduce cognate morfessor multilingual variant morfessor method show approach improve translation quality particularly estonian less resources train translation model
paper describe memad project entry wmt multimodal machine translation share task propose adapt transformer neural machine translation nmt architecture multi modal set paper also describe preliminary experiment text translation systems lead us choice top score system english german english french accord automatic metrics flickr18 experiment show effect visual feature system small largest gain come quality underlie text nmt system find appropriate use additional data effective
hallmark variational autoencoders vaes text process combination powerful encoder decoder model lstms simple latent distributions typically multivariate gaussians model pose difficult optimization problem especially bad local optimum variational posterior always equal prior model use latent variable kind collapse encourage kl divergence term objective work experiment another choice latent distribution namely von mises fisher vmf distribution place mass surface unit hypersphere choice prior posterior kl divergence term depend variance vmf distribution give us ability treat fix hyperparameter show avert kl collapse consistently give better likelihoods gaussians across range model condition include recurrent language model bag word document model analysis properties vmf representations show learn richer nuanced structure latent representations gaussian counterparts
word sense static may temporal spatial corpus specific scopes identify scopes might benefit exist wsd systems largely paper study corpus specific word sense adapt three exist predominant novel sense discovery algorithms identify corpus specific sense make use text data available form millions digitize book newspaper archive two different source corpora propose automate methods identify corpus specific word sense various time point conduct extensive thorough human judgment experiment rigorously evaluate compare performance approach post adaptation output three algorithms format accuracy result also comparable roughly forty-five sixty report corpus specific sense judge genuine
translations capture important information languages use implicit supervision learn linguistic properties semantic representations information centric view translate texts may consider semantic mirror original text significant variations observe across various languages use disambiguate give expression use linguistic signal ground translation parallel corpora consist massive amount human translations large linguistic variation apply increase abstractions propose use highly multilingual machine translation model find language independent mean representations initial experiment show neural machine translation model indeed learn setup show learn algorithm pick information relation languages order optimize transfer lean share parameters model create continuous language space represent relationships term geometric distance visualize illustrate languages cluster accord language families group open door new ideas data drive language typology promise model techniques empirical cross linguistic research
goal orient go dialogue systems colloquially know goal orient chatbots help users achieve predefined goal eg book movie ticket within close domain first step understand user goal use natural language understand techniques goal know bot must manage dialogue achieve goal conduct respect learn policy success dialogue system depend quality policy turn reliant availability high quality train data policy learn method instance deep reinforcement learn due domain specificity amount available data typically low allow train good dialogue policies paper introduce transfer learn method mitigate effect low domain data availability transfer learn base approach improve bot success rate twenty relative term distant domains double close domains compare model without transfer learn moreover transfer learn chatbots learn policy five ten time faster finally transfer learn approach complementary additional process warm start show joint application give best outcomes
speak language understand slu systems goal orient chatbots personal assistants rely initial natural language understand nlu module determine intent extract relevant information user query take input slu systems usually help users solve problems relatively narrow domains require large amount domain train data lead significant data availability issue inhibit development successful systems alleviate problem propose technique data selection low data regime enable us train fewer label sentence thus smaller label cost propose submodularity inspire data rank function ratio penalty marginal gain select data point label base information extract textual embed space show distance embed space viable source information use data selection method outperform two know active learn techniques enable cost efficient train nlu unit moreover propose selection technique need model retrain selection step make time efficient well
researchers claim language acquisition critically dependent experience linguistic input order increase complexity set test hypothesis use simple recurrent neural network srn train predict word sequence childes five million word corpus speech direct children first demonstrate age order childes exhibit gradual increase linguistic complexity next compare performance two group srns train childes either age order specifically assess learn grammatical semantic structure show train age order input facilitate learn semantic sequential structure find advantage eliminate model train input utterance boundary information remove
deep neural network recently show achieve highly competitive performance many computer vision task due abilities explore much larger hypothesis space however since deep architectures like stack rnns tend suffer vanish gradient overfitting problems effect still understudy many nlp task inspire propose novel multi layer rnn model call densely connect bidirectional long short term memory dc bi lstm paper essentially represent layer concatenation hide state precede layer hide state follow recursively pass layer representation subsequent layer evaluate propose model five benchmark datasets sentence classification dc bi lstm depth twenty successfully train obtain significant improvements traditional bi lstm even less parameters moreover model promise performance compare state art approach
deep learn techniques achieve success aspect base sentiment analysis recent years however two important issue still remain study ie one efficiently represent target especially target contain multiple word two utilize interaction target leave right contexts capture important word paper propose approach call leave center right separate neural network rotatory attention lcr rot better address two problems approach two characteristics one three separate lstms ie leave center right lstms correspond three part review leave context target phrase right context two rotatory attention mechanism model relation target leave right contexts target2context attention use capture indicative sentiment word leave right contexts subsequently context2target attention use capture important word target lead two side representation target leave aware target right aware target compare approach three benchmark datasets ten relate methods propose recently result show approach significantly outperform state art techniques
wealth structure eg wikidata unstructured data world available today present incredible opportunity tomorrow artificial intelligence far integration two different modalities difficult process involve many decisions concern best represent information capture useful hand label large amount data deeptype overcome challenge explicitly integrate symbolic information reason process neural network type system first construct type system second use constrain output neural network respect symbolic structure achieve reformulate design problem mix integer problem create type system subsequently train neural network reformulation discrete variables select parent child relations ontology type within type system continuous variables control classifier fit type system original problem solve exactly propose two step algorithm one heuristic search stochastic optimization discrete variables define type system inform oracle learnability heuristic two gradient descent fit classifier parameters apply deeptype problem entity link three standard datasets ie wikidisamb30 conll yago tac kbp two thousand and ten find outperform exist solutions wide margin include approach rely human design type system recent deep learn base entity embeddings explicitly use symbolic information let us integrate new entities without retrain
study feature selection mean optimize baseline clickbait detector employ clickbait challenge two thousand and seventeen challenge task score clickbaitiness give twitter tweet scale zero clickbait one strong clickbait unlike approach submit challenge baseline approach base manual feature engineer compete box many deep learn base approach show scale feature selection efforts heuristically identify better perform feature subsets catapult performance baseline classifier second rank overall beat twelve compete approach improve baseline performance twenty demonstrate traditional classification approach still keep deep learn task
word language reflect structure human mind allow us transmit thoughts individuals however language represent subset rich detail cognitive architecture ask kinds common knowledge semantic memory capture word mean lexical semantics examine prominent computational model represent word vectors multidimensional space proximity word vectors approximate semantic relatedness relate word appear similar contexts space call word embeddings learn pattern lexical co occurrences natural language despite popularity fundamental concern word embeddings appear semantically rigid inter word proximity capture overall similarity yet human judgments object similarities highly context dependent involve multiple distinct semantic feature example dolphins alligator appear similar size differ intelligence aggressiveness could context dependent relationships recover word embeddings address issue introduce powerful domain general solution semantic projection word vectors onto line represent various object feature like size line extend word small big intelligence dumb smart danger safe dangerous method intuitively analogous place object mental scale two extremes recover human judgments across range object categories properties thus show word embeddings inherit wealth common knowledge word co occurrence statistics flexibly manipulate express context dependent mean
text mine relations chemicals proteins increasingly important task chemprot track biocreative vi aim promote development evaluation systems automatically detect chemical protein relations run text pubmed abstract manuscript describe submission ensemble three systems include support vector machine convolutional neural network recurrent neural network output combine use decision base majority vote stack chemprot system obtain seven thousand, two hundred and sixty-six precision five thousand, seven hundred and thirty-five recall f score six thousand, four hundred and ten demonstrate effectiveness machine learn base approach automatic relation extraction biomedical literature submission achieve highest performance task two thousand and seventeen challenge
exist text generation methods tend produce repeat bore expressions tackle problem propose new text generation model call diversity promote generative adversarial network dp gin propose model assign low reward repeatedly generate text high reward novel fluent text encourage generator produce diverse informative text moreover propose novel language model base discriminator better distinguish novel text repeat text without saturation problem compare exist classifier base discriminators experimental result review generation dialogue generation task demonstrate model generate substantially diverse informative text exist baselines code available https githubcom lancopku dpgan
text summarization condense text shorter version retain important informations abstractive summarization recent development generate new phrase rather simply copy rephrase sentence within original text recently neural sequence sequence model achieve good result field abstractive summarization open new possibilities applications industrial purpose however practitioners observe model still use large part original text output summaries make often similar extractive frameworks address drawback first introduce new metric measure much summary extract input text secondly present novel method rely diversity factor compute neural network loss improve diversity summaries generate neural abstractive model implement beam search finally show method make system less extractive also improve overall rouge score state art methods least two point
sellers user user marketplaces inundate question potential buyers answer often already available product description collect dataset around 590k question answer conversations online marketplace propose question answer system select sentence product description use neural network rank model explore multiple encode strategies recurrent neural network fee forward attention layer yield good result paper present demo interactively pose buyer question visualize rank score product description sentence live online list
article propose auto encode text byte level use convolutional network recursive architecture motivation explore whether possible scalable homogeneous text generation byte level non sequential fashion simple task auto encode show non sequential text generation fix length representation possible also achieve much better auto encode result recurrent network propose model multi stage deep convolutional encoder decoder framework use residual connections contain one hundred and sixty parameterized layer encoder decoder contain share group modules consist either pool upsampling layer make network recursive term abstraction level representation result six large scale paragraph datasets report three languages include arabic chinese english analyse conduct study several properties propose model
report evaluation effectiveness exist knowledge base embed model relation prediction relation extraction wide range benchmarks also describe new benchmark much larger complex previous ones introduce help validate effectiveness task result demonstrate knowledge base embed model generally effective relation prediction unable give improvements state art neural relation extraction model exist strategies point limitations exist methods
paper introduce novel approach base bidirectional recurrent autoencoder perform globally optimize non projective dependency parse via semi supervise learn syntactic analysis complete end neural process generate latent head representation lhr without algorithmic constraint linear complexity result latent syntactic structure use directly semantic task lhr transform usual dependency tree compute simple vectors similarity believe model potential compete much complex state art parse architectures
background objective code assignment paramount importance many level modern hospitals ensure accurate bill process create valid record patient care history however cod process tedious subjective require medical coders extensive train study aim evaluate performance deep learn base systems automatically map clinical note icd nine medical cod methods evaluations research focus end end learn methods without manually define rule traditional machine learn algorithms well state art deep learn methods recurrent neural network convolution neural network apply medical information mart intensive care mimic iii dataset extensive number experiment apply different settings test algorithm result find show deep learn base methods outperform conventional machine learn methods assessment best model could predict top ten icd nine cod six thousand, nine hundred and fifty-seven f1 eight thousand, nine hundred and sixty-seven accuracy could estimate top ten icd nine categories seven thousand, two hundred and thirty-three f1 eight thousand, five hundred and eighty-eight accuracy implementation also outperform exist work certain evaluation metrics conclusion set standard metrics utilize assess performance icd nine code assignment mimic iii dataset develop evaluation tool resources available online use baseline research
paper mainly concern ability quickly automatically distinguish word sense dynamic semantic space new term new sense appear frequently space build fly constantly evolve data set wikipedia repositories patent grant applications large set legal document technology assist review e discovery immediacy rule supervision well use priori train set show various sense term automatically make apparent simple cluster algorithm sense vector semantic space consider semantic space build use random vectors algorithm work kind embed provide meaningful similarities term compute fulfill least two basic condition term close mean high similarities term unrelated mean near zero similarities
ubuntu dialogue corpus largest public available dialogue corpus make feasible build end end deep neural network model directly conversation data one challenge ubuntu dialogue corpus large number vocabulary word paper propose method combine general pre train word embed vectors generate task specific train set address issue integrate character embed chen et al enhance lstm method esim use evaluate effectiveness propose method task next utterance selection propose method demonstrate significant performance improvement original esim new model achieve state art result ubuntu dialogue corpus douban conversation corpus addition investigate performance impact end utterance end turn token tag
paper present novel prototype biomedical term normalization electronic health record excerpt unify medical language system umls metathesaurus despite multilingual cross lingual design first focus process clinical text spanish exist tool language specific purpose tool base apache lucene index metathesaurus generate map candidates input text use ixa pipeline basic language process resolve ambiguities ukb toolkit evaluate measure agreement metamap two english spanish parallel corpora addition present web base interface tool
present dismo multi level annotator speak language corpora integrate part speech tag basic disfluency detection annotation multi word unit recognition dismo hybrid system use combination lexical resources rule statistical model base conditional random field crf paper present first public version dismo french system train performance evaluate 57k token corpus include different varieties french speak three countries belgium france switzerland dismo support multi level annotation scheme tokenisation minimal word units complement multi word unit group associate pos tag well separate level annotate disfluencies discourse phenomena present system architecture linguistic resources hierarchical tag set result show dismo achieve precision ninety-five finest tag set nine hundred and sixty-eight coarse tag set pos tag non punctuate sound align transcriptions speak french also offer substantial possibilities automate multi level annotation
end end neural machine translation nmt achieve notable success past years translate handful resource rich language pair still suffer data scarcity problem low resource language pair domains tackle problem propose interactive multimodal framework zero resource neural machine translation instead passively expose large amount parallel corpora learners implement encoder decoder architecture engage cooperative image description game thus develop image caption neural machine translation model need communicate order succeed game experimental result iapr tc12 multi30k datasets show propose learn mechanism significantly improve state art methods
recent work speak language translation slt attempt build end end speech text translation without use source language transcription learn decode however large quantities parallel texts europarl opensubtitles available train machine translation systems large 100h open source parallel corpora include speech source language align text target language paper try fill gap augment exist monolingual corpus librispeech corpus use automatic speech recognition derive read audiobooks librivox project carefully segment align gather french e book correspond english audio book librispeech align speech segment sentence level respective translations obtain 236h usable parallel data paper present detail process well manual evaluation conduct small subset corpus evaluation show automatic alignments score reasonably correlate human judgments bilingual alignment quality believe corpus make available online useful replicable experiment direct speech translation general speak language translation experiment
try reproduce result paper natural language inference interaction space submit iclr two thousand and eighteen conference part iclr two thousand and eighteen reproducibility challenge initially aware code available start implement network scratch evaluate version model stanford nli dataset reach eight thousand, six hundred and thirty-eight accuracy test set paper claim eight hundred and eighty accuracy main difference understand come optimizers way model selection perform
sequence sequence seq2seq model play important role recent success various natural language process methods machine translation text summarization speech recognition however current seq2seq model trouble preserve global latent information long sequence word variational autoencoder vae alleviate problem learn continuous semantic space input sentence however solve problem completely paper propose new recurrent neural network rnn base seq2seq model rnn semantic variational autoencoder rnn svae better capture global latent information sequence word reflect mean word sentence properly without regard position within sentence construct document information vector use attention information final state encoder every prior hide state mean standard deviation continuous semantic space learn use vector take advantage variational method use document information vector find semantic space sentence become possible better capture global latent feature sentence experimental result three natural language task ie language model miss word imputation paraphrase identification confirm propose rnn svae yield higher performance two benchmark model
neural machine translation systems require large amount train data resources even quality translations may insufficient users domains case output system must revise human agent do post edit stage follow interactive machine translation protocol explore incremental update neural machine translation systems post edit interactive translation process modifications aim incorporate new knowledge edit sentence translation system update model perform fly sentence correct via online learn techniques addition implement novel interactive adaptive system able react single character interactions system greatly reduce human effort require obtain high quality translations order stress proposals conduct exhaustive experiment vary amount type data available train result show online learn effectively achieve objective reduce human effort require post edit interactive machine translation stag moreover adaptive systems also perform well scenarios scarce resources show neural machine translation system rapidly adapt specific domain exclusively mean online learn techniques
text representation fundamental concern natural language process especially text classification recently many neural network approach delicate representation model eg fasttext cnn rnn many hybrid model attention mechanisms claim achieve state art specific text classification datasets however lack unify benchmark compare model reveal advantage sub components various settings implement twenty popular text representation model classification ten datasets paper reconsider text classification task perspective neural network get serval effect analysis result
paper study analyse comment rhetorical figure present interest poetry first half twentieth century figure first trace back famous poet past compare classical latin prose linguistic theory call show represent syntactic structure classify noncanonical structure position discontinuous displace linguistic elements spec xp projections various level constituency introduce lfg lexical functional grammar theory allow us connect syntactic noncanonical structure informational structure psycholinguistic theories complexity evaluation end two computational linguistics experiment evaluate result first one use best online parsers italian parse poetic structure second one use getarun system create ca foscari computational linguistics laboratory show first approach unable cope structure due use statistical probabilistic information contrary second one symbolic rule base system far superior allow also complete semantic pragmatic analysis
demonstrate network visualization technique analyze recurrent state inside lstms grus use commonly language acoustic model interpret intermediate state network activations inside end end model remain open challenge method allow users understand exactly much history encode inside recurrent state grapheme sequence model procedure train multiple decoders predict prior input history compile result decoders user obtain signature recurrent kernel characterize memory behavior demonstrate method usefulness reveal information divergence base recurrent factorize kernels visualize character level differences memory n gram recurrent language model extract knowledge history encode layer grapheme base end end asr network
many applications require categorization text document use predefined categories main approach perform text categorization learn label examples many task may difficult find examples one language easy others problem learn examples one languages classify categorize another call cross lingual learn work present novel approach solve general cross lingual text categorization problem method generate train document set language independent feature use feature train yield language independent classifier classification stage generate language independent feature unlabeled document apply classifier new representation build feature generator utilize hierarchical language independent ontology concept set support document language involve preprocessing stage use support document build set language independent feature generators one language collection generators use map document language independent feature space methodology work general cross lingual text categorization problems able learn mix languages classify document language also present method exploit hierarchical structure ontology create virtual support document languages test method use wikipedia ontology commonly use test collections cross lingual text categorization find outperform exist methods
online community new word come go today haha may replace tomorrow lol change online write usually study social process innovations diffuse network individuals speech community unlike type innovation language change shape constrain system take part investigate link social structural factor language change undertake large scale analysis nonstandard word growth online community reddit find dissemination across many linguistic contexts sign growth word appear linguistic contexts grow faster survive longer also find social dissemination likely play less important role explain word growth decline previously hypothesize
investigate end end speech text translation corpus audiobooks specifically augment task previous work investigate extreme case source language transcription available learn decode also study midway case source language transcription available train time case single model train decode source speech target text single pass experimental result show possible train compact efficient end end speech translation model setup also distribute corpus hope speech translation baseline corpus challenge future
propose unify implicit dialog framework goal orient information seek task conversational search applications aim enable dialog interactions domain data without reply explicitly encode rule utilize underlie data representation build components require dialog interaction refer implicit dialog work propose framework consist pipeline end end trainable modules centralize knowledge representation use semantically grind multiple dialog modules associate set tool integrate framework gather end users input continuous improvement system goal facilitate development conversational systems identify components data adapt reuse across many end user applications demonstrate approach create conversational agents several independent domains
work tackle problem sentence boundary detection apply french binary classification task sentence boundary sentence boundary combine convolutional neural network subword level information vectors word embed representations learn wikipedia take advantage word morphology word represent bag character n grams decide use big write dataset french gigaword instead standard size transcriptions train evaluate propose architectures intention use train model posterior real life asr transcriptions three different architectures test show similar result general accuracy model overpasses ninety-six three model good f1 score reach value ninety-seven regard sentence boundary class however sentence boundary class reflect lower score decrease f1 metric seven hundred and seventy-eight one model use subword level information vectors seem effective lead conclude morphology word encode embeddings representations behave like pixels image make feasible use convolutional neural network architectures
distinguish lexical relations long term pursuit natural language process nlp domain recently order detect lexical relations like hypernymy meronymy co hyponymy etc distributional semantic model use extensively form even though lot efforts make detect hypernymy relation problem co hyponymy detection rarely investigate paper propose novel supervise model various network measure utilize identify co hyponymy relation high accuracy perform better par state art model
neural machine translation nmt widely use recent years significant improvements many language pair although state art nmt systems generate progressively better translations idiom translation remain one open challenge field idioms category multiword expressions interest language phenomenon overall mean expression compose mean part first important challenge lack dedicate data set learn evaluate idiom translation paper address problem create first large scale data set idiom translation data set automatically extract widely use german english translation corpus include language direction target evaluation set sentence contain idioms regular train corpus sentence include idioms mark release data set use perform preliminary nmt experiment first step towards better idiom translation
large sense annotate datasets increasingly necessary train deep supervise systems word sense disambiguation however gather high quality sense annotate data many instance possible laborious expensive task lead proliferation automatic semi automatic methods overcome call knowledge acquisition bottleneck short survey present overview sense annotate corpora annotate either manually semiautomatically currently available different languages feature distinct lexical resources inventory sense ie wordnet wikipedia babelnet furthermore provide reader general statistics dataset analysis specific feature
paper short empirical study performance centrality classification base iterative term set expansion methods distributional semantic model iterative term set expansion interactive process use distributional semantics model user label term belong seek term set system use label supply user new candidate term label try maximize number positive examples find centrality base methods long history term set expansion compare classification methods base simple margin method active learn approach classification use support vector machine examine performance various centrality classification base methods variety distributional model five different term set show active learn base methods consistently outperform centrality base methods
summarize accomplishments multi disciplinary workshop explore computational scientific issue surround discovery linguistic units subwords word language without orthography study replacement orthographic transcriptions image translate text well resourced language help unsupervised discovery raw speech
paper propose method classify movie genres look text review data use large movie review dataset v10 imdb paper compare k nearest neighbor knn model multilayer perceptron mlp use tf idf input feature paper also discuss different evaluation metrics use multi label classification data use research knn model perform best accuracy five hundred and fifty-four ham loss forty-seven
introduce new type deep contextualized word representation model one complex characteristics word use eg syntax semantics two use vary across linguistic contexts ie model polysemy word vectors learn function internal state deep bidirectional language model bilm pre train large text corpus show representations easily add exist model significantly improve state art across six challenge nlp problems include question answer textual entailment sentiment analysis also present analysis show expose deep internals pre train network crucial allow downstream model mix different type semi supervision signal
paper propose new universal machine translation approach focus languages limit amount parallel data propose approach utilize transfer learn approach share lexical sentence level representations across multiple source languages one target language lexical part share universal lexical representation support multilingual word level share sentence level share represent model experts source languages share source encoders languages enable low resource language utilize lexical sentence representations higher resource languages approach able achieve twenty-three bleu romanian english wmt2016 use tiny parallel corpus 6k sentence compare eighteen bleu strong baseline system use multilingual train back translation furthermore show propose approach achieve almost twenty bleu dataset fine tune pre train multi lingual system zero shoot set
build satisfy chatbot ability manage goal orient multi turn dialogue accurate model human conversation crucial paper concentrate task response selection multi turn human computer conversation give context previous approach show weakness capture information rare keywords appear either context correct response struggle long input sequence propose cross convolution network ccn multi frequency word embed address problems train several model use ubuntu dialogue dataset largest freely available multi turn base dialogue corpus build ensemble model average predictions multiple model achieve new state art dataset considerable improvements compare previous best result
open information extraction oie task unsupervised creation structure information text oie often use start point number downstream task include knowledge base construction relation extraction question answer oie methods target domain independent evaluate primarily newspaper encyclopedic general web text article evaluate performance oie scientific texts originate ten different discipline use two state art oie systems apply crowd source approach find oie systems perform significantly worse scientific text encyclopedic text also provide error analysis suggest areas work reduce errors corpus sentence judgments make available
present novel deep learn architecture address natural language inference nli task exist approach mostly rely simple read mechanisms independent encode premise hypothesis instead propose novel dependent read bidirectional lstm network dr bilstm efficiently model relationship premise hypothesis encode inference also introduce sophisticate ensemble strategy combine propose model noticeably improve final predictions finally demonstrate result improve additional preprocessing step evaluation show dr bilstm obtain best single model ensemble model result achieve new state art score stanford nli dataset
paper introduce set resources tool aim provide support natural language process text speech synthesis speech recognition romanian tool general purpose use language successfully train system fifty languages participate universal dependencies share task resources relevant romanian language process
calculate semantic similarity sentence long deal problem area natural language process semantic analysis field crucial role play research relate text analytics semantic similarity differ domain operation differ paper present methodology deal issue incorporate semantic similarity corpus statistics calculate semantic similarity word sentence propose method follow edge base approach use lexical database methodology apply variety domains methodology test benchmark standards mean human similarity dataset test two datasets give highest correlation value word sentence similarity outperform similar model word similarity obtain pearson correlation coefficient eight thousand, seven hundred and fifty-three sentence similarity correlation obtain eight thousand, seven hundred and ninety-four
traditional event detection methods heavily rely manually engineer rich feature recent deep learn approach alleviate problem automatic feature engineer efforts like tradition methods far focus single token event mention whereas practice events also phrase instead use forward backward recurrent neural network fbrnns detect events either word phrase best knowledge one first efforts handle multi word events also first attempt use rnns event detection experimental result demonstrate fbrnn competitive state art methods ace two thousand and five rich ere two thousand and fifteen event detection task
paper report work nlp tool contest icon two thousand and seventeen share task sentiment analysis indian languages sail code mix implement system use machine learn algo rithm call multinomial nai bay train use n gram sentiwordnet feature also use small sentiwordnet english small sentiwordnet bengali use sentiwordnet hindi language test system hindi english bengali english code mix social media data set release contest performance system close best system participate contest bengali english hindi english run system rank 3rd position submit run award 3rd prize contest
argument mine core technology automate argument search large document collections despite usefulness task current approach argument mine design use specific text type fall short apply heterogeneous texts paper propose new sentential annotation scheme reliably applicable crowd workers arbitrary web texts source annotations twenty-five thousand instance cover eight controversial topics result cross topic experiment show attention base neural network generalize best unseen topics outperform vanilla bilstm model six accuracy eleven f score
machine learn quintessential solution many ai problems learn still heavily dependent specific train data learn model incorporate prior knowledge bayesian set learn model ability access organise world knowledge demand work propose enhance learn model world knowledge form knowledge graph kg fact triple natural language process nlp task aim develop deep learn model extract relevant prior support facts knowledge graph depend task use attention mechanism introduce convolution base model learn representations knowledge graph entity relation cluster order reduce attention space show propose method highly scalable amount prior information process apply generic nlp task use method show significant improvement performance text classification news20 dbpedia datasets natural language inference stanford natural language inference snli dataset also demonstrate deep learn model train well substantially less amount label train data access organise world knowledge form knowledge graph
supervise learn model typically train single dataset performance model rely heavily size dataset ie amount data available grind truth learn algorithms try generalize solely base data present train work propose inductive transfer learn method augment learn model infuse similar instance different learn task natural language process nlp domain propose use instance representations source dataset textitwithout inherit anything source learn model representations instance textitsource textittarget datasets learn retrieval relevant source instance perform use soft attention mechanism textitlocality sensitive hash augment model train target dataset approach simultaneously exploit local textitinstance level information well macro statistical viewpoint dataset use approach show significant improvements three major news classification datasets baseline experimental evaluations also show propose approach reduce dependency label data significant margin comparable performance propose cross dataset learn procedure show one achieve competitive better performance learn single dataset
although measure intrinsic quality key factor advancement machine translation mt successfully deploy mt require consider intrinsic quality also user experience include aspects trust work introduce method study users modulate trust mt system see errorful disfluent inadequate output amidst good fluent adequate output conduct survey determine users respond good translations compare translations either adequate fluent fluent adequate pilot study users respond strongly disfluent translations surprisingly much less concern adequacy
develop speech technologies low resource languages become active research field last decade among others bayesian model show promise result artificial examples still lack situ experiment work apply state art bayesian model unsupervised acoustic unit discovery aud real low resource language scenario also show bayesian model naturally integrate information resourceful languages mean informative prior lead consistent discover units finally discover acoustic units use either one best sequence lattice perform word segmentation word segmentation result show bayesian approach clearly outperform segmental dtw baseline corpus
distribute representations word learn text prove successful various natural language process task recent time methods represent word vectors compute text use predictive model word2vec dense count base model glove others attempt represent distributional thesaurus network structure neighborhood word set word adequate context overlap motivate recent surge research network embed techniques deepwalk line node2vec etc turn distributional thesaurus network dense word vectors investigate usefulness distributional thesaurus embed improve overall word representation first attempt show combine propose word representation obtain distributional thesaurus embed state art word representations help improve performance significant margin evaluate nlp task like word similarity relatedness synonym detection analogy detection additionally show even without use handcraft lexical resources come representations comparable performance word similarity relatedness task compare representations lexical resource use
argue without commit fallacy one main requirements ideal debate even debate rule strictly enforce fallacious arguments punish arguers often lapse attack opponent ad hominem argument exist research lack solid empirical investigation typology ad hominem arguments well potential cause paper fill gap one perform several large scale annotation study two experiment various neural architectures validate work hypotheses controversy reasonableness three provide linguistic insights trigger ad hominem use explainable neural network architectures
explore multitask model neural translation speech augment order reflect two intuitive notions first introduce model second task decoder receive information decoder first task since higher level intermediate representations provide useful information second apply regularization encourage transitivity invertibility show application notions jointly train model improve performance task low resource speech transcription translation also lead better performance use attention information word discovery unsegmented input
present neural model question generation knowledge base triple zero shoot setup generate question triple contain predicate subject type object type see train time model leverage triple occurrences natural language corpus encoder decoder architecture pair original part speech copy action mechanism generate question benchmark human evaluation show model set new state art zero shoot qg
detect novelty entire document artificial intelligence ai frontier problem widespread nlp applications extractive document summarization track development news events predict impact scholarly article etc important though problem unaware benchmark document level data correctly address evaluation automatic novelty detection techniques classification framework bridge gap present resource benchmarking techniques document level novelty detection create resource via event specific crawl news document across several domains periodic manner release annotate corpus necessary statistics show use develop system problem concern
chatbots take advantage success message apps recent advance artificial intelligence become popular help business improve customer service chat users sake conversation engagement celebrity personal bots however develop improve chatbot require understand data generate users dialog data different nature simple question answer interaction context temporal properties turn order create different understand data paper propose novelty metric compute dialogs similarity base text content also information relate dialog structure experimental result perform switchboard dataset show use evidence textual content dialog structure lead accurate result use measure isolation
paper present open source neural machine translation toolkit name cytonmt https githubcom arthurxlw cytonmt toolkit build scratch use c nvidia gpu accelerate libraries toolkit feature train efficiency code simplicity translation quality benchmarks show cytonmt accelerate train speed six hundred and forty-five one thousand, one hundred and eight neural network various size achieve competitive translation quality
implicit arguments syntactically connect predicate therefore hard extract previous work use model large number feature evaluate small datasets propose train model implicit argument prediction simple cloze task data generate automatically scale allow us use neural model draw narrative coherence entity salience predictions show model superior performance synthetic natural data
word embed useful approach capture co occurrence structure large text corpora however addition text data often additional covariates associate individual corpus document eg demographic author time venue publication would like embed naturally capture information propose cover new tensor decomposition model vector embeddings covariates cover jointly learn emphbase embed word well weight diagonal matrix model covariate affect base embed obtain author venue specific embed example simply multiply base embed associate transformation matrix main advantage approach data efficiency interpretability covariate transformation experiment demonstrate joint model learn substantially better covariate specific embeddings compare standard approach learn separate embed covariate use relevant subset data well relate methods furthermore cover encourage embeddings topic align dimension specific independent mean allow covariate specific embeddings compare topic enable downstream differential analysis empirically evaluate benefit algorithm datasets demonstrate use address many natural question covariate effect accompany code paper find http githubcom kjtian cover
social tag movies reveal wide range heterogeneous information movies like genre plot structure soundtracks metadata visual emotional experience information valuable build automatic systems create tag movies automatic tag systems help recommendation engines improve retrieval similar movies well help viewers know expect movie advance paper set task collect corpus movie plot synopses tag describe methodology enable us build fine grain set around seventy tag expose heterogeneous characteristics movie plot multi label associations tag 14k movie plot synopses investigate tag correlate movies flow emotions throughout different type movies finally use corpus explore feasibility infer tag plot synopses expect corpus useful task analysis narratives relevant
introduce new task call multimodal name entity recognition mner noisy user generate data tweet snapchat caption comprise short text accompany image social media post often come inconsistent incomplete syntax lexical notations limit surround textual contexts bring significant challenge ner end create new dataset mner call snapcaptions snapchat image caption pair submit public crowd source stories fully annotate name entities build upon state art bi lstm word character base ner model one deep image network incorporate relevant visual context augment textual information two generic modality attention module learn attenuate irrelevant modalities amplify informative ones extract contexts adaptive sample token propose mner model modality attention significantly outperform state art text ner model successfully leverage provide visual contexts open potential applications mner myriads social media platforms
paper describe lidioms data set multilingual rdf representation idioms currently contain five languages english german italian portuguese russian data set intend support natural language process applications provide link idioms across languages underlie data crawl integrate various source ensure quality crawl data idioms evaluate least two native speakers herein present model devise structure data also provide detail link lidioms well know multilingual data set babelnet result data set comply best practice accord linguistic link open data community
generation natural language resource description framework rdf data recently gain significant attention due continuous growth link data number approach generate natural language languages english however work propose generate brazilian portuguese texts rdf address research gap present rdf2pt approach verbalize rdf data brazilian portuguese language evaluate rdf2pt open questionnaire forty-four native speakers divide experts non experts result suggest rdf2pt able generate text similar generate humans hence easily understand
paper present novel deep multimodal framework predict human emotions base sentence level speak language architecture two distinctive characteristics first extract high level feature text audio via hybrid deep multimodal structure consider spatial information text temporal information audio high level associations low level handcraft feature second fuse feature use three layer deep neural network learn correlations across modalities train feature extraction fusion modules together allow optimal global fine tune entire structure evaluate propose framework iemocap dataset result show promise performance achieve six hundred and four weight accuracy five emotion categories
feel emotion critical characteristic distinguish people machine among multi modal resources emotion detection textual datasets contain least additional information addition semantics hence adopt widely test develop systems however textual emotional datasets consist emotion label individual word sentence document make challenge discuss contextual flow emotions paper introduce emotionlines first dataset emotions label utterances dialogue base textual content dialogues emotionlines collect friends tv script private facebook messenger dialogues one seven emotions six ekman basic emotions plus neutral emotion label utterance five amazon mturkers total twenty-nine thousand, two hundred and forty-five utterances two thousand dialogues label emotionlines also provide several strong baselines emotion detection model emotionlines paper
speak language understand system traditionally design pipeline number components first audio signal process automatic speech recognizer transcription n best hypotheses recognition result natural language understand system classify text structure data domain intent slot stream consumers dialog system hand free applications components usually develop optimize independently paper present study end end learn system speak language understand unify approach infer semantic mean directly audio feature without intermediate text representation study show train model achieve reasonable good result demonstrate model capture semantic attention directly audio feature
paper propose study problem court view generation fact description criminal case task aim improve interpretability charge prediction systems help automatic legal document generation formulate task text text natural language generation nlg problem sequenceto sequence model achieve cut edge performances many nlg task however due non distinctions fact descriptions hard seq2seq model generate charge discriminative court view work explore charge label tackle issue propose label condition seq2seq model attention problem decode court view condition encode charge label experimental result show effectiveness method
semantic parse offer many opportunities improve natural language understand present semantically annotate parallel corpus english german italian dutch sentence align scoped mean representations order capture semantics negation modals quantification presupposition trigger semantic formalism base discourse representation theory concepts represent wordnet synsets thematic roles verbnet relations translate scoped mean representations set clauses enable us compare purpose semantic parser evaluation check translations do compute precision recall match clauses similar way do abstract mean representations show match tool evaluate scoped mean representations accurate efficient apply match tool three baseline semantic parsers yield f score forty-three fifty-four pilot study perform automatically find change mean compare mean representations translations comparison turn additional way find annotation mistake ii find instance semantic analysis need improve
single document summarization task produce shorter version document preserve principal information content paper conceptualize extractive summarization sentence rank task propose novel train algorithm globally optimize rouge evaluation metric reinforcement learn objective use algorithm train neural summarization model cnn dailymail datasets demonstrate experimentally outperform state art extractive abstractive systems evaluate automatically humans
automatic speech recognition asr systems often need develop extremely low resource languages serve end use audio content categorization search universal phone recognition natural consider transcribe speech available train asr system language adapt universal phone model use small amount minutes rather hours transcribe speech also need study particularly state art dnn base acoustic model darpa lorelei program provide framework low resource asr study provide extrinsic metric evaluate asr performance humanitarian assistance disaster relief set paper present kaldi base systems program employ universal phone model approach asr describe recipes rapid adaptation universal asr system result obtain significantly outperform result obtain many compete approach nist lorehlt two thousand and seventeen evaluation datasets
describe system semeval two thousand and eighteen share task semantic relation extraction classification scientific paper focus classification task simple piecewise convolution neural encoder perform decently end end manner simple inter task data augmentation signifi cantly boost performance model best perform systems stand 8th twenty team classification task noisy data 12th twenty-eight team classification task clean data
generate plausible fluent sentence desire properties long challenge recent work use recurrent neural network rnns variants predict follow word give previous sequence target label paper propose novel framework generate constrain sentence via gibbs sample candidate sentence revise update iteratively sample new word replace old ones experiment show effectiveness propose method generate plausible diverse sentence
syntactic rule natural language typically need make reference hierarchical sentence structure however simple examples language learners receive often equally compatible linear rule children consistently ignore linear explanations settle instead correct hierarchical one fact motivate proposal learner hypothesis space constrain include hierarchical rule examine proposal use recurrent neural network rnns constrain way simulate acquisition question formation hierarchical transformation fragment english find rnn architectures tend learn hierarchical rule suggest hierarchical cue within language combine implicit architectural bias inherent certain rnns may sufficient induce hierarchical generalizations likelihood acquire hierarchical generalization increase language include additional cue hierarchy form subject verb agreement underscore role cue hierarchy learner input
millions users share experience social media sit twitter turn generate valuable data public health monitor digital epidemiology analyse population health global scale first critical task applications classify whether personal health event mention call phm problem task challenge many reason include typically short length social media post inventive spell lexicons figurative language include hyperbole use diseases like heart attack cancer emphasis health self report problem even challenge rarely report frequent ambiguously express condition stroke address problem propose general robust method detect phms social media call wespad combine lexical syntactic word embed base context base feature wespad able generalize examples automatically distort word embed space effectively detect true health mention unlike previously propose state art supervise deep learn techniques wespad require relatively little train data make possible adapt minimal effort new disease condition evaluate wespad establish publicly available flu detection benchmark new dataset construct mention multiple health condition experiment show wespad outperform baselines state art methods especially case number proportion true health mention train data small
language spread complex mechanism involve issue like culture economics migration population etc paper propose set methods model dynamics spread system model randomness language spread propose batch markov monte carlo simulation migrationbmmcsm algorithm agent treat language stack agent learn languages migrate base propose batch markov property accord transition matrix migration matrix since population play crucial role language spread also introduce mortality fertility mechanism control birth death simulate agents bmmcsm algorithm simulation result bmmcsm show numerical geographic distribution languages vary across time change distribution fit world cultural economic development trend next construct matrix entries directly calculate historical statistics entries unknown thus key success bmmcsm lie accurate estimation transition matrix estimate unknown entries supervision know entries achieve first construct twenty twenty five factor tensor x characterize entry train random forest regressor know entries use train regressor predict unknown entries reason choose random forestrf compare single decision tree conquer problem fit shapiro test also suggest residual rf subject normal distribution
bidirectional lstm blstm rnn base speech synthesis system among best parametric text speech tts systems term naturalness generate speech especially naturalness prosody however model complexity inference cost blstm prevent usage many runtime applications meanwhile deep fee forward sequential memory network dfsmn show consistent performance blstm word error rate wer runtime computation cost speech recognition task since speech synthesis also require model long term dependencies compare speech recognition paper investigate deep fsmn dfsmn speech synthesis objective subjective experiment show compare blstm tts method dfsmn system generate synthesize speech comparable speech quality drastically reduce model complexity speech generation time
paper describe system use task1 affect tweet combine two different approach first one call n stream convnets deep learn approach second one xgboost regresseor base set embed lexicons base feature system evaluate test set task outperform approach arabic version valence intensity regression task valence ordinal classification task
speak language translation slt become widely use become communication tool help cross language barriers one challenge slt translation language without gender agreement language gender agreement english arabic paper introduce approach tackle limitation enable neural machine translation system produce gender aware translation show nmt system model speaker listener gender information produce gender aware translation propose method generate data use adapt nmt system produce gender aware propose approach achieve significant improvement translation quality two bleu point
core part linguistic typology classification languages accord linguistic properties detail world atlas language structure wals manually prohibitively time consume part evidence fact one hundred seven thousand languages speak world fully cover wals learn distribute language representations use predict typological properties massively multilingual scale additionally quantitative qualitative analyse language embeddings tell us language similarities encode nlp model task different typological level representations learn unsupervised manner alongside task three typological level phonology grapheme phoneme prediction phoneme reconstruction morphology morphological inflection syntax part speech tag consider eight hundred languages find significant differences language representations encode depend target task instance although norwegian bokmaal danish typologically close one another phonologically distant reflect language embeddings grow relatively distant phonological task also able predict typological feature wals high accuracies even unseen language families
quality annotate corpus essential especially apply research despite recent focus web science community research cyberbullying community dose still standard benchmarks paper publish first quality annotate corpus second offensive word lexicon capture different type type harassment sexual harassment ii racial harassment iii appearance relate harassment iv intellectual harassment v political harassmentwe crawl data twitter use offensive lexicon rely human judge annotate collect tweet wrt contextual type use offensive word sufficient reliably detect harassment corpus consist twenty-five thousand annotate tweet five contextual type please share novel annotate corpus lexicon research community instruction acquire corpus publish git repository
live blog increasingly popular news format cover break news live events online journalism online news websites around world use medium give readers minute minute update event good summaries enhance value live blog reader often available paper study way collect corpora automatic live blog summarization empirical evaluation use well know state art summarization systems show live blog corpus pose new challenge field summarization make tool publicly available reconstruct corpus encourage research community replicate result
describe algorithm automatic classification idiomatic literal expressions start point word give text segment paragraph highranking representatives common topic discussion less likely part idiomatic expression additional hypothesis contexts idioms occur typically affective therefore incorporate simple analysis intensity emotions express contexts investigate bag word topic representation one three paragraph contain expression classify idiomatic literal target phrase extract topics paragraph contain idioms paragraph contain literals use unsupervised cluster method latent dirichlet allocation lda blei et al two thousand and three since idiomatic expressions exhibit property non compositionality assume usually present different semantics word use local topic treat idioms semantic outliers identification semantic shift outlier detection thus topic representation allow us differentiate idioms literals use local semantic contexts result encourage
automatic abstractive text summarization important challenge research topic natural language process among many widely use languages chinese language special property chinese character contain rich information comparable word exist chinese text summarization methods either adopt totally character base word base representations fail fully exploit information carry representations accurately capture essence article propose hybrid word character approach hwc preserve advantage word base character base representations evaluate advantage propose hwc approach apply two exist methods discover generate state art performance margin twenty-four rouge point widely use dataset lcsts addition find issue contain lcsts dataset offer script remove overlap pair summary short text create clean dataset community propose hwc approach also generate best performance new clean lcsts dataset
text summarization extensively study problem traditional approach text summarization rely heavily feature engineer contrast propose fully data drive approach use feedforward neural network single document summarization train evaluate model standard duc two thousand and two dataset show result comparable state art model propose model scalable able produce summary arbitrarily size document break original document fix size part feed recursively network
present gradient tree boost base structure learn model jointly disambiguate name entities document gradient tree boost widely use machine learn algorithm underlie many top perform natural language process systems surprisingly work limit use gradient tree boost tool regular classification regression problems despite structure nature language best knowledge work first one employ structure gradient tree boost sgtb algorithm collective entity disambiguation define global feature previous disambiguation decisions jointly model local feature system able produce globally optimize entity assignments mention document exact inference prohibitively expensive globally normalize model solve problem propose bidirectional beam search gold path bibsg approximate inference algorithm variant standard beam search algorithm bibsg make use global information past future perform better local search experiment standard benchmark datasets show sgtb significantly improve upon publish result specifically sgtb outperform previous state art neural system near one absolute accuracy popular aida conll dataset
read understand text one important component computer aid diagnosis clinical medicine also major research problem field nlp work introduce question answer task call medqa study answer question clinical medicine use knowledge large scale document collection aim medqa answer real world question large scale read comprehension propose solution seareader modular end end read comprehension model base lstm network dual path attention architecture novel dual path attention model information flow two perspectives ability simultaneously read individual document integrate information across multiple document experiment seareader achieve large increase accuracy medqa compete model additionally develop series novel techniques demonstrate interpretation question answer process seareader
work relation extraction form prediction look short span text within single sentence contain single entity pair mention approach often consider interactions across mention require redundant computation mention pair ignore relationships express across sentence boundaries problems exacerbate document rather sentence level annotation common biological text response propose model simultaneously predict relationships mention pair document form pairwise predictions entire paper abstract use efficient self attention encoder pair mention score allow us perform multi instance learn aggregate mention form entity pair representations adapt settings without mention level annotation jointly train predict name entities add corpus weakly label data experiment two biocreative benchmark datasets achieve state art performance biocreative v chemical disease relation dataset model without external kb resources also introduce new dataset order magnitude larger exist human annotate biological information extraction datasets accurate distantly supervise alternatives
machine translation popular test bed research neural sequence sequence model despite much recent research still lack understand model practitioners report performance degradation large beam estimation rare word lack diversity final translations study relate issue inherent uncertainty task due existence multiple valid translations single source sentence extrinsic uncertainty cause noisy train data propose tool metrics assess uncertainty data capture model distribution affect search strategies generate translations result show search work remarkably well model tend spread much probability mass hypothesis space next propose tool assess model calibration show easily fix shortcomings current model part study release multiple human reference translations two popular benchmarks
complexities arabic language morphology orthography dialects make sentiment analysis arabic challenge also text feature extraction short message like tweet order gauge sentiment make task even difficult recent years deep neural network often employ show good result sentiment classification natural language process applications word embed word distribute approach current powerful tool capture together closest word contextual text paper describe construct word2vec model large arabic corpus obtain ten newspapers different arab countries apply different machine learn algorithms convolutional neural network different text feature selections report improve accuracy sentiment classification ninety-one ninety-five publicly available arabic language health sentiment dataset one
semantic match natural language sentence identify relationship two sentence core research problem underlie many natural language task depend whether train data available prior research propose unsupervised distance base scheme supervise deep learn scheme sentence match however previous approach either omit fail fully utilize order hierarchical flexible structure language object well interactions paper propose hierarchical sentence factorization technique factorize sentence hierarchical representation components different scale reorder predicate argument form propose sentence factorization technique lead invention one new unsupervised distance metric calculate semantic distance pair text snippets solve penalize optimal transport problem preserve logical relationship word reorder sentence two new multi scale deep learn model supervise semantic train base factorize sentence hierarchies apply techniques text pair similarity estimation text pair relationship classification task base multiple datasets stsbenchmark microsoft research paraphrase identification msrp dataset sick dataset etc extensive experiment show propose hierarchical sentence factorization use significantly improve performance exist unsupervised distance base metrics well multiple supervise deep learn model base convolutional neural network cnn long short term memory lstm
paper describe xnmt extensible neural machine translation toolkit xnmt distin guishes open source nmt toolkits focus modular code design purpose enable fast iteration research replicable reliable result paper describe design xnmt experiment configuration system demonstrate utility task machine translation speech recognition multi task machine translation parse xnmt available open source https githubcom neulab xnmt
paper describe system semeval two thousand and eighteen task eleven machine comprehension use commonsense knowledge use three way attentive network trian model interactions passage question answer incorporate commonsense knowledge augment input relation embed graph general knowledge conceptnet speer et al two thousand and seventeen result system achieve state art performance eight thousand, three hundred and ninety-five accuracy official test data code publicly available https githubcom intfloat commonsense rc
monolingual data demonstrate helpful improve translation quality statistical machine translation smt systems neural machine translation nmt systems especially resource poor domain adaptation task parallel data rich enough paper propose novel approach better leverage monolingual data neural machine translation jointly learn source target target source nmt model language pair joint optimization method train process start two initial nmt model pre train parallel data direction two model iteratively update incrementally decrease translation losses train data iteration step nmt model first use translate monolingual data one language form pseudo train data nmt model two new nmt model learn parallel data together pseudo train data nmt model expect improve better pseudo train data generate next step experiment result chinese english english german translation task show approach simultaneously improve translation quality source target target source model significantly outperform strong baseline systems enhance monolingual data model train include back translation
research multilingual speech emotion recognition face problem available speech corpora differ important ways annotation methods interaction scenarios inconsistencies complicate build multilingual system present result cross lingual multilingual emotion recognition english french speech data similar characteristics term interaction human human conversations explore possibility fine tune pre train cross lingual model small number sample target language great interest low resource languages gain insights learn deploy convolutional neural network perform analysis attention mechanism inside network
paper describe development end end factoid question answer system vietnamese language system combine statistical model ontology base methods chain process modules provide high quality mappings natural language text entities present challenge development intelligent user interface isolate language like vietnamese show techniques develop inflectional languages apply question answer system answer wide range general knowledge question promise accuracy test set
recent work propose neural model dialog act classification speak dialogs however explore role usefulness acoustic information propose neural model process lexical acoustic feature classification result two benchmark datasets reveal acoustic feature helpful improve overall accuracy finally deeper analysis show acoustic feature valuable three case dialog act sufficient data lexical information limit strong lexical cue present
demorphy morphological analyzer german build onto large compactified lexicons german morphological dictionary guesser base german declension suffix also provide german provide state art morphological analyzer demorphy implement python ease usability accompany documentation package suitable academic commercial purpose wit permissive licence
historically natural language process area give much attention many researchers one main motivation beyond interest relate word prediction problem state give set word sentence one recommend next word literature problem solve methods base syntactic semantic analysis solely analysis achieve practical result end user applications instance latent semantic analysis handle semantic feature text suggest word consider syntactical rule hand model treat methods together achieve state art result eg deep learn model demand high computational effort make model infeasible certain type applications advance technology mathematical model possible develop faster systems accuracy work propose hybrid word suggestion model base naive bay latent semantic analysis consider neighbour word around unfilled gap result show model could achieve four hundred and forty-two accuracy msr sentence completion challenge
end end e2e automatic speech recognition asr systems directly map acoustics word use unify model previous work mostly focus e2e train single model integrate acoustic language model whole although e2e train benefit sequence model simplify decode pipelines large amount transcribe acoustic data usually require traditional acoustic language model techniques utilize paper novel modular train framework e2e asr propose separately train neural acoustic language model train stage still perform end end inference decode stage acoustics phoneme model a2p phoneme word model p2w train use acoustic data text data respectively phone synchronous decode psd module insert a2p p2w reduce sequence lengths without precision loss finally modules integrate acousticsto word model a2w jointly optimize use acoustic data retain advantage sequence model experiment three hundred hour switchboard task show significant improvement direct a2w model efficiency train decode also benefit propose method
identify implicit discourse relations text span challenge task require understand mean text tackle task recent study try several deep learn methods exploit syntactic information work explore idea incorporate syntactic parse tree neural network specifically employ tree lstm model tree gru model base tree structure encode arguments relation moreover leverage constituent tag control semantic composition process tree structure neural network experimental result show method achieve state art performance pdtb corpus
unsupervised learn representations polysemous word generate large pseudo multi sense since unsupervised methods overly sensitive contextual variations paper address pseudo multi sense detection word embeddings dimensionality reduction sense pair propose novel principal analysis method term ex rpca design detect pseudo multi sense real multi sense ex rpca empirically show pseudo multi sense generate systematically unsupervised method moreover multi sense word embeddings improve simple linear transformation base ex rpca improve word embed outperform original one fifty-six point stanford contextual word similarity scws dataset hope simple yet effective approach help linguistic analysis multi sense word embeddings future
comprehend mean natural language primary objective natural language process nlp text comprehension cornerstone achieve objective upon problems like chat bots language translation others achieve report summary attentive reader design better emulate human read process along dictiontary base solution regard vocabulary oov word data generate answer base machine comprehension read passages question squad benchmark implementation feature two popular model match lstm dynamic coattention able reach close match result obtain humans
average word embeddings common baseline sophisticate sentence embed techniques however typically fall short performances complex model infersent generalize concept average word embeddings power mean word embeddings show concatenation different type power mean word embeddings considerably close gap state art methods monolingually substantially outperform complex techniques cross lingually addition propose method outperform different recently propose baselines sif sent2vec solid margin thus constitute much harder beat monolingual baseline data code publicly available
chinese language evolve lot long term development therefore native speakers trouble read sentence write ancient chinese paper propose build end end neural model automatically translate ancient contemporary chinese however exist ancient contemporary chinese parallel corpora align sentence level sentence align corpora limit make difficult train model build sentence level parallel train data model propose unsupervised algorithm construct sentence align ancient contemporary pair use fact align sentence pair share many tokens base align corpus propose end end neural model copy mechanism local attention translate ancient contemporary chinese experiment show propose unsupervised algorithm achieve nine hundred and ninety-four f1 score sentence alignment translation model achieve two thousand, six hundred and ninety-five bleu ancient contemporary three thousand, six hundred and thirty-four bleu contemporary ancient
work introduce task open type relation argument extraction orae give corpus query entity q knowledge base relation egq author notable work title x model extract argument non standard entity type entities extract standard name entity tagger eg x title book work art corpus distantly supervise dataset base wikidata relations obtain release address task develop compare wide range neural model task yield large improvements strong baseline obtain neural question answer system impact different sentence encode architectures answer extraction methods systematically compare encoder base gate recurrent units combine conditional random field tagger give best result
rely entirely attention mechanism transformer introduce vaswani et al two thousand and seventeen achieve state art result machine translation contrast recurrent convolutional neural network explicitly model relative absolute position information structure instead require add representations absolute position input work present alternative approach extend self attention mechanism efficiently consider representations relative position distance sequence elements wmt two thousand and fourteen english german english french translation task approach yield improvements thirteen bleu three bleu absolute position representations respectively notably observe combine relative absolute position representations yield improvement translation quality describe efficient implementation method cast instance relation aware self attention mechanisms generalize arbitrary graph label input
clinical note often describe important aspects patient stay therefore critical medical research clinical concept extraction cce name entities problems test treatments aid form understand note provide foundation many downstream clinical decision make task historically task pose standard name entity recognition ner sequence tag problem solve feature base methods use handengineered domain knowledge recent advance however demonstrate efficacy lstm base model ner task include cce work present cliner twenty simple install open source tool extract concepts clinical text cliner twenty use word character level lstm model achieve state art performance ease use tool also include pre train model available public use
recently advancements deep learn allow development end end train goal orient dialog systems although systems already achieve good performance simplifications limit usage real life scenarios work address two limitations ignore positional information fix number possible response candidates propose use positional encode input model word order user utterances furthermore use feedforward neural network able generate output word word longer restrict fix number possible response candidates use positional encode able achieve better accuracies dialog babi task use feedforward neural network generate response able save computation time space consumption
emojis small image commonly include social media text message combination visual textual content message build modern way communication automatic systems use deal paper extend recent advance emoji prediction put forward multimodal approach able predict emojis instagram post instagram post compose picture together texts sometimes include emojis show emojis predict use text also use picture main find incorporate two synergistic modalities combine model improve accuracy emoji prediction task result demonstrate two modalities text image encode different information use emojis therefore complement
paper present question answer system use neural network interpret question learn dbpedia repository train sequence sequence neural network model n triple extract dbpedia infobox properties since properties represent natural language use question answer dialogues movie subtitle although automatic evaluation show low overlap generate answer compare gold standard set manual inspection show promise outcomes experiment work
recent advance neural model natural language process automatic generation classical chinese poetry draw significant attention due artistic cultural value previous work mainly focus generate poetry give keywords text information visual inspirations poetry rarely explore generate poetry image much challenge generate poetry text since image contain rich visual information describe completely use several keywords good poem convey image accurately paper propose memory base neural model exploit image generate poems specifically encoder decoder model topic memory network propose generate classical chinese poetry image best knowledge first work attempt generate classical chinese poetry image neural network comprehensive experimental investigation human evaluation quantitative analysis demonstrate propose model generate poems convey image accurately
community question answer cqa forums popular nowadays represent effective mean communities around particular topics share information unfortunately information always factual thus explore new dimension context cqa ignore far check veracity answer particular question cqa forums new problem create specialize dataset propose novel multi faceted model capture information answer content say author profile say rest community forum say external authoritative source information external support evaluation result show map value eight thousand, six hundred and fifty-four twenty-one point absolute baseline
task fine grain entity type classification fetc consist assign type hierarchy entity mention text exist methods rely distant supervision thus susceptible noisy label context overly specific train sentence previous methods attempt address issue heuristics help hand craft feature instead propose end end solution neural network model use variant cross entropy loss function handle context label hierarchical loss normalization cope overly specific ones also previous work solve fetc multi label classification follow ad hoc post process contrast solution elegant use public word embeddings train single label jointly learn representations entity mention context show experimentally approach robust noise consistently outperform state art establish benchmarks task
question retrieval crucial subtask community question answer previous research focus supervise model depend heavily train data manual feature engineer paper propose novel unsupervised framework namely reduce attentive match network ramn compute semantic match two question ramn integrate together deep semantic representations shallow lexical mismatch information initial rank produce external search engine first time propose attention autoencoders generate semantic representations question addition employ lexical mismatch capture surface match two question derive importance word question conduct experiment open cqa datasets semeval two thousand and sixteen semeval two thousand and seventeen experimental result show unsupervised model obtain comparable performance state art supervise methods semeval two thousand and sixteen task three outperform best system semeval two thousand and seventeen task three wide margin
recent work show recurrent neural network rnns implicitly capture exploit hierarchical information train solve common natural language process task language model linzen et al two thousand and sixteen neural machine translation shi et al two thousand and sixteen contrast ability model structure data non recurrent neural network receive little attention despite success many nlp task gehring et al two thousand and seventeen vaswani et al two thousand and seventeen work compare two architectures recurrent versus non recurrent respect ability model hierarchical structure find recurrency indeed important purpose
recent years increase propagation hate speech social media urgent need effective counter measure draw significant investment governments company researchers large number methods develop automate hate speech detection online aim classify textual content non hate hate speech case method may also identify target characteristics ie type hate race religion hate speech however notice significant difference performance two ie non hate vs hate work argue focus latter problem practical reason show much challenge task analysis language typical datasets show hate speech lack unique discriminative feature therefore find long tail dataset difficult discover propose deep neural network structure serve feature extractors particularly effective capture semantics hate speech methods evaluate largest collection hate speech datasets base twitter show able outperform best perform method five percentage point macro average f1 eight percentage point challenge case identify hateful content
cryptocurrencies digital tokens digital currencies eg btc eth xrp neo rapidly gain grind use value understand among public bring astonish profit investors unlike money bank systems digital tokens require central authorities decentralize pose significant challenge credit rat icos currently subject government regulations make reliable credit rat system ico project necessary urgent paper introduce icorating first learn base cryptocurrency rat system exploit natural language process techniques analyze various aspects two thousand, two hundred and fifty-one digital currencies date white paper content found team github repositories websites etc supervise learn model use correlate life span price change cryptocurrencies feature best set propose system able identify scam ico project eighty-three precision hope work help investors identify scam icos attract efforts automatically evaluate analyze ico project
completely amaze fake news click bait totally invade cyber space let us face everybody hat three simple reason reason two absolutely amaze achieve time election completely blow mind agree go know somebody stop research fake news click bait detection trust us totally great research really make mistake best research ever seriously come look neural network attention mechanism sentiment lexicons author profile name lexical feature semantic feature absolutely totally test trust us result number really big number best number ever oh analysis absolutely top notch analysis interest come read shock truth fake news click bait bulgarian cyber space believe find
language identification social media text still remain challenge task due properties like code mix inconsistent phonetic transliterations paper present supervise learn approach language identification word level low resource bengali english code mix data take social media employ two methods word encode namely character base root phone base train deep lstm model utilize two model create two ensemble model use stack threshold technique give nine thousand, one hundred and seventy-eight nine thousand, two hundred and thirty-five accuracies respectively test data
rao two thousand and thirteen report raise phenomenon xan x consonant vowel chengdu dialect mandarin ie realize epsilon young speakers ae older speakers offer acoustic analysis design acoustic study examine realization xan speakers different age old vs young gender male vs female group x represent three condition one unaspirated consonants c p k two aspirate consonants ch ph th kh three high vowels v seventeen native speakers ask read xan character f1 value extract comparison result confirm raise effect rao two thousand and thirteen ie young speakers realize epsilon whereas older speakers part realize ae also female speakers raise male speakers within age group interestingly within van condition older speakers raise ian yan interpret first assimilate precede front high vowels older speakers become phonologized younger speakers condition include chan show possible trajectory ongoing sound change chengdu dialect
contextual influence language often exhibit substantial cross lingual regularities example verbose situations require finer distinctions however regularities sometimes obscure semantic syntactic differences use newly collect dataset color reference game mandarin chinese release public confirm variety constructions display sensitivity contextual difficulty chinese english show neural speaker agent train bilingual data simple multitask learn approach display human like pattern context dependence pragmatically informative monolingual chinese counterpart moreover expense language specific semantic understand result speaker model learn different basic color term systems english chinese noteworthy cross lingual influence identify synonyms two languages use vector analogy operations output layer despite exposure parallel data
analysis informative content sentiments social users attempt quite intensively recent past systems usable monolingual data fail give poor result use data code mix property gather attention encourage researchers work crisis prepare gold standard bengali english code mix data language polarity tag sentiment analysis purpose paper discuss systems prepare collect filter raw twitter data order reduce manual work annotation hybrid systems combine rule base supervise model develop language sentiment tag final corpus annotate group annotators follow guidelines gold standard corpus thus obtain impressive inter annotator agreement obtain term kappa value various metrics like code mix index cmi code mix factor cf along various aspects language emotion also qualitatively poll code mix sentiment properties corpus
language model difficult incorporate entity relationships knowledge base one solution use reranker train global feature global feature derive n best list however train reranker require manually annotate n best list expensive obtain propose method base contrastive estimation method alleviate need data experiment music domain demonstrate global feature well feature extract external knowledge base incorporate reranker final model simple ensemble language model reranker achieve forty-four absolute word error rate improvement lstm language model blind test data
semantic parse process map natural language sentence formal representation mean work use neural network approach transform natural language sentence query ontology database sparql language method rely handcraft rule high quality lexicons manually build templates handmade complex structure approach base vector space model neural network propose model base two learn step first step generate vector representation sentence natural language sparql query second step use vector representation input neural network lstm attention mechanism generate model able encode natural language decode sparql
paper present feature base name entity recognition ner model achieve start art accuracy vietnamese language combine word word shape feature pos chunk brown cluster base feature word embed base feature conditional random field crf model also explore effect word segmentation pos tag chunk result many popular vietnamese nlp toolkits accuracy propose feature base ner model work first work systematically perform extrinsic evaluation basic vietnamese nlp toolkits downstream ner task experimental result show automatically generate word segmentation useful pos chunk information generate vietnamese nlp tool show benefit propose feature base ner model
hateful comment swearwords sometimes even death threats become reality many people today online environments especially true journalists politicians artists public figure paper describe hate direct towards individuals measure online environments use simple dictionary base approach present case study swedish politicians use examples study discuss shortcomings propose dictionary base approach also outline possibilities potential refinements propose approach
current model word representationsegglove successfully capture fine grain semantics however semantic similarity exhibit word embeddings suitable resolve bridge anaphora require knowledge associative similarity ie relatedness instead semantic similarity information synonyms hypernyms create word embeddings embeddingspp capture relatedness explore syntactic structure noun phrase demonstrate use embeddingspp alone achieve around thirty accuracy bridge anaphora resolution isnotes corpus furthermore achieve substantial gain state art system hou et al two thousand and thirteen bridge antecedent selection
work propose new language model paradigm ability perform prediction moderation information flow multiple granularities neural lattice language model model construct lattice possible paths sentence marginalize across lattice calculate sequence probabilities optimize parameters approach allow us seamlessly incorporate linguistic intuitions include polysemy existence multi word lexical items language model experiment multiple language model task show english neural lattice language model utilize polysemous embeddings able improve perplexity nine hundred and ninety-five relative word level baseline chinese model handle multi character tokens able improve perplexity two thousand and ninety-four relative character level baseline
introduce large dataset narrative texts question texts intend use machine comprehension task require reason use commonsense knowledge dataset complement similar datasets focus stories everyday activities go movies work garden question require commonsense knowledge specifically script knowledge answer show mode data collection via crowdsourcing result substantial amount inference question dataset form basis share task commonsense script knowledge organize semeval two thousand and eighteen provide challenge test case broader natural language understand community
paper introduce new publicly available dataset verification textual source fever fact extraction verification consist one hundred and eighty-five thousand, four hundred and forty-five claim generate alter sentence extract wikipedia subsequently verify without knowledge sentence derive claim classify support refute notenoughinfo annotators achieve six thousand, eight hundred and forty-one fleiss kappa first two class annotators also record sentence form necessary evidence judgment characterize challenge dataset present develop pipeline approach compare suitably design oracles best accuracy achieve label claim accompany correct evidence three thousand, one hundred and eighty-seven ignore evidence achieve five thousand and ninety-one thus believe fever challenge testbed help stimulate progress claim verification textual source
introduce senteval toolkit evaluate quality universal sentence representations senteval encompass variety task include binary multi class classification natural language inference sentence similarity set task select base appear community consensus regard appropriate evaluations universal sentence representations toolkit come script download preprocess datasets easy interface evaluate sentence encoders aim provide fairer less cumbersome centralize way evaluate sentence representations
study approach problem distinguish general profanity hate speech social media something widely consider use new dataset annotate specifically task employ supervise classification along set feature include n grams skip grams cluster base word representations apply approach base single classifiers well advance ensemble classifiers stack generalization achieve best result eighty accuracy three class classification task analysis result reveal discriminate hate speech profanity simple task may require feature capture deeper understand text always possible surface n grams variability gold label annotate data due differences subjective adjudications annotators also issue directions future work discuss
story cloze test system present four sentence prompt story must determine one two potential end right end story previous work show ignore train set train model validation set achieve high accuracy task due stylistic differences story end train set validation test set follow approach present simpler fully neural approach story cloze test use skip think embeddings stories fee forward network achieve close state art performance task without feature engineer also find consider last sentence prompt instead whole prompt yield higher accuracy approach
study propose advance neural speech recognition directly incorporate attention model within connectionist temporal classification ctc framework particular derive new context vectors use time convolution feature model attention part ctc network improve attention model utilize content information extract network represent implicit language model finally introduce vector base attention weight apply context vectors across time individual components evaluate system three thousand, four hundred hours microsoft cortana voice assistant task demonstrate propose model consistently outperform baseline model achieve twenty relative reduction word error rat
acoustic word model base connectionist temporal classification ctc criterion show natural end end e2e model directly target word output units however word base ctc model suffer vocabulary oov issue model limit number word output layer map remain word oov output node hence word base ctc model recognize frequent word model network output nod first attempt improve acoustic word model hybrid ctc model consult letter base ctc word base ctc model emit oov tokens test time propose much better solution train mix unit ctc model decompose oov word sequence frequent word multi letter units evaluate three thousand, four hundred hours microsoft cortana voice assistant task final acoustic word solution improve baseline word base ctc relative one thousand, two hundred and nine word error rate wer reduction combine propose attention ctc e2e model without use language model lm complex decoder outperform traditional context dependent phoneme ctc strong lm decoder relative six hundred and seventy-nine
machine translation make rapid advance recent years millions people use today online translation systems mobile applications order communicate across language barriers question naturally arise whether systems approach achieve parity human translations paper first address problem define accurately measure human parity translation describe microsoft machine translation system measure quality translations widely use wmt two thousand and seventeen news translation task chinese english find latest neural machine translation system reach new state art translation quality human parity compare professional human translations also find significantly exceed quality crowd source non professional translations
word vectors require significant amount memory storage pose issue resource limit devices like mobile phone gpus show high quality quantize word vectors use one two bits per parameter learn introduce quantization function word2vec furthermore show train quantization function act regularizer train word vectors english wikipedia two thousand and seventeen evaluate standard word similarity analogy task question answer squad quantize word vectors take eight 16x less space full precision thirty-two bite word vectors also outperform word similarity task question answer
paper describe system get state art result semeval two thousand and eighteen task eleven machine comprehension use commonsense knowledge paper present neural network call hybrid multi aspects hma model mimic human intuitions deal multiple choice read comprehension model aim produce predictions multiple aspects calculate attention among text question choices combine result final predictions experimental result show hma model could give substantial improvements baseline system get first place final test set leaderboard accuracy eight thousand, four hundred and thirteen
relation classification important semantic process task field natural language process paper propose task relation classification chinese literature text new dataset chinese literature text construct facilitate study task present novel model name structure regularize bidirectional recurrent convolutional neural network sr brcnn identify relation entities propose model learn relation representations along shortest dependency path sdp extract structure regularize dependency tree benefit reduce complexity whole model experimental result show propose method significantly improve f1 score one hundred and three outperform state art approach chinese literature text
paper describe result first share task word sense induction wsi russian language similar share task conduct past romance germanic languages explore performance sense induction disambiguation methods slavic language share many feature slavic languages rich morphology virtually free word order participants ask group contexts give word accordance sense provide beforehand instance give word bank set contexts word eg bank financial institution accept deposit river bank slope beside body water participant ask cluster contexts unknown advance number cluster correspond case company area sense word bank purpose evaluation campaign develop three new evaluation datasets base sense inventory different sense granularity contexts datasets sample texts wikipedia academic corpus russian explanatory dictionary russian overall eighteen team participate competition submit three hundred and eighty-three model multiple team manage substantially outperform competitive state art baselines previous years base sense embeddings
paper give overview russian semantic similarity evaluation russe share task hold conjunction dialogue two thousand and fifteen conference exist lot comparative study semantic similarity yet analysis measure ever perform russian language explore problem russian language even interest language feature rich morphology free word order make significantly different english german well study languages attempt bridge gap propose share task semantic similarity russian nouns key contribution evaluation methodology base four novel benchmark datasets russian language analysis one hundred and five submissions nineteen team reveal successful approach english distributional skip gram model directly applicable russian well one hand best result contest obtain sophisticate supervise model combine evidence different source hand completely unsupervised approach skip gram model estimate large scale corpus able score among top five systems
introduce new lexical resource enrich framester knowledge graph link framnet wordnet verbnet resources semantic feature text corpora feature extract distributionally induce sense inventory subsequently link manually construct frame representations boost performance frame disambiguation context since framester frame base knowledge graph enable full fledge owl query reason resource pave way development novel deeper semantic aware applications could benefit combination knowledge text complex symbolic representations events participants together resource also provide software develop evaluation task word frame disambiguation wfd
human evaluation natural language generation nlg often suffer inconsistent user rat previous research tend attribute problem individual user preferences show quality human judgements also improve experimental design present novel rank base magnitude estimation method rankme combine use continuous scale relative assessments show rankme significantly improve reliability consistency human rat compare traditional evaluation methods addition show possible evaluate nlg systems accord multiple distinct criteria important error analysis finally demonstrate rankme combination bayesian estimation system quality cost effective alternative rank multiple nlg systems
emotions widely affect human decision make fact take account affective compute goal tailor decision support emotional state individuals however accurate recognition emotions within narrative document present challenge undertake due complexity ambiguity language performance improvements achieve deep learn yet demonstrate paper specific nature task require customization recurrent neural network regard bidirectional process dropout layer mean regularization weight loss function addition propose sent2affect tailor form transfer learn affective compute network pre train different task ie sentiment analysis output layer subsequently tune task emotion recognition result performance evaluate holistic set across six benchmark datasets find recurrent neural network transfer learn consistently outperform traditional machine learn altogether find considerable implications use affective compute
propose two model special case authorship verification problem task investigate whether two document give pair write author consider authorship verification problem small large scale datasets underlie small scale problem two main challenge first author document unknown us previous write sample available second two document short hundred thousand word may differ considerably genre topic solve propose transformation encoder transform one document pair document transformation generate loss use recognizable feature verify author pair identical large scale problem various author engage examples available larger length parallel recurrent neural network propose compare language model two document evaluate methods various type datasets include authorship identification datasets pan competition amazon review machine learn article experiment show methods achieve stable competitive performance compare baselines
style transfer task automatically transform piece text one particular style another major barrier progress field lack train evaluation datasets well benchmarks automatic metrics work create largest corpus particular stylistic transfer formality show techniques machine translation community serve strong baselines future work also discuss challenge use automatic metrics
sentiment analysis essential many real world applications stance detection review analysis recommendation system sentiment analysis become difficult data noisy collect social media india multilingual country people use one languages communicate within switch languages call code switch code mix depend upon type mix paper present overview share task sentiment analysis code mix data pair hindi english bengali english collect different social media platform paper describe task dataset evaluation baseline participant systems
previous work show possible improve speech recognition learn acoustic feature pair acoustic articulatory data example use canonical correlation analysis cca deep extensions one limitation prior work learn feature model difficult port new datasets domains articulatory data available speech corpora work study problem acoustic feature learn set access external domain mismatch dataset pair speech articulatory measurements either without label develop methods acoustic feature learn settings base deep variational cca extensions use source target domain data label use approach improve phonetic recognition accuracies timit wall street journal analyze number design choices
traditional approach semantic parse sp work train individual model available parallel dataset text mean pair paper explore idea polyglot semantic translation learn semantic parse model train multiple datasets natural languages particular focus translate text code signature representations use software component datasets richardson kuhn 2017ab advantage model use parse wide variety input natural languages output program languages mix input languages use single unify model facilitate model type develop novel graph base decode framework achieve state art performance datasets apply method two benchmark sp task
attention base neural abstractive summarization systems equip copy mechanisms show promise result despite success notice system generate summary mostly entirely copy phrase sentence sometimes multiple consecutive sentence input paragraph effectively perform extractive summarization paper verify behavior use latest neural abstractive summarization system pointer generator network propose simple baseline method allow us control amount copy without retrain experiment indicate method provide strong baseline abstractive systems look obtain high rouge score minimize overlap source article substantially reduce n gram overlap original article keep within two point original model rouge score
wikipedia exist two hundred and eighty-seven languages content unevenly distribute among work investigate generation open domain wikipedia summaries underserved languages use structure data wikidata end propose neural network architecture equip copy action learn generate single sentence comprehensible textual summaries wikidata triple demonstrate effectiveness propose approach evaluate set baselines two languages different natures arabic morphological rich language larger vocabulary english esperanto construct language know easy acquisition
write read dynamic process author compose text sequence word produce sequence one author hop cause revisitation certain thoughts ideas others process composition revisitation readers order time mean text investigate lens dynamical systems common technique analyze behavior dynamical systems know recurrence quantification analysis rqa use method analyze sequential structure text rqa treat text sequential measurement much like time series thus see kind dynamic natural language process nlp extension several benefit part suite time series analysis tool many measure extract one common framework secondly measure close relationship commonly use measure natural language process finally use recurrence analysis offer opportunity expand analysis text develop theoretical descriptions derive complex dynamic systems showcase example analysis eight thousand texts gutenberg project compare well know nlp approach describe r package crqanlp use conjunction r library crqa
sgnmt decode platform machine translation allow par various modern neural model translation different kinds constraints symbolic model paper describe three use case sgnmt currently play active role one teach sgnmt use course work student theses mphil machine learn speech language technology university cambridge two research research work cambridge mt group base sgnmt three technology transfer show sgnmt help transfer research find laboratory industry eg product sdl plc
train model automatic correction machine translate text usually rely data consist source mt human post edit triplets provide source sentence examples translation errors correspond corrections make human post editor ideally large amount data kind allow model learn reliable correction pattern effectively apply test stage unseen source mt pair practice however limit availability call solutions also integrate train process source knowledge along direction state art result recently achieve systems addition limit amount available train data exploit artificial corpora approximate elements gold train instance automatic translations follow idea present escape largest freely available synthetic corpus automatic post edit release far escape consist millions entries mt element train triplets obtain translate source side publicly available parallel corpora use target side artificial human post edit translations obtain phrase base neural model mt paradigm escape contain seventy-two million triplets english german thirty-three millions english italian result total one hundred and forty-four sixty-six million instance respectively usefulness escape prove experiment general domain scenario challenge one automatic post edit language directions model train artificial data always improve mt quality statistically significant gain current version escape freely download http hltsharefbkeu qt21 escapehtml
paper present ongoing work produce expressive tts reader use text dialogue applications system call sparsar use read english poetry far apply text text fully analyze phonetic phonological level syntactic semantic level addition system access restrict list typical pragmatically mark phrase expressions use convey specific discourse function speech act need specialize intonational contour text transform poem like structure line correspond breath group semantically syntactically consistent stanzas correspond paragraph boundaries analogical parameters relate tobi theoretical indices number double paper concentrate short stories fables
paper present kernel base learn approach two thousand and eighteen complex word identification cwi share task approach base combine multiple low level feature character n grams high level semantic feature either automatically learn use word embeddings extract lexical knowledge base namely wordnet feature extraction employ kernel method learn phase feature matrix first transform normalize kernel matrix binary classification task simple versus complex employ support vector machine regression task predict complexity level word word complex label complex annotators employ v support vector regression apply approach three english data set contain document wikipedia wikinews news domains best result competition third place english wikipedia data set however paper also report better post competition result
paper describe allennlp platform research deep learn methods natural language understand allennlp design support researchers want build novel language understand model quickly easily build top pytorch allow dynamic computation graph provide one flexible data api handle intelligent batch pad two high level abstractions common operations work text three modular extensible experiment framework make good science easy also include reference implementations high quality approach core semantic problems eg semantic role label palmer et al two thousand and five language understand applications eg machine comprehension rajpurkar et al two thousand and sixteen allennlp ongoing open source effort maintain engineer researchers allen institute artificial intelligence
paper describe infosys participation 2nd social media mine health applications share task amia two thousand and seventeen task two mine social media message health drug relate information receive significant interest pharmacovigilance research task target develop automate classification model identify tweet contain descriptions personal intake medicine towards objective train stack ensemble shallow convolutional neural network cnn model annotate dataset provide organizers use random search tune hyper parameters cnn submit ensemble best model prediction task system secure first place among nine team micro average f score six hundred and ninety-three
sentiment analysis key component various text mine applications numerous sentiment classification techniques include conventional deep learn base methods propose literature exist methods high quality train set assume give nevertheless construct high quality train set consist highly accurate label challenge real applications difficulty stem fact text sample usually contain complex sentiment representations annotation subjective address challenge study leverage new label strategy utilize two level long short term memory network construct sentiment classifier lexical cue useful sentiment analysis utilize conventional study example polar privative word play important roles sentiment analysis new encode strategy rho hot encode propose alleviate drawbacks one hot encode thus effectively incorporate useful lexical cue compile three chinese data set basis label strategy propose methodology experiment three data set demonstrate propose method outperform state art algorithms
automatic interpretation relation constituents noun compound eg olive oil source baby oil purpose important task many nlp applications recent approach typically base either noun compound representations paraphrase former initially show promise result recent work suggest success stem memorize single prototypical word relation explore neural paraphrase approach demonstrate superior performance memorization possible
machine translation mt deploy range use case millions people daily basis therefore doubt utility mt however everyone convince mt useful especially productivity enhancer human translators chapter address issue describe mt currently deploy output evaluate could enhance especially mt quality improve central issue acceptance longer single gold standard measure quality situation mt deploy need bear mind especially respect expect shelf life translation
report describe participant name entity recognition system vlsp two thousand and eighteen evaluation campaign formalize task sequence label problem use bio encode scheme apply feature base model combine word word shape feature brown cluster base feature word embed base feature compare several methods deal nest entities dataset show combine tag entities level train sequence label model joint tag model improve accuracy nest name entity recognition
unsupervised vector representations sentence document major build block many language task sentiment classification however current methods uninterpretable slow require large train datasets recent word vector base proposals implicitly assume distance word embed space equally important regardless context introduce contextual salience cosal measure word importance use distribution context vectors normalize distance weight cosal rely insight unusual word vectors disproportionately affect phrase vectors bag word model cosal base weight produce accurate unsupervised sentence document representations classification require little computation evaluate single covariance calculation train cosal support small contexts context word outperform skipthought benchmarks beat tf idf benchmarks competitive unsupervised state art
sentiment analysis become establish field nlp community research languages english hinder lack resources although much research multi lingual cross lingual sentiment analysis focus unsupervised semi supervise approach still require large number resources reach performance supervise approach mind introduce two datasets supervise aspect level sentiment analysis basque catalan resourced languages provide high quality annotations benchmarks hope useful grow community researchers work languages
chapter describe question answer system win system human computer question answer hcqa competition thirty first annual conference neural information process systems nip competition require participants address factoid question answer task refer quiz bowl address task use two novel neural network model combine model conventional information retrieval model use supervise machine learn model system achieve best performance among systems submit competition match six top human quiz experts wide margin
stance detection subproblem sentiment analysis stance author piece natural language text particular target either explicitly state text explore stance output usually give favor neither paper target stance detection sport relate tweet present performance result svm base stance classifiers tweet first describe three versions proprietary tweet data set annotate stance information make publicly available research purpose next evaluate svm classifiers use different feature set stance detection data set employ feature base unigrams bigrams hashtags external link emoticons lastly name entities result indicate joint use feature base unigrams hashtags name entities svm classifiers plausible approach stance detection problem sport relate tweet
paper propose novel deep neural network architecture speech2vec learn fix length vector representations audio segment excise speech corpus vectors contain semantic information pertain underlie speak word close vectors embed space correspond underlie speak word semantically similar propose model view speech version word2vec design base rnn encoder decoder framework borrow methodology skipgrams continuous bag word train learn word embeddings directly speech enable speech2vec make use semantic information carry speech exist plain text learn word embeddings evaluate analyze thirteen widely use word similarity benchmarks outperform word embeddings learn word2vec transcriptions
recently propose data collection frameworks endanger language documentation aim collect speech language interest also collect translations high resource language render collect resource interpretable focus scenario explore whether improve transcription quality extremely low resource settings assistance text translations present neural multi source model evaluate several variations three low resource datasets find multi source model share attention outperform baselines reduce transcription character error rate one hundred and twenty-three
word embeddings commonly use start point many nlp model achieve state art performances however large vocabulary many dimension float point representations expensive term memory calculations make unsuitable use low resource devices method propose paper transform real value embeddings binary embeddings preserve semantic information require one hundred and twenty-eight two hundred and fifty-six bits vector lead small memory footprint fast vector operations model base autoencoder architecture also allow reconstruct original vectors binary ones experimental result semantic similarity text classification sentiment analysis task show binarization word embeddings lead loss two accuracy vector size reduce ninety-seven furthermore top k benchmark demonstrate use binary vectors thirty time faster use real value vectors
knowledge base question answer rely availability facts majority find structure source eg wikipedia info box wikidata one major components extract facts unstructured text relation extraction paper propose novel method create distant weak supervision label train large scale system also provide new evidence effectiveness neural network approach decouple model architecture feature design state art neural network system surprisingly much simpler classifier train similar feature perform par highly complex neural network system 75x reduction train time suggest feature bigger contributor final performance
entry introduce topic machine learn provide overview relevance apply linguistics language learn discussion focus give introduction methods applications machine learn apply linguistics provide reference study
speech text translation many potential applications low resource languages typical approach cascade speech recognition machine translation often impossible since transcripts need train speech recognizer usually available low resource languages recent work find neural encoder decoder model learn directly translate foreign speech high resource scenarios without need intermediate transcription investigate whether approach also work settings data computation limit make approach efficient make several architectural change include change character level word level decode find choice yield crucial speed improvements allow us train fewer computational resources yet still perform well frequent word explore model train twenty one hundred and sixty hours data find although model train less data considerably lower bleu score still predict word relatively high precision recall around fifty model train fifty hours data versus around sixty full one hundred and sixty hour model thus may still useful low resource scenarios
machine comprehension representative task natural language understand typically give context paragraph objective answer question depend context problem require model complex interactions context paragraph question lately attention mechanisms find quite successful task particular attention mechanisms attention flow context question question context prove quite useful paper study two state art attention mechanisms call bi directional attention flow bidaf dynamic co attention network dcn propose hybrid scheme combine two architectures give better overall performance moreover also suggest new simpler attention mechanism call double cross attention dca provide better result compare bidaf co attention mechanisms provide similar performance hybrid scheme objective paper focus particularly attention layer suggest improvements experimental evaluations show propose model achieve superior result stanford question answer dataset squad compare bidaf dcn attention mechanisms
demonstrate utility new methodological tool neural network word embed model large scale text analysis reveal model produce richer insights cultural associations categories possible prior methods word embeddings represent semantic relations word geometric relationships vectors high dimensional space operationalizing relational model mean consistent contemporary theories identity culture show dimension induce word differences eg man woman rich poor black white liberal conservative vector space closely correspond dimension cultural mean projection word onto dimension reflect widely share cultural connotations compare survey responses label historical data pilot method test stability associations demonstrate applications word embeddings macro cultural investigation longitudinal analysis coevolution gender class associations unite state 20th century comparative analysis historic distinctions markers gender class yous britain argue success high dimensional model motivate move towards high dimensional theorize mean identities cultural process
text segmentation task divide document contiguous segment base semantic structure longstanding challenge language understand previous work text segmentation focus unsupervised methods cluster graph search due paucity label data work formulate text segmentation supervise learn problem present large new dataset text segmentation automatically extract label wikipedia moreover develop segmentation model base dataset show generalize well unseen natural text
stack overflow great source natural language question code solutions ie question code pair critical many task include code retrieval annotation exist research question code pair collect heuristically tend low quality paper investigate new problem systematically mine question code pair stack overflow contrast heuristically collect formulate predict whether code snippet standalone solution question propose novel bi view hierarchical neural network capture program content textual context code snippet ie two view make prediction two manually annotate datasets python sql domain framework substantially outperform heuristic methods least fifteen higher f1 accuracy furthermore present staqc stack overflow question code pair largest dataset date 148k python 120k sql question code pair automatically mine use framework various case study demonstrate staqc greatly help develop data hungry model associate natural language program language
interaction web increase incidents aggression relate events like troll cyberbullying flame hate speech etc increase manifold across globe behaviour like bully hate speech predate internet reach extent internet give unprecedented power influence affect live billions people utmost significance importance preventive measure take provide safeguard people use web web remain viable medium communication connection general paper discuss development aggression tagset annotate corpus hindi english code mix data two popular social network social media platforms india twitter facebook corpus annotate use hierarchical tagset three top level tag ten level two tag final dataset contain approximately 18k tweet 21k facebook comment release research field
paper discuss attempt develop automatic language identification system five closely relate indo aryan languages india awadhi bhojpuri braj hindi magahi compile comparable corpora vary length languages various resources discuss method creation corpora detail use corpora language identification system develop currently give state art accuracy nine thousand, six hundred and forty-eight also use corpora study similarity five languages lexical level first data base study extent closeness languages
self attention method encode sequence vectors relate vectors base pairwise similarities model recently show promise result model discrete sequence non trivial apply acoustic model due computational model issue paper apply self attention acoustic model propose several improvements mitigate issue first self attention memory grow quadratically sequence length address downsampling technique second find previous approach incorporate position information model unsuitable explore representations hybrid model end third stress importance local context acoustic signal propose gaussian bias approach allow explicit control context range experiment find model approach strong baseline base lstms network network connections much faster compute besides speed find interpretability strength self attentional acoustic model demonstrate self attention head learn linguistically plausible division labor
differentiate intrinsic language word transliterable word key step aid text process task involve different natural languages consider problem unsupervised separation transliterable word native word text malayalam language outline key observation diversity character beyond word stem develop optimization method score word base nativeness method rely usage probability distributions character n grams refine step nativeness score iterative optimization formulation use empirical evaluation illustrate method dtim provide significant improvements nativeness score malayalam establish dtim prefer method task
present new dataset machine comprehension medical domain dataset use clinical case report around one hundred thousand gap fill query case apply several baselines state art neural readers dataset observe considerable gap performance twenty f1 best human machine readers analyze skills require successful answer show reader performance vary depend applicable skills find inferences use domain knowledge object track frequently require skills recognize omit information spatio temporal reason difficult machine
consider two different data set syntactic parameters discuss detect relations parameters heat kernel method develop belkin niyogi produce low dimensional representations data base laplace eigenfunctions preserve neighborhood information analyze different connectivity cluster structure arise two datasets regions maximal variance two parameter space belkin niyogi construction identify preferable choices independent variables compute cluster coefficients variance
present simple extension glove representation learn model begin general purpose representations update base data specialize domain show result representations lead faster learn better result variety task
present deep communicate agents encoder decoder architecture address challenge represent long document abstractive summarization deep communicate agents task encode long text divide across multiple collaborate agents charge subsection input text encoders connect single decoder train end end use reinforcement learn generate focus coherent summary empirical result demonstrate multiple communicate encoders lead higher quality summary compare several strong baselines include base single encoder multiple non communicate encoders
paper study dependent type events use treat verb phrase anaphora introduce framework extend dependent type semantics dts new atomic type neo davidsonian events extend operator return new events share properties events reference verb phrase anaphora propose framework along illustrative examples use present brief overview necessary background major challenge pose verb phrase anaphora
text article false claim especially news recently become aggravate internet users article wide circulation readers face difficulty discern fact fiction previous work credibility assessment focus factual analysis linguistic feature task main challenge distinction feature true false article paper propose novel approach call credibility outcome credo aim score credibility article open domain set credo consist different modules capture various feature responsible credibility article feature include credibility article source author semantic similarity article relate credible article retrieve knowledge base sentiments convey article neural network architecture learn contribution modules overall credibility article experiment snopes dataset reveal credo outperform state art approach base linguistic feature
consider task word level language model study possibility combine hide state base short term representations medium term representations encode dynamical weight language model work extend recent experiment language model dynamically evolve weight cast language model problem online learn learn framework meta learner train gradient descent continuously update language model weight
automatic speech recognition asr widely research supervise approach many low resourced languages lack audio text align data supervise methods apply work propose framework achieve unsupervised asr read english speech dataset audio text unaligned first stage word level audio segment utterances represent vector representation extract sequence sequence autoencoder phonetic information speaker information disentangle secondly semantic embeddings audio segment train vector representations use skip gram model last least unsupervised method utilize transform semantic embeddings audio segment text embed space finally transform embeddings map word framework towards unsupervised asr train unaligned text speech
topic model family statistical base algorithms summarize explore index large collections text document decade research lead computer scientists topic model spread social science new generation data drive social scientists search tool explore large collections unstructured text recently social scientists contribute topic model literature developments causal inference tool handle problem multi modality paper provide literature review evolution topic model include extensions document covariates methods evaluation interpretation advance interactive visualizations along aspect relevance application social science research
recognize even correct translations always semantically equivalent automatically detect mean divergences parallel sentence pair deep neural model bilingual semantic similarity train parallel corpus without manual annotation show semantic model detect divergences accurately model base surface feature derive word alignments divergences matter neural machine translation
recurrent neural network rnns achieve impressive result variety linguistic process task suggest induce non trivial properties language investigate extent rnns learn track abstract hierarchical syntactic structure test whether rnns train generic language model objective four languages italian english hebrew russian predict long distance number agreement various constructions include evaluation nonsensical sentence rnns rely semantic lexical cue colorless green ideas eat chair sleep furiously italian compare model performance human intuitions language model train rnns make reliable predictions long distance agreement lag much behind human performance thus bring support hypothesis rnns shallow pattern extractors also acquire deeper grammatical competence
present model encode sentence embed vectors specifically target transfer learn nlp task model efficient result accurate performance diverse transfer task two variants encode model allow trade off accuracy compute resources variants investigate report relationship model complexity resource consumption availability transfer task train data task performance comparisons make baselines use word level transfer learn via pretrained word embeddings well baselines use transfer learn find transfer learn use sentence embeddings tend outperform word level transfer transfer learn via sentence embeddings observe surprisingly good performance minimal amount supervise train data transfer task obtain encourage result word embed association test weat target detect model bias pre train sentence encode model make freely available download tf hub
cross lingual hypernymy detection involve determine word one language fruit hypernym word another language pomme ie apple french ability detect hypernymy cross lingually aid solve cross lingual versions task textual entailment event coreference propose bisparse dep family unsupervised approach cross lingual hypernymy detection learn sparse bilingual word embeddings base dependency contexts show bisparse dep significantly improve performance task compare approach base lexical context approach also robust show promise low resource settings dependency base embeddings learn use parser train relate languages negligible loss performance also crowd source challenge dataset task four languages russian french arabic chinese embeddings datasets publicly available
slot fill critical task natural language understand nlu dialog systems state art approach treat sequence label problem adopt model bilstm crf model work relatively well standard benchmark datasets face challenge context e commerce slot label informative carry richer expressions work inspire unique structure e commerce knowledge base propose novel multi task model cascade residual connections jointly learn segment tag name entity tag slot fill experiment show effectiveness propose cascade residual structure model one hundred and forty-six advantage f1 score strong baseline methods new chinese e commerce shop assistant dataset achieve competitive accuracies standard dataset furthermore online test deploy dominant e commerce platform show one hundred and thirty improvement accuracy understand user utterances model already go production e commerce platform
paper study problem automatically extract short title manually write longer description e commerce products display mobile devices new extractive summarization problem short text input propose feature enrich network model combine three different categories feature parallel experimental result show framework significantly outperform several baselines substantial gain forty-five moreover produce extractive summarization dataset e commerce short texts release research community
neural machine translation nmt new paradigm machine translation attention mechanism become dominant approach state art record many language pair variants attention mechanism use temporal attention one scalar value assign one context vector correspond source word paper propose fine grain 2d attention mechanism dimension context vector receive separate attention score experiment task en de en fi translation fine grain attention method improve translation quality term bleu score addition alignment analysis reveal fine grain attention mechanism exploit internal structure context vectors
work tackle problem speech emotion classification one issue area affective computation amount annotate data limit hand number ways emotion express verbally enormous due variability speakers one factor limit performance generalization propose simple method extract audio sample movies use textual sentiment analysis result possible automatically construct larger dataset audio sample positive negative emotional neutral speech show pretraining recurrent neural network dataset yield better result challenge emotiw corpus experiment show potential benefit combine textual sentiment analysis vocal information
acoustic emotion recognition aim categorize affective state speaker still difficult task machine learn model difficulties come scarcity train data general subjectivity emotion perception result low annotator agreement uncertainty feature relevant robust ones classification paper tackle latter problem inspire recent success transfer learn methods propose set architectures utilize neural representations infer train large speech databases acoustic emotion recognition task experiment iemocap dataset show ten relative improvements accuracy f1 score baseline recurrent neural network train end end emotion recognition
wassa two thousand and seventeen emoint share task goal predict emotion intensity value tweet message give text tweet emotion category anger joy fear sadness participants ask build system assign emotion intensity value emotion intensity estimation challenge problem give short length tweet noisy structure text lack annotate data solve problem develop ensemble two neural model process input character word level lexicon drive system correlation score across four emotions average determine bottom line competition metric system rank place forth full intensity range third five one range intensity among twenty-three systems time write june two thousand and seventeen
medical professionals search publish literature specify type patients medical interventions outcome measure interest paper demonstrate feature encode syntactic pattern improve performance state art sequence tag model linear neural information extraction medically relevant categories present analysis type pattern exploit semantic space induce ie distribute representations learn identify multi token pattern show learn representations differ substantially constituent unigrams suggest pattern capture contextual information otherwise lose
traditional neural machine translation nmt involve fix train procedure sentence sample epoch reality sentence well learn initial epochs however use approach well learn sentence would continue train along sentence well learn ten thirty epochs result wastage time propose efficient method dynamically sample sentence order accelerate nmt train approach weight assign sentence base measure difference train cost two iterations epoch certain percentage sentence dynamically sample accord weight empirical result base nist chinese english wmt english german task depict propose method significantly accelerate nmt train improve nmt performance
neural network base model commonly regard event detection word wise classification task suffer mismatch problem word event trigger especially languages without natural word delimiters chinese paper propose nugget proposal network npns solve word trigger mismatch problem directly propose entire trigger nuggets center character regardless word boundaries specifically npns perform event detection character wise paradigm hybrid representation character first learn capture structural semantic information character word base learn representations trigger nuggets propose categorize exploit character compositional structure chinese event trigger experiment ace2005 tac kbp two thousand and seventeen datasets show npns significantly outperform state art methods
framenet computational linguistics resource compose semantic frame high level concepts represent mean word paper present approach gather frame disambiguation annotations sentence use crowdsourcing approach multiple workers per sentence capture inter annotator disagreement perform experiment set four hundred and thirty-three sentence annotate frame framenet corpus show aggregate crowd annotations achieve f1 score greater sixty-seven compare expert linguists highlight case crowd annotation correct even though expert disagreement argue need multiple annotators per sentence importantly examine case crowd workers could agree demonstrate case exhibit ambiguity either sentence frame task argue collapse case single discrete truth value ie correct incorrect inappropriate create arbitrary target machine learn
ability consolidate information different type core intelligence tremendous practical value allow learn one task benefit generalizations learn others paper tackle challenge task improve semantic parse performance take ucca parse test case amr sdp universal dependencies ud parse auxiliary task experiment three languages use uniform transition base system learn architecture parse task despite notable conceptual formal domain differences show multitask learn significantly improve ucca parse domain domain settings
explore strategies incorporate target syntax neural machine translation specifically focus syntax ensembles contain multiple sentence representations formulate beam search ensembles use wfsts describe delay sgd update train procedure especially effective long representations like linearize syntax approach give state art performance difficult japanese english task
one thousand, nine hundred and thirty-six thirty-eight work progress administration interview thousands former slave life experience interview crucial understand peculiar institution standpoint slave issue relate bias cloud analyse interview problem investigate problem candour wpa slave narratives widely hold historical community strict racial caste system deep south compel black ex slave tell white interviewers think want hear suggest significant difference candour depend whether interviewer white black work attempt quantitatively characterise race relate problem candour prior work either impressionistic qualitative nature utilise exceedingly simple quantitative methodology contrast use sophisticate statistical methods particular word frequency sentiment analysis comparative topic model lda try identify differences content sentiment express ex slave front white interviewers versus black interviewers sentiment analysis methodology ultimately unsuccessful due complexity task word frequency analysis comparative topic model methods show strong evidence content express front white interviewers different black interviewers particular find ex slave speak much unfavourable aspects slavery like whip slave patrollers front interviewers race hope sophisticate statistical methodology help improve robustness argument existence problem candour slave narratives would seek deny revisionist purpose
dialogue systems hotel tourist information typically simplify richness domain focus system utterances select attribute price location type room however much content typically available hotels often many fifty distinct instantiate attribute individual entity new methods need use content generate natural dialogues hotel information general domain rich complex content describe three experiment aim collect data inform nlg hotels dialogues show surprisingly sentence original write hotel descriptions provide webpages hotel stylistically good match conversational interaction quantify stylistic feature characterize differences original textual data collect dialogic data plan use stylistic model generation score retrieve utterances use hotel dialogues
parallelizable attention network neural transformer fast train however due auto regressive architecture self attention decoder decode procedure become slow alleviate issue propose average attention network alternative self attention network decoder neural transformer average attention network consist two layer average layer model dependencies previous position gate layer stack average layer enhance expressiveness propose attention network apply network decoder part neural transformer replace original target side self attention model mask trick dynamic program model enable neural transformer decode sentence four time faster original version almost loss train time translation performance conduct series experiment wmt17 translation task six different language pair obtain robust consistent speed up decode
frequent use emojis social media platforms create new form multimodal social interaction develop methods study representation emoji semantics help improve future multimodal communication systems paper explore usage semantics emojis time compare emoji embeddings train corpus different season show emojis use differently depend time year moreover propose method take account time information emoji prediction systems outperform state art systems show use time information accuracy emojis significantly improve
aspect term extraction eat key sub task aspect base sentiment analysis aim extract explicit aspect expressions online user review present new framework tackle eat exploit two useful clue namely opinion summary aspect detection history opinion summary distil whole input sentence condition current token aspect prediction thus tailor make summary help aspect prediction token another clue information aspect detection history distil previous aspect predictions leverage coordinate structure tag schema constraints upgrade aspect prediction experimental result four benchmark datasets clearly demonstrate framework outperform state art methods
propose fully unsupervised framework ad hoc cross lingual information retrieval clir require bilingual data framework leverage share cross lingual word embed space term query document represent irrespective actual language share embed space induce solely basis monolingual corpora two languages iterative process base adversarial neural network experiment standard clef clir collections three language pair vary degrees language similarity english dutch italian finnish demonstrate usefulness propose fully unsupervised approach clir model unsupervised cross lingual embeddings outperform baselines utilize cross lingual embeddings induce rely word level document level alignments demonstrate improvements achieve unsupervised ensemble clir model believe propose framework first step towards development effective clir model language pair domains parallel data scarce non existent
split rephrase complex sentence several shorter sentence convey mean challenge problem nlp show vanilla seq2seq model reach high score propose benchmark narayan et al two thousand and seventeen suffer memorization train set contain eighty-nine unique simple sentence validation test set aid present new train development test data split neural model augment copy mechanism outperform best report baseline eight hundred and sixty-eight bleu foster progress task
propose hypothesis baseline diagnose natural language inference nli especially nli dataset assume inference occur base purely relationship context hypothesis follow assess entailment relations ignore provide context degenerate solution yet experiment ten distinct nli datasets find approach refer hypothesis model able significantly outperform majority class baseline across number nli datasets analysis suggest statistical irregularities may allow model perform nli datasets beyond achievable without access context
demonstrate replace lstm encoder self attentive architecture lead improvements state art discriminative constituency parser use attention make explicit manner information propagate different locations sentence use analyze model propose potential improvements example find separate positional content information encoder lead improve parse accuracy additionally evaluate different approach lexical representation parser achieve new state art result single model train penn treebank nine thousand, three hundred and fifty-five f1 without use external data nine thousand, five hundred and thirteen f1 use pre train word representations parser also outperform previous best publish accuracy figure eight nine languages spmrl dataset
study explore creation machine learn model automatically identify whether neonatal intensive care unit nicu patient diagnose neonatal jaundice particular hospitalization base associate clinical note develop number techniques text preprocessing feature selection compare effectiveness different classification model show use ensemble decision tree classification adaboost bag outperform support vector machine svm current state art technique neonatal jaundice cod
although much effort recently devote train high quality sentence embeddings still poor understand capture downstream task often base sentence classification commonly use evaluate quality sentence representations complexity task make however difficult infer kind information present representations introduce ten probe task design capture simple linguistic feature sentence use study embeddings generate three different encoders train eight distinct ways uncover intrigue properties encoders train methods
target orient sentiment classification aim classify sentiment polarities individual opinion target sentence rnn attention seem good fit characteristics task indeed achieve state art performance examine drawbacks attention mechanism obstacles block cnn perform well classification task propose new model overcome issue instead attention model employ cnn layer extract salient feature transform word representations originate bi directional rnn layer two layer propose component generate target specific representations word sentence meanwhile incorporate mechanism preserve original contextual information rnn layer experiment show model achieve new state art performance benchmarks
paper describe system submit semeval two thousand and eighteen task three irony detection english tweet subtask team binarizer irony detection key task many natural language process work method treat ironical tweet consist smaller part contain different emotions break tweet separate phrase use dependency parser embed phrase use lstm base neural network model pre train predict emoticons tweet finally train fully connect network achieve classification
seq2seq base neural architectures become go architecture apply sequence sequence language task despite excellent performance task recent work note model usually fully capture linguistic structure require generalize beyond dense section data distribution citeettinger2017towards likely fail sample tail end distribution input noisy citepbelkinovnmtbreak different lengths citepbentivoglinmtlength paper look model ability generalize simple symbol rewrite task clearly define structure find model ability generalize structure beyond train distribution depend greatly choose random seed even performance standard test set remain suggest model ability capture generalizable structure highly sensitive moreover sensitivity may apparent evaluate standard test set
fast expansion natural language functionality intelligent virtual agents critical achieve engage informative interactions however develop accurate model new natural language domains time data intensive process propose efficient deep neural network architectures maximally use available resources transfer learn methods apply expand understand capabilities popular commercial agent evaluate hundreds new domains design internal external developers demonstrate propose methods significantly increase accuracy low resource settings enable rapid development accurate model less data
highlight practical yet rarely discuss problem dialogue state track dst namely handle unknown slot value previous approach generally assume predefined candidate list thus design output unknown value especially speak language understand slu module absent many end end e2e systems describe paper e2e architecture base pointer network ptrnet effectively extract unknown slot value still obtain state art accuracy standard dstc2 benchmark also provide extensive empirical evidence show track unknown value challenge approach bring significant improvement help effective feature dropout technique
neural machine translation nmt researchers face challenge un see vocabulary oov word translation solve researchers propose split western languages english german sub word compound paper try address oov issue improve nmt adequacy harder language chinese whose character even sophisticate composition integrate chinese radicals nmt model different settings address unseen word challenge chinese english translation hand also consider semantic part mt system since chinese radicals usually carry essential mean word construct meaningful radicals new character integrate nmt systems model use attention base nmt system strong baseline system experiment standard chinese english nist translation share task data two thousand and six two thousand and eight show design model outperform baseline model wide range state art evaluation metrics include lepor beer character addition bleu nist score especially adequacy level translation also interest find result various experiment settings performance word character chinese nmt different languages instance fully character level nmt may perform well state art languages researchers demonstrate recently however chinese nmt model word boundary knowledge important model learn
biomedical concept normalization link concept mention texts semantically equivalent concept biomedical knowledge base task challenge concepts different expressions natural languages eg paraphrase necessarily present knowledge base concept normalization non english biomedical text even challenge non english resources tend much smaller contain less synonyms overcome limitations non english terminologies propose cross lingual candidate search concept normalization use character base neural translation model train multilingual biomedical terminology model train spanish french dutch german versions umls evaluation model carry french quaero corpus show outperform team clef ehealth two thousand and fifteen two thousand and sixteen additionally compare performance commercial translators spanish french dutch german versions mantra model perform similarly well free charge run locally particularly important clinical nlp applications medical document underlay strict privacy restrictions
many machine translation mt paper propose novel approach show improvements self define baselines experimental set paper often differ one another hard determine propose approach really useful advance state art chinese english translation common translation direction mt paper although one widely accept experimental set chinese english mt goal paper propose benchmark evaluation setup chinese english machine translation effectiveness new propose mt approach directly compare previous approach towards end also build highly competitive state art mt system train large scale train set system outperform report result nist openmt test set almost paper publish major conferences journals computational linguistics artificial intelligence past eleven years argue standardize benchmark data performance important meaningful comparison
every person speak write flavor native language influence number factor content tend talk gender social status geographical origin attempt perform machine translation mt variations significant effect system perform translation capture well standard one size fit model paper propose simple parameter efficient adaptation technique require adapt bias output softmax particular user mt system either directly factor approximation experiment ted talk three languages demonstrate improvements translation accuracy better reflection speaker traits target text
word embeddings recently impose standard represent word mean nlp semantic similarity word pair become common evaluation benchmark representations vector cosine typically use similarity metric paper report experiment rank base metric perform comparably vector cosine similarity estimation outperform recently introduce challenge task outlier detection thus suggest rank base measure improve cluster quality
problem aspect base sentiment analysis deal classify sentiments negative neutral positive give aspect sentence traditional sentiment classification task involve treat entire sentence text document classify sentiments base word let us assume sentence acceleration car fast reliability horrible difficult sentence two aspects conflict sentiments entity consider machine learn techniques deep learn encode information interest one aspect sentiment let us explore various pre process step feature methods use facilitate solve task
investigate lattice structure lstm model chinese ner encode sequence input character well potential word match lexicon compare character base methods model explicitly leverage word word sequence information compare word base methods lattice lstm suffer segmentation errors gate recurrent cells allow model choose relevant character word sentence better ner result experiment various datasets show lattice lstm outperform word base character base lstm baselines achieve best result
neural machine translation nmt model typically train fix size input output vocabularies create important bottleneck accuracy generalization capability solution various study propose segment word sub word units perform translation sub lexical level however statistical word segmentation methods recently show prone morphological errors lead inaccurate translations paper propose overcome problem replace source language embed layer nmt bi directional recurrent neural network generate compositional representations input desire level granularity test approach low resource set five languages different morphological typologies different composition assumptions train nmt compose word representations character n grams approach consistently outperform one hundred and seventy-one two hundred and forty-eight bleu point nmt learn embeddings statistically generate sub word units
neural machine translation nmt accelerate deep learn neural network statistical base approach due plethora programmability commodity heterogeneous compute architectures fpgas gpus massive amount train corpuses generate news outlets government agencies social media train learn classifier neural network entail tune hyper parameters would yield best performance unfortunately number parameters machine translation include discrete categories well continuous options make combinatorial explosive problem research explore optimize hyper parameters train deep learn neural network machine translation specifically work investigate train language model marian nmt result compare nmt various hyper parameter settings across variety modern gpu architecture generations single node multi node settings reveal insights hyper parameters matter term performance word process per second convergence rat translation accuracy provide insights best achieve high perform nmt systems
mine electronic health record patients satisfy set predefined criteria know medical informatics phenotyping phenotyping numerous applications outcome prediction clinical trial recruitment retrospective study supervise machine learn phenotyping typically rely sparse patient representations bag word consider alternative involve learn patient representations develop neural network model learn patient representations show learn representations general enough obtain state art performance standard comorbidity detection task
extract meaningful topics texts structure consider properly paper aim analyze structure time series document collection news article series scientific paper wherein topics evolve along time depend multiple topics past also relate time end propose dynamic static topic model simultaneously consider dynamic structure temporal topic evolution static structure topic hierarchy time show result experiment collections scientific paper propose method outperform conventional model moreover show example extract topic structure find helpful analyze research activities
machine read comprehension mrc real web data usually require machine answer question analyze multiple passages retrieve search engine compare mrc single passage multi passage mrc challenge since likely get multiple confuse answer candidates different passages address problem propose end end neural model enable answer candidates different passages verify base content representations specifically jointly train three modules predict final answer base three factor answer boundary answer content cross passage answer verification experimental result show method outperform baseline large margin achieve state art performance english ms marco dataset chinese dureader dataset design mrc real world settings
paper report participation share task word sense induction disambiguation russian language russe two thousand and eighteen team rank 2nd wiki wiki dataset contain mostly homonyms 5th bts rnc active dict datasets contain mostly polysemous word among nineteen participants method employ extremely naive imply represent contexts ambiguous word average word embed vectors use shelf pre train distributional model vector representations cluster mainstream cluster techniques thus produce group correspond ambiguous word sense side result show word embed model train small balance corpora superior train large noisy data intrinsic evaluation also downstream task like word sense induction
describe deploy scalable system organize publish scientific literature heterogeneous graph facilitate algorithmic manipulation discovery result literature graph consist 280m nod represent paper author entities various interactions eg authorships citations entity mention reduce literature graph construction familiar nlp task eg entity extraction link point research challenge due differences standard formulations task report empirical result task methods describe paper use enable semantic feature wwwsemanticscholarorg
create new nli test set show deficiency state art model inferences require lexical world knowledge new examples simpler snli test set contain sentence differ one word sentence train set yet performance new test set substantially worse across systems train snli demonstrate systems limit generalization ability fail capture many simple inferences
propose novel coherence model write asynchronous conversations eg forums email show applications coherence assessment thread reconstruction task conduct research two step first propose improvements recently propose neural entity grid model lexicalize entity transition extend model asynchronous conversations incorporate underlie conversational structure entity grid representation feature computation model achieve state art result standard coherence assessment task monologue conversations outperform exist model also demonstrate effectiveness reconstruct thread structure
present approach neural machine translation nmt support multiple domains single model allow switch domains translate core idea treat text domains distinct languages use multilingual nmt methods create multi domain translation systems show approach result significant translation quality gain fine tune also explore whether knowledge pre specify text domains necessary turn also know quite high translation quality reach
propose method leverage unlabeled data learn match model response selection retrieval base chatbots method employ sequence sequence architecture seq2seq model weak annotator judge match degree unlabeled pair perform learn weak signal unlabeled data experimental result two public data set indicate match model get significant improvements learn propose method
reveal implicit semantic relation constituents noun compound important many nlp applications address literature either classification task set pre define relations produce free text paraphrase explicate relations exist paraphrase methods lack ability generalize hard time interpret infrequent new noun compound propose neural model generalize better represent paraphrase continuous space generalize unseen noun compound rare paraphrase model help improve performance noun compound paraphrase classification task
problem amr text generation recover text represent mean input amr graph current state art method use sequence sequence model leverage lstm encode linearize amr structure although able model non local semantic information sequence lstm lose information amr graph structure thus face challenge large graph result long sequence introduce neural graph sequence model use novel lstm structure directly encode graph level semantics standard benchmark model show superior result exist methods literature
election manifestos document intentions motives view political party often use analyse party fine grain position particular issue well coarse grain position party leave right spectrum paper propose two stage model automatically perform level analysis manifestos first step employ hierarchical multi task structure deep model predict fine coarse grain position second step perform post hoc calibration coarse grain position use probabilistic soft logic empirically show propose model outperform state art approach granularities use manifestos twelve countries write ten different languages
automatic evaluate performance open domain dialogue system challenge problem recent work neural network base metrics show promise opportunities automatic dialogue evaluation however exist methods mainly focus monolingual evaluation train metric flexible enough transfer across different languages address issue propose adversarial multi task neural metric advmt multi lingual dialogue evaluation share feature extraction across languages evaluate propose model two different languages experiment show adversarial multi task neural metric achieve high correlation human annotation yield better performance monolingual ones various exist metrics
recent years neural machine translation nmt prove get impressive result additional linguistic feature input word improve word level nmt additional character feature use improve character level nmt far paper show radicals chinese character kanji character feature information easily provide improvements character level nmt experiment wat2016 japanese chinese scientific paper excerpt corpus aspec jp find propose method improve translation quality accord two aspects perplexity bleu character level nmt radical input feature model get state art result four thousand and sixty-one bleu point test set improvement eighty-six bleu point best system wat2016 japanese chinese translation subtask aspec jp improvements character level nmt additional input feature fifteen fourteen bleu point development test set test set corpus respectively
gender prediction typically focus lexical social network feature yield good performance make systems highly language topic platform dependent cross lingual embeddings circumvent limitations capture gender specific style less propose alternative bleach text ie transform lexical string abstract feature study provide evidence feature allow better transfer across languages moreover present first study ability humans perform cross lingual gender prediction find human predictive power prove similar bleach model perform better lexical model
word vector specialisation also know retrofit portable light weight approach fine tune arbitrary distributional word vector space inject external knowledge rich lexical resources wordnet design post process methods update vectors word occur external lexicons leave representations unseen word intact paper show constraint drive vector space specialisation extend unseen word propose novel post specialisation method preserve useful linguistic knowledge see word b propagate external signal unseen word order improve vector representations well post specialisation approach explicits non linear specialisation function form deep neural network learn predict specialise vectors original distributional counterparts learn function use specialise vectors unseen word approach applicable post process model yield considerable gain initial specialisation model intrinsic word similarity task two downstream task dialogue state track lexical text simplification positive effect persist across three languages demonstrate importance specialise full vocabulary distributional word vector space
create intelligent conversational system understand vision language one ultimate goals artificial intelligence aicitewinograd1972understanding extensive research focus vision language generation however limit research touch combine two modalities goal drive dialog context propose multimodal hierarchical reinforcement learn framework dynamically integrate vision language task orient visual dialog framework jointly learn multimodal dialog state representation hierarchical dialog policy improve dialog task success efficiency also propose new technique state adaptation integrate context awareness dialog state representation evaluate propose framework state adaptation technique image guess game achieve promise result
character level neural machine translation nmt model recently achieve impressive result many language pair mainly well indo european language pair languages share write system however translate chinese english gap two different write systems pose major challenge lack systematic correspondence individual linguistic units paper enable character level nmt chinese break chinese character linguistic units similar indo european languages use wubi encode scheme preserve original shape semantic information character also reversible show promise result train wubi base model character subword level recurrent well convolutional model
word embed key component many downstream applications process natural languages exist approach often assume existence large collection text learn effective word embed however corpus may available low resource languages paper study effectively learn word embed model corpus million tokens situation co occurrence matrix sparse co occurrences many word pair unobserved contrast exist approach often sample unobserved word pair negative sample argue zero entries co occurrence matrix also provide valuable information design positive unlabeled learn pu learn approach factorize co occurrence matrix validate propose approach four different languages
age root indo european language family receive much attention since application bayesian phylogenetic methods gray atkinson2003 root age indo european family tend decrease age support anatolian origin hypothesis age support steppe origin hypothesis application new model chang et al two thousand and fifteen however none publish work indo european phylogenetics study effect tree priors phylogenetic analyse indo european family paper intend fill gap explore effect tree priors different aspects indo european family phylogenetic inference apply three tree priors uniform fossilize birth death fbd coalescent five publicly available datasets indo european language family evaluate posterior distribution tree bayesian analysis use bay factor find support steppe origin hypothesis case two tree priors report median ninety-five highest posterior density hpd interval root age three tree priors model comparison suggest either uniform prior fbd prior suitable coalescent prior datasets belong indo european language family
comment online article provide extend view improve user engagement automatically make comment thus become valuable functionality online forums intelligent chatbots etc paper propose new task automatic article comment introduce large scale chinese dataset millions real comment human annotate subset characterize comment vary quality incorporate human bias comment quality develop automatic metrics generalize broad set popular reference base metrics exhibit greatly improve correlations human evaluations
positive effect add subword information word embeddings demonstrate predictive model paper investigate whether similar benefit also derive incorporate subwords count model evaluate impact different type subwords n grams unsupervised morphemes result confirm importance subword information learn representations rare vocabulary word
despite impressive quality improvements yield neural machine translation nmt systems control translation output adhere user provide terminology constraints remain open problem describe approach constrain neural decode base finite state machine multi stack decode support target side constraints well constraints correspond align input text span demonstrate performance framework multiple translation task motivate need constrain decode attentions mean reduce misplacement duplication translate user constraints
paper investigate use discourse aware reward reinforcement learn guide model generate long coherent text particular propose learn neural reward model cross sentence order mean approximate desire discourse structure empirical result demonstrate generator train learn reward produce coherent less repetitive text model train cross entropy reinforcement learn commonly use score reward
analysis propose new topic model study yearly trend marvel cinematic universe fanfictions three level character popularity character image topics vocabulary pattern topics find character appearances fanfictions become diverse years thank constant introduction new character feature film case captain america multi dimensional character development well receive fanfiction world
dialogue systems task name entity recognition ner name entity link nel vital preprocessing step understand user intent especially open domain interaction rely domain specific inference ucsc effort one fund team two thousand and seventeen amazon alexa prize contest yield slugbot open domain social bot aim casual conversation discover several challenge specifically associate ner nel build slugbot ne label coarse grain entity type link useful ontology moreover discover traditional approach perform well context even systems design operate tweet social media data work well dialogue systems paper introduce slugbot name entity recognition dialogue systems slugnerds ner nel tool optimize address issue describe two new resources build part work slugentitydb schemaactuator believe resources useful research community
train accurate classifiers require many label label provide limit information one bite binary classification work propose babblelabble framework train classifiers annotator provide natural language explanation label decision semantic parser convert explanations programmatic label function generate noisy label arbitrary amount unlabeled data use train classifier three relation extraction task find users able train classifiers comparable f1 score five 100times faster provide explanations instead label furthermore give inherent imperfection label function find simple rule base semantic parser suffice
paper propose hybrid semi markov conditional random field scrfs neural sequence label natural language process base conventional conditional random field crfs scrfs design task assign label segment extract feature describe transition segment instead word paper improve exist scrf methods employ word level segment level information simultaneously first word level label utilize derive segment score scrfs second crf output layer scrf output layer integrate unify neural network train jointly experimental result conll two thousand and three name entity recognition ner share task show model achieve state art performance external knowledge use
consider task detect contractual obligations prohibitions show self attention mechanism improve performance bilstm classifier previous state art task allow focus indicative tokens also introduce hierarchical bilstm convert sentence embed process sentence embeddings classify sentence apart faster train hierarchical bilstm outperform flat one even latter consider surround sentence hierarchical model broader discourse view
exist research response generation chatbot focus textbffirst response generation aim teach chatbot say first response eg sentence appropriate conversation context eg user query paper introduce new task textbfsecond response generation term improv chat aim teach chatbot say second response say first response respect conversation context lighten burden user keep conversation go specifically propose general learn base framework develop retrieval base system generate second responses users query chatbot first response input present approach build conversation corpus improv chat public forums social network well neural network base model response match rank include preliminary experiment result paper work could advance better deep match model retrieval base systems generative model generation base systems well extensive evaluations real life applications
academic paper submit conferences journals evaluate paper professionals time consume inequality due personal factor reviewers paper order assist professionals evaluate academic paper propose novel task automatic academic paper rat aapr automatically determine whether accept academic paper build new dataset task propose novel modularized hierarchical convolutional neural network achieve automatic academic paper rat evaluation result show propose model outperform baselines large margin dataset code available urlhttps githubcom lancopku aapr
simultaneous interpretation translation speak word real time highly challenge physically demand methods predict interpreter confidence adequacy interpret message number potential applications computer assist interpretation interfaces pedagogical tool propose task predict simultaneous interpreter performance build exist methodology quality estimation qe machine translation output experiment five settings three language pair extend qe pipeline estimate interpreter performance approximate meteor evaluation metric propose novel feature reflect interpretation strategy evaluation measure improve prediction accuracy
abstractive text summarization highly difficult problem sequence sequence model show success improve performance task however generate summaries often inconsistent source content semantics case generate summaries model select semantically unrelated word respect source content probable output problem attribute heuristically construct train data summaries unrelated source content thus contain semantically unrelated word spurious word correspondence paper propose regularization approach sequence sequence model make use model learn regularize learn objective alleviate effect problem addition propose practical human evaluation method address problem exist automatic evaluation method evaluate semantic consistency source content properly experimental result demonstrate effectiveness propose approach outperform almost exist model especially propose approach improve semantic consistency four term human evaluation
present novel end end reinforcement learn approach automatic taxonomy induction set term prior methods treat problem two phase task ie detect hypernymy pair follow organize pair tree structure hierarchy argue two phase methods may suffer error propagation effectively optimize metrics capture holistic structure taxonomy approach representations term pair learn use multiple source information use determine textitwhich term select textitwhere place taxonomy via policy network components train end end manner cumulative reward measure holistic tree metric train taxonomies experiment two public datasets different domains show approach outperform prior state art taxonomy induction methods one hundred and ninety-six ancestor f1
recurrent neural network rnns represent years state art neural machine translation recently new architectures propose leverage parallel computation gpus better classical rnns faster train inference combine different sequence sequence model also lead performance improvements new model completely depart original recurrent architecture decide investigate make rnns efficient work propose new recurrent nmt architecture call simple recurrent nmt build class fast weakly recurrent units use layer normalization multiple attentions experiment wmt14 english german wmt16 english romanian benchmarks show model represent valid alternative lstms achieve better result significantly lower computational cost
present set experiment demonstrate deep recurrent neural network rnns learn internal representations capture soft hierarchical notions syntax highly vary supervision consider four syntax task different depths parse tree word predict part speech well first parent second grandparent third level great grandparent constituent label appear predictions make representations produce different depths network pretrained one four objectives dependency parse semantic role label machine translation language model every case find correspondence network depth syntactic depth suggest soft syntactic hierarchy emerge effect robust across condition indicate model encode significant amount syntax even absence explicit syntactic train supervision
neural machine translation require large amount parallel train text learn reasonable quality translation model particularly inconvenient language pair enough parallel text available paper use monolingual linguistic resources source side address challenge problem base multi task learn approach specifically scaffold machine translation task auxiliary task include semantic parse syntactic parse name entity recognition effectively inject semantic syntactic knowledge translation model would otherwise require large amount train bitext empirically evaluate show effectiveness multi task learn approach three translation task english french english farsi english vietnamese
conventional open information extraction open ie systems usually build hand craft pattern nlp tool syntactic parse yet face problems error propagation paper propose neural open ie approach encoder decoder framework distinct exist methods neural open ie approach learn highly confident arguments relation tuples bootstrapped state art open ie system empirical study large benchmark dataset show neural open ie system significantly outperform several baselines maintain comparable computational efficiency
ukb open source collection program perform among task knowledge base word sense disambiguation wsd since release two thousand and nine often use box sub optimal settings show nine years later state art knowledge base wsd case show pitfalls release open source nlp software without optimal default settings precise instructions reproducibility
resurgence chat base dialog systems consumer enterprise applications much success develop data drive rule base natural language model understand human intent since model require large amount data domain knowledge expand equivalent service new market disrupt language barriers inhibit dialog automation paper present user study evaluate utility box machine translation technology one rapidly bootstrap multilingual speak dialog systems two enable exist human analysts understand foreign language utterances additionally evaluate utility machine translation human assist environments portion traffic process analysts english spanish experiment observe high potential dialog automation well potential human analysts process foreign language utterances high accuracy
automatic machine learn systems inadvertently accentuate perpetuate inappropriate human bias past work examine inappropriate bias largely focus individual systems benchmark dataset examine inappropriate bias systems first time present equity evaluation corpus eec consist eight thousand, six hundred and forty english sentence carefully choose tease bias towards certain race genders use dataset examine two hundred and nineteen automatic sentiment analysis systems take part recent share task semeval two thousand and eighteen task one affect tweet find several systems show statistically significant bias consistently provide slightly higher sentiment intensity predictions one race one gender make eec freely available
paper explore sentiment composition phrase least one positive least one negative word phrase like happy accident best winter break compile dataset oppose polarity phrase manually annotate real value score sentiment association use dataset analyze linguistic pattern present oppose polarity phrase finally apply several unsupervised supervise techniques sentiment composition determine efficacy dataset best system incorporate information phrase constituents part speech sentiment association score embed vectors obtain accuracy eighty oppose polarity phrase
team nrc canada participate two share task amia two thousand and seventeen workshop social media mine health applications smm4h task one classification tweet mention adverse drug reactions task two classification tweet describe personal medication intake task train support vector machine classifiers use variety surface form sentiment domain specific feature nine team participate task submissions rank first task one third task two handle considerable class imbalance prove crucial task one apply sample technique reduce class imbalance one hundred and ten twelve standard n gram feature n grams generalize domain term well general domain domain specific word embeddings substantial impact overall performance task hand include sentiment lexicon feature result improvement
morphological analysis involve predict syntactic traits word eg pos noun case acc gender fem previous work morphological tag improve performance low resource languages lrls cross lingual train high resource language hrl family limit strict often false assumption tag set exactly overlap hrl lrl paper propose method cross lingual morphological tag aim improve information share languages relax assumption propose model use factorial conditional random field neural network potentials make possible one utilize expressive power neural network representations smooth superficial differences surface form two model pairwise transitive relationships tag three accurately generate tag set unseen rare train data experiment four languages universal dependencies treebank demonstrate superior tag accuracies exist cross lingual approach
generic word embeddings train large scale generic corpora domain specific ds word embeddings train data domain interest paper propose method combine breadth generic embeddings specificity domain specific embeddings result embeddings call domain adapt da word embeddings form align correspond word vectors use canonical correlation analysis cca relate nonlinear kernel cca evaluation result sentiment classification task show da embeddings substantially outperform generic ds embeddings use input feature standard state art sentence encode algorithms classification
report series experiment different semantic model top various statistical model extractive text summarization though statistical model may better capture word co occurrences distribution around text fail detect context sense sentence word whole semantic model help us gain better insight context sentence show tune weight different model help us achieve significant result various benchmarks learn pre train vectors use semantic model give corpus give addition spike performance use weigh techniques different statistical model refine result statistical model use tf idf textrank jaccard cosine similarities semantic model use wordnet base model propose two model base glove vectors facebook infersent test approach duc two thousand and four dataset generate one hundred word summaries discuss system algorithms analysis also propose test possible improvements rouge score use compare summarizers
one key task fine grain sentiment analysis product review extract product aspects feature users express opinions paper focus supervise aspect extraction use deep learn unlike highly sophisticate supervise deep learn model paper propose novel yet simple cnn model employ two type pre train embeddings aspect extraction general purpose embeddings domain specific embeddings without use additional supervision model achieve surprisingly good result outperform state art sophisticate exist methods knowledge paper first report double embeddings base cnn model aspect extraction achieve good result
work focus confidence model neural semantic parsers build upon sequence sequence model outline three major cause uncertainty design various metrics quantify factor metrics use estimate confidence score indicate whether model predictions likely correct beyond confidence estimation identify part input contribute uncertain predictions allow users interpret model verify refine input experimental result show confidence model significantly outperform widely use method rely posterior probability improve quality interpretation compare simply rely attention score
field natural language process nlp grow rapidly new research publish daily along abundance tutorials codebases online resources order learn dynamic field stay date latest research students well educators researchers must constantly sift multiple source find valuable relevant information address situation introduce tutorialbank new publicly available dataset aim facilitate nlp education research manually collect categorize six thousand, three hundred resources nlp well relate field artificial intelligence ai machine learn ml information retrieval ir dataset notably largest manually pick corpus resources intend nlp education include academic paper additionally create search engine command line tool resources annotate corpus include list research topics relevant resources topic prerequisite relations among topics relevant sub part individual resources among annotations release dataset present several avenues research
know little neural language model lm use prior linguistic context paper investigate role context lstm lm ablation study specifically analyze increase perplexity prior context word shuffle replace drop two standard datasets penn treebank wikitext two find model capable use two hundred tokens context average sharply distinguish nearby context recent fifty tokens distant history model highly sensitive order word within recent sentence ignore word order long range context beyond fifty tokens suggest distant past model rough semantic field topic find neural cache model grave et al 2017b especially help lstm copy word within distant context overall analysis provide better understand neural lms use context also shed light recent success cache base model
inquiry fundamental communication machine effectively collaborate humans unless ask question work build neural network model task rank clarification question model inspire idea expect value perfect information good question one whose expect answer useful study problem use data stackexchange plentiful online resource people routinely ask clarify question post better offer assistance original poster create dataset clarification question consist 77k post pair clarification question answer three domains stackexchange askubuntu unix superuser evaluate model five hundred sample dataset expert human judgments demonstrate significant improvements control baselines
introduce structure projection intermediate gradients optimization technique spigot new method backpropagating neural network include hard decision structure predictions eg parse intermediate layer spigot require marginal inference unlike structure attention network kim et al two thousand and seventeen reinforcement learn inspire solutions yogatama et al two thousand and seventeen like call straight estimators hinton two thousand and twelve spigot define gradient like quantities associate intermediate nondifferentiable operations allow backpropagation spigot proxy aim ensure parameter update intermediate structure remain well form experiment two structure nlp pipelines syntactic semantic dependency parse semantic parse follow sentiment classification show train spigot lead larger improvement downstream task modularly train pipeline straight estimator structure attention reach new state art semantic dependency parse
release community six large scale sense annotate datasets multiple language pave way supervise multilingual word sense disambiguation datasets cover nouns english wordnet translations languages total millions sense tag sentence experiment prove corpora effectively use train set supervise wsd systems surpass state art low resourced languages provide competitive result english manually annotate train set accessible data available trainomaticorg
paper present ted lium release three corpus dedicate speech recognition english multiply two available data train acoustic model comparison ted lium two present recent development automatic speech recognition asr systems comparison two previous release ted lium corpus two thousand and twelve two thousand and fourteen demonstrate pass two hundred and seven four hundred and fifty-two hours transcribe speech train data really useful end end asr systems hmm base state art ones even hmm base asr system still outperform end end asr system size audio train data four hundred and fifty-two hours respectively word error rate wer sixty-six one hundred and thirty-seven last propose two repartitions ted lium release three corpus legacy one one exist release two new one calibrate design make experiment speaker adaptation like two first release ted lium three corpus freely available research community
use dependency triple automatically extract web scale corpus perform unsupervised semantic frame induction cast frame induction problem triclustering problem generalization cluster triadic data replicable benchmarks demonstrate propose graph base approach triframes show state art result task framenet derive dataset perform par competitive methods verb class cluster task
recent bio tag base neural semantic role label model high perform assume gold predicate part input incorporate span level feature propose end end approach jointly predict predicate arguments span relations model make independent decisions relationship hold every possible word span pair learn contextualized span representations provide rich share input feature decision experiment demonstrate approach set new state art propbank srl without gold predicate
semantic parse aim map natural language utterances structure mean representations work propose structure aware neural architecture decompose semantic parse process two stag give input utterance first generate rough sketch mean low level information variable name arguments gloss fill miss detail take account natural language input sketch experimental result four datasets characteristic different domains mean representations show approach consistently improve performance achieve competitive result despite use relatively simple decoders
name entities usually composable extensible typical examples name symptoms diseases medical areas distinguish entities general entities name textitcompound entities paper present attention base bi gru capsnet model detect hypernymy relationship compound entities model consist several important components avoid vocabulary problem english word chinese character compound entities feed bidirectional gate recurrent units attention mechanism design focus differences two compound entities since different case hypernymy relationship compound entities capsule network finally employ decide whether hypernymy relationship exist experimental result demonstrate
explore story generation creative systems build coherent fluent passages text topic collect large dataset 300k human write stories pair write prompt online forum dataset enable hierarchical story generation model first generate premise transform passage text gain improvements novel form model fusion improve relevance story prompt add new gate multi scale self attention mechanism model long range context experiment show large improvements strong baselines automate human evaluations human judge prefer stories generate approach strong non hierarchical model factor two one
text many domains involve significant amount name entities predict ing entity name often challenge language model appear less frequent train corpus paper propose novel effective approach build discriminative language model learn entity name leverage entity type information also introduce two benchmark datasets base recipes java program cod evalu eat propose model experimental sults show model achieve five hundred and twenty-two better perplexity recipe generation two thousand, two hundred and six code generation state art language model
ask good question large scale open domain conversational systems quite significant yet rather untouched task substantially different traditional question generation require question various pattern also diverse relevant topics observe good question natural composition interrogatives topic word ordinary word interrogatives lexicalize pattern question topic word address key information topic transition dialogue ordinary word play syntactical grammatical roles make natural sentence devise two type decoders textitsoft type decoder textithard type decoder type distribution three type estimate use modulate final generation distribution extensive experiment show type decoders outperform state art baselines generate meaningful question
current abstractive text summarization model base sequence sequence model seq2seq source content social media long noisy difficult seq2seq learn accurate semantic representation compare source content annotate summary short well write moreover share mean source content work supervise learn representation source content summary implementation regard summary autoencoder assistant supervisor seq2seq follow previous work evaluate model popular chinese social media dataset experimental result show model achieve state art performances benchmark dataset
sentence translate one correct sentence however exist neural machine translation model use one correct translations target correct sentence punish incorrect sentence train stage since correct translations one sentence share similar bag word possible distinguish correct translations incorrect ones bag word paper propose approach use sentence bag word target train stage order encourage model generate potentially correct sentence appear train set evaluate model chinese english translation dataset experiment show model outperform strong baselines bleu score four hundred and fifty-five
present machine learn approach rank first place arabic dialect identification adi close share task two thousand and eighteen vardial evaluation campaign propose approach combine several kernels use multiple kernel learn kernels base character p grams also know n grams extract speech phonetic transcripts also use kernel base dialectal embeddings generate audio record organizers learn stage independently employ kernel discriminant analysis kda kernel ridge regression krr preliminary experiment indicate krr provide better classification result approach shallow simple empirical result obtain two thousand and eighteen adi close share task prove achieve best performance furthermore top macro f1 score five thousand, eight hundred and ninety-two significantly better second best score five thousand, seven hundred and fifty-nine two thousand and eighteen adi share task accord statistical significance test perform organizers nevertheless obtain even better post competition result macro f1 score six thousand, two hundred and twenty-eight use audio embeddings release organizers competition similar approach include phonetic feature also rank first adi close share task two thousand and seventeen vardial evaluation campaign surpass second best method four hundred and sixty-two therefore conclude multiple kernel learn method best approach date arabic dialect identification
coreference resolution aim identify text mention refer real world entity state art end end neural coreference model consider text span document potential mention learn link antecedent possible mention paper propose improve end end coreference resolution system one use biaffine attention model get antecedent score possible mention two jointly optimize mention detection accuracy mention cluster log likelihood give mention cluster label model achieve state art performance conll two thousand and twelve share task english test set
semantic relations often signal prepositional possessive mark extreme polysemy bedevil analysis automatic interpretation introduce new annotation scheme corpus task disambiguation prepositions possessives english unlike previous approach annotations comprehensive respect type tokens markers use broadly applicable supersense class rather fine grain dictionary definitions unite prepositions possessives class inventory distinguish marker lexical contribution role mark context predicate scene strong interannotator agreement rat well encourage disambiguation result establish supervise methods speak viability scheme task
children learn first language face multiple problems induction learn mean word build meaningful phrase word accord syntactic rule consider children might solve problems efficiently solve jointly via computational model learn syntax semantics multi word utterances ground reference game select well study empirical case children aware pattern link syntactic semantic properties word properties pick base nouns tend relate shape prenominal adjectives tend refer properties color show children apply inductive bias accurately reflect statistics child direct speech induce similar bias computational model capture children behavior classic adjective learn experiment model incorporate bias also demonstrate clear data efficiency learn relative baseline model learn without form syntax sensitive overhypotheses word mean thus solve complex joint inference problem may make full problem language acquisition easier harder
date little work assess discourse coherence methods real world data address present new corpus real world texts gcdc well first large scale evaluation lead discourse coherence algorithms show neural model include two introduce sentavg parseq tend perform best analyze performance differences discuss pattern observe low coherence texts four domains
make multiple heterogeneous treebanks train monolingual dependency parser open question start investigate previously suggest little evaluate strategies exploit multiple treebanks base concatenate train set without fine tune go propose new method base treebank embeddings perform experiment several languages show many case fine tune treebank embeddings lead substantial improvements single treebanks concatenation average gain twenty thirty-five las point argue treebank embeddings prefer due conceptual simplicity flexibility extensibility
paper describe spot difference corpus contain fifty-four interactions pair subject interact find differences two similar scenes setup use participants metadata detail collection describe release corpus task orient spontaneous dialogues release include rich transcriptions annotations audio video believe dataset constitute valuable resource study several dimension human communication go turn take study refer expressions preliminary analyse look task success many differences find total number differences evolve time addition look scene complexity provide rgb components entropy could relate speech overlap interruptions expression uncertainty find tendency complex scenes competitive interruptions
present new open source parallel corpus consist news article collect bianet magazine online newspaper publish turkish news often along translations english kurdish paper describe collection process corpus statistical properties validate benefit use bianet corpus evaluate bilingual multilingual neural machine translation model english turkish english kurdish directions
goal sentiment sentiment translation change underlie sentiment sentence keep content main challenge lack parallel data solve problem propose cycle reinforcement learn method enable train unpaired data collaboration neutralization module emotionalization module evaluate approach two review datasets yelp amazon experimental result show approach significantly outperform state art systems especially propose method substantially improve content preservation performance bleu score improve one hundred and sixty-four two thousand, two hundred and forty-six fifty-six one thousand, four hundred and six two datasets respectively
propose efficient dynamic oracle train two planar transition base parser linear time parser ninety-nine coverage non projective syntactic corpora novel approach outperform static train strategy vast majority languages test score better datasets arc hybrid parser enhance swap transition handle unrestricted non projectivity
pitch accent detection often make use acoustic lexical feature base fact pitch accent tend correlate certain word paper extend pitch accent detector involve convolutional neural network include word embeddings state art vector representations word examine effect feature within corpus cross corpus experiment three english datasets result show word embeddings improve performance corpus dependent experiment also potential make generalization unseen data challenge
introduce novel graph base framework abstractive meet speech summarization fully unsupervised rely annotations work combine strengths multiple recent approach address weaknesses moreover leverage recent advance word embeddings graph degeneracy apply nlp take exterior semantic knowledge account design custom diversity informativeness measure experiment ami icsi corpus show system improve state art code data publicly available system interactively test
abstract mean representations amrs broad coverage sentence level semantic representations amrs represent sentence root label direct acyclic graph amr parse challenge partly due lack annotate alignments nod graph word correspond sentence introduce neural parser treat alignments latent variables within joint probabilistic model concepts relations alignments exact inference require marginalize alignments infeasible use variational auto encode framework continuous relaxation discrete alignments show joint model preferable use pipeline align parse parser achieve best report result standard benchmark seven hundred and forty-four ldc2016e25
paper describe win contribution semeval two thousand and eighteen task four character identification multiparty dialogues simple standard model one key innovation entity library result show innovation greatly facilitate identification infrequent character generic nature model find potentially relevant task require effective learn sparse unbalance data
paper present large scale corpus non task orient dialogue response selection contain 27k distinct prompt 82k responses collect social media annotate corpus define five grade rat scheme bad mediocre acceptable good excellent accord relevance coherence informativeness interestingness potential move conversation forward test validity usefulness produce corpus compare various unsupervised supervise model response selection experimental result confirm propose corpus helpful train response selection model
simplify sentence attentive neural network sequence sequence model dub s4 model include novel word copy mechanism loss function exploit linguistic similarities original simplify sentence also jointly use pre train fine tune word embeddings capture semantics complex sentence mitigate effect limit data train evaluate pair sentence thousands news article observe eighty-eight point improvement bleu score sequence sequence baseline however learn word substitutions remain difficult sequence sequence model promise text generation task style transfer
paper present first study aim capture stylistic similarity word unsupervised manner propose extend continuous bag word cbow model mikolov et al two thousand and thirteen learn style sensitive word vectors use wider context window assumption style word utterance consistent addition introduce novel task predict lexical stylistic similarity create benchmark dataset task experiment dataset support assumption demonstrate propose extensions contribute acquisition style sensitive word embeddings
success many natural language process nlp task bind number quality annotate data often shortage train data paper ask question combine neural network nn regular expressions improve supervise learn nlp answer develop novel methods exploit rich expressiveness res different level within nn show combination significantly enhance learn effectiveness small number train examples available evaluate approach apply speak language understand intent detection slot fill experimental result show approach highly effective exploit available train data give clear boost unaware nn
propose novel neural method extract drug drug interactions ddis texts use external drug molecular structure information encode textual drug pair convolutional neural network molecular pair graph convolutional network gcns concatenate output two network experiment show gcns predict ddis molecular structure drug high accuracy molecular information enhance text base ddi extraction two hundred and thirty-nine percent point f score ddiextraction two thousand and thirteen share task data set
present architecture generate medical texts learn informative continuous representation discriminative feature train input system dataset caption medical x ray acquire continuous representations particular interest use many machine learn techniques discrete high dimensional nature textual input obstacle use adversarially regularize autoencoder create realistic text unconditional conditional set show technique applicable medical texts often contain syntactic domain specific shorthands quantitative evaluation show achieve lower model perplexity traditional lstm generator
reformulate problem encode multi scale representation sequence language model cast continuous learn framework propose hierarchical multi scale language model short time scale dependencies encode hide state lower level recurrent neural network longer time scale dependencies encode dynamic lower level network meta learner update weight lower level neural network online meta learn fashion use elastic weight consolidation higher level prevent catastrophic forget continuous learn framework
recent developments field biomedicine make large volumes biomedical literature available medical practitioners due large size lack efficient search strategies medical practitioners struggle obtain necessary information available biomedical literature moreover sophisticate search engines age intelligent enough interpret clinicians question facts reflect urgent need information retrieval system accept query medical practitioners natural language return answer quickly efficiently paper present implementation machine intelligence base clinical question answer system cliniqa answer medical practitioner question system rigorously evaluate different text mine algorithms best components system select system make use unify medical language system semantic analysis question medical document addition system employ supervise machine learn algorithms classification document identify focus question answer selection effective domain specific heuristics design answer rank performance evaluation hundred clinical question show effectiveness approach
study task generate wikipedia article question answer pair cover content beyond single sentence propose neural network approach incorporate coreference knowledge via novel gate mechanism compare model take account sentence level information heilman smith two thousand and ten du et al two thousand and seventeen zhou et al two thousand and seventeen find linguistic knowledge introduce coreference representation aid question generation significantly produce model outperform current state art apply system compose answer span extraction system passage level qg system ten thousand top rank wikipedia article create corpus one million question answer pair also provide qualitative analysis large scale generate corpus wikipedia
understand social power structure affect way interact one another great interest social scientists want answer fundamental question human behavior well computer scientists want build automatic methods infer social contexts interactions paper employ advancements extra propositional semantics extraction within nlp study author commitment reflect social context interaction specifically investigate whether level commitment express individuals organizational interaction reflect hierarchical power structure part find subordinate use significantly instance non commitment superiors importantly also find subordinate attribute proposition agents often superiors aspect study finally show enrich lexical feature commitment label capture important distinctions social mean
recurrent neural network rnns powerful autoregressive sequence model use generate natural language output tend overly generic repetitive self contradictory postulate objective function optimize rnn language model amount overall perplexity text expressive enough capture notion communicative goals describe linguistic principles grice maxims propose learn mixture multiple discriminative model use complement rnn generator guide decode process human evaluation demonstrate text generate system prefer baselines large margin significantly enhance overall coherence style information content generate text
real world language problems require learn heterogenous corpora raise problem learn robust model generalise well similar domain dissimilar domain instance see train require learn underlie task learn irrelevant signal bias specific individual domains propose novel method optimise domain accuracy base joint learn structure neural model domain specific domain general components couple adversarial train domain evaluate multi domain language identification multi domain sentiment analysis show substantial improvements standard domain adaptation techniques domain adversarial train
write text often provide sufficient clue identify author gender age important attribute consequently authorship train evaluation corpora unforeseen impact include differ model performance different user group well privacy implications paper propose approach explicitly obscure important author characteristics train time representations learn invariant attribute evaluate two task show lead increase privacy learn representations well robust model vary evaluation condition include domain corpora
story comprehension require deep semantic understand narrative make challenge task inspire previous study roc story cloze test propose novel method track various semantic aspects external neural memory chain encourage focus particular semantic aspect evaluate task story end prediction model demonstrate superior performance collection competitive baselines set new state art
small perturbations input severely distort intermediate representations thus impact translation quality neural machine translation nmt model paper propose improve robustness nmt model adversarial stability train basic idea make encoder decoder nmt model robust input perturbations enable behave similarly original input perturb counterpart experimental result chinese english english german english french translation task show approach achieve significant improvements strong nmt systems also improve robustness nmt model
sophisticate neural base techniques develop read comprehension approach model answer independent manner ignore relations answer candidates problem even worse open domain scenarios candidates multiple passages combine answer single question paper formulate read comprehension extract select two stage procedure first extract answer candidates passages select final answer combine information candidates furthermore regard candidate extraction latent variable train two stage process jointly reinforcement learn result approach improve state art performance significantly two challenge open domain read comprehension datasets analysis demonstrate effectiveness model components especially information fusion candidates joint train extract select procedure
gibson et al two thousand and seventeen argue color name shape pattern communicative need support claim show color name systems across languages support precise communication warm color cool color object talk tend warm color rather cool color present new analyse alter picture show greater communicative precision warm cool color greater communicative need may explain perceptual structure however use information theoretic analysis also show color name across languages bear sign communicative need beyond would predict perceptual structure alone conclude color name shape perceptual structure traditionally argue pattern communicative need argue gibson et al although reason advance
propose unify model combine strength extractive abstractive summarization one hand simple extractive model obtain sentence level attention high rouge score less readable hand complicate abstractive model obtain word level dynamic attention generate readable paragraph model sentence level attention use modulate word level attention word less attend sentence less likely generate moreover novel inconsistency loss function introduce penalize inconsistency two level attentions end end train model inconsistency loss original losses extractive abstractive model achieve state art rouge score informative readable summarization cnn daily mail dataset solid human evaluation
paper introduce rule base approach annotate locative directional expressions arabic natural language text annotation base construct semantic map spatiality domain challenge twofold first need study locative directional expressions express linguistically texts second need automatically annotate relevant textual segment accordingly research method use article analytic descriptive validate approach specific novel rich expressions show promise result use nooj software tool implement finite state transducers annotate linguistic elements accord locative directional expressions conclusion nooj allow us write linguistic rule automatic annotation arabic text locative directional expressions
mine social media message health drug relate information receive significant interest pharmacovigilance research social media sit eg twitter use monitor drug abuse adverse reactions drug usage analyze expression sentiments relate drug study base aggregate result large population rather specific set individuals order conduct study individual level specific cohorts identify post mention intake medicine user necessary towards objective train different deep neural network classification model publicly available annotate dataset study performances identify mention personal intake medicine tweet also design train new architecture stack ensemble shallow convolutional neural network cnn ensembles use random search tune hyperparameters model share detail value take hyperparameters best learn model different deep neural network architectures system produce state art result micro average f score six hundred and ninety-three
literature automate sarcasm detection mainly focus lexical syntactic semantic level analysis text however sarcastic sentence express contextual presumptions background commonsense knowledge paper propose cascade contextual sarcasm detector adopt hybrid approach content context drive model sarcasm detection online social media discussions latter cascade aim extract contextual information discourse discussion thread also since sarcastic nature form expression vary person person cascade utilize user embeddings encode stylometric personality feature users use along content base feature extractors convolutional neural network cnns see significant boost classification performance large reddit corpus
different semantic interpretation task text entailment question answer require classification semantic relations term entities within text however case possible assign direct semantic relation entities term paper propose approach composite semantic relation classification extend traditional semantic relation classification task different exist approach use machine learn model build lexical distributional word vector feature propose model use combination large commonsense knowledge base binary relations distributional navigational algorithm sequence classification provide solution composite semantic relation classification problem
paper provide comparative analysis performance four state art distributional semantic model dsms eleven languages contrast native language specific model use machine translation english base dsms experimental result show significant improvement average one hundred and sixty-seven spearman correlation use state art machine translation approach result also show benefit use informative corpus outweigh possible errors introduce machine translation languages combination machine translation word2vec english distributional model provide best result consistently average spearman correlation sixty-eight
understand narrative require read line reason unspoken obvious implications events people mental state capability trivial humans remarkably hard machine facilitate research address challenge introduce new annotation framework explain naive psychology story character fully specify chain mental state respect motivations emotional reactions work present new large scale dataset rich low level annotations establish baseline performance several new task suggest avenues future research
one possible ways obtain continuous space sentence representations train neural machine translation nmt systems recent attention mechanism however remove single point neural network source sentence representation extract propose several variations attentive nmt architecture bring meet point back empirical evaluation suggest better translation quality worse learn sentence representations serve wide range classification similarity task
natural language generation lie core generative dialogue systems conversational agents describe ensemble neural language generator present several novel methods data representation augmentation yield improve result model test model three datasets restaurant tv laptop domains report objective subjective evaluations best model use range automatic metrics well human evaluators show approach achieve better result state art model datasets
revisit domain adaptation parsers neural era first show recent advance word representations greatly diminish need domain adaptation target domain syntactically similar source domain evidence train parser wall street jour nal alone achieve ninety f1 brown corpus syntactically dis tant domains provide simple way adapt parser use dozens partial annotations instance increase percentage error free geometry domain parse hold set forty-five seventy-three use approximately five dozen train examples process demon strate new state art single model result wall street journal test set nine hundred and forty-three absolute increase seventeen previous state art nine hundred and twenty-six
online petition cost effective way citizens collectively engage policy makers democracy predict popularity petition commonly measure signature count base textual content utility policy makers well post petition work model task use cnn regression auxiliary ordinal regression objective demonstrate effectiveness propose approach use uk us government petition datasets
argue extrapolation examples outside train space often easier model capture global structure rather maximise local fit train data show true two popular model decomposable attention model word2vec
deep learn research relation classification achieve solid performance general domain study propose convolutional neural network cnn architecture multi pool operation medical relation classification clinical record explore loss function category level constraint matrix experiment use two thousand and ten i2b2 va relation corpus demonstrate model depend external feature outperform previous single model methods best model competitive exist ensemble base method
investigate new commonsense inference task give event describe short free form text x drink coffee morning system reason likely intents x want stay awake reactions x feel alert event participants support study construct new crowdsourced corpus twenty-five thousand event phrase cover diverse range everyday events situations report baseline performance task demonstrate neural encoder decoder model successfully compose embed representations previously unseen events reason likely intents reactions event participants addition demonstrate commonsense inference people intents reactions help unveil implicit gender inequality prevalent modern movie script
present new dataset model comprehend paragraph process eg photosynthesis important genre text describe dynamic world new dataset propara first contain natural rather machine generate text change world along full annotation entity state location existence change 81k datapoints end task track location existence entities text challenge causal effect action often implicit need infer find previous model work well synthetic data achieve mediocre performance propara introduce two new neural model exploit alternative mechanisms state prediction particular use lstm input encode span prediction new model improve accuracy nineteen dataset model available community http dataallenaiorg propara
recently span base constituency parse achieve competitive accuracies extremely simple model use bidirectional rnns model span however minimal span parser stern et al 2017a hold current state art accuracy chart parser run cubic time ofn3 slow longer sentence applications beyond sentence boundaries end end discourse parse joint sentence boundary detection parse propose linear time constituency parser rnns dynamic program use graph structure stack beam search run time ofn b2 b beam size speed ofn blog b integrate cube prune compare chart parse baselines linear time parser substantially faster long sentence penn treebank order magnitude faster discourse parse achieve highest f1 accuracy penn treebank among single model end end systems
aspect base sentiment analysis absa provide detail information general sentiment analysis aim predict sentiment polarities give aspects entities text summarize previous approach two subtasks aspect category sentiment analysis acsa aspect term sentiment analysis atsa previous approach employ long short term memory attention mechanisms predict sentiment polarity concern target often complicate need train time propose model base convolutional neural network gate mechanisms accurate efficient first novel gate tanh relu units selectively output sentiment feature accord give aspect entity architecture much simpler attention layer use exist model second computations model could easily parallelize train convolutional layer time dependency lstm layer gate units also work independently experiment semeval datasets demonstrate efficiency effectiveness model
present novel neural architecture argument reason comprehension task semeval two thousand and eighteen simple neural network consist three part collectively judge whether logic build set give sentence claim reason warrant plausible model utilize contextualized word vectors pre train large machine translation mt datasets form transfer learn help mitigate lack train data quantitative analysis show simply leverage lstms train mt datasets outperform several baselines non transfer model achieve accuracies seventy development set sixty test set
neural machine translation nmt systems recently obtain state art many machine translation systems popular language pair availability data low resourced language pair research field due lack bilingual data paper attempt build first nmt systems low resourced language pairsjapanese vietnamese also show significant improvements combine advance methods reduce adverse impact data sparsity improve quality nmt systems addition propose variant byte pair encode algorithm perform effective word segmentation vietnamese texts alleviate rare word problem persist nmt systems
task obfuscate write style use sequence model previously investigate framework obfuscation transfer input text explicitly rewrite another style approach also often lead major alterations semantic content input work propose obfuscation invariance investigate extent model train explicitly style invariant preserve semantics evaluate architectures parallel non parallel corpora compare automatic human evaluations obfuscate sentence experiment show style classifier performance reduce chance level whilst automatic evaluation output seemingly equal model apply style transfer however base human evaluation demonstrate trade level obfuscation observe quality output term mean preservation grammaticality
dialog act recognition important step dialog systems since reveal intention behind utter word approach task use word level tokenization contrast paper explore use character level tokenization relevant since information sub word level relate function word thus intention also explore use different context windows around token able capture important elements affix furthermore assess importance punctuation capitalization perform experiment switchboard dialog act corpus dihana corpus case experiment show character level tokenization lead better performance typical word level approach also approach able capture complementary information thus best result achieve combine tokenization level
sentence representations capture wide range information capture local feature base character word n grams paper examine usefulness universal sentence representations evaluate quality machine translation although difficult train sentence representations use small scale translation datasets manual evaluation sentence representations train large scale data task improve automatic evaluation machine translation experimental result wmt two thousand and sixteen dataset show propose method achieve state art performance sentence representation feature
work distinguish translate original text un protocol corpus model problem classification problem achieve ninety-five classification accuracy begin derive parallel corpus different language pair annotate translation direction classify data use various feature extraction methods compare different methods well ability distinguish translate original texts different languages annotate corpus publicly available
work present new state art reconstruction surface realizations obfuscate text identify lack sufficient train data major obstacle train high perform model solve issue generate large amount synthetic train data also propose preprocessing techniques make structure contain input feature accessible sequence model model rank first evaluation metrics english portion two thousand and eighteen surface realization share task
propose new deep neural network model train scheme text classification model sequence convolution neural networksseq2cnn consist two block sequential block summarize input texts convolution block receive summary input classify label seq2cnn train end end classify various length texts without preprocessing input fix length also present gradual weight shiftgws method stabilize train gws apply model loss function compare model word base textcnn train different data preprocessing methods obtain significant improvement classification accuracy word base textcnn without ensemble data augmentation
recent advance neural network architecture train algorithms show effectiveness representation learn neural network base model generate better representation traditional ones ability automatically learn distribute representation sentence document end propose novel model address several issue adequately model previously propose model memory problem incorporate knowledge document structure model use hierarchical structure self attention mechanism create sentence document embeddings architecture mirror hierarchical structure document turn enable us obtain better feature representation attention mechanism provide extra source information guide summary extraction new model treat summarization task classification problem model compute respective probabilities sentence summary membership model predictions break several feature information content salience novelty positional representation propose model evaluate two well know datasets cnn daily mail duc two thousand and two experimental result show model outperform current extractive state art considerable margin
propose novel two layer attention network base bidirectional long short term memory sentiment analysis novel two layer attention network take advantage external knowledge base improve sentiment prediction use knowledge graph embed generate use wordnet build model combine two layer attention network supervise model base support vector regression use multilayer perceptron network sentiment analysis evaluate model benchmark dataset semeval two thousand and seventeen task five experimental result show propose model surpass top system semeval two thousand and seventeen task five model perform significantly better improve state art system semeval two thousand and seventeen task five seventeen thirty-seven point sub track one two respectively
paper report practical application novel approach validate knowledge wordnet use adimen sumo particular paper focus cross check wordnet meronymy relations knowledge encode adimen sumo validation approach test large set competency question cqs derive semi automatically knowledge encode wordnet sumo map apply efficient first order logic automate theorem provers unfortunately despite create manually knowledge resources free errors discrepancies consequence result cqs plausible accord knowledge include adimen sumo thus first focus semi automatically improve alignment knowledge resources second perform minimal set corrections ontology aim minimize manual effort require extensive validation process report strategies follow change make effort need impact validate wordnet meronymy relations use improve versions map ontology base new result discuss implications appropriate corrections need future enhancements
introduce neural read comprehension model integrate external commonsense knowledge encode key value memory cloze style set instead rely document question interaction discrete feature prior work model attend relevant external knowledge combine knowledge context representation infer answer allow model attract imply knowledge external knowledge source explicitly state text relevant infer answer model improve result strong baseline hard common nouns dataset make strong competitor much complex model include knowledge explicitly model also provide evidence background knowledge use rc process
different word embed model capture different aspects linguistic properties inspire us propose model maxlstm cnn employ multiple set word embeddings evaluate sentence similarity relation represent word multiple word embeddings maxlstm cnn encoder generate novel sentence embed learn similarity relation sentence embeddings via multi level comparison method maxlstm cnn consistently show strong performances several task ie measure textual similarity identify paraphrase recognize textual entailment accord experimental result sts benchmark dataset sick dataset semeval maxlstm cnn outperform state art methods textual similarity task model use hand craft feature eg alignment feature ngram overlap dependency feature well require pre train word embeddings dimension
aspect term extraction one important subtasks aspect base sentiment analysis previous study show use dependency tree structure representation promise task however dependency tree structure involve one directional propagation dependency tree paper first propose novel bidirectional dependency tree network extract dependency structure feature give sentence key idea explicitly incorporate representations gain separately bottom top propagation give dependency syntactic tree end end framework develop integrate embed representations bilstm plus crf learn tree structure sequential feature solve aspect term extraction problem experimental result demonstrate propose model outperform state art baseline model four benchmark semeval datasets
introduce morse recurrent encoder decoder model produce morphological analyse word sentence encoder turn relevant information word context fix size vector representation decoder generate sequence character lemma follow sequence individual morphological feature show generate morphological feature individually rather combine tag allow model handle rare unseen tag outperform whole tag model addition generate morphological feature sequence rather eg unordered set allow model produce arbitrary number feature represent multiple inflectional group morphologically complex languages obtain state art result nine languages different morphological complexity low resource high resource transfer learn settings also introduce trmor2018 new high accuracy turkish morphology dataset morse implementation trmor2018 dataset available online support future researchfootnotesee urlhttps githubcom ai ku morsejl morse implementation julia knet citeknet2016mlsys urlhttps githubcom ai ku trmor2018 new turkish dataset
paper present state art model introduce new dataset ground language learn goal develop model learn follow new instructions give prior instruction perception action examples base work sail dataset consist navigational instructions action maze like environment new model propose achieve best result date sail dataset use improve perceptual component represent relative position object also analyze problems sail dataset regard size balance argue performance small fix size dataset longer good measure differentiate state art model introduce sailx synthetic dataset generator perform experiment size balance dataset control
word sense disambiguation wsd aim identify correct mean polysemous word particular context lexical resources like wordnet prove great help wsd knowledge base methods however previous neural network wsd always rely massive label data context ignore lexical resources like gloss sense definitions paper integrate context gloss target word unify framework order make full use label data lexical knowledge therefore propose gas gloss augment wsd neural network jointly encode context gloss target word gas model semantic relationship context gloss improve memory network framework break barriers previous supervise methods knowledge base methods extend original gloss word sense via semantic relations wordnet enrich gloss information experimental result show model outperform state theart systems several english word wsd datasets
present poetwannabe chatbot submit university wroclaw nip two thousand and seventeen conversational intelligence challenge rank first ex aequo able conduct conversation user natural language primary functionality dialogue system context aware question answer qa secondary function maintain user engagement chatbot compose number sub modules independently prepare reply user prompt assess confidence answer question dialogue system rely heavily factual data source mostly wikipedia dbpedia data real user interactions public forums well data concern general literature applicable modules train large datasets use gpus however comply competition requirements final system compact run commodity hardware
neural model question answer qa document achieve significant performance improvements although effective model scale large corpora due complex model interactions document question moreover recent work show model sensitive adversarial input paper study minimal context require answer question find question exist datasets answer small set sentence inspire observation propose simple sentence selector select minimal set sentence fee qa model overall system achieve significant reductions train fifteen time inference time thirteen time accuracy comparable better state art squad newsqa triviaqa squad open furthermore experimental result analyse show approach robust adversarial input
traditionally refer expression generation reg model first decide form content reference discourse entities text typically rely feature salience grammatical function paper present new approach neuralreg rely deep neural network make decisions form content one go without explicit feature extraction use delexicalized version webnlg corpus show neural model substantially improve two strong baselines data model publicly available
computational approach historical linguistics propose since half century within last decade line research receive major boost owe transfer ideas software computational biology release several large electronic data resources suitable systematic comparative work article central research topic new wave computational historical linguistics introduce discuss automatic assessment genetic relatedness automatic cognate detection phylogenetic inference ancestral state reconstruction demonstrate mean case study automatically reconstruct proto romance word list lexical data fifty modern romance languages dialects
rise neural network particularly recurrent neural network produce significant advance part speech tag accuracy one characteristic common among model presence rich initial word encode encode typically compose recurrent character base representation learn pre train word embeddings however encode consider context wider single word subsequent recurrent layer word sub word information interact paper investigate model use recurrent neural network sentence level context initial character word base representations particular show optimal result obtain integrate context sensitive representations synchronize train meta model learn combine state present result part speech morphological tag state art performance number languages
nmt word sometimes drop source generate repeatedly translation explore novel strategies address coverage problem change attention transformation approach allocate fertilities source word use bind attention word receive experiment various sparse constrain attention transformations propose new one constrain sparsemax show differentiable sparse empirical evaluation provide three languages pair
cross lingual information extraction clie important challenge task especially low resource scenarios tackle challenge propose train method call halo enforce local region hide state neural model generate target tokens semantic structure tag simple powerful technique enable neural model learn semantics aware representations robust noise without introduce extra parameter thus yield better generalization high low resource settings
sentence pair model critical many nlp task paraphrase identification semantic textual similarity natural language inference state art neural model task rely pretrained word embed compose sentence level semantics vary ways however work attempt verify whether really need pretrained embeddings task paper study effective subword level character character n gram representations sentence pair model though well know subword model effective task single sentence input include language model machine translation systematically study sentence pair model task semantic string similarities texts matter experiment show subword model without pretrained word embed achieve new state art result two social media datasets competitive result news data paraphrase identification
natural language generators task orient dialogue must effectively realize system dialogue action associate semantics many applications also desirable generators control style utterance date work task orient neural generation primarily focus semantic fidelity rather achieve stylistic goals work style do contexts difficult measure content preservation present three different sequence sequence model carefully test well disentangle content style use statistical generator personage synthesize new corpus eighty-eight thousand restaurant domain utterances whose style vary accord model personality give us total control semantic content stylistic variation train data vary amount explicit stylistic supervision give three model show explicit model simultaneously achieve high fidelity semantic stylistic goals model add context vector thirty-six stylistic parameters input hide state encoder time step show benefit explicit stylistic supervision even amount train data large
learn sentence vectors generalise well challenge task paper compare three methods learn phrase embeddings one use lstms two use recursive net three variant method two use pos information phrase train model dictionary definitions word obtain reverse dictionary application similar felix et al one see embeddings transfer new task also train test rotten tomatoes dataset two train keep sentence embeddings fix well fine tune
user generate texts review social media valuable source information online review important assets users buy product see movie make decision therefore rat review one reliable factor users read trust review paper analyze texts review evaluate predict rat moreover study effect lexical feature generate text well sentimental word accuracy rat prediction analysis show word high information gain score efficient compare word high tf idf value addition explore best number feature predict rat review
multi word expressions verb particle constructions idiomatically combine phrase phrasal idioms something common elements contribute argument structure predicate implicate expression radically lexicalize theories grammar avoid string term logical form tree write categorial grammars avoid wrap operation make predictions categories involve verb particles phrasal idioms may require singleton type substitute one value one kind value type asymmetric arguments also narrowly constrain kind semantic value correspond syntactic categories idiomatically combine phrase subcategorize singleton type exploit another locally computable compositional property correspondence every syntactic expression project head word mwes see empirically realize categorial possibilities rather lacuna theory lexicalizable syntactic categories
sentiment analysis arabic challenge task due rich morphology language moreover task complicate apply twitter data know highly informal noisy paper develop hybrid method sentiment analysis arabic tweet specific arabic dialect saudi dialect several feature engineer evaluate use feature backward selection method hybrid method combine corpus base lexicon base method develop several classification model two way three way four way best f1 score model sixty-nine billion, nine hundred and sixty-one million, six hundred and thirty-five thousand, five hundred and seven respectively
multimodal affective compute learn recognize interpret human affect subjective information multiple data source still challenge hard extract informative feature represent human affect heterogeneous input ii current fusion strategies fuse different modalities abstract level ignore time dependent interactions modalities address issue introduce hierarchical multimodal architecture attention word level fusion classify utter ance level sentiment emotion text audio data introduce model outperform state art approach publish datasets demonstrate model able visualize interpret synchronize attention modalities
build tool code mix data rapidly gain popularity nlp research community data exponentially rise social media work code mix data contain several challenge especially due grammatical inconsistencies spell variations addition previous know challenge social media scenarios article present novel architecture focus normalize phonetic type variations commonly see code mix data one main feature architecture addition normalize also utilize back transliteration word identification case model achieve accuracy nine thousand and twenty-seven test data
intent classification widely research english data deep learn approach base neural network word embeddings challenge chinese intent classification stem fact unlike english word make twenty-six phonologic alphabet letter chinese logographic chinese character basic semantic unit informative mean vary much contexts chinese word embeddings alone inadequate represent word pre train embeddings suffer align well task hand account inadequacy leverage chinese character information propose low effort generic way dynamically integrate character embed base feature map word embed base input whose result word character embeddings stack contextual information extraction module incorporate context information predictions top propose model employ ensemble method combine single model obtain final result approach data independent without rely external source like pre train word embeddings propose model outperform baseline model exist methods
use sequence sequence framework many neural conversation model chit chat succeed naturalness response nevertheless neural conversation model tend give generic responses specific give message still remain challenge alleviate tendency propose method promote message relevant diverse responses neural conversation model use self attention time efficient well effective furthermore present investigation effective self attention deep comparison standard dialogue generation experiment result show propose method improve standard dialogue generation various evaluation metrics
non projective parse useful handle cycle reentrancy amr graph explore idea introduce greedy leave right non projective transition base parser parse configuration oracle decide whether create concept whether connect pair exist concepts algorithm handle reentrancy arbitrary cycle natively ie within transition system model evaluate ldc2015e86 corpus obtain result close state art include smatch sixty-four show good behavior reentrant edge
sentiment analysis low resource languages suffer lack annotate corpora estimate high perform model machine translation bilingual word embeddings provide relief cross lingual sentiment approach however either require large amount parallel data sufficiently capture sentiment information introduce bilingual sentiment embeddings blse jointly represent sentiment information source target language model require small bilingual lexicon source language corpus annotate sentiment monolingual word embeddings language perform experiment three language combinations spanish catalan basque sentence level cross lingual sentiment classification find model significantly outperform state art methods four six experimental setups well capture complementary information machine translation analysis result embed space provide evidence represent sentiment information resource poor target language without annotate data language
usage part day nouns night time specific greet good night vary across languages culture show possibilities twitter offer study semantics term variability countries mine worldwide sample multilingual tweet temporal greet study frequencies vary relation local time result provide insights semantics temporal expressions cultural sociological factor influence usage
paper investigate use machine translation mt bootstrap natural language understand nlu system new language use case large scale voice control device goal decrease cost time need get annotate corpus new language still large enough coverage user request different methods filter mt data order keep utterances improve nlu performance language specific post process methods investigate methods test large scale nlu task translate around ten millions train utterances english german result show large improvement use mt data grammar base house data collection baseline reduce manual effort greatly filter post process approach improve result
paper describe participation first share task word sense induction disambiguation russian language russe two thousand and eighteen panchenko et al two thousand and eighteen several dozens ambiguous word participants ask group text fragment contain accord sense word provide beforehand therefore induction part task instance word bank set text fragment also know contexts word occur eg bank financial institution accept deposit river bank slope beside body water give participant ask cluster contexts unknown advance number cluster correspond case company area sense word bank organizers propose three evaluation datasets vary complexity text genres base respectively texts wikipedia web page dictionary russian language present two experiment positive negative one base respectively cluster contexts represent weight average word embeddings machine translation use two state art production neural machine translation systems team show second best result two datasets third best result remain one dataset among eighteen participate team manage substantially outperform competitive state art baselines previous years base sense embeddings
last years lot interest achieve kind complex reason use deep neural network model like memory network memnns combine external memory storages attention mechanisms architectures however lack complex reason mechanisms could allow instance relational reason relation network rns hand show outstanding result relational reason task unfortunately computational cost grow quadratically number memories something prohibitive larger problems solve issue introduce work memory network memnn architecture novel work memory storage reason module model retain relational reason abilities rn reduce computational complexity quadratic linear test model text qa dataset babi visual qa dataset nlvr jointly train babi 10k set new state art achieve mean error less five moreover simple ensemble two model solve twenty task joint version benchmark
prepositions among frequent word english play complex roles syntax semantics sentence surprisingly pose well know difficulties automatic process sentence prepositional attachment ambiguities idiosyncratic use phrase exist methods preposition representation treat prepositions different content word eg word2vec glove addition recent study aim solve prepositional attachment preposition selection problems depend heavily external linguistic resources use dataset specific word representations paper use word triple count one triple preposition capture preposition interaction attachment complement derive preposition embeddings via tensor decomposition large unlabeled corpus reveal new geometry involve hadamard products empirically demonstrate utility paraphrase phrasal verbs furthermore preposition embeddings use simple feature two challenge downstream task preposition selection prepositional attachment disambiguation achieve result comparable better state art multiple standardize datasets
dyadic interactions among humans mark speakers continuously influence react term responses behaviors among others understand interpersonal dynamics affect behavior important successful treatment psychotherapy domains traditional scheme automatically identify behavior purpose often look target speaker work propose markov model target speaker behavior influence past behavior well perception partner behavior base lexical feature apart incorporate additional potentially useful information model also control degree partner affect target speaker evaluate propose model task classify negative behavior couple therapy show accurate single speaker model furthermore investigate degree optimal influence relate well couple long term via relate relationship outcomes
present computational analysis cognate effect spontaneous linguistic productions advance non native speakers introduce large corpus highly competent non native english speakers use set carefully select lexical items show lexical choices non natives affect cognates native language effect powerful able reconstruct phylogenetic language tree indo european language family solely frequencies specific lexical items english author various native languages quantitatively analyze non native lexical choice highlight cognate facilitation one important phenomena shape language non native speakers
present new methodology high quality label fashion domain crowd workers instead experts focus aspect base sentiment analysis task methods filter inaccurate input crowd workers preserve different worker label capture inherent high variability opinions demonstrate quality label data base facebook fasttext framework baseline
book power make us feel happiness sadness pain surprise sorrow author dexterity use emotions captivate readers make difficult put book paper model flow emotions book use recurrent neural network quantify usefulness predict success book obtain best weight f1 score sixty-nine predict book success multitask set simultaneously predict success genre book
cross lingual document classification aim train document classifier resources one language transfer different language without additional resources several approach propose literature current best practice evaluate subset reuters corpus volume two however subset cover languages english german french spanish almost publish work focus transfer english german addition observe class prior distributions differ significantly languages argue complicate evaluation multilinguality paper propose new subset reuters corpus balance class priors eight languages add italian russian japanese chinese cover languages different respect syntax morphology etc provide strong baselines language transfer directions use multilingual word sentence embeddings respectively goal offer freely available framework evaluate cross lingual document classification hope foster mean research important area
paper describe submissions efficiency track gpus workshop neural machine translation generation members university edinburgh adam mickiewicz university tilde university alicante focus efficient implementation recurrent deep learn model implement amun fast inference engine neural machine translation improve performance efficient mini batch algorithm fuse softmax operation k best extraction algorithm submissions use amun first second third fastest gpu efficiency track
distant supervision become standard method relation extraction however even though efficient method come cost result distantly supervise train sample often noisy combat noise recent state art approach focus select one best sentence calculate soft attention weight set sentence one specific entity pair however methods suboptimal false positive problem still key stumble bottleneck performance argue incorrectly label candidate sentence must treat hard decision rather deal soft attention weight paper describe radical solution explore deep reinforcement learn strategy generate false positive indicator automatically recognize false positives relation type without supervise information unlike removal operation previous study redistribute negative examples experimental result show propose strategy significantly improve performance distant supervision compare state art systems
distant supervision effectively label data relation extraction suffer noise label problem recent work mainly perform soft bag level noise reduction strategies find relatively better sample sentence bag suboptimal compare make hard decision false positive sample sentence level paper introduce adversarial learn framework name dsgan learn sentence level true positive generator inspire generative adversarial network regard positive sample generate generator negative sample train discriminator optimal generator obtain discrimination ability discriminator greatest decline adopt generator filter distant supervision train dataset redistribute false positive instance negative set way provide clean dataset relation classification experimental result show propose strategy significantly improve performance distant supervision relation extraction compare state art systems
neural machine translation nmt draw much attention due promise translation performance recently however several study indicate nmt often generate fluent unfaithful translations paper propose method alleviate problem use phrase table recommendation memory main idea add bonus word worthy recommendation nmt make correct predictions specifically first derive prefix tree accommodate candidate target phrase search phrase translation table accord source sentence construct recommendation word set match candidate target phrase previously translate target word nmt determine specific bonus value recommendable word use attention vector phrase translation probability finally integrate bonus value nmt improve translation result extensive experiment demonstrate propose methods obtain remarkable improvements strong attentionbased nmt
learn high quality domain word embeddings important achieve good performance many nlp task general purpose embeddings train large scale corpora often sub optimal domain specific applications however domain specific task often large domain corpora train high quality domain embeddings paper propose novel lifelong learn set domain embed perform new domain embed system see many past domains try expand new domain corpus exploit corpora past domains via meta learn propose meta learner characterize similarities contexts word many domain corpora help retrieve relevant data past domains expand new domain corpus experimental result show domain embeddings produce process improve performance downstream task
neural machine translation nmt drawback generate high frequency word owe computational cost softmax function output layer japanese english nmt japanese predicate conjugation cause increase vocabulary size example one verb many nineteen surface varieties research focus predicate conjugation compress vocabulary size japanese vocabulary list fill various form verbs propose methods use predicate conjugation information without discard linguistic information propose methods generate low frequency word deal unknown word two methods consider introduce conjugation information first consider token conjugation token second consider embed vector conjugation feature result use methods demonstrate vocabulary size compress approximately eight hundred and sixty-one tanaka corpus nmt model output word train data set furthermore bleu score improve ninety-one point japanese english translation thirty-two point english japanese translation aspec
standard machine translation systems process sentence isolation hence ignore extra sentential information even though extend context prevent mistake ambiguous case improve translation coherence introduce context aware neural machine translation model design way flow information extend context translation model control analyze experiment english russian subtitle dataset observe much capture model deal improve pronoun translation measure correspondences induce attention distributions coreference relations observe model implicitly capture anaphora consistent gain sentence pronouns need gendered translation beside improvements anaphoric case model also improve overall bleu context agnostic version seven simple concatenation context source sentence six
word order source target languages significantly influence translation quality machine translation preordering effectively address problem previous preordering methods require manual feature design make language dependent design costly paper propose preordering method recursive neural network learn feature raw input experiment show propose method achieve comparable gain translation quality state art method without manual feature design
propose learn approach map context dependent sequential instructions action address problem discourse state dependencies attention base model consider history interaction state world train start goal state without access demonstrations propose sestra learn algorithm take advantage single step reward observations immediate expect reward maximization evaluate scone domains show absolute accuracy improvements ninety-eight two hundred and fifty-three across domains approach use high level logical representations
high quality arguments essential elements human reason decision make process however effective argument construction challenge task human machine work study novel task automatically generate arguments different stance give statement propose encoder decoder style neural network base argument generation model enrich externally retrieve evidence wikipedia model first generate set talk point phrase intermediate representation follow separate decoder produce final argument base input keyphrases experiment large scale dataset collect reddit show model construct arguments topic relevant content popular sequence sequence generation model accord automatic evaluation human assessments
paper describe duluth urop systems participate semeval two thousand and eighteen task two multilingual emoji prediction rely variety ensembles make classifiers use naive bay logistic regression random forest use unigram bigram feature try offset skewness data use oversampling task evaluation result place us 19th forty-eight systems english evaluation 5th twenty-one spanish evaluation realize simple change preprocessing could significantly improve result make change attain result would place us sixth english evaluation second spanish
hypernym discovery task identify potential hypernyms give term hypernym generalize word super ordinate specific word paper explore several approach rely co occurrence frequencies word pair hearst pattern base regular expressions word embeddings create umbc corpus system babbage participate subtask 1a english place 6th nineteen systems identify concept hypernyms 12th eighteen systems entity hypernyms
paper describe umdsub system participate task two semeval two thousand and eighteen develop system predict emoji give raw text english tweet system multi channel convolutional neural network base subword embeddings representation tweet model improve character word base methods two system place 21st forty-eight participate systems official evaluation
present openseq2seq tensorflow base toolkit train sequence sequence model feature distribute mix precision train benchmarks machine translation speech recognition task show model build use openseq2seq give state art performance fifteen 3x less train time openseq2seq currently provide build block model solve wide range task include neural machine translation automatic speech recognition speech synthesis
ask effective question powerful social skill paper seek build computational model learn discriminate effective question ineffective ones arm capability future advance systems evaluate quality question provide suggestions effective question word create large scale real world dataset contain four hundred thousand question collect reddit ask anything thread thread resemble online press conference question compete attention host dataset enable development class computational model predict whether question answer develop new convolutional neural network architecture variable length context demonstrate efficacy model compare state art baselines human judge
forum thread lengthy rich content concise thread summaries benefit newcomers seek information participate discussion study however examine task forum thread summarization work make first attempt adapt hierarchical attention network thread summarization model draw recent development neural attention mechanisms build sentence thread representations use summarization result indicate propose approach outperform range competitive baselines redundancy removal step crucial achieve outstanding result
investigate new train paradigm extractive summarization traditionally human abstract use derive goldstandard label extraction units however label often inaccurate human abstract source document easily align word level paper convert human abstract set cloze style comprehension question system summaries encourage preserve salient source content useful answer question share common word abstract use reinforcement learn explore space possible extractive summaries introduce question focus reward function promote concise fluent informative summaries experiment show propose method effective surpass state art systems standard summarization dataset
website privacy policies long read difficult understand sophisticate language make privacy notice less effective people become even less will share personal information perceive privacy policy vague paper focus decode vagueness natural language process perspective thoroughly identify vague term linguistic scope remain elusive challenge work seek learn vector representations word privacy policies use deep neural network vector representations feed interactive visualization tool lstmvis test ability discover syntactically semantically relate vague term approach hold promise model understand language vagueness
student course feedback generate daily classrooms online course discussion forums traditionally instructors manually analyze responses costly manner work propose new approach summarize student course feedback base integer linear program ilp framework approach allow different student responses share co occurrence statistics alleviate sparsity issue experimental result student feedback corpus show approach outperform range baselines term rouge score human evaluation
teach large class remain great challenge primarily difficult attend student need timely manner automatic text summarization systems leverage summarize student feedback submit immediately lecture leave discover make good summary student responses work explore new methodology effectively extract summary phrase student responses phrase tag number students raise issue phrase evaluate along two dimension respect text content informative well form measure rouge metric additionally shall attend press student need measure newly propose metric work enable phrase base annotation highlight scheme new summarization task phrase base framework allow us summarize student responses set bullet point present instructor promptly
present novel abstractive summarization framework draw recent development treebank abstract mean representation amr framework source text parse set amr graph graph transform summary graph text generate summary graph focus graph graph transformation reduce source semantic graph summary graph make use exist amr parser assume eventual availability amr text generator framework data drive trainable specifically design particular domain experiment gold standard amr annotations system parse show promise result code available https githubcom summarization
paper present method design specific high order dependency factor linear chain conditional random field crfs name entity recognition ner name entities tend separate multiple outside tokens text thus first order crf well second order crf may innately lose transition information distant name entities propose design use outside label ner transmission medium precedent entity information crf empirical result apparently demonstrate possible exploit long distance label dependency original first order linear chain crf structure upon ner reduce computational loss rather second order crf
paper describe hypernym discovery system participation semeval two thousand and eighteen task nine aim discover best set candidate hypernyms input concepts entities give search space pre define vocabulary introduce neural network architecture concern task empirically study various neural network model build representations latent space word phrase evaluate model include convolutional neural network long short term memory network gate recurrent unit recurrent convolutional neural network also explore different embed methods include word embed sense embed better performance
large scale knowledge graph provide vast amount structure facts entities short textual description often useful succinctly characterize entity type unfortunately many knowledge graph entities lack textual descriptions paper introduce dynamic memory base network generate short open vocabulary description entity jointly leverage induce fact embeddings well dynamic context generate sequence word demonstrate ability architecture discern relevant information accurate generation type description pit system several strong baselines
investigate incorporation character base word representations standard cnn base relation extraction model experiment two common neural architectures cnn lstm learn word vector representations character embeddings task biocreative v cdr corpus extract relationships chemicals diseases show model exploit character base word representations improve model use information obtain state art result relative previous neural approach
present study describe submission semeval two thousand and eighteen task one affect tweet spanish approach aim demonstrate beneficial automatically generate additional train data translate train data languages ii apply semi supervise learn method find strong support approach model outperform regular model subtasks however create stepwise ensemble different model oppose simply average result increase performance place second ei reg second ei oc fourth v reg fifth v oc four spanish subtasks participate
machine translation systems require semantic knowledge grammatical understand neural machine translation nmt systems often assume information capture attention mechanism decoder ensure fluency recent work show incorporate explicit syntax alleviate burden model type knowledge however require parse expensive explore question syntax model need translation address issue introduce model simultaneously translate induce dependency tree way leverage benefit structure investigate syntax nmt must induce maximize performance show dependency tree one language pair dependent two improve translation quality
exist neural relation extraction nre model rely distant supervision suffer wrong label problems paper propose novel adversarial train mechanism instance relation extraction alleviate noise issue compare previous denoising methods propose method better discriminate informative instance noisy ones method also efficient flexible apply various nre architectures show experiment large scale benchmark dataset relation extraction denoising method effectively filter noisy instance achieve significant improvements compare state art model
present approach event coreference resolution develop general framework cluster use supervise representation learn propose neural network architecture novel cluster orient regularization core term objective function term encourage model create embeddings event mention amenable cluster use agglomerative cluster embeddings build event coreference chain within cross document coreference ecb corpus model obtain better result model require significantly pre annotate information work provide insight motivate result new general approach solve coreference cluster problems representation learn
follow detail presentation core conflictual relationship theme ccrt objective relevant methods describe verbalization visualization data also term data mine text mine knowledge discovery data correspondence analysis methodology also term geometric data analysis show case study comprehensive reveal computational efficiency depend analysis process structure illustrative reveal aspects case study relatively extensive dream report use geometric data analysis confirm validity ccrt method
encoder decoder model typically employ word frequently use train corpus reduce computational cost exclude noise however vocabulary set may still include word interfere learn encoder decoder model paper propose method select suitable word learn encoders utilize frequency also co occurrence information capture use hit algorithm apply propose method two task machine translation grammatical error correction japanese english translation method achieve bleu score fifty-six point baseline also outperform baseline method english grammatical error correction f05 measure one hundred and forty-eight point higher
despite impressive progress high resource settings neural machine translation nmt still struggle low resource domain scenarios often fail match quality phrase base translation propose novel technique combine back translation multilingual nmt improve performance difficult case technique train single model directions language pair allow us back translate source target monolingual data without require auxiliary model continue train augment parallel data enable cycle improvement single model incorporate source target parallel data improve translation directions byproduct model reduce train deployment cost significantly compare uni directional model extensive experiment show technique outperform standard back translation low resource scenarios improve quality cross domain task effectively reduce cost across board
many natural language process task model structure prediction solve search problem paper distill ensemble multiple model train different initialization single model addition learn match ensemble probability output reference state also use ensemble explore search space learn encounter state exploration experimental result two typical search base structure prediction task transition base dependency parse neural machine translation show distillation effectively improve single model performance final model achieve improvements one hundred and thirty-two las two hundred and sixty-five bleu score two task respectively strong baselines outperform greedy structure prediction model previous literatures
question answer complex question often model graph construction traversal task solver must build traverse graph facts answer explain give question multi hop inference show extremely challenge model able aggregate two facts overwhelm semantic drift tendency long chain facts quickly drift topic major barrier current inference model even elementary science question require average four six facts answer explain work empirically characterize difficulty build traverse graph sentence connect lexical overlap evaluate chance sentence aggregation quality nine thousand, seven hundred and eighty-four manually annotate judgments across knowledge graph build three free text corpora include study guide simple wikipedia demonstrate semantic drift tend high aggregation quality low four three highlight scenarios maximize likelihood meaningfully combine information
word several sense connotations evolve time due semantic shift closely relate word may gain different even opposite mean years evolution relevant study language cultural change tool currently available diachronic semantic analysis significant inherent limitations suitable real time analysis article demonstrate linearity random vectors techniques enable build time series congruent word embeddings semantic space compare combine linearly without loss precision time period detect diachronic semantic shift show approach yield time trajectories polysemous word amazon apple enable follow semantic drift gender bias across time reveal shift instantiations stable concepts hurricane president fast linear approach easily distribute many processors follow real time stream social media twitter facebook result time dependent semantic space combine simple additions subtractions
challenge task word embeddings capture emergent mean polarity combination individual word example exist approach word embeddings assign high probabilities word penguin fly frequently co occur fail capture fact occur opposite sense penguins fly hypothesize humans associate single polarity sentiment word word contribute overall polarity combination word depend upon word combine analogous behavior microscopic particles exist possible state time interfere give rise new state depend upon relative phase make use hilbert space representation particles quantum mechanics subscribe relative phase word complex number investigate two quantum inspire model derive mean combination word propose model achieve better performances state art non quantum model binary sentence classification task
sentence match widely use various natural language task natural language inference paraphrase identification question answer task understand logical semantic relationship two sentence require yet challenge although attention mechanism useful capture semantic relationship properly align elements two sentence previous methods attention mechanism simply use summation operation retain original feature enough inspire densenet densely connect convolutional network propose densely connect co attentive recurrent neural network layer use concatenate information attentive feature well hide feature precede recurrent layer enable preserve original co attentive feature information bottommost word embed layer uppermost recurrent layer alleviate problem ever increase size feature vectors due dense concatenation operations also propose use autoencoder dense concatenation evaluate propose architecture highly competitive benchmark datasets relate sentence match experimental result show architecture retain recurrent attentive feature achieve state art performances task
investigate use different syntactic dependency representations neural relation classification task compare conll stanford basic universal dependencies scheme compare syntax agnostic approach perform error analysis order gain better understand result
opennmt open source toolkit neural machine translation nmt system prioritize efficiency modularity extensibility goal support nmt research model architectures feature representations source modalities maintain competitive performance reasonable train requirements toolkit consist model translation support well detail pedagogical documentation underlie techniques opennmt use several production mt systems modify numerous research paper implement across several deep learn frameworks
present semantic parser abstract mean representations learn parse string tree representations compositional structure amr graph allow us use standard neural techniques supertagging dependency tree parse constrain linguistically principled type system present two approximative decode algorithms achieve state art accuracy outperform strong baselines
plethora entity link el approach recently develop many claim multilingual mag multilingual agdistis approach show recently outperform state art multilingual el seven languages demo extend mag support el forty different languages include especially low resources languages ukrainian greek hungarian croatian portuguese japanese korean demo rely online web service allow easy access entity link approach disambiguate dbpedia wikidata demo show use mag mean post request well use user friendly web interface data use demo available https hobbitdatainformatikuni leipzigde agdistis
paper discuss two exist approach correlation analysis automatic evaluation metrics human score area natural language generation experiment show depend usage system sentence level correlation analysis correlation result automatic score human judgments inconsistent
propose lightly supervise approach information extraction particular name entity classification combine benefit traditional bootstrapping ie use limit annotations interpretability extraction pattern robust learn approach propose representation learn algorithm iteratively learn custom embeddings multi word entities extract pattern match example entities per category demonstrate representation base approach outperform three state art bootstrapping approach two datasets conll two thousand and three ontonotes additionally use embeddings approach output globally interpretable model consist decision list rank pattern base proximity average entity embed give class show interpretable model perform close complete bootstrapping model prove representation learn use produce interpretable model small loss performance
examine prosodic entrainment cooperative game dialogs new feature set describe register pitch accent shape rhythmic aspects utterances well establish feature present entrainment profile detect within across dialog entrainment speakers gender role game turn feature set undergo entrainment different quantitative qualitative ways partly attribute different function furthermore interactions speaker gender role describer vs follower suggest gender dependent strategies cooperative solution orient interactions female describers entrain male describers least data suggest slight advantage latter strategy task success
previous approach multilingual semantic dependency parse treat languages independently without exploit similarities semantic structure across languages experiment new approach combine resources pair languages conll two thousand and nine share task build polyglot semantic role labeler notwithstanding absence parallel data dissimilarity annotations languages approach result improvement srl performance multiple languages monolingual baseline analysis polyglot model show advantageous lower resource settings
paper propose method automatically identify future events lebanon economy arabic texts challenge threefold first need build corpus arabic texts cover lebanon economy second need study future events express linguistically texts third need automatically identify relevant textual segment accordingly validate method construct corpus form web show promise result use slcsas system semantic analysis base contextual explorer method alkhalil morpho sys system morpho syntactic analysis
paraphrase plagiarism identification represent complex task give plagiarize texts intentionally modify several reword techniques accordingly paper introduce two new measure evaluate relatedness two give texts semantically inform similarity measure semantically inform edit distance measure able extract semantic information either external resource distribute representation word result informative feature train supervise classifier detect paraphrase plagiarism obtain result indicate propose metrics consistently good detect different type paraphrase plagiarism addition result competitive state art methods advantage represent much simple equally effective solution
recurrent neural network find success variety natural language process applications general model sequential data investigate properties natural language data affect lstm ability learn nonlinguistic task recall elements input find model train natural language data able recall tokens much longer sequence model train non language sequential data furthermore show lstm learn solve memorization task explicitly use subset neurons count timesteps input hypothesize pattern structure natural language data enable lstms learn provide approximate ways reduce loss understand effect different train data learnability lstms remain open question
binary classifiers often employ discriminators gin base unsupervised style transfer systems ensure transfer sentence similar sentence target domain one difficulty approach error signal provide discriminator unstable sometimes insufficient train generator produce fluent language paper propose new technique use target domain language model discriminator provide richer stable token level feedback learn process train generator minimize negative log likelihood nll generate sentence evaluate language model use continuous approximation discrete sample generator model train use back propagation end end fashion moreover empirical result show use language model structure discriminator possible forgo adversarial step train make process stable compare model previous work use convolutional neural network cnns discriminators show approach lead improve performance three task word substitution decipherment sentiment modification relate language translation
work propose adversarial learn method reward estimation reinforcement learn rl base task orient dialog model current rl base task orient dialog systems require access reward signal either user feedback user rat user rat however may always consistent available practice furthermore online dialog policy learn rl typically require large number query users suffer sample efficiency problem address challenge propose adversarial learn method learn dialog reward directly dialog sample reward use optimize dialog policy policy gradient base rl evaluation restaurant search domain show propose adversarial dialog learn method achieve advance dialog success rate compare strong baseline methods discuss covariate shift problem online adversarial dialog learn show address partial access user feedback
study sequential language game two players private information communicate achieve common goal game successful player must infer partner private information partner message ii generate message likely help goal iii reason pragmatically partner strategy propose model capture three characteristics demonstrate importance capture human behavior new goal orient dataset collect use crowdsourcing
entity resolution aim resolve repeat reference entity document form core component natural language process nlp research field possess immense potential improve performance nlp field like machine translation sentiment analysis paraphrase detection summarization etc area entity resolution nlp see proliferation research two separate sub areas namely anaphora resolution coreference resolution review article aim clarify scope two task entity resolution also carry detail analysis datasets evaluation metrics research methods adopt tackle nlp problem survey motivate aim provide reader clear understand constitute nlp problem issue require attention
social media become one main channel peo ple communicate share view society often detect view whether person favor neu tral towards give topic opinions social media useful various company present new dataset consist three thousand, five hundred and forty-five english hindi code mix tweet opinion towards demoneti sation implement india two thousand and sixteen follow large countrywide debate present baseline supervise classification system stance detection develop use dataset use various machine learn techniques achieve accuracy five hundred and eighty-seven ten fold cross validation
social media platforms like twitter facebook come two largest mediums use people express view ward different topics generation large user data make nlp task like sentiment analysis opinion mine much important use sarcasm texts social media become popular trend lately use sarcasm reverse mean polarity imply text pose challenge many nlp task task sarcasm detection text gain importance commer cial security service present first english hindi code mix dataset tweet mark presence sarcasm irony token also annotate language tag present baseline su pervised classification system develop use dataset achieve average f score seven hundred and eighty-four use random forest classifier perform ten fold cross validation
character level model become popular approach specially accessibility ability handle unseen data however little know ability reveal underlie morphological structure word crucial skill high level semantic analysis task semantic role label srl work train various type srl model use word character morphology level information analyze performance character compare word morphology several languages conduct depth error analysis morphological typology analyze strengths limitations character level model relate domain data train data size long range dependencies model complexity exhaustive analyse would light important characteristics character level model semantic capability
age social news important understand type reactions evoke news source various level credibility present work seek better understand users react trust deceptive news source across two popular different social media platforms end one develop model classify user reactions one nine type answer elaboration question etc two measure speed type reaction trust deceptive news source 108m twitter post 62m reddit comment show significant differences speed type reactions trust deceptive news source twitter far smaller differences reddit
name entity recognition ner among slu task usually extract semantic information textual document ner speech make pipeline process consist process first automatic speech recognition asr audio process ner asr output approach disadvantage error propagation metric tune asr systems sub optimal regard final task reduce space search asr output level know integrate approach outperform sequential ones apply paper present first study end end approach directly extract name entities speech though unique neural architecture way joint optimization able asr ner experiment carry french data easily accessible compose data distribute several evaluation campaign experimental result show end end approach provide better result f measure069 test data classical pipeline approach detect name entity categories f measure065
propose lstm base model hierarchical architecture name entity recognition code switch twitter data model use bilingual character representation transfer learn address vocabulary word order mitigate data noise propose use token replacement normalization 3rd workshop computational approach linguistic code switch share task achieve second place six thousand, two hundred and seventy-six harmonic mean f1 score english spanish language pair without use gazetteer knowledge base information
lack text data major issue code switch language model paper introduce multi task learn base language model share syntax representation languages leverage linguistic information tackle low resource data issue model jointly learn language model part speech tag code switch utterances way model able identify location code switch point improve prediction next word approach outperform standard lstm base language model improvement ninety-seven seventy-four perplexity seame phase phase ii dataset respectively
paper describe submissions marian team wnmt two thousand and eighteen share task investigate combinations teacher student train low precision matrix products auto tune methods optimize transformer model gpu cpu integrate methods new average attention network recently introduce faster transformer variant create number high quality high performance model gpu cpu dominate pareto frontier share task
examine various type noise parallel train data impact quality neural machine translation systems create five type artificial noise analyze degrade performance neural statistical machine translation find neural model generally harm noise statistical model one especially egregious type noise learn copy input sentence
despite long history name entity recognition ner task natural language process community previous work rarely study task conversational texts texts challenge contain lot word variations increase number vocabulary oov word high number oov word pose difficulty word base neural model meanwhile plenty evidence effectiveness character base neural model mitigate oov problem report empirical evaluation neural sequence label model character embed tackle ner task indonesian conversational texts experiment show one character model outperform word embed model four f1 point two character model perform better oov case improvement high fifteen f1 point three character model robust high oov rate
propose long short term memory lstm attention mechanism classify psychological stress self conduct interview transcriptions apply distant supervision automatically label tweet base hashtag content complement expand size corpus additional data use initialize model parameters fine tune use interview data improve model robustness especially expand vocabulary size bidirectional lstm model attention find best model term accuracy seven hundred and forty-one f score seven hundred and forty-three furthermore show distant supervision fine tune enhance model performance sixteen accuracy twenty-one f score attention mechanism help model select informative word
announce share task ucca parse english german french call participants submit systems ucca cross linguistically applicable framework semantic representation build extensive typological work support rapid annotation ucca pose challenge exist parse techniques exhibit reentrancy result dag structure discontinuous structure non terminal nod correspond complex semantic units give success recent semantic parse share task sdp amr expect task significant contribution advancement ucca parse particular semantic parse general furthermore exist applications semantic evaluation base ucca greatly benefit better automatic methods ucca parse competition website https competitionscodalaborg competitions nineteen thousand, one hundred and sixty
paper investigate ability artificial neural network judge grammatical acceptability sentence goal test linguistic competence introduce corpus linguistic acceptability cola set ten thousand, six hundred and fifty-seven english sentence label grammatical ungrammatical publish linguistics literature baselines train several recurrent neural network model acceptability classification find model outperform unsupervised model lau et al two thousand and sixteen cola error analysis specific grammatical phenomena reveal lau et al model learn systematic generalizations like subject verb object order however model test perform far human level wide range grammatical constructions
multi relational semantic similarity datasets define semantic relations two short texts multiple ways eg similarity relatedness yet systems date design capture relations target one relation time propose multi label transfer learn approach base lstm make predictions several relations simultaneously aggregate losses update parameters multi label regression approach jointly learn information provide multiple relations rather treat separate task approach outperform single task approach traditional multi task learn approach also achieve state art performance one relation human activity phrase dataset
incrementality ubiquitous human human interaction beneficial human computer interaction topic research different part nlp community mostly focus specific topic hand even though incremental systems deal similar challenge regardless domain survey consolidate categorize approach identify similarities differences computation data show trade off consider focus lie evaluate incremental systems standard metrics often fail capture incremental properties system come suitable evaluation scheme non trivial
perform text normalization ie transformation word write speak form use memory augment neural network addition dynamic memory access storage mechanism present neural architecture serve language agnostic text normalization system avoid kind unacceptable errors make lstm base recurrent neural network successfully reduce frequency mistake show novel architecture indeed better alternative propose system require significantly lesser amount data train time compute resources additionally perform data sample circumvent data sparsity problem semiotic class show sufficient examples particular class improve performance text normalization system although occurrences errors still remain certain semiotic class demonstrate memory augment network meta learn capabilities open many doors superior text normalization system
sequence sequence learn model still require several days reach state art performance large benchmark datasets use single machine paper show reduce precision large batch train speedup train nearly 5x single eight gpu machine careful tune implementation wmt fourteen english german translation match accuracy vaswani et al two thousand and seventeen five hours train eight gpus obtain new state art two hundred and ninety-three bleu train eighty-five minutes one hundred and twenty-eight gpus improve result two hundred and ninety-eight bleu train much larger paracrawl dataset wmt fourteen english french task obtain state art bleu four hundred and thirty-two eighty-five hours one hundred and twenty-eight gpus
ability model automatically detect dialogue act important step toward understand spontaneous speech instant message however difficult infer dialogue act surface utterance highly depend context utterance speaker linguistic knowledge especially arabic dialects paper propose statistical dialogue analysis model recognize utterance dialogue act use multi class hierarchical structure model automatically acquire probabilistic discourse knowledge dialogue corpus collect annotate manually multi genre egyptian call center extensive experiment conduct use support vector machine classifier evaluate system performance result attain term average f measure score nine hundred and twelve show propose approach moderately improve f measure approximately twenty
language decode study identify word representations use predict brain activity response novel word sentence anderson et al two thousand and sixteen pereira et al two thousand and eighteen unspoken assumption study process linguistic information transform share semantic space semantic representations use variety linguistic non linguistic task claim current study vastly underdetermine content representations algorithms brain deploy produce consume computational task design solve illustrate indeterminacy extension sentence decode experiment pereira et al two thousand and eighteen show standard evaluations fail distinguish language process model deploy different mechanisms optimize solve different task conclude suggest change brain decode paradigm support stronger claim neural representation
third oriental language recognition olr challenge ap18 olr introduce paper include data profile task evaluation principles follow events last two years namely ap16 olr ap17 olr challenge year focus challenge task include one short duration utterances two confuse languages three open set recognition previous events data ap18 olr also provide speechocean nsfc m2asr project baselines base vector model neural network construct participants reference report baseline result three task demonstrate three task truly challenge data free participants kaldi recipes baselines publish online
recent years emotion detection text become popular due vast potential applications market political science psychology human computer interaction artificial intelligence etc access huge amount textual data especially opinionated self expression text also play special role bring attention field paper review work do identify emotion expressions text argue although many techniques methodologies model create detect emotion text various reason make methods insufficient although essential need improve design architecture current systems factor complexity human emotions use implicit metaphorical language express lead us think purpose standard methodologies enough capture complexities important pay attention linguistic intricacies emotion expression
natural language inference nli task determine natural language hypothesis infer give premise justifiable manner nli propose benchmark task natural language understand exist model perform well standard datasets nli achieve impressive result across different genres text however extent model understand semantic content sentence unclear work propose evaluation methodology consist automatically construct stress test allow us examine whether systems ability make real inferential decisions evaluation six sentence encoder model stress test reveal strengths weaknesses model respect challenge linguistic phenomena suggest important directions future work area
availability large diachronic corpora provide impetus grow body quantitative research language evolution mean change central quantities research token frequencies linguistic elements texts change frequency take reflect popularity selective fitness element however corpus frequencies may change wide variety reason include purely random sample effect corpora compose contemporary media fiction texts within underlie topics ebb flow cultural socio political trend work introduce simple model control topical fluctuations corpora topical cultural advection model demonstrate provide robust baseline variability word frequency change time validate model diachronic corpus span two centuries carefully control artificial language change scenario use correct topical fluctuations historical time series finally use model show emergence new word typically correspond rise trend topic suggest lexical innovations occur due grow communicative need subspace lexicon topical cultural advection model use quantify
recently neural machine translation achieve remarkable progress introduce well design deep neural network encoder decoder framework optimization perspective residual connections adopt improve learn performance encoder decoder deep architectures advance attention connections apply well inspire success densenet model computer vision problems paper propose densely connect nmt architecture densenmt able train efficiently nmt propose densenmt allow dense connection create new feature encoder decoder also use dense attention structure improve attention quality experiment multiple datasets show densenmt structure competitive efficient
goal orient go dialogue systems colloquially know goal orient chatbots help users achieve predefined goal eg book movie ticket within close domain first step understand user goal use natural language understand techniques goal know bot must manage dialogue achieve goal conduct respect learn policy success dialogue system depend quality policy turn reliant availability high quality train data policy learn method instance deep reinforcement learn due domain specificity amount available data typically low allow train good dialogue policies master thesis introduce transfer learn method mitigate effect low domain data availability transfer learn base approach improve bot success rate twenty relative term distant domains double close domains compare model without transfer learn moreover transfer learn chatbots learn policy five ten time faster finally transfer learn approach complementary additional process warm start show joint application give best outcomes
latent tree learn model represent sentence compose word accord induce parse tree base downstream task model often outperform baselines use externally provide syntax tree drive composition order work contribute new latent tree learn model base shift reduce parse competitive downstream performance non trivial induce tree b analysis tree learn shift reduce model chart base model
paper present customizable datacentric system automatically generate common misspell complex health relate term spell variant generator rely dense vector model learn large unlabeled text use find semantically close term original seed keyword follow filter term lexically dissimilar beyond give threshold process execute recursively converge new term similar lexically semantically seed keyword find weight intra word character sequence similarities allow problem specific customization system dataset prepare study system outperform current state art medication name variant generation best f1 score sixty-nine f1 four score seventy-eight extrinsic evaluation system set cancer relate term show increase sixty-seven retrieval rate twitter post generate variants include propose spell variant generator several advantage current state art type variant generators capable filter lexically similar semantically dissimilar term ii number variants generate low many low frequency ambiguous misspell filter iii system fully automatic customizable easily executable base system fully unsupervised show supervision maybe employ adjust weight task specific customization performance significant relative simplicity propose approach make much need misspell generation resource health relate text mine noisy source source code system make publicly available research purpose
self normalize discriminative model approximate normalize probability class without compute partition function context language model property particularly appeal may significantly reduce run time due large word vocabularies study provide comprehensive investigation language model self normalization first theoretically analyze inherent self normalization properties noise contrastive estimation nce language model compare empirically softmax base approach self normalize use explicit regularization suggest hybrid model compel properties finally uncover surprise negative correlation self normalization perplexity across board well regularity observe errors may potentially use improve self normalization algorithms future
paper introduce drcd delta read comprehension dataset open domain traditional chinese machine read comprehension mrc dataset dataset aim standard chinese machine read comprehension dataset source dataset transfer learn dataset contain ten thousand and fourteen paragraph two thousand, one hundred and eight wikipedia article thirty thousand question generate annotators build baseline model achieve f1 score eight thousand, nine hundred and fifty-nine f1 score human performance nine thousand, three hundred and thirty
japanese predicate argument structure pas analysis involve zero anaphora resolution notoriously difficult improve performance japanese pas analysis straightforward increase size corpora annotate pas however since prohibitively expensive promise take advantage large amount raw corpora paper propose novel japanese pas analysis model base semi supervise adversarial train raw corpus experiment model outperform exist state art model japanese pas analysis
use six thousand, six hundred and thirty-eight case descriptions societal impact submit evaluation research excellence framework ref two thousand and fourteen replicate topic model latent dirichlet allocation lda make context compare result factor analytic result use traditional word document matrix principal component analysis pca remove small fraction document sample example average much larger impact lda pca base model extent largest distortion case pca less effect smallest distortion lda base model term semantic coherence however lda model outperform pca base model topic model inform us statistical properties document set study result statistical use semantic interpretation example grant selections micro decision make scholarly work without follow use domain specific semantic map
describe novel method efficiently elicit scalar annotations dataset construction system quality estimation human judgments contrast direct assessment annotators assign score items directly online pairwise rank aggregation score derive annotator comparison items hybrid approach easl efficient annotation scalar label propose proposal lead increase correlation grind truth far greater annotator efficiency suggest strategy improve mechanism dataset creation manual system evaluation
recent study show macroscopic pattern continuity change course centuries detect analysis time series extract massive textual corpora similar data drive approach already revolutionise natural sciences widely believe hold similar potential humanities social sciences drive mass digitisation project currently way couple ever increase number document bear digital new interactive tool require discover extract macroscopic pattern vast quantities textual data present history playground interactive web base tool discover trend massive textual corpora tool make use scalable algorithms first extract trend textual corpora make available real time search discovery present users interface explore data include tool algorithms standardization regression change point detection relative frequencies ngrams multi term indices comparison trend across different corpora
learn social media content basis many real world applications include information retrieval recommendation systems among others contrast previous work focus mainly single modal bi modal learn propose learn social media content fuse jointly textual acoustic visual information jtav effective strategies propose extract fine grain feature modality attbigru dcrnn also introduce cross modal fusion attentive pool techniques integrate multi modal information comprehensively extensive experimental evaluation conduct real world datasets demonstrate propose model outperform state art approach large margin
much progress make encode text sequence sequence vectors less attention pay aggregate precede vectors output rnn cnn fix size encode vector usually simple max average pool use bottom passive way aggregation lack guidance task information paper propose aggregation mechanism obtain fix size encode dynamic rout policy dynamic rout policy dynamically decide much information need transfer word final encode text sequence follow work capsule network design two dynamic rout policies aggregate output rnn cnn encode layer final encode vector compare aggregation methods dynamic rout refine message accord state final encode vector experimental result five text classification task show method outperform aggregate model significant margin relate source code release github page
use pre train word embeddings input layer common practice many natural language process nlp task largely neglect neural machine translation nmt paper conduct systematic analysis effect use pre train source side monolingual word embed nmt compare several strategies fix update embeddings nmt train vary amount data also propose novel strategy call dual embed blend fix update strategies result suggest pre train embeddings helpful properly incorporate nmt especially parallel data limit additional domain monolingual data readily available
semantic role label srl approach supervise methods require significant amount annotate corpus annotation require linguistic expertise paper propose multi task active learn framework semantic role label entity recognition er auxiliary task alleviate need extensive data use additional information er help srl evaluate approach indonesian conversational dataset experiment show multi task active learn outperform single task active learn method standard multi task learn accord result active learn efficient use twelve less train data compare passive learn single task multi task set also introduce new dataset srl indonesian conversational domain encourage research area
generative document model act bag word input attempt focus semantic content thereby partially forego syntactic information argue preferable keep original word order intact explicitly account syntactic structure instead propose extension neural variational document model miao et al two thousand and sixteen exactly separate local syntactic context global semantic representation document model build variational autoencoder framework define generative document model base next word prediction name approach sequence aware variational autoencoder since contrast predecessor operate true input sequence series experiment observe stronger topicality learn representations well increase robustness syntactic noise train data
understand able react customer feedback fundamental task provide good customer service however two major obstacles international company automatically detect mean customer feedback global multilingual environment firstly widely acknowledge categorisation class mean customer feedback secondly applicability one mean categorisation exist customer feedback multiple languages questionable paper extract representative real world sample customer feedback microsoft office customers multiple languages english spanish japaneseand conclude five class categorisationcomment request bug complaint meaningless mean classification could use across languages realm customer feedback analysis
luminoso participate semeval two thousand and eighteen task capture discriminative attribute system base conceptnet open knowledge graph focus general knowledge paper describe train linear classifier small number semantically inform feature achieve f1 score seven thousand, three hundred and sixty-eight task close task high score seventy-five
slot fill paradigm user refer back slot context conversation goal contextual understand system resolve refer expressions appropriate slot context large scale multi domain systems present two challenge scale large potentially unbounded set slot value deal diverse schemas present neural network architecture address slot value scalability challenge reformulate contextual interpretation decision carryover slot set possible candidates deal heterogenous schemas introduce simple data drive method trans form candidate slot experiment show approach scale multiple domains provide competitive result strong baseline
propose formal definition task suggestion mine context wide range open domain applications human perception term emphsuggestion subjective effect preparation hand label datasets task suggestion mine exist work either lack formal problem definition annotation procedure provide domain application specific definitions moreover many previously use manually label datasets remain proprietary first present annotation study base observations propose formal task definition annotation procedure create benchmark datasets suggestion mine study also provide publicly available label datasets suggestion mine multiple domains
aim work explore possible limitations exist methods cross language word embeddings evaluation address lack correlation intrinsic extrinsic cross language evaluation methods prove hypothesis construct english russian datasets extrinsic intrinsic evaluation task compare performances five different cross language model result say score even different intrinsic benchmarks correlate conclude use human reference grind truth cross language word embeddings proper unless one understand native speakers process semantics cognition
introduce scalable bayesian preference learn method identify convince arguments absence gold standard rat ings rank contrast previous work avoid need separate methods perform quality control train data predict rank perform pairwise classification bayesian approach effective solution face sparse noisy train data previously use identify convince arguments one issue scalability address develop stochastic variational inference method gaussian process gp preference learn show method apply predict argument convincingness crowdsourced data outperform previous state art particularly train small amount unreliable data demonstrate bayesian approach enable effective active learn thereby reduce amount data require identify convince arguments new users domains word embeddings principally use neural network result show word embeddings combination linguistic feature also benefit gps predict argument convincingness
code corpora observe large software systems know far repetitive predictable natural language corpora difference simply arise syntactic limitations program languages arise differences author decisions make writers natural program language texts conjecture differences entirely due syntax also fact read write code un natural humans require substantial mental effort people prefer write code ways familiar reader writer support argument present result two set study one first set aim attenuate effect syntax two second aim measure repetitiveness text write settings eg second language technical specialize jargon also effortful write find find repetition source code entirely result grammar constraints thus repetition must result human choice evidence find similar repetitive behavior technical learner corpora conclusively show language use humans mitigate difficulty consistent theory
multi source translation approach exploit multiple input eg two different languages increase translation accuracy paper examine approach multi source neural machine translation nmt use incomplete multilingual corpus translations miss practice many multilingual corpora complete due difficulty provide translations relevant languages example ted talk english talk subtitle small portion languages ted support exist study multi source translation explicitly handle situations study focus use incomplete multilingual corpora multi encoder nmt mixture nmt experts examine simple implementation miss source translations replace special symbol methods allow us use incomplete corpora train time test time experiment real incomplete multilingual corpora ted talk multi source nmt tokens achieve higher translation accuracies measure bleu one one nmt systems
present challenge set french english machine translation base approach introduce isabelle cherry foster emnlp two thousand and seventeen challenge set make sentence expect relatively difficult machine translate correctly straightforward translations tend linguistically divergent present set five hundred and six manually construct french sentence three hundred and seven target kinds structural divergences paper mention remain one hundred and ninety-nine sentence design test ability systems correctly translate difficult grammatical word prepositions report result use challenge set test two different systems namely google translate deepl two different date october two thousand and seventeen january two thousand and eighteen result data make publicly available
paper propose domain adversarial train dat algorithm alleviate accent speech recognition problem order reduce mismatch label source domain data standard accent unlabeled target domain data heavy accent augment learn objective kaldi tdnn network domain adversarial train dat objective encourage model learn accent invariant feature experiment three mandarin accent show dat yield seven hundred and forty-five relative character error rate reduction transcriptions accent speech compare baseline train standard accent data also find benefit dat use combination train automatic transcriptions accent data furthermore find dat superior multi task learn accent speech recognition
propagation unreliable information rise many place around world expansion facilitate rapid spread information anonymity grant internet spread unreliable information wellstudied issue associate negative social impact previous work identify significant differences structure news article reliable unreliable source us media goal work explore differences brazilian media find significant feature two data set one brazilian news portuguese another one us news english result show feature relate write style prominent data set despite language difference feature universal behavior significant us brazilian news article finally combine data set use universal feature build machine learn classifier predict source type news article reliable unreliable
understand affect video segment bring researchers language audio video domains together current multimodal research area deal various techniques fuse modalities mostly treat segment video independently motivate work zadeh et al two thousand and seventeen poria et al two thousand and seventeen present architecture relational tensor network use inter modal interactions within segment intra segment also consider sequence segment video model inter segment inter modal interactions also generate rich representations text audio modalities leverage richer audio linguistic context alongwith fuse fine grain knowledge base polarity score text present result model cmu mosei dataset show model outperform many baselines state art methods sentiment classification emotion recognition
document describe find second workshop neural machine translation generation hold concert annual conference association computational linguistics acl two thousand and eighteen first summarize research trend paper present proceed note particular interest linguistic structure domain adaptation data augmentation handle inadequate resources analysis model second describe result workshop share task efficient neural machine translation participants task create mt systems accurate efficient
methods unsupervised hypernym detection may broadly categorize accord two paradigms pattern base distributional methods paper study performance approach several hypernymy task find simple pattern base methods consistently outperform distributional methods common benchmark datasets result show pattern base model provide important contextual constraints yet capture distributional methods
discourse study concessions consider among argumentative strategies increase persuasion aim empirically test hypothesis calculate distribution argumentative concessions persuasive vs non persuasive comment changemyview subreddit constitute challenge task since concessions always part argument draw theoretically inform typology concessions conduct annotation task label set polysemous lexical markers introduce argumentative concession observe distribution thread achieve achieve persuasion annotation use expert novice annotators ultimate goal conduct study large datasets present self train method automatically identify argumentative concessions use linguistically motivate feature achieve moderate f1 five hundred and seventy-four development set four hundred and sixty test set via self train method result comparable state art result similar task identify explicit discourse connective type penn discourse treebank find manual label classification experiment indicate type argumentative concessions investigate almost equally likely use win lose arguments changemyview dataset result seem contradict theoretical assumptions provide reason discrepancy relate changemyview subreddit
multilingual machine translation address task translate multiple source target languages propose task specific attention model simple effective technique improve quality sequence sequence neural multilingual translation approach seek retain much parameter share generalization nmt model possible still allow language specific specialization attention model particular language pair task experiment four languages europarl corpus show use target specific model attention provide consistent gain translation quality possible translation directions compare model parameters share observe improve translation quality even extreme low resource zero shoot translation directions model never saw explicitly pair parallel data
dynamic oracles provide strong supervision train constituency parsers exploration must custom define give parser transition system explore use policy gradient method parser agnostic alternative addition directly optimize tree level metric f1 policy gradient potential reduce exposure bias allow exploration train moreover require dynamic oracle supervision four constituency parsers three languages method substantially outperform static oracle likelihood train almost settings parsers dynamic oracle available include novel oracle define transition system dyer et al two thousand and sixteen policy gradient typically recapture substantial fraction performance gain afford dynamic oracle
automatic sarcasm detection methods traditionally design maximum performance specific domain pose challenge wish transfer approach exist novel domains may typify different language characteristics develop general set feature evaluate different train scenarios utilize domain domain train data best perform scenario train employ domain adaptation step achieve f1 seven hundred and eighty well baseline f1 measure five hundred and fifteen three hundred and forty-five also show approach outperform best result prior work target domain
word frequency assume correlate word familiarity strength correlation thoroughly investigate paper report analysis correlation word familiarity rat list obtain psycholinguistic experiment log frequency obtain various corpora different kinds size terabyte scale english japanese major find threefold first give corpus familiarity necessary word achieve high frequency familiar word necessarily frequent second correlation increase corpus data size third corpus speak language correlate better one write language find suggest cognitive familiarity rat correlate frequency highly speak rather write language
neural network approach name entity recognition reduce need carefully hand craft feature feature remain state art systems lexical feature mostly discard exception gazetteers work show unfair lexical feature actually quite useful propose embed word entity type low dimensional vector space train annotate data produce distant supervision thank wikipedia compute offline feature vector represent word use vanilla recurrent neural network model representation yield substantial improvements establish new state art f1 score eight thousand, seven hundred and ninety-five ontonotes fifty match state art performance f1 score nine thousand, one hundred and seventy-three study conll two thousand and three dataset
recent years witness surge publications aim trace temporal change lexical semantics use distributional methods particularly prediction base word embed model however vein research lack cohesion common terminology share practice establish areas natural language process paper survey current state academic research relate diachronic word embeddings semantic shift detection start discuss notion semantic shift continue overview exist methods trace time relate shift word embed model propose several ax along methods compare outline main challenge emerge subfield nlp well prospect possible applications
current evaluation metrics question answer base machine read comprehension mrc systems generally focus lexical overlap candidate reference answer rouge bleu however bias may appear metrics use specific question type especially question inquire yes opinions entity list paper make adaptations metrics better correlate n gram overlap human judgment answer two question type statistical analysis prove effectiveness approach adaptations may provide positive guidance development real scene mrc systems
neural network model show promise result text classification however solutions limit dependence availability annotate data prospect leverage resource rich languages enhance text classification resource poor languages fascinate performance resource poor languages significantly improve resource availability constraints offset end present twin bidirectional long short term memory bi lstm network share parameters consolidate contrastive loss function base similarity metric model learn representation resource poor resource rich sentence common space use similarity assign annotation tag hence model project sentence similar tag closer different tag farther evaluate model classification task sentiment analysis emoji prediction resource poor languages hindi telugu resource rich languages english spanish model significantly outperform state art approach task across metrics
propose learn acoustic word embeddings temporal context query example qbe speech search temporal context include lead trail word sequence word assume exist speak word pair train database pad word pair original temporal context form fix length speech segment pair obtain acoustic word embeddings deep convolutional neural network cnn train speech segment pair triplet loss shift fix length analysis window search content obtain run sequence embeddings way search speak query equivalent match acoustic word embeddings experiment show propose acoustic word embeddings learn temporal context effective qbe speech search outperform state art frame level feature representations reduce run time computation since dynamic time warp require qbe speech search also find important sufficient speech segment pair train deep cnn effective acoustic word embeddings
annotation corpus discourse relations benefit nlp task machine translation question answer paper present scidtb domain specific discourse treebank annotate scientific article different widely use rst dt pdtb scidtb use dependency tree represent discourse structure flexible simplify extent sacrifice structural integrity discuss label framework annotation workflow statistics scidtb furthermore treebank make benchmark evaluate discourse dependency parsers provide several baselines fundamental work
address problem simultaneous translation modify neural mt decoder operate dynamically build encoder attention propose tunable agent decide best segmentation strategy user define bleu loss average proportion ap constraint agent outperform previously propose wait diff wait worse agents cho esipova two thousand and sixteen bleu lower latency secondly propose data drive change neural mt train better match incremental decode framework
deep neural network model chinese zero pronoun resolution learn semantic information zero pronoun candidate antecedents tend short sight often make local decisions typically predict coreference chain zero pronoun one single candidate antecedent one link time overlook long term influence future decisions ideally model useful information precede potential antecedents critical later predict zero pronoun candidate antecedent pair study show integrate local global decision make exploit deep reinforcement learn model help reinforcement learn agent model learn policy select antecedents sequential manner useful information provide earlier predict antecedents could utilize make later coreference decisions experimental result ontonotes fifty dataset show technique surpass state art model
automatic resolution rumour challenge task break smaller components make pipeline include rumour detection rumour track stance classification lead final outcome determine veracity rumour previous work step process rumour verification develop separate components output one feed next propose multi task learn approach allow joint train main auxiliary task improve performance rumour verification examine connection dataset properties outcomes multi task learn model use
lexical ambiguity make difficult compute various useful statistics corpus give word form might represent several morphological feature bundle one however use unsupervised learn fit model probabilistically disambiguate word form present approach employ neural network smoothly model prior distribution feature bundle even rare ones although basic model consider token context property allow operate simple list unigram type count partition count among different analyse unigram discuss evaluation metrics novel task report result five languages
general model methods apply diverse languages natural question well expect model work languages differ typological profile work develop evaluation framework fair cross linguistic comparison language model use translate text model ask predict approximately information conduct study twenty-one languages demonstrate languages textual expression information harder predict n gram lstm language model show complex inflectional morphology performance differences among languages
statistical morphological inflectors typically train fully supervise type level data one remain open research question follow effectively exploit raw token level data improve performance end introduce novel generative latent variable model semi supervise learn inflection generation enable posterior inference latent variables derive efficient variational inference procedure base wake sleep algorithm experiment twenty-three languages use universal dependencies corpora simulate low resource set find improvements ten absolute accuracy case
work part speech pos tag focus high resource languages examine low resource active learn settings simulate study evaluate pos tag techniques actual endanger language griko present resource contain one hundred and fourteen narratives griko along sentence level translations italian provide gold annotations test set base previously collect small corpus investigate several traditional methods well methods take advantage monolingual data project cross lingual pos tag show combination semi supervise method cross lingual transfer appropriate extremely challenge set best tagger achieve accuracy seven hundred and twenty-nine apply active learn scheme use collect sentence level annotations test set achieve improvements twenty-one percentage point
sentiment analysis also call opinion mine field study analyze people opinionssentiments attitudes emotions songs important sentiment analysis since songs mood mutually dependent base select song become easy find mood listener future use recommendation song lyric rich source datasets contain word helpful analysis classification sentiments generate days observe lot inter sentential intra sentential code mix songs vary impact audience study impact create telugu songs dataset contain telugu english code mix pure telugu songs paper classify songs base arousal excite non excite develop language identification tool introduce code mix feature obtain additional feature system additional feature attain four five accuracy greater traditional approach dataset
extractive read comprehension systems often locate correct answer question context document also tend make unreliable guess question correct answer state context exist datasets either focus exclusively answerable question use automatically generate unanswerable question easy identify address weaknesses present squad twenty latest version stanford question answer dataset squad squad twenty combine exist squad data fifty thousand unanswerable question write adversarially crowdworkers look similar answerable ones well squad twenty systems must answer question possible also determine answer support paragraph abstain answer squad twenty challenge natural language understand task exist model strong neural system get eighty-six f1 squad eleven achieve sixty-six f1 squad twenty
capture interactions among multiple predicate argument structure pass crucial issue task analyze pas japanese paper propose new japanese pas analysis model integrate label prediction information arguments multiple pass extend input last layer standard deep bidirectional recurrent neural network bi rnn model model use mechanisms pool attention aim directly capture potential interactions among multiple pass without disturb word order distance experiment show propose model improve prediction accuracy specifically case predicate argument indirect dependency relation achieve new state art overall f1 standard benchmark corpus
multi choice read comprehension challenge task involve match passage question answer pair paper propose new co match approach problem jointly model whether passage match question candidate answer experimental result race dataset demonstrate approach achieve state art performance
exponential increase usage wikipedia key source scientific knowledge among researchers make absolutely necessary metamorphose knowledge repository integral self contain source information direct utilization unfortunately reference support content wikipedia entity page far complete reference section ill form wikipedia page section edit frequently section page appropriate surrogates automatically enhance reference section paper propose novel two step approach wikiref leverage wikilinks present scientific wikipedia target page thereby ii recommend highly relevant reference include target page appropriately automatically borrow reference section wikilinks first step build classifier ascertain whether wikilink potential source reference follow step recommend reference target page reference section wikilinks classify potential source reference first step perform extensive evaluation approach datasets two different domains computer science physics computer science achieve notably good performance precision1 forty-four reference recommendation oppose thirty-eight obtain competitive baseline physics dataset obtain similar performance boost ten respect competitive baseline
recurrent neural network grammars rnngs generative model treestring pair rely neural network evaluate derivational choices parse use beam search yield variety incremental complexity metrics word surprisal parser action count use regressors human electrophysiological responses naturalistic text derive two amplitude effect early peak p600 like later peak contrast non syntactic neural language model yield reliable effect model comparisons attribute early peak syntactic composition within rnng pattern result recommend rnngbeam search combination mechanistic model syntactic process occur normal human language comprehension
present corpus five thousand richly annotate abstract medical article describe clinical randomize control trials annotations include demarcations text span describe patient population enrol interventions study compare outcomes measure pico elements span annotate granular level eg individual interventions within mark map onto structure medical vocabulary acquire annotations diverse set workers vary level expertise cost describe data collection process corpus detail outline set challenge nlp task would aid search medical literature practice evidence base medicine
harmful speech various form plague social media different ways need crackdown different degrees hate speech abusive behavior amongst classification need base complex ramifications need define hold accountable racist sexist particular group community paper primarily describe create ontological classification harmful speech base degree hateful intent use annotate twitter data accordingly key contribution paper new dataset tweet create base ontological class degrees harmful speech find text also propose supervise classification system recognize respective harmful speech class texts hence
introduce task predict adverbial presupposition trigger also solve task require detect recur similar events discourse context applications natural language generation task summarization dialogue systems create two new datasets task derive penn treebank annotate english gigaword corpora well novel attention mechanism tailor task attention mechanism augment baseline recurrent neural network without need additional trainable parameters minimize add computational cost mechanism demonstrate model statistically outperform number baselines include lstm base language model
multilingual topic model enable crosslingual task extract consistent topics multilingual corpora model require parallel comparable train corpora limit ability generalize paper first demystify knowledge transfer mechanism behind multilingual topic model define alternative equivalent formulation base analysis relax assumption train data require exist model create model require dictionary train experiment show new method effectively learn coherent multilingual topics partially fully incomparable corpora limit amount dictionary resources
indigenous languages american continent highly diverse however receive little attention technological perspective paper review research digital resources available nlp systems focus languages present main challenge research question arise distant languages low resource scenarios face would like encourage nlp research linguistically rich diverse areas like americas
dialogue act da tag crucial speak language understand systems provide general representation speakers intents bind particular dialogue system unfortunately publicly available data set da annotation base different annotation scheme thus incompatible moreover scheme often cover aspects necessary open domain human machine interaction paper propose methodology map several publicly available corpora subset iso standard order create large task independent train corpus da classification show feasibility use corpus train domain independent da tagger test domain conversational data argue importance train multiple corpora achieve robustness across different da categories
paper analyze several neural network design variations sentence pair model compare performance extensively across eight datasets include paraphrase identification semantic textual similarity natural language inference question answer task although model claim state art performance original paper often report one two select datasets provide systematic study show encode contextual information lstm inter sentence interactions critical ii tree lstm help much previously claim surprisingly improve performance twitter datasets iii enhance sequential inference model best far larger datasets pairwise word interaction model achieve best performance less data available release implementations open source toolkit
attention base long short term memory lstm network prove useful aspect level sentiment classification however due difficulties annotate aspect level data exist public datasets task relatively small largely limit effectiveness neural model paper explore two approach transfer knowledge document level data much less expensive obtain improve performance aspect level sentiment classification demonstrate effectiveness approach four public datasets semeval two thousand and fourteen two thousand and fifteen two thousand and sixteen show attention base lstm benefit document level knowledge multiple ways
generate natural language require convey content appropriate style explore two relate task generate text vary formality monolingual formality transfer formality sensitive machine translation propose solve task jointly use multi task learn show model achieve state art performance formality transfer able perform formality sensitive translation without explicitly train style annotate translation examples
domain adaptation sentiment analysis challenge due fact supervise classifiers sensitive change domain two prominent approach problem structural correspondence learn autoencoders however either require long train time suffer greatly highly divergent domains inspire recent advance cross lingual sentiment analysis provide novel perspective cast domain adaptation problem embed projection task model take input two mono domain embed space learn project bi domain space jointly optimize one project across domains two predict sentiment perform domain adaptation experiment twenty source target domain pair sentiment classification report novel state art result eleven domain pair include amazon domain adaptation datasets semeval two thousand and thirteen two thousand and sixteen datasets analysis show model perform comparably state art approach domains similar perform significantly better highly divergent domains code available https githubcom jbarnesspain domainblse
back translation become commonly employ heuristic semi supervise neural machine translation technique straightforward apply lead state art result work offer principled interpretation back translation approximate inference generative model bitext show standard implementation back translation correspond single iteration wake sleep algorithm propose model moreover interpretation suggest natural iterative generalization demonstrate lead improvement sixteen bleu
classic pipeline model task orient dialogue system require explicit model dialogue state hand craft action space query domain specific knowledge base conversely sequence sequence model learn map dialogue history response current turn without explicit knowledge base query work propose novel framework leverage advantage classic pipeline sequence sequence model framework model dialogue state fix size distribute representation use representation query knowledge base via attention mechanism experiment stanford multi turn multi domain task orient dialogue dataset show framework significantly outperform sequence sequence base baseline model automatic human evaluation
multilingual societies like india code mix social media texts comprise majority internet detect sentiment code mix user opinions play crucial role understand social economic political trend paper propose ensemble character trigrams base lstm model word ngrams base multinomial naive bay mnb model identify sentiments hindi english hi en code mix data ensemble model combine strengths rich sequential pattern lstm model polarity keywords probabilistic ngram model identify sentiments sparse inconsistent code mix data experiment reallife user code mix data reveal approach yield state art result compare several baselines deep learn base propose methods
paper present design implementation effectiveness generate personalize suggestions email reply personalize email responses base users style personality model users persona base past responses email model add language base model create across users use past responses user email users model capture typical responses user give particular context context include email receive recipient email external signal calendar activities preferences etc context along users personality eg extrovert formal reserve etc use suggest responses responses mixture multiple modes email reply textual audio clip etc help make responses mimic user much possible help user productive retain mark responses
neural machine translation nmt systems usually train large amount bilingual sentence pair translate one sentence time ignore inter sentence information may make translation sentence ambiguous even inconsistent translations neighbor sentence order handle issue propose inter sentence gate model use encoder encode two adjacent sentence control amount information flow precede sentence translation current sentence inter sentence gate way propose model capture connection sentence fuse recency neighbor sentence neural machine translation several nist chinese english translation task experiment demonstrate propose inter sentence gate model achieve substantial improvements baseline
investigate design challenge construct effective efficient neural sequence label systems reproduce twelve neural sequence label model include state art structure conduct systematic model comparison three benchmarks ie ner chunk pos tag misconceptions inconsistent conclusions exist literature examine clarify statistical experiment comparison analysis process reach several practical conclusions useful practitioners
investigate behavior map learn machine translation methods map translate word project word embed space different languages locally approximate map use linear map find vary across word embed space demonstrate underlie map non linear importantly show locally linear map vary amount tightly correlate distance neighborhoods train result use test non linear methods drive design accurate map word translation
large scale knowledge graph kgs freebase generally incomplete reason multi hop mh kg paths thus important capability need question answer nlp task require knowledge world mh kg reason include diverse scenarios eg give head entity relation path predict tail entity give two entities connect relation paths predict unknown relation present rops recurrent one hop predictors predict entities step mh kb paths use recurrent neural network vector representations entities relations two benefit model mh paths arbitrary lengths update entity relation representations train signal step ii handle different type mh kg reason unify framework model show state art two important multi hop kg reason task knowledge base completion path query answer
paper formalize problem automatic fill blank question generation use two standard nlp machine learn scheme propose concrete deep learn model present empirical study base data obtain language learn platform show propose settings offer promise result
accurate prediction students knowledge fundamental build block personalize learn systems propose novel ensemble model predict student knowledge gap apply approach student trace data online educational platform duolingo achieve highest score evaluation metrics three datasets two thousand and eighteen share task second language acquisition model describe model discuss relevance task compare would setup production environment personalize education
exist methods hypernymy detection mainly rely statistics big corpus either mine co occur pattern like animals cat embed word interest context aware vectors approach therefore limit availability large enough corpus cover term interest provide sufficient contextual information represent mean work propose new paradigm hyperdef hypernymy detection express word mean encode word definitions along context drive representation two main benefit definitional sentence express sense specific corpus independent mean word hence definition drive approach enable strong generalization train model expect work well open domain testbeds ii global context large corpus definitions provide complementary information word consequently model hyperdef train task agnostic data get state art result multiple benchmarks
evaluate various compositional model bag word representations compositional rnn base model several extrinsic supervise unsupervised evaluation benchmarks result confirm weight vector average outperform context sensitive model benchmarks structural feature encode rnn model also useful certain classification task analyze evaluation datasets identify aspects mean measure characteristics various model explain performance variance
accurately identify distant recurrences breast cancer electronic health record ehr important clinical care secondary analysis although multiple applications develop computational phenotyping breast cancer distant recurrence identification still rely heavily manual chart review study aim develop model identify distant recurrences breast cancer use clinical narratives structure data ehr apply metamap extract feature clinical narratives also retrieve structure clinical data ehr use feature train support vector machine model identify distant recurrences breast cancer patients train model use one thousand, three hundred and ninety-six double annotate subject validate model use five hundred and ninety-nine double annotate subject addition validate model set four thousand, nine hundred and four single annotate subject generalization test obtain high area curve auc score ninety-two sd001 cross validation use train dataset obtain auc score ninety-five ninety-three hold test generalization test use five hundred and ninety-nine four thousand, nine hundred and four sample respectively model accurately efficiently identify distant recurrences breast cancer combine feature extract unstructured clinical narratives structure clinical data
article review recent advance apply natural language process nlp electronic health record ehrs computational phenotyping nlp base computational phenotyping numerous applications include diagnosis categorization novel phenotype discovery clinical trial screen pharmacogenomics drug drug interaction ddi adverse drug event ade detection well genome wide phenome wide association study significant progress make algorithm development resource construction computational phenotyping among survey methods well design keyword search rule base systems often achieve good performance however construction keyword rule list require significant manual effort difficult scale supervise machine learn model favor capable acquire classification pattern structure data recently deep learn unsupervised learn receive grow attention former favor performance latter ability find novel phenotypes integrate heterogeneous data source become increasingly important show promise improve model performance often better performance achieve combine multiple modalities information despite many advance challenge opportunities remain nlp base computational phenotyping include better model interpretability generalizability proper characterization feature relations clinical narratives
multi label classification important yet challenge task natural language process complex single label classification label tend correlate exist methods tend ignore correlations label besides different part text contribute differently predict different label consider exist model paper propose view multi label classification task sequence generation problem apply sequence generation model novel decoder structure solve extensive experimental result show propose methods outperform previous work substantial margin analysis experimental result demonstrate propose methods capture correlations label also select informative word automatically predict different label
encoder decoder base sequence sequence learn s2s make remarkable progress recent years different network architectures use encoder decoder among convolutional neural network cnn self attention network san prominent ones two architectures achieve similar performances use different ways encode decode context cnn use convolutional layer focus local connectivity sequence san use self attention layer focus global semantics work propose double path network sequence sequence learn dpn s2s leverage advantage model use double path information fusion encode step develop double path architecture maintain information come different paths convolutional layer self attention layer separately effectively use encode context develop cross attention module gate use automatically pick information need decode step deeply integrate two paths cross attention type information combine well exploit experiment show propose method significantly improve performance sequence sequence learn state art systems
generative adversarial network gans promise approach language generation latest work introduce novel gin model language generation use n gram base metrics evaluation report single score best run paper argue often misrepresent true picture tell full story gin model extremely sensitive random initialization small deviations best hyperparameter choice particular demonstrate previously use bleu score sensitive semantic deterioration generate texts propose alternative metrics better capture quality diversity generate sample also conduct set experiment compare number gin model text conventional language model lm find neither consider model perform convincingly better lm
events text document interrelate complex ways paper study two type relation event coreference event sequence show popular tree like decode structure automate event coreference suitable event sequence end propose graph base decode algorithm applicable task new decode algorithm support flexible feature set task empirically event coreference system achieve state art performance tac kbp two thousand and fifteen event coreference task event sequence system beat strong temporal base oracle inform baseline discuss challenge study event relations
paper apply different nmt model problem historical spell normalization five languages english german hungarian icelandic swedish nmt model different level different attention mechanisms different neural network architectures result show nmt model much better smt model term character error rate vanilla rnns competitive grus lstms historical spell normalization transformer model perform better provide train data also find subword level model small subword vocabulary better character level model low resource languages addition propose hybrid method improve performance historical spell normalization
lack repeatability generalisability two significant threats continue scientific development natural language process language model learn methods complex scientific conference paper longer contain enough space technical depth require replication reproduction take target dependent sentiment analysis case study show recent work field consistently release code describe settings learn methods enough detail lack comparability generalisability train test validation data investigate generalisability enable state art comparative evaluations carry first reproduction study three group complementary methods perform first large scale mass evaluation six different english datasets reflect experience recommend future replication reproduction experiment always consider variety datasets alongside document release methods publish code order minimise barriers repeatability generalisability release code model zoo github jupyter notebooks aid understand full documentation recommend others paper submission time anonymised github account
textual analytics base representations document bag word reasonably successful however analysis require deeper insight language author properties contexts document create require richer representation systemic net one representation extensively use require human effort construct show systemic net algorithmically infer corpora result net plausible provide practical benefit knowledge discovery problems open new class practical analysis techniques textual analytics
mental health significant grow public health concern language usage leverage obtain crucial insights mental health condition need large scale label mental health relate datasets users diagnose one condition paper investigate creation high precision pattern identify self report diagnose nine different mental health condition obtain high quality label data without need manual label introduce smhd self report mental health diagnose dataset make available smhd novel large dataset social media post users one multiple mental health condition along match control users examine distinctions users language measure linguistic psychological variables explore text classification methods identify individuals mental condition language
model yous congressional legislation roll call vote receive significant attention previous literature however legislators across fifty state governments dc propose one hundred thousand bill year average enact thirty state level analysis receive relatively less attention due part difficulty obtain necessary data since state legislature guide procedures politics issue however difficult qualitatively asses factor affect likelihood legislative initiative succeed herein present several methods model likelihood bill receive floor action across fifty state dc utilize lexical content one million bill along contextual legislature legislator derive feature build predictive model allow comparison factor important lawmaking process furthermore show signal hold complementary predictive power together achieve average improvement accuracy eighteen state specific baselines
state art natural language process algorithms rely heavily efficient word segmentation urdu amongst languages word segmentation complex task exhibit space omission well space insertion issue partly due arabic script although cursive nature consist character inherent join non join attribute regardless word boundary paper present word segmentation system urdu use conditional random field sequence modeler orthographic linguistic morphological feature propose model automatically learn predict white space word boundary well zero width non joiner zwnj sub word boundary use manually annotate corpus model achieve f1 score ninety-seven word boundary identification eighty-five sub word boundary identification task make code corpus publicly available make result reproducible
build multi turn information seek conversation systems important challenge research topic although several advance neural text match model propose task generally efficient industrial applications furthermore rely large amount label data may available real world applications alleviate problems study transfer learn multi turn information seek conversations paper first propose efficient effective multi turn conversation model base convolutional neural network extend model adapt knowledge learn resource rich domain enhance performance finally deploy model industrial chatbot call alime assist https consumerservicetaobaocom online help observe significant improvement exist online model
development several multilingual datasets use semantic parse recent research efforts look problem learn semantic parsers multilingual setup however improve performance monolingual semantic parser specific language leverage data annotate different languages remain research question explore work present study show learn distribute representations logical form data annotate different languages use improve performance monolingual semantic parser extend two exist monolingual semantic parsers incorporate cross lingual distribute logical representations feature experiment show propose approach able yield improve semantic parse result standard multilingual geoquery dataset
state art handle rich morphology neural machine translation nmt break word form subword units overall vocabulary size units fit practical limit give nmt model gpu memory capacity paper compare two common linguistically uninformed methods subword construction bpe ste method implement tensor2tensor toolkit two linguistically motivate methods morfessor one novel method base derivational dictionary experiment german czech translation morphologically rich document far non motivate methods perform better furthermore iden tify critical difference bpe ste show simple pre process step bpe considerably increase translation quality evaluate automatic measure
paper aim aspect sentiment model aspect base sentiment analysis absa focus micro review task important order understand short review majority users write exist topic model target expert level long review sufficient co occurrence pattern observe current methods aggregate micro review use metadata information may effective well due metadata absence topical heterogeneity cold start problems end propose model call micro aspect sentiment model microasm microasm base observation short review one view sentiment aspect word pair build block information two cluster larger review compare current state art aspect sentiment model experiment show model provide better performance aspect level task aspect term extraction document level task sentiment classification
major proportion text summary include important entities find original text entities build topic summary moreover hold commonsense information link knowledge base base observations paper investigate usage link entities guide decoder neural text summarizer generate concise better summaries end leverage shelf entity link system els extract link entities propose entity2topic e2t module easily attachable sequence sequence model transform list entities vector representation topic summary current available els still sufficiently effective possibly introduce unresolved ambiguities irrelevant entities resolve imperfections els encode entities selective disambiguation b pool entity vectors use firm attention apply e2t simple sequence sequence model attention mechanism base model see significant improvements performance gigaword sentence title cnn long document multi sentence highlight summarization datasets least two rouge point
use user product information sentiment analysis important especially cold start users products whose number review limit however current model deal cold start problem typical review websites paper present hybrid contextualized sentiment classifier hcsc contain two modules one fast word encoder return word vectors embed short long range dependency feature two cold start aware attention csaa attention mechanism consider existence cold start problem attentively pool encode word vectors hcsc introduce share vectors construct similar users products use original distinct vectors sufficient information ie cold start decide frequency guide selective gate vector experiment show term rmse hcsc perform significantly better compare famous datasets despite less complexity thus train much faster importantly model perform significantly better previous model train data sparse cold start problems
tremendous amount user generate data social network sit lead gain popularity automatic text classification field computational linguistics past decade within domain one problem draw attention many researchers automatic humor detection texts depth semantic understand text require detect humor make problem difficult automate increase number social media users many multilingual speakers often interchange languages post social media call code mix introduce challenge field linguistic analysis social media content barman et al two thousand and fourteen like spell variations non grammatical structure sentence past research include detect pun texts kao et al two thousand and sixteen humor one line mihalcea et al two thousand and ten single language tremendous amount code mix data available online need develop techniques detect humor code mix tweet paper analyze task humor detection texts describe freely available corpus contain english hindi code mix tweet annotate humoroush non humorousn tag also tag word tweet language tag english hindi others moreover describe experiment carry corpus provide baseline classification system distinguish humorous non humorous texts
sentence classification task additional contexts neighbor sentence may improve accuracy classifier however contexts domain dependent thus use another classification task inappropriate domain contrast propose use translate sentence context always available regardless domain find naive feature expansion translations gain marginal improvements may decrease performance classifier due possible inaccurate translations thus produce noisy sentence vectors end present multiple context fix attachment mcfa series modules attach multiple sentence vectors fix noise vectors use sentence vectors context show method perform competitively compare previous model achieve best classification performance multiple data set first use translations domain free contexts sentence classification
provide detail overview various approach propose date solve task open information extraction present major challenge systems face show evolution suggest approach time depict specific issue address addition provide critique commonly apply evaluation procedures assess performance open ie systems highlight directions future work
rapid expansion usage social media network sit lead huge amount unprocessed user generate data use text mine author profile problem automatically determine profile aspects like author gender age group text gain much popularity computational linguistics past research author profile concentrate english texts cite12 however many users often change language post social media call code mix develop challenge field text classification author profile like variations spell non grammatical structure transliteration cite3 english hindi code mix annotate datasets social media content present online cite4 paper analyze task author gender prediction code mix content present corpus english hindi texts collect twitter annotate author gender also explore language identification every word corpus present supervise classification baseline system use various machine learn algorithms identify gender author use text base character word level feature
paper describe ncrf toolkit neural sequence label ncrf design quick implementation different neural sequence label model crf inference layer provide users inference build custom model structure configuration file flexible neural feature design utilization build pytorch core operations calculate batch make toolkit efficient acceleration gpu also include implementations state art neural sequence label model lstm crf facilitate reproduce refinement methods
generate abstract collection document desirable capability many real world applications however abstractive approach multi document summarization thoroughly investigate paper study feasibility use abstract mean representation amr semantic representation natural language ground linguistic theory form content representation approach condense source document set summary graph follow amr formalism summary graph transform set summary sentence surface realization step framework fully data drive flexible component optimize independently use small scale domain train data perform experiment benchmark summarization datasets report promise result also describe opportunities challenge advance line research
seq2seq learn produce promise result summarization however many case system summaries still struggle keep mean original intact may miss important word relations play critical roles syntactic structure source sentence paper present structure infuse copy mechanisms facilitate copy important word relations source sentence summary sentence approach naturally combine source dependency structure copy mechanism abstractive sentence summarizer experimental result demonstrate effectiveness incorporate source side syntactic information system propose approach compare favorably state art methods
introduce framework quantify semantic variation common word communities practice set topic relate communities show mean shift share across relate communities others community specific therefore independent discuss topic propose find evidence favour sociolinguistic theories socially drive semantic variation result evaluate use independent language model task furthermore investigate extralinguistic feature show factor prominence dissemination word relate semantic variation
relation syntax prosody syntax prosody interface active area research mostly linguistics typically study control condition recently prosody also successfully use data base train syntax parsers however gap control detail study individual effect syntax prosody large scale application prosody syntactic parse shallow analysis respective influence paper close gap investigate significance correlations prosodic realization specific syntactic function use linear mix effect model large corpus read german encyclopedic texts use corpus able analyze prosodic structure perform diverse set speakers try optimize factual content delivery normalization speaker obtain significant effect eg confirm subject function compare object function positive effect pitch duration word negative effect loudness
present model predict individual users dialog system understand produce utterances base user group contrast previous work user group specify beforehand learn train evaluate two refer expression generation task experiment show model identify user group learn effectively talk dynamically assign unseen users correct group interact system
increase demand goal orient conversation systems assist users various day day activities book ticket restaurant reservations shop etc exist datasets build conversation systems focus monolingual conversations hardly work multilingual code mix conversations datasets systems thus cater multilingual regions world india common people speak one language seamlessly switch result code mix conversations example hindi speak user look book restaurant would typically ask kya tum restaurant mein ek table book karne mein meri help karoge help book table restaurant facilitate development code mix conversation model build goal orient dialog dataset contain code mix conversations specifically take text dstc2 restaurant reservation dataset create code mix versions hindi english bengali english gujarati english tamil english also establish initial baselines dataset use exist state art model dataset along baseline implementations make publicly available research purpose
investigate task learn follow natural language instructions jointly reason visual observations language input contrast exist methods start learn demonstrations lfd use reinforcement learn rl fine tune model parameters propose novel policy optimization algorithm dynamically schedule demonstration learn rl propose train paradigm provide efficient exploration better generalization beyond exist methods compare exist ensemble model best single model base propose method tremendously decrease execution error fifty block world environment illustrate exploration strategy rl algorithm also include systematic study evolution policy entropy train
paper present overall efforts improve performance code switch speech recognition system use semi supervise train methods lexicon learn acoustic model south east asian mandarin english seame data first investigate semi supervise lexicon learn approach adapt canonical lexicon mean alleviate heavily accent pronunciation issue within code switch conversation local area result learn lexicon yield improve performance furthermore attempt use semi supervise train deal transcriptions highly mismatch human transcribers asr system specifically conduct semi supervise train assume poorly transcribe data unsupervised data find semi supervise acoustic model lead improve result finally make limitation conventional n gram language model due data sparsity issue perform lattice rescoring use neural network language model significant wer reduction obtain
neural text classification model typically treat output label categorical variables lack description semantics force parametrization dependent label set size hence unable scale large label set generalize unseen ones exist joint input label text model overcome issue exploit label descriptions unable capture complex label relationships rigid parametrization gain unseen label happen often expense weak performance label see train paper propose new input label model generalize previous model address limitations compromise performance see label model consist joint non linear input label embed controllable capacity joint space dependent classification unit train cross entropy loss optimize classification performance evaluate model full resource low zero resource text classification multilingual news biomedical text large label set model outperform monolingual multilingual model leverage label semantics previous joint input label space model scenarios
despite fast developmental pace new sentence embed methods still challenge find comprehensive evaluations different techniques past years saw significant improvements field sentence embeddings especially towards development universal sentence encoders could provide inductive transfer wide variety downstream task work perform comprehensive evaluation recent methods use wide variety downstream linguistic feature probe task show simple approach use bag word recently introduce language model deep context dependent word embeddings prove yield better result many task compare sentence encoders train entailment datasets also show however still far away universal encoder perform consistently across several downstream task
skip gram word2vec recent method create vector representations word distribute word representations use neural network representation gain popularity various areas natural language process seem capture syntactic semantic information word without explicit supervision respect propose subgram refinement skip gram model consider also word structure train process achieve large gain skip gram original test set
generative adversarial network gans gain lot attention machine learn community due ability learn mimic input data distribution gans consist discriminator generator work tandem play min max game learn target underlie data distribution feed data point sample simpler distribution like uniform gaussian distribution train allow synthetic generation examples sample target distribution investigate application gans generate synthetic feature vectors use speech emotion recognition specifically investigate two set up vanilla gin learn distribution lower dimensional representation actual higher dimensional feature vector ii conditional gin learn distribution higher dimensional feature vectors condition label emotional class belong potential practical application synthetically generate sample measure improvement classifier performance synthetic data use along real data train perform cross validation analyse follow cross corpus study
slot fill important problem speak language understand slu natural language process nlp involve identify user intent assign semantic concept word sentence paper present word feature vector method combine convolutional neural network cnn consider eighteen word feature word feature construct merge similar word label introduce concept external library propose feature set approach beneficial build relationship word train dataset feature computational result report use atis dataset comparisons traditional cnn well bi directional sequential cnn also present
recently neural machine translation nmt extend multilinguality handle one translation direction single system multilingual nmt show competitive performance pure bilingual systems notably low resource settings prove work effectively efficiently thank share representation space force across languages induce sort transfer learn furthermore multilingual nmt enable call zero shoot inference across language pair never see train time despite increase interest framework depth analysis multilingual nmt model capable still miss motivate work provide quantitative comparative analysis translations produce bilingual multilingual zero shoot systems ii investigate translation quality two currently dominant neural architectures mt recurrent transformer ones iii quantitatively explore closeness languages influence zero shoot translation analysis leverage multiple professional post edit automatic translations several different systems focus automatic standard metrics bleu ter widely use error categories lexical morphology word order errors
paper propose self attentive bidirectional long short term memory sa bilstm network predict multiple emotions emotionx challenge bilstm exhibit power model word dependencies extract relevant feature emotion classification build top bilstm self attentive network model contextual dependencies utterances helpful classify ambiguous emotions achieve five hundred and ninety-six five hundred and fifty unweighted accuracy score textitfriends textitemotionpush test set respectively
open domain response generation achieve remarkable progress recent years sometimes yield short uninformative responses propose new paradigm response generation response generation edit significantly increase diversity informativeness generation result assumption plausible response generate slightly revise exist response prototype prototype retrieve pre define index provide good start point generation grammatical informative design response edit model edit vector form consider differences prototype context current context edit vector feed decoder revise prototype response current context experiment result large scale dataset demonstrate response edit model outperform generative retrieval base model various aspects
empirically investigate learn partial feedback neural machine translation nmt partial feedback collect ask users highlight correct chunk translation propose simple effective way utilize feedback nmt train demonstrate common machine translation problem domain mismatch train deployment reduce solely base chunk level user feedback conduct series simulation experiment test effectiveness propose method result show chunk level feedback outperform sentence base feedback two hundred and sixty-one bleu absolute
paper investigate recurrent deep neural network dnns combination regularization techniques dropout zoneout regularization post layer benchmark choose timit phone recognition task due popularity broad availability community also simulate low resource scenario helpful minor languages also prefer phone recognition task much sensitive acoustic model quality large vocabulary continuous speech recognition task recent years recurrent dnns push error rat automatic speech recognition clear winner propose architectures dropout use regularization technique case combination regularization techniques together model ensembles omit however ensemble recurrent dnns perform best achieve average phone error rate ten experiment one thousand, four hundred and eighty-four minimum one thousand, four hundred and sixty-nine core test set slightly lower best publish per date accord knowledge finally contrast paper publish open source script easily replicate result help continue development
recently increase focus misinformation stimulate research fact check task assess truthfulness claim research automate task conduct variety discipline include natural language process machine learn knowledge representation databases journalism substantial progress relevant paper article publish research communities often unaware use inconsistent terminology thus impede understand progress paper survey automate fact check research stem natural language process relate discipline unify task formulations methodologies across paper author furthermore highlight use evidence important distinguish factor among cut across task formulations methods conclude propose avenues future nlp research automate fact check
semantic annotation fundamental deal large scale lexical information map information enumerable set categories rule algorithms apply foundational ontology class use formal set categories task previous alignment wordnet noun synsets dolce provide start point ontology base annotation nlp task verbs also substantial importance work present extension wordnet dolce noun map align verbs accord link nouns denote perdurants transfer verb dolce class assign noun best represent verb occurrence evaluate usefulness resource implement foundational ontology base semantic annotation framework assign high level foundational category word phrase text compare similar annotation tool obtain increase nine hundred and five accuracy
understand semantic relationships term fundamental task natural language process applications structure resources express relationships formal way ontologies still scarce large number linguistic resources gather dictionary definitions become available understand semantic structure natural language definitions fundamental make useful semantic interpretation task base analysis subset wordnet gloss propose set semantic roles compose semantic structure dictionary definition show relate definition syntactic configuration identify pattern use development information extraction frameworks semantic model
identification semantic relations term within texts fundamental task natural language process support applications require lightweight semantic interpretation model currently semantic relation classification concentrate relations evaluate open domain data work provide critique set abstract relations use semantic relation classification regard ability express relationships term find domain specific corpora base analysis work propose alternative semantic relation model base reuse extend set abstract relations present dolce ontology result set relations well ground allow capture wide range relations could thus use foundation automatic classification semantic relations
natural language definitions term serve rich source knowledge structure comprehensible semantic model essential enable use semantic interpretation task propose method provide set tool automatically build graph world knowledge base natural language definitions adopt conceptual model compose set semantic roles dictionary definitions train classifier automatically label definitions prepare data later convert graph representation wordnetgraph knowledge graph build noun verb wordnet definitions accord methodology successfully use interpretable text entailment recognition approach use paths graph provide clear justifications entailment decisions
paper main goal detect movie reviewer opinion use hide conditional random field model allow us capture dynamics reviewer opinion transcripts long unsegmented audio review analyze system high level linguistic feature compute level inter pausal segment feature include syntactic feature statistical word embed model subjectivity lexicons propose system evaluate ict mmmo corpus obtain f1 score eighty-two better logistic regression recurrent neural network approach also offer discussion shed light capacity system adapt word embed model learn general write texts data speak movie review thus model dynamics opinion
paper determine multi layer ensembling improve performance multilingual intent classification develop novel multi layer ensembling approach ensembles different model initializations different model architectures also introduce new bank domain dataset compare result standard atis dataset chinese smp2017 dataset determine ensembling performance multilingual multi domain contexts run ensemble experiment across three datasets conclude ensembling provide significant performance increase multi layer ensembling risk way improve performance intent classification also find diverse ensemble simple model reach perform comparable much sophisticate state art model best f one score atis bank smp nine thousand, seven hundred and fifty-four nine thousand, one hundred and seventy-nine nine thousand, three hundred and fifty-five respectively compare well state art atis best submission smp2017 competition total ensembling performance increase achieve twenty-three one hundred and ninety-six four hundred and four f one respectively
self report diagnosis statements widely employ study language relate mental health social media however exist research largely ignore temporality mental health diagnose work introduce rsdd time new dataset five hundred and ninety-eight manually annotate self report depression diagnosis post reddit include temporal information diagnosis annotations include whether mental health condition present recently diagnosis happen furthermore include exact temporal span relate date diagnosis information valuable various computational methods examine mental health social media one mental health state static also test several baseline classification extraction approach suggest extract temporal information self report diagnosis statements challenge
ontology alignment task identify semantically equivalent entities two give ontologies different ontologies different representations entity result need de duplicate entities merge ontologies propose method enrich entities ontology external definition context information use additional information ontology alignment develop neural architecture capable encode additional information available show addition external data result f1 score sixty-nine ontology alignment evaluation initiative oaei largebio snomed nci subtask comparable entity level matchers sota system
resources label corpora necessary train automatic model within natural language process nlp field historically large number resources regard broad number problems available mostly english one problems know personality identification base psychological model eg big five model goal find traits subject personality give instance text write subject paper introduce new corpus spanish call texts personality identification txpi corpus help develop model automatically assign personality trait author text document corpus txpi contain information four hundred and sixteen mexican undergraduate students demographics information age gender academic program enrol finally additional contribution present set baselines provide comparison scheme research
list popular medium personal information management task increasingly track electronic form mobile desktop organizers potential software support correspond task mean intelligent agents work area personal assistants task work focus classify user intention information extraction show methods perform well across two corpora span sub domains one release
effectively use full syntactic parse information neural network nns solve relational task eg question similarity still open problem paper propose inject structural representations nns learn svm model use tree kernels tks relatively pair question thousands gold standard gs train data typically scarce ii predict label large corpus question pair iii pre train nns large corpus result quora semeval question similarity datasets show nns train approach learn accurate model especially fine tune gs
point interest pois restaurants hotels barber shop part urban areas irrespective specific locations name pois often reveal valuable information relate local culture landmarks influential families figure events place name long study geographers eg understand origins relations family name however lack large scale empirical study examine localness place name change geographic distance addition enhance understand coherence geographic regions empirical study also significant geographic information retrieval inform computational model improve accuracy place name disambiguation work conduct empirical study base one hundred and twelve thousand and seventy-one pois seven us metropolitan areas extract open yelp dataset propose adopt term frequency inverse document frequency geographic contexts identify local term use poi name analyze usages across different poi type result show uneven usage local term across poi type highly consistent among different geographic regions also examine decay effect poi name similarity increase distance among pois analysis focus urban poi name present methods generalize place type well mountain peak streets
coherence across multiple turn major challenge state art dialogue model arguably successful approach automatically learn text coherence entity grid rely model pattern distribution entities across multiple sentence text originally apply evaluation automatic summaries news genre among many extensions model also successfully use assess dialogue coherence nevertheless original grid extensions model intents crucial aspect study widely literature connection dialogue structure propose augment original grid document representation dialogue intentional structure conversation model outperform original grid representation text discrimination insertion two main standard task coherence assessment across three different dialogue datasets confirm intents play key role model dialogue coherence
intuitive way human write paraphrase sentence replace word phrase original sentence correspond synonyms make necessary change ensure new sentence fluent grammatically correct propose novel approach model process dictionary guide edit network effectively conduct rewrite source sentence generate paraphrase sentence jointly learn selection appropriate word level phrase level paraphrase pair context original sentence shelf dictionary well generation fluent natural language sentence specifically system retrieve set word level phrase level araphrased pair derive paraphrase database ppdb original sentence use guide decision word might delete insert soft attention mechanism sequence sequence framework conduct experiment two benchmark datasets paraphrase generation namely mscoco quora dataset evaluation result demonstrate dictionary guide edit network outperform baseline methods
incorporate prior knowledge like lexical constraints model output generate meaningful coherent sentence many applications dialogue system machine translation image caption etc however exist rnn base model incrementally generate sentence leave right via beam search make difficult directly introduce lexical constraints generate sentence paper propose new algorithmic framework dub bfgan address challenge specifically employ backward generator forward generator generate lexically constrain sentence together use discriminator guide joint train two generators assign reward signal due difficulty bfgan train propose several train techniques make train process stable efficient extensive experiment two large scale datasets human evaluation demonstrate bfgan significant improvements previous methods
understand historical texts must aware language include emotional connotation attach word change time paper aim estimate emotion associate give word former language stag english german emotion represent follow popular valence arousal dominance vad annotation scheme expressive polarity alone exist word emotion induction methods typically suit address overcome limitation present adaptations two popular algorithms vad measure effectiveness diachronic settings present first gold standard historical word emotions create scholars proficiency respective language stag cover english german contrast claim previous work find indicate hand select small set seed word supposedly stable emotional mean actually harmful rather helpful
learn real world data stream continuously update model without explicit supervision new challenge nlp applications machine learn components work develop adaptive learn system text simplification improve underlie learn rank model usage data ie users employ system task simplification experimental result show period time performance embed paraphrase rank model increase steadily improve score six thousand, two hundred and eighty-eight seven thousand, five hundred and seventy base ndcg10 evaluation metrics knowledge first study nlp component adaptively improve usage
emotion representation map erm goal convert exist emotion rat one representation format another one eg map valence arousal dominance annotations word sentence ekman basic emotions vice versa erm thus consider alternative word emotion induction wei techniques automatic emotion lexicon construction may also help mitigate problems come proliferation emotion representation format recent years propose new neural network approach erm outperform previous state art equally important present refine evaluation methodology gather strong evidence model yield result almost reliable human annotations even cross lingual settings base result generate new emotion rat thirteen typologically diverse languages claim near gold quality least
generate character level feature important step achieve good result various natural language process task alleviate need human labor generate hand craft feature methods utilize neural architectures convolutional neural network cnn recurrent neural network rnn automatically extract feature propose show great result however cnn generate position independent feature rnn slow since need process character sequentially paper propose novel method use densely connect network automatically extract character level feature propose method require language task specific assumptions show robustness effectiveness faster cnn rnn base methods evaluate method three sequence label task slot tag part speech pos tag name entity recognition ner obtain state art performance nine thousand, six hundred and sixty-two f1 score nine thousand, seven hundred and seventy-three accuracy slot tag pos tag respectively comparable performance state art nine thousand, one hundred and thirteen f1 score ner
multi turn conversation understand major challenge build intelligent dialogue systems work focus retrieval base response match multi turn conversation whose relate work simply concatenate conversation utterances ignore interactions among previous utterances context model paper formulate previous utterances context use propose deep utterance aggregation model form fine grain context representation detail self match attention first introduce route vital information utterance model match response refine utterance final match score obtain attentive turn aggregation experimental result show model outperform state art methods three multi turn conversation benchmarks include newly introduce e commerce dialogue corpus
representation learn foundation machine read comprehension state art model deep learn methods broadly use word character level representations however character naturally minimal linguistic unit addition simple concatenation character word embed previous model actually give suboptimal solution paper propose use subword rather character word embed enhancement also empirically explore different augmentation strategies subword augment embed enhance cloze style read comprehension model reader detail present reader use subword level representation augment word embed short list handle rare word effectively thorough examination conduct evaluate comprehensive performance generalization ability propose reader experimental result show propose approach help reader significantly outperform state art baselines various public datasets
answer question university admission exams gaokao chinese challenge ai task since require effective representation capture complicate semantic relations question answer work propose hybrid neural model deep question answer task history examinations model employ cooperative gate neural network retrieve answer assistance extra label give neural turing machine labeler empirical study show labeler work well small train dataset gate mechanism good fetch semantic representation lengthy answer experiment question answer demonstrate propose model obtain substantial performance gain various neural model baselines term multiple evaluation metrics
use dynamic time warp dtw supervision train convolutional neural network cnn base keyword spot system use small set speak isolate keywords aim allow rapid deployment keyword spot system new language support urgent unite nations un relief program part africa languages extremely resourced development annotate speech resources infeasible first use one thousand, nine hundred and twenty record keywords forty keyword type thirty-four minutes speech exemplars dtw base template match system apply untranscribed broadcast speech use result dtw score target train cnn unlabelled speech way use thirty-four minutes label speech leverage large amount unlabelled data train result cnn keyword spotter match performance dtw base system substantially outperform cnn classifier train keywords improve area roc curve fifty-four sixty-four cnn system several order magnitude faster runtime dtw system represent viable keyword spotter extremely limit dataset
extend sequence sequence model possibility control characteristics style generate output via attention generate priori decode latent code vector train initial attention base sequence sequence model use variational auto encoder condition representations input sequence latent code vector space generate attention matrices sample code vector specific regions latent space decode impose prior attention generate seq2seq model output steer towards certain attribute demonstrate task sentence simplification latent code vector allow control output length lexical simplification enable fine tune optimize different evaluation metrics
resources non english languages scarce paper address problem context machine translation automatically extract parallel sentence pair multilingual article available internet paper use end end siamese bidirectional recurrent neural network generate parallel sentence comparable multilingual article wikipedia subsequently show use harvest dataset improve bleu score nmt phrase base smt systems low resource language pair english hindi english tamil compare train exclusively limit bilingual corpora collect language pair
work address challenge arise extract entities textual data include high cost data annotation model accuracy select appropriate evaluation criteria overall quality annotation present framework integrate entity set expansion ese active learn al reduce annotation cost sparse data provide online evaluation method feedback incremental interactive learn framework allow rapid annotation subsequent extraction sparse data maintain high accuracy evaluate framework three publicly available datasets show drastically reduce cost sparse entity annotation average eighty-five forty-five reach nine ten f score respectively moreover method exhibit robust performance across datasets
recent study sequence sequence learn demonstrate rnn encoder decoder structure successfully generate chinese poetry however exist methods generate poetry give first line user intent theme paper propose three stage multi modal chinese poetry generation approach give picture first line title line poem successively generate three stag accord characteristics chinese poems propose hierarchy attention seq2seq model effectively capture character phrase sentence information contexts improve symmetry deliver poems addition latent dirichlet allocation lda model utilize title generation improve relevance whole poem title compare strong baseline experimental result demonstrate effectiveness approach use machine evaluations well human judgments
pool essential component wide variety sentence representation embed model paper explore generalize pool methods enhance sentence embed propose vector base multi head attention include widely use max pool mean pool scalar self attention special case model benefit properly design penalization term reduce redundancy multi head attention evaluate propose model three different task natural language inference nli author profile sentiment classification experiment show propose model achieve significant improvement strong sentence encode base methods result state art performances four datasets propose approach easily implement problems discuss paper
explore recently introduce definition model technique provide tool evaluation different distribute vector representations word model dictionary definitions word work study problem word ambiguities definition model propose possible solution employ latent variable model soft attention mechanisms quantitative qualitative evaluation analysis model show take account word ambiguity polysemy lead performance improvement
propose entity centric neural cross lingual coreference model build multi lingual embeddings language independent feature perform intrinsic extrinsic evaluations model intrinsic evaluation show model train english test chinese spanish achieve competitive result model train directly chinese spanish respectively extrinsic evaluation show english model help achieve superior entity link accuracy chinese spanish test set top two thousand and fifteen tac system without use annotate data chinese spanish
statistical language model lm play key role automatic speech recognition asr systems use conversational agents asr systems provide high accuracy variety speak style domains vocabulary argots paper present dnn base method adapt lm user agent interaction base generalize contextual information predict optimal context dependent set lm interpolation weight show framework contextual adaptation provide accuracy improvements different possible mixture lm partition relevant one goal orient conversational agents natural partition data request application two non goal orient conversational agents data partition use topic label come predictions topic classifier obtain relative wer improvement three one pass decode strategy six two pass decode framework unadapted model also show fifteen relative improvement recognize name entities significant value conversational asr systems
chart constraints specify string position constituent may begin end show speed chart parsers pcfgs generalize chart constraints expressive grammar formalisms describe neural tagger predict chart constraints high precision constraints accelerate pcfg tag parse combine effectively prune techniques coarse fine supertagging overall speedup two order magnitude improve accuracy
large scale veterinary clinical record become powerful resource patient care research however clinicians lack time resource annotate patient record standard medical diagnostic cod veterinary visit capture free text note lack standard cod make challenge use clinical data improve patient care also major impediment cross species translational research rely ability accurately identify patient cohorts specific diagnostic criteria humans animals order reduce cod burden veterinary clinical practice aid translational research develop deep learn algorithm deeptag automatically infer diagnostic cod veterinary free text note deeptag train newly curated dataset one hundred and twelve thousand, five hundred and fifty-eight veterinary note manually annotate experts deeptag extend multi task lstm improve hierarchical objective capture semantic structure diseases foster human machine collaboration deeptag also learn abstain examples uncertain defer human experts result improve performance deeptag accurately infer disease cod free text even challenge cross hospital settings text come different clinical settings ones use train enable automate disease annotation across broad range clinical diagnose minimal pre process technical framework work apply medical domains currently lack medical cod resources
due fact korean highly agglutinative character rich language previous work korean morphological analysis typically employ use sub character feature know graphemes otherwise utilize comprehensive prior linguistic knowledge ie dictionary know morphological transformation form action model create assumption character level dictionary less morphological analysis intractable due number action require present study multi stage action base model perform morphological transformation part speech tag use arbitrary units input apply case character level korean morphological analysis among model employ prior linguistic knowledge achieve state art word sentence level tag accuracy sejong korean corpus use propose data drive bi lstm model
paper analyse contribution language metrics potentially linguistic structure classify french learners english accord level common european framework reference languages cefrl purpose build model prediction learner level function language complexity feature use efcamdat corpus database one million write assignments learners apply language complexity metrics texts build representation match language metrics texts assign cefrl level lexical syntactic metrics compute lca lsa korpus several supervise learn model build use gradient boost tree keras neural network methods contrast pair cefrl level result show possible implement pairwise distinctions especially level range a1 b1 a1a2 nine hundred and sixteen auc a2b1 nine hundred and four auc model explanation reveal significant linguistic feature predictiveness corpus word tokens word type appear play significant role determine level show level highly dependent specific semantic profile
word choice dependent cultural context writers subject different word use describe similar action object feature base factor class race gender geography political affinity exploratory techniques base locate count word may therefore lead conclusions reinforce culturally inflect boundaries offer new method dualneighbors algorithm link thematically similar document within across discursive linguistic barriers reveal cross cultural connections qualitative quantitative evaluations technique show apply two cultural datasets interest researchers across humanities social sciences open source implementation dualneighbors algorithm provide assist application
although attention base neural machine translation nmt achieve remarkable progress recent years still suffer issue repeat drop translations alleviate issue propose novel key value memory augment attention model nmt call kvmematt specifically maintain timely update keymemory keep track attention history fix value memory store representation source sentence throughout whole translation process via nontrivial transformations iterative interactions two memories decoder focus appropriate source word predict next target word decode step therefore improve adequacy translations experimental result chineseenglish wmt17 germanenglish translation task demonstrate superiority propose model
paper show game theoretic work conversation combine theory discourse structure provide framework study interpretive bias interpretive bias essential feature learn understand also something use pervert subvert truth framework develop provide tool understand analyze range interpretive bias factor contribute
paper examine use case general adversarial network gans field market particular analyze gin model replicate text pattern successful product list airbnb peer peer online market short term apartment rentals define diehl martinez kamalu dmk loss function new class function force model generate output include set user define keywords allow general adversarial network recommend way reword phrase list description increase likelihood book although tailor analysis airbnb data believe framework establish general model generative algorithms use produce text sample purpose market
speak dialog systems conduct fluid conversational interactions users systems must sensitive turn take cue produce user model design effective decisions make appropriate system speak traditional end turn model decisions make utterance end point limit ability model fast turn switch overlap flexible approach model turn take continuous manner use rnns system predict speech probability score discrete frame within future window continuous predictions represent generalize turn take behaviors observe train data apply make decisions limit end turn detection paper investigate optimal speech relate feature set make predictions pause overlap conversation find traditional acoustic feature perform well part speech feature generally perform worse word feature show current model outperform previously report baselines
domain classification speak dialog systems correct detection domain ood utterances crucial reduce confusion unnecessary interaction cost users systems previous work usually utilize ood detectors train separately domain ind classifiers confidence thresholding ood detection give target evaluation score paper introduce neural joint learn model domain classification ood detection dynamic class weight use model train satisfice give ood false acceptance rate far maximize domain classification accuracy evaluate two domain classification task utterances large speak dialogue system show approach significantly improve domain classification performance satisfice give target fars
automatic post edit ape systems aim correct systematic errors make machine translators paper propose neural ape system encode source src machine translate mt sentence two separate encoders leverage share attention mechanism better understand two input contribute generation post edit pe sentence empirical observations show mt incorrect attention shift weight toward tokens src sentence properly edit incorrect translation model train evaluate official data wmt16 wmt17 ape domain english german share task additionally use extra 500k artificial data provide share task system able reproduce accuracies systems train data time provide better interpretability
task orient dialogue systems speak language understand slu refer task parse natural language user utterances semantic frame make use context prior dialogue history hold key effective slu state art approach slu use memory network encode context process multiple utterances dialogue turn result significant trade off accuracy computational efficiency hand downstream components like dialogue state tracker dst already keep track dialogue state serve summary dialogue history work propose efficient approach encode context prior utterances slu specifically architecture include separate recurrent neural network rnn base encode module accumulate dialogue context guide frame parse sub task share slu dst experiment demonstrate effectiveness approach dialogues two domains
machine translation polysynthetic fusional languages challenge task get complicate limit amount parallel text available thus translation performance far state art high resource intensively study language pair would light phenomena hamper automatic translation polysynthetic languages study translations three low resource polysynthetic languages nahuatl wixarika yorem nokki spanish vice versa find morpheme morpheme alignment important amount information contain polysynthetic morphemes spanish counterpart translation often omit conduct qualitative analysis thus identify morpheme type commonly hard align ignore translation process
treat grammatical error correction gec classification problem study different type errors target word identify classifier predict correct word form set possible choices propose novel neural network base feature representation classification model train use large text corpora without human annotations specifically use rnns attention represent leave right context target word feature embeddings learn jointly end end fashion experimental result show novel approach outperform classifier methods conll two thousand and fourteen test set f05 four thousand, five hundred and five model simple effective suitable industrial production
asr system usually predict punctuation capitalization lack punctuation cause problems result presentation confuse human reader andoff shelf natural language process algorithms overcome limitations train two variants deep neural network dnn sequence label model bidirectional long short term memory blstm convolutional neural network cnn predict punctuation model train fisher corpus include punctuation annotation experiment combine time align punctuate fisher corpus transcripts use sequence alignment algorithm neural network train common web crawl glove embed word fisher transcripts align conversation side indicators word time infomation cnns yield better precision blstms tend better recall blstms make fewer mistake overall punctuation predict cnn accurate especially case question mark result constitute significant evidence distribution word time well pre train embeddings useful punctuation prediction task
incorporate linguistic world common sense knowledge ai nlp systems currently important research area several open problems challenge time process store knowledge lexical resources straightforward task tutorial propose address complementary goals two methodological perspectives use nlp methods help process construct enrich lexical resources use lexical resources improve nlp applications two main type audience benefit tutorial work language resources interest become acquaint automatic nlp techniques end goal speed ease process resource curation hand researchers nlp would like benefit knowledge lexical resources improve systems model slide tutorial available https bitbucketorg luisespinosa lr nlp
paper present first neural base machine translation system train translate standard national varieties language take pair brazilian european portuguese example compare performance method phrase base statistical machine translation system report performance improvement nine bleu point translate european brazilian portuguese two bleu point translate opposite direction also carry human evaluation experiment native speakers brazilian portuguese indicate humans prefer output produce neural base system comparison statistical system
present wombat python tool support nlp practitioners access word embeddings code wombat address common research problems include unify access scale robust reproducible preprocessing code use wombat access word embeddings cleaner readable easier reuse also much efficient code use standard memory methods python script use wombat evaluate seven large word embed collections 87m embed vectors total simple semeval sentence similarity task involve two hundred and fifty raw sentence pair complete ten second end end standard notebook computer
past years sentiment analysis increasingly shift attention representational frameworks expressive semantic polarity positive negative neutral however richer format like basic emotions valence arousal dominance variants therefrom root psychological research tend proliferate number representation scheme emotion encode thus large amount representationally incompatible emotion lexicons develop various research group adopt one emotion representation format consequence reusability resources decrease comparability systems use paper propose solve dilemma methods tool map different representation format onto sake mutual compatibility interoperability language resources present first large scale investigation representation mappings four typologically diverse languages find evidence approach produce near gold quality emotion lexicons even cross lingual settings finally use model create new lexicons eight typologically diverse languages
use technology increase data analysis become integral many businesses ability quickly access interpret data become important ever information retrieval technologies utilize organizations company manage information systems process despite information retrieval large amount data efficient organize relational databases user still need master db language schema completely formulate query put burden organizations company hire employees proficient db languages schemas formulate query reduce burden already overstretch data team many organizations look tool allow non developers query databases unfortunately write valid sql query answer question user try ask always easy even seemingly simple question like start company receive 200m fund actually hard answer let alone convert sql query define start company size location duration time incorporate may fine user work database already familiar users familiar database need centralize system effectively translate natural language query specific database query different customer database type number factor dramatically affect system architecture set algorithms use translate nl query structure query representation
linguistic typology aim capture structural semantic variation across world languages large scale typology could provide excellent guidance multilingual natural language process nlp particularly languages suffer lack human label resources present extensive literature survey use typological information development nlp techniques survey demonstrate date use information exist typological databases result consistent modest improvements system performance show due intrinsic limitations databases term coverage feature granularity employment typological feature include advocate new approach adapt broad discrete nature typological categories contextual continuous nature machine learn algorithms use contemporary nlp particular suggest approach could facilitate recent developments data drive induction typological knowledge
task discover topics text corpora dominate latent dirichlet allocation topic model decade order apply approach massive text corpora vocabulary need reduce considerably large computer cluster gpus typically require moreover number topics must provide beforehand depend corpus characteristics often difficult estimate especially massive text corpora unfortunately topic quality time complexity sensitive choice paper describe alternative approach discover topics base min hash handle massive text corpora large vocabularies use modest computer hardware require fix number topics advance basic idea generate multiple random partition corpus vocabulary find set highly co occur word cluster produce final topics contrast probabilistic topic model topics distributions complete vocabulary topics discover propose approach set highly co occur word interestingly topics underlie various thematics different level granularity extensive qualitative quantitative evaluation use twenty newsgroups 18k reuters 800k spanish wikipedia 1m english wikipedia 5m corpora show propose approach able consistently discover meaningful coherent topics remarkably time complexity propose approach linear respect corpus vocabulary size non parallel implementation able discover topics entire english edition wikipedia five million document one million word less seven hours
goal orient dialogue systems typically communicate backend eg database web api complete certain task reach goal intents dialogue system recognize mostly include system developer statically open dialogue system work small set well curated data apis manual intent creation scalable paper introduce straightforward methodology intent creation base semantic annotation data service web method natural language understand nlu module goal orient dialogue system adapt newly introduce apis without require heavy developer involvement able extract intents necessary slot fill schemaorg annotations also able create set initial train sentence classify user utterances generate intents demonstrate approach nlu module state art dialogue system development framework
syntactic dependency annotations concentrate surface functional structure sentence semantic dependency annotations aim capture word relationships closely relate mean sentence use graph structure representations extend lstm base syntactic parser dozat man two thousand and seventeen train generate graph structure result system achieve state art performance beat previous substantially complex state art system six label f1 add linguistically richer input representations push margin even higher allow us beat nineteen label f1
current multimodal sentiment analysis frame sentiment score prediction general machine learn task however sentiment score actually represent often overlook measurement opinions affective state sentiment score generally consist two aspects polarity intensity decompose sentiment score two aspects study convey individual modalities combine multimodal model naturalistic monologue set particular build unimodal multimodal multi task learn model sentiment score prediction main task polarity intensity classification auxiliary task experiment show sentiment analysis benefit multi task learn individual modalities differ convey polarity intensity aspects sentiment
paper discuss enrichment manually develop resource telugu lexicon ontosensenet ontosensenet ontological sense annotate lexicon mark verb telugu primary secondary sense area research relatively recent large scope development provide introductory work enrich ontosensenet promote research telugu classifiers adopt learn sense relevant feature word resource also automate tag sense type verbs perform comparative analysis different classifiers apply ontosensenet result experiment prove automate enrichment resource effective use svm classifiers adaboost ensemble
present work aim generate systematically annotate corpus support enhancement sentiment analysis task telugu use word level sentiment annotations ontosensenet extract eleven thousand adjectives two hundred and fifty-three adverbs eight thousand, four hundred and eighty-three verbs sentiment annotation do language experts discuss methodology follow polarity annotations validate develop resource work aim develop benchmark corpus extension sentiwordnet baseline accuracy model lexeme annotations apply sentiment predictions fundamental aim paper validate study possibility utilize machine learn algorithms word level sentiment annotations task automate sentiment identification furthermore accuracy improve annotate bi grams extract target corpus
paper model fundamental frequency contour mandarin cantonese speech decision tree dnns deep neural network different kinds f0 representations model architectures test decision tree dnns new model call additive blstm additive bidirectional long short term memory predict base f0 contour residual f0 contour two blstms propose respect objective measure rmse correlation apply tone dependent tree together sample normalization delta feature regularization within decision tree framework perform best new additive blstm model delta feature regularization perform even better subjective listen test mandarin cantonese compare random forest model multiple decision tree additive blstm model also hold confirm advantage new model accord listeners preference
development internet natural language process nlp sentiment analysis important task become vital information processingsentiment analysis include aspect sentiment classification aspect sentiment provide complete depth result increase attention aspect level different context word sentence influence sentiment polarity sentence variably polarity vary base different aspects sentence take sentence buy new camera picture quality amaze battery life short example aspect picture quality expect sentiment polarity positive battery life aspect consider sentiment polarity negative therefore aspect important consider explore aspect sentiment sentence recurrent neural network rnn regard good model deal natural language process rnns get good performance aspect sentiment classification include target dependent lstm td lstm target connection lstm tc lstm tang 2015a b ae lstm lstm aeat lstm wang et al 2016there also extensive literatures sentiment classification utilize convolutional neural network little literature aspect sentiment classification use convolutional neural network paper develop attention base input layer aspect information consider input layer incorporate attention base input layer convolutional neural network cnn introduce context word information experiment incorporate aspect information cnn improve latter aspect sentiment classification performance without use syntactic parser external sentiment lexicons benchmark dataset twitter get better performance compare model
shi huang lee two thousand and seventeen obtain state art result english chinese dependency parse combine dynamic program implementations transition base dependency parsers minimal set bidirectional lstm feature however result limit projective parse paper extend approach support non projectivity provide first practical implementation mh4 algorithm ofn4 mildly nonprojective dynamic program parser high coverage non projective treebanks make mh4 compatible minimal transition base feature set introduce transition base interpretation parser items map sequence transition thus obtain first implementation global decode non projective transition base parse demonstrate empirically effective projective counterpart parse number highly non projective languages
zipf law find many human relate field include language frequency word persistently find power law function frequency rank know zipf law however much dispute whether universal law statistical artifact little know mechanisms may shape answer question study conduct large scale cross language investigation zipf law statistical result show zipf laws fifty languages share three segment structural pattern segment demonstrate distinctive linguistic properties lower segment invariably bend downwards deviate theoretical expectation find indicate deviation fundamental universal feature word frequency distributions natural languages statistical error low frequency word computer simulation base dual process theory yield zipf law structural pattern suggest zipf law natural languages motivate common cognitive mechanisms result show zipf law languages motivate cognitive mechanisms like dual process govern human verbal behaviors
lexical analysis believe crucial step towards natural language understand widely study recent years end end lexical analysis model recurrent neural network gain increase attention report introduce deep bi gru crf network jointly model word segmentation part speech tag name entity recognition task train model use several massive corpus pre tag best chinese lexical analysis tool together small yet high quality human annotate corpus conduct balance sample different corpora guarantee influence human annotations fine tune crf decode layer regularly train progress evaluate linguistic experts model achieve nine hundred and fifty-five accuracy test set roughly thirteen relative error reduction previously best chinese lexical analysis tool model computationally efficient achieve speed 23k character per second one thread
paper describe enrichment ontosensenet verb centric lexical resource indian languages major contribution work preservation authentic telugu dictionary develop computational version important native speakers better annotate sense type word mean telugu hence efforts make develop aforementioned telugu dictionary annotations do manually manually annotate gold standard corpus consist eight thousand, four hundred and eighty-three verbs two hundred and fifty-three adverbs one thousand, six hundred and seventy-three adjectives annotations do native speakers accord define annotation guidelines paper provide overview annotation procedure present validation develop resource inter annotator agreement additional word telugu wordnet add resource crowd source annotation statistics compare sense annotate lexicon resource insights
today massive amount musical knowledge store write form testimonies date far back several centuries ago work present different natural language process nlp approach harness potential text collections automatic music knowledge discovery cover different phase prototypical nlp pipeline namely corpus compilation text mine information extraction knowledge graph generation sentiment analysis approach present alongside different use case ie flamenco renaissance popular music large collections document process conclusions stem data drive analyse present discuss
evaluate generation systems automatic metrics bleu cost nothing run show correlate poorly human judgment lead systematic bias certain model improvements hand average human judgments unbiased gold standard often expensive paper use control variates combine automatic metrics human evaluation obtain unbiased estimator lower cost human evaluation alone practice however obtain seven thirteen cost reduction evaluate summarization open response question answer systems prove estimator optimal unbiased estimator lower cost theory highlight two fundamental bottleneck automatic metric prompt show human evaluators need improve obtain greater cost save
rule base machine translation data efficient big data base machine translation approach make appropriate languages low bilingual corpus resources ie minority languages however rule base approach decline popularity relative big data cousins primarily extensive train labour require define language rule address present semantic representation one treat bits mean individual concepts two modify specify one another build network relate entities space time also representation three encapsulate proposition thereby define concepts term concepts support abstraction underlie linguistic ontological detail feature afford exact yet intuitive semantic representation aim handle great variety language reduce labour train time propose natural language generation parse translation strategies also amenable probabilistic model thus learn necessary rule example data
recurrent neural network achieve great success many nlp task however difficulty parallelization recurrent structure take much time train rnns paper introduce slice recurrent neural network srnns could parallelize slice sequence many subsequences srnns ability obtain high level information multiple layer extra parameters prove standard rnn special case srnn use linear activation function without change recurrent units srnns one hundred and thirty-six time fast standard rnns could even faster train longer sequence experiment six largescale sentiment analysis datasets show srnns achieve better performance standard rnns
copy mechanism show effectiveness sequence sequence base neural network model text generation task abstractive sentence summarization question generation however exist work model copy point mechanism consider single word copy source sentence paper propose novel copy framework name sequential copy network seqcopynet learn copy single word also copy sequence input sentence leverage pointer network explicitly select sub span source side target side integrate sequential copy mechanism generation process encoder decoder paradigm experiment abstractive sentence summarization question generation task show propose seqcopynet copy meaningful span outperform baseline model
sentence score sentence selection two main step extractive document summarization systems however previous work treat two separate subtasks paper present novel end end neural network framework extractive document summarization jointly learn score select sentence first read document sentence hierarchical encoder obtain representation sentence build output summary extract sentence one one different previous methods approach integrate selection strategy score model directly predict relative importance give previously select sentence experiment cnn daily mail dataset show propose framework significantly outperform state art extractive summarization model
paper introduce textbfchinese textbfai textbflaw challenge dataset cail2018 first large scale chinese legal dataset judgment prediction dataset contain twenty-six million criminal case publish supreme people court china several time larger datasets exist work judgment prediction moreover annotations judgment result detail rich consist applicable law article charge prison term expect infer accord fact descriptions case comparison implement several conventional text classification baselines judgment prediction experimental result show still challenge current model predict judgment result legal case especially prison term help researchers make improvements legal judgment prediction dataset baselines release cail competitionfootnotehttp cailcipscorgcn
make type languages probable others instance know almost speak languages contain vowel phoneme field linguistic typology seek answer question thereby divine mechanisms underlie human language work tackle problem vowel system typology ie propose generative probability model vowels language contain contrast previous work work directly acoustic information first two formant value rather model discrete set phonemic symbols ipa develop novel generative probability model report result base corpus two hundred and thirty-three languages
quantify linguistic complexity different languages morphological systems verify empirical trade paradigm size irregularity language inflectional paradigms may either large size highly irregular never methodology measure paradigm irregularity entropy surface realization paradigm hard jointly predict surface form paradigm estimate variational approximation measurements take large morphological paradigms thirty-one typologically diverse languages
since amount information internet grow rapidly easy user find relevant information query tackle issue much attention pay automatic document summarization key point successful document summarizer good document representation traditional approach base word overlap mostly fail produce kind representation word embed distribute representation word show excellent performance allow word match semantic level naively concatenate word embeddings make common word dominant turn diminish representation quality paper employ word embeddings improve weight scheme calculate input matrix latent semantic analysis method two embed base weight scheme propose combine calculate value matrix new weight scheme modify versions augment weight entropy frequency new scheme combine strength traditional weight scheme word embed propose approach experimentally evaluate three well know english datasets duc two thousand and two duc two thousand and four multilingual two thousand and fifteen single document summarization english propose model perform comprehensively better compare state art methods least one rouge point lead conclusion provide better document representation better document summary result
notions concreteness imageability traditionally important psycholinguistics gain significance semantic orient natural language process task paper investigate predictability two concepts via supervise learn use word embeddings explanatory variables perform predictions within across languages exploit collections cross lingual embeddings align single vector space show notions concreteness imageability highly predictable within across languages moderate loss twenty correlation predict across languages show cross lingual transfer via word embeddings efficient simple transfer via bilingual dictionaries
deep neural network show good data model capabilities deal challenge large datasets wide range application areas convolutional neural network cnns offer advantage select good feature long short term memory lstm network prove good abilities learn sequential data approach report provide improve result areas image process voice recognition language translation natural language process nlp task sentiment classification short text message twitter challenge task complexity increase arabic language sentiment classification task arabic rich language morphology addition availability accurate pre process tool arabic another current limitation along limit research available area paper investigate benefit integrate cnns lstms report obtain improve accuracy arabic sentiment analysis different datasets additionally seek consider morphological diversity particular arabic word use different sentiment classification level
word segmentation low level nlp task non trivial considerable number languages paper present sequence tag framework apply word segmentation wide range languages different write systems typological characteristics additionally investigate correlations various typological factor word segmentation accuracy experimental result indicate segmentation accuracy positively relate word boundary markers negatively number unique non segmental term base analysis design small set language specific settings extensively evaluate segmentation system universal dependencies datasets model obtain state art accuracies ud languages perform substantially better languages non trivial segment chinese japanese arabic hebrew compare previous work
paper illustrate interface tool develop crowd source explain annotation procedure detail tool name parupalli padajaalam mean web word parupalli aim tool populate ontosensenet sentiment polarity annotate telugu resource recent work show importance word level annotations sentiment analysis basis aim analyze importance sense annotations obtain ontosensenet perform task sentiment analysis explain fea tures extract ontosensenet telugu furthermore compute explain adverbial class distribution verbs ontosensenet task know aid disambiguate word sense help enhance performance word sense disambiguation wsd task
explore novel approach semantic role label srl cast sequence sequence process employ attention base model enrich copy mechanism ensure faithful regeneration input sequence enable interleave generation argument role label apply model monolingual set perform propbank srl english language data constrain sequence generation set enforce copy mechanism allow us analyze performance special properties model manually label data benchmarking state art sequence label model show model able solve srl argument label task english data yet structural decode constraints need add make model truly competitive work represent first step towards advance generative srl label setups
present nmt keras flexible toolkit train deep learn model put particular emphasis development advance applications neural machine translation systems interactive predictive translation protocols long term adaptation translation system via continuous learn nmt keras base extend version popular keras library run theano tensorflow state art neural machine translation model deploy use follow high level framework provide keras give high modularity flexibility also extend tackle different problems image video caption sentence classification visual question answer
paper present system base svm ensembles train character word discriminate five similar languages indo aryan family hindi braj bhasha awadhi bhojpuri magahi investigate performance individual feature combine output single classifiers maximize performance system compete indo aryan language identification ili share task organize within vardial evaluation campaign two thousand and eighteen best entry competition name ilidentification score eight thousand, eight hundred and ninety-five f1 score rank 3rd eight team
paper describe system hit scir submit conll two thousand and eighteen share task multilingual parse raw text universal dependencies base submission stanford win system conll two thousand and seventeen share task make two effective extensions one incorporate deep contextualized word embeddings part speech tagger parser two ensembling parsers train different initialization also explore different ways concatenate treebanks improvements experimental result development data show effectiveness methods final evaluation system rank first accord las seven thousand, five hundred and eighty-four outperform systems large margin
depression one common major concern society proper monitor use devices aid detection could helpful prevent together distress analysis interview corpus daic use build metric base depression detection design metric describe level depression use negative sentence classify participant accordingly score generate algorithm level denote intensity depression result show measure depression complex use text alone factor take consideration paper limitations measure depression use text describe future suggestions make
find name people kill police become increasingly important police shoot get public attention police kill detection unfortunately much work literature address problem early work field citekeith2017identifying propose distant supervision framework base expectation maximization deal multiple appearances name document however base framework take full advantage deep learn model necessitate use hand design feature improve detection performance work present novel deep learn method solve problem police kill recognition propose method rely hierarchical lstms model multiple sentence contain person name interest introduce supervise attention mechanisms base semantical word list dependency tree upweight important contextual word experiment demonstrate benefit propose model yield state art performance police kill detection
paper propose joint architecture capture language rhyme meter sonnet model assess quality generate poems use crowd expert judgements stress rhyme model perform well generate poems largely indistinguishable human write poems expert evaluation however reveal vanilla language model capture meter implicitly machine generate poems still underperform term readability emotion research show importance expert evaluation poetry generation future research look beyond rhyme meter focus poetic language
smooth essential tool many nlp task therefore numerous techniques develop purpose past one widely use smooth methods kneser ney smooth kns variants include modify kneser ney smooth mkns widely consider among best smooth methods available although create original kns intention author develop smooth method preserve marginal distributions original model property maintain develop mkns article would like overcome propose refine version mkns preserve marginal distributions keep advantage previous versions beside advantageous properties novel smooth method show achieve result mkns standard language model task
method pair comparisons establish method psychology article apply obtain continuous sentiment score word comparisons make test persons create initial lexicon n199 german word two fold pair comparison experiment ten different test persons probabilistic model take account logistic model show best agreement result comparison experiment initial lexicon use different ways one create special purpose sentiment lexica addition arbitrary word compare initial word test persons cross validation experiment suggest eighteen two fold comparisons necessary estimate score new yet unknown word provide word select modification method silverstein farrell another application initial lexicon evaluation automatically create corpus base lexica evaluation compare corpus base lexica sentiws senticnet sentiwordnet senticnet four perform best technical report correct extend version presentation make icdm sentire workshop two thousand and sixteen
hierarchical multiscale lstm chung et al 2016a state art language model learn interpretable structure character level input model provide fertile grind cognitive computational linguistics study however high complexity architecture train procedure implementations might hinder applicability provide detail reproduction ablation study architecture shed light potential caveats purpose complex deep learn architectures show simplify certain aspects architecture fact improve performance also investigate linguistic units segment learn various level model argue quality correlate overall performance model language model
paper investigate censorship linguistic perspective collect corpus censor uncensored post number topics build classifier predict censorship decisions independent discussion topics investigation reveal strongest linguistic indicator censor content corpus readability
information extraction traditionally focus extract relations identifiable entities yet texts often also contain count information state subject specific relation number object without mention object example california divide fifty-eight counties count quantifiers help variety task query answer knowledge base curation neglect prior work paper develop first full fledge system extract count information text call cinex employ distant supervision use fact count knowledge base train seed develop novel techniques deal several challenge non maximal train seed due incompleteness knowledge base ii sparse skew observations text source iii high diversity linguistic pattern experiment five human evaluate relations show cinex achieve sixty average precision extract count information large scale experiment demonstrate potential knowledge base enrichment apply cinex two thousand, four hundred and seventy-four frequent relations wikidata cinex assert existence 25m facts one hundred and ten distinct relations twenty-eight exist wikidata facts relations
paper describe approach result participation task one multilingual information extraction clef ehealth two thousand and eighteen challenge address task automatically assign icd ten cod french death certificate use dictionary base approach use materials provide task organizers term icd ten terminology normalize tokenized store tree data structure levenshtein distance use detect typos frequent abbreviations detect manually create small set system achieve f score seven hundred and eighty-six precision seven hundred and ninety-four recall seven hundred and seventy-nine score substantially higher average score systems participate challenge
study collect annotate human human role play dialogues domain weight management two roles conversation seeker look ways lose weight helper provide suggestions help seeker weight loss journey chat dialogues collect annotate novel annotation scheme inspire popular health behavior change theory call trans theoretical model health behavior change also build classifiers automatically predict annotation label use corpus find classification accuracy improve oracle segmentations interlocutors sentence provide compare directly classify unsegmented sentence
introduce dataset contain human author descriptions target locations end trip taxi ride scenario describe data collection method novel annotation scheme support understand descriptions target locations dataset contain target location descriptions synthetic real world image well visual annotations grind truth label dimension vehicles object coordinate target locationdistance direction target location vehicles object use various visual language task also perform pilot experiment corpus could apply visual reference resolution domain
propose novel neural network model joint part speech pos tag dependency parse model extend well know bist graph base dependency parser kiperwasser goldberg two thousand and sixteen incorporate bilstm base tag component produce automatically predict pos tag parser benchmark english penn treebank model obtain strong uas las score nine thousand, four hundred and fifty-one nine thousand, two hundred and eighty-seven respectively produce fifteen absolute improvements bist graph base parser also obtain state art pos tag accuracy nine thousand, seven hundred and ninety-seven furthermore experimental result parse sixty-one big universal dependencies treebanks raw texts show model outperform baseline udpipe straka strakov two thousand and seventeen eight higher average pos tag score thirty-six higher average las score addition model also obtain state art downstream task score biomedical event extraction opinion analysis applications code available together pre train model https githubcom datquocnguyen jptdp
paper describe design use graph base parse framework toolkit uniparse release open source python software package uniparse framework novelly streamline research prototyping development evaluation graph base dependency parse architectures uniparse enable highly efficient sufficiently independent easily readable easily extensible implementations dependency parser components distribute toolkit ready make configurations implementations current state art first order graph base parsers include even efficient cython implementations encoders decoders well require specialise loss function
introduce substantially extend version jeseme interactive website visually explore computationally derive time variant information word mean lexical emotions assemble five large diachronic text corpora jeseme design scholars digital humanities alternative consult manually compile print dictionaries information available tool uniquely combine state art distributional semantics nuanced model human emotions two information stream deem beneficial data drive interpretation texts humanities
cross lingual semantic textual similarity systems estimate degree mean similarity two sentence different language state art algorithms usually employ machine translation combine vast amount feature make approach strongly supervise resource rich difficult use poorly resourced languages paper study linear transformations project monolingual semantic space share space use bilingual dictionaries propose novel transformation build best ideas prior work experiment unsupervised techniques sentence similarity base semantic space show significantly improve word weight transformation outperform methods together word weight lead promise result several datasets different languages
generalize word analogy task across languages provide new intrinsic evaluation method cross lingual semantic space experiment six languages within different language families include english german spanish italian czech croatian state art monolingual semantic space transform share space use dictionaries word translations compare several linear transformations rank experiment monolingual transformation bilingual one semantic space transform another multilingual semantic space transform onto english space versions semantic space show test linear transformations preserve relationships word word analogies lead impressive result achieve average accuracy five hundred and eleven four hundred and thirty-one three hundred and eighty-two monolingual bilingual multilingual semantic space respectively
advance nlp help advance cognitive model examine role artificial neural network current state art many common nlp task return classic case study one thousand, nine hundred and eighty-six rumelhart mcclelland famously introduce neural architecture learn transduce english verb stem past tense form shortly thereafter pinker prince one thousand, nine hundred and eighty-eight present comprehensive rebuttal many rumelhart mcclelland claim much force attack center empirical inadequacy rumelhart mcclelland one thousand, nine hundred and eighty-six model today however model severely outmode show encoder decoder network architectures use modern nlp systems obviate pinker prince criticisms without require simplication past tense map problem suggest empirical performance modern network warrant examination utility linguistic cognitive model
deep learn approach sentiment classification fully exploit sentiment linguistic knowledge paper propose multi sentiment resource enhance attention network mean alleviate problem integrate three kinds sentiment linguistic knowledge eg sentiment lexicon negation word intensity word deep neural network via attention mechanisms use various type sentiment resources mean utilize sentiment relevant information different representation subspaces make effective capture overall semantics sentiment negation intensity word sentiment prediction experimental result demonstrate mean robust superiority strong competitors
license restrictions often become impossible strictly reproduce research result twitter data already months creation corpus situation worsen gradually time pass tweet become inaccessible critical issue reproducible accountable research social media partly solve challenge annotate new twitter like corpus alternative large social medium license compatible reproducible experiment mastodon manually annotate dialogues sentiments corpus train multi task hierarchical recurrent network joint sentiment dialog act recognition experimentally demonstrate transfer learn may efficiently achieve task analyze specific correlations sentiments dialogues social media annotate corpus deep network release open source license
extraction raw text knowledge base entities fine grain type often cast prediction flat set entity type label neglect rich hierarchies type entities contain curated ontologies previous attempt incorporate hierarchical structure yield little benefit restrict shallow ontologies paper present new methods use real complex bilinear mappings integrate hierarchical information yield substantial improvement flat predictions entity link fine grain entity type achieve new state art result end end model benchmark figer dataset also present two new human annotate datasets contain wide deep hierarchies release community encourage research direction medmentions collection pubmed abstract 246k mention map massive umls ontology typenet align freebase type wordnet hierarchy obtain nearly 2k entity type experiment three datasets show substantial gain hierarchy aware train
deep learn techniques recently show successful many natural language process task form state art systems require however large amount annotate data often miss paper explore use domain adversarial learn regularizer avoid overfitting train domain invariant feature deep complex neural network low resource zero resource settings new target domains languages case new languages show monolingual word vectors directly use train without prealignment projection common space learn ad hoc train time reach final performance pretrained multilingual word vectors
neural machine translation nmt common practice stack number recurrent fee forward layer encoder decoder result addition new layer improve translation quality significantly however also lead significant increase number parameters paper propose share parameters across layer thereby lead recurrently stack nmt model empirically show translation quality model recurrently stack single layer six time comparable translation quality model stack six separate layer also show use pseudo parallel corpora back translation lead significant improvements translation quality
syllables play important role speech synthesis speech recognition speak document retrieval novel low cost language agnostic approach divide word correspond syllables present hybrid genetic algorithm construct categorization phone optimize syllabification categorization use top hide markov model sequence classifier find syllable boundaries technique show promise preliminary result train test english word
work focus effectively leverage integrate information concept level well word level via project concepts word lower dimensional space retain critical semantics broad context opinion understand system investigate use fuse embed several core nlp task name entity detection classification automatic speech recognition reranking target sentiment analysis
ecolexicon english corpus eec two hundred and thirty-one million word corpus contemporary environmental texts compile lexicon research group development ecolexicon faber leon arauz reimerink two thousand and sixteen san martin et al two thousand and seventeen terminological knowledge base environment available open corpus well know corpus query system sketch engine kilgarriff et al two thousand and fourteen mean user even without subscription freely access query corpus paper eec introduce de scribe build compile query exploit base functionalities provide sketch engine parameters texts eec classify
keyword extraction fundamental task natural language process facilitate map document concise set representative single multi word phrase keywords text document primarily extract use supervise unsupervised approach paper present unsupervised technique use combination theme weight personalize pagerank algorithm neural phrase embeddings extract rank keywords also introduce efficient way process text document train phrase embeddings use exist techniques share evaluation dataset derive exist dataset use choose underlie embed model evaluations rank keyword extraction perform two benchmark datasets comprise short abstract inspec long scientific paper semeval two thousand and ten show produce result better state art systems
customers face task make purchase unfamiliar product domain might useful provide overview product set help understand expect paper present evaluate method summarise set products natural language focus price range common product feature across set product feature impact price study participants report find summaries useful find evidence summaries influence selections make participants
paper describe system submit share task aggression identification facebook post comment team nishnik previous work demonstrate lstms achieve remarkable performance natural language process task deploy lstm model attention unit system rank 6th 4th hindi subtask facebook comment subtask generalize social media data respectively rank 17th 10th correspond english subtasks
topic identification topic id real world unstructured audio audio instance variable topic shift first break sequential segment segment independently classify first present general purpose method topic id speak segment low resource languages use cascade universal acoustic model translation lexicons english english language topic classification next instead classify segment independently demonstrate explore contextual dependencies across sequential segment provide large improvements particular propose attention base contextual model able leverage contexts selective manner test contextual non contextual model four lorelei languages one attention base contextual model significantly outperform context independent model
previous work show neural encoder decoder speech recognition improve hierarchical multitask learn auxiliary task add intermediate layer deep encoder explore effect hierarchical multitask learn context connectionist temporal classification ctc base speech recognition investigate several aspects approach consistent previous work observe performance improvements telephone conversational speech recognition specifically eval2000 test set train subword level ctc model auxiliary phone loss intermediate layer analyze effect number experimental variables like interpolation constant position auxiliary loss function performance lower resource settings relationship pretraining multitask learn observe hierarchical multitask approach improve standard multitask train higher data experiment low resource settings standard multitask train work well best result obtain combine hierarchical multitask learn pretraining improve word error rat thirty-four absolute eval2000 test set
research show sequence sequence neural model particularly attention mechanism successfully generate classical chinese poems however neural model capable generate poems match specific style impulsive style li bai famous poet tang dynasty work propose memory augment neural model enable generation style specific poetry key idea memory structure store poems desire style generate humans use similar fragment adjust generation demonstrate propose algorithm generate poems flexible style include style particular era individual poet
robust dialogue belief track key component maintain good quality dialogue systems task dialogue systems try solve become increasingly complex require scalability multi domain semantically rich dialogues however current approach difficulty scale domains dependency model parameters dialogue ontology paper novel approach introduce fully utilize semantic similarity dialogue utterances ontology term allow information share across domains evaluation perform recently collect multi domain dialogues dataset one order magnitude larger currently available corpora model demonstrate great capability handle multi domain dialogues simultaneously outperform exist state art model single domain dialogue track task
language analysis reveal underlie social power relations exist participants interaction prior work within nlp show promise area performance automatically predict power relations use nlp analysis social interactions remain want paper present novel neural architecture capture manifestations power within individual email aggregate order preserve way order infer direction power pair participants email thread obtain accuracy eight hundred and four one hundred and one improvement state art methods task apply model task predict power relations individuals base entire set message exchange also model significantly outperform the700 accuracy use prior state art techniques obtain accuracy eight hundred and thirty
recent years situation awareness recognise critical part effective decision make particular crisis management one way extract value allow better situation awareness develop system capable analyse dataset multiple post cluster consistent post different view stories world view however challenge require understand data include determine consistent data data corroborate data attempt address problems article propose subject verb object semantic suffix tree cluster svosstc system support special focus twitter content novelty value svosstc emphasis utilise subject verb object svo typology order construct semantically consistent world view individuals particularly involve crisis response might achieve enhance picture situation social media data evaluate system ability provide enhance situation awareness test exist approach include human data analysis use variety real world scenarios result indicate noteworthy degree evidence eg cluster granularity meaningfulness affirm suitability rigour approach moreover result highlight article proposals innovative practical system contributions research field
previous study show linguistic feature word possession genitive grammatical case employ word representations name entity recognition ner tagger improve performance morphologically rich languages however taggers require external morphological disambiguation md tool function hard obtain non existent many languages work propose model alleviate need disambiguators jointly learn ner md taggers languages one provide list candidate morphological analyse show do independent morphological annotation scheme differ among languages experiment employ three different model architectures join two task show joint learn improve ner performance furthermore morphological disambiguator performance show competitive
coronary artery disease cad one lead cause cardiovascular disease deaths cad condition progress rapidly diagnose treat early stage may eventually lead irreversible state heart muscle death invasive coronary arteriography gold standard technique cad diagnosis coronary arteriography texts describe part stenosis much stenosis detail crucial conduct severity classification cad paper employ recurrent capsule network rcn extract semantic relations clinical name entities chinese coronary arteriography texts automatically find maximal stenosis lumen inference severe cad accord improve method gensini experimental result corpus collect shanghai shuguang hospital show propose method achieve accuracy nine hundred and seventy severity classification cad
unsupervised learn attractive method easily derive meaningful data representations vast amount unlabeled data representations embeddings often yield superior result many task whether use directly feature subsequent train stag however quality embeddings highly dependent assume knowledge unlabeled data system extract information without supervision domain portability also limit unsupervised learn often require train domain corpora achieve robustness work present multitask paradigm unsupervised contextual learn behavioral interactions address unsupervised domain adaption introduce online multitask objective unsupervised learn show sentence embeddings generate process increase performance affective task
determine correct form verb context require understand syntactic structure sentence recurrent neural network show perform task error rate comparable humans despite fact design explicit syntactic representations examine extent syntactic representations network similar use humans process sentence compare detail pattern errors rnns humans make task despite significant similarities attraction errors asymmetry singular plural subject error pattern differ important ways particular complex sentence relative clauses error rat increase rnns decrease humans furthermore rnns show cumulative effect attractors humans conclude least respect syntactic representations acquire rnns fundamentally different use humans
create linguistic resource often do use machine learn model filter content go human annotator go final resource however budget often limit amount available data exceed amount affordable annotation order optimize benefit invest human work argue decide model one employ depend generalize evaluation metrics f score also gain metric model highest f score may necessarily best sequence predict class may lead waste fund annotate false positives yield zero improvement linguistic resource exemplify point case study use real data task build verb noun idiom dictionary show give choice three systems vary f score system highest f score yield highest profit word case cost benefit trade favorable system lower f score
automatic speech recognition still challenge learn useful intermediate representations use high level abstract target units word reason character phoneme base systems tend outperform word base systems hundreds hours train data use paper first show hierarchical multi task train encourage formation useful intermediate representations achieve perform connectionist temporal classification different level network target different granularity model thus perform predictions multiple scale input standard 300h switchboard train setup hierarchical multi task architecture exhibit improvements single task architectures number parameters model obtain one hundred and forty word error rate eval2000 switchboard subset without decoder language model outperform current state art acoustic word model
semantic parse define process map natural language sentence machine interpretable formal representation mean semantic parse use lstm encoder decoder neural network become promise approach however human automate translation natural language provide grammaticality guarantee sentence generate guarantee particularly important practical case data base query critical errors sentence ungrammatical work propose neural architecture call encoder cfg decoder whose output conform give context free grammar result show implementation architecture display correctness provide benchmark accuracy level better literature
ubiquitous method natural language process word embeddings extensively employ map semantic properties word dense vector representation capture semantic syntactic relations among word vectors correspond word meaningful relative neither vector dimension absolute interpretable mean introduce additive modification objective function embed learn algorithm encourage embed vectors word semantically relate predefined concept take larger value along specify dimension leave original semantic learn mechanism mostly unaffected word align word already determine relate along predefined concepts therefore impart interpretability word embed assign mean vector dimension predefined concepts derive external lexical resource paper choose roget thesaurus observe alignment along choose concepts limit word thesaurus extend relate word well quantify extent interpretability assignment mean experimental result manual human evaluation result also present verify propose method increase interpretability also demonstrate preservation semantic coherence result vector space use word analogy word similarity test test show interpretability impart word embeddings obtain propose framework sacrifice performances common benchmark test
clinical text classification important problem medical natural language process exist study conventionally focus rule knowledge source base feature engineer exploit effective feature learn capability deep learn methods study propose novel approach combine rule base feature knowledge guide deep learn techniques effective disease classification critical step method include identify trigger phrase predict class examples use trigger phrase train convolutional neural network word embeddings unify medical language system umls entity embeddings evaluate method two thousand and eight integrate informatics biology bedside i2b2 obesity challenge result show method outperform state art methods
paper investigate statistical model compression apply natural language understand nlu model small footprint nlu model important enable offline systems hardware restrict devices decrease demand model load latency cloud base systems compress nlu model present two main techniques parameter quantization perfect feature hash techniques complementary exist model prune strategies l1 regularization perform experiment large scale nlu system result show approach achieve fourteen fold reduction memory usage compare original model minimal predictive performance impact
sequence sequence seq2seq learn recently use abstractive extractive summarization current study seq2seq model use ebay product description summarization propose novel document context base seq2seq model use rnns abstractive extractive summarizations intuitively similar humans read title abstract contextual information read document give humans high level idea document use idea propose seq2seq model start contextual information first time step input obtain better summaries manner output summaries document centric generic overcome one major hurdle use generative model generate document context user behavior seller provide information train evaluate model human extract golden summaries document contextual seq2seq model outperform standard seq2seq model moreover generate human extract summaries prohibitively expensive scale therefore propose semi supervise technique extract approximate summaries use train seq2seq model scale semi supervise model evaluate human extract summaries find similar efficacy provide side side comparison abstractive extractive summarizers contextual non contextual evaluation dataset overall provide methodologies use evaluate propose techniques large document summarization furthermore find techniques highly effective case exist techniques
computational visual storytelling produce textual description events interpretations depict sequence image texts make possible advance cross disciplinary approach natural language process generation computer vision define computational creative visual storytelling one ability alter tell story along three aspects speak different environments produce variations base narrative goals adapt narrative audience aspects creative storytelling effect narrative yet explore visual storytelling paper present pipeline task modules object identification single image inferencing multi image narration serve preliminary design build creative visual storyteller pilot design sequence image annotation task present analyze collect corpus describe plan towards automation
pca base sequence vector seq2vec dimension reduction method text classification problem call tree structure multi stage principal component analysis tmpca present paper theoretical analysis applicability tmpca demonstrate extension previous work su huang kuo unlike conventional word vector embed methods tmpca method conduct dimension reduction sequence level without label train data furthermore preserve sequential structure input sequence show tmpca computationally efficient able facilitate sequence base text classification task preserve strong mutual information input output mathematically also demonstrate experimental result dense fully connect network train tmpca preprocessed data achieve better performance state art fasttext neural network base solutions
paper present gdiclassification entry second german dialect identification gdi share task organize within scope vardial evaluation campaign two thousand and eighteen present system base svm classifier ensembles train character word system train collection speech transcripts five swiss german dialects provide organizers transcripts include dataset contain speakers basel bern lucerne zurich entry challenge reach six thousand, two hundred and three f1 score rank third eight team
publish article high impact english journals difficult scholars around world especially non native english speak scholars nness struggle proficiency english order uncover differences english scientific write native english speak scholars ness nness collect large scale data set contain one hundred and fifty thousand full text article publish plos two thousand and six two thousand and fifteen divide article three group accord ethnic background first correspond author obtain ethnea examine scientific write style english two fold perspective linguistic complexity one syntactic complexity include measurements sentence length sentence complexity two lexical complexity include measurements lexical diversity lexical density lexical sophistication observations suggest marginal differences group syntactical lexical complexity
dialog act da recognition task widely explore years recently approach task explore different dnn architectures combine representations word segment generate segment representation provide cue intention study explore mean generate informative segment representations explore different network architectures also consider different token representations word level also character functional level word level addition commonly use uncontextualized embeddings explore use contextualized representations provide information concern word sense segment structure character level tokenization important capture intention relate morphological aspects capture word level finally functional level provide abstraction word shift focus structure segment also explore approach enrich segment representation context information history dialog term classifications surround segment turn take history kind information already prove important disambiguation das previous study nevertheless able capture additional information consider summary dialog history wider turn take context combine best approach step achieve result surpass previous state art generic da recognition swda mrda two widely explore corpora task furthermore consider past future context simulate annotation scenario approach achieve performance similar human annotator swda surpass mrda
although neural machine translationnmt yield promise translation performance unfortunately suffer translation sue tu et al two thousand and sixteen study become research hotspots nmt present study mainly apply dominant automatic evaluation metrics bleu evaluate overall translation quality respect adequacy uency however unable accurately measure ability nmt systems deal mention issue paper propose two quantitative metrics otem utem automatically evaluate system perfor mance term translation respectively metrics base proportion mismatch n grams gold ref erence system translation evaluate metrics compare score human evaluations value pearson cor relation coefficient reveal strong correlation moreover depth analyse various translation systems indicate inconsistency tween bleu propose metrics highlight necessity significance metrics
argumentation mine require identification complex discourse structure lately apply success monolingually work show exist resources however adequate assess cross lingual due heterogeneity lack complexity therefore create suitable parallel corpora human machine translate popular dataset consist persuasive student essay german french spanish chinese compare annotation projection ii bilingual word embeddings base direct transfer strategies cross lingual find former perform considerably better almost eliminate loss cross lingual transfer moreover find annotation projection work equally well use either costly human cheap machine translations code data available urlhttp githubcom ukplab coling2018 xlingargumentmining
recent debate adults theory mind use fuel surprise failures perspective take communication suggest perspective take relatively effortful speakers listeners allocate resources achieve successful communication begin observation share goal induce natural division labor resources one agent choose allocate toward perspective take depend expectations allocation formalize idea resource rational model augment recent probabilistic weight account mechanism costly control degree perspective take series simulations first derive intermediate degree perspective weight optimal tradeoff expect cost benefit perspective take present two behavioral experiment test novel predictions model experiment one manipulate presence absence occlusions director matcher task find speakers spontaneously produce informative descriptions account know unknowns partner private view experiment two compare script utterances use confederate prior work produce interactions unscripted directors find confederate systematically less informative listeners would initially expect give presence occlusions listeners use violations adaptively make fewer errors time take together work suggest people simply mindblind use contextually appropriate expectations navigate division labor partner discuss resource rational framework may provide deeply explanatory foundation understand flexible perspective take process constraints
recent advance statistical machine translation via adoption neural sequence sequence model empower end end system achieve state art many wmt benchmarks performance machine translation mt system usually evaluate automatic metric bleu golden reference provide validation however model inference production deployment golden reference prohibitively available require expensive human annotation bilingual expertise order address issue quality evaluation qe without reference propose general framework automatic evaluation translation output wmt quality evaluation task first build conditional target language model novel bidirectional transformer name neural bilingual expert model pre train large parallel corpora feature extraction qe inference bilingual expert model simultaneously produce joint latent representation source translation real value measurements possible erroneous tokens base prior knowledge learn parallel data subsequently feature feed simple bi lstm predictive model quality evaluation experimental result show approach achieve state art performance quality estimation track wmt two thousand and seventeen two thousand and eighteen
text classification areas sentiment analysis subjectivity objectivity analysis opinion polarity convolutional neural network gain special attention performance accuracy work apply recent advance cnns propose novel architecture multiple block convolutional highways mbch achieve improve accuracy multiple popular benchmark datasets compare previous architectures mbch base new techniques architectures include highway network densenet batch normalization bottleneck layer addition cope limitations exist pre train word vectors use input cnn propose novel method improve word vectors iwv iwv improve accuracy cnns use text classification task
summarize content contribute individuals challenge people make different lexical choices even describe events however remain significant need summarize content examples include student responses post class reflective question product review news article publish different news agencies relate events high lexical diversity document hinder system ability effectively identify salient content reduce summary redundancy paper overcome issue introduce integer linear program base summarization framework incorporate low rank approximation sentence word co occurrence matrix intrinsically group semantically similar lexical items conduct extensive experiment datasets student responses product review news document approach compare favorably number extractive baselines well neural abstractive summarization system paper finally shed light propose framework effective summarize content high lexical variety
introduce variability maintain coherence core task learn generate utterances conversation standard neural encoder decoder model extensions use conditional variational autoencoder often result either trivial digressive responses overcome explore novel approach inject variability neural encoder decoder via use external memory mixture model namely variational memory encoder decoder vmed associate memory read mode latent mixture distribution timestep model capture variability observe sequential data natural conversations empirically compare propose model recent approach various conversational datasets result show vmed consistently achieve significant improvement others metric base qualitative evaluations
discover whether word semantically relate identify specific semantic relation hold crucial importance nlp essential task like query expansion ir within context different methodologies propose either exclusively focus single lexical relation eg hypernymy vs random learn specific classifiers capable identify multiple semantic relations eg hypernymy vs synonymy vs random paper propose another way look problem rely multi task learn paradigm particular want study whether learn process give semantic relation eg hypernymy improve concurrent learn another semantic relation eg co hyponymy within context particularly examine benefit semi supervise learn train prediction function perform label data jointly many unlabeled ones preliminary result base simple learn strategies state art distributional feature representations show concurrent learn lead improvements vast majority test situations
high quality automatic speech recognition asr prerequisite speech base applications research state art asr software freely available language dependent acoustic model lack languages english due limit amount freely available train data train acoustic model german kaldi two datasets distribute creative commons license result model freely redistributable lower cost entry german asr model train total four hundred and twelve hours german read speech data achieve relative word error reduction twenty-six add data speak wikipedia corpus previously best freely available german acoustic model recipe dataset best model achieve word error rate one thousand, four hundred and thirty-eight tuda de test set due large amount speakers diversity topics include train data model robust speaker variation topic shift
automatic grade new approach need adapt latest technology automatic grade become important technology rapidly become powerful score exams essay especially 1990s onwards partially wholly automate grade systems use computational methods evolve become major area research particular demand score natural language responses create need tool apply automatically grade responses paper focus concept automatic grade short answer question typical uk gcse system provide useful feedback answer students present experimental result dataset provide introductory computer science class university north texas first apply standard data mine techniques corpus student answer purpose measure similarity student answer model answer base number common word evaluate relation similarities mark award scorers consider approach group student answer cluster cluster would award mark feedback give answer cluster manner demonstrate cluster indicate group students award similar score word cluster compare show cluster construct base many word model answer use main novelty paper design model predict mark base similarities student answer model answer
present deep generative model bilingual sentence pair machine translation model generate source target sentence jointly share latent representation parameterised neural network perform efficient train use amortise variational inference reparameterised gradients additionally discuss statistical implications joint model propose efficient approximation maximum posteriori decode fast test time predictions demonstrate effectiveness model three machine translation scenarios domain train mix domain train learn mix gold standard synthetic data experiment show consistently joint formulation outperform conditional model ie standard neural machine translation scenarios
concept tag type structure learn need natural language understand nlu systems task mean label domain ontology assign word sequence paper review algorithms develop last twenty five years perform comparative evaluation generative discriminative deep learn methods two public datasets report statistical variability performance measurements third contribution release repository algorithms datasets recipes nlu evaluation
paper present extension low resource parallel corpus collect endanger language griko make useful computational research corpus consist three hundred and thirty utterances twenty minutes speech transcribe translate italian annotations word level speech transcription speech translation alignments corpus also include morphosyntactic tag word level gloss apply automatic unit discovery method pseudo phone also generate detail corpus collect clean process illustrate use zero resource task present baseline result task speech translation alignment unsupervised word discovery dataset available online aim encourage replicability diversity computational language documentation experiment
name entities text document name people organization location type object document exist real world persist research challenge use computational techniques identify entities text document identify several text mine tool algorithms utilize leverage discover name entities improve nlp applications paper method cluster prominent name people organizations base semantic similarity text corpus propose method rely common name entity recognition techniques recent word embeddings model semantic similarity score generate use word embeddings model name entities use cluster similar entities people organizations type two human judge evaluate ten variations method run corpus consist four thousand, eight hundred and twenty-one article specific topic performance method measure use three quantitative measure result three metrics demonstrate method effective cluster semantically similar name entities
last several years field natural language process propel forward explosion use deep learn model survey provide brief introduction field quick overview deep learn architectures methods sift plethora recent study summarize large assortment relevant contributions analyze research areas include several core linguistic process issue addition number applications computational linguistics discussion current state art provide along recommendations future research field
paper propose novel data augmentation method attention base end end automatic speech recognition e2e asr utilize large amount text pair speech signal inspire back translation technique propose field machine translation build neural text encoder model predict sequence hide state extract pre train e2e asr encoder sequence character use hide state target instead acoustic feature possible achieve faster attention learn reduce computational cost thank sub sample e2e asr encoder also use hide state avoid model speaker dependencies unlike acoustic feature train text encoder model generate hide state large amount unpaired text e2e asr decoder retrain use generate hide state additional train data experimental evaluation use librispeech dataset demonstrate propose method achieve improvement asr performance reduce number unknown word without need pair data
paper describe several techniques improve acoustic language model automatic speech recognition asr system operate code switch cs speech focus recognition frisian dutch radio broadcast one mix languages namely frisian resourced language previous work propose several automatic transcription strategies cs speech increase amount available train speech data work explore acoustic model benefit monolingual speech data belong high resourced mix language purpose train state art ams ineffective due lack train data significantly increase amount cs speech monolingual dutch speech moreover improve language model lm create code switch text practice almost non existent one generate text use recurrent lms train transcriptions train cs speech data two add transcriptions automatically transcribe cs speech data three translate dutch text extract transcriptions large dutch speech corpora report significantly improve cs asr performance due increase acoustic textual train data
work investigate joint use articulatory acoustic feature automatic speech recognition asr pathological speech despite long last efforts build speaker text independent asr systems people dysarthria performance state art systems still considerably lower type speech normal speech prominent reason inferior performance high variability pathological speech characterize spectrotemporal deviations cause articulatory impairments due various etiologies cope high variation propose use speech representations utilize articulatory information together acoustic properties designate acoustic model namely fuse feature map convolutional neural network fcnn perform frequency convolution acoustic feature time convolution articulatory feature train test dutch flemish pathological speech corpus asr performance fcnn base asr system use joint feature compare neural network architectures conventional cnns time frequency convolutional network tfcnns several train scenarios
present first efforts towards build single multilingual automatic speech recognition asr system process code switch cs speech five languages speak within population contrast relate prior work focus recognition cs speech bilingual scenarios recently compile small five language corpus south african soap opera speech contain examples cs five languages occur various contexts use english matrix language switch indigenous languages asr system present work train four corpora contain english isizulu english isixhosa english setswana english sesotho cs speech interpolation multiple language model train language pair enable asr system hypothesize mix word sequence five languages evaluate various state art acoustic model train five lingual train data report asr accuracy language recognition performance development test set south african multilingual soap opera corpus
paper investigate cross lingual document embed method improve current neural machine translation framework base document vector ntdv simply nv nv develop self attention mechanism neural machine translation nmt framework nv pair parallel document different languages project share layer model however pair nv embeddings guarantee similar paper add distance constraint train objective function nv two embeddings parallel document require close possible new method call constrain nv cnv cross lingual document classification task new cnv perform well nv outperform publish study require forward pass decode compare previous nv cnv need translator test method lighter flexible
convolutional neural network cnn recurrent neural network rnn model become mainstream methods relation classification propose unify architecture exploit advantage cnn rnn simultaneously identify medical relations clinical record word embed feature model learn phrase level feature cnn layer feature representations directly feed bidirectional gate recurrent unit gru layer capture long term feature dependencies evaluate model two clinical datasets experiment demonstrate model perform significantly better previous single model methods datasets
unprecedented growth internet users recent years result abundance unstructured information form social media text large percentage population actively engage health social network share health relate information paper address important timely topic analyze users sentiments emotions wrt medical condition towards examine users popular medical forums patientinfodailystrengthorg post important topics asthma allergy depression anxiety first provide benchmark setup task crawl data define sentiment specific fine grain medical condition recover exist deteriorate propose effective architecture use convolutional neural network cnn data drive feature extractor support vector machine svm classifier develop sentiment feature sensitive medical context show use medical sentiment feature along extract feature cnn improve model performance addition dataset also evaluate approach benchmark clef ehealth two thousand and fourteen corpora show model outperform state art techniques
neural machine translation nmt computational cost output layer increase size target side vocabulary use limit size vocabulary instead may significant decrease translation quality trade derive softmax base loss function handle dictionary word independently word similarity consider paper propose novel nmt loss function include word similarity form distance word embed space propose loss function encourage nmt decoder generate word close reference embed space help decoder choose similar acceptable word actual best candidates include vocabulary due size limitation experiment use aspec japanese english iwslt17 english french data set propose method show improvements standard nmt baseline datasets especially iwslt17 en fr achieve one hundred and seventy-two bleu one hundred and ninety-nine meteor target side vocabulary limit one thousand word propose method demonstrate substantial gain one hundred and seventy-two meteor aspec ja en
study application active learn techniques translation unbounded data stream via interactive neural machine translation main idea select unbounded stream source sentence worth supervise human agent user interactively translate sample validate data useful adapt neural machine translation model propose two novel methods select sample validate exploit information attention mechanism neural machine translation system experiment show inclusion active learn techniques pipeline allow reduce effort require process increase quality translation system moreover enable balance human effort require achieve certain translation quality moreover neural system outperform classical approach large margin
present open information extraction ie approach use two layer transformation stage consist clausal disembedding layer phrasal disembedding layer together rhetorical relation identification way convert sentence present complex linguistic structure simplify syntactically sound sentence extract proposition represent two layer hierarchy form core relational tuples accompany contextual information semantically link via rhetorical relations comparative evaluation demonstrate reference implementation graphene outperform state art open ie systems construction correct n ary predicate argument structure moreover show exist open ie approach benefit transformation process framework
work define task teaser generation provide evaluation benchmark baseline systems process generate teasers teaser short read suggestion article illustrative include curiosity arouse elements entice potential readers read particular news items teasers one main vehicles transmit news social media users compile novel dataset teasers systematically accumulate tweet select conform teaser definition compare number neural abstractive architectures task teaser generation overall best perform system see et al2017 seq2seq pointer network
work describe system build three english subtasks semeval two thousand and sixteen task three department computer science university houston uh pattern recognition human language technology prhlt research center universitat politecnica de valencia uh prhlt system represent instance use lexical semantic base similarity measure text pair semantic feature include use distribute representations word knowledge graph generate babelnet multilingual semantic network framenet lexical database experimental result outperform random google search engine baselines three english subtasks approach obtain highest result subtask b compare task participants
paper doubly attentive transformer machine translation model datnmt present doubly attentive transformer decoder normally join spatial visual feature obtain via pretrained convolutional neural network conquer gap image caption translation framework transformer decoder figure take care source language word part image freely methods two separate attention components enhance multi head attention layer doubly attentive transformer generate word target language find propose model effectively exploit scarce multimodal machine translation data also large general domain text machine translation corpora image text image caption corpora experimental result show propose doubly attentive transformer decoder perform better single decoder transformer model give state art result english german multimodal machine translation task
fast grow amount information internet make research automatic document summarization urgent effective solution information overload many approach propose base different strategies latent semantic analysis lsa however lsa apply document summarization limitations diminish performance work try overcome limitations apply statistic linear algebraic approach combine syntactic semantic process text first part speech tagger utilize reduce dimension lsa weight term four adjacent sentence add weight scheme calculate input matrix take account word order syntactic relations addition new lsa base sentence selection algorithm propose term description combine sentence description topic turn make generate summary informative diverse ensure effectiveness propose lsa base sentence selection algorithm extensive experiment arabic english do four datasets use evaluate new model linguistic data consortium ldc arabic newswire corpus essex arabic summaries corpus easc duc2002 multilingual mss two thousand and fifteen dataset experimental result four datasets show effectiveness propose model arabic english datasets perform comprehensively better compare state art methods
paper present system trac two thousand and eighteen share task aggression identification best systems english dataset use combination lexical semantic feature however hindi data use lexical feature give us best result obtain weight f1 measure five thousand, nine hundred and twenty-one english facebook task rank 12th five thousand, six hundred and sixty-three english social media task rank 6th six thousand, two hundred and ninety-two hindi facebook task rank 1st four thousand, eight hundred and fifty-three hindi social media task rank 2nd
examine whether neural natural language process nlp systems reflect historical bias train data define general benchmark quantify gender bias variety neural nlp task empirical evaluation state art neural coreference resolution textbook rnn base language model train benchmark datasets find significant gender bias model view occupations mitigate bias cda generic methodology corpus augmentation via causal interventions break associations gendered gender neutral word empirically show cda effectively decrease gender bias preserve accuracy also explore space mitigation strategies cda prior approach word embed debiasing wed compositions show cda outperform wed drastically word embeddings train pre train embeddings two methods effectively compose also find train proceed original data set gradient descent gender bias grow loss reduce indicate optimization encourage bias cda mitigate behavior
paper present effective approach parallel corpus mine use bilingual sentence embeddings embed model train produce similar representations exclusively bilingual sentence pair translations achieve use novel train method introduce hard negative consist sentence translations degree semantic similarity quality result embeddings evaluate parallel corpus reconstruction assess machine translation systems train gold vs mine sentence pair find sentence embeddings use reconstruct unite nations parallel corpus sentence level precision four hundred and eighty-nine en fr five hundred and forty-nine en es adapt document level match achieve parallel document match accuracy comparable significantly computationally intensive approach jakob two thousand and ten use reconstruct parallel data able train nmt model perform nearly well model train original data within one two bleu
cross lingual cross domain correspondences play key roles task range machine translation transfer learn recently purely unsupervised methods operate monolingual embeddings become effective alignment tool current state art methods however involve multiple step include heuristic post hoc refinement strategies paper cast correspondence problem directly optimal transport ot problem build idea word embeddings arise metric recovery algorithms indeed exploit gromov wasserstein distance measure similarities pair word relate across languages show ot objective estimate efficiently require little tune result performance comparable state art various unsupervised word translation task
rnn language model achieve state art perplexity result prove useful suite nlp task yet unclear syntactic generalizations learn investigate whether state art rnn language model represent long distance filler gap dependencies constraints examine rnn behavior experimentally control sentence design expose filler gap dependencies show rnns represent relationship multiple syntactic position large span text furthermore show rnns learn subset know restrictions filler gap dependencies know island constraints rnns show evidence wh islands adjunct islands complex np islands study demonstrate state art rnn model able learn generalize empty syntactic position
recent approach bilingual dictionary induction find linear alignment word vector space two languages show project two languages onto third latent space rather directly onto equivalent term expressivity make easier learn approximate alignments modify approach also allow support languages include alignment process obtain even better performance low resource settings
character language model access surface morphological pattern clear whether learn abstract morphological regularities instrument character language model several probe find develop specific unit identify word boundaries extension morpheme boundaries allow capture linguistic properties regularities units language model prove surprisingly good identify selectional restrictions english derivational morphemes task require morphological syntactic awareness thus conclude morphemes overlap extensively word language character language model perform morphological abstraction
neural text generation neural machine translation summarization image caption beam search widely use improve output text quality however neural generation set hypotheses finish different step make difficult decide end beam search ensure optimality propose provably optimal beam search algorithm always return optimal score complete hypothesis modulo beam size finish soon optimality establish finish later baseline counter neural generation tendency shorter hypotheses also introduce bound length reward mechanism allow modify version beam search algorithm remain optimal experiment neural machine translation demonstrate principled beam search algorithm lead improvement bleu score previously propose alternatives
punctuation strong indicator syntactic structure parsers train text punctuation often rely heavily signal punctuation diversion however since human language process rely punctuation extent informal texts therefore often leave punctuation also use punctuation ungrammatically emphatic creative purpose simply mistake show dependency parsers sensitive absence punctuation alternative use b neural parsers tend sensitive vintage parsers c train neural parsers without punctuation outperform box parsers across scenarios punctuation depart standard punctuation main experiment synthetically corrupt data study effect punctuation isolation avoid potential confound also show effect domain data
propose novel dependency base hybrid tree model semantic parse convert natural language utterance machine interpretable mean representations unlike previous state art model semantic information interpret latent dependency natural language word joint representation dependency information capture interactions semantics natural language word integrate neural component model propose efficient dynamic program algorithm perform tractable inference extensive experiment standard multilingual geoquery dataset eight languages demonstrate propose approach able achieve state art performance across several languages analysis also justify effectiveness use new dependency base representation
neural machine translation usually adopt autoregressive model suffer exposure bias well consequent error propagation problem many previous work discuss relationship error propagation emphaccuracy drop ie leave part translate sentence often better right part leave right decode model problem paper conduct series analyse deeply understand problem get several interest find one role error propagation accuracy drop overstate literature although indeed contribute accuracy drop problem two characteristics language play important role cause accuracy drop leave part translation result right branch language eg english likely accurate right part right part accurate leave branch language eg japanese discoveries confirm different model structure include transformer rnn sequence generation task text summarization
neural machine translation nmt typically leverage monolingual data train backtranslation investigate alternative simple method use monolingual data nmt train combine score pre train fix language model lm score translation model tm tm train scratch achieve train translation model predict residual probability train data add prediction lm enable tm focus capacity model source sentence since rely lm fluency show method outperform previous approach integrate lms nmt architecture simpler require gate network balance tm lm observe gain twenty-four two hundred and thirty-six bleu four test set english turkish turkish english estonian english xhosa english top ensembles without lm compare method alternative ways utilize monolingual data backtranslation shallow fusion cold fusion
task word level quality estimation qe consist take source sentence machine generate translation predict word output correct wrong paper propose method effectively encode local global contextual information target word use three part neural network approach first part use embed layer represent word part speech tag languages second part leverage one dimensional convolution layer integrate local context information target word third part apply stack fee forward recurrent neural network encode global context sentence make predictions model submit cmu entry wmt2018 share task qe achieve strong result rank first three six track
paper present challenge community generative adversarial network gans perfectly align independent english word embeddings induce use algorithm base distributional information alone fail two different embeddings algorithms believe understand key understand modern word embed algorithms limitations instability dynamics gans paper show case alignment fail exist linear transform two embeddings algorithm bias lead non linear differences b similar effect easily obtain vary hyper parameters one plausible suggestion base initial experiment differences inductive bias embed algorithms lead optimization landscape riddle local optima lead small basin convergence present challenge paper technical contribution
paper describe multimodal neural machine translation systems develop lium cvc wmt18 share task multimodal translation year propose several modifications previous multimodal attention architecture order better integrate convolutional feature refine use encoder side information final constrain submissions rank first english french second english german language pair among constrain submissions accord automatic evaluation metric meteor
paper describe microsoft university edinburgh submission automatic post edit share task wmt2018 base train data systems wmt2017 share task implement model last share task introduce improvements base extensive parameter share next experiment implementation dual source transformer model data selection domain submissions decisively win smt post edit sub task establish new state art close second equal one thousand, six hundred and forty-six vs one thousand, six hundred and fifty ter nmt sub task base rather weak result nmt sub task hypothesize neural neural ape might actually useful
paper describe microsoft submission wmt2018 news translation share task participate one language direction english german system follow current best practice combine state art model new data filter dual conditional cross entropy filter sentence weight methods train fairly standard transformer big model update version edinburgh train scheme wmt2017 experiment different filter scheme paracrawl accord automatic metrics bleu reach highest score subtask nearly two bleu point margin next strongest system base human evaluation rank first among constrain systems believe mostly cause data filter weight regime
work introduce dual conditional cross entropy filter noisy parallel data sentence pair noisy parallel corpus compute cross entropy score accord two inverse translation model train clean data penalize divergent cross entropies weigh penalty cross entropy average model sort thresholding accord score result better subsets parallel data achieve higher bleu score model train parallel data filter paracrawl model train clean wmt data evaluate method context wmt2018 share task parallel corpus filter achieve overall highest rank score share task score top three four subtasks
introduce advance information extraction pipeline automatically process large collections unstructured textual data purpose investigative journalism pipeline serve new input processor upcoming major release new leak twenty software develop cooperation large german news organization use case journalists receive large collection file several gigabytes contain unknown content collections may originate either official disclosures document eg freedom information act request unofficial data leak software prepare visually aid exploration collection quickly learn potential stories contain data base automatic extraction entities co occurrence document contrast comparable project focus follow three major requirements particularly serve use case investigative journalism cross border collaborations one composition multiple state art nlp tool entity extraction two support multi lingual document set forty languages three fast easy use extraction full text metadata entities various file format
recent years see grow interest conversational agents chatbots good fit automate customer support domain need operate narrow interest part inspire recent advance neural machine translation esp rise sequence sequence seq2seq attention base model transformer apply various task open new research directions question answer chatbots conversational systems still many case might feasible even preferable use simple information retrieval techniques thus compare three different modelsi retrieval model ii sequence sequence model attention iii transformer experiment twitter customer support dataset contain two million post customer support service twenty major brand show seq2seq model outperform two term semantics word overlap
popular application machine translation mt gisting mt consume make sense text foreign language evaluation usefulness mt gisting surprisingly uncommon classical method use read comprehension questionnaires rcq informants ask answer professionally write question language foreign text machine translate language recently gap fill gf form cloze test propose cheaper alternative rcq gf certain word remove reference translations readers ask fill gap leave use machine translate text hint paper report thefirst time comparative evaluation use rcq gf translations multiple mt systems foreign texts systematic study effect variables gap density gap selection strategies document context gf main find study rcq gf clearly identify mt useful b global rcq gf rank mt systems mostly agreement c gf score vary widely across informants make comparisons among mt systems hard unlike rcq frame around document gf evaluation frame sentence level find support use gf cheaper alternative rcq
propose novel model neural machine translation nmt different conventional method model predict future text length word decode time step generation help information future prediction information model stop generation without translate enough content experimental result demonstrate model significantly outperform baseline model besides analysis reflect model effective prediction length word untranslated content
recent work neural machine translation begin explore document translation however translate online multi speaker conversations still open problem work propose task translate bilingual multi speaker conversations explore neural architectures exploit source target side conversation histories task initiate evaluation task introduce datasets extract europarl v7 opensubtitles2016 experiment four language pair confirm significance leverage conversation history term bleu manual evaluation
transfer learn prove effective technique neural machine translation low resource condition exist methods require common target language language relatedness specific train trick regimes present simple transfer learn method first train parent model high resource language pair continue train lowresource pair replace train corpus child model perform significantly better baseline train lowresource pair first show target different languages observe improvements even unrelated languages different alphabets
design build first neural temporal dependency parser utilize neural rank model minimal feature engineer parse time expressions events text temporal dependency tree structure evaluate parser two domains news report narrative stories parse evaluation setup gold time expressions events provide parser reach eighty-one seventy f score unlabeled label parse respectively result competitive alternative approach end end evaluation setup time expressions events automatically recognize parser beat two strong baselines data domains experimental result discussions would light nature temporal dependency structure different domains provide insights believe valuable future research area
advent social media recent years feed highly undesirable phenomena proliferation offensive language hate speech sexist remark etc internet light several efforts automate detection moderation abusive content however deliberate obfuscation word users evade detection pose serious challenge effectiveness efforts current state art approach abusive language detection base recurrent neural network explicitly address problem resort generic oov vocabulary embed unseen word however use single embed unseen word lose ability distinguish obfuscate non obfuscate rare word paper address problem design model compose embeddings unseen word experimentally demonstrate approach significantly advance current state art abuse detection datasets two different domains namely twitter wikipedia talk page
noisy non standard input text disastrous mistranslations modern machine translation mt systems grow research interest create noise robust mt systems however yet publicly available parallel corpora naturally occur noisy input translations thus previous work resort evaluate synthetically create datasets paper propose benchmark dataset machine translation noisy text mtnt consist noisy comment reddit wwwredditcom professionally source translations commission translations english comment french japanese well french japanese comment english order 7k 37k sentence per language pair qualitatively quantitatively examine type noise include dataset demonstrate exist mt model fail badly number noise relate phenomena even perform adaptation small train set domain data indicate dataset provide attractive testbed methods tailor handle noisy text mt data publicly available wwwcscmuedu pmichel1 mtnt
data augmentation seek manipulate available data train improve generalization ability model investigate two data augmentation proxies permutation flip neural dialog response selection task various model multiple datasets include chinese english languages different standard data augmentation techniques method combine original synthesize data prediction empirical result show approach gain one three recall one point baseline model full scale small scale settings
consider cross domain sentiment classification problem sentiment classifier learn source domain generalize target domain approach explicitly minimize distance source target instance embed feature space difference source target minimize exploit additional information target domain consolidate idea semi supervise learn jointly employ two regularizations entropy minimization self ensemble bootstrapping incorporate unlabeled target data classifier refinement experimental result demonstrate propose approach better leverage unlabeled data target domain achieve substantial improvements baseline methods various experimental settings
distant supervision popular method perform relation extraction text know produce noisy label progress relation extraction classification make crowdsourced corrections distant supervise label evidence indicate still would better paper explore problem propagate human annotation signal gather open domain relation classification crowdtruth methodology crowdsourcing capture ambiguity annotations measure inter annotator disagreement approach propagate annotations sentence similar low dimensional embed space expand number label two order magnitude experiment show significant improvement sentence level multi class relation classifier
argument mine relatively recent discipline concentrate extract claim premise discourse infer structure however many exist work consider micro level study discussion thread sufficiently paper tackle discussion thread main contributions follow one novel combination scheme focus micro level inner inter post scheme discussion thread two annotation large scale civic discussion thread scheme three parallel constrain pointer architecture pcpa novel end end technique discriminate sentence type inner post relations inter post interactions simultaneously experimental result demonstrate propose model show better accuracy term relations extraction comparison exist state art model
recent advance data text generation lead use large scale datasets neural network model train end end without explicitly model say order work present neural network architecture incorporate content selection plan without sacrifice end end train decompose generation task two stag give corpus data record pair descriptive document first generate content plan highlight information mention order generate document take content plan account automatic human base evaluation experiment show model outperform strong baselines improve state art recently release rotowire dataset
common sense reason become increasingly important advancement natural language process word embeddings successful explain aspects coffee tea make similar could relate shop paper propose explicit word representation build upon distributional hypothesis represent mean semantic roles allow inference relations mesh support affordance base indexical hypothesis find model improve state art unsupervised word similarity task allow direct inference new relations vector space
recent years see deep learn distribute representations word sentence make impact number natural language process task similarity entailment sentiment analysis introduce new task understand mental health concepts derive cognitive behavioural therapy cbt define mental health ontology base cbt principles annotate large corpus phenomena exhibit perform understand use deep learn distribute representations result show performance deep learn model combine word embeddings sentence embeddings significantly outperform non deep learn model difficult task understand module essential component statistical dialogue system deliver therapy
paper introduce adversarial attention network a3net machine read comprehension model extend exist approach two perspectives first adversarial train apply several target variables within model rather input embeddings control norm adversarial perturbations accord norm original target variables jointly add perturbations several target variables train effective regularization method adversarial train improve robustness generalization model second propose multi layer attention network utilize three kinds high efficiency attention mechanisms multi layer attention conduct interaction question passage within layer contribute reasonable representation understand model combine two contributions enhance diversity dataset information extract ability model time meanwhile construct a3net webqa dataset result show model outperform state art model improve fuzzy score seven thousand, three hundred and fifty seven hundred and seventy
attention mechanisms often use deep neural network distantly supervise relation extraction ds distinguish valid noisy instance however traditional one vector attention model insufficient learn different contexts selection valid instance predict relationship entity pair alleviate issue propose novel multi level structure two matrix self attention mechanism ds multi instance learn mil framework use bidirectional recurrent neural network propose method structure word level self attention mechanism learn two matrix row vector represent weight distribution different aspects instance regard two entities target mil issue structure sentence level attention learn two matrix row vector represent weight distribution selection different valid stances experiment conduct two publicly available ds datasets show propose framework multi level structure self attention mechanism significantly outperform state art baselines term pr curve pn f1 measure
paper present approach tackle implicit emotion share task iest organize part wassa two thousand and eighteen emnlp two thousand and eighteen give tweet certain word remove ask predict emotion miss word work experiment neural transfer learn tl methods model base lstm network augment self attention mechanism use weight various pretrained model initialize specific layer network leverage big collection unlabeled twitter message pretraining word2vec word embeddings set diverse language model moreover utilize sentiment analysis dataset pretraining model encode emotion relate information submit model consist ensemble aforementioned tl model team rank 3rd thirty participants achieve f1 score seven hundred and three
propose novel methodology generate domain specific large scale question answer qa datasets purpose exist annotations nlp task demonstrate instance methodology generate large scale qa dataset electronic medical record leverage exist expert annotations clinical note various nlp task community share i2b2 datasets result corpus emrqa one million question logical form four hundred thousand question answer evidence pair characterize dataset explore learn potential train baseline model question logical form question answer map
paper propose neural semantic parse approach sequence action model semantic parse end end semantic graph generation process method simultaneously leverage advantage two recent promise directions semantic parse firstly model use semantic graph represent mean sentence tight couple knowledge base secondly leverage powerful representation learn prediction ability neural network model propose rnn model effectively map sentence action sequence semantic graph generation experiment show method achieve state art performance overnight dataset get competitive performance geo atis datasets
propose decompose instruction execution goal prediction action generation design model map raw visual observations goals use lingunet language condition image generation network generate action require complete model train demonstration without external resources evaluate approach introduce two benchmarks instruction follow lani navigation task chai agent execute household instructions evaluation demonstrate advantage model decomposition illustrate challenge pose new benchmarks
address problem reconstruct articulatory movements give audio phonetic label scarce availability multi speaker articulatory data make difficult learn reconstruction generalize new speakers across datasets first consider xrmb dataset audio articulatory measurements phonetic transcriptions available show phonetic label use input deep recurrent neural network reconstruct articulatory feature general helpful acoustic feature match mismatch train test condition second experiment test novel approach attempt build articulatory feature prior articulatory information extract phonetic label approach recover vocal tract movements directly acoustic dataset without use articulatory measurement result show articulatory feature generate approach correlate fifty-nine pearson product moment correlation measure articulatory feature
paper propose new approach evaluate informativeness transcriptions come automatic speech recognition systems approach base notion informativeness focus framework automatic text summarization perform transcriptions first glance estimate informative content various automatic transcriptions explore capacity automatic text summarization overcome informative loss use automatic summary evaluation protocol without reference base informative content compute divergence probability distributions different textual representations manual automatic transcriptions summaries set evaluations analysis allow us judge quality transcriptions term informativeness assess ability automatic text summarization compensate problems raise transcription phase
conduct two experiment study effect context metaphor paraphrase aptness judgments first amt crowd source task speakers rank metaphor paraphrase candidate sentence pair short document contexts paraphrase aptness second train composite dnn predict human judgments first binary classifier mode gradient rat find mean human judgments dnn predictions add document context compress aptness score towards center scale raise low context rat decrease high context score offer provisional explanation compression effect
past share task emotions use data overt expressions emotions happy see well subtle expressions emotions infer instance event descriptions datasets focus stimulus emotion first time propose share task systems predict emotions large automatically label dataset tweet without access word denote emotions base intention call implicit emotion share task iest systems infer emotion mostly context every tweet occurrence explicit emotion word mask tweet collect manner likely include description emotion stimulus altogether thirty team submit result range macro f1 score twenty-one seventy-one baseline maxent bag word bigrams obtain f1 score sixty available participants development phase study human annotators suggest automatic methods outperform human predictions possibly hone subtle textual clue use humans corpora resources result available share task website http implicitemotionswassa2018com
understand causal explanations reason give happen one life find important psychological factor link physical mental health causal explanations often study manual identification phrase limit sample personal write automatic identification causal explanations social media challenge rely contextual sequential cue offer larger scale alternative expensive manual rat open door new applications eg study prevail beliefs cause climate change explore automate causal explanation analysis build discourse parse present two novel subtasks causality detection determine whether causal explanation exist causal explanation identification identify specific phrase explanation achieve strong accuracies task find different approach best svm causality prediction f1 seven hundred and ninety-one hierarchy bidirectional lstms causal explanation identification f1 eight hundred and fifty-three finally explore applications complete pipeline f1 eight hundred and sixty-eight show demographic differences mention causal explanation association word sentiment change use within causal explanation
neural conversation model tend generate safe generic responses input due limitations likelihood base decode objectives generation task diverse output conversation address challenge propose simple yet effective approach incorporate side information form distributional constraints generate responses propose two constraints help generate content rich responses base model syntax topics griffiths et al two thousand and five semantic similarity arora et al two thousand and sixteen evaluate approach variety competitive baselines use automatic metrics human judgments show propose approach generate responses much less generic without sacrifice plausibility work demo code find https githubcom abaheti95 dc neuralconversation
novel graph tree conversion mechanism call deep tree generation dtg algorithm first propose predict text data represent graph dtg method generate richer accurate representation nod vertices graph add flexibility explore vertex neighborhood information better reflect second order proximity homophily equivalence graph deep tree recursive neural network dtrnn method present use classify vertices contain text data graph demonstrate effectiveness dtrnn method apply three real world graph datasets show dtrnn method outperform several state art benchmarking methods
introduce weakly supervise approach infer property abstractness word expressions complete absence label data exploit minimal linguistic clue contextual usage concept manifest textual data train sufficiently powerful classifiers obtain high correlation human label result imply applicability approach additional properties concepts additional languages resource scarce scenarios
semantic parse denotations face two key challenge model train one give denotations eg answer search good candidate semantic parse two choose best model update algorithm propose effective general solutions use policy shape bias search procedure towards semantic parse compatible text provide better supervision signal train addition propose update equation generalize three different families learn algorithms enable fast model exploration experiment recently propose sequential question answer dataset framework lead new state art model outperform previous work fifty absolute exact match accuracy
neural machine translation nmt low resource settings morphologically rich languages make difficult part data sparsity vocabulary word several methods use help reduce sparsity notably byte pair encode bpe character base cnn layer charcnn however charcnn largely neglect possibly compare bpe rather combine argue reconsideration charcnn base cross lingual improvements low resource data translate eight languages english use multi way parallel collection ted transcripts find case use bpe charcnn perform best hebrew use charcnn word best
recurrent neural network rnns state art sequence model natural language however remain poorly understand grammatical characteristics natural language implicitly learn represent consequence optimize language model objective deploy methods control psycholinguistic experimentation would light extent rnn behavior reflect incremental syntactic state grammatical dependency representations know characterize human linguistic behavior broadly test two publicly available long short term memory lstm english sequence model learn test new japanese lstm demonstrate model represent maintain incremental syntactic state always generalize way humans furthermore none model learn appropriate grammatical dependency configurations license reflexive pronouns negative polarity items
natural language generators task orient dialog able vary style output utterance still effectively realize system dialog action associate semantics use neural generation train response generation component conversational agents promise simplify process produce high quality responses new domains knowledge little investigation neural generators task orient dialog vary response style know experiment model generate responses different style see train still maintain ing semantic fidelity input mean representation show model train achieve single stylis tic personality target produce output combine stylistic target carefully evaluate multivoice output semantic fidelity similarities differences linguistic feature characterize original train style show contrary predictions learn model always simply interpolate model parameters rather produce style distinct novel personalities train
paper present approach investigate nature semantic information capture word embeddings propose method extend exist human elicit semantic property dataset gold negative examples use crowd judgments experimental approach test ability supervise classifiers identify semantic feature word embed vectors com par feature identification method base full vector cosine similarity idea behind method properties identify classifiers full vector comparison capture embeddings properties identify either method result provide initial indication semantic properties relevant way entities interact eg dangerous capture perceptual information eg color represent conclude though preliminary result show method suitable identify properties capture embeddings
present simple approach improve direct speech text translation st source language low resource pre train model high resource automatic speech recognition asr task fine tune parameters st demonstrate approach effective pre train three hundred hours english asr data improve spanish english st one hundred and eight two hundred and two bleu twenty hours spanish english st train data available ablation study find pre train encoder acoustic model account improvement despite fact share language task target language text source language audio apply insight show pre train asr help st even asr language differ source target st languages pre train french asr also improve spanish english st finally show approach improve performance true low resource task pre train combination english asr french asr improve mboshi french st four hours data available thirty-five seventy-one bleu
configurational information sentence free word order language sanskrit limit use thus context entire sentence desirable even basic process task word segmentation propose structure prediction framework jointly solve word segmentation morphological tag task sanskrit build energy base model adopt approach generally employ graph base parse techniques mcdonald et al 2005a carreras two thousand and seven model outperform state art f score nine thousand, six hundred and ninety-two percentage improvement seven hundred and six use less one tenth task specific train data find use graph base ap proach instead traditional lattice base sequential label approach lead percentage gain one hundred and twenty-six f score segmentation task
statistical significance test play important role draw conclusions experimental result nlp paper particularly valuable tool one would like establish superiority one algorithm another appendix complement guide test statistical significance nlp present citedror2018hitchhiker propose valid statistical test common task evaluation measure field
paper describe system submit share task social media mine health applications team light previous work demonstrate lstms achieve remarkable performance natural language process task deploy ensemble two lstm model first one pretrained language model append classifier take word input second one lstm model attention unit take character tri gram input call ensemble two model neural drugnet system rank 2nd second share task automatic classification post describe medication intake
paper document team copenhagen system place first conll sigmorphon two thousand and eighteen share task universal morphological reinflection task two overall accuracy four thousand, nine hundred and eighty-seven task two focus morphological inflection context generate inflect word form give lemma word context occur previous sigmorphon share task focus context agnostic inflection inflection context task introduce year approach encoder decoder architecture character sequence three core innovations contribute improvement performance one wide context window two multi task learn approach auxiliary task msd prediction three train model multilingual fashion
speak language understand slu essential component conversational systems consider contexts provide informative cue better understand history leverage contextual slu however prior work pay attention relate content history utterances ignore temporal information dialogues intuitive recent utterances important least recent ones time aware attention decay manner therefore paper allow model automatically learn time decay attention function attentional weight dynamically decide base content role contexts effectively integrate content aware time aware perspectives demonstrate remarkable flexibility complex dialogue contexts experiment benchmark dialogue state track challenge dstc4 dataset show propose dynamically context sensitive time decay attention mechanisms significantly improve state art model contextual understand performance
stance detection critical component rumour fake news identification involve extraction stance particular author take relate give claim express text paper investigate stance classification russian introduce new dataset rustance russian tweet news comment multiple source cover multiple stories well text classification approach stance detection benchmarks data language well present openly available dataset first kind russian paper present baseline stance prediction language
neural machine translation nmt improve include document level contextual information purpose propose hierarchical attention model capture context structure dynamic manner model integrate original nmt architecture another level abstraction condition nmt model previous hide state experiment show hierarchical attention significantly improve bleu score strong nmt baseline state art context aware methods encoder decoder benefit context complementary ways
major obstacle reinforcement learn base sentence generation large action space whose size equal vocabulary size target side language improve efficiency reinforcement learn present novel approach reduce action space base dynamic vocabulary prediction method first predict fix size small vocabulary input generate target sentence input specific vocabularies use supervise reinforcement learn step also test time experiment six machine translation two image caption datasets method achieve faster reinforcement learn sim27x faster less gpu memory sim23x less full vocabulary counterpart reinforcement learn method consistently lead significant improvement bleu score score equal better baselines use full vocabularies faster decode time sim3x faster cpus
addition syntax aware decode neural machine translation nmt systems require effective tree structure neural network syntax aware attention model language generation model sensitive sentence structure exploit top tree structure model call drnn doubly recurrent neural network first propose alvarez melis jaakola two thousand and seventeen create nmt model call seq2drnn combine sequential encoder tree structure decode augment syntax aware attention model unlike previous approach syntax base nmt use dependency parse model method use constituency parse argue provide useful information translation addition use syntactic structure sentence add new connections tree structure decoder neural network seq2drnnsync compare nmt model sequential state art syntax base nmt model show model produce fluent translations better reorder since model capable translation constituency parse time also compare parse accuracy neural parse model
current dialogue systems engage users especially train end end without rely proactive reengaging script strategies zhang et al two thousand and eighteen show engagement level end end dialogue model increase condition text personas provide personalize back story model however dataset use zhang et al two thousand and eighteen synthetic limit size contain around 1k different personas paper introduce new dataset provide five million personas seven hundred million persona base dialogues experiment show scale train use personas still improve performance end end systems addition show task benefit wide coverage dataset fine tune model data zhang et al two thousand and eighteen achieve state art result
sequence sequence seq2seq model often employ settings target output natural language however syntactic properties language generate model well understand explore whether output belong formal realistic grammar employ english resource grammar erg broad coverage linguistically precise hpsg base grammar english french english parallel corpus analyze parseability grammatical constructions occur output seq2seq translation model ninety-three model translations parseable suggest learn generate conform grammar model trouble learn distribution rarer syntactic rule pinpoint several constructions differentiate translations reference model
propose post ocr text correction approach digitise texts romanise sanskrit owe lack resources approach use ocr model train languages write roman currently exist dataset available romanise sanskrit ocr bootstrap dataset four hundred and thirty image scan two different settings correspond grind truth train synthetically generate train image settings find use copy mechanism gu et al two thousand and sixteen yield percentage increase seven hundred and sixty-nine character recognition rate crr current state art model solve monotone sequence sequence task schnober et al two thousand and sixteen find system robust combat ocr prone errors obtain crr eight thousand, seven hundred and one ocr output crr three thousand, five hundred and seventy-six one dataset settings human judgment survey perform model show propose model result predictions faster comprehend faster improve human systems
neural machine translation nmt systems operate primarily word sub word ignore lower level pattern morphology present character aware decoder design capture pattern translate morphologically rich languages achieve character awareness augment softmax embed layer attention base encoder decoder model convolutional neural network operate spell word investigate performance wide variety morphological phenomena translate english fourteen typologically diverse target languages use ted multi target dataset low resource set character aware decoder provide consistent improvements bleu score gain three hundred and five addition analyze relationship gain obtain properties target language find evidence model indeed exploit morphological pattern
present uppsala system conll two thousand and eighteen share task universal dependency parse system pipeline consist three components first perform joint word sentence segmentation second predict part speech tag morphological feature third predict dependency tree word tag instead train single parse model treebank train model multiple treebanks one language closely relate languages greatly reduce number model official test run rank 7th twenty-seven team las mlas metrics system obtain best score overall word segmentation universal pos tag morphological feature
address problem detect duplicate question forums important step towards automate process answer new question find annotate potential duplicate manually tedious costly automatic methods base machine learn viable alternative however many forums annotate data ie question label experts duplicate thus promise solution use domain adaptation another forum annotations focus adversarial domain adaptation derive important find perform well properties domains important regard experiment stackexchange data show average improvement fifty-six best baseline across multiple pair domains
propose mixture experts approach unsupervised domain adaptation multiple source key idea explicitly capture relationship target example different source domains relationship express point set metric determine combine predictors train various domains metric learn unsupervised fashion use meta train experimental result sentiment analysis part speech tag demonstrate approach consistently outperform multiple baselines robustly handle negative transfer
propose method stack multiple long short term memory lstm layer model sentence contrast conventional stack lstms hide state feed input next layer suggest architecture accept hide memory cell state precede layer fuse information leave lower context use soft gate mechanism lstms thus architecture modulate amount information deliver horizontal recurrence also vertical connections useful feature extract lower layer effectively convey upper layer dub architecture cell aware stack lstm cas lstm show experiment model bring significant performance gain standard lstms benchmark datasets natural language inference paraphrase detection sentiment classification machine translation also conduct extensive qualitative analysis understand internal behavior suggest approach
exist recursive neural network rvnn architectures utilize structure parse tree ignore syntactic tag provide products parse present novel rvnn architecture provide dynamic compositionality consider comprehensive syntactic information derive structure linguistic tag specifically introduce structure aware tag representation construct separate tag level tree lstm control composition function exist word level tree lstm augment representation supplementary input gate function tree lstm extensive experiment show model build upon propose architecture obtain superior competitive performance several sentence level task sentiment analysis natural language inference compare previous tree structure model sophisticate neural model
data scarcity one main obstacles domain adaptation speak language understand slu due high cost create manually tag slu datasets recent work neural text generative model particularly latent variable model variational autoencoder vae show promise result regard generate plausible natural sentence paper propose novel generative architecture leverage generative power latent variable model jointly synthesize fully annotate utterances experiment show exist slu model train additional synthetic examples achieve performance gain approach help alleviate data scarcity issue slu task many datasets also indiscriminately improve language understand performances various slu model support extensive experiment rigorous statistical test
natural language process nlp one traditionally consider single task eg part speech tag single language eg english time however recent work show beneficial take advantage relatedness task well languages work examine concept relatedness explore utilise build nlp model require less manually annotate data large selection nlp task investigate substantial language sample comprise sixty languages result show potential joint multitask multilingual model hint linguistic insights gain model
distributional model provide convenient way model semantics use dense embed space derive unsupervised learn algorithms however dimension dense embed space design resemble human semantic knowledge moreover embeddings often build single source information typically text data even though neurocognitive research suggest semantics deeply link language perception paper combine multimodal information text image base representations derive state art distributional model produce sparse interpretable vectors use joint non negative sparse embed depth analyse compare sparse model human derive behavioural neuroimaging data demonstrate ability predict interpretable linguistic descriptions human grind truth semantic knowledge
sentence compression task shorten sentence retain original mean model tend train large corpora contain pair verbose compress sentence remove need pair corpora emulate summarization task add noise extend sentence train denoising auto encoder recover original construct end end train regime without need examples compress sentence conduct human evaluation model standard text summarization dataset show perform comparably supervise baseline base grammatical correctness retention mean despite expose target data unsupervised model learn generate imperfect reasonably readable sentence summaries although underperform supervise model base rouge score model competitive supervise baseline base human evaluation grammatical correctness retention mean
recent neural machine translation nmt systems greatly improve encoder decoder model attention mechanisms sub word units however important differences languages logographic alphabetic write systems long overlook study focus differences use simple approach improve performance nmt systems utilize decompose sub character level information logographic languages result indicate approach improve translation capabilities nmt systems chinese english also improve nmt systems chinese japanese utilize share information bring similar sub character units
understand sentence like whereas ten white americans live poverty line twenty-eight african americans important identify individual facts eg poverty rat distinct demographic group also higher order relations eg disparity paper propose task textual analogy parse tap model higher order mean output tap frame style mean representation explicitly specify share eg poverty rat compare eg white americans vs african americans ten vs twenty-eight component facts mean representation enable new applications rely discourse understand automate chart generation quantitative text present new dataset tap baselines model successfully use ilp enforce structural constraints problem
adversarial evaluation stress test model understand natural language past approach expose superficial pattern result adversarial examples limit complexity diversity propose human loop adversarial generation human author guide break model aid author interpretations model predictions interactive user interface apply generation framework question answer task call quizbowl trivia enthusiasts craft adversarial question result question validate via live human computer match although question appear ordinary humans systematically stump neural information retrieval model adversarial question cover diverse phenomena multi hop reason entity type distractors expose open challenge robust question answer
word embed design represent semantic mean word low dimensional vectors state art methods learn word embeddings word2vec glove use word co occurrence information learn embeddings real number vectors obscure human paper propose image enhance skip gram model learn ground word embeddings represent word vectors hyper plane image vectors experiment show image vectors word embeddings learn model highly correlate indicate model able provide vivid image base explanation word embeddings
investigate task distractor generation multiple choice read comprehension question examinations contrast previous work aim prepare word short phrase distractors instead endeavor generate longer semantic rich distractors closer distractors real read comprehension examinations take read comprehension article pair question correct option input goal generate several distractors somehow relate answer consistent semantic context question trace article propose hierarchical encoder decoder framework static dynamic attention mechanisms tackle task specifically dynamic attention combine sentence level word level attention vary recurrent time step generate readable sequence static attention modulate dynamic attention focus question irrelevant sentence sentence contribute correct option propose framework outperform several strong baselines first prepare distractor generation dataset real read comprehension question human evaluation compare distractors generate baselines generate distractors functional confuse annotators
emergence web twenty technology expansion line social network current internet users ability add review rat opinions social media commercial news web sit sentiment analysis aim classify review review automatic way literature numerous approach propose automatic sentiment analysis different language contexts language properties make sentiment analysis challenge regard work present comprehensive survey exist arabic sentiment analysis study cover various approach techniques propose literature moreover highlight main difficulties challenge arabic sentiment analysis propose techniques literature overcome barriers
present new kind question answer dataset openbookqa model open book exams assess human understand subject open book come question set one thousand, three hundred and twenty-nine elementary level science facts roughly six thousand question probe understand facts application novel situations require combine open book fact eg metal conduct electricity broad common knowledge eg suit armor make metal obtain source exist qa datasets document knowledge base generally self contain focus linguistic understand openbookqa probe deeper understand topic context common knowledge language express human performance openbookqa close ninety-two many state art pre train qa methods perform surprisingly poorly worse several simple neural baselines develop oracle experiment design circumvent knowledge retrieval bottleneck demonstrate value open book additional facts leave challenge solve retrieval problem multi hop set close large gap human performance
major focus natural language understand right aim semantic role label srl task despite share lot process characteristics even task purpose surprisingly jointly consider two relate task never formally report previous work thus paper make first attempt let srl enhance text comprehension inference specify verbal predicate correspond semantic roles term deep learn model embeddings enhance explicit contextual semantic role label fine grain semantics show salient label conveniently add exist model significantly improve deep learn model challenge text comprehension task extensive experiment benchmark machine read comprehension inference datasets verify propose semantic learn help system reach new state art strong baselines enhance well pretrained language model latest progress
goal semantic role label srl discover predicate argument structure sentence play critical role deep process natural language paper introduce simple yet effective auxiliary tag dependency base srl enhance syntax agnostic model multi hop self attention syntax agnostic model achieve competitive performance state art model conll two thousand and nine benchmarks english chinese
local model interpretation methods explain individual predictions assign importance value input feature value often determine measure change confidence feature remove however confidence neural network robust measure model uncertainty issue make reliably judge importance input feature difficult address change test time behavior neural network use deep k nearest neighbor without harm text classification accuracy algorithm provide robust uncertainty metric use generate feature importance value result interpretations better align human perception baseline methods finally use interpretation method analyze model predictions dataset annotation artifacts
exist datasets natural language inference nli propel research language understand propose new method automatically derive nli datasets grow abundance large scale question answer datasets approach hinge learn sentence transformation model convert question answer pair declarative form despite primarily train single qa dataset show successfully apply variety qa resources use system automatically derive new freely available dataset 500k nli examples qa nli show exhibit wide range inference phenomena rarely see previous nli datasets
responses task orient dialogue systems often realize multiple proposition whose ultimate form depend use sentence plan discourse structure operations example recommendation may consist explicitly evaluative utterance eg chanpen thai best option along content relate justification discourse relation eg great food service combine multiple proposition single phrase neural generation methods integrate sentence plan surface realization one end end learn framework previous work show neural generators one perform common sentence plan discourse structure operations two make decisions whether realize content single sentence multiple sentence three generalize sentence plan discourse relation operations beyond see train systematically create large train corpora exhibit particular sentence plan operations test neural model see learn compare model without explicit latent variables sentence plan ones provide explicit supervision train show model additional supervision reproduce sentence plan discourse operations generalize situations unseen train
paper present language independent deep learn architecture adapt task multiword expression mwe identification employ neural architecture comprise convolutional recurrent layer addition optional crf layer top system participate open track parseme share task automatic identification verbal mwes due use pre train wikipedia word embeddings outperform participate systems open close track overall macro average mwe base f1 score five thousand, eight hundred and nine average among languages particular strength system superior performance unseen data entries
pater target article build persuasive case establish stronger tie theoretical linguistics connectionism deep learn commentary extend arguments semantics focus particular issue learn compositionality lexical mean
several recent attempt improve accuracy grammar induction systems bound recursive complexity induction model ponvert et al two thousand and eleven noji johnson two thousand and sixteen shain et al two thousand and sixteen jin et al two thousand and eighteen modern depth bound grammar inducers show accurate early unbounded pcfg inducers technique never compare unbounded induction within system part previous depth bound model build around sequence model complexity grow exponentially maximum allow depth present work instead apply depth bound within chart base bayesian pcfg inducer johnson et al 2007b bound switch sample tree without bound result show depth bound indeed significantly effective limit search space inducer thereby increase accuracy result parse model moreover parse result english chinese german show bound model new inference technique able produce parse tree accurately competitively state art constituency base grammar induction model
multi label text classification mltc aim assign multiple label sample dataset label usually internal correlations however traditional methods tend ignore correlations label order capture correlations label sequence sequence seq2seq model view mltc task sequence generation problem achieve excellent performance task however seq2seq model suitable mltc task essence reason require humans predefine order output label output label mltc task essentially unordered set rather order sequence conflict strict requirement seq2seq model label order paper propose novel sequence set framework utilize deep reinforcement learn capture correlations label also reduce dependence label order extensive experimental result show propose method outperform competitive baselines large margin
neural machine translation nmt model usually train word level loss use teacher force algorithm evaluate translation improperly also suffer exposure bias sequence level train reinforcement framework mitigate problems word level loss performance unstable due high variance gradient estimation ground present method differentiable sequence level train objective base probabilistic n gram match avoid reinforcement framework addition method perform greedy search train use predict word context inference alleviate problem exposure bias experiment result nist chinese english translation task show method significantly outperform reinforcement base algorithms achieve improvement fifteen bleu point average strong baseline system
present first exploration mean shift short periods time online communities use distributional representations create small annotate dataset use assess performance standard model mean shift detection short term mean shift find model problems distinguish mean shift referential phenomena propose measure contextual variability remedy
neural machine translation nmt significantly improve quality automatic translation model one main challenge current systems translation rare word present generic approach address weakness external model annotate train data experts control model expert interaction pointer network reinforcement learn experiment use phrase base model simulate experts complement neural machine translation model show model train copy annotations output consistently demonstrate benefit propose framework outof domain translation scenarios lexical resources improve ten bleu point translation directions english spanish german english
generate structure query language sql natural language emerge research topic paper present new learn paradigm indirect supervision answer natural language question instead sql query paradigm facilitate acquisition train data due abundant resources question answer pair various domains internet expel difficult sql annotation job end end neural model integrate reinforcement learn propose learn sql generation policy within answer drive learn paradigm model evaluate datasets different domains include movie academic publication experimental result show model outperform baseline model
paper describe submission conll two thousand and eighteen ud share task extend lstm base neural network design sequence tag additionally generate character level sequence network jointly train produce lemmas part speech tag morphological feature sentence segmentation tokenization dependency parse handle udpipe twelve baseline result demonstrate viability propose multitask architecture although performance still remain far state art
despite recent work read comprehension rc progress mostly limit english due lack large scale datasets languages work introduce first rc system languages without rc train data give target language without rc train data pivot language rc train data eg english method leverage exist rc resources pivot language combine competitive rc model pivot language attentive neural machine translation nmt model first translate data target pivot language obtain answer use rc model pivot language finally recover correspond answer original language use soft alignment attention score nmt model create evaluation set rc data two non english languages namely japanese french evaluate method experimental result datasets show method significantly outperform back translation baseline state art product level machine translation system
despite success achieve various natural language process task word embeddings difficult interpret due dense vector representations paper focus interpret embeddings various aspects include sense separation vector dimension definition generation specifically give context together target word algorithm first project target word embed high dimensional sparse vector pick specific dimension best explain semantic mean target word encode contextual information sense target word indirectly infer finally algorithm apply rnn generate textual definition target word human readable form enable direct interpretation correspond word embed paper also introduce large high quality context definition dataset consist sense definitions together multiple example sentence per polysemous word valuable resource definition model word sense disambiguation conduct experiment show superior performance bleu score human evaluation test
previous work indonesian part speech pos tag hard compare evaluate common dataset furthermore spite success neural network model english pos tag rarely explore indonesian paper explore various techniques indonesian pos tag include rule base crf neural network base model evaluate model idn tag corpus new state art nine thousand, seven hundred and forty-seven f1 score achieve recurrent neural network provide standard future work release dataset split use publicly
capture semantic relations word vector space contribute many natural language process task one promise approach exploit lexico syntactic pattern feature word pair paper propose novel model pattern base approach neural latent relational analysis nlra nlra generalize co occurrences word pair lexico syntactic pattern obtain embeddings word pair co occur overcome critical data sparseness problem encounter previous pattern base model experimental result measure relational similarity demonstrate nlra outperform previous pattern base model addition combine vector offset model nlra achieve performance comparable state art model exploit additional semantic relational data
recognize lexical semantic relations word pair important task many applications natural language process one mainstream approach task exploit lexico syntactic paths connect two target word reflect semantic relations word pair however method require consider word co occur sentence requirement hardly satisfy zipf law state content word occur rarely paper propose novel methods neural model ppathw1 w2 solve problem propose model ppathw1 w2 learn unsupervised manner generalize co occurrences word pair dependency paths model use augment path data word pair co occur corpus extract feature capture relational information word pair experimental result demonstrate methods improve previous neural approach base dependency paths successfully solve focus problem
news article title content link structure often reveal political ideology however exist work automatic political ideology detection leverage textual cue draw inspiration recent advance neural inference propose novel attention base multi view model leverage cue view identify ideology evince news article model draw advance representation learn natural language process network science capture cue textual content network structure news article empirically evaluate model battery baselines show model outperform state art ten percentage point f1 score
although neural network approach achieve remarkable success variety nlp task many struggle answer question require commonsense knowledge believe main reason lack commonsense mboxconnections concepts remedy provide simple effective method leverage external commonsense knowledge base conceptnet pre train direct indirect relational function concepts show pre train function could easily add exist neural network model result show incorporate commonsense base function improve baseline three question answer task require commonsense reason analysis show system mboxdiscovers leverage useful evidence external commonsense knowledge base miss exist neural network model help derive correct answer
recent advance deep neural model allow us build reliable name entity recognition ner systems without handcraft feature however methods require large amount manually label train data efforts replace human annotations distant supervision conjunction external dictionaries generate noisy label pose significant challenge learn effective neural model propose two neural model suit noisy distant supervision dictionary first traditional sequence label framework propose revise fuzzy crf layer handle tokens multiple possible label identify nature noisy label distant supervision go beyond traditional framework propose novel effective neural model autoner new tie break scheme addition discuss refine distant supervision better ner performance extensive experiment three benchmark datasets demonstrate autoner achieve best performance use dictionaries additional human effort deliver competitive result state art supervise benchmarks
gang involve youth cities chicago increasingly turn social media post experience intents online situations experience loss love one online expression emotion may evolve aggression towards rival gang ultimately real world violence paper present novel system detect aggression loss social media system feature use domain specific resources automatically derive large unlabeled corpus contextual representations emotional semantic content user recent tweet well interactions users incorporate context convolutional neural network cnn lead significant improvement
cross lingual transfer word embeddings aim establish semantic mappings among word different languages learn transformation function correspond word embed space successfully solve problem would benefit many downstream task translate text classification model resource rich languages eg english low resource languages supervise methods problem rely availability cross lingual supervision either use parallel corpora bilingual lexicons label data train may available many low resource languages paper propose unsupervised learn approach require cross lingual label data give two monolingual word embed space language pair algorithm optimize transformation function directions simultaneously base distributional match well minimize back translation losses use neural network implementation calculate sinkhorn distance well define distributional similarity measure optimize objective back propagation evaluation benchmark datasets bilingual lexicon induction cross lingual word similarity prediction show stronger competitive performance propose method compare state art supervise unsupervised baseline methods many language pair
many classification model work poorly short texts due data sparsity address issue propose topic memory network short text classification novel topic memory mechanism encode latent topic representations indicative class label different prior work focus extend feature external knowledge pre train topics model jointly explore topic inference text classification memory network end end manner experimental result four benchmark datasets show model outperform state art model short text classification meanwhile generate coherent topics
script propose model stereotypical event sequence find narratives apply make variety inferences include fill gap narratives resolve ambiguous reference paper propose first formal framework script base hide markov model hmms framework support robust inference learn algorithms lack previous cluster model develop algorithm structure parameter learn base expectation maximization evaluate number natural datasets result show algorithm superior several inform baselines predict miss events partial observation sequence
conventional topic model ineffective topic extraction microblog message data sparseness exhibit short message lack structure contexts result poor message level word co occurrence pattern address issue organize microblog message conversation tree base reposting reply relations propose unsupervised model jointly learn word distributions represent one different roles conversational discourse two various latent topics reflect content information explicitly distinguish probabilities message vary discourse roles contain topical word model able discover cluster discourse word indicative topical content automatic evaluation large scale microblog corpora joint model yield topics better coherence score competitive topic model previous study qualitative analysis model output indicate model induce meaningful representations discourse topics present empirical study microblog summarization base output joint model result show jointly model discourse topic representations effectively indicate summary worthy content microblog conversations
datasets boost state art solutions question answer qa systems prove possible ask question natural language manner however users still use query like systems type keywords search answer study validate part question essential obtain valid answer order conclude take advantage lime framework explain prediction local approximation find grammar natural language disregard qa state art model answer properly even ask word high coefficients calculate lime accord knowledge first time qa model explain lime
paper present result investigation importance verbs deep learn qa system train squad dataset show main verbs question carry little influence decisions make system ninety research case swap verbs antonyms change system decision track phenomenon insides net analyze mechanism self attention value contain hide layer rnn finally recognize characteristics squad dataset source problem work refer recently popular topic adversarial examples nlp combine investigate deep net structure
arabic widely speak language long rich history exist corpora language technology focus mostly modern arabic varieties therefore study history language far mostly limit manual analyse small scale work present large scale historical corpus write arabic language span one thousand, four hundred years describe efforts clean process corpus use arabic nlp tool include identification reuse text study history arabic language use novel automatic periodization algorithm well techniques find confirm establish division write arabic modern standard classical arabic confirm establish periodizations suggest write arabic may divisible still periods development
report present study eight corpora online hate speech demonstrate nlp techniques use collect analyze jihadist extremist racist sexist content analysis multilingual corpora show different contexts share certain characteristics hateful rhetoric expose main feature focus text classification text profile keyword collocation extraction along manual annotation qualitative study
work investigate alignment problem state art multi head attention model base transformer architecture demonstrate alignment extraction transformer model improve augment additional alignment head multi head source target attention component use compute sharper attention weight describe use alignment head achieve competitive performance study effect add alignment head simulate dictionary guide translation task user want guide translation use pre define dictionary entries use propose approach achieve thirty-eight bleu improvement use dictionary comparison twenty-four bleu baseline case also propose alignment prune speed decode alignment base neural machine translation anmt speed translation factor eighteen without loss translation performance carry experiment share wmt two thousand and sixteen englishtoromanian news task bolt chinesetoenglish discussion forum task
important component achieve language understand master composition sentence mean immediate challenge solve problem opacity sentence vector representations produce current neural sentence composition model present method address challenge develop task directly target compositional mean information sentence vector representations high degree precision control enable creation control task introduce specialize sentence generation system produce large annotate sentence set meet specify syntactic semantic lexical constraints describe detail method generation system present result experiment apply method probe compositional information embeddings number exist sentence composition model find method able extract useful information differ capacities model discuss implications result respect systems capture sentence information make available public use datasets use experiment well generation system
automatic evaluation semantic rationality important yet challenge task current automatic techniques well identify whether sentence semantically rational methods base language model measure sentence rationality commonness methods base similarity human write sentence fail human write reference available paper propose novel model call sememe word match neural network swm nn tackle semantic rationality evaluation take advantage sememe knowledge base hownet advantage model utilize proper combination sememes represent fine grain semantic mean word within specific contexts use fine grain semantic representation help model learn semantic dependency among word evaluate effectiveness propose model build large scale rationality evaluation dataset experimental result dataset show propose model outperform competitive baselines fifty-four improvement accuracy
sequential neural network model powerful tool variety natural language process nlp task sequential nature model raise question extent model implicitly learn hierarchical structure typical human language kind grammatical phenomena acquire focus task agreement prediction basque case study task require implicit understand sentence structure acquisition complex consistent morphological system analyze experimental result two syntactic prediction task verb number prediction suffix recovery find sequential model perform worse agreement prediction basque one might expect basis previous agreement prediction work english tentative find base diagnostic classifiers suggest network make use local heuristics proxy hierarchical structure sentence propose basque agreement prediction task challenge benchmark model attempt learn regularities human language
textual entailment fundamental task natural language process refer directional relation text fragment premise infer hypothesis recent years deep learn methods achieve great success task many consider inter sentence word word interactions premise hypothesis pair however consider asymmetry interactions different paraphrase identification sentence similarity evaluation textual entailment essentially determine directional asymmetric relation premise hypothesis paper propose simple effective way enhance exist textual entailment algorithms use asymmetric word embeddings experimental result scitail snli datasets show learn asymmetric word embeddings could significantly improve word word interaction base textual entailment model noteworthy propose awe deiste model get twenty-one accuracy improvement prior state art scitail
submission report work progress learn simplify interpret languages mean recurrent model data construct reflect core properties natural language model formal syntax semantics recursive syntactic structure compositionality preliminary result suggest lstm network generalise compositional interpretation albeit favorable learn set well pace curriculum extensive train data leave right right leave composition
semantic specialization process fine tune pre train distributional word vectors use external lexical knowledge eg wordnet accentuate particular semantic relation specialize vector space post process specialization methods applicable arbitrary distributional vectors limit update vectors word occur external lexicons ie see word leave vectors word unchanged propose novel approach specialize full distributional vocabulary adversarial post specialization method propagate external lexical knowledge full distributional space exploit word see resources train examples learn global specialization function function learn combine standard l2 distance loss adversarial loss adversarial component produce realistic output vectors show effectiveness robustness propose method across three languages three task word similarity dialog state track lexical simplification report consistent improvements distributional word vectors vectors specialize state art specialization frameworks finally also propose cross lingual transfer method zero shoot specialization successfully specialize full target distributional space without lexical knowledge target language without bilingual data
joe pater target article call greater interaction neural network research linguistics expand call show interaction benefit field linguists contribute research neural network language technologies clearly delineate linguistic capabilities expect systems construct control experimental paradigms determine whether desiderata meet direction neural network benefit scientific study language provide infrastructure model human sentence process evaluate necessity particular innate constraints language acquisition
graphemes languages encode pronunciation though explicit others languages like spanish straightforward map graphemes phonemes map convolute languages like english speak languages cantonese present even challenge pronunciation model one standard write form two closest graphemic origins logographic han character subset logographic character implicitly encode pronunciation work propose multimodal approach predict pronunciation cantonese logographic character use neural network geometric representation logographs pronunciation cognates historically relate languages propose framework improve performance one hundred and eighty-one two hundred and fifty respective unimodal multimodal baselines
machine read comprehension mrc require reason knowledge involve document knowledge world however exist datasets typically dominate question well solve context match fail test capability encourage progress knowledge base reason mrc present knowledge base mrc paper build new dataset consist forty thousand and forty-seven question answer pair annotation dataset design successfully answer question require understand knowledge involve document implement framework consist question answer model question generation model take knowledge extract document well relevant facts external knowledge base freebase probase reverb nell result show incorporate side information external kb improve accuracy baseline question answer system compare standard mrc model bidaf also provide difficulty dataset lay remain challenge
conversational semantic parse table require knowledge acquire reason abilities well explore current state art approach motivate fact propose knowledge aware semantic parser improve parse performance integrate various type knowledge paper consider three type knowledge include grammar knowledge expert knowledge external resource knowledge first grammar knowledge empower model effectively replicate previously generate logical form effectively handle co reference ellipsis phenomena conversation second base expert knowledge propose decomposable model controllable compare traditional end end model put burden learn trial error end end way third external resource knowledge ie provide pre train language model entity type model use improve representation question table better semantic understand conduct experiment sequentialqa dataset result show knowledge aware model outperform state art approach incremental experimental result also prove usefulness various knowledge analysis show approach ability derive mean representation context dependent utterance leverage previously generate outcomes
dialogue systems usually build either generation base retrieval base approach yet benefit advantage different model paper propose retrieval enhance adversarial train reat method neural response generation distinct exist approach reat method leverage encoder decoder framework term adversarial train paradigm take advantage n best response candidates retrieval base system construct discriminator empirical study large scale public available benchmark dataset show reat method significantly outperform vanilla seq2seq model well conventional adversarial train approach
hate speech commonly define communication disparage target group people base characteristic race colour ethnicity gender sexual orientation nationality religion characteristic due massive rise user generate web content social media amount hate speech also steadily increase past years interest online hate speech detection particularly automation task continuously grow along societal impact phenomenon paper describe hate speech dataset compose thousands sentence manually label contain hate speech sentence extract stormfront white supremacist forum custom annotation tool develop carry manual label task among things allow annotators choose whether read context sentence label paper also provide thoughtful qualitative quantitative study result dataset several baseline experiment different classification model dataset publicly available
paper propose emo2vec encode emotional semantics vectors train emo2vec multi task learn six different emotion relate task include emotion sentiment analysis sarcasm classification stress detection abusive language classification insult detection personality recognition evaluation emo2vec show outperform exist affect relate representations sentiment specific word embed deepmoji embeddings much smaller train corpora concatenate glove emo2vec achieve competitive performances state art result several task use simple logistic regression classifier
methodology present solve arithmetic problems sinhala language use neural network system comprise keyword identification b question identification c mathematical operation identification combine use neural network naive bay classification use order identify keywords conditional random field identify question operation perform identify keywords achieve expect result one vs classification do use neural network sentence function combine neural network build equation solve problem paper compare methodology aris mahoshadha method present paper mahoshadha2 learn solve arithmetic problems accuracy seventy-six
lake baroni two thousand and eighteen recently introduce scan data set consist simple command pair action sequence intend test strong generalization abilities recurrent sequence sequence model initial experiment suggest model may fail lack ability extract systematic rule take closer look scan show always capture kind generalization design mitigate propose complementary dataset require map action back original command call nacs show model well scan necessarily well nacs nacs exhibit properties closely align realistic use case sequence sequence model
present semantic wordrank swr unsupervised method generate extractive summary single document build weight word graph semantic co occurrence edge swr score sentence use article structure bias pagerank algorithm softplus function adjustment promote topic diversity use spectral subtopic cluster word movers distance metric evaluate swr duc two summbank datasets show swr produce better summaries state art algorithms duc two common rouge measure show measure summbank swr outperform three human annotators aka judge compare favorably combine performance judge
impression section radiology report summarize crucial radiology find natural language play central role communicate find physicians however process generate impressions summarize find time consume radiologists prone errors propose automate generation radiology impressions neural sequence sequence learn propose customize neural model task learn encode study background information use information guide decode process large dataset radiology report collect actual hospital study model outperform exist non neural neural baselines rouge metrics blind experiment board certify radiologist indicate sixty-seven sample system summaries least good correspond human write summaries suggest significant clinical validity knowledge work represent first attempt direction
paper describe efforts predict current future psychological health childhood essay within scope clpsych two thousand and eighteen share task experiment number different model include recurrent convolutional network poisson regression support vector regression l1 l2 regularize linear regression obtain best result train development data l2 regularize linear regression ridge regression also get best score main metrics official test task predict psychological health essay write age eleven years task b predict later psychological health essay write age eleven
introduce task automatic live comment live comment also call video barrage emerge feature online video sit allow real time comment viewers fly across screen like bullets roll right side screen live comment mixture opinions video chit chat comment automatic live comment require ai agents comprehend videos interact human viewers also make comment good testbed ai agent ability deal dynamic vision language work construct large scale live comment dataset two thousand, three hundred and sixty-one videos eight hundred and ninety-five thousand, nine hundred and twenty-nine live comment introduce two neural model generate live comment base visual textual contexts achieve better performance previous neural baselines sequence sequence model finally provide retrieval base evaluation protocol automatic live comment model ask sort set candidate comment base log likelihood score evaluate metrics mean reciprocal rank put together demonstrate first livebot
article comment provide supplementary opinions facts readers thereby increase attraction engagement article therefore automatically comment helpful improve activeness community online forums news websites previous work show train automatic comment system require large parallel corpora although part article naturally pair comment websites article comment unpaired internet fully exploit unpaired data completely remove need parallel data propose novel unsupervised approach train automatic article comment model rely nothing unpaired article comment model base retrieval base comment framework use news retrieve comment base similarity topics topic representation obtain neural variational topic model train unsupervised manner evaluate model news comment dataset experiment show propose topic base approach significantly outperform previous lexicon base model model also profit pair corpora achieve state art performance semi supervise scenarios
better understand effectiveness continue train analyze major components neural machine translation system encoder decoder embed space consider component contribution capacity domain adaptation find freeze single component continue train minimal impact performance performance surprisingly good single component adapt hold rest model fix also find continue train move model far domain model compare sensitivity analysis metric suggest domain model provide good generic initialization new domain
paper present method automatic catchphrase extract legal case document utilize deep neural network construct score model extraction system achieve comparable performance systems use corpus wide citation information use system
work present unsupervised approach summarize sentence abstractive way use variational autoencoder vae vae know learn semantically rich latent variable represent high dimensional input vaes train learn reconstruct input probabilistic latent variable explicitly provide information output length train influence vae encode information thus manipulate inference instruct decoder produce shorter output sequence lead express input sentence fewer word show different summarization data set shorter sentence beat simple baseline yield higher rouge score try reconstruct whole sentence
automation text summarisation biomedical publications press need due plethora information available line paper explore impact several supervise machine learn approach extract multi document summaries give query particular compare classification regression approach query base extractive summarisation use data provide bioasq challenge tackle problem annotate sentence train classification systems show simple annotation approach outperform regression base summarisation
paper describe macquarie university contribution bioasq challenge bioasq 6b phase b focus extraction ideal answer task approach instance query base multi document summarisation particular paper focus experiment relate deep learn reinforcement learn approach use submit run best run use deep learn model regression base framework deep learn architecture use feature derive output lstm chain word embeddings plus feature base similarity query sentence position reinforcement learn approach proof concept prototype train global policy use reinforce global policy implement neural network use tfidf feature encode candidate sentence question context
one biggest challenge end end language generation mean representations dialogue systems make output natural vary take large corpus 50k crowd source utterances restaurant domain develop text analysis methods systematically characterize type sentence train data automatically label train data allow us conduct two kinds experiment neural generator first test effect train system different stylistic partition quantify effect smaller stylistically control train data second propose method label style variants train show modify style generate utterances use stylistic label contrast compare methods use exist large corpus show vary term semantic quality stylistic control
dialogue response generation traditional generative model generate responses solely input query model rely insufficient information generate specific response since certain query could answer multiple ways consequentially model tend output generic dull responses impede generation informative utterances recently researchers attempt fill information gap exploit information retrieval techniques generate response current query similar dialogues retrieve entire train data consider additional knowledge source may harvest massive information generative model could overwhelm lead undesirable performance paper propose new framework exploit retrieval result via skeleton response paradigm first skeleton generate revise retrieve responses novel generative model use generate skeleton original query response generation experimental result show approach significantly improve diversity informativeness generate responses
numerals contain much information financial document crucial financial decision make play different roles financial analysis process paper aim understand mean numerals financial tweet fine grain crowd base forecast propose taxonomy classify numerals financial tweet seven categories extend categories several subcategories neural network base model word character level encoders propose seven way classification seventeen way classification perform backtest confirm effectiveness numeric opinions make crowd work first attempt understand numerals financial social media data provide first comparison fine grain opinion individual investors analysts base forecast price numeral corpus use experiment call finnum ten available research purpose
paper describe dataset german latin textitground truth gt historical ocr form print text line image pair transcription dataset call textitgt4histocr consist three hundred and thirteen thousand, one hundred and seventy-three line pair cover wide period print date incunabula 15th century 19th century book print fraktur type openly available cc forty license special form gt line image transcription pair make directly usable train state art recognition model ocr software employ recur neural network lstm architecture tesseract four ocropus also provide pretrained ocropus model subcorpora dataset yield ninety-five early print ninety-eight 19th century fraktur print character accuracy rat unseen test case perl script harmonize gt produce different transcription rule give hint construct gt ocr purpose requirements may differ linguistically motivate transcriptions
explore human drive approach annotation curated train ct annotation frame teach system use interactive search identify informative snippets text annotate unlike traditional approach either annotate preselected text use active learn train annotator perform eighty hours ct thirty event type nist tac kbp event argument extraction evaluation combine annotation ace result six reduction error learn curve ct plateaus slowly full document annotation three nlp researchers perform ct one event type show much sharper learn curve three exceed ace performance less ninety minutes suggest ct provide benefit annotator deeply understand system
paper propose modularized sense induction representation learn model jointly learn bilingual sense embeddings align well vector space cross lingual signal english chinese parallel corpus exploit capture collocation distribute characteristics language pair model evaluate stanford contextual word similarity scws dataset ensure quality monolingual sense embeddings addition introduce bilingual contextual word similarity bcws large high quality dataset evaluate cross lingual sense embeddings first attempt measure whether learn embeddings indeed align well vector space propose approach show superior quality sense embeddings evaluate monolingual bilingual space
neural abstractive summarization increasingly study prior work mainly focus summarize single speaker document news scientific publications etc dialogues different interactions speakers usually define dialogue act interactive signal may provide informative cue better summarize dialogues paper propose explicitly leverage dialogue act neural summarization model sentence gate mechanism design model relationship dialogue act summary experiment show propose model significantly improve abstractive summarization performance compare state art baselines ami meet corpus demonstrate usefulness interactive signal provide dialogue act
know natural language determiners conservative psycholinguistic experiment indicate children exhibit correspond learnability bias face task learn new determiners however recent work indicate bias towards conservativity observe train stage artificial neural network work investigate whether learnability bias exhibit children part due distribution quantifiers natural language share result five experiment contrast distribution conservative vs non conservative determiners train data demonstrate aquisitional issue non conservative quantifiers explain distribution natural language data favor conservative quantifiers find indicate bias language acquisition data might innate representational
paper describe system submit uhh conll sigmorphon two thousand and eighteen share task universal morphological reinflection propose neural architecture base concepts uzh makarov et al two thousand and seventeen add new ideas techniques key concept evaluate different combinations parameters result system language agnostic network model aim reduce number learn edit operations introduce equivalence class graphical feature individual character try pinpoint advantage drawbacks approach compare different network configurations evaluate result wide range languages
readmission discharge hospital disruptive costly regardless reason however particularly problematic psychiatric patients predict patients may readmitted critically important also difficult clinical narratives psychiatric electronic health record ehrs span wide range topics vocabulary therefore psychiatric readmission prediction model must begin robust interpretable topic extraction component create data pipeline use document vector similarity metrics perform topic extraction psychiatric ehr data service long term goal create readmission risk classifier show initial result topic extraction model identify additional feature incorporate future
cross domain text classification aim build classifier target domain leverage data source target domain one promise idea minimize feature distribution differences two domains exist study explicitly minimize differences exact alignment mechanism align feature one one feature alignment projection matrix etc exact alignment however restrict model learn ability impair model performance classification task semantic distributions different domains different address problem propose novel group alignment align semantics group level addition help model learn better semantic group semantics within group also propose partial supervision model learn source domain end embed group alignment partial supervision cross domain topic model propose cross domain label lda cdl lda standard 20newsgroup reuters dataset extensive quantitative classification perplexity etc qualitative topic detection experiment conduct show effectiveness propose group alignment partial supervision
webportal news statistics appearence persons write news develop extension measure relationship public persons depend time parameter relationship may vary time train corpus english german news article build measure extract persons occurrence text via pretrained name entity extraction construct time series count person pearson correlation slide window use measure relation two persons
paper accompany release opusparcus new paraphrase corpus six european languages german english finnish french russian swedish corpus consist paraphrase pair sentence language mean approximately thing paraphrase extract opensubtitles2016 corpus contain subtitle movies tv show informal colloquial genre occur subtitle make data interest language resource instance perspective computer assist language learn target language opusparcus data partition three type data set train development test set train set large consist millions sentence pair compile automatically help probabilistic rank function development test set consist sentence pair check manually set contain approximately one thousand sentence pair verify acceptable paraphrase two annotators
tackle task automatically identify comparative sentence categorize intend preference eg python better nlp libraries matlab python better matlab end manually annotate seven thousand, one hundred and ninety-nine sentence two hundred and seventeen distinct target item pair several domains twenty-seven sentence contain orient comparison sense better worse gradient boost model base pre train sentence embeddings reach f1 score eighty-five experimental evaluation model use extract comparative sentence pro con argumentation comparative argument search engines debate technologies
learn follow human instructions long pursue goal artificial intelligence task become particularly challenge prior knowledge employ language assume rely handful examples learn work past rely hand cod components manually engineer feature provide strong inductive bias make learn situations possible contrast seek establish whether knowledge acquire automatically neural network system two phase train procedure slow offline learn stage network learn general structure task fast online adaptation phase network learn language new give speaker control experiment show network expose familiar instructions contain novel word model adapt efficiently new vocabulary moreover even human speakers whose language usage depart significantly artificial train language network still make use automatically acquire inductive bias learn follow instructions effectively
paper show unsupervised sense representations use improve hypernymy extraction present method extract disambiguate hypernymy relationships propagate hypernyms set synonyms synsets construct embeddings set establish sense aware relationships match synsets evaluation two gold standard datasets english russian show method successfully recognize hypernymy relationships find standard hearst pattern wiktionary datasets respective languages
style transfer task transfer attribute sentence eg formality maintain semantic content key challenge style transfer strike balance compete goals one preserve mean improve style transfer accuracy prior research identify task mean preservation generally harder attain evaluate paper propose two extensions state art style transfer model aim improve mean preservation style transfer evaluation show extensions help grind mean better improve transfer accuracy
generative adversarial network gans achieve significant success generate real value data however discrete nature text hinder application gin text generation task instead use standard gin objective propose improve text generation gin via novel approach inspire optimal transport specifically consider match latent feature distributions real synthetic sentence use novel metric term feature mover distance fmd formulation lead highly discriminative critic easy optimize objective overcome mode collapse brittle train problems exist methods extensive experiment conduct variety task evaluate propose model empirically include unconditional text generation style transfer non parallel text unsupervised cipher crack propose model yield superior performance demonstrate wide applicability effectiveness
learn intents slot label user utterances fundamental step speak language understand slu dialog systems state art neural network base methods deployment often suffer performance degradation encounter paraphrase utterances vocabulary word rarely observe train set address challenge problem introduce novel paraphrase base slu model integrate exist slu model order improve overall performance propose two new paraphrase generators use rnn sequence sequence base neural network suitable application experiment exist benchmark house datasets demonstrate robustness model rare complex paraphrase utterances even adversarial test distributions
despite simplicity bag n grams sen tence representation find excel nlp task however ceived much attention recent years fur ther analysis properties necessary propose framework investigate amount type information capture general purpose bag n grams sentence represen tation first use sentence reconstruction tool obtain bag n grams representa tion contain general information sentence run prediction task sen tence length word content phrase content word order use obtain representation look specific type information capture representation analysis demonstrate bag n grams representa tion contain sentence structure level formation however incorporate n grams higher order n empirically help little encode information general except phrase content information
order learn universal sentence representations previous methods focus complex recurrent neural network supervise learn paper propose mean max attention autoencoder mean max aae within encoder decoder framework autoencoder rely entirely multihead self attention mechanism reconstruct input sequence encode propose mean max strategy apply mean max pool operations hide vectors capture diverse information input enable information steer reconstruction process dynamically decoder perform attention mean max representation train model large collection unlabelled data obtain high quality representations sentence experimental result broad range ten transfer task demonstrate model outperform state art unsupervised single methods include classical skip thoughts advance skip thoughtsln model furthermore compare traditional recurrent neural network mean max aae greatly reduce train time
sequence generative model rnn variants lstm gru show promise performance abstractive document summarization however still issue limit performance especially deal ing long sequence one issue best knowledge current model employ unidirectional decoder reason past still limit retain future context give prediction make model suffer generate unbalance output moreover unidirec tional attention base document summarization capture partial aspects attentional regularities due inherit challenge document summarization end propose end end trainable bidirectional rnn model tackle aforementioned issue model bidirectional encoder decoder architecture encoder decoder bidirectional lstms forward decoder initialize last hide state backward encoder backward decoder initialize last hide state ward encoder addition bidirectional beam search mechanism propose approximate inference algo rithm generate output summaries bidi rectional model enable model reason past future generate balance output result experimental result cnn daily mail dataset show propose model outperform current abstractive state art model considerable mar gin
proposal rumoureval two thousand and nineteen run early two thousand and nineteen part year semeval event since first rumoureval share task two thousand and seventeen interest automate claim validation greatly increase dangers fake news become mainstream concern yet automate support rumour check remain infancy reason important share task area continue provide focus effort likely increase therefore propose continuation veracity rumour determine previously supportive goal tweet discuss classify accord stance take regard rumour scope extend compare first rumoureval dataset substantially expand include reddit well twitter data additional languages also include
paper empirically evaluate utility transfer multi task learn challenge semantic classification task semantic interpretation noun noun compound comprehensive series experiment depth error analysis show transfer learn via parameter initialization multi task learn via parameter share help neural classification model generalize highly skew distribution relations demonstrate dual annotation two distinct set relations set compound exploit improve overall accuracy neural classifier f1 score less frequent difficult relations
present three enhancements exist encoder decoder model open domain conversational agents aim effectively model coherence promote output diversity one introduce measure coherence glove embed similarity dialogue context generate response two filter train corpora base measure coherence obtain topically coherent lexically diverse context response pair three train response generator use conditional variational autoencoder model incorporate measure coherence latent variable use context gate guarantee topical consistency context promote lexical diversity experiment opensubtitles corpus show substantial improvement competitive neural model term bleu score well metrics coherence diversity
grow amount comment make online discussions difficult moderate human moderators antisocial behavior common occurrence often discourage users participate discussion propose neural network base method partially automate moderation process consist two step first detect inappropriate comment moderators see second highlight inappropriate part within comment make moderation faster evaluate method data major slovak news discussion platform
paper present nict participation wmt18 share news translation task participate eight translation directions four language pair estonian english finnish english turkish english chinese english translation direction prepare state art statistical smt neural nmt machine translation systems nmt systems train transformer architecture use provide parallel data enlarge large quantity back translate monolingual data generate new incremental train framework primary submissions task result simple combination smt nmt systems systems rank first estonian english finnish english language pair constraint accord bleu case
paper present nict participation wmt18 share parallel corpus filter task organizers provide one billion word german english corpus crawl web part paracrawl project corpus noisy build acceptable neural machine translation nmt system use clean data wmt18 share news translation task design several feature train classifier score sentence pair noisy data finally sample one hundred million ten million word build correspond nmt systems empirical result show nmt systems train sample data achieve promise performance
latent variable model prefer choice conversational model compare sequence sequence seq2seq model tend generate generic repetitive responses despite train latent variable model remain difficult paper propose latent topic conversational model ltcm augment seq2seq neural latent topic component better guide response generation make train easier neural topic component encode information source sentence build global topic distribution word consult seq2seq model generation step study detail latent representation learn vanilla model ltcm extensive experiment contribute better understand train conditional latent model languages result show sample learn latent representations ltcm generate diverse interest responses subjective human evaluation judge also confirm ltcm overall prefer option
many character level task frame sequence sequence transduction target word natural language show leverage target language model derive unannotated target corpora combine precise alignment train data yield state art result cognate projection inflection generation phoneme grapheme conversion
unsupervised cross lingual embeddings map provide unique tool completely unsupervised translation even languages different script work use method task unsupervised cross lingual match product classifications work also investigate limitations unsupervised vector alignment also suggest two techniques align product classifications base descriptions use hierarchical information translations
input optimization methods google deep dream create interpretable representations neurons computer vision dnns propose evaluate ways transfer technology nlp result suggest gradient ascent gumbel softmax layer produce n gram representations outperform naive corpus search term target neuron activation representations highlight differences syntax awareness language visual model imaginet architecture
paper introduce document ground dataset text conversations define document ground conversations conversations content specify document dataset specify document wikipedia article popular movies dataset contain four thousand, one hundred and twelve conversations average two thousand, one hundred and forty-three turn per conversation position dataset provide relevant chat history generate responses also provide source information model could use describe two neural architectures provide benchmark performance task generate next response also evaluate model engagement fluency find information document help generate engage fluent responses
capabilities categorize clause base type situation entity eg events state generic statements clause introduce discourse benefit many nlp applications observe situation entity type clause depend discourse function clause play paragraph interpretation discourse function depend heavily paragraph wide contexts propose build context aware clause representations predict situation entity type clauses specifically propose hierarchical recurrent neural network model read whole paragraph time jointly learn representations clauses paragraph extensively model context influence inter dependencies clauses experimental result show model achieve state art performance clause level situation entity classification genre rich mascwiki corpus approach human level performance
systematic benchmark evaluation play important role process improve technologies question answer qa systems currently number exist evaluation methods natural language nl qa systems consider final answer limit utility within black box style evaluation herein propose subdivide evaluation approach enable finer grain evaluation qa systems present evaluation tool target nl question nlq interpretation step initial step qa pipeline result experiment use two public benchmark datasets suggest get deeper insight performance qa system use propose approach provide better guidance improve systems use black box style approach
toxic comment classification become active research field many recently propose approach however approach address task challenge others still remain unsolved directions research need end compare different deep learn shallow approach new large comment dataset propose ensemble outperform individual model validate find second dataset result ensemble enable us perform extensive error analysis reveal open challenge state art methods directions towards pending future research challenge include miss paradigmatic context inconsistent dataset label
recent work show learn better visual semantic embeddings leverage image descriptions one language investigate detail condition affect performance type ground language learn model show multilingual train improve bilingual train low resource languages benefit train higher resource languages demonstrate multilingual model train equally well either translations comparable sentence pair annotate set image multiple language enable improvements via additional caption caption rank objective
natural language generation nlg critical component speak dialogue system divide two phase one sentence plan decide overall sentence structure two surface realization determine specific word form flatten sentence structure string rise deep learn modern nlg model base sequence sequence seq2seq model basically contain encoder decoder structure nlg model generate sentence scratch jointly optimize sentence plan surface realization however simple encoder decoder architecture usually fail generate complex long sentence decoder difficulty learn grammar diction knowledge well paper introduce nlg model hierarchical attentional decoder hierarchy focus leverage linguistic knowledge specific order experiment show propose method significantly outperform traditional seq2seq model smaller model size design hierarchical attentional decoder apply various nlg systems furthermore different generation strategies base linguistic pattern investigate analyze order guide future nlg research work
cross lingual entity link xel aim grind entity mention write language english knowledge base kb wikipedia xel languages challenge owe limit availability resources supervision address challenge develop first xel approach combine supervision multiple languages jointly enable approach augment limit supervision target language additional supervision high resource language like english b train single entity link model multiple languages improve upon individually train model language extensive evaluation three benchmark datasets across eight languages show approach significantly improve current state art also provide analyse two limit resource settings zero shoot set supervision target language available b low resource set supervision target language available analysis provide insights limitations zero shoot xel approach realistic scenarios show value joint supervision low resource settings
seq2seq model base recurrent neural network rnns recently receive lot attention domain semantic parse question answer principle train directly pair natural language utterances logical form performance limit amount available data alleviate problem propose exploit various source prior knowledge well formedness logical form model weight context free grammar likelihood certain entities present input utterance also present logical form model weight finite state automata grammar automata combine together efficient intersection algorithm form soft guide background rnn test method extension overnight dataset show strongly improve rnn baseline also outperform non rnn model base rich set hand craft feature
present system rapidly customize event extraction capability find new event type arguments system allow user find expand filter event trigger new event type explore unannotated corpus system automatically generate mention level event annotation automatically train neural network model find correspond event additionally system use ace corpus train argument model extract actor place time arguments event type include ones see train data experiment show less ten minutes human effort per event type system achieve good performance sixty-seven novel event type code documentation demonstration video release open source githubcom
generate english transliteration name write foreign script important challenge step multilingual knowledge acquisition information extraction exist approach transliteration generation require large five thousand number train examples difficulty contrast transliteration discovery somewhat easier task involve pick plausible transliteration give list work present bootstrapping algorithm use constrain discovery improve generation use five hundred train examples show source annotators matter hours open task languages large number train examples unavailable evaluate transliteration generation performance well improvement bring cross lingual candidate generation entity link typical downstream task present comprehensive evaluation approach nine languages write unique script
article present whisper speech detector far field domain propose system consist long short term memory lstm neural network train log filterbank energy lfbe acoustic feature model train evaluate record human interactions voice control far field devices whisper normal phonation modes compare multiple inference approach utterance level classification examine trajectories lstm posteriors addition engineer set feature base signal characteristics inherent whisper speech evaluate effectiveness separate whisper normal speech benchmarking feature use multilayer perceptrons mlp lstms suggest propose feature combination lfbe feature help us improve classifiers prove enough data lstm model indeed capable learn whisper characteristics lfbe feature alone compare simpler mlp model use lfbe feature engineer separate whisper normal speech addition prove lstm classifiers accuracy improve incorporation propose engineer feature
distinguish arguments adjuncts verb longstanding nontrivial problem natural language process argumenthood information important task semantic role label srl prepositional phrase pp attachment disambiguation theoretical linguistics many diagnostic test argumenthood exist often yield conflict potentially gradient result especially case syntactically oblique items pps propose two pp argumenthood prediction task branch two motivations one binary argument adjunct classification pps verbnet two gradient argumenthood prediction use human judgments gold standard report result prediction model use pretrained word embeddings linguistically inform feature best result task one acc0955 f10954 elmobilstm two pearson r0624 word2vecmlp furthermore demonstrate utility argumenthood prediction improve sentence representations via performance gain srl sentence encoder pretrained task
perform automatic paraphrase detection subtitle data opusparcus corpus comprise six european languages german english finnish french russian swedish train two type supervise sentence embed model word average wa model gate recurrent average network gran model find gran outperform wa robust noisy train data better result obtain noisier data less cleaner data additionally experiment datasets without reach level performance domain mismatch train test data
present analysis inner work convolutional neural network cnns process text cnns use computer vision interpret project filter image space discrete sequence input cnns remain mystery aim understand method network process classify text examine common hypotheses problem filter accompany global max pool serve ngram detectors show filter may capture several different semantic class ngrams use different activation pattern global max pool induce behavior separate important ngrams rest finally show practical use case derive find form model interpretability explain train model derive concrete identity filter bridge gap visualization tool vision task nlp prediction interpretability explain predictions code implementation available online githubcom sayaendo interpret cnn text
internet users generate content unprecedented rat build intelligent systems capable discriminate useful content within ocean information thus become urgent need paper aim predict usefulness amazon review exploit feature come shelf argumentation mine system argue usefulness review fact strictly relate argumentative content whereas use already train system avoid costly need relabeling novel dataset result obtain large publicly available corpus support hypothesis
effort assist factcheckers process factchecking tackle claim detection task one necessary stag prior determine veracity claim consist identify set sentence long text deem capable factchecked paper collaborative work full fact independent factchecking charity academic partner leverage expertise professional factcheckers develop annotation schema benchmark automate claim detection consistent across time topics annotators previous approach annotation schema use crowdsource annotation dataset sentence uk political tv show introduce approach base universal sentence representations perform classification achieve f1 score eighty-three five relative improvement state art methods claimbuster claimrank system deploy production receive positive user feedback
exist dialog datasets contain sequence utterances responses without explicit background knowledge associate result development model treat conversation sequence sequence generation task ie give sequence utterances generate response sequence overly simplistic view conversation also emphatically different way humans converse heavily rely background knowledge topic oppose simply rely previous sequence utterances example common humans involuntarily produce utterances copy suitably modify background article read topic facilitate development natural conversation model mimic human process converse create new dataset contain movie chat wherein response explicitly generate copy modify sentence unstructured background knowledge plot comment review movie establish baseline result dataset 90k utterances 9k conversations use three different model pure generation base model ignore background knowledge ii generation base model learn copy information background knowledge require iii span prediction base model predict appropriate response span background knowledge
present paper survey neural approach conversational ai develop last years group conversational systems three categories one question answer agents two task orient dialogue agents three chatbots category present review state art neural approach draw connection traditional approach discuss progress make challenge still face use specific systems model case study
run sentence common grammatical mistake little research tackle problem date work introduce two machine learn model correct run sentence outperform lead methods relate task punctuation restoration whole sentence grammatical error correction due limit annotate data error experiment artificially generate train data clean newswire text find suggest artificial train data viable task discuss implications correct run ons type mistake low coverage error annotate corpora
unsupervised representation learn algorithms word2vec elmo improve accuracy many supervise nlp model mainly take advantage large amount unlabeled text however supervise model learn task specific label data main train phase therefore propose cross view train cvt semi supervise learn algorithm improve representations bi lstm sentence encoder use mix label unlabeled data label examples standard supervise learn use unlabeled examples cvt teach auxiliary prediction modules see restrict view input eg part sentence match predictions full model see whole input since auxiliary modules full model share intermediate representations turn improve full model moreover show cvt particularly effective combine multi task learn evaluate cvt five sequence tag task machine translation dependency parse achieve state art result
biomedical literature common entity boundaries align word boundaries therefore effective identification entity span require approach capable consider tokens smaller word introduce novel subword approach name entity recognition ner use byte pair encode bpe combination convolutional recurrent neural network produce byte level tag entities present experimental result several standard biomedical datasets namely biocreative vi bio id jnlpba genetag datasets demonstrate competitive performance bypass specialize domain expertise need create biomedical text tokenization rule
previous research word embeddings show sparse representations either learn top exist dense embeddings obtain model constraints train time benefit increase interpretability properties degree dimension understand human associate recognizable feature data paper transfer idea sentence embeddings explore several approach obtain sparse representation introduce novel quantitative automate evaluation metric sentence embed interpretability base topic coherence methods observe increase interpretability compare dense model dataset movie dialogs scene descriptions ms coco dataset
toxic online content become major issue today world due exponential increase use internet people different culture educational background differentiate hate speech offensive language key challenge automatic detection toxic text content paper propose approach automatically classify tweet twitter three class hateful offensive clean use twitter dataset perform experiment consider n grams feature pass term frequency inverse document frequency tfidf value multiple machine learn model perform comparative analysis model consider several value n n grams tfidf normalization methods tune model give best result achieve nine hundred and fifty-six accuracy upon evaluate test data also create module serve intermediate user twitter
multilingual societies like indian subcontinent use code switch languages much popular convenient users paper study offense abuse detection code switch pair hindi english ie hinglish pair speak task make difficult due non fix grammar vocabulary semantics spell hinglish language apply transfer learn make lstm base model hate speech classification model surpass performance show current best model establish state art unexplored domain hinglish offensive text classificationwe also release model embeddings train research purpose
work improve monolingual sentence alignment text simplification specifically text standard simple wikipedia introduce convolutional neural network structure model similarity two sentence due limitation available parallel corpora model train semi supervise way use output knowledge base high performance align system apply result similarity score rescore knowledge base output adapt model small hand align dataset experiment show rescoring adaptation improve performance knowledge base method
original goal social media platform facilitate users indulge healthy meaningful conversations often find become avenue wanton attack want alleviate issue hence try provide detail analysis abusive behavior monitor twitter complexity natural language construct make task challenge show apply contextual attention long short term memory network help us give near state art result multiple benchmarks abuse detection data set twitter
neural architecture name entity recognition achieve great success field natural language process currently dominate architecture consist bi directional recurrent neural network rnn encoder conditional random field crf decoder paper propose deformable stack structure name entity recognition connections two adjacent layer dynamically establish evaluate deformable stack structure adapt different layer model achieve state art performances ontonotes dataset
motivate recent find probabilistic model acceptability judgments propose syntactic log odds ratio slor normalize language model score metric referenceless fluency evaluation natural language generation output sentence level introduce wpslor novel wordpiece base version harness compact language model even though word overlap metrics like rouge compute help hand write reference referenceless methods obtain significantly higher correlation human fluency score benchmark dataset compress sentence finally present rouge lm reference base metric natural extension wpslor case available reference show rouge lm yield significantly higher correlation human judgments baseline metrics include wpslor
neural state art sequence sequence seq2seq model often perform well small train set address paradigm completion morphological task give partial paradigm generate miss form propose two new methods minimal resource set paradigm transduction since assume paradigms available train neural seq2seq model able capture relationships paradigm cells tie idiosyncracies train set paradigm transduction mitigate problem exploit input subset inflect form test time ii source selection high precision ship multi source model learn automatically select one multiple source predict target inflection perform well minimal resource set ship alternative identify reliable source train data limit fifty-two language benchmark dataset outperform previous state art nine hundred and seventy-one absolute accuracy
although end end neural text speech tts methods tacotron2 propose achieve state art performance still suffer two problems one low efficiency train inference two hard model long dependency use current recurrent neural network rnns inspire success transformer network neural machine translation nmt paper introduce adapt multi head attention mechanism replace rnn structure also original attention mechanism tacotron2 help multi head self attention hide state encoder decoder construct parallel improve train efficiency meanwhile two input different time connect directly self attention mechanism solve long range dependency problem effectively use phoneme sequence input transformer tts network generate mel spectrograms follow wavenet vocoder output final audio result experiment conduct test efficiency performance new network efficiency transformer tts network speed train four hundred and twenty-five time faster compare tacotron2 performance rigorous human test show propose model achieve state art performance outperform tacotron2 gap forty-eight close human quality four hundred and thirty-nine vs four hundred and forty-four mos
address jointly two important task question answer community forums give new question find relate exist question ii find relevant answer new question use auxiliary task complement previous two ie iii find good answer respect thread question question comment thread use deep neural network dnns learn meaningful task specific embeddings incorporate conditional random field crf model multitask set perform joint learn complex graph structure dnns alone achieve competitive result train produce embeddings crf make use embeddings dependencies task improve result significantly consistently across variety evaluation metrics thus show complementarity dnns structure learn
event extraction practical utility natural language process real world common phenomenon multiple events exist sentence extract difficult extract single event previous work model associations events sequential model methods suffer lot low efficiency capture long range dependencies paper propose novel jointly multiple events extraction jmee framework jointly extract multiple event trigger arguments introduce syntactic shortcut arc enhance information flow attention base graph convolution network model graph information experiment result demonstrate propose framework achieve competitive result compare state art methods
paper present extension stochastic answer network san one state art machine read comprehension model able judge whether question unanswerable extend san contain two components span detector binary classifier judge whether question unanswerable components jointly optimize experiment show san achieve result competitive state art stanford question answer dataset squad twenty facilitate research field release code https githubcom kevinduh sanmrc
mixture softmaxes mos show effective address expressiveness limitation softmax base model despite know advantage mos practically seal large consumption memory computational time due need compute multiple softmaxes work set unleash power mos practical applications investigate improve word cod scheme could effectively reduce vocabulary size hence relieve memory computation burden show bpe propose hybrid lightrnn lead improve encode mechanisms halve time memory consumption mos without performance losses mos achieve improvement fifteen bleu score iwslt two thousand and fourteen german english corpus improvement seventy-six cider score image caption moreover larger wmt two thousand and fourteen machine translation dataset mos boost transformer yield two hundred and ninety-five bleu score english german four hundred and twenty-one bleu score english french outperform single softmax transformer eight four bleu score respectively achieve state art result wmt two thousand and fourteen english german task
bridge gap capabilities state art factoid question answer qa users ask need large datasets real user question capture various question phenomena users interest diverse ways question formulate introduce comqa large dataset real user question exhibit different challenge aspects compositionality temporal reason comparisons comqa question come wikianswers community qa platform typically contain question satisfactorily answerable exist search engine technology large crowdsourcing effort clean question dataset group question paraphrase cluster annotate cluster answer comqa contain eleven thousand, two hundred and fourteen question group four thousand, eight hundred and thirty-four paraphrase cluster detail process construct comqa include measure take ensure high quality make effective use crowdsourcing also present extensive analysis dataset result achieve state art systems comqa demonstrate dataset driver future research qa
exist question answer qa datasets fail train qa systems perform complex reason provide explanations answer introduce hotpotqa new dataset 113k wikipedia base question answer pair four key feature one question require find reason multiple support document answer two question diverse constrain pre exist knowledge base knowledge schemas three provide sentence level support facts require reason allow qa systems reason strong supervision explain predictions four offer new type factoid comparison question test qa systems ability extract relevant facts perform necessary comparison show hotpotqa challenge latest qa systems support facts enable model improve performance make explainable predictions
large scale natural language understand nlu systems typically train large quantities data require fast scalable train strategy typical design nlu systems consist domain level nlu modules domain classifier intent classifier name entity recognizer hypotheses nlu interpretations consist various intentslot combinations domain specific modules typically aggregate another downstream component ranker integrate output domain level recognizers return score list cross domain hypotheses ideal ranker exhibit follow two properties prefer relevant hypothesis give input top hypothesis b interpretation score correspond hypothesis produce ranker calibrate calibration allow final nlu interpretation score comparable across domains propose novel ranker strategy address aspects also maintain domain specific modularity design optimization loss function modularized ranker present result decrease top hypothesis error rate well maintain model calibration also experiment extension involve train domain specific rankers datasets curated independently domain allow asynchronization propose ranker design showcases follow improve nlu performance unweighted aggregation strategy ii cross domain calibrate performance iii support use case involve train ranker datasets curated domain independently
work deal non native children speech investigate multi task transfer learn approach adapt multi language deep neural network dnn speakers specifically children learn foreign language application scenario characterize young students learn english german read sentence second languages well mother language paper analyze discuss techniques train effective dnn base acoustic model start children native speech perform adaptation limit non native audio material multi lingual model adopt baseline common phonetic lexicon define term units international phonetic alphabet ipa share across three languages hand italian german english dnn adaptation methods base transfer learn evaluate significant non native evaluation set result show result non native model allow significant improvement respect mono lingual system adapt speakers target language
work propose novel method train neural network perform single document extractive summarization without heuristically generate extractive label call approach banditsum treat extractive summarization contextual bandit cb problem model receive document summarize context choose sequence sentence include summary action policy gradient reinforcement learn algorithm use train model select sequence sentence maximize rouge score perform series experiment demonstrate banditsum able achieve rouge score better comparable state art extractive summarization converge use significantly fewer update step compete approach addition show empirically banditsum perform significantly better compete approach good summary sentence appear late source document
predict context dependent non literal utterances like sarcastic ironic expressions still remain challenge task nlp go beyond linguistic pattern encompass common sense share knowledge crucial components capture complex morpho syntactic feature usually serve indicators irony sarcasm across dynamic contexts propose model use character level vector representations word base elmo test model seven different datasets derive three different data source provide state art performance six otherwise offer competitive result
recent work use auxiliary prediction task classifiers investigate properties lstm representations begin would light pretrained representations like elmo peters et al two thousand and eighteen cove mccann et al two thousand and seventeen beneficial neural language understand model still though yet clear understand choice pretraining objective affect type linguistic information model learn mind compare four objectives language model translation skip think autoencoding ability induce syntactic part speech information make fair comparison task hold constant quantity genre train data well lstm architecture find representations language model consistently perform best syntactic auxiliary prediction task even train relatively small amount data result suggest language model may best data rich pretraining task transfer learn applications require syntactic information also find representations randomly initialize freeze lstms perform strikingly well syntactic auxiliary task effect disappear amount train data auxiliary task reduce
dependency tree help relation extraction model capture long range relations word however exist dependency base model either neglect crucial information eg negation prune dependency tree aggressively computationally inefficient difficult parallelize different tree structure propose extension graph convolutional network tailor relation extraction pool information arbitrary dependency structure efficiently parallel incorporate relevant information maximally remove irrelevant content apply novel prune strategy input tree keep word immediately around shortest path two entities among relation might hold result model achieve state art performance large scale tacred dataset outperform exist sequence dependency base neural model also show detail analysis model complementary strengths sequence model combine improve state art
paper introduce iterative text summarization iteration base model supervise extractive text summarization inspire observation often necessary human read article multiple time order fully understand summarize content current summarization approach read document generate document representation result sub optimal representation address issue introduce model iteratively polish document representation many pass document part model also introduce selective read mechanism decide accurately extent sentence model update experimental result cnn dailymail duc2002 datasets demonstrate model significantly outperform state art extractive systems evaluate machine humans
present neural network base approach classify online hate speech general well racist sexist speech particular use pre train word embeddings max mean pool simple fully connect transformations embeddings able predict occurrence hate speech three commonly use publicly available datasets model match outperform state art f1 performance three datasets use significantly fewer parameters minimal feature preprocessing compare previous methods
compare three new datasets question answer squad twenty quac coqa along several new feature one unanswerable question two multi turn interactions three abstractive answer show datasets provide complementary coverage first two aspects weak coverage third datasets structural similarity single extractive model easily adapt datasets show improve baseline result squad twenty coqa despite similarity model train one dataset ineffective another dataset find moderate performance improvement pretraining encourage cross evaluation release code conversion datasets https githubcom my89 co squac
language model base approach story plot generation attempt construct plot sample language model lm predict next character word sentence add story lm techniques lack ability receive guidance user achieve specific goal result stories clear sense progression lack coherence present reward shape technique analyze story corpus produce intermediate reward backpropagated pre train lm order guide model towards give goal automate evaluations show technique create model generate story plot consistently achieve specify goal human subject study show generate stories plausible event order baseline plot generation techniques
present paper aim present lemmatization word level error correction system sorani kurdish propose hybrid approach base morphological rule n gram language model call lemmatization error correction systems peyv renus respectively first tool present sorani kurdish best knowledge peyv lemmatizer show eight hundred and sixty-seven accuracy renus use lexicon obtain nine hundred and sixty-four accuracy without lexicon correction system eighty-seven accuracy two fundamental text process tool tool pave way research natural language process applications sorani kurdish
introduce adaptive input representations neural language model extend adaptive softmax grave et al two thousand and seventeen input representations variable capacity several choices factorize input output layer whether model word character sub word units perform systematic comparison popular choices self attentional architecture experiment show model equip adaptive embeddings twice fast train popular character input cnn lower number parameters wikitext one hundred and three benchmark achieve one hundred and eighty-seven perplexity improvement one hundred and five perplexity compare previously best publish result billion word benchmark achieve two thousand, three hundred and two perplexity
neural network base approach become widespread abstractive text summarization though previously propose model abstractive text summarization address problem repetition content summary explicitly consider information structure one reason previous model fail account information structure generate summary standard datasets include summaries variable lengths result problems analyze information flow specifically manner first sentence relate follow sentence therefore use dataset contain summaries three bullet point propose neural network base abstractive summarization model consider information structure generate summaries experimental result show information structure summary control thus improve performance overall summarization
even though machine learn become major scene dialogue research community real breakthrough block scale data available address fundamental obstacle introduce multi domain wizard oz dataset multiwoz fully label collection human human write conversations span multiple domains topics size 10k dialogues least one order magnitude larger previous annotate task orient corpora contribution work apart open source dataset label dialogue belief state dialogue action two fold firstly detail description data collection procedure along summary data structure analysis provide propose data collection pipeline entirely base crowd source without need hire professional annotators secondly set benchmark result belief track dialogue act response generation report show usability data set baseline future study
paper study empirically validity measure referential success refer expressions involve gradual properties specifically study ability several measure referential success predict success user choose right object give refer expression experimental result indicate certain fuzzy measure success able predict human accuracy reference resolution measure therefore suitable estimation success otherwise refer expression produce generation algorithm especially case properties domain assume crisp denotations
paper introduce novel natural language generation task term text morph target generate intermediate sentence fluency smooth two input sentence propose morph network consist edit vector generation network sentence edit network train jointly specifically edit vectors generate recurrent neural network model lexical gap source sentence target sentence sentence edit network iteratively generate new sentence current edit vector sentence generate previous step conduct experiment ten million text morph sequence extract yelp review dataset experiment result show propose method outperform baselines text morph task also discuss directions opportunities future research text morph
recent iterate response ir model pragmatics conceptualize language use recursive process agents reason increase communicative efficiency model generally define complete utterances however substantial evidence pragmatic reason take place incrementally production comprehension address incremental ir model compare incremental global versions use computational simulations assess incremental model exist experimental data tuna corpus refer expression generation show model capture phenomena reach global versions
propose simple robust non parameterized approach build sentence representations inspire gram schmidt process geometric theory build orthogonal basis subspace span word surround context sentence model semantic mean word sentence base two aspects one relatedness word vector subspace already span contextual word word novel semantic mean shall introduce new basis vector perpendicular exist subspace follow motivation develop innovative method base orthogonal basis combine pre train word embeddings sentence representations approach require zero parameters along efficient inference performance evaluate approach eleven downstream nlp task model show superior performance compare non parameterized alternatives competitive approach rely either large amount label data prolong train time
stylistic variation critical render utterances generate conversational agents natural engage paper focus sequence sequence model open domain dialogue response generation propose new method evaluate extent model able generate responses reflect different personality traits
build model logistic regression linear support vector machine large dataset consist regular news article news satirical websites show linear classifiers corpus sixty thousand article perform precision nine hundred and eighty-seven recall nine hundred and fifty-two random test set news hand test classifier publication source completely unknown train accuracy eight hundred and eighty-two f1 score seven hundred and sixty-three achieve another result show algorithm distinguish news write news agency pay article customers result accuracy ninety-nine
code switch refer usage two languages within sentence discourse global phenomenon among multilingual communities emerge independent area research increase demand code switch automatic speech recognition asr systems development code switch speech corpus become highly desirable however train systems limit code switch resources available yet work present first efforts build code switch asr system indian context purpose create hindi english code switch speech database database contain speech utterances code switch properties also cover session speaker variations like pronunciation accent age gender etc database apply several speech signal process applications code switch asr language identification language model speech synthesis etc paper mainly present analysis statistics collect code switch speech corpus later performance result asr task report create database
speak language understand slu systems widely use handle customer care callsa traditional slu system consist acoustic model language model lm areused decode utterance natural language understand nlu model predict theintent share across different domains lm nlu model need trainedspecifically every new task however prepare enough data train model prohibitivelyexpensive paper introduce efficient method expand limit domain data theprocess start train preliminary nlu model base logistic regression domaindata since feature base onn twelve grams detect informative n gramsfor intent class use n grams find sample domain corpus that1 contain desire n gram two similar intent label ones meet firstconstraint use train new lm model ones meet constraints use train anew nlu model result two divergent experimental setups show propose approachreduces thirty absolute classification error rate cer compare preliminary modelsand significantly outperform traditional data expansion algorithms ones base onsemi supervise learn tf idf embed vectors
despite deep recurrent neural network rnns demonstrate strong performance text classification train rnn model often expensive require extensive collection annotate data may available overcome data limitation issue exist approach leverage either pre train word embed sentence representation lift burden train rnns scratch paper show jointly learn sentence representations multiple text classification task combine pre train word level sentence level encoders result robust sentence representations useful transfer learn extensive experiment analyse use wide range transfer linguistic task endorse effectiveness approach
obtain datasets label facilitate model development challenge machine learn task difficulty heighten medical image data limit accessibility label require costly time effort train medical specialists medical image study however often accompany medical report produce radiologist identify important feature correspond scan physicians specifically train radiology propose methodology approximate image level label radiology study associate report use general purpose language process tool medical concept extraction sentiment analysis simple manually craft heuristics false positive reduction use approach label one hundred and seventy-five thousand head ct study presence thirty-three feature indicative eleven clinically relevant condition twenty-seven thirty keywords yield positive result three occurrences lower bind confidence intervals create estimate percentage accurately label report eighty-five average ninety-five though noisier manual label result suggest method viable mean label medical image scale
paper summarise experimental setup result first share task end end e2e natural language generation nlg speak dialogue systems recent end end generation systems promise since reduce need data annotation however currently limit small delexicalised datasets e2e nlg share task aim assess whether novel approach generate better quality output learn dataset contain higher lexical richness syntactic complexity diverse discourse phenomena compare sixty-two systems submit seventeen institutions cover wide range approach include machine learn architectures majority implement sequence sequence model seq2seq well systems base grammatical rule templates
introduce automatic system achieve state art result winograd schema challenge wsc common sense reason task require diverse complex form inference knowledge method use knowledge hunt module gather text web serve evidence candidate problem resolutions give input problem system generate relevant query send search engine extract classify knowledge return result weigh make resolution approach improve f1 performance full wsc twenty-one previous best represent first system exceed five f1 demonstrate approach competitive choice plausible alternatives copa task suggest generally applicable
de identification process remove eighteen protect health information phi clinical note order text consider individually identifiable recent advance natural language process nlp allow use deep learn techniques task de identification paper present deep learn architecture build latest nlp advance incorporate deep contextualized word embeddings variational drop bi lstms test architecture two gold standard datasets show architecture achieve state art performance data set also converge faster systems without use dictionaries knowledge source
paper present extensive comparative study four neural network model include fee forward network convolutional network recurrent network long short term memory network two sentence classification datasets english vietnamese text show english dataset convolutional network model without feature engineer outperform competitive sentence classifiers rich hand craft linguistic feature demonstrate glove word embeddings consistently better skip gram word embeddings word count vectors also show superiority convolutional neural network model vietnamese newspaper sentence dataset strong baseline model experimental result suggest good practice apply neural network model sentence classification
common entity mention contain mention recursively paper introduce scalable transition base method model nest structure mention first map sentence nest mention designate forest mention correspond constituent forest shift reduce base system learn construct forest structure bottom manner action sequence whose maximal length guarantee three time sentence length base stack lstm employ efficiently effectively represent state system continuous space system incorporate character base component capture letter level pattern model achieve state art result ace datasets show effectiveness detect nest mention
work propose novel segmental hypergraph representation model overlap entity mention prevalent many practical datasets show model build top new representation able capture feature interactions capture previous model maintain low time complexity inference also present theoretical analysis formally assess representation better alternative representations report literature term representational power couple neural network feature learn model achieve state art performance three benchmark datasets annotate overlap mention
study propose novel way identify sentiment phrase use legal domain add complexity language use law inability exist systems accurately predict sentiments word law main motivations behind study transfer learn approach use domain adaptation task well propose methodology achieve improvement six compare source model accuracy legal domain
dependency parse one important natural language process task assign syntactic tree texts due wider availability dependency corpora improve parse machine learn techniques parse accuracies supervise learn base systems significantly improve however due nature supervise learn parse systems highly rely manually annotate train corpora work reasonably good domain data performance drop significantly test domain texts bridge performance gap domain domain thesis investigate three semi supervise techniques domain dependency parse namely co train self train dependency language model approach use easily obtainable unlabelled data improve domain parse accuracies without need expensive corpora annotation evaluations several english domains multi lingual data show quite good improvements parse accuracy overall work conduct survey semi supervise methods domain dependency parse extend compare number important semi supervise methods unify framework comparison techniques show self train work equally well co train domain parse dependency language model improve domain accuracies
structural information important natural language understand although current neural net base model limit ability take local syntactic information fail use high level large scale structure document information valuable text understand since contain author strategy express information build effective representation form appropriate output modes propose neural net base model zoom network capable represent leverage text structure long document develop analyze rhythm extract critical information generally zn consist encode neural net build hierarchical representation document interpret neural model read information multi level issue label action policy net model train hybrid paradigm supervise learn distinguish right wrong decision reinforcement learn determine goodness among multiple right paths apply propose model long text sequence label task performance exceed baseline model bilstm crf ten f1 measure
negation scope annotate several english chinese corpora highly accurate model task languages learn annotations unfortunately annotations available languages could model detect negation scope apply language train develop neural model learn cross lingual word embeddings universal dependencies english test chinese show work surprisingly well find model syntax helpful even monolingual settings cross lingual word embeddings help relatively little analyse case still difficult task
present simple accurate span base model semantic role label srl model directly take account possible argument span score label decode time greedily select higher score label span one advantage model allow us design use span level feature difficult use token base bio tag approach experimental result demonstrate ensemble model achieve state art result eight hundred and seventy-four f1 eight hundred and seventy f1 conll two thousand and five two thousand and twelve datasets respectively
translation pronouns present special challenge machine translation day since often require context outside current sentence recent work model access information across sentence boundaries see moderate improvements term automatic evaluation metrics bleu however metrics quantify overall translation quality ill equip measure gain additional context argue different kind evaluation need assess well model translate inter sentential phenomena pronouns paper therefore present test suite contrastive translations focus specifically translation pronouns furthermore perform experiment several context aware model show gain bleu moderate systems outperform baselines large margin term accuracy contrastive test set experiment also show effectiveness parameter tie multi encoder architectures
emotion recognition conversations challenge task recently gain popularity due potential applications however large scale multimodal multi party emotional conversational database contain two speakers per dialogue miss thus propose multimodal emotionlines dataset meld extension enhancement emotionlines meld contain thirteen thousand utterances one thousand, four hundred and thirty-three dialogues tv series friends utterance annotate emotion sentiment label encompass audio visual textual modalities propose several strong multimodal baselines show importance contextual multimodal information emotion recognition conversations full dataset available use http affective meldgithubio
paper demonstrate word sense disambiguation wsd improve neural machine translation nmt widen source context consider model sense potentially ambiguous word first introduce three adaptive cluster algorithms wsd base k mean chinese restaurant process random walk apply large word contexts represent low rank space evaluate semeval share task data learn word vectors jointly sense vectors define best wsd method within state art nmt system show concatenation vectors use sense selection mechanism base weight average sense vectors outperform several baselines include sense aware ones demonstrate translation five language pair improvements one bleu point strong nmt baselines four accuracy ambiguous nouns verbs twenty score manually several challenge word
essence two tag methods direct tag tag sentence compression tag information need use regular expression base inherent language pattern natural language though many advantage extract regular data direct tag applicable situations data need extract regular surround word regular relatively regular use information compression cut information need tag data need way increase precision data undermine recall data
auto encoders compress input data latent space representation reconstruct original data representation latent representation easily interpret humans paper propose train auto encoder encode input text human readable sentence unpaired abstractive summarization thereby achieve auto encoder compose generator reconstructor generator encode input text shorter word sequence reconstructor recover generator input generator output make generator output human readable discriminator restrict output generator resemble human write sentence take generator output summary input text abstractive summarization achieve without document summary pair train data promise result show english chinese corpora
present framework generate natural language description structure data table problem come category data text natural language generation nlg modern data text nlg systems typically employ end end statistical neural architectures learn limit amount task specific label data therefore exhibit limit scalability domain adaptability interpretability unlike systems modular pipeline base approach require task specific parallel data rather rely monolingual corpora basic shelf nlp tool make system scalable easily adaptable newer domains system employ three stag pipeline convert entries structure data canonical form ii generate simple sentence atomic entry canonicalized representation iii combine sentence produce coherent fluent adequate paragraph description sentence compound co reference replacement modules experiment benchmark mix domain dataset curated paragraph description table reveal superiority system exist data text approach also demonstrate robustness system accept popular datasets cover diverse data type knowledge graph key value map
read comprehension task test ability model process long term context remember salient information recent work show relatively simple neural methods attention sum reader perform well task however systems still significantly trail human performance analysis suggest many remain hard instance relate inability track entity reference throughout document work focus hard entity track case two extensions one additional entity feature two train multi task track objective show simple modifications improve performance independently combination outperform previous state art lambada dataset particularly difficult entity examples
attempt improve overall translation quality increase focus integrate linguistic elements machine translation mt significant progress achieve especially recently neural model automatically evaluate output systems still open problem current practice mt evaluation rely single reference translation even though many ways translate particular text tend disregard higher level information discourse propose novel approach assess translate output base source text rather reference translation measure extent semantics discourse elements discourse relations particular source text preserve mt output challenge detect discourse relations source text determine whether relations correctly transfer crosslingually target language without reference translation methodology could use independently discourse level evaluation component metrics time substantial amount mt online would benefit evaluation source text serve benchmark
transliteration convert word source language eg english word target language eg vietnamese conversion consider phonological structure target language transliterate output need pronounceable target language example word vietnamese begin consonant cluster phonologically invalid thus would incorrect output transliteration system statistical transliteration approach albeit widely adopt explicitly model target language phonology often result invalid output problem compound limit linguistic resources available convert foreign word transliterate word target language work present phonology augment statistical framework suitable transliteration especially limit linguistic resources available propose concept pseudo syllables structure represent segment foreign word organize accord syllables target language phonology perform transliteration experiment vietnamese cantonese show propose framework outperform statistical baseline four thousand, four hundred and sixty-eight relative limit train examples five hundred and eighty-seven entries
development dialog techniques conversational search attract attention enable users interact search engine natural efficient manner however compare natural language understand traditional task orient dialog focus slot fill track query understand e commerce conversational search quite different challenge due diverse user expressions complex intentions work define real world problem query track e commerce conversational search goal update internal query round interaction also propose self attention base neural network handle task machine comprehension perspective build novel e commerce query track dataset operational e commerce search engine experimental result dataset suggest propose model outperform several baseline methods substantial gain exact match accuracy f1 score show potential machine comprehension like model task
spontaneous speak dialogue often disfluent contain pause hesitations self corrections false start process phenomena essential understand speaker intend mean control flow conversation furthermore process need word word incremental allow downstream process begin early possible order handle real spontaneous human conversational behaviour addition developer point view highly desirable able develop systems train clean examples also able generalise diverse disfluent variations data thereby enhance data efficiency robustness paper present multi task lstm base model incremental detection disfluency structure hook component incremental interpretation eg incremental semantic parser else simply use clean current utterance produce train system switchboard dialogue act swda corpus present accuracy dataset model outperform prior neural network base incremental approach ten percentage point swda employ simpler architecture test model generalisation potential evaluate model babi dataset without additional train babi dataset synthesise goal orient dialogues control distribution disfluencies type show approach good generalisation potential shed light type disfluency might amenable domain general process
state art neural machine translation systems despite different architectural skeletons eg recurrence convolutional share indispensable feature attention however exist attention methods token base ignore importance phrasal alignments key ingredient success phrase base statistical machine translation paper propose novel phrase base attention methods model n grams tokens attention entities incorporate phrase base attentions recently propose transformer network demonstrate approach yield improvements thirteen bleu english german five bleu german english translation task wmt newstest2014 use wmt sixteen train data
paper propose new rich resource enhance amr aligner produce multiple alignments new transition system amr parse along oracle parser aligner tune oracle parser via pick alignment lead highest score achievable amr graph experimental result show aligner outperform rule base aligner previous work achieve higher alignment f1 score consistently improve two open source amr parsers base aligner transition system develop transition base amr parser parse sentence amr graph directly ensemble parsers word pos tag input lead six hundred and eighty-four smatch f1 score
although transformer translation model vaswani et al two thousand and seventeen achieve state art performance variety translation task use document level context deal discourse phenomena problematic transformer still remain challenge work extend transformer model new context encoder represent document level context incorporate original encoder decoder large scale document level parallel corpora usually available introduce two step train method take full advantage abundant sentence level parallel corpora limit document level parallel corpora experiment nist chinese english datasets iwslt french english datasets show approach improve transformer significantly
analyse understand languages word boundaries base morphological analysis japanese chinese thai desirable perform appropriate word segmentation word embeddings inherently difficult languages recent years various language model base deep learn make remarkable progress methodologies utilize character level feature successfully avoid difficult problem however model feed character level feature languages often cause overfitting due large number character type paper propose ce clcnn character level convolutional neural network use character encoder tackle problems propose ce clcnn end end learn model image base character encoder ie ce clcnn handle character target document image various experiment find confirm ce clcnn capture closely embed feature visually semantically similar character achieve state art result several open document classification task paper report performance ce clcnn wikipedia title estimation task analyse internal behaviour
simple reference game central theoretical empirical importance study situate language use although language provide rich compositional truth conditional semantics facilitate reference speakers listeners may sometimes lack overall lexical cognitive resources guarantee successful reference mean alone however language also rich associational structure serve resource achieve successful reference investigate use associational information set associational information available simplify version popular game codenames use optimal experiment design techniques compare range model vary type associative information deploy level pragmatic sophistication human behavior set find listeners behavior reflect direct bigram collocational associations strongly word embed semantic knowledge graph base associations little evidence pragmatically sophisticate behavior either speakers listeners type might predict recursive reason model rational speech act theory result would light nature lexical resources speakers listeners bring bear achieve reference associative mean alone
semantic role theory consider roles small universal set unanalyzed entities mean formally restrictions role combinations argue semantic roles co occur verb representations mean hide restrictions role combinations demonstrate practical evidence base approach build depth analysis largest verb database verbnet consequences approach consider
semantic role theory widely use approach event representation yet multiple indications semantic role paradigm necessary sufficient cover elements event structure conduct analysis semantic role representation events provide empirical evidence insufficiency consequence hybrid role scalar approach result consider preliminary investigation semantic roles coverage event representation
work investigate alternative model neural machine translation nmt propose novel architecture employ multi dimensional long short term memory mdlstm translation model state art methods source target sentence treat one dimensional sequence time view translation two dimensional 2d map use mdlstm layer define correspondence source target word extend beyond current sequence sequence backbone nmt model 2d structure source target sentence align 2d grid propose topology show consistent improvements attention base sequence sequence model two wmt two thousand and seventeen task germanleftrightarrowenglish
morphological declension aim inflect nouns indicate number case gender important task natural language process nlp research proposal seek address degree recurrent neural network rnns efficient learn decline noun case give challenge data sparsity process morphologically rich languages also flexibility sentence structure languages believe model morphological dependencies improve performance neural network model suggest carry various experiment understand interpretable feature may lead better generalization learn model cross lingual task
address fine grain multilingual language identification provide language code every token sentence include codemixed text contain multiple languages text prevalent online document social media message board show fee forward network simple globally constrain decoder accurately rapidly label codemixed monolingual text one hundred languages one hundred language pair model outperform previously publish multilingual approach term accuracy speed yield 800x speed one hundred and ninety-five average absolute gain three codemixed datasets furthermore outperform several benchmark systems monolingual language identification
paper present neural network classifier approach detect within cross document event coreference effectively use event mention base feature approach yet rely event argument feature semantic roles spatiotemporal arguments experimental result ecb dataset show approach produce f1 score significantly outperform state art methods within document cross document event coreference resolution use b3 ceafe evaluation measure get worse f1 score muc measure however use conll measure average three score approach slightly better f1 within document event coreference resolution significantly better cross document event coreference resolution
european libraries archive fill encipher manuscripts early modern period include military diplomatic correspondence record secret societies private letter although encipher classical cryptographic algorithms content unavailable work historians therefore attack problem automatically convert cipher manuscript image plaintext develop unsupervised model character segmentation character image cluster decipherment cluster sequence experiment pipelined joint model give empirical result multiple cipher
text simplification ts view monolingual translation task translate text variations within single language recent neural ts model draw insights neural machine translation learn lexical simplification content reduction use encoder decoder model different neural machine translation obtain enough ordinary simplify sentence pair ts expensive time consume build target side simplify sentence play important role boost fluency statistical ts investigate use simplify sentence train change network architecture propose pair simple train sentence synthetic ordinary sentence via back translation treat synthetic data additional train data train encoder decoder model use synthetic sentence pair original sentence pair obtain substantial improvements available wikilarge data wikismall data compare state art methods
sanskrit grammatical tradition commence panini astadhyayi mostly padasastra culminate vakyasastra hand bhartrhari grammarian philosopher bhartrhari authoritative work vakyapadiya matter study modern scholars least fifty years since ashok aklujkar submit phd dissertation harvard university notions sentence word meaningful linguistic unit language subject matter discussion many work follow later scholars apply philological techniques critically establish text work bhartrhari others devote explore philosophical insights others study work point view modern linguistics psychology others try justify view logical discussions paper present fresh view study bhartrhari work especially vakyapadiya view field natural language process nlp specifically call cognitive nlp study definitions sentence give bhartrhari begin second chapter vakyapadiya research one definitions conduct experiment follow methodology silent read sanskrit paragraph collect gaze behavior data participants analyze understand underlie comprehension procedure human mind present result evaluate statistical significance result use test discuss caveats work also present general remark experiment usefulness method gain insights work bhartrhari
intention identification core issue dialog management however due non canonicality speak language difficult extract content automatically conversation style utterances much challenge languages like korean japanese since agglutination morphemes make difficult machine parse sentence understand intention suggest guideline problem merge issue flexibly neural paraphrase systems introduce recently propose structure annotation scheme korean question command result corpus widely applicable field argument mine scheme dataset expect help machine understand intention natural language grasp core mean conversation style instructions
speech emotion recognition challenge task extensive reliance place model use audio feature build well perform classifiers paper propose novel deep dual recurrent encoder model utilize text data audio signal simultaneously obtain better understand speech data emotional dialogue compose sound speak content model encode information audio text sequence use dual recurrent neural network rnns combine information source predict emotion class architecture analyze speech data signal level language level thus utilize information within data comprehensively model focus audio feature extensive experiment conduct investigate efficacy properties propose model propose model outperform previous state art methods assign data one four emotion categories ie angry happy sad neutral model apply iemocap dataset reflect accuracies range six hundred and eighty-eight seven hundred and eighteen
introduce new language representation model call bert stand bidirectional encoder representations transformers unlike recent language representation model bert design pre train deep bidirectional representations unlabeled text jointly condition leave right context layer result pre train bert model fine tune one additional output layer create state art model wide range task question answer language inference without substantial task specific architecture modifications bert conceptually simple empirically powerful obtain new state art result eleven natural language process task include push glue score eight hundred and five seventy-seven point absolute improvement multinli accuracy eight hundred and sixty-seven forty-six absolute improvement squad v11 question answer test f1 nine hundred and thirty-two fifteen point absolute improvement squad v20 test f1 eight hundred and thirty-one fifty-one point absolute improvement
predict reader rat text quality challenge task involve estimate different subjective aspects text like structure clarity etc subjective aspects better handle use cognitive information one source cognitive information gaze behaviour paper show gaze behaviour indeed help effectively predict rat text quality first model text quality function three properties organization coherence cohesion demonstrate capture gaze behaviour help predict properties hence overall quality report improvements obtain add gaze feature traditional textual feature score prediction also hypothesize reader fully understand text correspond gaze behaviour would give better indication assign rat oppose partial understand experiment validate hypothesis show greater agreement give rat predict rat reader full understand text
present comparison word base character base sequence sequence model data text natural language generation generate natural language descriptions structure input datasets two recent generation challenge model achieve comparable better automatic evaluation result best challenge submissions subsequent detail statistical human analyse would light differences two input representations diversity generate texts control experiment synthetic train data generate templates demonstrate ability neural model learn novel combinations templates thereby generalize beyond linguistic structure train
surprise property word vectors word analogies often solve vector arithmetic however unclear arithmetic operators correspond non linear embed model skip gram negative sample sgns provide formal explanation phenomenon without make strong assumptions past theories make vector space word distribution theory several implications past work conjecture linear substructures exist vector space relations represent ratios prove hold sgns provide novel justification addition sgns word vectors show automatically weight frequent word weight scheme ad hoc lastly offer information theoretic interpretation euclidean distance vector space justify use capture word dissimilarity
current measure evaluate text simplification systems focus evaluate lexical text aspects neglect structural aspects paper propose first measure address structural aspects text simplification call samsa leverage recent advance semantic parse assess simplification quality decompose input base semantic structure compare output samsa provide reference less automatic evaluation procedure avoid problems reference base methods face due vast space valid simplifications give sentence human evaluation experiment show samsa substantial correlation human judgments well deficiency exist reference base measure evaluate structural simplification
sentence split major simplification operator present simple efficient split algorithm base automatic semantic parser split text amenable fine tune simplification operations particular show neural machine translation effectively use situation previous application machine translation simplification suffer considerable disadvantage conservative often fail modify source way split base semantic parse propose alleviate issue extensive automatic human evaluation show propose method compare favorably state art combine lexical structural simplification
coreference resolution important task natural language understand resolution ambiguous pronouns longstanding challenge nonetheless exist corpora capture ambiguous pronouns sufficient volume diversity accurately indicate practical utility model furthermore find gender bias exist corpora systems favor masculine entities address present release gap gender balance label corpus eight thousand, nine hundred and eight ambiguous pronoun name pair sample provide diverse coverage challenge pose real world text explore range baselines demonstrate complexity challenge best achieve six hundred and sixty-nine f1 show syntactic structure continuous neural model provide promise complementary cue approach challenge
knowledge graphkg compose entities descriptions attribute relationship entities find application scenarios various natural language process task typical knowledge graph like wikidata entities usually large number attribute difficult know ones important importance attribute valuable piece information various applications span information retrieval natural language generation paper propose general method use external user generate text data evaluate relative importance entity attribute specific use word sub word embed techniques match external textual data back entities attribute name value rank attribute match cohesiveness best knowledge first work apply vector base semantic match important attribute identification method outperform previous traditional methods also apply outcome detect important attribute language generation task compare previous generate text new method generate much customize informative message
automatic text summarization generally consider challenge task nlp community one challenge publicly available large dataset relatively rare difficult construct problem even worse low resource languages indonesian paper present indosum new benchmark dataset indonesian text summarization dataset consist news article manually construct summaries notably dataset almost 200x larger previous indonesian summarization dataset domain evaluate various extractive summarization approach obtain encourage result demonstrate usefulness dataset provide baselines future research code dataset available online permissive license
propose neural machine read model construct dynamic knowledge graph procedural text build graph recurrently step describe procedure use track evolve state participant entities harness extend recently propose machine read comprehension mrc model query entity state since state generally communicate span text mrc model perform well extract entity centric span explicit structure evolve knowledge graph representations model construct use downstream question answer task improve machine comprehension text demonstrate empirically two comprehension task recently propose propara dataset dalvi et al two thousand and eighteen model achieve state art result show model competitive recipes dataset kiddon et al two thousand and fifteen suggest may generally applicable present evidence model knowledge graph help impose commonsense constraints predictions
abstractive summarization study use neural sequence transduction methods datasets large pair document summary examples however datasets rare model train generalize domains recently progress make learn sequence sequence mappings unpaired examples work consider set document product business review summaries provide propose end end neural model architecture perform unsupervised abstractive summarization propose model consist auto encoder mean representations input review decode reasonable summary review rely review specific feature consider variants propose architecture perform ablation study show importance specific components show automate metrics human evaluation generate summaries highly abstractive fluent relevant representative average sentiment input review finally collect reference evaluation dataset show model outperform strong extractive baseline
current lexical simplification approach rely heavily heuristics corpus level feature always align human judgment create human rat word complexity lexicon fifteen thousand english word propose novel neural readability rank model gaussian base feature vectorization layer utilize human rat measure complexity give word phrase model perform better state art systems different lexical simplification task evaluation datasets additionally also produce simpleppdb lexical resource ten million simplify paraphrase rule apply model paraphrase database ppdb
current success deep neural network dnns increasingly broad range task involve artificial intelligence strongly depend quality quantity label train data general scarcity label data often observe many natural language process task one important issue address semi supervise learn ssl promise approach overcome issue incorporate large amount unlabeled data paper propose novel scalable method ssl text classification task unique property method mixture expert imitator network imitator network learn imitate estimate label distribution expert network unlabeled data potentially contribute set feature classification experiment demonstrate propose method consistently improve performance several type baseline dnns also demonstrate method data better performance property promise scalability amount unlabeled data
probabilistic topic model popular choice first step crosslingual task enable knowledge transfer extract multilingual feature many multilingual topic model develop assumptions train corpus quite vary clear well model apply various train condition paper systematically study knowledge transfer mechanisms behind different multilingual topic model broad set experiment four model ten languages provide empirical insights inform selection future development multilingual topic model
bleu widely consider informative metric text text generation include text simplification ts ts include lexical structural aspects paper show bleu suitable evaluation sentence split major structural simplification operation manually compile sentence split gold standard corpus contain multiple structural paraphrase perform correlation analysis human judgments find low correlation bleu grammaticality mean preservation parameters sentence split involve moreover bleu often negatively correlate simplicity essentially penalize simpler sentence
sequence sequence seq2seq neural model actively investigate abstractive summarization nevertheless exist neural abstractive systems frequently generate factually incorrect summaries vulnerable adversarial information suggest crucial lack semantic understand paper propose novel semantic aware neural abstractive summarization model learn generate high quality summaries semantic interpretation salient content novel evaluation scheme adversarial sample introduce measure well model identify topic information model yield significantly better performance popular pointer generator summarizer human evaluation also confirm system summaries uniformly informative faithful well less redundant seq2seq model
pronouns frequently omit pro drop languages chinese generally lead significant challenge respect production complete translations recently wang et al two thousand and eighteen propose novel reconstruction base approach alleviate drop pronoun dp translation problems neural machine translation model work improve original model two perspectives first employ share reconstructor better exploit encoder decoder representations second jointly learn translate predict dps end end manner avoid errors propagate external dp prediction model experimental result show approach significantly improve translation performance dp prediction accuracy
paper describe umons solution multimodal machine translation task present third conference machine translation wmt18 explore novel architecture call deepgru base recent find relate task neural image caption nic model present follow section lead best meteor translation score constrain english image german english image french sub task
neural image caption nic neural caption generation attract lot attention last years describe image natural language emerge challenge field computer vision language process therefore lot research focus drive task forward new creative ideas far goal maximize score automate metric one come plurality new modules techniques add model become complex resource hungry paper take small step backwards order study architecture interest trade performance computational complexity tackle every component neural caption model propose one solution lighten model overall ideas inspire two relate task multimodal monomodal neural machine translation
universal language representation holy grail machine translation mt thank new neural mt approach seem good perspectives towards goal paper propose new architecture base combine variational autoencoders encoder decoders introduce interlingual loss additional train objective add force interlingual loss able train multiple encoders decoders language share common universal representation since final objective universal representation produce close result similar input sentence language propose evaluate encode sentence two different languages decode latent representations language compare output preliminary result wmt two thousand and seventeen turkish english task show propose architecture capable learn universal language representation simultaneously train translation directions state art result
recent research efforts show neural architectures effective conventional information extraction task name entity recognition yield state art result standard newswire datasets however despite significant resources require train model performance model train one domain typically degrade dramatically apply different domain yet extract entities new emerge domains social media significant interest paper empirically investigate effective methods conveniently adapt exist well train neural ner model new domain unlike exist approach propose lightweight yet effective methods perform domain adaptation neural model specifically introduce adaptation layer top exist neural architectures train use source domain data require conduct extensive empirical study show approach significantly outperform state art methods
word create equal fact form aristocratic graph latent hierarchical structure next generation unsupervised learn word embeddings reveal paper justify notion delta hyperbolicity tree likeliness space propose embed word cartesian product hyperbolic space theoretically connect gaussian word embeddings fisher geometry connection allow us introduce novel principled hypernymy score word embeddings moreover adapt well know glove algorithm learn unsupervised word embeddings type riemannian manifold explain solve analogy task use riemannian parallel transport generalize vector arithmetics new type geometry empirically base extensive experiment prove embeddings train unsupervised first simultaneously outperform strong popular baselines task similarity analogy hypernymy detection particular word hypernymy obtain new state art fully unsupervised wbless classification accuracy
diacritization process attempt restore short vowels arabic write text typically omit process essential applications text speech tts diacritization modern standard arabic msa still hold lion share research dialectal arabic da diacritization limit paper present contribution result automatic diacritization two sub dialects maghrebi arabic namely tunisian moroccan use character level deep neural network architecture stack two bi lstm layer crf output layer model achieve word error rate twenty-seven thirty-six moroccan tunisian respectively capable implicitly identify sub dialect input
automate approach text readability assessment essential language powerful tool improve understandability texts write publish language however persian language speak one hundred and ten million speakers lack system unlike languages english french chinese limit research study carry build accurate reliable text readability assessment system persian language present research first persian dataset text readability assessment gather first model persian text readability assessment use machine learn introduce experiment show model accurate could assess readability persian texts high degree confidence result study use number applications medical educational text readability evaluation potential cornerstone future study persian text readability assessment
word embeddings undoubtedly useful components many nlp task paper present word embeddings linguistic resources train largest date digital greek language corpus also present live web tool test greek word embeddings offer analogy similarity score similar word function explorer one could interact greek word vectors
neural machine translation nmt notoriously sensitive noise noise almost inevitable practice one special kind noise homophone noise word replace word similar pronunciations propose improve robustness nmt homophone noise one jointly embed textual phonetic information source sentence two augment train dataset homophone noise interestingly achieve better translation quality robustness find though weight put phonetic rather textual information experiment show method significantly improve robustness nmt homophone noise also surprisingly improve translation quality clean test set
universal dependencies ud universal morphology unimorph project present schemata annotate morphosyntactic detail language project also provide corpora annotate text many languages ud token level unimorph type level corpus build different annotators language specific decisions hinder goal universal schemata compatibility tag project annotations could use validate additionally availability type token level resources would boon task parse homograph disambiguation ease interoperability present deterministic map universal dependencies v2 feature unimorph schema validate approach lookup unimorph corpora find macro average six thousand, four hundred and thirteen recall also note incompatibilities due paucity data either side finally present critical evaluation foundations strengths weaknesses two annotation project
multi source translation systems translate multiple languages single target language use information multiple source systems achieve large gain accuracy train systems necessary corpora parallel text multiple source target language however corpora rarely complete practice due difficulty provide human translations relevant languages paper propose data augmentation approach fill incomplete part use multi source neural machine translation nmt experiment result vary different language combinations significant gain observe use source language similar target language
article describe application machine learn ml linguistic model generate persian poems fact teach machine read learn persian poems generate fake poems style original poems two well know poets use hafez one thousand, three hundred and ten one thousand, three hundred and ninety saadi one thousand, two hundred and ten one thousand, two hundred and ninety-two poems first fee machine hafez poems generate fake poems style fee machine hafez saadi poems generate new style poems combination two poets style emotional hafez rational saadi elements idea combination different style ml open new gate extend treasure past literature different culture result show enough memory process power time possible generate reasonable good poems
develop neural morphological tag disambiguation model estonian first experiment two neural architectures morphological tag standard multiclass classifier treat morphological tag single unit sequence model handle morphological tag sequence morphological category value secondly complement model analyse generate rule base estonian morphological analyser vabamorf thus perform soft morphological disambiguation compare two ways supplement neural morphological tagger output firstly add combine analyse embeddings word representation input neural tag model secondly adopt attention mechanism focus relevant analyse generate experiment three estonian datasets show neural architectures consistently outperform non neural baselines include hmm disambiguate vabamorf augment model output result performance boost model
advent representation learn methods enable large performance gain various language task alleviate need manual feature engineer engineer representations usually base linguistic understand therefore interpretable learn representations harder interpret empirically study complementarity approach provide linguistic insights would help reach better compromise interpretability performance present infodens framework study learn engineer representations text context text classification task design simplify task feature engineer well provide groundwork extract learn feature combine approach infodens flexible extensible short learn curve easy integrate many available widely use natural language process tool
conll sigmorphon two thousand and eighteen share task supervise learn morphological generation feature data set one hundred and three typologically diverse languages apart extend number languages involve earlier supervise task generate inflect form year share task also feature new second task ask participants inflect word sentential context similar cloze task second task feature seven languages task one receive twenty-seven submissions task two receive six submissions task feature low medium high data condition nearly submissions feature neural component build highly rank systems earlier two thousand and seventeen share task inflection task task one forty-one fifty-two languages present last year inflection task show improvement best systems low resource set cloze task task two prove difficult submissions manage consistently improve upon simple neural baseline system lemma repeat baseline
paper introduce use semantic hash embed task intent classification achieve state art performance three frequently use benchmarks intent classification small dataset challenge task data hungry state art deep learn base systems semantic hash attempt overcome challenge learn robust text classification current word embed base dependent vocabularies one major drawbacks methods vocabulary term especially small train datasets use wider vocabulary case intent classification chatbots typically small datasets extract internet communication two problems arise use internet communication first datasets miss lot term vocabulary use word embeddings efficiently second users frequently make spell errors typically model intent classification train spell errors difficult think ways users make mistake model depend word vocabulary always face issue ideal classifier handle spell errors inherently semantic hash overcome challenge achieve state art result three datasets askubuntu chatbot web application benchmarks available online https githubcom kumar shridhar know intent
recent years substantial work do language tag code mix data use large amount data build model article present three strategies build word level language tagger code mix data use low resources secure accuracy higher baseline model best perform system get accuracy around ninety-one combine ensemble system achieve accuracy around nine hundred and twenty-six
machine read comprehension mrc become enormously popular recently attract lot attention however exist read comprehension datasets mostly english paper introduce span extraction dataset chinese machine read comprehension add language diversities area dataset compose near twenty thousand real question annotate wikipedia paragraph human experts also annotate challenge set contain question need comprehensive understand multi sentence inference throughout context present several baseline systems well anonymous submissions demonstrate difficulties dataset release dataset host second evaluation workshop chinese machine read comprehension cmrc two thousand and eighteen hope release dataset could accelerate chinese machine read comprehension research resources available https githubcom ymcui cmrc2018
sequence sequence seq2seq model often lack diversity generate translations attribute limitation seq2seq model capture lexical syntactic variations parallel corpus result different style genres topics ambiguity translation process paper develop novel sequence sequence mixture s2smix model improve translation diversity quality adopt committee specialize translation model rather single translation model mixture component select train dataset via optimization marginal loglikelihood lead soft cluster parallel corpus experiment four language pair demonstrate superiority mixture model compare seq2seq baseline standard diversity boost beam search mixture model use negligible additional parameters incur extra computation cost decode
spite recent success dialogue act da classification majority prior work focus text base classification oracle transcriptions ie human transcriptions instead automatic speech recognition asr transcriptions speak dialog systems however agent would access noisy asr transcriptions may suffer performance degradation due domain shift paper explore effectiveness use acoustic textual signal either oracle asr transcriptions investigate speaker domain adaptation da classification multimodal model prove superior unimodal model particularly oracle transcriptions available also propose effective method speaker domain adaptation achieve competitive result
recent work show encoder decoder attention mechanisms neural machine translation nmt different word alignment statistical machine translation paper focus analyze encoder decoder attention mechanisms case word sense disambiguation wsd nmt model hypothesize attention mechanisms pay attention context tokens translate ambiguous word explore attention distribution pattern translate ambiguous nouns counter intuitively find attention mechanisms likely distribute attention ambiguous noun rather context tokens comparison nouns conclude attention mechanism main mechanism use nmt model incorporate contextual information wsd experimental result suggest nmt model learn encode contextual information necessary wsd encoder hide state attention mechanism transformer model reveal first layer gradually learn align source target tokens last layer learn extract feature relate unaligned context tokens
propose method name super character sentiment classification method convert sentiment classification problem image classification problem project texts image apply cnn model classification text feature extract automatically generate super character image hence need explicit step embed word character numerical vector representations experimental result large social media corpus show super character method consistently outperform methods sentiment classification topic classification task ten large social media datasets millions content four different languages include chinese japanese korean english
task orient dialog systems typically first parse user utterances semantic frame comprise intents slot previous work task orient intent slot fill work restrict one intent per query one slot label per token thus model complex compositional request alternative semantic parse systems represent query logical form challenge annotate parse propose hierarchical annotation scheme semantic parse allow representation compositional query efficiently accurately parse standard constituency parse model release dataset 44k annotate query fbme semanticparsingdialog show parse model outperform sequence sequence approach dataset
timeline summarization tls create overview long run events via date daily summaries important date tls differ standard multi document summarization mds importance date selection interdependencies summaries different date short summaries compare number corpus document however show mds optimization model use submodular function adapt yield well perform tls model design objective function constraints model temporal dimension inherent tls importantly adaptations retain elegance advantage original mds model clear separation feature inference performance guarantee scalability little need supervision current tls specific model lack open source implementation framework model describe paper available online
task answer question give text passage show great developments model performance thank community efforts build useful datasets recently doubt whether rapid progress base truly understand language question ask table question answer tableqa task task answer query give table show exist efforts use answer evaluation supervision tableqa show deteriorate performances adversarial settings perturbations affect answer insight naturally motivate develop new model understand question table precisely goal propose neural operator neop multi layer sequential network attention supervision answer query give table neop use multiple selective recurrent units selrus help interpretability answer model experiment show use operand information train model significantly improve performance interpretability tableqa model neop outperform previous model big margin
accurate prediction conversation topics valuable signal create coherent engage dialog systems work focus context aware topic classification methods identify topics free form human chatbot dialogs extend previous work neural topic classification unsupervised topic keyword detection incorporate conversational context dialog act feature annotate data show incorporate context dialog act lead relative gain topic classification accuracy thirty-five unsupervised keyword detection recall eleven conversational interactions topics frequently span multiple utterances show topical metrics topical depth highly correlate dialog evaluation metrics coherence engagement imply conversational topic model predict user satisfaction work detect conversation topics keywords use guide chatbots towards coherent dialog
propose simple unsupervised method extract pseudo parallel monolingual sentence pair comparable corpora representative two different text style news article scientific paper approach require seed parallel corpus instead rely solely hierarchical search pre train embeddings document sentence demonstrate effectiveness method automatic extrinsic evaluation text simplification normal simple wikipedia show pseudo parallel sentence extract method supplement exist parallel data even lead competitive performance
currently biaffine classifier attract attention method introduce attention mechanism model binary relations instance field dependency parse deep biaffine parser dozat man achieve state art performance graph base dependency parser english penn treebank conll two thousand and seventeen share task hand report parameter redundancy weight matrix biaffine classifiers ofn2 parameters result overfitting n number dimension paper attempt reduce parameter redundancy assume either symmetry circularity weight matrices experiment conll two thousand and seventeen share task dataset model achieve better comparable accuracy treebanks sixteen parameter reduction
large parallel corpora automatically obtain web document elsewhere often exhibit many corrupt part bind negatively affect quality systems model learn corpora paper describe frequent problems find data data affect neural machine translation systems well identify deal solutions summarise set script remove problematic sentence input corpora
simultaneous translation translate sentence finish useful many scenarios notoriously difficult due word order differences conventional seq seq framework suitable full sentence translation propose novel prefix prefix framework simultaneous translation implicitly learn anticipate single translation model within framework present simple yet surprisingly effective wait k policy train generate target sentence concurrently source sentence always k word behind experiment show strategy achieve low latency reasonable quality compare full sentence translation four directions zh en de en
name entity recognition ner focus extraction semantically meaningful name entities semantic class text serve indispensable component several stream natural language process nlp task relation extraction event extraction dependency tree hand also convey crucial semantic level information show previously information use improve performance ner sasano kurohashi two thousand and eight ling weld two thousand and twelve work investigate better utilize structure information convey dependency tree improve performance ner specifically unlike exist approach exploit dependency information design local feature show certain global structure information dependency tree exploit build ner model information provide guide learn inference extensive experiment show propose novel dependency guide ner model perform competitively model base conventional semi markov conditional random field require significantly less run time
paper introduce new annotate corpus base exist informal text corpus nus sms corpus chen kan two thousand and thirteen new corpus include seventy-six thousand, four hundred and ninety noun phrase twenty-six thousand, five hundred sms message annotate university students explore several graphical model include novel variant semi markov conditional random field semi crf task noun phrase chunk demonstrate empirical evaluations new dataset new variant yield similar accuracy run significantly lower run time compare conventional semi crf
paper focus study recognize discontiguous entities motivate previous work propose use novel hypergraph representation jointly encode discontiguous entities unbounded length overlap one another compare exist approach first formally introduce notion model ambiguity define difficulty level interpret output model formally analyze theoretical advantage model previous exist approach base linear chain crfs empirical result also show model able achieve significantly better result evaluate standard data many discontiguous entities
wide range applications play important role daily work modern human translator however computational tool design aid process translation benefit translation small minority seven thousand languages world may call privilege languages translators work remain languages marginalize languages digital world benefit tool speed production translation privilege languages may ask whether possible bridge gap available languages marginalize ones paper propose framework computer assist translation marginalize languages implementation web application spanish guarani translation propose system base new theory phrase level translation contexts adequate bilingual corpora available translation generalize segment refer minimal dependency translation previous work
neural machine translation nmt become standard translate use subword units allow open vocabulary improve accuracy infrequent word byte pair encode bpe variants predominant approach generate subwords unsupervised resource free empirically effective however granularity subword units hyperparameter tune language task use methods grid search tune may do inexhaustively skip entirely due resource constraints lead sub optimal performance paper propose method automatically tune parameter use one train pass incrementally introduce new vocabulary online base hold validation loss begin smaller general subwords add larger specific units course train method match result find grid search optimize segmentation granularity without additional train time also show benefit train efficiency performance improvements rare word due way embeddings larger units incrementally construct combine smaller units
work tackle problem armenian name entity recognition provide silver gold standard datasets well establish baseline result popular model present one hundred and sixty-three thousand token name entity corpus automatically generate annotate wikipedia another fifty-three thousand, four hundred token corpus news sentence manual annotation people organization location name entities corpora use train evaluate several popular name entity recognition model alongside datasets release fifty one hundred two hundred three hundred dimensional glove word embeddings train collection armenian texts wikipedia news blog encyclopedia
ability infer persona dialogue applications areas range computational narrative analysis personalize dialogue generation introduce neural model learn persona embeddings supervise character trope classification task model encode dialogue snippets imdb representations capture various categories film character best perform model use multi level attention mechanism set utterances also utilize prior knowledge form textual descriptions different tropes apply learn embeddings find similar character across different movies cluster movies accord distribution embeddings use short conversational text input ability learn prior knowledge use memory suggest methods could apply domains
measure semantic similarity two sentence semantic textual similarity sts fundamental many nlp applications despite remarkable result supervise settings adequate label little attention pay task low resource languages insufficient label exist approach mostly leverage machine translation techniques translate sentence rich resource language approach either beget language bias impractical industrial applications speak language scenario often rigorous efficiency require work propose multilingual framework tackle sts task low resource language eg spanish arabic indonesian thai utilize rich annotation data rich resource language eg english approach extend basic monolingual sts framework share multilingual encoder pretrained translation task incorporate rich resource language data exploit nature share multilingual encoder one sentence multiple representations different target translation language use ensemble model improve similarity evaluation demonstrate superiority method state art approach semeval sts task significant improvement non mt method well online industrial product mt method fail beat baseline approach still consistently improvements
neural morphological tag regard extension pos tag task treat morphological tag monolithic label ignore internal structure propose view morphological tag composite label explicitly model internal structure neural sequence tagger explore three different neural architectures compare performance crf simple neural multiclass baselines evaluate model forty-nine languages show neural architecture model morphological label sequence morphological category value perform significantly better baselines establish state art result morphological tag languages
world proliferate data ability rapidly summarize text grow importance automatic summarization text think sequence sequence problem another area natural language process solve sequence sequence problem machine translation rapidly evolve due development attention base encoder decoder network work apply modern techniques abstractive summarization perform analysis various attention mechanisms summarization goal develop approach architecture aim improve state art particular modify optimize translation model self attention generate abstractive sentence summaries effectiveness base model along attention variants compare analyze context standardize evaluation set test metrics however show metrics limit ability effectively score abstractive summaries propose new approach base intuition abstractive model require abstractive evaluation
reason imply relationships eg paraphrastic common sense encyclopedic pair word crucial many cross sentence inference problems paper propose new methods learn use embeddings word pair implicitly represent background knowledge relationships pairwise embeddings compute compositional function word representations learn maximize pointwise mutual information pmi contexts two word co occur add representations cross sentence attention layer exist inference model eg bidaf qa esim nli instead extend replace exist word embeddings experiment show gain twenty-seven recently release squad20 thirteen multinli representations also aid better generalization gain around six seven adversarial squad datasets eighty-eight adversarial entailment test set glockner et al two thousand and eighteen
paper introduce first dataset evaluate english chinese bilingual contextual word similarity namely bcws https githubcom miulab bcws dataset consist two thousand and ninety-one english chinese word pair correspond sentential contexts similarity score annotate human annotate dataset higher consistency compare similar datasets establish several baselines bilingual embed task benchmark experiment model cross lingual sense representations provide dataset potential move artificial intelligence monolingual understand towards multilingual understand
introduce method reduce constituent parse sequence label word wt generate label encode one number ancestors tree word wt wt1 common two nonterminal symbol lowest common ancestor first prove propose encode function injective tree without unary branch practice approach make extensible constituency tree collapse unary branch use ptb ctb treebanks testbeds propose set fast baselines achieve nine hundred and seven f score ptb test set outperform vinyals et al two thousand and fifteen sequence sequence parser addition sacrifice accuracy approach achieve fastest constituent parse speed report date ptb wide margin
explore whether possible build lighter parsers statistically equivalent correspond standard version wide set languages show different structure morphologies testbed use universal dependencies transition base dependency parsers train fee forward network exist research assume de facto standard embed feature rely pre computation trick obtain speed up explore feature size reduce whether translate speed up negligible impact accuracy experiment show grand daughter feature remove majority treebanks without significant negative positive las difference also show size embeddings notably reduce
paper propose new model capable recognize overlap mention introduce novel notion mention separators effectively use capture mention overlap one another top novel multigraph representation introduce show efficient exact inference still perform present theoretical analysis differences model recently propose model recognize overlap mention discuss possible implications differences extensive empirical analysis standard datasets demonstrate effectiveness approach
recognise dialogue act da important many natural language process task dialogue generation intention recognition paper propose dual attention hierarchical recurrent neural network da classification model partially inspire observation conversational utterances normally associate da topic former capture social act latter describe subject matter however dependency das topics utilise exist systems da classification novel dual task specific attention mechanism model able utterances capture information das topics well information interactions experimental result show model topic auxiliary task model significantly improve da classification yield better comparable performance state art method three public datasets
tackle acned compare entities short sentence wikidata graph create context vector graph deep learn challenge problem never apply acned main contribution present experimental study recent neural techniques well discussion graph feature important disambiguation task addition new dataset wikidatadisamb create allow clean scalable evaluation acned wikidata entries use reference future research end result show acbi lstm encode graph triplets perform best improve upon baseline model score rmf1 value nine hundred and sixteen wikidatadisamb test set
schizophrenia one disable difficult treat human medical health condition rank top ten cause disability worldwide puzzle part due difficulty identify basic fundamental components several study show manifestations schizophrenia eg negative symptoms include blunt speech prosody well disorganization symptoms lead disorder language understand perspective linguistics however schizophrenia research keep pace technologies computational linguistics especially semantics pragmatics examine write schizophrenia patients analyze syntax semantics pragmatics addition analyze tweet self pro claim schizophrenia patients publicly discuss diagnose write sample dataset syntactic feature find successful classification whereas less structure twitter dataset combination feature perform best
work investigate legal concepts expression portuguese concentrate order attorneys brazil bar exam use corpus form collection multiple choice question three norms relate ethics part oab exam language resources princeton wordnet openwordnet pt tool antconc freeling begin investigate concepts word miss repertory concepts word portuguese knowledge base openwordnet pt add concepts word openwordnet pt hence obtain representation texts contain lexical knowledge base
recent work identify study small cohort twitter users whose pregnancies birth defect outcomes could observe via publicly available tweet exploit social media large scale potential complement limit methods study birth defect lead infant mortality depend development automatic methods primary objective study take first step towards scale use social media observe pregnancies birth defect outcomes namely develop methods automatically detect tweet users report birth defect outcomes annotate pre process approximately twenty-three thousand tweet mention birth defect order train evaluate supervise machine learn algorithms include feature engineer deep learn base classifiers also experiment various sample sample approach address class imbalance support vector machine svm classifier train original imbalanced data set n grams word cluster structural feature achieve best baseline performance positive class f1 score sixty-five defect class fifty-one possible defect class contributions include natural language process nlp supervise machine learn methods automatically detect tweet users report birth defect outcomes ii comparison feature engineer deep learn base classifiers train imbalanced sample sample data iii error analysis could inform classification improvements use publicly available corpus future work focus automate user level analyse cohort inclusion
dialogue state track core part speak dialogue system estimate beliefs possible user goals every dialogue turn however current approach difficult scale large dialogue domains one follow limitations model work situation slot value ontology change dynamically b number model parameters proportional number slot c model extract feature base hand craft lexicons tackle challenge propose statenet universal dialogue state tracker independent number value share parameters across slot use pre train word vectors instead explicit semantic dictionaries experiment two datasets show approach overcome limitations also significantly outperform performance state art approach
task linearization find grammatical order give set word traditional model use statistical methods syntactic linearization systems generate sentence along syntactic tree show state art performance recent work show multi layer lstm language model outperform competitive statistical syntactic linearization systems without use syntax paper study neural syntactic linearization build transition base syntactic linearizer leverage fee forward neural network observe significantly better result compare lstm language model task
fame project aim develop automatic speech recognition asr system frisian dutch code switch cs speech extract archive local broadcaster ultimate goal build speak document retrieval system unlike dutch frisian low resourced language limit amount manually annotate speech data paper describe several automatic annotation approach enable use large amount raw bilingual broadcast data acoustic model train semi supervise set previously show best perform asr system obtain two stage multilingual deep neural network dnn train use eleven hours manually annotate cs speech reference data together speech data high resourced languages compare quality transcriptions provide bilingual asr system several approach use language recognition system assign language label raw speech segment front end use monolingual asr resources transcription investigate automatic annotation speakers appear raw broadcast data first label pseudo speaker tag use speaker diarization system link know speakers appear reference data use speaker recognition system speaker label essential speaker adaptive train propose set train acoustic model use manually automatically annotate data run recognition experiment development test data fame speech corpus quantify quality automatic annotations asr cs detection result demonstrate potential use automatic language speaker tag semi supervise bilingual acoustic model train
neural network model successful natural language inference best model reach ninety accuracy benchmarks however success model turn largely benchmark specific show model train natural language inference dataset draw one benchmark fail perform well others even notion inference assume benchmarks similar train six high perform neural network model different datasets show one problems generalize replace original test set test set take another corpus design task light result argue current neural network model able generalize well task natural language inference find use large pre train language model help transfer learn datasets similar enough result also highlight current nli datasets cover different nuances inference extensively enough
multiword expressions mwes exhibit regular idiosyncratic properties idiosyncrasy require lexical encode parallel component word time intricate regularity hand call mean flexible factorization avoid redundant descriptions share properties however far non redundant general purpose lexical encode mwes receive satisfactory solution offer proof concept challenge might effectively address within extensible metagrammar xmg object orient metagrammar framework first make exist metagrammatical resource frenchtag grammar mwe aware evaluate factorization gain incremental implementation xmg dataset extract mwe annotate reference corpus
previous work neural text generation graph structure data rely standard sequence sequence methods approach linearise input graph feed recurrent neural network paper propose alternative encoder base graph convolutional network directly exploit input structure report result two graph sequence datasets empirically show benefit explicitly encode input graph structure
show zipf law use scale language model lm take advantage train data gpus lm play key role many important natural language applications speech recognition machine translation scale lm important since widely accept community data like data eventually would like train terabytes tbs text trillions word modern train methods far goal various bottleneck especially memory within gpus communication across gpus paper show zipf law address bottleneck group parameters common word character sequence n number unique word type n size train set tokens local batch size k g gpus dimension embed matrix reduce original per gpu memory communication asymptotic complexity thetagkd thetagk ud empirically find propto gk064 four publicly available large datasets scale number gpus sixty-four factor eight train time speed factor 67times character lms 63times word lms negligible loss accuracy weak scale one hundred and ninety-two gpus tieba dataset show thirty-five improvement lm prediction accuracy train ninety-three gb data 25times larger publicly available sota dataset take 125times increase train time compare three gb dataset run six gpus
build large scale datasets train code switch language model challenge expensive alleviate problem use parallel corpus major workaround however exist solutions use linguistic constraints may capture real data distribution work propose novel method learn generate code switch sentence parallel corpora model use seq2seq model combination pointer network align choose word monolingual sentence form grammatical code switch sentence experiment show train language model use augment sentence improve perplexity score ten compare lstm baseline
previous study show incorporate external information could improve translation quality neural machine translation nmt systems however inevitably noise external information severely reduce benefit exist methods could receive incorporation tackle problem study pay special attention discrimination noise incorporation argue exist two kinds noise external information ie global noise local noise affect translations whole sentence specific word respectively accordingly propose general framework learn jointly discriminate global local noise external information could better leverage model train dataset derive original parallel corpus without external label data annotation experimental result various real world scenarios language pair neural architectures indicate discriminate noise contribute significant improvements translation quality able better incorporate external information even noisy condition
paper describe memad project entry iwslt speech translation share task address translation english audio german text pipeline end end model track participate former three contrastive systems try also latter able finish end end model time systems start transcribe audio text automatic speech recognition asr model train ted lium english speech recognition corpus ted lium afterwards fee transcripts english german text base neural machine translation nmt model systems employ three different translation model train separate train set compile english german part ted speech translation corpus ted trans opensubtitles2018 section opus collection paper also describe experiment lead final systems experiment indicate use opensubtitles2018 train significantly improve translation performance also experiment various pre postprocessing routines nmt module much success best score system attain bleu score one thousand, six hundred and forty-five test set year task
propose new approach natural language understand consider input text image apply 2d convolutional neural network learn local global semantics sentence variations ofthe visual pattern word approach demonstrate possible get semantically meaningful feature image text without use optical character recognition sequential process pipelines techniques traditional natural language understand algorithms require validate approach present result two applications text classification dialog model use 2d convolutional neural network able outperform state art accuracy result non latin alphabet base text classification achieve promise result eight text classification datasets furthermore approach outperform memory network use vocabulary entities fromtask four babi dialog dataset
aspect term sentiment analysis atsa longstanding challenge natural language understand require fine grain semantical reason target entity appear text manual annotation aspects laborious time consume amount label data limit supervise learn paper propose semi supervise method atsa problem use variational autoencoder base transformer vaet model latent distribution via variational inference disentangle latent representation aspect specific sentiment lexical context method induce underlie sentiment prediction unlabeled data benefit atsa classifier method classifier agnostic ie classifier independent module various advance supervise model integrate experimental result obtain semeval two thousand and fourteen task four show method effective four classical classifiers propose method outperform two general semisupervised methods achieve state art performance
automatic extraction clinical concepts essential step turn unstructured data within clinical note structure actionable information work propose clinical concept extraction model automatic annotation clinical problems treatments test clinical note utilize domain specific contextual word embed contextual word embed model first train corpus mixture clinical report relevant wikipedia page clinical domain next bidirectional lstm crf model train clinical concept extraction use contextual word embed model test propose model i2b2 two thousand and ten challenge dataset propose model achieve best performance among report baseline model outperform state art model thirty-four term f1 score
cross language text summarization clts generate summaries language different language source document recent methods use information languages generate summaries informative sentence however methods performance vary accord languages reduce quality summaries paper propose compressive framework generate cross language summaries order analyze performance especially stability test system extractive baselines dataset available four languages english french portuguese spanish generate english french summaries automatic evaluation show method outperform extractive state art clts methods better stable rouge score languages
semantic textual similarity sts basis many applications natural language process nlp system combine convolution recurrent neural network measure semantic similarity sentence use convolution network take account local context word lstm consider global context sentence combination network help preserve relevant information sentence improve calculation similarity sentence model achieve good result competitive best state art systems
recent end end task orient dialog systems use memory architectures incorporate external knowledge dialogs current work make simplify assumptions structure knowledge base use triple represent knowledge combine dialog utterances context well knowledge base kb result part memory cause explosion memory size make reason memory harder addition memory design force hierarchical properties data fit triple structure memory require memory reader infer relationships across otherwise connect attribute paper relax strong assumptions make exist architectures separate memories use model dialog context kb result instead use triple store kb result introduce novel multi level memory architecture consist cells query correspond result multi level memory first address query follow result finally key value pair within result conduct detail experiment three publicly available task orient dialog data set find method conclusively outperform current state art model report fifteen twenty-five increase entity f1 bleu score
text similarity calculation fundamental problem natural language process relate field recent years deep neural network develop perform task high performances achieve neural network usually train label data supervise learn creation label data usually costly short paper address unsupervised learn text similarity calculation propose new method call word embed base edit distance wed incorporate word embed edit distance experiment three benchmark datasets show wed outperform state art unsupervised methods include edit distance tf idf base cosine word embed base cosine jaccard index etc
article present design implementation logoscope first tool especially develop detect new word french language document allow public access web interface semi automatic tool collect new word daily browse online versions french well know newspapers le monde le figaro l equipe lib eration la croix les echo contrast exist tool essentially dedicate dictionary development logoscope attempt give complete account context new word occur addition commonly give morpho syntactic information also provide information textual discursive contexts word creation particular automatically determine journalistic topics text contain new word article first give general overview develop tool describe approach take discuss linguistic background guide design decisions present computational methods use implement
introduce novel dynamic oracles train two accurate know shift reduce algorithms constituent parse top order transition base parsers case dynamic oracles manage notably increase accuracy comparison obtain perform classic static train addition improve performance state art order shift reduce parser achieve best accuracy date nine hundred and twenty f1 obtain fully supervise single model greedy shift reduce constituent parser wsj benchmark
one major downsides deep learn suppose need vast amount train data techniques appear ill suit nlp areas annotate data limit less resourced languages emotion analysis many nuanced hard acquire annotation format conduct questionnaire study indicate indeed vast majority researchers emotion analysis deem neural model inferior traditional machine learn train data limit stark contrast survey result provide empirical evidence english polish portuguese commonly use neural architectures train surprisingly observations outperform n gram base ridge regression one hundred data point analysis suggest high quality pre train word embeddings main factor achieve result
exist entailment datasets mainly pose problems answer without attention grammar word order learn syntax require compare examples different grammar word order change desire classification introduce several datasets base synthetic transformations natural entailment examples snli fever teach aspects grammar word order show without retrain popular entailment model unaware syntactic differences change mean retrain popular entailment model learn compare syntax properly
universal morphology unimorph project collaborative effort improve nlp handle complex morphology across world languages project release annotate morphological data use universal tagset unimorph schema inflect form associate lemma typically carry underlie lexical mean bundle morphological feature schema additional support data tool also release per language basis available unimorph base center language speech process clsp johns hopkins university baltimore maryland sponsor darpa lorelei program paper detail advance make collection annotation dissemination project resources since initial unimorph release describe lrec two thousand and sixteen lexical resources
disentangle conversations mix together single stream message difficult task make harder lack large manually annotate datasets create new dataset seventy-seven thousand, five hundred and sixty-three message manually annotate reply structure graph disentangle conversations define internal conversation structure dataset sixteen time larger previously release datasets combine first include adjudication annotation disagreements first include context use data examine prior work particular find eighty conversations widely use dialogue corpus either miss message contain extra message manually annotate data present opportunity develop robust data drive methods conversation disentanglement help advance dialogue research
vector model language base contextual aspects language distributions word co occur text truth conditional model focus logical aspects language compositional properties word compose form sentence truth conditional approach denotation sentence determine truth condition take truth value set possible worlds context change potential similar vector model degree co occurrence word context determine similar mean word paper put two model together develop vector semantics language base simply type lambda calculus model natural language provide two type vector semantics static one use techniques familiar truth conditional tradition dynamic one base form dynamic interpretation inspire heim context change potentials show dynamic model apply entailment corpus sentence provide examples
people often entities interest task search information extraction task goal find much information possible people specify name however text reference people pronouns generic descriptions professor german chancellor therefore important coreference resolution systems able link different type mention correct person name evaluate two state art coreference resolution systems subtask name person coreference interest identify person mention name along mention person pronoun generic noun phrase analysis reveal standard coreference metrics reflect adequately requirements task penalize systems identify mention name reward systems even systems find correctly mention entity fail link proper name student name introduce new metrics evaluate name person coreference address discrepancies present simple rule base name entity recognition drive system outperform current state art systems task specific metrics perform par traditional coreference evaluations finally present similar evaluation coreference resolution name entities show rule base approach effective person name coreference name entity type
human read behavior sensitive surprisal predictable word tend read faster unexpectedly apply surprisal word currently read also surprisal upcoming successor word fixate yet find interpret evidence readers extract lexical information parafoveally call interpretation question angele et al two thousand and fifteen show successor effect appear even contexts successor word yet visible hypothesize successor surprisal predict read time approximate reader uncertainty upcoming word test hypothesis read time corpus use lstm language model find successor surprisal entropy independent predictors read time independence suggest entropy alone unlikely full explanation successor surprisal effect
present new task suspicious news detection use micro blog text task aim support human experts detect suspicious news article verify costly crucial step verify truthfulness article specifically task give set post sns refer news article goal judge whether article verify task create publicly available dataset japanese provide benchmark result use several basic machine learn techniques experimental result show model reduce cost manual fact check process
despite virtually ubiquitous sequence sequence model challenge lack diversity inability externally control paper speculate fundamental shortcoming sequence generation model decode do strictly leave right mean output value generate earlier profound effect generate later address issue propose novel middle decoder architecture begin initial middle word simultaneously expand sequence directions facilitate information flow maintain consistent decode introduce dual self attention mechanism allow us model complex dependencies output illustrate performance model task video caption well synthetic sequence de noise task middle decoder achieve significant improvements de noise competitive performance task video caption quantifiably improve caption diversity furthermore perform qualitative analysis demonstrate ability effectively control generation process decoder
focus problem language model code switch language context automatic speech recognition asr language model code switch language challenge least three reason one lack available large scale code switch data train two lack replicable evaluation setup asr direct yet isolate language model performance intricacies asr system three reliance generative model tackle three issue propose asr motivate evaluation setup decouple asr system choice vocabulary provide evaluation dataset english spanish code switch setup lend discriminative train approach demonstrate work better generative language model finally explore variety train protocols verify effectiveness train large amount monolingual data follow fine tune small amount code switch data generative discriminative case
work investigate task textual response generation multimodal task orient dialogue system work base recently release multimodal dialogue mmd dataset saha et al two thousand and seventeen fashion domain introduce multimodal extension hierarchical recurrent encoder decoder hred model show extension outperform strong baselines term text base similarity metrics also showcase shortcomings current vision language model perform error analysis system output
dialogue systems conversational agents become increasingly popular modern society build agent capable hold intelligent conversation users challenge problem artificial intelligence demo demonstrate deep learn base conversational social agent call ruuh facebookcom ruuh design team microsoft india converse wide range topics ruuh need think beyond utilitarian notion merely generate relevant responses meet wider range user social need like express happiness user favorite team win share cute comment show picture user pet agent also need detect respond abusive language sensitive topics troll behavior users many problems pose significant research challenge demonstrate demo agent interact two million real world users till date generate one hundred and fifty million user conversations
deep read model question answer demonstrate promise performance last couple years however current systems tend learn cleverly extract span source document base similarity question instead seek appropriate answer indeed read machine able detect relevant passages document regard question importantly able reason important piece document order produce answer require motivate purpose present reviewqa question answer dataset base hotel review question dataset link set relational understand competencies expect model master indeed question come associate type characterize require competency framework possible benchmark main families model get overview strengths weaknesses give model set task evaluate dataset corpus contain five hundred thousand question natural language one hundred thousand hotel review setup projective answer question need extract document like recent datasets select among set candidates contain possible answer question dataset finally present several baselines dataset
exist model open domain comment generation difficult train produce repetitive uninteresting responses problem due multiple contradictory responses single article rigidity retrieval methods solve problem propose combine approach retrieval generation methods propose attentive scorer retrieve informative relevant comment leverage user generate data use comment together article input sequence sequence model copy mechanism show robustness model alleviate aforementioned issue use large scale comment generation dataset result show propose generative model significantly outperform strong baseline seq2seq attention information retrieval model around twenty-seven thirty bleu one point respectively
carry experiment deep learn model summarization across domains news personal stories meet medical article order understand content selection perform find many sophisticate feature state art extractive summarizers improve performance simpler model result suggest easier create summarizer new domain previous work suggest bring question benefit deep learn model summarization domains massive datasets ie news time suggest important question new research summarization namely new form sentence representations external knowledge source need better suit summarization task
propose methodology estimate human behaviors psychotherapy sessions use mutli label multi task learn paradigms discuss problem behavioral cod data human interactions annotate label describe relevant human behaviors interest describe two relate yet distinct corpora consist therapist client interactions psychotherapy sessions experimentally compare propose learn approach estimate behaviors interest datasets specifically compare single multiple label learn approach single multiple task learn approach evaluate performance approach incorporate turn context demonstrate prediction performance gain achieve use propose paradigms discuss insights model provide complex interactions
empirical methods geoparsing thus far lack standard evaluation framework describe task metrics data use compare state art systems evaluation make inconsistent even unrepresentative real world usage lack distinction different type toponyms necessitate new guidelines consolidation metrics detail toponym taxonomy implications name entity recognition ner beyond address deficiencies manuscript introduce new framework three part part one task definition clarify via corpus linguistic analysis propose fine grain pragmatic taxonomy toponyms part two metrics discuss review rigorous evaluation include recommendations ner geoparsing practitioners part three evaluation data share via new dataset call geowebnews provide test train examples enable immediate use contributions addition fine grain geotagging toponym resolution geocoding dataset also suitable prototyping evaluate machine learn nlp model
character base neural model recently prove useful many nlp task however gap sophistication methods learn representations sentence word character model learn representations sentence deep complex model learn representations word shallow simple also spite considerable research learn character embeddings still clear kind architecture best capture character word representations address question first investigate gap methods learn word sentence representations conduct detail experiment comparisons different state art convolutional model also investigate advantage disadvantage constituents furthermore propose intnet funnel shape wide convolutional neural architecture sample learn representations internal structure word compose character limit supervise train corpora evaluate propose model six sequence label datasets include name entity recognition part speech tag syntactic chunk depth analysis show intnet significantly outperform character embed model obtain new state art performance without rely external knowledge resources
paper propose additionsubtraction twin gate recurrent network atr simplify neural machine translation recurrent units atr heavily simplify smallest number weight matrices among units exist gate rnns simple addition subtraction operation introduce twin gate mechanism build input forget gate highly correlate despite simplification essential non linearities capability model long distance dependencies preserve additionally propose atr transparent lstm gru due simplification forward self attention easily establish atr make propose network interpretable experiment wmt14 translation task demonstrate atr base neural machine translation yield competitive performance english german english french language pair term translation quality speed experiment nist chinese english translation natural language inference chinese word segmentation verify generality applicability atr different natural language process task
machine translation shift end end approach base deep neural network state art achieve impressive result popular language pair english french english chinese however english vietnamese shortage parallel corpora expensive hyper parameter search present practical challenge neural base approach paper highlight efforts improve english vietnamese translations two directions one build largest open vietnamese english corpus date two extensive experiment latest neural model achieve highest bleu score experiment provide practical examples effectively employ different neural machine translation model low resource language pair
neural methods several recent successes semantic parse though yet face challenge produce mean representations base formal semantics present sequence sequence neural semantic parser able produce discourse representation structure drss english sentence high accuracy outperform traditional drs parsers facilitate learn output represent drss sequence flat clauses introduce method verify produce drss well form interpretable compare model use character word input see somewhat surprisingly former perform better latter show eliminate variable name output use de bruijn indices increase parser performance add silver train data boost performance even
investigate lattice lstm network chinese word segmentation cws utilize word subwords integrate character sequence feature subsequences information match lexicon match subsequences serve information shortcut tunnel link start end character directly gate units use control contribution multiple input link formula derivation comparison show lattice lstm extension standard lstm ability take multiple input previous lattice lstm model take word embeddings lexicon input prove subword encode give comparable performance benefit rely external segmentor contribution lattice lstm come lexicon pretrained embeddings information find lexicon information contribute pretrained embeddings information control experiment experiment show lattice structure subword encode give competitive better result previous state art methods four segmentation benchmarks detail analyse conduct compare performance word encode subword encode lattice lstm also investigate performance lattice lstm structure different circumstances model work fail
speech recognition mix language difficulties adapt end end framework due lack data overlap phone set example word one english wan chinese propose ctc base end end automatic speech recognition model intra sentential english mandarin code switch model train joint train monolingual datasets fine tune mix language corpus decode process apply beam search combine ctc predictions language model score propose method effective leverage monolingual corpus detect language transition improve cer five
examine prosodic entrainment speak dialogs separately several dialog act cooperative competitive game entrainment measure intonation feature derive superpositional intonation stylization well rhythm feature find differences relate cooperative competitive nature game well dialog act properties intrinsic authority supportiveness distributional characteristics cooperative game dialog act high authority give knowledge high frequency show entrainment result discuss amongst others respect degree active entrainment control cooperative behavior
generative adversarial network gans promise approach text generation unlike traditional language model lm suffer problem exposure bias however major hurdle understand potential gans text generation lack clear evaluation metric work propose approximate distribution text generate gin permit evaluate traditional probability base lm metrics apply approximation procedure several gin base model show currently perform substantially worse state art lms evaluation procedure promote better understand relation gans lms accelerate progress gin base text generation
recent work achieve remarkable result train neural machine translation nmt systems fully unsupervised way new dedicate architectures rely monolingual corpora work propose define unsupervised nmt unmt nmt train supervision synthetic bilingual data approach straightforwardly enable use state art architectures propose supervise nmt replace human make bilingual data synthetic bilingual data train propose initialize train unmt synthetic bilingual data generate unsupervised statistical machine translation usmt unmt system incrementally improve use back translation preliminary experiment show approach achieve new state art unsupervised machine translation wmt16 german english news translation task translation directions
significant roadblock multilingual neural language model lack label non english data one potential method overcome issue learn cross lingual text representations use transfer performance train english task non english task despite little task specific non english data paper explore natural setup learn cross lingual sentence representations dual encoder provide comprehensive evaluation cross lingual representations number monolingual cross lingual zero shoot shoot learn task also give analysis different learn cross lingual embed space
present large scale dataset record machine read comprehension require commonsense reason experiment dataset demonstrate performance state art mrc systems fall far behind human performance record represent challenge future research bridge gap human machine commonsense read comprehension record available http nlpjhuedu record
relation extraction distant supervision noisy label make difficult train quality model previous neural model address problem use attention mechanism attend sentence likely express relations improve model combine distant supervision data additional directly supervise data use supervision attention weight find joint train type supervision lead better model improve model ability identify noisy sentence addition find sigmoidal attention weight max pool achieve better performance commonly use weight average attention setup propose method achieve new state art result widely use fb nyt dataset
standard evaluations deep learn model semantics use naturalistic corpora limit tell us fidelity learn representations corpora rarely come good measure semantic complexity overcome limitation present method generate data set multiply quantify natural language inference nli examples semantic complexity precisely characterize use method show variety common architectures nli inevitably fail encode crucial information model force lexical alignments avoid damage information loss
modern information extraction ie systems implement sequential taggers model local dependencies non local non sequential context however valuable source information improve predictions paper introduce graphie framework operate graph represent broad set dependencies textual units ie word sentence algorithm propagate information connect nod graph convolutions generate richer representation exploit improve word level predictions evaluation three different task namely textual social media visual information extraction show graphie consistently outperform state art sequence tag model significant margin
propose attentive neural network task name entity recognition vietnamese propose attentive neural model make use character base language model word embeddings encode word vector representations neural network architecture encoder attention decoder layer utilize encode knowledge input sentence label entity tag experimental result show propose attentive neural network achieve state art result benchmark name entity recognition datasets vietnamese comparison hand craft feature base model neural model
readability disambiguation write text appropriate word segmentation recommend documentation also hold digitize texts language agglutinative far scriptio continua instance korean language problem become significant however device users days find challenge communicate via key stroke handicap also unskilled study propose real time assistive technology utilize automatic word segmentation design digital minorities familiar electronic type propose data drive system train upon speak korean language corpus various non canonical expressions dialects guarantee comprehension contextual information quantitative qualitative comparison text process toolkits show reliability propose system fit colloquial non normalize texts fulfill aim supportive technology
present corpus encompass complete history conversations contributors wikipedia one largest online collaborative communities record intermediate state conversations include comment reply also modifications deletions restorations data offer unprecedented view online conversation level detail support new research question pertain process challenge large scale online collaboration illustrate corpus potential two case study highlight new perspectives earlier work first explore person conversational behavior depend relate discussion venue second show community moderation toxic behavior happen higher rate previously estimate finally reconstruction framework design language agnostic show extract high quality conversational data chinese english
judge veracity sentence make one claim important challenge problem many dimension recent fever task ask participants classify input sentence either support refute notenoughinfo use wikipedia source true facts surface task explain decision selection sentence trust source multi task neural approach use semantic lexical frame framenet jointly find relevant evidential sentence trust source ii use classify input sentence veracity evaluation efficient three parameter model fever dataset show improvement ninety state art baseline retrieve relevant sentence seventy relative improvement classification
self attention network san recently attract increase interest due fully parallelize computation flexibility model dependencies enhance multi head attention mechanism allow model jointly attend information different representation subspaces different position vaswani et al two thousand and seventeen work propose novel convolutional self attention network csan offer san abilities one capture neighbor dependencies two model interaction multiple attention head experimental result wmt14 english german translation task demonstrate propose approach outperform strong transformer baseline exist work enhance locality san compare previous work model introduce new parameters
one first step utterance interpretation pipeline many task orient conversational ai systems identify user intents correspond slot since data collection machine learn model task time consume desirable make use exist data high resource language train model low resource languages however development model largely hinder lack multilingual train data paper present new data set 57k annotate utterances english 43k spanish 86k thai 5k across domains weather alarm reminder use data set evaluate three different cross lingual transfer methods one translate train data two use cross lingual pre train embeddings three novel method use multilingual machine translation encoder contextual word representations find give several hundred train examples target language latter two methods outperform translate train data low resource settings multilingual contextual word representations give better result use cross lingual static embeddings also compare cross lingual methods use monolingual resources form contextual elmo representations find give small amount target language data method outperform cross lingual methods highlight need sophisticate cross lingual methods
natural disasters conflict information happen often confuse messy distribute across many source would like able automatically identify relevant information assemble coherent narratives happen make task accessible neural model introduce story salads mixtures multiple document generate scale exploit wikipedia hierarchy generate salads exhibit challenge inference problems story salads give rise novel challenge cluster task objective group sentence narratives demonstrate simple bag word similarity cluster fall short task necessary take account global context coherence
nmt far get without attention without separate encode decode answer question introduce recurrent neural translation model use attention separate encoder decoder eager translation model low latency write target tokens soon read first source token use constant memory decode perform par standard attention base model bahdanau et al two thousand and fourteen better long sentence
many concept text generation systems require domain specific linguistic resources produce high quality texts manually construct resources tedious costly focus naturalowl publicly available state art natural language generator owl ontologies propose methods extract web sentence plan natural language name two important type domain specific linguistic resources use generator experiment show texts generate use linguistic resources extract methods semi automatic manner minimal human involvement perceive almost good texts generate use manually author linguistic resources much better texts produce use linguistic resources extract relation entity identifiers ontology
read strategies show improve comprehension level especially readers lack adequate prior knowledge process knowledge accumulation time consume human readers resource demand impart rich general domain knowledge deep language model via pre train inspire read strategies identify cognitive science give limit computational resources pre train model fix number train instance propose three general strategies aim improve non extractive machine read comprehension mrc back forth read consider original reverse order input sequence ii highlight add trainable embed text embed tokens relevant question candidate answer iii self assessment generate practice question candidate answer directly text unsupervised manner fine tune pre train language model radford et al two thousand and eighteen propose strategies largest general domain multiple choice mrc dataset race obtain fifty-eight absolute increase accuracy previous best result achieve pre train model fine tune race without use strategies fine tune result model target mrc task lead absolute improvement sixty-two average accuracy previous state art approach six representative non extractive mrc datasets different domains ie arc openbookqa mctest semeval two thousand and eighteen task eleven rocstories multirc result demonstrate effectiveness propose strategies versatility general applicability fine tune model incorporate strategies core code available https githubcom nlpdata strategy
concept text generation typically employ pipeline architecture often lead suboptimal texts content selection example may greedily select important facts may require however many word express may undesirable space limit expensive select facts possibly slightly less important may allow lexicalization stage use much fewer word report facts space decisions make content selection lexicalization may also lead fewer sentence aggregation opportunities affect length readability result texts build upon publicly available state art natural language generator semantic web ontologies article present integer linear program model unlike pipeline architectures jointly consider choices available content selection lexicalization sentence aggregation avoid greedy local decisions produce compact texts ie texts report facts per word compact texts desirable example generate advertisements include web search result summarize structure information limit space extend version propose model also consider limit form refer expression generation avoid redundant sentence approximation two model use longer texts need generate experiment three ontologies confirm propose model lead compact texts compare pipeline systems deterioration improvements perceive quality generate texts
count base word alignment methods ibm model fast align struggle small parallel corpora therefore present alternative approach base cross lingual word embeddings clwes train purely monolingual data main contribution unsupervised objective adapt clwes parallel corpora experiment twenty-five five hundred sentence method outperform fast align also show fine tune objective consistently improve clwe baseline
crucial information practice healthcare record free form text create enormous opportunity high impact nlp however annotate healthcare datasets tend small expensive obtain raise question make maximally efficient use available data end develop lstm crf model combine unsupervised word representations hand build feature representations derive publicly available healthcare ontologies show combine model yield superior performance five datasets diverse kinds healthcare text clinical social scientific commercial involve label complex multi word span pick different healthcare concepts also introduce new label dataset identify treatment relations drug diseases
paraphrase root semantics show effectiveness transformers vaswani et al two thousand and seventeen paraphrase generation improvements incorporate propbank label via multi encoder evaluate mscoco wikianswers find transformers fast effective semantic augmentation transformers lstms lead sizable two three point gain bleu meteor ter importantly find surprisingly large gain human evaluations compare previous model nevertheless manual inspection generate paraphrase reveal ample room improvement even best model produce human acceptable paraphrase twenty-eight caption chia dataset sharma et al two thousand and eighteen fail spectacularly sentence wikipedia overall result point potential incorporate semantics task highlight need stronger evaluation
sentiment topic analysis common methods use social media monitor essentially methods answer question talk regard x people feel regard x paper investigate another venue social media monitor namely issue ownership agenda set concepts political science use explain voter choice electoral outcomes argue issue alignment agenda set see kind semantic source similarity kind similar source issue owner p talk issue x measure use word document embed techniques present work progress towards measure kind condition similarity introduce new notion similarity predictive embeddings test method measure similarity politically align media political party condition bloc specific issue
present atomic atlas everyday commonsense reason organize 877k textual descriptions inferential knowledge compare exist resources center around taxonomic knowledge atomic focus inferential knowledge organize type relations variables eg x pay compliment likely return compliment propose nine relation type distinguish cause vs effect agents vs theme voluntary vs involuntary events action vs mental state generatively train rich inferential knowledge describe atomic show neural model acquire simple commonsense capabilities reason previously unseen events experimental result demonstrate multitask model incorporate hierarchical structure relation type lead accurate inference compare model train isolation measure automatic human evaluation
first propose new task name dialogue description dial2desc unlike exist dialogue summarization task meet summarization maintain natural flow conversation describe object action people talk dial2desc system take dialogue text input output concise description object action involve conversation read short description one quickly extract main topic conversation build clear picture mind without read listen whole conversation base exist dialogue dataset build new dataset one hundred thousand dialogue description pair step forward demonstrate one get accurate descriptive result use new neural attentive model exploit interaction utterances different speakers compare baselines
one challenge dialogue agents recognize feel conversation partner reply accordingly key communicative skill straightforward humans recognize acknowledge others feel conversation significant challenge ai systems due paucity suitable publicly available datasets train evaluation work propose new benchmark empathetic dialogue generation empatheticdialogues novel dataset 25k conversations ground emotional situations experiment indicate dialogue model use dataset perceive empathetic human evaluators compare model merely train large scale internet conversation data also present empirical comparisons dialogue model adaptations empathetic respond leverage exist model datasets without require lengthy train full model
work introduce novel algorithm solve textbook question answer tqa task describe realistic qa problems compare recent task mainly focus two relate issue analysis tqa dataset first solve tqa problems require comprehend multi modal contexts complicate input data tackle issue extract knowledge feature long text lessons merge visual feature establish context graph texts image propose new module f gcn base graph convolutional network gcn second scientific term spread chapters subject split tqa dataset overcome call domain issue learn qa problems introduce novel self supervise open set learn process without annotations experimental result show model significantly outperform prior state art methods moreover ablation study validate methods incorporate f gcn extract knowledge multi modal contexts newly propose self supervise learn process effective tqa problems
propose nest recurrent neural network nest rnn model english spell error correction generate pseudo data base phonetic similarity train model fuse orthographic information context whole train end end fashion avoid feature engineer rely noisy channel model traditional methods experiment show propose method superior exist systems correct spell errors
propose multilingual model recognize big five personality traits text data four different languages english spanish dutch italian analysis show word similar semantic mean different languages necessarily correspond personality traits therefore propose personality alignment method globaltrait map trait source language target language english word correlate positively trait close together multilingual vector space use align embeddings train transfer personality relate train feature high resource languages english low resource languages get better multilingual result compare use simple monolingual unaligned multilingual embeddings achieve average f score increase across three languages except english sixty-five seven hundred and thirty-four eighty-four compare monolingual model multilingual use cnn personality align embeddings also show relatively good performance regression task better classification result evaluate model separate chinese dataset
code switch cs refer linguistic phenomenon speaker use different languages utterance alternate utterances work study end end e2e approach mandarin english code switch speech recognition cssr task first examine effectiveness use data augmentation byte pair encode bpe subword units importantly propose multitask learn recipe language identification task explicitly learn addition e2e speech recognition task furthermore introduce efficient word vocabulary expansion method language model alleviate data sparsity issue code switch scenario experimental result seame data mandarin english cs corpus demonstrate effectiveness propose methods
encoder decoder typical framework neural machine translation nmt different structure develop improve translation performance transformer one promise structure leverage self attention mechanism capture semantic dependency global view however distinguish relative position different tokens well tokens locate leave right current token focus local information around current token either alleviate problems propose novel attention mechanism name hybrid self attention network hysan accommodate specific design mask self attention network extract various semantic global local information leave right part context finally squeeze gate introduce combine different kinds sans fusion experimental result three machine translation task show propose framework outperform transformer baseline significantly achieve superior result state art nmt systems
current neural machine translation nmt employ language specific encoder represent source sentence adopt language specific decoder generate target translation language dependent design lead large scale network parameters make duality parallel data underutilized address problem propose paper language independent representor replace encoder decoder use weight share share representor reduce large portion network parameters also facilitate us fully explore language duality jointly train source target target source leave right right leave translations within multi task learn framework experiment show propose framework obtain significant improvements conventional nmt model resource rich low resource translation task quarter parameters
read text common become stick unfamiliar word phrase polysemous word novel sense rarely use idioms internet slang emerge entities humans figure mean expressions immediate local context consult dictionaries definitions search document web find global context help interpretation machine help us work type context important machine solve problem answer question undertake task describe give phrase natural language base local global contexts solve task propose neural description model consist two context encoders description decoder contrast exist methods non standard english explanation ni two thousand and seventeen definition generation noraset two thousand and seventeen gadetsky two thousand and eighteen model appropriately take important clue local global contexts experimental result three exist datasets include wordnet oxford urban dictionaries dataset newly create wikipedia demonstrate effectiveness method previous work
cross lingual word embeddings aim capture common linguistic regularities different languages benefit various downstream task range machine translation transfer learn recently show embeddings effectively learn align two disjoint monolingual vector space linear transformation word map work focus learn word map without supervision signal previous work task adopt parametric metrics measure distribution differences typically require sophisticate alternate optimization process either form emphminmax game intermediate emphdensity estimation alternate optimization process relatively hard unstable order avoid sophisticate alternate optimization propose learn unsupervised word map directly maximize mean discrepancy distribution transfer embed target embed extensive experimental result show propose model outperform competitive baselines large margin
study first investigate novel capsule network dynamic rout linear time neural machine translation nmt refer textsccapsnmt textsccapsnmt use aggregation mechanism map source sentence matrix pre determine size apply deep lstm network decode target sequence source representation unlike previous work citesutskever2014sequence store source sentence passive bottom way dynamic rout policy encode source sentence iterative process decide credit attribution nod lower higher layer textsccapsnmt two core properties run time linear length sequence provide flexible way select represent aggregate part whole information source sentence wmt14 english german task larger wmt14 english french task textsccapsnmt achieve comparable result state art nmt systems best knowledge first work capsule network empirically investigate sequence sequence problems
paper introduce how2 multimodal collection instructional videos english subtitle crowdsourced portuguese translations also present integrate sequence sequence baselines machine translation automatic speech recognition speak language translation multimodal summarization make available data code several multimodal natural language task hope stimulate research similar challenge obtain deeper understand multimodality language process
work propose model interaction visual textual feature multi modal neural machine translation mmt latent variable model latent variable see multi modal stochastic embed image description foreign language use target language decoder also predict image feature importantly model formulation utilise visual textual input train require image available test time show latent variable mmt formulation improve considerably strong baselines include multi task learn approach elliott k ad ar two thousand and seventeen conditional variational auto encoder approach toyama et al two thousand and sixteen finally show improvements due predict image feature addition condition ii impose constraint minimum amount information encode latent variable iii train additional target language image descriptions ie synthetic data
suggestion mine increasingly become important task along sentiment analysis today cyberspace world people express sentiments dispositions towards entities service also spend considerable time share experience advice fellow customers product service providers two fold agenda help fellow customers likely share similar experience motivate producer bring specific change offer would appreciate customers current work propose hybrid deep learn model identify whether review text contain suggestion model employ semi supervise learn leverage useful information large amount unlabeled data evaluate performance propose model benchmark customer review dataset comprise review hotel electronics domains propose approach show f score six hundred and fifty-six six hundred and fifty-five hotel electronics review datasets respectively performances significantly better compare exist state art system
transfer learn approach neural machine translation nmt train nmt model assist target language pair parent model later fine tune source target language pair interest child model target language many case assist language different word order source language show divergent word order adversely limit benefit transfer learn little parallel corpus source target language available bridge divergence propose pre order assist language sentence match word order source language train parent model experiment many language pair show bridge word order gap lead significant improvement translation quality
emotion detection conversations necessary step number applications include opinion mine chat history social media thread debate argumentation mine understand consumer feedback live conversations etc currently systems treat party conversation individually adapt speaker utterance paper describe new method base recurrent neural network keep track individual party state throughout conversation use information emotion classification model outperform state art significant margin two different datasets
propose dual ces novel unsupervised query focus multi document extractive summarizer dual ces design better handle tradeoff saliency focus summarization end dual ces employ two step dual cascade optimization approach saliency base pseudo feedback distillation overall dual ces significantly outperform state art unsupervised alternatives dual ces even show able outperform strong supervise summarizers
paper propose multilingual encoder decoder architecture capable obtain multilingual sentence representations mean incorporate intermediate attention bridge share across languages train model language specific encoders decoders connect via self attention share layer call attention bridge layer exploit semantics language perform translation develop language independent mean representation efficiently use transfer learn present new framework efficient development multilingual nmt use model schedule train test approach systematic way multi parallel data set show model achieve substantial improvements strong bilingual model also work well zero shoot translation demonstrate ability abstraction transfer learn
generate coherent cohesive long form texts challenge task previous work rely large amount human generate texts train neural language model however attempt explicitly improve neural language model perspectives coherence cohesion work propose new neural language model equip two neural discriminators provide feedback signal level sentence cohesion paragraph coherence model train use simple yet efficient variant policy gradient call negative critical sequence train propose eliminate need train separate critic estimate baseline result demonstrate effectiveness approach show improvements strong baseline recurrent attention base bidirectional mle train neural language model
word embeddings induce local context prevalent nlp simple effective context base multilingual embed learner levy et al two thousand and seventeen id sentence id method another line work induce high perform multilingual embeddings concepts dufter et al two thousand and eighteen paper propose coco simple scalable method combine context base concept base learn sentence align corpus concepts extract via sample word associate concept id sentence id embed learn first work successfully combine context base concept base embed learn show coco perform well two different application scenarios parallel bible corpus one thousand languages low resource europarl twelve languages high resource among methods applicable corpora coco perform best evaluation setup six task
demonstrate surprise strength unimodal baselines multimodal domains make concrete recommendations best practice future research exist work often compare random majority class baselines argue unimodal approach better capture reflect dataset bias therefore provide important comparison assess performance multimodal techniques present unimodal ablations three recent datasets visual navigation qa see twenty-nine absolute gain performance publish baselines
ability select appropriate story end first step towards perfect narrative comprehension story end prediction require explicit clue within context also implicit knowledge commonsense construct reasonable consistent story however previous approach explicitly use background commonsense knowledge present neural story end selection model integrate three type information narrative sequence sentiment evolution commonsense knowledge experiment show model outperform state art approach public dataset rocstory cloze task performance gain add additional commonsense knowledge significant
world data store relational databases access require specialize knowledge structure query language sql put reach many people recent research thread natural language process nlp aim alleviate problem automatically translate natural language question sql query propose solutions great start lack robustness easily generalize methods require high quality descriptions database table columns widely use train dataset wikisql heavily bias towards use descriptions part question work propose solutions problems entirely eliminate need column descriptions rely solely content augment wikisql dataset paraphrase column name reduce bias show accuracy exist methods drop train augment column agnostic dataset method reach state art accuracy rely column content
disclaimer paper concern violent online harassment describe subject adequate level realism examples collect tweet involve violent threaten vulgar hateful speech language context racial sexual political appearance intellectual harassment presence significant amount harassment user generate content negative impact call robust automatic detection approach require identify different form type harassment earlier work classify harass language term hurtfulness abusiveness sentiment profanity however identify understand harassment accurately essential determine context represent interrelate condition occur paper introduce notion contextual type harassment involve five categories sexual ii racial iii appearance relate iv intellectual v political utilize annotate corpus twitter distinguish type harassment study context type shed light linguistic mean interpretation distribution conduct two line investigation extensive linguistic analysis statistical distribution unigrams build type ware classifiers automate identification type specific harassment experiment demonstrate classifiers provide competitive accuracy identify analyze harassment social media present extensive discussion major observations effectiveness type aware classifiers use detail comparison setup provide insight role type dependent feature
propose chatbot namely mocha make good use relevant entities generate responses augment meta path information mocha able mention proper entities follow conversation flow
sequence generation reinforcement learn rl receive significant attention recently however challenge methods sparse reward problem rl train process scalar guide signal often available entire sequence generate type sparse reward tend ignore global structural information sequence cause generation sequence semantically inconsistent paper present model base rl approach overcome issue specifically propose novel guider network model sequence generation environment assist next word prediction provide intermediate reward generator optimization extensive experiment show propose method lead improve performance unconditional conditional sequence generation task
solve math word problems challenge task require accurate natural language understand bridge natural language texts math expressions motivate intuition human generate equations give problem texts paper present neural approach automatically solve math word problems operate symbols accord semantic mean texts paper view process generate equation bridge semantic world symbolic world propose neural math solver base encoder decoder framework propose model encoder design understand semantics problems decoder focus track semantic mean generate symbols decide symbol generate next preliminary experiment conduct dataset math23k model significantly outperform state art single model best non retrieval base model ten accuracy demonstrate effectiveness bridge symbolic semantic worlds math word problems
although neural machine translation nmt achieve impressive progress recently usually train clean parallel data set hence work well input sentence production automatic speech recognition asr system due enormous errors source solve problem propose simple effective method improve robustness nmt case speech translation simulate noise exist realistic output asr system inject clean parallel data nmt work similar word distributions train test besides also incorporate chinese pinyin feature easy get speech translation improve translation performance experiment result show method stable performance outperform baseline average three hundred and twelve bleu multiple noisy test set even achieve generalization improvement wmt seventeen chinese english test set
address problem abstractive summarization two directions propose novel dataset new model first collect reddit tifu dataset consist 120k post online discussion forum reddit use informal crowd generate post text source contrast exist datasets mostly use formal document source news article thus dataset could less suffer bias key sentence usually locate begin text favorable summary candidates already inside text similar form second propose novel abstractive summarization model name multi level memory network mmn equip multi level memory store information text different level abstraction quantitative evaluation user study via amazon mechanical turk show reddit tifu dataset highly abstractive mmn outperform state art summarization model
paper propose novel adversarial train approach end end speech recognition use criticize language model clm way clm automatic speech recognition asr model challenge learn iteratively improve performance since clm take text input huge quantities unpaired text data utilize approach within end end train moreover apply end end asr model use deep learn base language model frameworks compatible exist end end decode method initial result example experimental setup demonstrate propose approach able gain consistent improvements efficiently auxiliary text data different scenarios
recent years witness dramatic shift towards techniques drive neural network variety nlp task undoubtedly neural language model nlms reduce perplexity impressive amount progress however come substantial cost performance term inference latency energy consumption particularly concern deployments mobile devices paper examine quality performance tradeoff various language model techniques represent knowledge first make observation compare state art nlms classic kneser ney kn lms term energy usage latency perplexity prediction accuracy use two standard benchmarks raspberry pi find order increase latency energy usage correspond less change perplexity difference much less pronounce desktop
achieve long term goal machine able engage humans conversation model captivate interest speak partner communication ground image whereby dialogue conduct base give photo setup naturally appeal humans hu et al two thousand and fourteen work study large scale architectures datasets goal test set neural architectures use state art image text representations consider various ways fuse components test model collect dataset ground human human conversations speakers ask play roles give provide emotional mood style use traits also key factor engagingness guo et al two thousand and nineteen dataset image chat consist 202k dialogues 202k image use two hundred and fifteen possible style traits automatic metrics human evaluations engagingness show efficacy approach particular obtain state art performance exist igc task best perform model almost par humans image chat test set prefer four hundred and seventy-seven time
word sense disambiguation wsd predominant approach generally involve supervise system train sense annotate corpora limit quantity corpora however restrict coverage performance systems article propose new method solve issue take advantage knowledge present wordnet especially hypernymy hyponymy relationships synsets order reduce number different sense tag necessary disambiguate word lexical database method lead state art result wsd evaluation task improve coverage supervise systems reduce train time size model without additional train data addition exhibit result significantly outperform state art method combine ensembling technique addition wordnet gloss tag train corpus
overall objective social dialogue systems support engage entertain lengthy conversations wide variety topics include social chit chat apart raw dialogue data user provide rat common signal use train systems produce engage responses paper show social dialogue systems train effectively raw unannotated data use dataset real conversations collect two thousand and seventeen alexa prize challenge develop neural ranker select good system responses user utterances ie responses likely lead long engage conversations show one neural ranker consistently outperform several strong baselines train optimise user rat two train larger amount data use conversation length objective ranker perform better one train use rat ultimately reach precision1 eighty-seven advance make data collection social conversational agents simpler less expensive future
paper explore problem rank short social media post respect user query use neural network instead start complex architecture proceed bottom examine effectiveness simple word level siamese architecture augment attention base mechanisms capture semantic soft match query post tokens extensive experiment datasets trec microblog track show simple model achieve better effectiveness exist approach far complex exploit diverse set relevance signal also much faster implementations samcnn simple attention base match cnn model share community support future work
neural model knowledge base data typically employ compositional representations graph object entity relation embeddings systematically combine evaluate truth candidate knowedge base entry use model inspire harmonic grammar propose tokenize triplet embeddings subject process optimization respect learn well formedness condition knowledge base triplets result model know gradient graph lead sizable improvements implement companion compositional model also show supracompositional triplet token embeddings produce interpretable properties prove helpful perform inference result triplet representations
sequence sequence seq2seq model witness notable success generate natural conversational exchange notwithstanding syntactically well form responses generate neural network model prone acontextual short generic work introduce topical hierarchical recurrent encoder decoder thred novel fully data drive multi turn response generation system intend produce contextual topic aware responses model build upon basic seq2seq model augment hierarchical joint attention mechanism incorporate topical concepts previous interactions response generation train model provide clean high quality conversational dataset mine reddit comment evaluate thred two novel automate metrics dub semantic similarity response echo index well human evaluation experiment demonstrate propose model able generate diverse contextually relevant responses compare strong baselines
research commercial machine translation far neglect importance properly handle spell lexical grammar divergences occur among language varieties notable case standard national varieties brazilian european portuguese canadian european french popular online machine translation service keep distinct show evident side effect model varieties unique class generation inconsistent translations work investigate problem train neural machine translation english specific pair language varieties assume label unlabeled parallel texts low resource condition report experiment english two pair dialects europeanbrazilian portuguese european canadian french two pair standardize varieties croatian serbian indonesian malay show significant bleu score improvements baseline systems translation similar languages learn multilingual task share representations
pretraining sentence encoders language model relate unsupervised task recently show effective language understand task supplement language model style pretraining train data rich supervise task natural language inference obtain additional performance improvements glue benchmark apply supplementary train bert devlin et al two thousand and eighteen attain glue score eight hundred and eighteen state art two twenty-four two thousand and nineteen fourteen point improvement bert also observe reduce variance across random restart set approach yield similar improvements apply elmo peters et al 2018a radford et al two thousand and eighteen model addition benefit supplementary train particularly pronounce data constrain regimes show experiment artificially limit train data
train model map natural language instructions program give target world supervision require search good program train time search commonly do use beam search space partial program program tree length instructions grow find good program become difficult work propose search algorithm use target world state know train time train critic network predict expect reward every search state score search state beam interpolate expect reward likelihood program represent search state moreover search space program compress state program executions augment recent entities action scone dataset show algorithm dramatically improve performance three domains compare standard beam search baselines
although neural machine translation make significant progress recently integrate multiple overlap arbitrary prior knowledge source remain challenge work propose use posterior regularization provide general framework integrate prior knowledge neural machine translation represent prior knowledge source feature log linear model guide learn process neural translation model experiment chinese english translation show approach lead significant improvements
natural language process heavily anglo centric demand model work languages english greater ever yet task transfer model one language another expensive term annotation cost engineer time effort paper present general framework easily effectively transfer neural model english languages framework rely task representations form weak supervision model task agnostic mean many exist neural architectures port languages minimal effort requirement unlabeled parallel data loss define task representations evaluate framework transfer english sentiment classifier three different languages battery test show model outperform number strong baselines rival state art result rely complex approach significantly resources data additionally find framework propose paper able capture semantically rich meaningful representations across languages despite lack direct supervision
aim better exploit limit amount parallel text available low resource settings introduce differentiable reconstruction loss neural machine translation nmt loss compare original input reconstruct input obtain back translate translation hypotheses input language leverage differentiable sample bi directional nmt train model end end without introduce additional parameters approach achieve small consistent bleu improvements four language pair translation directions outperform alternative differentiable reconstruction strategy base hide state
propose method transfer knowledge across neural machine translation nmt model mean share dynamic vocabulary approach allow extend initial model give language pair cover new languages adapt vocabulary long new data become available ie introduce new vocabulary items include initial model parameter transfer mechanism evaluate two scenarios adapt train single language nmt system work new language pair ii continuously add new language pair grow multilingual nmt system scenarios goal improve translation performance minimize train convergence time preliminary experiment span five languages different train data size ie 5k 50k parallel sentence show significant performance gain range three hundred and eighty-five one thousand, three hundred and sixty-three bleu different language directions moreover compare train nmt model scratch transfer learn approach allow us reach higher performance train four total train step
neural machine translation nmt model learn representations contain substantial linguistic information however clear information fully distribute attribute individual neurons develop unsupervised methods discover important neurons nmt model methods rely intuition different model learn similar properties require costly external supervision show experimentally translation quality depend discover neurons find many capture common linguistic phenomena finally show control nmt translations predictable ways modify activations individual neurons
open domain dialogue intelligent agents exhibit use knowledge however convince demonstrations date popular sequence sequence model typically generate hope generic utterances memorize weight model map input utterances output rather employ recall knowledge context use knowledge far prove difficult part lack supervise learn benchmark task exhibit knowledgeable open dialogue clear ground end collect release large dataset conversations directly ground knowledge retrieve wikipedia design architectures capable retrieve knowledge read condition finally generate natural responses best perform dialogue model able conduct knowledgeable discussions open domain topics evaluate automatic metrics human evaluations new benchmark allow measure improvements important research direction
newberry et al detect evolutionary force language change nature five hundred and fifty-one two thousand and seventeen tackle important difficult problem linguistics test selective theories language change null model drift apply test population genetics frequency increment test number relevant examples suggest stochasticity previously appreciate role language evolution replicate result find overall observation hold result produce approach individual time series sensitive corpus organize temporal segment bin furthermore use large set simulations conjunction bin systematically explore range applicability frequency increment test conclude care exercise interpret result test like frequency increment test individual series give researcher degrees freedom available apply test corpus data fundamental differences genetic linguistic data find implications selection test temporal bin general well demonstrate usefulness simulations evaluate methods newly introduce field
slot fill crucial component task orient dialog systems parse user utterances semantic concepts call slot ontology define collection slot value slot take widely use practice treat slot fill sequence label task suffer two drawbacks first ontology usually pre define fix current methods unable predict new label unseen slot second one hot encode slot label ignore semantic mean relations slot implicit natural language descriptions observations motivate us propose novel model call elastic conditional random field ecrf open ontology slot fill ecrfs leverage neural feature utterance slot descriptions able model interactions different slot experimental result show ecrfs outperform exist model domain cross doamin task especially predictions unseen slot value
clickbaits catchy headline frequently use social media outlets order allure viewers click thus lead dubious content venal scheme thrive exploit curiosity naive social media users direct traffic web page visit otherwise paper propose novel semi supervise classification base approach employ attentions sample gumbel softmax distribution distill contexts fairly important clickbait detection additional loss attention weight use encode prior knowledge furthermore propose confidence network enable learn weak label improve robustness noisy label show merely thirty strongly label sample achieve ninety-seven accuracy current state art methods clickbait detection
recent work multilingual neural machine translation report competitive performance respect bilingual model surprisingly good performance even zeroshot translation directions observe train time investigate zero shoot translation particularly lowresource multilingual set propose simple iterative train procedure leverage duality translations directly generate system zero shoot directions translations produce system sub optimal since contain mix language share vocabulary use together original parallel data fee iteratively train multilingual network time allow system learn generate increasingly better output approach show effective improve two zero shoot directions multilingual model particular observe gain nine bleu point baseline multilingual model two hundred and eight bleu pivot mechanism use two bilingual model analysis show also slight improvement non zero shoot language directions
implicit discourse relation classification one difficult step discourse parse difficulty stem fact coherence relation must infer base content discourse relational arguments therefore effective encode relational arguments crucial importance propose new model implicit discourse relation classification consist classifier sequence sequence model train generate representation discourse relational arguments try predict relational arguments include suitable implicit connective train possible implicit connectives annotate part pdtb corpus along memory network model could generate refine representations task standard eleven way classification method outperform previous state art systems pdtb benchmark multiple settings include cross validation
rnn language model achieve state art result various task exactly represent syntax yet unclear investigate whether rnn language model learn humanlike word order preferences syntactic alternations collect language model surprisal score control sentence stimuli exhibit major syntactic alternations english heavy np shift particle shift dative alternation genitive alternation show rnn language model reproduce human preferences alternations base np length animacy definiteness collect human acceptability rat stimuli first acceptability judgment experiment directly manipulate predictors syntactic alternations show rnns performance similar human acceptability rat match n gram baseline model result show rnns learn abstract feature weight animacy definiteness underlie soft constraints syntactic alternations
propose compare methods gradient base domain adaptation self attentive neural machine translation model demonstrate large proportion model parameters freeze adaptation minimal reduction translation quality encourage structure sparsity set offset tensors learn via group lasso regularization evaluate technique batch incremental adaptation across multiple data set language pair system architecture combine state art self attentive model compact domain adaptation provide high quality personalize machine translation space time efficient
paper describe recent performance improvements production marchex speech recognition system spontaneous customer business telephone conversations previous work focus domain language acoustic model train work employ state art semi supervise lattice free maximum mutual information lf mmi train process supervise full lattices unlabeled audio marchex english modern evaluation set conversational north american english observe thirty-three thirty-two agent thirty-six caller reduction absolute word error rate wer 3x faster decode speed performance two thousand and seventeen production system expect improvement boost marchex call analytics system performance especially natural language process pipeline
study approach improve fine grain short answer question answer model integrate coarse grain data annotate paragraph level relevance show coarsely annotate data bring significant performance gain experiment demonstrate standard multi task learn approach share representations effective way leverage coarse grain annotations instead explicitly model latent fine grain short answer variables optimize marginal log likelihood directly use newly propose emphposterior distillation learn objective since latent variable methods explicit access relationship fine coarse task result significantly larger improvements coarse supervision
work explore better adaptation methods low resource languages use external language model lm framework transfer learn first build language independent asr system unify sequence sequence s2s architecture share vocabulary among languages adaptation perform lm fusion transfer external lm integrate decoder network attention base s2s model whole adaptation stage effectively incorporate linguistic context target language also investigate various seed model transfer learn experimental evaluations use iarpa babel data set show lm fusion transfer improve performances target five languages compare simple transfer learn external text data available final system drastically reduce performance gap hybrid systems
paper describe cis slot fill system tac cold start evaluations two thousand and fifteen extend improve system build evaluation last year paper mainly describe change last year system especially focus coreference classification component coreference perform several analysis prepare resource simplify end end system improve runtime classification propose use neural network train convolutional recurrent neural network combine traditional evaluation methods namely pattern support vector machine run two thousand and fifteen evaluation design directly assess effect network end end performance system cis system achieve rank three slot fill systems participate task
frame unsupervised machine translation mt context multi task learn mtl combine insights directions leverage shelf neural mt architectures train unsupervised mt model parallel data show model achieve reasonably good performance competitive model purpose build unsupervised mt finally propose improvements allow us apply model english turkish truly low resource language pair
sentence embed effective feature representation deep learn base nlp task one prevail line methods use recursive latent tree structure network embed sentence task specific structure however exist model explicit mechanism emphasize task informative word tree structure end propose attentive recursive tree model ar tree word dynamically locate accord importance task specifically construct latent tree sentence propose important first strategy place attentive word nearer root thus ar tree inherently emphasize important word bottom composition sentence embed propose end end reinforce train strategy ar tree demonstrate consistently outperform least comparable state art sentence embed methods three sentence understand task
code switch deal alternative languages speech text partially speaker depend domain relate completely explain phenomenon linguistic rule challenge compare monolingual task insufficient data issue code switch mitigate issue without expensive human annotation propose unsupervised method code switch data augmentation utilize generative adversarial network generate intra sentential code switch sentence monolingual sentence apply propose method two corpora result show generate code switch sentence improve performance code switch language model
representation learn foundation machine read comprehension inference state art model character level representations broadly adopt alleviate problem effectively represent rare complex word however character natural minimal linguistic unit representation word embed compose due ignore linguistic coherence consecutive character inside word paper present general subword augment embed framework learn compose computationally derive subword level representations survey series unsupervised segmentation methods subword acquisition different subword augment strategies text understand show subword augment embed significantly improve baselines various type text understand task english chinese benchmarks
propose deepchannel robust data efficient interpretable neural model extractive document summarization give document summary pair estimate salience score model use attention base deep neural network represent salience degree summary yield document devise contrastive train strategy learn salience estimation network use learn salience score guide iteratively extract salient sentence document generate summary experiment model achieve state art rouge score cnn daily mail dataset also show strong robustness domain test duc2007 test set moreover model reach rouge one f one score three thousand, nine hundred and forty-one cnn daily mail test set merely one one hundred train set demonstrate tremendous data efficiency
article present first depth linguistic study human feel substantial research incorporate affective categories linguistic analysis eg sentiment lesser extent emotion diverse category human feel thus far investigate survey extensive interdisciplinary literature around feel construct work definition constitute feel propose nine broad categories feel identify potential feel word base pointwise mutual information morphological variants word feel google n gram corpus present manual annotation exercise three hundred and seventeen wordnet sense one hundred word categorise feel one nine propose categories feel proceed annotate eleven thousand, three hundred and eighty-six wordnet sense word create wordnet feel new affective dataset identify three thousand, six hundred and sixty-four word sense feel associate one nine categories feel wordnet feel use conjunction datasets sentiwordnet annotate word sense complementary affective properties valence intensity
describe universitat alacant submissions word sentence level machine translation mt quality estimation qe share task wmt two thousand and eighteen approach word level mt qe build previous work mark word machine translate sentence textitok textitbad extend determine word sequence word need insert gap word sentence level submission simply use edit operations predict word level approach approximate ter method present rank first sub task identify insertions gap three six datasets second rest
rapidly develop neural model achieve competitive performance chinese word segmentation cws traditional counterparts however methods encounter computational inefficiency especially long sentence increase model complexity slower decoders paper present simple neural segmenter directly label gap existence adjacent character alleviate exist drawback segmenter fully end end capable perform segmentation fast also show performance difference different tag set experiment show segmenter provide comparable performance state art
long short term memory lstm neural net architectures design capture sequence information human language generally compose hierarchical structure raise question whether lstms learn hierarchical structure explore question well form bracket prediction task use two type bracket model lstm demonstrate system learnable lstm first step demonstrate entire class cfls also learnable observe model require exponential memory term number character embed depth sub linear memory suffice still model memorize train input learn distinguish relevant irrelevant information hand also observe model generalize well conclude lstms learn relevant underlie context free rule suggest good overall performance attain rather efficient way evaluate nuisance variables lstms way quickly reach good result many natural language task understand generate natural language one investigate concepts make direct use natural language structural nature
date bulk research single channel speech separation conduct use clean near field read speech representative many modern applications work develop procedure construct high quality synthetic overlap datasets necessary deep learn base separation frameworks produce datasets representative realistic applications use chime five mixer six corpora evaluate standard methods data demonstrate shortcomings current source separation performance also demonstrate value wide variety data train robust model generalize well multiple condition
current conversational systems follow simple command answer basic question difficulty maintain coherent open end conversations specific topics competitions like conversational intelligence convai challenge organize push research development towards goal article present detail rllchatbot participate two thousand and seventeen convai challenge goal research better understand current deep learn reinforcement learn tool use build robust yet flexible open domain conversational agent provide thorough description dialog system build train mostly public domain datasets use ensemble model first contribution work detail description analysis different text generation model addition novel message rank selection methods moreover new open source conversational dataset present train data significantly improve recallk score rank selection mechanisms compare baseline model responsible select message return interaction
embed audio signal segment vectors fix dimensionality attractive follow process easier efficient example model classify index audio word2vec previously propose show able represent audio segment speak word vectors carry information phonetic structure signal segment however linguistic unit word syllable phoneme text form correspond unlimited number audio segment vector representations inevitably spread embed space cause confusion therefore desire better cluster audio embeddings correspond linguistic unit compactly distribute paper inspire siamese network propose approach achieve goal include identify positive negative pair unlabeled data siamese style train disentangle acoustic factor speaker characteristics audio embed handle unbalance data distribution embed process learn adjacency relationships among data point do unsupervised way improve performance obtain preliminary experiment librispeech data set include cluster characteristics analysis applications speak term detection
name entity recognition ner bidirectional recurrent neural network become state art technology recent years compete approach vary respect pre train word embeddings well model character embeddings represent sequence information effectively ner german language texts model variations study extensively evaluate performance different word character embeddings two standard german datasets special focus vocabulary word f score eighty-two germeval fourteen dataset eighty-five conll three dataset achieve near state art performance task publish several pre train model wrap micro service base docker allow easy integration german ner applications via json api
investigate different strategies automatic offensive language classification german twitter data employ sequentially combine bilstm cnn neural network base model three transfer learn task improve classification performance background knowledge test compare one supervise category transfer social media data annotate near offensive language categories two weakly supervise category transfer tweet annotate emojis contain three unsupervised category transfer tweet annotate topic cluster obtain latent dirichlet allocation lda investigate effect three different strategies mitigate negative effect catastrophic forget transfer learn result indicate transfer learn general improve offensive language detection best result achieve pre train model unsupervised topic cluster tweet combination thematic user cluster information
paper present ims contribution poleval two thousand and eighteen share task submit systems subtasks task one subtask dependency parse use ensemble system conll two thousand and seventeen ud share task system first preprocesses sentence crf pos morphological tagger predict supertags neural tagger employ multiple instance three different parsers merge output apply blend system achieve second place four participate team paper show components system responsible final performance goal subtask b predict enhance graph approach consist two step parse sentence ensemble system subtask apply twelve simple rule obtain final dependency graph rule introduce additional enhance arc tokens conj head conjuncts predict semantic relations system rank first three participate team paper show examples rule design analyze relation quality automatically parse tree accuracy enhance graph
data selection techniques apply neural machine translation nmt aim increase performance model retrieve subset sentence use train data one possible data selection techniques transductive learn methods select data base test set ie document translate limitation methods date use source side test set guarantee sentence select correct translations translations suitable give test set domain corpora subtitle corpora may contain parallel sentence inaccurate translations cause localization length restrictions order try fix problem paper propose use approximate target side addition source side select suitable sentence pair train model approximate target side build pre translate source side work explore performance general idea one specific data selection approach call feature decay algorithms fda train german english nmt model data select use test set source approximate target side mixture find reveal model build use combination output fda use test set approximate target side perform better solely use test set obtain statistically significant improvement fifteen bleu point model train data five bleu point strong fda baseline use source side information
translate speech special consideration conversational speech phenomena disfluencies necessary machine translation train data consist well form write texts cause issue translate spontaneous speech previous work introduce intermediate step speech recognition asr machine translation mt remove disfluencies make data better match typical translation text significantly improve performance however rise end end speech translation systems intermediate step must incorporate sequence sequence architecture though translate speech datasets exist typically news rehearse speech without many disfluencies eg ted disfluencies translate reference eg fisher generate clean translations disfluent speech clean reference necessary evaluation introduce corpus clean target data fisher spanish english dataset task compare different architectures handle disfluencies provide baseline remove disfluencies end end translation
word vector representations crucial part natural language process nlp human computer interaction paper propose novel word vector representation confusion2vec motivate human speech production perception encode representational ambiguity humans employ acoustic similarity cue contextual cue decode information focus model incorporate source information representational ambiguity acoustics manifest word confusions often resolve humans machine contextual cue range representational ambiguities emerge various domains acoustic perception morphological transformations paraphrase nlp task like machine translation etc work present case study application automatic speech recognition asr word confusions relate acoustic similarity present several techniques train acoustic perceptual similarity representation ambiguity term confusion2vec learn unsupervised generate data asr confusion network lattice like structure appropriate evaluations confusion2vec formulate gauge acoustic similarity addition semantic syntactic word similarity evaluations confusion2vec able model word confusions efficiently without compromise semantic syntactic word relations thus effectively enrich word vector space extra task relevant ambiguity information provide intuitive exploration two dimensional confusion2vec space use principal component analysis embed relate semantic syntactic acoustic relationships potential confusion2vec utilization uncertainty present lattices demonstrate small examples relate asr error correction
modern information systems change idea data process idea concept process mean instead process word systems process semantic concepts carry mean share contexts concepts ontology commonly use structure capture knowledge certain area via provide concepts relations traditionally concept hierarchies build manually knowledge engineer domain experts however manual construction concept hierarchy suffer several limitations coverage enormous cost extension maintenance ontology learn usually refer semi automatic support ontology development usually divide step go concepts identification pass hierarchy non hierarchy relations detection seldom axiom extraction reasonable say among step current frontier establishment concept hierarchies since backbone ontologies therefore good concept hierarchy already valuable resource many ontology applications automatic construction concept hierarchies texts complex task much work propose approach better extract relations concepts different proposals never contrast set data across different languages comparison important see whether complementary incremental also see whether present different tendencies towards recall precision paper evaluate different methods basis hierarchy metrics density depth evaluation metrics recall precision result would light comprehensive set methods accord literature area
aim paper twofold first use vector space distributional compositional categorical model mean compare mean sentence irish english thus ascertain sentence translation another sentence use cosine similarity score shall outline procedure translate nouns understand context use conceptual space model cognition shall use metrics category convexrel determine distance concepts determine noun translation another noun paper focus applications irish member gaelic family languages
general data protection regulation gdpr pose enormous challenge company organizations respect understand implement maintain contain constraints report conrelminer method use untangle gdpr gdpr filter group along roles mention gdpr reduction sentence read analysts show moreover output conrelminer cluster graph relations sentence display interpret overall goal illustrate effort implement gdpr reduce structure meaningful representation relevant gdpr sentence find
easy first parse rely subtree rank build complete parse tree whereas intermediate state parse process represent various subtrees whose internal structural information key lead later parse action decisions explore better representation subtrees detail work introduce bottom subtree encode method base child sum tree lstm start easy first dependency parser without handcraft feature show effective subtree encoder promote parse process make greedy search easy first parser achieve promise result benchmark treebanks compare state art baselines furthermore help current pre train language model improve state art result easy first approach
end end approach recently become popular mean simplify train deployment speech recognition systems however often require large amount data perform well large vocabulary task aim make end end approach usable broader range researchers explore potential use end end methods small vocabulary contexts smaller datasets may use significant drawback small vocabulary systems difficulty expand vocabulary beyond original train sample therefore also study strategies extend vocabulary examples per new class shoot learn result show attention base encoder decoder competitive strong baseline small vocabulary keyword classification task reach nine hundred and seventy-five accuracy tensorflow speech command dataset also show promise result shoot learn problem simple strategy achieve six hundred and eighty-eight accuracy new keywords ten examples new class score go eight hundred and eighty-four larger set one hundred examples
implicit arguments detect solely syntactic cue make harder extract predicate argument tuples present new model implicit argument prediction draw read comprehension cast predicate argument tuple miss argument query also draw pointer network multi hop computation model show good performance argument cloze task well nominal implicit argument prediction task
train recurrent neural network language model use distribute device learn framework call federate learn purpose next word prediction virtual keyboard smartphones server base train use stochastic gradient descent compare train client devices use federate average algorithm federate algorithm enable train higher quality dataset use case show achieve better prediction recall work demonstrate feasibility benefit train language model client devices without export sensitive user data servers federate learn environment give users greater control use data simplify task incorporate privacy default distribute train aggregation across population client devices
sustain engage conversation critical chatbots make good use relevant knowledge equip knowledge base chatbots able extract conversation relate attribute entities facilitate context model response generation work distinguish use attribute entity incorporate encoder decoder architecture different manners base augment architecture chatbot namely mike able generate responses refer proper entities collect knowledge validate propose approach build movie conversation corpus propose approach significantly outperform four knowledge ground model
paper present neural architecture vietnamese sequence label task include part speech pos tag name entity recognition ner apply model describe citelample etal2016n16 one combination bidirectional long short term memory conditional random field rely two source information word character base word representations learn supervise corpus pre train word embeddings learn unannotated corpora experiment benchmark datasets show work achieve state art performances task nine thousand, three hundred and fifty-two accuracy pos tag nine thousand, four hundred and eighty-eight f1 ner sourcecode available
relation extraction task identify predefined relationship entities play essential role information extraction knowledge base construction question answer exist relation extractors make predictions entity pair locally individually ignore implicit global clue available across different entity pair knowledge base often lead conflict among local predictions different entity pair paper propose joint inference framework employ global clue resolve disagreements among local predictions exploit two kinds clue generate constraints capture implicit type cardinality requirements relation constraints examine either hard style soft style effectively explore integer linear program formulation experimental result english chinese datasets show propose framework effectively utilize two categories global clue resolve disagreements among local predictions thus improve various relation extractors clue applicable datasets experiment also indicate clue learn automatically exist knowledge base perform comparably better refine human
humans capable process speech make use multiple sensory modalities example environment conversation take place generally provide semantic acoustic context help us resolve ambiguities recall name entities motivate many work study integration visual information speech recognition pipeline specifically previous work propose multistep visual adaptive train approach improve accuracy audio base automatic speech recognition asr system approach however end end require fine tune whole model adaptation layer paper propose novel end end multimodal asr systems compare adaptive approach use range visual representations obtain state art convolutional neural network show adaptive train effective s2s model lead absolute improvement fourteen word error rate end end systems although perform better baseline improvements slightly less adaptive train eight absolute wer reduction single best model use ensemble decode end end model reach wer fifteen lowest score among systems
recent years long short term memory lstm successfully use model sequential data variable length however lstm still experience difficulty capture long term dependencies work try alleviate problem introduce dynamic skip connection learn directly connect two dependent word since dependency information train data propose novel reinforcement learn base method model dependency relationship connect dependent word propose model compute recurrent transition function base skip connections provide dynamic skip advantage rnns always tackle entire sentence sequentially experimental result three natural language process task demonstrate propose method achieve better performance exist methods number prediction experiment propose model outperform lstm respect accuracy nearly twenty
cross lingual entity link map entity mention source language correspond entry structure knowledge base different target language previous work rely heavily bilingual lexical resources bridge gap source target languages resources scarce unavailable many low resource languages address problem investigate zero shoot cross lingual entity link assume bilingual lexical resources available source low resource language specifically propose pivot base entity link leverage information high resource pivot language train character level neural entity link model transfer source low resource language zero shoot manner experiment nine low resource languages transfer total fifty-four languages show propose pivot base framework improve entity link accuracy seventeen absolute average baseline systems zero shoot scenario also investigate use language universal phonological representations improve average accuracy absolute thirty-six transfer languages use different script
recent deep learn model show improve result natural language generation nlg irrespective provide sufficient annotate data however modest train data may harm model performance thus build generator utilize much knowledge low resource set data crucial issue nlg paper present variational neural base generation model tackle nlg problem limit label dataset integrate variational inference encoder decoder generator introduce novel auxiliary autoencoding effective train procedure experiment show propose methods outperform previous model sufficient train dataset also show strong ability work acceptably well train data scarce
large portion real life utterances intention solely decide either semantic syntactic characteristics although sociolinguistic pragmatic information digitize least phonetic feature indispensable understand speak language especially head final languages korean sentence final prosody great importance identify speaker intention paper suggest system identify inherent intention speak utterance give transcript case use auxiliary acoustic feature main point separate distinction case discrimination intention require acoustic cue thus propose classification system decide whether give utterance fragment statement question command rhetorical question command utilize intonation dependency come head finality base intuitive understand korean language engage data annotation construct network identify intention speech validate utility test sentence system combine date speech recognizers expect flexibly insert various language understand modules
end end speech recognition systems model text directly sequence character sub word current approach sub word extraction consider character sequence frequencies time produce inferior sub word segmentation might lead erroneous speech recognition output propose pronunciation assist sub word model pasm sub word extraction method leverage pronunciation information word experiment show propose method greatly improve upon character base baseline also outperform commonly use byte pair encode methods
disclose overlap multiple relations sentence still keep challenge current work term neural model inconveniently assume sentence explicitly map relation label handle multiple relations properly overlap feature relations either ignore difficult identify tackle new issue propose novel approach multi label relation extraction capsule network act considerably better current convolutional recurrent net identify highly overlap relations within individual sentence better cluster feature precisely extract relations devise attention base rout algorithm slide margin loss function embed capsule network experimental result show propose approach indeed extract highly overlap feature achieve significant performance improvement relation extraction compare state art work
nowadays increase number customers favor use e commerce apps browse purchase products since merchants usually incline employ redundant informative product title attract customers attention great importance concisely display short product title limit screen cell phone previous researchers mainly consider textual information long product title lack human like view train evaluation procedure paper propose multi modal generative adversarial network mm gin short product title generation innovatively incorporate image information attribute tag product textual information original long title mm gin treat short title generation reinforcement learn process generate title evaluate discriminator human like view
investigate feasibility sequence level knowledge distillation sequence sequence seq2seq model large vocabulary continuous speech recognition lvscr first use pre train larger teacher model generate multiple hypotheses per utterance beam search input train student model use hypotheses generate teacher pseudo label place original grind truth label evaluate propose method use wall street journal wsj corpus achieve ninety-eight time parameter reduction accuracy loss seventy word error rate wer increase
social network serve valuable communication channel call help offer assistance coordinate rescue activities disaster social network twitter allow users continuously update relevant information especially useful crisis rapidly change condition make crucial able access accurate information promptly social media help directly affect inform others condition grind real time thus enable rescue workers coordinate efforts effectively better meet survivors need paper present new sequence sequence base framework forecast people need disasters use social media weather data consist two long short term memory lstm model one encode input sequence weather information play conditional decoder decode encode vector forecast survivors need case study utilize data collect hurricane sandy two thousand and twelve hurricane harvey hurricane irma two thousand and seventeen analyze result compare obtain use statistical language model n gram lstm generative model propose sequence sequence method forecast people need successfully either model new approach show great promise enhance disaster management activities evacuation plan commodity flow management
analysis thousands movies book reveal cultural products weave stereotypical gender roles morality tales perpetuate gender inequality storytelling use word embed techniques reveal construct emotional dependency female character male character stories
exist work dialog systems consider conversation content neglect personality user bot interact beget several unsolved issue paper present personalize end end model attempt leverage personalization goal orient dialogs first introduce profile model encode user profile distribute embeddings refer conversation history similar users preference model capture user preferences knowledge base entities handle ambiguity user request two model combine personalize memn2n experiment show propose model achieve qualitative performance improvements state art methods human evaluation also outperform approach term task completion rate user satisfaction
cross entropy loss common choice come multiclass classification task language model particular minimize loss result language model good quality show possible fine tune model make perform even better fine tune sum cross entropy loss reverse kullback leibler divergence latter estimate use discriminator network train advance fine tune probabilities rare word usually underestimate language model become bigger novel approach propose allow us reach state art quality penn treebank perplexity decrease five hundred and twenty-four five hundred and twenty-one fine tune algorithm rather fast scale well different architectures datasets require almost hyperparameter tune hyperparameter need tune learn rate
fake news rumor incorrect information misinformation detection nowadays crucial issue might serious consequences social fabrics rate information increase rapidly due availability enormous web information source include social media feed news blog online newspapers etc paper develop various deep learn model detect fake news classify pre define fine grain categories first develop model base convolutional neural network cnn bi directional long short term memory bi lstm network representations obtain two model feed multi layer perceptron model mlp final classification experiment benchmark dataset show promise result overall accuracy four thousand, four hundred and eighty-seven outperform current state art
present methods automatic classification patent applications use annotate dataset provide organizers alta two thousand and eighteen share task classify patent applications goal task use computational methods categorize patent applications accord coarse grain taxonomy eight class base international patent classification ipc test variety approach task best result seven hundred and seventy-eight micro average f1 score achieve svm ensembles use combination word character feature team bmz rank first among fourteen team competition
present submission wmt18 multimodal translation task main feature submission apply self attentive network instead recurrent neural network evaluate two methods incorporate visual feature model first include image representation another input network second train model predict visual feature use auxiliary objective submission acquire textual multimodal additional data propose methods yield significant improvements recurrent network self attentive textual baselines
explore deep convolutional neural network cnns small two dimensional kernels primarily use model spatial relations image also effective speech recognition analyze representations learn deep cnns compare deep neural network dnn representations vectors context acoustic model adaptation explore whether interpretable information decode learn representations evaluate ability discriminate speakers acoustic condition noise type gender use aurora four dataset extract whole model embeddings capture information learn across whole network layer specific embeddings enable understand flow information across network also use learn representations additional input time delay neural network tdnn aurora four mgb three english datasets find deep cnn embeddings outperform dnn embeddings acoustic model adaptation auxiliary feature base deep cnn embeddings result similar word error rat vectors
multi source sequence sequence task attention mechanism model several ways topic thoroughly study recurrent architectures paper extend previous work encoder decoder attention transformer architecture propose four different input combination strategies encoder decoder attention serial parallel flat hierarchical evaluate methods task multimodal translation translation multiple source languages experiment show model able use multiple source improve single source baselines
autoregressive decode part sequence sequence model prevent massive parallelization inference time non autoregressive model enable decoder generate output symbols independently parallel present novel non autoregressive architecture base connectionist temporal classification evaluate task neural machine translation unlike non autoregressive methods operate several step model train end end conduct experiment wmt english romanian english german datasets model achieve significant speedup autoregressive model keep translation quality comparable non autoregressive model
unsupervised methods learn rich contextualized token representations obviate need explicit model linguistic structure neural network model semantic role label srl address question incorporate massively successful elmo embeddings peters et al two thousand and eighteen lisa strubell et al two thousand and eighteen strong linguistically inform neural network architecture srl experiment conll two thousand and five share task find though elmo perform typical word embeddings begin close gap f1 lisa predict gold syntactic parse syntactically inform model still perform syntax free model use elmo especially domain data result suggest linguistic structure indeed still relevant golden age deep learn nlp
community question answer forums quora stackoverflow rich knowledge resources often cater information topics overlook major search engines answer submit forums often elaborate contain spam mar slur business promotions difficult reader go numerous answer gauge community opinion result summarization become prioritize task cqa forums number efforts make summarize factoid cqa little work exist summarize non factoid cqa believe due lack considerably large annotate dataset cqa summarization create cqasumm first huge annotate cqa summarization dataset filter forty-four million yahoo answer l6 dataset sample thread best answer double reference summary build hundred word summaries treat answer candidates document summarization provide script generate dataset introduce new task community question answer summarization multi document summarization widely study news article datasets especially duc tac challenge use news corpora however document cqa higher variance contradict opinion lesser amount overlap compare popular multi document summarization techniques evaluate performance cqa corpora look state art understand case exist multi document summarizers mds fail find mds workflows build entirely factual news corpora whereas corpus fair share opinion base instance therefore introduce opiniosumm new mds outperform best baseline forty-six wrt rouge one score
attention base methods connectionist temporal classification ctc network promise research directions end end automatic speech recognition asr joint ctc attention model achieve great success utilize architectures multi task train joint decode work present novel multi encoder multi resolution memr framework base joint ctc attention model two heterogeneous encoders different architectures temporal resolutions separate ctc network work parallel extract complimentary acoustic information hierarchical attention mechanism use combine encoder level information demonstrate effectiveness propose model experiment conduct wall street journal wsj chime four result relative word error rate wer reduction one hundred and eighty three hundred and twenty-one moreover propose memr model achieve thirty-six wer wsj eval92 test set best wer report end end system benchmark
dialogue act da classification challenge problem dialogue interpretation aim attach semantic label utterances characterize speaker intention currently many exist approach formulate da classification problem range multi classification structure prediction suffer two limitations methods either handcraft feature base limit memories b adversarial examples correctly classify traditional train methods address issue paper first cast problem question answer problem propose improve dynamic memory network hierarchical pyramidal utterance encoder moreover apply adversarial train train propose model evaluate model two public datasets ie switchboard dialogue act corpus maptask corpus extensive experiment show propose model robust also achieve better performance compare state art baselines
target base sentiment analysis involve opinion target extraction target sentiment classification however exist work usually study one two sub task alone hinder practical use paper aim solve complete task target base sentiment analysis end end fashion present novel unify model apply unify tag scheme framework involve two stack recurrent neural network upper one predict unify tag produce final output result primary target base sentiment analysis lower one perform auxiliary target boundary prediction aim guide upper network improve performance primary task explore inter task dependency propose explicitly model constrain transition target boundaries target sentiment polarities also propose maintain sentiment consistency within opinion target via gate mechanism model relation feature current word previous word conduct extensive experiment three benchmark datasets framework achieve consistently superior result
sentence specificity quantify level detail sentence characterize organization information discourse information useful many downstream applications specificity prediction systems predict coarse label binary ternary train tailor toward specific domains eg news goal work generalize specificity prediction domains label data available output nuanced real value specificity rat present unsupervised domain adaptation system sentence specificity prediction specifically design output real value estimate binary train label calibrate value predictions appropriately regularize posterior distribution label towards reference distribution show framework generalize well three different domains five thousand and sixty-eight mean absolute error reduction current state art system train news sentence specificity also demonstrate potential work improve quality informativeness dialogue generation systems
paper report increment state art hate speech detection english hindi code mix tweet compare three typical deep learn model use domain specific embeddings experiment benchmark dataset english hindi code mix tweet observe use domain specific embeddings result improve representation target group improve f score
paper present novel approach multi task learn language understand lu dialogue state track dst task orient dialogue systems multi task train enable share neural network layer responsible encode user utterance lu dst improve performance reduce number network parameters propose framework dst operate set candidate value slot mention far candidate set generate use lu slot annotations current user utterance dialogue act correspond precede system utterance dialogue state estimate previous turn enable dst handle slot large unbounded set possible value deal slot value see train furthermore bridge gap train inference investigate use schedule sample lu output current user utterance well dst output precede turn
ensure readability text often write present due format text format devices help writer effectively convey narrative time help readers pick structure discourse comprehend convey information number linguistic theories discourse structure text however theories consider unformatted text multimedia text contain rich format feature leverage various nlp task paper study discourse feature multimedia text communicative function fulfil context examine multimedia discourse feature use improve information extraction system show discourse text layout feature provide information complementary lexical semantic information commonly use information extraction case study use feature harvest structure subject knowledge geometry textbooks show harvest structure knowledge use improve exist solver geometry problems make accurate well explainable
corpus phonetics become increasingly popular method research linguistic analysis advance speech technology computational power large scale process speech data become viable technique tutorial introduce speech scientist engineer various automatic speech process tool include acoustic model creation force alignment use kaldi automatic speech recognition toolkit povey et al two thousand and eleven force alignment use fave align rosenfelder et al two thousand and fourteen montreal force aligner mcauliffe et al two thousand and seventeen penn phonetics lab force aligner yuan liberman two thousand and eight well stop consonant burst alignment use autovot keshet et al two thousand and fourteen tutorial provide general overview program step step instructions run program well several tip trick
problem short text match formulate follow give pair sentence question match model determine whether input pair mean model automatically identify question mean wide range applications question answer sit modern chatbots article describe approach team hahu solve problem context cikm analyticup two thousand and eighteen cross lingual short text match question pair sponsor alibaba solution end end system base current advance deep learn avoid heavy feature engineer achieve improve performance traditional machine learn approach log loss score first second round contest thirty-five thirty-nine respectively team rank 7th one thousand and twenty-seven team overall rank scheme organizers consist two contest score well innovation system integrity understand data well practicality solution business
sequence sequence seq2seq model successfully apply automatic math word problem solve despite simplicity drawback still remain math word problem correctly solve one equations non deterministic transduction harm performance maximum likelihood estimation paper consider uniqueness expression tree propose equation normalization method normalize duplicate equations moreover analyze performance three popular seq2seq model math word problem solve find model specialty solve problems consequently ensemble model propose combine advantage experiment dataset math23k show ensemble model equation normalization significantly outperform previous state art methods
discourse coherence play important role translation one text however previous report model focus improve performance individual sentence ignore cross sentence link dependencies affect coherence text paper propose use discourse context reward refine translation quality discourse perspective particular generate translation individual sentence first next deliberate preliminary produce translations train model learn policy produce discourse coherent text reward teacher practical result multiple discourse test datasets indicate model significantly improve translation quality state art baseline system one hundred and twenty-three bleu score moreover model generate discourse coherent text obtain twenty-two bleu improvements evaluate discourse metrics
online review platforms popular way users post review express opinions towards product service well valuable users company find overall opinions customers review tend accompany rat star rat become common approach users give feedback quantitative way generally likert scale one five star social media platforms like facebook twitter automate review rat prediction system useful determine rat user would give product service exist work review rat prediction focus specific domains restaurants hotels however ignore fact review domains less frequently rat dentists lack sufficient data build reliable prediction model paper experiment twelve datasets pertain twelve different review domains vary level popularity assess performance predictions across different domains introduce model leverage aspect phrase embeddings extract review enable development domain cross domain review rat prediction systems experiment show review rat prediction systems outperform baselines cross domain review rat prediction system particularly significant least popular review domains leverage train data domains lead remarkable improvements performance domain review rat prediction system instead suitable popular review domains provide model build train data pertain target domain suitable data abundant
neural generative model become popular achieve promise performance short text conversation task generally train build one one map input post output response however give post often associate multiple reply simultaneously real applications previous research task mainly focus improve relevance informativeness top one generate response post work study generate multiple accurate diverse responses post paper propose novel response generation model consider set responses jointly generate multiple diverse responses simultaneously reinforcement learn algorithm design solve model experiment two short text conversation task validate multiple responses generate model obtain higher quality larger diversity compare various state art generative model
automatic storytelling challenge since require generate long coherent natural language describe sensible sequence events despite considerable efforts automatic story generation past prior work either restrict plot plan generate stories narrow domain paper explore open domain story generation write stories give title topic input propose plan write hierarchical generation framework first plan storyline generate story base storyline compare two plan strategies dynamic schema interweave story plan surface realization text static schema plan entire storyline generate stories experiment show explicit storyline plan generate stories diverse coherent topic generate without create full plan accord automatic human evaluations
previous work bridge anaphora resolution poesio et al two thousand and four hou et al 2013b use syntactic preposition pattern calculate word relatedness however pattern consider nps head nouns hence fully capture semantics nps recently hou two thousand and eighteen create word embeddings embeddingspp capture associative similarity ie relatedness nouns explore syntactic structure noun phrase embeddingspp contain word representations nouns paper create new word vectors combine embeddingspp glove new word embeddings embeddingsbridging general lexical knowledge resource bridge allow us represent mean np beyond head easily therefore develop deterministic approach bridge anaphora resolution represent semantics np base head noun modifications show simple approach achieve competitive result compare best system hou et al2013b explore markov logic network model problem additionally improve result bridge anaphora resolution report hou two thousand and eighteen combine simple deterministic approach hou et al2013b best system mln ii
bias language commonly occur around topics controversial nature thus stir disagreement different involve party discussion due fact language use specifically understand use phrase stances cohesive within particular group however cohesiveness hold across group collaborative environments environments impartial language desire eg wikipedia news media statements language therein represent equally involve party neutrally phrase bias language introduce presence inflammatory word phrase statements may incorrect one side thus violate consensus work focus specific case phrase bias may introduce specific inflammatory word phrase statement purpose propose approach rely recurrent neural network order capture inter dependencies word phrase introduce bias perform thorough experimental evaluation show advantage neural base approach competitors rely word lexicons hand craft feature detect bias language able distinguish bias statements precision p092 thus significantly outperform baseline model improvement thirty finally release largest corpus statements annotate bias language
opinion mine mainly involve three elements feature feature relations opinion expressions relate opinion attribute eg polarity feature opinion relations although many work emerge achieve aim gain information previous research typically handle three elements isolation give sufficient information extraction result hence complexity run time information extraction increase paper propose opinion mine extraction algorithm jointly discover main opinion mine elements specifically algorithm automatically build kernels combine closely relate word new term word level phrase level base dependency relations ensure accuracy opinion expressions polarity base fuzzy measurements opinion degree intensifiers opinion pattern three thousand, four hundred and fifty-eight analyze review show propose algorithm effectively identify main elements simultaneously outperform baseline methods propose algorithm use analyze feature among heterogeneous products category feature feature comparison help select weaker feature recommend correct specifications begin life product comparison interest observations reveal example negative polarity video dimension higher product usability dimension product yet enhance dimension product usability effectively improve product c two thousand and fifteen elsevier ltd right reserve
dependency grammar induction task learn dependency syntax without annotate train data traditional graph base model global inference achieve state art result task require ofn3 run time transition base model enable faster inference ofn time complexity performance still lag behind work propose neural transition base parser dependency grammar induction whose inference procedure utilize rich neural feature ofn time complexity train parser integration variational inference posterior regularization variance reduction techniques result framework outperform previous unsupervised transition base dependency parsers achieve performance comparable graph base model english penn treebank universal dependency treebank empirical comparison show approach substantially increase parse speed graph base model
paper present adapt system build basque english low resource mt evaluation campaign basque low resourced morphologically rich language pose challenge neural machine translation model usually achieve better performance train large set data accordingly use synthetic data improve translation quality produce model build use authentic data proposal use back translate data create new sentence system train data b translate sentence close test set model fine tune document translate
much effort devote evaluate whether multi task learn leverage learn rich representations use various natural language process nlp stream applications however still lack understand settings multi task learn significant effect work introduce hierarchical model train multi task learn setup set carefully select semantic task model train hierarchical fashion introduce inductive bias supervise set low level task bottom layer model complex task top layer model model achieve state art result number task namely name entity recognition entity mention detection relation extraction without hand engineer feature external nlp tool like syntactic parsers hierarchical train supervision induce set share semantic representations lower layer model show move bottom top layer model hide state layer tend represent complex semantic information
languages constant flux drive external factor cultural societal technological change well partially understand internal motivations word acquire new mean lose old sense new word coin borrow languages obsolete word slide obscurity understand characteristics shift mean use word useful work content historical texts interest general public also find automatic lexical semantic change detection model diachronic conceptual change currently incorporate approach measure document across time similarity information retrieval long term document archive design ocr algorithms recent years see surge interest academic community computational methods tool support inquiry diachronic conceptual change lexical replacement article extract survey recent computational techniques tackle lexical semantic change currently review article focus diachronic conceptual change extension semantic change
structure query express languages sql sparql xquery offer convenient explicit way users express information need number task work present approach answer directly text data without store result database specifically look case knowledge base query entities relations approach combine distribute query answer eg triple pattern fragment model build extractive question answer importantly apply distribute query answer able simplify model learn problem train model large portion five hundred and seventy-two relations within wikidata achieve average seventy f1 measure across model also present systematic method construct necessary train data task knowledge graph describe prototype implementation
end end e2e model directly predict output character sequence give input speech good candidates device speech recognition e2e model however present numerous challenge order truly useful model must decode speech utterances stream fashion real time must robust long tail use case must able leverage user specific context eg contact list must extremely accurate work describe efforts build e2e speech recognizer use recurrent neural network transducer experimental evaluations find propose approach outperform conventional ctc base model term latency accuracy number evaluation categories
mine entity synonym set ie set term refer entity important task many entity leverage applications previous work either rank term base similarity give query term treat problem two phase task ie detect synonymy pair follow organize pair synonym set however approach fail model holistic semantics set suffer error propagation issue propose new framework name synsetmine efficiently generate entity synonym set give vocabulary use example set external knowledge base distant supervision synsetmine consist two novel modules one set instance classifier jointly learn represent permutation invariant synonym set whether include new instance ie term set two set generation algorithm enumerate vocabulary apply learn set instance classifier detect entity synonym set experiment three real datasets different domains demonstrate effectiveness efficiency synsetmine mine entity synonym set
analyze gendered communities define three different ways text users sentiment differences across representations reveal facets communities distinctive identities social group topic attitudes two communities may high text similarity user similarity vice versa word usage also vary accord clearcut binary perspective gender community specific sentiment lexicons demonstrate sentiment useful indicator word social mean community value especially context discussion content user demographics result show social platforms reddit active settings different constructions gender
news headline mislead readers overrate false information identify advance better assist readers choose proper news stories consume research introduce million scale pair news headline body text dataset incongruity label uniquely utilize detect news stories mislead headline dataset develop two neural network hierarchical architectures model complex textual representation news article measure incongruity headline body text also present data augmentation method dramatically reduce text input size model handle independently investigate paragraph news stories boost performance experiment qualitative evaluations demonstrate propose methods outperform exist approach efficiently detect news stories mislead headline real world
present method learn bilingual translation dictionaries english bantu languages show exploit grammatical structure common bantu languages enable bilingual dictionary induction languages train data unavailable
consider problem recognize mention human sense text contribution method acquire label data learn method train data experiment show effectiveness propose data label approach learn model task sense recognition text
often miss exist knowledge base facts relationships encode common sense knowledge unnamed entities paper propose extract novel common sense relationships pertain sense perception concepts sound smell
citation function provenance two cornerstone task citation analysis give citation former task determine rhetorical role latter locate text cite paper contain relevant cite information hypothesize two task synergistically relate build model validate claim task show single layer convolutional neural network cnn outperform exist state art baselines importantly show two task indeed synergistic jointly train task multi task learn setup demonstrate additional performance gain altogether model improve current state arts two statistical significance citation function provenance prediction task
geolocation online information essential component geospatial application previous work geolocation focus twitter paper quantify compare performance text base geolocation methods social media data draw blogger twitter introduce novel set location specific feature highly informative easily interpretable show achieve error rate reductions one hundred and twenty-five respect best previously propose geolocation feature also show despite post longer text blogger users significantly harder geolocate twitter users additionally investigate effect train test different media cross media predictions combine multiple social media source multi media predictions finally explore geolocability social media relation three user dimension state gender industry
build open domain multi turn conversation system one interest challenge task artificial intelligence many research efforts dedicate build dialogue systems yet would light model conversation flow ongoing dialogue besides common people talk highly relevant aspects conversation topics coherent drift naturally demonstrate necessity dialogue flow model end present multi turn cue word drive conversation system reinforcement learn method rlcw strive select adaptive cue word greatest future credit therefore improve quality generate responses introduce new reward measure quality cue word term effectiveness relevance optimize model long term conversations reinforcement approach adopt paper experiment real life dataset demonstrate model consistently outperform set competitive baselines term simulate turn diversity human evaluation
manner articulation detection use deep neural network require priori knowledge attribute discriminative feature decent phoneme alignments however generate appropriate phoneme alignment complex performance depend choice optimal number senones gaussians etc first part work exploit manner articulation detection use connectionist temporal classification ctc need phoneme alignment later modify state art character base posteriors generate ctc use manner articulation ctc detector beam search decode perform modify posteriors impact open source datasets an4 librispeech observe
detect deception natural language wide variety applications hide nature currently public large scale source label deceptive text work introduce mafiascum dataset one collection seven hundred game mafia players randomly assign either deceptive non deceptive roles interact via forum post nine thousand document compile dataset contain message write single player single game corpus use construct set hand pick linguistic feature base prior deception research well set average word vectors enrich subword information logistic regression classifier fit combination feature set achieve average precision thirty-nine chance twenty-six auroc sixty-eight five thousand word document fifty word document average precision twenty-nine chance twenty-three auroc fifty-nine achieve one https bitbucketorg bopjesvla thesis src
extractive summarization useful physicians better manage digest electronic health record ehrs however train supervise model require disease specific medical background thus expensive study utilize intrinsic correlation multiple ehrs generate pseudo label train supervise model external annotation experiment real patient data validate model effective summarize crucial disease specific information patients
many natural language question require recognize reason qualitative relationships eg science economics medicine challenge answer corpus base methods qualitative model provide tool support reason semantic parse task map question model formidable challenge present quarel dataset diverse story question involve qualitative relationships characterize challenge techniques begin address dataset two thousand, seven hundred and seventy-one question relate nineteen different type quantities example jenny observe robot vacuum cleaner move slower live room carpet bedroom carpet carpet friction contribute one simple flexible conceptual framework represent kinds question two quarel dataset include logical form exemplify parse challenge three two novel model task build extensions type constrain semantic parse first model call quasp significantly outperform shelf tool quarel second quaspzero demonstrate zero shoot capability ie ability handle new qualitative relationships without require additional train data something possible previous model work thus make inroads answer complex qualitative question require reason scale new relationships low cost dataset model available http dataallenaiorg quarel
many approach propose tackle problem abstract mean representation amr parse help solve various natural language process issue recently paper provide overview different methods amr parse performances analyze legal document conduct experiment different amr parsers annotate dataset extract english version japanese civil code result show limitations well open room improvements current parse techniques apply complicate domain
although generation base dialogue systems widely research response generations exist systems low diversities likely reason problem maximum likelihood estimation mle softmax cross entropy sce loss mle train model generate frequent responses enormous generation candidates although actual dialogues various responses base context paper propose new objective function call inverse token frequency itf loss individually scale smaller loss frequent token class larger loss rare token class function encourage model generate rare tokens rather frequent tokens complicate model train stable replace objective function opensubtitles dialogue dataset loss model establish state art dist one seven hundred and fifty-six unigram diversity score maintain good bleu one score japanese twitter reply dataset loss model achieve dist one score comparable grind truth
recently large number neural mechanisms model propose sequence learn self attention exemplify transformer model graph neural network gnns attract much attention paper propose approach combine draw complementary strengths two methods specifically propose contextualized non local neural network cntextbf3 dynamically construct task specific structure sentence leverage rich local dependencies within particular neighborhood experimental result ten nlp task text classification semantic match sequence label show propose model outperform competitive baselines discover task specific dependency structure thus provide better interpretability users
entity link aim link entity mention texts knowledge base neural model achieve recent success task however exist methods rely local contexts resolve entities independently may usually fail due data sparsity local information address issue propose novel neural model collective entity link name ncel ncel apply graph convolutional network integrate local contextual feature global coherence information entity link improve computation efficiency approximately perform graph convolution subgraph adjacent entity mention instead entire text introduce attention scheme improve robustness ncel data noise train model wikipedia hyperlinks avoid overfitting domain bias experiment evaluate ncel five publicly available datasets verify link performance well generalization ability also conduct extensive analysis time complexity impact key modules qualitative result demonstrate effectiveness efficiency propose method
machine read comprehension mrc multiple choice question require machine read give passage select correct answer among several candidates paper propose novel approach call convolutional spatial attention csa model better handle mrc multiple choice question propose model could fully extract mutual information among passage question candidates form enrich representations furthermore merge various attention result propose use convolutional operation dynamically summarize attention value within different size regions experimental result show propose model could give substantial improvements various state art systems race semeval two thousand and eighteen task11 datasets
ambiguities introduce recombination morphemes construct several possible inflections word make prediction syntactic traits morphologically rich languages mrls notoriously complicate task propose multi task deep morphological analyzer mt dma character level neural morphological analyzer base multitask learn word level tag markers hindi urdu mt dma predict set six morphological tag word indo aryan languages part speech pos gender g number n person p case c tense aspect modality tam marker well lemma l jointly learn one trainable framework show effectiveness train deep neural network simultaneous optimization multiple loss function share initial parameters context aware morphological analysis exploit character level feature phonological space optimize tag use multi objective genetic algorithm model establish new state art accuracy score upon seven task languages mt dma publicly accessible code model data available https githubcom saurav0074 morphanalyzer
natural language process deep learn revolution shift focus conventional hand craft symbolic representations dense input adequate representations learn automatically corpora however particularly work low resource languages small amount symbolic lexical resources user generate lexicons often available even gold standard corpora additional linguistic information though often neglect recent neural approach cross lingual tag typically rely word subword embeddings representations effective recent work show clear benefit combine best worlds integrate conventional lexical information improve neural cross lingual part speech pos tag however little know complementary additional information extent improvements depend coverage quality external resources paper seek fill gap provide first thorough analysis contributions lexical resources cross lingual pos tag neural time
vocabulary oov word pose serious challenge machine translation mt task particular low resource language lrl pair ie language pair parallel corpora exist work adapt variants seq2seq model perform transduction word hindi bhojpuri lrl instance learn set cognate pair build bilingual dictionary hindi bhojpuri word demonstrate model effectively use language pair limit parallel corpora model work character level grasp phonetic orthographic similarities across multiple type word adaptations whether synchronic diachronic loan word cognates describe train aspects several character level nmt systems adapt task characterize typical errors method improve bleu score sixty-three hindi bhojpuri translation task show transductions generalize well languages apply successfully hindi bangla cognate pair work see important step process resolve oov word problem arise mt task ii create effective parallel corpora resource constrain languages iii leverage enhance semantic knowledge capture word level embeddings perform character level task
word sense induction wsi task automatically discover multiple sense mean word three main challenge domain adaptability novel sense detection sense granularity flexibility current latent variable model know solve first two challenge flexible different word sense granularities differ much among word aardvark one sense play fifty sense current model either require hyperparameter tune nonparametric induction number sense find ineffective thus aim eliminate requirements solve sense granularity problem propose autosense latent variable model base two observations one sense represent distribution topics two sense generate pair target word neighbor word observations alleviate problem throw garbage sense b additionally induce fine grain word sense result show great improvements state art model popular wsi datasets also show autosense able learn appropriate sense granularity word finally apply autosense unsupervised author name disambiguation task sense granularity problem evident show autosense evidently better compete model share data code https githubcom rktamplayo autosense
propose segmental neural language model combine generalization power neural network ability discover word like units latent unsegmented character sequence contrast previous segmentation model treat word segmentation isolate task model unify word discovery learn word fit together form sentence condition model visual context word mean grind representations non linguistic modalities experiment show unconditional model learn predictive distributions better character lstm model discover word competitively nonparametric bayesian word segmentation model model language conditional visual context improve performance
biomedical domain lack sharable datasets often limit possibility develop natural language process systems especially dialogue applications natural language understand model overcome issue explore data generation use templates terminologies data augmentation approach namely report experiment use paraphrase word representations learn large ehr corpus fasttext elmo learn nlu model without available dataset evaluate nlu task natural language query ehrs divide slot fill intent classification sub task slot fill task obtain f score seventy-six elmo representation classification task mean f score seventy-one result show method could use develop baseline system
recent years sequence sequence learn neural network attention mechanism achieve great progress however still challenge especially neural machine translation nmt lower translation quality long sentence paper present hierarchical deep neural network architecture improve quality long sentence translation propose network embed sequence sequence neural network two level category hierarchy follow coarse fine paradigm long sentence input split shorter sequence well process coarse category network long distance dependencies short sentence able handle network base sequence sequence neural network concatenate correct fine category network experiment show method achieve superior result higher bleubilingual evaluation understudy score lower perplexity better performance imitate expression style word usage traditional network
article describe new approach quality improvement automate dialogue systems customer support service analysis produce paper demonstrate dependency quality retrieval base dialogue system quality choice negative responses propose approach imply choose negative sample accord distribution responses train set implementation negative sample randomly choose original response distribution artificial distribution negative responses uniform distribution distribution obtain transformation original one result obtain implement systems report paper confirm significant improvement automate dialogue systems quality case use negative responses transform distribution
deep learn base natural language process model prove powerful need large scale dataset due significant gap real world task exist chinese corpus paper introduce large scale corpus informal chinese corpus contain around thirty-seven million book review fifty thousand netizen comment news explore informal word frequencies corpus show difference corpus exist ones corpus use train deep learn base natural language process task chinese word segmentation sentiment analysis
previously researchers pay attention creation unambiguous morpheme embeddings independent corpus information play important role express exact mean word parataxis languages like chinese paper construct chinese lexical semantic ontology base word formation propose novel approach implant structure rational knowledge distribute representation morpheme level naturally avoid heavy disambiguation corpus design template create instance pseudo sentence merely piece knowledge morphemes build lexicon exploit hierarchical information tackle data sparseness problem instance proliferation technique apply base similarity expand collection pseudo sentence distribute representation morphemes train pseudo sentence use word2vec evaluation validate paradigmatic syntagmatic relations morpheme embeddings apply obtain embeddings word similarity measurement achieve significant improvements classical model five spearman score eight percentage point show promise prospect adoption new source knowledge
present two architectures multi task learn neural sequence model approach allow relationships different task learn dynamically rather use ad hoc pre define structure previous work adopt idea message pass graph neural network propose general textbfgraph multi task learn framework different task communicate effective interpretable way conduct extensive experiment text classification sequence label evaluate approach multi task learn transfer learn empirical result show model outperform competitive baselines also learn interpretable transferable pattern across task
article present rule base approach transliterate two mostly use orthographies sorani kurdish work consist detect character word remove possible ambiguities map target orthography describe different challenge kurdish text mine propose novel ideas concern transliteration task sorani kurdish transliteration system name wergor achieve eight thousand, two hundred and seventy-nine overall precision ninety-nine detect double usage character also present manually transliterate corpus kurdish
name entity recognition ner one task natural language process greatly benefit use external knowledge source propose name entity recognition framework compose knowledge base feature extractors deep learn model include contextual word embeddings long short term memory lstm layer conditional random field crf inference layer use entity link module integrate system wikipedia combination effective neural architecture external resources allow us obtain state art result recognition polish proper name evaluate model data poleval two thousand and eighteen ner challenge outperform methods reduce error rate two hundred and twenty-four compare win solution work show combine neural ner model entity link model knowledge base effective recognize name entities use ner model alone
work present novel approach leverage lexical information speaker diarization introduce speaker diarization system directly integrate lexical well acoustic information speaker cluster process thus propose adjacency matrix integration technique integrate word level speaker turn probabilities speaker embeddings comprehensive way propose method work without reference transcript word word boundary information provide asr system show propose method improve baseline speaker diarization system solely base speaker embeddings achieve meaningful improvement callhome american english speech dataset
verbs occur different syntactic environments frame investigate whether artificial neural network encode grammatical distinctions necessary infer idiosyncratic frame selectional properties verbs introduce five datasets collectively call fava contain aggregate nearly 10k sentence label grammatical acceptability illustrate different verbal argument structure alternations test whether model distinguish acceptable english verb frame combinations unacceptable ones use sentence embed alone converge evidence construct lava correspond word level dataset investigate whether syntactic feature extract word embeddings model perform reliable classifications verbal alternations others suggest representations encode fine grain lexical information incomplete hard extract differences word sentence level model show information present word embeddings pass stream sentence embeddings
joint representation learn word entities benefit many nlp task well explore cross lingual settings paper propose novel method joint representation learn cross lingual word entities capture mutually complementary knowledge enable cross lingual inferences among knowledge base texts method require parallel corpora automatically generate comparable data via distant supervision use multi lingual knowledge base utilize two type regularizers align cross lingual word entities design knowledge attention cross lingual attention reduce noise conduct series experiment three task word translation entity relatedness cross lingual entity link result qualitatively quantitatively demonstrate significance method
present result first fact extraction verification fever share task task challenge participants classify whether human write factoid claim could support refute use evidence retrieve wikipedia receive entries twenty-three compete team nineteen score higher previously publish baseline best perform system achieve fever score six thousand, four hundred and twenty-one paper present result share task summary systems highlight commonalities innovations among participate systems
speech classifiers paralinguistic traits traditionally learn diverse hand craft low level feature select relevant information task hand explore alternative selection learn jointly classifier feature extraction recent work speech recognition show improve performance speech feature learn waveform extend approach paralinguistic classification propose neural network learn filterbank normalization factor compression power raw speech jointly rest architecture apply model dysarthria detection sentence level audio record start strong attention base baseline mel filterbanks outperform standard low level descriptors show learn filter normalization compression improve fix feature ten absolute accuracy also observe gain opensmile feature learn jointly feature extraction normalization compression factor architecture constitute first attempt learn jointly operations raw audio speech classification task
slot fill paradigm user refer back slot context conversation goal contextual understand system resolve refer expressions appropriate slot context paper build context carryover systemcitepnaik2018contextualsc provide scalable multi domain framework resolve reference however scale approach across languages trivial task due large demand acquisition annotate data target language main focus cross lingual methods reference resolution way alleviate need annotate data target language cross lingual setup assume access annotate resources well well train model source language little annotate data target language paper explore three different approach cross lingual transfer textemdash delexicalization data augmentation multilingual embeddings machine translation compare approach low resource set well large resource set experiment show multilingual embeddings delexicalization via data augmentation significant impact low resource set gain diminish amount available data target language increase furthermore combine machine translation get performance close actual live data target language twenty-five data project target language
fundamental trade effectiveness efficiency need balance design online question answer system effectiveness come sophisticate function extractive machine read comprehension mrc efficiency obtain improvements preliminary retrieval components candidate document selection paragraph rank give complexity real world multi document mrc scenario difficult jointly optimize end end system address problem develop novel deep cascade learn model progressively evolve document level paragraph level rank candidate texts precise answer extraction machine read comprehension specifically irrelevant document paragraph first filter simple function efficiency consideration jointly train three modules remain texts better track answer document extraction paragraph extraction answer extraction experiment result show propose method outperform previous state art methods two large scale multi document benchmark datasets ie triviaqa dureader addition online system stably serve typical scenarios millions daily request less 50ms
dialog response rank use rank response candidates consider relation dialog history although researchers address concept open domain dialogs little attention focus task orient dialogs furthermore previous study analyze whether response rank improve performance exist dialog systems real human computer dialogs speech recognition errors paper propose context aware dialog response rank system system reranks responses two step one calculate match score candidate response current dialog context two combine match score probability distribution candidates exist dialog system response rank use neural word embed base model handcraft logistic regression base ensemble model improve performance recently propose end end task orient dialog system real dialogs speech recognition errors
paper describe novel hierarchical attention network read comprehension style question answer aim answer question give narrative paragraph propose method attention fusion conduct horizontally vertically across layer different level granularity question paragraph specifically first encode question paragraph fine grain language embeddings better capture respective representations semantic level propose multi granularity fusion approach fully fuse information global attend representations finally introduce hierarchical attention network focus answer span progressively multi level softalignment extensive experiment large scale squad triviaqa datasets validate effectiveness propose method time write paper january 12th two thousand and eighteen model achieve first position squad leaderboard single ensemble model also achieve state art result triviaqa addsent addone send datasets
supervise learn limit quantity quality label data field medical record tag write style hospitals vary drastically knowledge learn one hospital might transfer well another problem amplify veterinary medicine domain veterinary clinics rarely apply medical cod record propose train first large scale generative model algorithm automate disease cod demonstrate generative model learn discriminative feature additionally train supervise fine tune systematically ablate evaluate effect generative model final system performance compare performance model several baselines challenge cross hospital set substantial domain shift outperform competitive baselines large margin addition provide interpretation learn model
neural network model show great success natural language inference nli task determine whether premise entail hypothesis however recent study suggest model may rely fallible heuristics rather deep language understand introduce challenge set test whether nli systems adopt one heuristic assume sentence entail subsequences assume alice believe mary lie entail alice believe mary evaluate several competitive nli model challenge set find strong evidence rely subsequence heuristic
neural network base dialog model often lack robustness anomalous domain ood user input lead unexpected dialog behavior thus considerably limit model usage mission critical production environments problem especially relevant set dialog system bootstrapping limit train data access ood examples paper explore problem robustness systems anomalous input associate trade accuracies see unseen data present new dataset study robustness dialog systems ood input babi dialog task six augment ood content control way present turn dropout simple yet efficient negative sample base technique improve robustness neural dialog model demonstrate effectiveness apply hybrid code network family model hcns reach state art result ood augment dataset well original one specifically hcn train turn dropout achieve state art performance seventy-five per utterance accuracy augment dataset ood turn seventy-four f1 score ood detector furthermore introduce variational hcn enhance turn dropout achieve five hundred and sixty-five accuracy original babi task six dataset thus outperform initially report hcn result
hypernym discovery problem find term relationship give term introduce new context type relatedness measure differentiate hypernyms type semantic relationships document structure measure base hierarchical position term document presence otherwise definition text measure quantify document structure use multiple attribute class weight distance function
oral drug become increasingly common oncology care contrast intravenous chemotherapy administer clinic carefully track via structure electronic health record ehrs oral drug treatment self administer therefore track well often detail oral cancer treatment occur unstructured clinic note extract information critical understand patient treatment history yet challenge task treatment intervals must infer longitudinally explicit mention text well document timestamps work present tifti temporally integrate framework treatment intervals robust framework extract oral drug treatment intervals patient unstructured note tifti leverage distinct source temporal information break problem two separate subtasks document level sequence label date extraction label dataset metastatic renal cell carcinoma rcc patients exactly match label start date forty-six examples eighty-six examples within thirty days exactly match label end date fifty-two examples seventy-eight examples within thirty days without retrain model achieve similar level performance label dataset advance non small cell lung cancer nsclc patients
open end human chatbot interaction become commonplace sensitive content detection gain importance work propose two stage semi supervise approach bootstrap large scale data automatic sensitive language detection publicly available web resources explore various data selection methods include one use blacklist rank online discussion forums level sensitiveness follow randomly sample utterances two train weakly supervise model conjunction blacklist score sentence online discussion forums curate dataset data collection strategy flexible allow model detect implicit sensitive content manual annotations may difficult train model use publicly available annotate datasets well use propose large scale semi supervise datasets evaluate performance model twitter toxic wikipedia comment testsets well manually annotate speak language dataset collect large scale chatbot competition result show model train collect data outperform baseline model large margin domain domain testsets achieve f1 score nine hundred and fifty-five domain testset compare score seventy-five model train public datasets also showcase large scale two stage semi supervision generalize well across multiple class sensitivities hate speech racism sexual pornographic content etc without even provide explicit label class lead average recall nine hundred and fifty-five versus model train use annotate public datasets achieve average recall seven hundred and thirty-two across seven sensitive class domain testsets
question answer qa extract answer text give question natural language actively study exist model show promise outperform human performance train evaluate squad dataset however performance may replicate actual set need diagnose non trivial due complexity model thus propose web base ui provide model contribute qa performances integrate visualization analysis tool model explanation expect framework help qa model researchers refine improve model
previous work event extraction mainly focus predictions event trigger argument roles treat entity mention provide human annotators unrealistic entity mention usually predict exist toolkits whose errors might propagate event trigger argument role recognition recent work address problem jointly predict entity mention event trigger arguments however work limit use discrete engineer feature represent contextual information individual task interactions work propose novel model jointly perform predictions entity mention event trigger arguments base share hide representations deep learn experiment demonstrate benefit propose method lead state art performance event extraction
paper present end end response selection model track one 7th dialogue system technology challenge dstc7 task focus select correct next utterance set candidates give partial conversation propose end end neural network base enhance sequential inference model esim task propose model differ original esim model follow four aspects first new word representation method combine general pre train word embeddings estimate task specific train set adopt order address challenge vocabulary oov word second attentive hierarchical recurrent encoder ahre design capable encode sentence hierarchically generate descriptive representations aggregation third new pool method combine multi dimensional pool last state pool use instead simple combination max pool average pool original esim last modification layer add softmax layer emphasize importance last utterance context response selection release evaluation result dstc7 propose method rank second ubuntu dataset third advise dataset subtask one track one
paper present system description machine translation mt systems indic languages multilingual task two thousand and eighteen edition wat share task experiment rgnlp team explore statistical neural methods across language pair present extensive comparison language relate problems approach context low resourced settings pbsmt model highest score automatic evaluation metrics english telugu hindi bengali tamil portion share task
word segmentation task insert delete word boundary character order separate character sequence correspond word language article propose approach base beam search algorithm language model work byte character level latter component implement either n gram model recurrent neural network result system analyze text input word boundaries one token time character byte use information gather language model determine boundary must place current position aim use system preprocessing step microtext normalization system mean need effectively cope data sparsity present kind texts also strive surpass performance two readily available word segmentation systems well know accessible word breaker microsoft python module wordsegment grant jenks result show meet objectives hope continue improve precision efficiency system future
latency current neural base dialogue state track model prohibit use efficiently deployment production systems albeit highly accurate performance paper propose new scalable accurate neural dialogue state track model base recently propose global local self attention encoder glad model zhong et al use global modules share parameters estimators different type call slot dialogue state use local modules learn slot specific feature use one recurrent network global condition compare one slot recurrent network global local condition use glad model propose model reduce latency train inference time thirty-five average preserve performance belief state track nine thousand, seven hundred and thirty-eight turn request eight thousand, eight hundred and fifty-one joint goal accuracy evaluation multi domain dataset multi woz also demonstrate model outperform glad turn inform joint goal accuracy
significant amount information today world store structure semi structure knowledge base efficient simple methods query essential must restrict expertise formal query languages field semantic parse deal convert natural language utterances logical form easily execute knowledge base survey examine various components semantic parse system discuss prominent work range initial rule base methods current neural approach program synthesis also discuss methods operate use vary level supervision highlight key challenge involve learn systems
work present task modify image image edit program use natural language write command utilize corpus six thousand image edit text request alter real world image collect via crowdsourcing novel framework compose action entities map user natural language request executable command image edit program describe resolve previously label annotator disagreement vote process complete annotation corpus experiment different machine learn model find lstm svm bidirectional lstm crf joint model best detect image edit action associate entities give utterance
order machine learn garner widespread public adoption model must able provide interpretable robust explanations decisions well learn human provide explanations train time work extend stanford natural language inference dataset additional layer human annotate natural language explanations entailment relations implement model incorporate explanations train process output test time show corpus explanations call e snli use various goals obtain full sentence justifications model decisions improve universal sentence representations transfer domain nli datasets dataset thus open range research directions use natural language explanations improve model assert trust
multi emotion sentiment classification natural language process nlp problem valuable use case real world data demonstrate large scale unsupervised language model combine finetuning offer practical solution task difficult datasets include label class imbalance domain specific context train attention base transformer network vaswani et al two thousand and seventeen 40gb text amazon review mcauley et al two thousand and fifteen fine tune train set model achieve sixty-nine f1 score semeval task 1e c multi dimensional emotion classification problem mohammad et al two thousand and eighteen base plutchik wheel emotions plutchik one thousand, nine hundred and seventy-nine result competitive state art model include strong f1 score difficult emotion categories fear seventy-three disgust seventy-seven anger seventy-eight well competitive result rare categories anticipation forty-two surprise thirty-seven furthermore demonstrate application real world text classification task create narrowly collect text dataset real tweet several topics show finetuned model outperform general purpose commercially available apis sentiment multidimensional emotion classification dataset significant margin also perform variety additional study investigate properties deep learn architectures datasets algorithms achieve practical multidimensional sentiment classification overall find unsupervised language model finetuning simple framework achieve high quality result real world sentiment classification
neural sequence model achieve great success sentence level sentiment classification however model exceptionally complex base expensive feature model recognize value exist linguistic resource utilize insufficiently paper propose novel general method incorporate lexicon information include sentiment lexicons negation word intensifiers word annotate fine grain coarse grain label propose method first encode fine grain label sentiment embed concatenate word embed second coarse grain label utilize enhance attention mechanism give large weight sentiment relate word experimental result show method increase classification accuracy neural sequence model sst five mr dataset specifically enhance bi lstm model even compare tree lstm use expensive phrase level annotations analysis show case lexicon resource offer right annotations besides propose method capable overcome effect inevitably wrong annotations
presence toxic content become major problem many online communities moderators try limit problem implement refine comment filter toxic users constantly find new ways circumvent hypothesis modify toxic content keywords fool filter easy hide sentiment harder paper explore various aspects sentiment detection correlation toxicity use result implement toxicity detection tool test add sentiment information help detect toxicity three different real world datasets incorporate subversion datasets simulate user try circumvent system result show sentiment information positive impact toxicity detection subversive user
attention mechanism prove effective natural language process paper propose attention boost natural language inference model name aesim add word attention adaptive direction orient attention mechanisms traditional bi lstm layer natural language inference model eg esim make inference model aesim ability effectively learn representation word model local subsentential inference pair premise hypothesis empirical study snli multinli quora benchmarks manifest aesim superior original esim model
automatic text classification tc research use real world problems classification patient discharge summaries medical text report beneficial make medical document understandable doctor however electronic medical record emr texts contain sentence shorter general domain lead lack semantic feature ambiguity semantic tackle challenge propose add word cluster embed deep neural network improve short text classification concretely first use hierarchical agglomerative cluster cluster word vectors semantic space calculate cluster center vector represent implicit topic information word cluster finally expand word vector cluster center vector implement classifiers use cnn lstm respectively evaluate performance propose method conduct experiment public data set trec medical short sentence data set construct release us experimental result demonstrate propose method outperform state art baselines short sentence classification medical domain general domain
grow number applications users daily interact operate near real time chatbots digital companion knowledge work support systems name perform service desire user systems analyze user activity log explicit user input extremely fast particular text content eg form text snippets need process information extraction task regard aforementioned temporal requirements accomplish milliseconds limit number methods apply practically fast methods remain hand deliver worse result slower sophisticate natural language process nlp pipelines paper investigate propose methods real time capable name entity recognition ner first improvement step address word variations induce inflection example present german language approach ontology base make use several language information source like wiktionary evaluate use german wikipedia 94b character whole ner process take considerably less hour since precision recall higher comparably fast methods conclude quality gap high speed methods sophisticate nlp pipelines narrow bite without lose much runtime performance
deep learn nlp domain lack procedures analysis model robustness paper propose framework validate robustness question answer model model explainers propose robust model transgress initial notion semantic similarity induce word embeddings learn human like understand mean test property manipulate question two ways swap important question word one semantically correct synonym two word vector close embed space estimate importance word ask question locally interpretable model agnostic explanations method lime two step compare state art qanda model show although accuracy state art model high fragile change input moreover propose two adversarial train scenarios raise model sensitivity true synonyms seven accuracy measure find help understand model stable improve addition create publish new dataset may use validation robustness qanda model
motivate recent evidence point fragility high perform span prediction model direct attention multiple choice read comprehension particular work introduce novel method improve answer selection long document weight global normalization predictions portion document show apply method span prediction model adapt answer selection help model performance long summaries narrativeqa challenge read comprehension dataset answer selection task strongly improve task baseline performance three hundred and sixty-two mean reciprocal rank
study adapt semantic network adposition case supersenses snacs annotation mandarin chinese demonstrate supersense categories appropriate chinese adposition semantics annotate fifteen chapters little prince high interannotator agreement parallel corpus give insight differences construal two languages adpositions namely number construals frequent chinese rare unattested english corpus annotate corpus support automatic disambiguation adpositions chinese common inventory supersenses two languages potentially serve cross linguistic task machine translation
answer selection knowledge base question answer kbqa two important task question answer qa systems exist methods solve two task separately require large number repetitive work neglect rich correlation information task paper tackle answer selection kbqa task simultaneously via multi task learn mtl motivate follow motivations first answer selection kbqa regard rank problem one text level knowledge level second two task benefit answer selection incorporate external knowledge knowledge base kb kbqa improve learn contextual information answer selection fulfill goal jointly learn two task propose novel multi task learn scheme utilize multi view attention learn various perspectives enable task interact well learn comprehensive sentence representations experiment conduct several real world datasets demonstrate effectiveness propose method performance answer selection kbqa improve also multi view attention scheme prove effective assemble attentive information different representational perspectives
name entity recognition ner classic sequence label task essential component natural language understand nlu systems task orient dialog systems slot fill well decade different methods lookup use gazetteers domain ontology classifiers handcraft feature end end systems involve neural network architectures evaluate mostly language independent non conversational settings paper evaluate modify version recent state art neural architecture conversational set message often short noisy perform array experiment different combinations include previous utterance dialogue source additional feature use word character level embeddings train larger external corpus methods evaluate combine dataset form two public english task orient conversational datasets belong travel restaurant domains respectively additional evaluation also repeat experiment add automatically translate transliterate translate versions english dataset
present system keyword spot except frontend component feature generation entirely contain deep neural network dnn model train end end predict presence keyword stream audio main contributions work first efficient memoized neural network topology aim make better use parameters associate computations dnn hold memory previous activations distribute depth dnn second contribution method train dnn end end produce keyword spot score system significantly outperform previous approach term quality detection well size computation
intent detection essential component task orient dialogue systems years extensive research conduct result many state art model direct towards resolve user intents dialogue variety vector representations foruser utterances explore however model vectorization approach evaluate single language environment dialogude systems generally deal query different languages thus conduct experiment across combinations model various vectors representations code mix well multi language utterances evaluate model scale multi language environment aim find best suitable combination vector representation model process intent detection code mix utterances evaluate experiment two different datasets consist code mix utterances dataset consist english hindi code mix english hindi utterances
question answer one important difficult applications border information retrieval natural language process especially talk complex science question require form inference determine correct answer paper present two step method combine information retrieval techniques optimize question answer deep learn model natural language inference order tackle multi choice question answer science domain question answer pair use standard retrieval base model find relevant candidate contexts decompose main problem two different sub problems first assign correctness score candidate answer base context use retrieval model lucene second use deep learn architectures compute candidate answer infer well choose context consist sentence retrieve knowledge base end solvers combine use simple neural network predict correct answer propose two step model outperform best retrieval base solver three absolute accuracy
aspect category detection one important challenge subtasks aspect base sentiment analysis give set pre define categories task aim detect categories indicate implicitly explicitly give review sentence supervise machine learn approach perform well accomplish subtask note performance methods depend availability label train data often difficult costly obtain besides supervise methods require feature engineer perform well paper propose unsupervised method address aspect category detection task without need feature engineer method utilize cluster unlabeled review soft cosine similarity measure accomplish aspect category detection task experimental result semeval two thousand and fourteen restaurant dataset show propose unsupervised approach outperform several baselines substantial margin
conversational question answer cqa novel qa task require understand dialogue context different traditional single turn machine read comprehension mrc task cqa include passage comprehension coreference resolution contextual understand paper propose innovate contextualized attention base deep neural network sdnet fuse context traditional mrc model model leverage inter attention self attention comprehend conversation context extract relevant information passage furthermore demonstrate novel method integrate latest bert contextual model empirical result show effectiveness model set new state art result coqa leaderboard outperform previous best model sixteen f1 ensemble model improve result twenty-seven f1
unsupervised word embeddings become popular approach word representation nlp task however limitations semantics represent unsupervised embeddings inadequate fine tune embeddings lead suboptimal performance propose novel learn technique call delta embed learn apply general nlp task improve performance optimize tune word embeddings structure regularization apply embeddings ensure tune incremental way result tune word embeddings become better word representations absorb semantic information supervision without forget apply method various nlp task see consistent improvement performance evaluation also confirm tune word embeddings better semantic properties
distantly supervise relation extraction methods train extractor automatically align relation instance knowledge base kb unstructured text addition relation instance kbs often contain relevant side information aliases relations eg found co found aliases relation founderofcompany model usually ignore readily available side information paper propose reside distantly supervise neural relation extraction method utilize additional side information kbs improve relation extraction use entity type relation alias information impose soft constraints predict relations reside employ graph convolution network gcn encode syntactic information text improve performance even limit side information available extensive experiment benchmark datasets demonstrate reside effectiveness make reside source code available encourage reproducible research
explore performance latent variable model conditional text generation context neural machine translation nmt similar zhang et al augment encoder decoder nmt paradigm introduce continuous latent variable model feature translation process extend model co attention mechanism motivate parikh et al inference network compare vision domain latent variable model text face additional challenge due discrete nature language namely posterior collapse experiment different approach mitigate issue show conditional variational model improve upon discriminative attention base translation variational baseline present zhang et al finally present exploration learn latent space illustrate latent variable capable capture first report conditional variational model text meaningfully utilize latent variable without weaken translation model
language model lm interactive speech recognition systems train large amount data model parameters optimize past user data new application intents interaction type release systems time impose challenge adapt lms since exist train data longer sufficient model future user interactions unclear adapt lms new application intents without degrade performance exist applications paper propose solution estimate n gram count directly hand write grammar train lms b use constrain optimization optimize system parameters future use case degrade performance past usage evaluate approach new applications intents personal assistant system find adaptation improve word error rate fifteen new applications even adaptation data available application
product name recognition significant practical problem spur greater availability platforms discuss products social media product review functionalities online marketplaces customers product manufacturers online marketplaces may want identify product name unstructured text extract important insights sentiment surround product much extant research product name identification domain specific eg identify mobile phone model use supervise semi supervise methods massive number new products release market every year methods may require retrain update label data stay relevant may transfer poorly across domains research address challenge develop domain agnostic unsupervised algorithm identify product name base facebook post algorithm consist two general step candidate product name identification use shelf pretrained conditional random field crf model part speech tag set simple pattern b filter candidate name remove spurious entries use cluster word embeddings generate data
maximum likelihood estimation mle widely use sequence sequence task model train uniformly treat generation prediction target token multi class classification yield non smooth prediction probabilities target sequence tokens predict small probabilities tokens large probabilities accord empirical study find non smoothness probabilities result low quality generate sequence paper propose sentence wise regularization method aim output smooth prediction probabilities tokens target sequence propose method automatically adjust weight gradients token one sentence ensure predictions sequence uniformly well experiment three neural machine translation task one text summarization task show method outperform conventional mle loss task achieve promise bleu score wmt14 english german wmt17 chinese english translation task
people naturally understand emotions often also empathize around paper predict emotional valence empathic listener time listen speaker narrate life story use dataset provide omg empathy prediction challenge workshop hold conjunction ieee fg two thousand and nineteen present multimodal lstm model feature level fusion local attention predict empathic responses audio text visual feature best perform model use audio text feature achieve concordance correlation coefficient ccc twenty-nine thirty-two validation set generalize personalize track respectively achieve ccc fourteen fourteen hold test set discuss difficulties face lessons learn tackle challenge
present article identify qualitative differences statistical machine translation smt neural machine translation nmt output try answer two important question one nmt perform equivalently well respect smt two add extra flavor improve quality mt output employ simple sentence train units order obtain insights develop three core model viz smt model base moses toolkit follow character word level nmt model systems use english hindi english bengali language pair contain simple sentence well sentence complexity order preserve translations semantics respect target word sentence employ soft attention word level nmt model evaluate systems respect scenarios succeed fail finally quality translation validate use bleu ter metrics along manual parameters like fluency adequacy etc observe nmt outperform smt case simple sentence whereas smt outperform case type sentence
negative medical find prevalent clinical report yet discriminate positive find remain challenge task information extraction exist systems treat task pipeline two separate task ie name entity recognition ner rule base negation detection consider multi task problem present novel end end neural model jointly extract entities negations extend standard hierarchical encoder decoder ner model first adopt share encoder follow separate decoders two task architecture perform considerably better previous rule base machine learn base systems overcome problem increase parameter size especially low resource settings propose conditional softmax share decoder architecture achieve state art result ner negation detection two thousand and ten i2b2 va challenge dataset proprietary de identify clinical dataset
language documentation inherently time intensive process transcription gloss corpus management consume significant portion documentary linguists work advance natural language process help accelerate work use linguists past decisions train material question remain prioritize human involvement extend abstract describe beginnings new project attempt ease language documentation process use natural language process nlp technology base one methods adapt nlp tool new languages base recent advance massively multilingual neural network two backend apis interfaces allow linguists upload data describe current progress two front automatic phoneme transcription gloss finally briefly describe future directions
natural language understand challenge problem cover wide range task previous methods generally train task separately consider combine cross task feature enhance task performance paper incorporate logic information help natural language inference nli task story cloze test sct previous work sct consider various semantic information sentiment topic lack logic information sentence essential element stories thus propose extract logic information course story improve understand whole story logic information model help nli task experimental result prove strength logic information
paper present new corpus entailment problems corpus combine follow characteristics one precise leave implicit hypotheses two base real world texts ie premise write purpose test textual entailment three size one hundred and fifty corpus construct take problems real text entailment discover miss hypotheses use crowd experts believe corpus constitute first step towards wide coverage test precise natural language inference systems
era big data due expeditious exchange information web word use denote newer mean cause linguistic shift recent availability large amount digitize texts automate analysis evolution language become possible study mainly focus improve detection new word sense paper present unique proposal base network feature improve precision new word sense detection candidate word new sense birth detect compare sense cluster induce two different time point compare network properties subgraphs induce novel sense cluster across two time point use mean fractional change edge density structural similarity average path length feature svm classifier manual evaluation give precision value eighty-six seventy-four task new sense detection test two distinct time point pair comparison precision value range twenty-three thirty-two propose scheme use outline method therefore use new post hoc step improve precision novel word sense detection robust reliable way underlie framework use graph structure another important observation even though proposal post hoc step use isolation result decent performance achieve precision fifty-four sixty-two finally show method able detect well know historical shift eighty case
computerize document classification already order news article apple news app google personalize search feature group together match reader interest invisible therefore illegible decisions go tailor search subject critique scholars emphasize intelligence document good ability understand criteria search article attempt unpack procedures use computational classification texts translate term legible humanists examine opportunities render computational text classification process subject expert critique improvement
semantic pattern similarity interest though often encounter nlp task two sentence compare specific mean abstract semantic pattern eg preposition frame utilize siamese network model task show usefulness determine sql pattern unseen question database back question answer scenario approach achieve high accuracy contain build proxy confidence use keep precision arbitrarily high
current state art speech recognition systems build recurrent neural network acoustic language model rely feature extraction pipelines extract mel filterbanks cepstral coefficients paper present alternative approach base solely convolutional neural network leverage recent advance acoustic model raw waveform language model fully convolutional approach train end end predict character raw waveform remove feature extraction step altogether external convolutional language model use decode word wall street journal model match current state art librispeech report state art performance among end end model include deep speech two train twelve time acoustic data significantly linguistic data
recently advancements sequence sequence neural network architectures lead improve natural language understand build neural network base natural language understand component one main challenge collect enough train data generation synthetic dataset inexpensive quick way collect data since data often less variety real natural language neural network often problems generalize unseen utterances test work address challenge use multi task learn train domain real data alongside domain synthetic data improve natural language understand evaluate approach domain airline travel information two synthetic datasets domain real data test two datasets base subtitle movies series use attention base encoder decoder model able improve f1 score strong baselines eight thousand and seventy-six eight thousand, four hundred and ninety-eight smaller synthetic dataset
paper deal automatic analysis real life telephone conversations agent customer customer care service ccs application domain public transportation system paris purpose collect statistics customer problems order monitor service decide priorities intervention improve user satisfaction primary importance analysis detection theme object customer problems theme define application requirements part application ontology implicit ccs documentation due variety customer population structure conversations agent unpredictable conversation may one theme theme mention interleave mention facts irrelevant application purpose furthermore certain conversations theme mention localize specific conversation segment conversations mention localize consequence approach feature extraction without mention localization consider application domain relevant theme identify automatic procedure express specific sentence whose word hypothesize automatic speech recognition asr system asr system error prone word error rat high many reason among worth mention unpredictable background noise speaker accent various type speech disfluencies application task require composition proportion theme mention sequential decision strategy introduce paper perform survey large amount conversations make available give time period strategy sample conversations form survey contain enough data analyze high accuracy proportion estimate sufficient accuracy due unpredictable type theme mention appropriate consider methods theme hypothesization base global well local feature extraction two systems base type feature extraction consider strategy one four methods novel base new definition density theme mention localization high density zone whose boundaries need precisely detect sequential decision strategy start group theme hypotheses set different expect accuracy coverage level set accuracy improve consequent increase coverage new system new feature introduce execution trigger specific precondition meet hypotheses generate basic four systems experimental result provide corpus collect call center paris transportation system know ratp result show survey high accuracy coverage compose propose strategy systems make possible apply previously publish proportion estimation approach take account hypothesization errors
predict user behaviour website difficult task require integration multiple source information geo location user profile web surf history paper tackle problem predict user intent base query use access certain webpage make additional assumptions domain detection device use location use word information embed give query order build competitive classifiers label small fraction edi query intent prediction dataset citeedi challenge dataset use grind truth use various rule base approach automatically label rest dataset train classifiers evaluate quality automatic label grind truth dataset use recurrent convolutional network model represent word query multiple embed methods
large scale domain classification natural language understand leverage user domain enablement information refer prefer authenticate domains user attention mechanism show improve overall domain classification performance paper propose supervise enablement attention mechanism utilize sigmoid activation attention weight attention compute expressive power without weight sum constraint softmax attention attention weight explicitly encourage similar correspond elements grind truth one hot vector supervise attention attention information enable domains leverage self distillation evaluate actual utterances large scale ipda show approach significantly improve domain classification performance
paper introduce wav2letter fastest open source deep learn speech recognition framework wav2letter write entirely c use arrayfire tensor library maximum efficiency explain architecture design wav2letter system compare major open source speech recognition systems case wav2letter 2x faster optimize frameworks train end end neural network speech recognition also show wav2letter train time scale linearly sixty-four gpus highest test model one hundred million parameters high performance frameworks enable fast iteration often crucial factor successful research model tune new datasets task
voice enable commercial products ubiquitous typically enable lightweight device keyword spot kws full automatic speech recognition asr cloud asr systems require significant computational resources train inference mention copious amount annotate speech data kws systems hand less resource intensive limit capabilities comcast xfinity x1 entertainment platform explore middle grind asr kws introduce novel resource efficient neural network voice query recognition much accurate state art cnns kws yet easily train deploy limit resources evaluation dataset represent top two hundred voice query achieve low false alarm rate one query error rate six model perform inference 824x faster current asr system
past years witness rapid developments neural machine translation nmt recently advance model train techniques rnn base nmt rnmt show potential strength even compare well know transformer self attentional model although rnmt model possess deep architectures stack layer transition depth consecutive hide state along sequential axis still shallow paper enhance rnn base nmt increase transition depth consecutive hide state build novel deep transition rnn base architecture neural machine translation name dtmt model enhance hide hide transition multiple non linear transformations well maintain linear transformation path throughout deep transition well design linear transformation mechanism alleviate gradient vanish problem experiment show specially design deep transition modules dtmt achieve remarkable improvements translation quality experimental result chinese english translation task show dtmt outperform transformer model two hundred and nine bleu point achieve best result ever report dataset wmt14 english german english french translation task dtmt show superior quality state art nmt systems include transformer rnmt
sentiment analysis see much progress past two decades past years neural network approach primarily rnns cnns successful task recently new category neural network self attention network sans create utilize attention mechanism basic build block self attention network show effective sequence model task recurrence convolutions work explore effectiveness sans sentiment analysis demonstrate sans superior performance rnn cnn counterparts compare classification accuracy six datasets well model characteristics train speed memory consumption finally explore effect various san modifications multi head attention well two methods incorporate sequence position information sans
multi criteria chinese word segmentation promise challenge task exploit several different segmentation criteria mine common underlie knowledge paper propose flexible multi criteria learn chinese word segmentation usually segmentation criterion could decompose multiple sub criteria shareable segmentation criteria process word segmentation rout among sub criteria perspective present switch lstms segment word consist several long short term memory neural network lstm switcher automatically switch rout among lstms auto switch lstms model provide flexible solution multi criteria cws also easy transfer learn knowledge new criteria experiment show model obtain significant improvements eight corpora heterogeneous segmentation criteria compare previous method single criterion learn
article present automatic frame analysis system evaluate corpus french encyclopedic history texts annotate accord framenet formalism choose approach rely integrate sequence label model jointly optimize frame identification semantic role segmentation identification purpose study analyze task complexity several dimension hence provide detail evaluations feature selection point view data point view
recent advancements ai intelligent virtual assistants iva become ubiquitous part every home go forward witness confluence vision speech dialog system technologies enable ivas learn audio visual ground utterances conversations users object activities events surround part 7th dialog system technology challenge dstc7 audio visual scene aware dialog avsd track explore topics dialog important contextual feature architecture along explorations around multimodal attention also incorporate end end audio classification convnet aclnet model present detail analysis experiment show model variations outperform baseline system present task
tokenization segmentation wide concept cover simple process separate punctuation word sophisticate process apply morphological knowledge neural machine translation nmt require limit size vocabulary computational cost enough examples estimate word embeddings separate punctuation split tokens word subwords prove helpful reduce vocabulary increase number examples word improve translation quality tokenization challenge deal languages separator word order assess impact tokenization quality final translation nmt experiment five tokenizers ten language pair reach conclusion tokenization significantly affect final translation quality best tokenizer differ different language pair
recurrent neural network rnns learn continuous vector representations symbolic structure sequence sentence representations often exhibit linear regularities analogies regularities motivate hypothesis rnns show regularities implicitly compile symbolic structure tensor product representations tprs smolensky one thousand, nine hundred and ninety additively combine tensor products vectors represent roles eg sequence position vectors represent fillers eg particular word test hypothesis introduce tensor product decomposition network tpdns use tprs approximate exist vector representations demonstrate use synthetic data tpdns successfully approximate linear tree base rnn autoencoder representations suggest representations exhibit interpretable compositional structure explore settings lead rnns induce structure sensitive representations contrast tpdn experiment show representations four model train encode naturally occur sentence largely approximate bag word marginal improvements sophisticate structure conclude tpdns provide powerful method interpret vector representations standard rnns induce compositional sequence representations remarkably well approximate tprs time exist train task sentence representation learn may sufficient induce robust structural representations
introduce pytext deep learn base nlp model framework build pytorch pytext address often conflict requirements enable rapid experimentation serve model scale achieve provide simple extensible interfaces model components use pytorch capabilities export model inference via optimize caffe2 execution engine report experience migrate experimentation production workflows pytext enable us iterate faster novel model ideas seamlessly ship industrial scale
paper deal automatic analysis conversations customer agent call centre customer care service purpose analysis hypothesize theme problems complaints discuss conversation theme define application documentation topics conversation may contain mention irrelevant application purpose multiple theme whose mention may interleave portion conversation well define two methods propose multiple theme hypothesization one base cosine similarity measure use bag feature extract entire conversation method introduce concept thematic density distribute around specific word position conversation addition automatically select word word bi grams possible gap successive word also consider select experimental result show result obtain propose methods outperform result obtain support vector machine data furthermore use theme skeleton conversation thematic densities derive possible extract components automatic conversation report use improve service performance index term multi topic audio document classification hu man human conversation analysis speech analytics distance bigrams
despite remarkable evolution deep neural network natural language process nlp interpretability remain challenge previous work largely focus model learn representation level break analysis study individual dimension neurons vector representation learn end end neural model nlp task propose two methods linguistic correlation analysis base supervise method extract relevant neurons respect extrinsic task cross model correlation analysis unsupervised method extract salient neurons wrt model evaluate effectiveness techniques ablate identify neurons reevaluate network performance two task neural machine translation nmt neural language model nlm present comprehensive analysis neurons aim address follow question localize distribute different linguistic properties model ii certain neurons exclusive properties others iii information less distribute nmt vs nlm iv important neurons identify linguistic correlation method overall task code publicly available part neurox toolkit dalvi et al two thousand and nineteen
present toolkit facilitate interpretation understand neural network model toolkit provide several methods identify salient neurons respect model external task user visualize select neurons ablate measure effect model accuracy manipulate control behavior model test time analysis potential serve springboard various research directions understand model better architectural choices model distillation control data bias
name entity recognition ner task identify mention rigid designators text belong predefined semantic type person location organization etc ner always serve foundation many natural language applications question answer text summarization machine translation early ner systems get huge success achieve good performance cost human engineer design domain specific feature rule recent years deep learn empower continuous real value vector representations semantic composition nonlinear process employ ner systems yield stat art performance paper provide comprehensive review exist deep learn techniques ner first introduce ner resources include tag ner corpora shelf ner tool systematically categorize exist work base taxonomy along three ax distribute representations input context encoder tag decoder next survey representative methods recent apply techniques deep learn new ner problem settings applications finally present readers challenge face ner systems outline future directions area
able recognize word slot detect intent utterance keen issue natural language understand exist work either treat slot fill intent detection separately pipeline manner adopt joint model sequentially label slot summarize utterance level intent without explicitly preserve hierarchical relationship among word slot intents exploit semantic hierarchy effective model propose capsule base neural network model accomplish slot fill intent detection via dynamic rout agreement schema rout schema propose synergize slot fill performance use infer intent representation experiment two real world datasets show effectiveness model compare alternative model architectures well exist natural language understand service
distant supervision relation extraction efficient method reduce labor cost widely use seek novel relational facts large corpora identify multi instance multi label problem however exist distant supervision methods suffer select important word sentence extract valid sentence bag towards end propose novel approach address problems paper firstly propose linear attenuation simulation reflect importance word sentence respect distance entities word secondly propose non independent identically distribute non iid relevance embed capture relevance sentence bag method capture complex information word hide relations also express mutual information instance bag extensive experiment benchmark dataset well validate effectiveness propose method
semantic parse task convert natural language utterances machine interpretable mean representations execute real world environment database scale semantic parse arbitrary domains face two interrelate challenge obtain broad coverage train data effectively cheaply develop model generalize compositional utterances complex intentions address challenge framework allow elicit train data domain ontology bootstrap neural parser recursively build derivations logical form framework mean representations describe sequence natural language templates template correspond decompose fragment underlie mean representation although artificial templates understand paraphrase humans create natural utterances result parallel triple utterances mean representations decompositions allow us train neural semantic parser learn compose rule derive mean representations crowdsource train data six domains cover single turn utterances exhibit rich compositionality sequential utterances complex task procedurally perform step develop neural semantic parsers perform compositional task general approach allow deploy neural semantic parsers quickly cheaply give domain ontology
write style person affirm unique identity indicator word use structure sentence clear measure identify author specific work stylometry subset authorship attribution long history begin 19th century still find use modern time emergence internet shift application attribution study towards non standard texts comparatively shorter different long texts research do aim paper focus study short online texts retrieve message application call whatsapp study distinctive feature macaronic language hinglish use supervise learn methods compare model various feature word n gram character n gram compare via methods viz naive bay classifier support vector machine conditional tree random forest find best discriminator corpora result show svm attain test accuracy ninety-five thousand and seventy-nine similarly naive bay attain accuracy ninety-four thousand, four hundred and fifty-five dataset conditional tree random forest fail perform well expect also find word unigram character three grams feature likely distinguish author accurately feature
propose novel methodology address dialog learn context goal orient conversational systems key idea quantize dialog space cluster create language model across cluster thus allow accurate choice next utterance conversation language model rely n grams associate cluster utterances quantize dialog language model methodology apply end end goal orient track latest dialog system technology challenge dstc6 objective find correct system utterance pool candidates order complete dialog user automate restaurant reservation system result show technique propose paper achieve high accuracy regard selection correct candidate utterance outperform state art approach base neural network
aspect level sentiment classification fine grain sentiment analysis task detect sentiment towards particular aspect sentence previous study develop various attention base methods generate aspect specific sentence representations however attention may inherently introduce noise downgrade performance paper propose constrain attention network simple yet effective solution regularize attention multi aspect sentiment analysis alleviate drawback attention mechanism specifically introduce orthogonal regularization multiple aspects sparse regularization single aspect experimental result two public datasets demonstrate effectiveness approach extend approach multi task settings outperform state art methods
natural language understand recently see surge progress use sentence encoders like elmo peters et al 2018a bert devlin et al two thousand and nineteen pretrained variants language model conduct first large scale systematic study candidate pretraining task compare nineteen different task alternatives complement language model primary result support use language model especially combine pretraining additional label data task however result mix across pretraining task show concern trend elmo pretrain freeze paradigm random baselines worryingly strong result vary strikingly across target task addition fine tune bert intermediate task often negatively impact downstream transfer positive trend see modest gain multitask train suggest development sophisticate multitask transfer learn techniques avenue research
develop method extract coherence feature paragraph match similar word sentence conduct experiment parallel german corpus contain two thousand human create two thousand machine translate paragraph result show method achieve best performance accuracy seven hundred and twenty-three equal error rate two hundred and ninety-eight compare previous methods various computer generate text include translation paper generation best accuracy six hundred and seventy-nine equal error rate three hundred and twenty experiment dutch another rich resource language low resource one japanese attain similar performances demonstrate efficiency coherence feature distinguish computer translate human create paragraph diverse languages
knowledge representation learn krl aim represent entities relations knowledge graph low dimensional semantic space widely use massive knowledge drive task article introduce reader motivations krl overview exist approach krl afterwards extensively conduct quantitative comparison analysis several typical krl methods three evaluation task knowledge acquisition include knowledge graph completion triple classification relation extraction also review real world applications krl language model question answer information retrieval recommender systems finally discuss remain challenge outlook future directions krl cod datasets use experiment find https githubcom thunlp openke
propose first multi task learn model joint vietnamese word segmentation part speech pos tag dependency parse particular model extend bist graph base dependency parser kiperwasser goldberg two thousand and sixteen bilstm crf base neural layer huang et al two thousand and fifteen word segmentation pos tag vietnamese benchmark datasets experimental result show joint model obtain state art competitive performances
paper propose variational self attention model vsam employ variational inference derive self attention model self attention vector random variables impose probabilistic distribution self attention mechanism summarize source information attention vector weight sum weight learn probabilistic distribution compare conventional deterministic counterpart stochastic units incorporate vsam allow multi modal attention distributions furthermore marginalize latent variables vsam robust overfitting experiment stance detection task demonstrate superiority method
text mine broad field sentiment mine important constituent try deduce behavior people towards specific item merchandise politics sport social media comment review sit etc many issue sentiment mine analysis classification one major issue review comment different languages like english arabic urdu etc handle language accord rule difficult task lot research work do english language sentiment analysis classification limit sentiment analysis work carry regional languages like arabic urdu hindi paper waikato environment knowledge analysis weka use platform execute different classification model text classification roman urdu text review dataset scrap different automobile sit extract roman urdu review contain one thousand positive one thousand negative review save weka attribute relation file format arff label examples train do eighty data rest use test purpose do use different model result analyze case result show multinomial naive bay outperform bag deep neural network decision tree random forest adaboost k nn svm classifiers term accuracy precision recall f measure
show constituency parse benefit unsupervised pre train across variety languages range pre train condition first compare benefit pre train fasttext elmo bert english find bert outperform elmo large part due increase model capacity whereas elmo turn outperform non contextual fasttext embeddings also find pre train beneficial across eleven languages test however large model size one hundred million parameters make computationally expensive train separate model language address shortcoming show joint multilingual pre train fine tune allow share small number parameters ten languages final model 10x reduction model size compare fine tune one model per language cause thirty-two relative error increase aggregate explore idea joint fine tune show give low resource languages way benefit larger datasets languages finally demonstrate new state art result eleven languages include english nine hundred and fifty-eight f1 chinese nine hundred and eighteen f1
acoustic word model base connectionist temporal classification ctc criterion natural end end e2e system directly target word output unit two issue exist system first current output ctc model rely current input account context weight input hard alignment issue second word base ctc model suffer vocabulary oov issue mean model frequently occur word tag remain word oov hence model limit capacity recognize fix set frequent word study propose address problems use combination attention mechanism mix units particular introduce attention ctc self attention ctc hybrid ctc mix unit ctc first blend attention model capabilities directly ctc network use attention ctc self attention ctc second alleviate oov issue present hybrid ctc use word letter ctc share hide layer hybrid ctc consult letter ctc word ctc emit oov propose much better solution train mix unit ctc decompose oov word sequence frequent word multi letter units evaluate three thousand, four hundred hours microsoft cortana voice assistant task final acoustic word solution use attention mix units achieve relative reduction word error rate wer vanilla word ctc one thousand, two hundred and nine e2e model without use language model lm complex decoder also outperform traditional context dependent cd phoneme ctc strong lm decoder six hundred and seventy-nine relative
heap law state large enough text corpus number type function tokens grow nkmbeta free parameters kbeta much write result various generalizations derive zipf law derive first principles completely novel expression type token curve prove superior accuracy real text expression naturally generalize equally accurate estimate count hapaxes higher n legomena
machine translation mt play important role benefit linguists sociologists computer scientists etc process natural language translate natural language demand grow exponentially past couple years consider enormous exchange information different regions different regional languages machine translation pose numerous challenge word one language equivalent word another language b two give languages may completely different structure c word one mean owe challenge along many others mt active area research five decades numerous methods propose past either aim improve quality translations generate study robustness systems measure performance many different languages literature review discuss statistical approach particular word base phrase base neural approach gain widespread prominence owe state art result across multiple major languages
understand passenger intents extract relevant slot important build block towards develop contextual dialogue system responsible handle certain vehicle passenger interactions autonomous vehicles av passengers give instructions amie automate vehicle multimodal cabin experience agent parse command properly trigger appropriate functionality av system amie scenarios describe usages support various natural command interact vehicle collect multimodal cabin data set multi turn dialogues passengers amie use wizard oz scheme explore various recent recurrent neural network rnn base techniques build hierarchical model recognize passenger intents along relevant slot associate action perform av scenarios experimental result achieve f1 score ninety-one utterance level intent recognition ninety-six slot extraction model
effective presentation skills help succeed business career academy paper present design speech assessment oral presentation algorithm speech evaluation base criteria optimal intonation pace speech optimal intonation vary language language develop automatic identification language presentation require propose algorithm test presentations deliver kazakh language test purpose feature kazakh phonemes extract use mfcc plp methods create hide markov model hmm five five kazakh phonemes kazakh vowel formants define correlation deviation rate fundamental frequency liveliness speech evaluate intonation presentation analyze establish threshold value monotone dynamic speech sixteen error intonation evaluation nineteen
speech common effective way communication humans modern consumer devices smartphones home hubs equip deep learn base accurate automatic speech recognition enable natural interaction humans machine recently researchers demonstrate powerful attack machine learn model fool produceincorrect result however nearly previous research adversarial attack focus image recognition object detection model short paper present first kind demonstration adversarial attack speech classification model algorithm perform target attack eighty-seven success add small background noise without know underlie model parameter architecture attack change least significant bits subset audio clip sample noise change eighty-nine human listener perception audio clip evaluate human study
match causal inference well study problem standard methods fail units match text document high dimensional rich nature data render exact match infeasible cause propensity score produce incomparable match make assess match quality difficult paper characterize framework match text document decompose exist methods one choice text representation two choice distance metric investigate different choices within framework affect quantity quality match identify systematic multifactor evaluation experiment use human subject altogether evaluate one hundred unique text match methods along five comparison methods take literature experimental result identify methods generate match higher subjective match quality current state art techniques enhance precision result develop predictive model estimate match quality pair text document function various distance score model find successfully mimic human judgment also allow approximate unsupervised evaluation new procedures employ identify best method illustrate utility text match two applications first engage substantive debate study media bias use text match control topic selection compare news article thirteen news source show condition text data lead precise causal inferences observational study examine effect medical intervention
representation sentence important task use way exchange data inter applications one main characteristic notation must minimal size representative form reduce transfer time hopefully process time well usually sentence representation associate process language grammar language affect represent sentence avoid language dependent notations come new representation use word mean do use lexicon like wordnet instead word use synsets syntactic relations universal much possible new notation call ston sentence object notation somehow similarities json mean minimal representative language independent syntactic representation also want readable easy create simplify develop simple automatic generators create test bank manually benefit use medium different part applications like text summarization language translation etc notation base four languages arabic english franch japanese case languages agree one representation also give diversity grammatical structure different world languages annotation may fail languages allow future improvements
paper introduce novel open domain socialbot amazon alexa prize competition aim carry friendly conversations users variety topics present modular system highlight different data source use human mind model data management additionally build employ natural language understand information retrieval tool apis expand knowledge base describe semistructured scalable framework craft topic specific dialogue flow give detail dialogue management scheme score mechanisms finally briefly evaluate performance system observe challenge open domain socialbot face
alternative question answer methods base feature engineer deep learn approach convolutional neural network cnns long short term memory model lstms recently propose semantic match question answer achieve good result however model combine additional feature word overlap bm25 score without combination model perform significantly worse methods base linguistic feature engineer paper propose attention base neural match model rank short answer text adopt value share weight scheme instead position share weight scheme combine different match signal incorporate question term importance learn use question attention network use popular benchmark trec qa data show relatively simple anmm model significantly outperform neural network model use question answer task competitive model combine additional feature anmm combine additional feature outperform baselines
supervise approach keyphrase extraction candidate phrase encode set hand craft feature machine learn algorithms train discriminate keyphrases non keyphrases although manually design feature show work well practice feature engineer difficult process require expert knowledge normally generalize well paper present surfke feature learn framework exploit text automatically discover pattern keyphrases exhibit model represent document graph automatically learn feature representation phrase propose model obtain remarkable improvements performance strong baselines
introduce first system towards novel task answer complex multisentence recommendation question tourism domain solution use pipeline two modules question understand answer question understand define sql like query language capture semantic intent question support operators like subset negation preference similarity often find recommendation question train compare traditional crfs well bidirectional lstm base model convert question semantic representation extend model semisupervised set partially label sequence gather crowdsourcing find best model perform semi supervise train bidilstmcrf hand design feature ccmchang et al two thousand and seven constraints finally end end qa system answer component convert question representation query fire underlie knowledge source experiment two different answer corpora demonstrate system significantly outperform baselines twenty pt higher accuracy seventeen pt higher recall
lack moderation online communities enable participants incur personal aggression harassment cyberbullying issue accentuate extremist radicalisation contemporary post truth politics scenario kind hostility usually express mean toxic language profanity abusive statements recently google develop machine learn base toxicity model attempt assess hostility comment unfortunately suggest say model deceive adversarial attack manipulate text sequence comment paper firstly characterise adversarial attack use obfuscation polarity transformations former deceive corrupt toxic trigger content typographic edit whereas latter deceive grammatical negation toxic content propose two stage approach counter attack anomalies bulding upon recently propose text deobfuscation method toxicity score model lastly conduct experiment approximately twenty-four thousand distort comment show way feasible restore toxicity adversarial variants incur roughly twofold increase process time even though novel adversary challenge would keep come derive versatile nature write language anticipate techniques combine machine learn text pattern recognition methods one target different layer linguistic feature would need achieve robust detection toxic language thus foster aggression free digital interaction
neural embeddings popular set methods represent word phrase text low dimensional vector typically fifty five hundred dimension however difficult interpret dimension meaningful manner create neural embeddings require extensive train tune multiple parameters hyperparameters present simple unsupervised method represent word phrase text low dimensional vector mean relative importance dimension transparent inspection create near comprehensive vector representation word select bigrams trigrams abbreviations use set title abstract pubmed corpus vector use create several novel implicit word word text text similarity metrics implicit word word similarity metrics correlate well human judgement word pair similarity relatedness outperform equal report methods variety biomedical benchmarks include several implementations neural embeddings train pubmed corpora implicit word word metrics capture different aspects word word relatedness word2vec base metrics partially correlate rho five eight depend task corpus vector representations word bigrams trigrams abbreviations pubmed titleabstracts publicly available http arrowsmithpsychuicedu release cc nc license several public web query interfaces also available site include one allow user specify give word view closely relate term accord direct co occurrence well different implicit similarity metrics
word sense disambiguation open problem natural language process particularly challenge useful unsupervised set word give text need disambiguate without use label data typically wsd systems use sentence small window word around target word context disambiguation computational complexity scale exponentially size context paper leverage formalism topic model design wsd system scale linearly number word context result system able utilize whole document context word disambiguate propose method variant latent dirichlet allocation topic proportion document replace synset proportion utilize information wordnet assign non uniform prior synset distribution word logistic normal prior document distribution synsets evaluate propose method senseval two senseval three semeval two thousand and seven semeval two thousand and thirteen semeval two thousand and fifteen english word wsd datasets show outperform state art unsupervised knowledge base wsd system significant margin
videos image sentence mediums express semantics one imagine picture read sentence describe scene word however even small change sentence significant semantic inconsistency correspond video image example change verb sentence mean may drastically change many efforts encode video sentence decode sentence video research study new scenario sentence video give sentence inaccurate semantic inconsistency sentence video word sentence result inaccurate description paper introduce new problem call visual text correction vtc ie find replace inaccurate word textual description video propose deep network simultaneously detect inaccuracy sentence fix replace inaccurate word method leverage semantic interdependence videos word well short term long term relations word sentence formulation part visual feature vector every single word dynamically select gate process furthermore train evaluate model propose approach automatically construct large dataset vtc problem experiment performance analysis demonstrate propose method provide good result also highlight general challenge solve vtc problem best knowledge work first kind visual text correction task
automatic speaker verification systems increasingly use primary mean authenticate costumers recently propose train speaker verification systems use end end deep neural model paper show systems vulnerable adversarial example attack adversarial examples generate add peculiar noise original speaker examples way almost indistinguishable original examples human listener yet generate waveforms sound speaker use fool system claim waveforms utter speaker b present white box attack end end deep network either train yoho ntimit also present two black box attack adversarial examples generate system train yoho attack system train ntimit adversarial examples generate system train mel spectrum feature set attack system train mfcc result suggest accuracy attack system decrease false positive rate dramatically increase
many question answer systems knowledge graph rely entity relation link components order connect natural language input underlie knowledge graph traditionally entity link relation link perform either dependent sequential task independent parallel task paper propose framework call earl perform entity link relation link joint task earl implement two different solution strategies provide comparative analysis paper first strategy formalisation joint entity relation link task instance generalise travel salesman problem gtsp order computationally feasible employ approximate gtsp solvers second strategy use machine learn order exploit connection density nod knowledge graph rely three base feature rank step order predict entities relations compare strategies evaluate dataset five thousand question strategies significantly outperform current state art approach entity relation link
propose machine talk machine m2m framework combine automation crowdsourcing rapidly bootstrap end end dialogue agents goal orient dialogues arbitrary domains m2m scale new task task schema api client dialogue system developer also customizable cater task specific interactions compare wizard oz approach data collection m2m achieve greater diversity coverage salient dialogue flow maintain naturalness individual utterances first phase simulate user bot domain agnostic system bot converse exhaustively generate dialogue outline ie sequence template utterances semantic parse second phase crowd workers provide contextual rewrite dialogues make utterances natural preserve mean entire process finish within hours propose new corpus three thousand dialogues span two domains collect m2m present comparisons popular dialogue datasets quality diversity surface form dialogue flow
present alime assist intelligent assistant design create innovative online shop experience e commerce base question answer qa alime assist offer assistance service customer service chat service able take voice text input incorporate context qa support multi round interaction currently serve millions customer question per day able address eighty-five paper demonstrate system present underlie techniques share experience deal real world qa e commerce field
understand recurrent network rule extraction long history take new interest due need interpret verify neural network one basic form represent stateful rule deterministic finite automata dfa previous research show extract dfas train second order recurrent network possible also relatively stable recently several new type recurrent network complicate architectures introduce handle challenge learn task usually involve sequential data however remain open problem whether dfas adequately extract model specifically clear dfa extraction affect apply different recurrent network train data set different level complexity investigate dfa extraction several widely adopt recurrent network train learn set seven regular tomita grammars first formally analyze complexity tomita grammars categorize grammars accord complexity empirically evaluate different recurrent network performance dfa extraction tomita grammars experiment show recurrent network extraction performance decrease complexity underlie grammar increase grammars lower complexity recurrent network obtain desirable extraction performance grammars highest level complexity several complicate model fail certain recurrent network satisfactory extraction performance
consider methods learn vector representations sql query support generalize workload analytics task include workload summarization index selection predict query trigger memory errors consider vector representations raw sql text optimize query plan evaluate methods synthetic real sql workloads find general algorithms base vector representations outperform exist approach rely specialize feature index recommendation cluster vector representations compress large workloads loss performance recommend index error prediction train classifier learn vectors automatically relate subtle syntactic pattern specific errors raise query execution surprisingly also find methods enable transfer learn model train one sql corpus apply unrelated corpus still enable good performance find general approach train large corpus sql query provide robust foundation variety workload analysis task database feature without require application specific feature engineer
unsupervised word translation non parallel inter lingual corpora attract much research interest recently neural network methods train adversarial loss function achieve high accuracy task despite impressive success recent techniques suffer typical drawbacks generative adversarial model sensitivity hyper parameters long train time lack interpretability paper make observation two sufficiently similar distributions align correctly iterative match methods present novel method first align second moment word distributions two languages iteratively refine alignment extensive experiment word translation european non european languages show method achieve better performance recent state art deep adversarial approach competitive supervise baseline also efficient easy parallelize cpu interpretable
majority exist speech emotion recognition research focus automatic emotion detection use train test data corpus collect condition performance systems show drop significantly cross corpus cross language scenarios address problem paper exploit transfer learn technique improve performance speech emotion recognition systems novel cross language cross corpus scenarios evaluations five different corpora three different languages show deep belief network dbns offer better accuracy previous approach cross corpus emotion recognition relative sparse autoencoder svm baseline system result also suggest use large number languages train use small fraction target data train significantly boost accuracy compare baseline also corpus limit train examples
chit chat model know several problems lack specificity display consistent personality often captivate work present task make chit chat engage condition profile information collect data train model condition give profile information ii information person talk result improve dialogues measure next utterance prediction since ii initially unknown model train engage partner personal topics show result dialogue use predict profile information interlocutors
unmoderated nature social media enable diffusion hoax turn jeopardise credibility information gather social media platforms exist research automate detection hoax limitation use relatively small datasets owe difficulty get label data turn limit research explore early detection hoax well explore factor effect size train data use slide windows mitigate problem introduce semi automate method leverage wikidata knowledge base build large scale datasets veracity classification focus celebrity death report enable us create dataset four thousand and seven report include thirteen million tweet fifteen fake experiment use class specific representations word embeddings show achieve f1 score near seventy-two within ten minutes first tweet post expand size train data follow semi automate mean dataset represent realistic scenario real distribution true commemorative false stories release use benchmark future research
offensive antagonistic language target individuals social group base personal characteristics also know cyber hate speech cyberhate frequently post widely circulate viathe world wide web consider key risk factor individual societal tension link toregional instability automate web base cyberhate detection important observe understandingcommunity regional societal tension especially online social network post rapidlyand widely view disseminate previous work involve use lexicons bag word orprobabilistic language parse approach often suffer similar issue cyberhate besubtle indirect thus depend occurrence individual word phrase lead significantnumber false negative provide inaccurate representation trend cyberhate problemmotivated us challenge think around representation subtle language use reference toperceived threats include immigration job prosperity hateful context propose anovel framework utilise language use around concept othering intergroup threat theory toidentify subtleties implement novel classification method use embed learn computesemantic distance part speech consider part othering narrative validate ourapproach conduct several experiment different type cyberhate namely religion disability race andsexual orientation f measure score classify hateful instance obtain apply ourmodel ninety-three eighty-six ninety-seven ninety-eight respectively provide significant improvement classifier accuracy overthe state art
analyze language learn agent train reinforcement learn component activeqa system buck et al two thousand and seventeen activeqa question answer frame reinforcement learn task agent sit user black box question answer system agent learn reformulate user question elicit optimal answer probe system many versions question generate via sequence sequence question reformulation model aggregate return evidence find best answer process instance emphmachine machine communication question reformulation model must adapt language increase quality answer return match language question answer system find agent learn transformations align semantic intuitions discover learn classical information retrieval techniques tf idf weight stem
paper describe wili two thousand and eighteen benchmark dataset monolingual write natural language identification wili two thousand and eighteen publicly available free charge dataset short text extract wikipedia contain one thousand paragraph two hundred and thirty-five languages total twenty-three thousand, five hundred paragraph wili classification dataset give unknown paragraph write one dominant language decide language
vocabulary word translation major problem translation low resource languages suffer lack parallel train data paper evaluate contributions target language context model towards translation oov word specifically case oov translations derive external knowledge source dictionaries develop neural non neural context model evaluate within phrase base self attention base neural machine translation systems result show neural language model integrate additional context beyond current sentence effective disambiguate possible oov word translations present efficient second pass lattice rescoring method wide context neural language model demonstrate performance improvements state art self attention base neural mt systems five six low resource language pair
search phrase word set large text array mean additional index consider use may reduce query process time order magnitude comparison standard invert file
paper present methods accelerate recurrent neural network base language model rnnlms online speech recognition systems firstly lossy compression past hide layer output history vector cache introduce order reduce number lm query next rnnlm computations deploy cpu gpu hybrid manner compute layer model advantageous platform add overhead data exchange cpu gpu compensate frame wise batch strategy performance propose methods evaluate librispeech test set indicate reduction history vector precision improve average recognition speed one hundred and twenty-three time minimum degradation accuracy hand cpu gpu hybrid parallelization enable rnnlm base real time recognition four time improvement speed
prosopography investigation common characteristics group people history collective study live involve study biographies solve historical problems biographies unavailable survive document secondary biographical data use quantitative prosopography involve analysis information wide variety source ordinary people paper present machine learn framework automatically design people gazetteer form basis quantitative prosopographical research gazetteer learn noisy text newspapers use name entity recognizer ner capable identify influential people make use custom design influential person index ipi corpus comprise fourteen thousand and twenty article local newspaper sun publish new york one thousand, eight hundred and ninety-six influential people identify algorithm include captain donald hankey english soldier dame nellie melba australian operatic soprano hugh allan canadian ship magnate sir hugh john mcdonald first prime minister canada
text review express sentiment customer towards particular product exploit sentiment analysis machine learn model use predict review score text review furthermore products costumers purchase past indicative products purchase future recommender systems exploit learn model purchase information predict items customer might interest propose transrev approach product recommendation problem integrate ideas recommender systems sentiment analysis multi relational learn joint learn objective transrev learn vector representations users items review embed review learn perform well input feature regression model sentiment prediction b always translate reviewer embed embed review items allow transrev approximate review embed test time difference embed item user embed approximate review embed use regression model predict review score item transrev outperform state art recommender systems large number benchmark data set moreover able retrieve user item review text train set whose embed similar approximate review embed
propose nest lstms nlstm novel rnn architecture multiple level memory nest lstms add depth lstms via nest oppose stack value memory cell nlstm compute lstm cell inner memory cell specifically instead compute value outer memory cell coutert ft odot ct one odot gt nlstm memory cells use concatenation ft odot ct one odot gt input inner lstm nlstm memory cell set coutert hinnert nest lstms outperform stack single layer lstms similar number parameters experiment various character level language model task inner memories lstm learn longer term dependencies compare higher level units stack lstm
present neural architecture model argumentative dialogue explicitly model interplay opinion holder oh reason challenger argument goal predict argument successfully change oh view model two components one vulnerable region detection attention model identify part oh reason amenable change two interaction encode identify relationship content oh reason challenger argument base evaluation discussions change view forum reddit two components work together predict oh change view outperform several baselines posthoc analysis suggest sentence pick attention model address frequently successful arguments unsuccessful ones
social media consider democratic space people connect interact regardless gender race demographic aspect despite numerous efforts explore demographic aspects social media still unclear whether social media perpetuate old inequalities offline world dissertation attempt identify gender race twitter users locate unite state use advance image process algorithms face investigate different demographic group connect differentiate regard linguistic style also interest quantify extent one group follow interact extent connections interactions reflect inequalities twitter also extract linguistic feature six categories affective attribute cognitive attribute lexical density awareness temporal reference social personal concern interpersonal focus order identify similarities differences message share twitter furthermore extract absolute rank difference top phrase demographic group dimension diversity use topics interest retrieve user analysis show users identify white male tend attain higher position term number followers number time another user list twitter clear differences way write across different demographic group gender race domains well topic interest hope effort stimulate development new theories demographic information online space finally develop web base system leverage demographic aspects users provide transparency twitter trend topics system
due growth population hear problems devices develop facilitate inclusion deaf people society use technology communication tool vision systems solution problem present use neural network autoencoders classification american sign language image result nine hundred and ninety-five accuracy error one thousand, six hundred and eighty-four obtain image classification
domain adaptation play important role speech recognition model particular domains low resources propose novel generative model base cyclic consistent generative adversarial network cyclegan unsupervised non parallel speech domain adaptation propose model employ multiple independent discriminators power spectrogram charge different frequency band result one better discriminators focus fine grain detail frequency feature two generator capable generate realistic domain adapt spectrogram demonstrate effectiveness method speech recognition gender adaptation model access supervise data one gender train evaluate test time model able achieve average seven hundred and forty-one phoneme error rate one thousand, one hundred and ten word error rate relative performance improvement compare baseline timit wsj dataset respectively qualitatively model also generate natural sound speech condition data domain
study explore capsule network dynamic rout text classification propose three strategies stabilize dynamic rout process alleviate disturbance noise capsule may contain background information successfully train series experiment conduct capsule network six text classification benchmarks capsule network achieve state art four six datasets show effectiveness capsule network text classification additionally show capsule network exhibit significant improvement transfer single label multi label text classification strong baseline methods best knowledge first work capsule network empirically investigate text model
nowadays internet represent vast informational space grow exponentially problem search relevant data become essential never algorithm propose article allow perform natural language query content document get comprehensive meaningful answer problem partially solve english squad contain enough data learn dataset russian methods use scientists applicable russian brain2 framework allow cope problem stand ability apply small datasets require impressive compute power algorithm illustrate sberbank russia strategy text assume use neuromodel consist sixty-five mln synapses train model able construct word word answer question base give text exist limitations current inability identify synonyms pronoun relations allegories nevertheless result conduct experiment show high capacity generalisation ability suggest approach
recent success deep learn model task extractive question answer qa hinge availability large annotate corpora however large domain specific annotate corpora limit expensive construct work envision system end user specify set base document label examples system exploit document structure create cloze style question base document pre train powerful neural network cloze style question fine tune model label examples evaluate propose system across three diverse datasets different domains find highly effective little label data attain fifty f1 score squad triviaqa less thousand label examples also release set 32m cloze style question practitioners use build qa systems
recurrent neural network rnn convolutional neural network cnn self attention network san commonly use produce context aware representations rnn capture long range dependency hard parallelize time efficient cnn focus local dependency perform well task san model dependencies via highly parallelizable computation memory requirement grow rapidly line sequence length paper propose model call bi directional block self attention network bi blosan rnn cnn free sequence encode require little memory rnn merit san bi blosan split entire sequence block apply intra block san block model local context apply inter block san output block capture long range dependency thus san need process short sequence small amount memory require additionally use feature level attention handle variation contexts around word use forward backward mask encode temporal order information nine benchmark datasets different nlp task bi blosan achieve improve upon state art accuracy show better efficiency memory trade exist rnn cnn san
present approach two different model deep shallow train separately data weight average output take final result deep approach use different combinations model like convolution neural network pretrained word2vec embeddings lstms get representations use train deep neural network clarity prediction also use attentive pool approach pool operation aware title category pair shallow approach use boost technique lightgbm feature generate use title categories find ensemble approach better job use alone suggest result deep shallow approach highly complementary
paper describe abstractive summarization method tabular data employ knowledge base semantic embed generate summary assume dataset contain descriptive text headers columns augment metadata system employ embed recommend subject type text segment recommendations aggregate small collection super type consider descriptive dataset exploit hierarchy type pre specify ontology use february two thousand and fifteen wikipedia knowledge base correspond dbpedia ontology type present experimental result open data take several source openml ckan dataworld illustrate effectiveness approach
propose generative model three type extra grammatical word formation phenomena abound english slang blend clippings reduplicatives adopt data drive approach couple linguistic knowledge propose simple model state art performance human annotate gold standard datasets overall model reveal insights generative process word formation slang insights increasingly relevant context rise prevalence slang non standard varieties internet
hate speech detection critical yet challenge problem natural language process nlp despite existence numerous study dedicate development nlp hate speech detection approach accuracy still poor central problem social media post short noisy exist hate speech detection solutions take post isolate input instance likely yield high false positive negative rat paper radically improve automate hate speech detection present novel model leverage intra user inter user representation learn robust hate speech detection twitter addition target tweet collect analyze user historical post model intra user tweet representations suppress noise single tweet also model similar tweet post users reinforce inter user representation learn techniques experimentally show leverage two representations significantly improve f score strong bidirectional lstm baseline model one hundred and one
describe audio dataset speak word design help train evaluate keyword spot systems discuss task interest challenge require specialize dataset different conventional datasets use automatic speech recognition full sentence suggest methodology reproducible comparable accuracy metrics task describe data collect verify contain previous versions properties conclude report baseline result model train dataset
consider real world task orient dialog settings agents need generate fluent natural language responses correct external action like database query update demonstrate apply customer support chat transcripts sequence sequence seq2seq model often generate short incoherent ungrammatical natural language responses dominate word occur high frequency train data phenomena arise synthetic datasets babi show seq2seq model nearly perfect develop techniques learn embeddings succinctly capture relevant information dialog history demonstrate nearest neighbor base approach learn neural embed space generate fluent responses however see methods able accurately predict execute external action show combine nearest neighbor seq2seq methods hybrid model nearest neighbor use generate fluent responses seq2seq type model ensure dialog coherency generate accurate external action show approach well suit customer support scenarios agents responses typically script drive correct external action critically important hybrid model customer support data achieve seventy-eight relative improvement fluency score one hundred and thirty improvement accuracy external call
propose usim semantic measure grammatical error correction gec measure semantic faithfulness output source thereby complement exist reference less measure rlms measure output grammaticality usim operate compare semantic symbolic structure source correction without rely manually curated reference experiment establish validity usim show one semantic annotation consistently apply ungrammatical text two valid corrections obtain high usim similarity score source three invalid corrections obtain lower score
vast availability text data enable widespread train use ai systems learn predict attribute text also generate text automatically however ai model also learn gender racial ethnic bias present train data paper present first system discover possibility give text portray gender stereotype associate occupation possibility exist system offer counter evidence opposite gender also associate occupation context user provide geography timespan system thus enable text de bias assist human loop system act text pre processor train ai model also help human story writers write stories free occupation level gender bias geographical temporal context choice
analyze two hundred and sixty-two million comment publish arabic language twitter july two thousand and fourteen january two thousand and fifteen isis strength reach peak group prominently expand territorial area control able measure share support aversion toward islamic state within online arab communities investigate two specific topics first exploit time granularity tweet link opinions daily events understand main determinants change trend support toward isis second take advantage geographical locations tweet explore relationship online opinions across countries number foreign fighters join isis
long short term memory lstm network recently show remarkable performance several task deal natural language generation image caption poetry composition yet work analyze text generate lstms order quantitatively evaluate extent artificial texts resemble generate humans compare statistical structure lstm generate language write natural language produce markov model various order particular characterize statistical structure language assess word frequency statistics long range correlations entropy measure main find lstm markov generate texts exhibit feature similar real ones word frequency statistics entropy measure lstm texts show reproduce long range correlations scale comparable find natural language moreover lstm network temperature like parameter control generation process show optimal value produce texts closest real language consistent across different statistical feature investigate
medical domain identify expand abbreviations clinical texts vital task better human machine understand challenge task many abbreviations ambiguous especially intensive care medicine texts phrase abbreviations frequently use besides fact universal dictionary clinical abbreviations universal rule abbreviation write texts difficult acquire expensive annotate even sometimes confuse domain experts paper propose novel effective approach exploit task orient resources learn word embeddings expand abbreviations clinical note achieve eight thousand, two hundred and twenty-seven accuracy close expert human performance
social media empower freedom expression individual voice also enable anti social behavior online harassment cyberbullying hate speech paper deepen understand online hate speech focus largely neglect crucial aspect hate speech target either direct towards specific person entity generalize towards group people share common protect characteristic perform first linguistic psycholinguistic analysis two form hate speech reveal presence interest markers distinguish type hate speech analysis reveal direct hate speech addition personal direct informal angrier often explicitly attack target via name call fewer analytic word word suggest authority influence generalize hate speech hand dominate religious hate characterize use lethal word murder exterminate kill quantity word million many altogether work provide data drive analysis nuances online hate speech enable deepen understand hate speech social implications also detection
recent years amaze advance deep learn methods machine read machine read machine reader extract answer give grind truth paragraph recently state art machine read model achieve human level performance squad read comprehension style question answer qa task success machine read inspire researchers combine information retrieval machine read tackle open domain qa however systems perform poorly compare read comprehension style qa difficult retrieve piece paragraph contain answer question study propose two neural network rankers assign score different passages base likelihood contain answer give question additionally analyze relative importance semantic similarity word level relevance match open domain qa
search personalization aim tailor search result specific user base user personal interest preferences ie user profile recent research approach search personalization model potential three way relationship submit query user search result ie document relationship use personalize search result user paper introduce novel embed model base capsule network recently breakthrough deep learn model three way relationships search personalization model user submit query return document embed vector vector space three way relationship describe triple query user document model three column matrix contain three embed vectors three column matrix feed deep learn architecture rank search result return basis ranker experimental result query log commercial web search engine show model achieve better performances basis ranker well strong search personalization baselines
paper describe participation amobee share sentiment analysis task semeval two thousand and eighteen participate english sub task spanish valence task system consist three part train task specific word embeddings train model consist gate recurrent units gru convolution neural network cnn attention mechanism train stack base ensembles sub task algorithm reach 3rd 1st place valence ordinal classification sub task english spanish respectively
cross lingual information retrieval challenge task absence align parallel corpora paper address problem consider topically align corpora design evaluate ir setup emphasize neither use sentence align corpora document align corpora use language specific resources dictionary thesaurus grammar rule instead use embed common space learn word correspondences directly test propose approach bilingual ir standard fire datasets bangla hindi english propose method superior state art method ir evaluation measure also term time requirements extend method successfully trilingual set
one key requirements facilitate semantic analytics information regard contemporary historical events web news social media availability reference knowledge repositories contain comprehensive representations events temporal relations exist knowledge graph popular examples include dbpedia yago wikidata focus mostly entity centric information insufficient term coverage completeness respect events temporal relations eventkg present paper multilingual event centric temporal knowledge graph address gap eventkg incorporate six hundred and ninety thousand contemporary historical events twenty-three million temporal relations extract several large scale knowledge graph semi structure source make available canonical representation
software repositories contain large amount textual data range source code comment issue descriptions question answer comment stack overflow make sense textual data topic model frequently use text mine tool discovery hide semantic structure text body latent dirichlet allocation lda commonly use topic model aim explain structure corpus group texts lda require multiple parameters work well rough sometimes conflict guidelines available parameters set paper contribute broad study parameters arrive good local optima github stack overflow text corpora ii posteriori characterisation text corpora relate eight program languages iii analysis corpus feature importance via per corpus lda configuration find one popular rule thumb topic model parameter configuration applicable corpora use experiment two corpora sample github stack overflow different characteristics require different configurations achieve good model fit three predict good configurations unseen corpora reliably find support researchers practitioners efficiently determine suitable configurations topic model analyse textual data contain software repositories
political identity often manifest language variation relationship two still relatively unexplored quantitative perspective study examine use catalan language local semi autonomous region catalonia spain twitter discourse relate two thousand and seventeen independence referendum corroborate prior find pro independence tweet likely include local language anti independence tweet also find catalan use often referendum relate discourse contexts contrary prior find language variation suggest strong role catalan language expression catalonian political identity
goal answer question paragraph describe process eg photosynthesis texts genre challenge effect action often implicit unstated require background knowledge inference reason change world state supply knowledge leverage verbnet build rulebase call semantic lexicon precondition effect action use along commonsense knowledge persistence answer question change evaluation show system procomp significantly outperform two strong read comprehension rc baselines contributions two fold semantic lexicon rulebase demonstration simulation base approach machine read outperform rc methods rely surface cue alone since work perform develop neural systems outperform procomp describe elsewhere dalvi et al naacl eighteen however semantic lexicon remain novel potentially useful resource integration neural systems remain currently unexplored opportunity improvements machine read process
many problems nlp require aggregate information multiple mention entity may far apart text exist recurrent neural network rnn layer bias towards short term dependencies hence suit task present recurrent layer instead bias towards coreferent dependencies layer use coreference annotations extract external system connect entity mention belong cluster incorporate layer state art read comprehension model improve performance three datasets wikihop lambada babi ai task large gain train data scarce
present first real world application methods improve neural machine translation nmt human reinforcement base explicit implicit user feedback collect ebay e commerce platform previous work confine simulation experiment whereas paper work real log feedback offline bandit learn nmt parameters conduct thorough analysis available explicit user judgments five star rat translation quality show reliable enough yield significant improvements bandit learn contrast successfully utilize implicit task base feedback collect cross lingual search task improve task specific machine translation quality metrics
automatic colorization process add color greyscale image condition process language allow end users manipulate colorize image feed different caption present two different architectures language condition colorization produce accurate plausible colorizations language agnostic version language base framework dramatically alter colorizations manipulate descriptive color word caption
query auto completion qac systems standard part search engines industry help users formulate query systems update suggestions user type character predict user intent use various signal one common popularity recently deep learn approach propose qac task specifically address main limitation previous popularity base methods inability predict unseen query work improve previous methods base neural language model goal build end end system particularly focus use real world data integrate user information personalize suggestions possible also make use time information study increase diversity suggestions study impact scalability empirical result demonstrate mark improvement two separate datasets previous best methods accuracy scalability make step towards neural query auto completion production search engines
online antisocial behavior cyberbullying harassment troll widespread problem threaten free discussion negative physical mental health consequences victims communities prior work propose automate methods identify hostile comment online discussions methods work retrospectively comment already post make difficult intervene interaction escalate paper instead consider problem forecast future hostilities online discussions decompose two task one give initial sequence non hostile comment discussion predict whether future comment contain hostility two give first hostile comment discussion predict whether lead escalation hostility subsequent comment thus aim forecast presence intensity hostile comment base linguistic social feature earlier comment evaluate approach introduce corpus 30k annotate instagram comment one thousand, one hundred post approach able predict appearance hostile comment instagram post ten hours future auc eighty-two task one furthermore distinguish high low level future hostility auc ninety-one task two
introduce new benchmark winobias coreference resolution focus gender bias corpus contain winograd schema style sentence entities correspond people refer occupation eg nurse doctor carpenter demonstrate rule base feature rich neural coreference system link gendered pronouns pro stereotypical entities higher accuracy anti stereotypical entities average difference two hundred and eleven f1 score finally demonstrate data augmentation approach combination exist word embed debiasing techniques remove bias demonstrate systems winobias without significantly affect performance exist coreference benchmark datasets dataset code available http winobiasorg
demonstrate current state art approach automate essay score aes well suit capture adversarially craft input grammatical incoherent sequence sentence develop neural model local coherence effectively learn connectedness feature sentence propose framework integrate jointly train local coherence model state art aes model evaluate approach number baselines experimentally demonstrate effectiveness aes task task flag adversarial input contribute development approach strengthen validity neural essay score model
depression rank largest contributor global disability also major reason suicide still many individuals suffer form depression treat various reason previous study show depression also effect language usage many depress individuals use social media platforms internet general get information discuss problems paper address early detection depression use machine learn model base message social platform particular convolutional neural network base different word embeddings evaluate compare classification base user level linguistic metadata ensemble approach show achieve state art result current early detection task furthermore currently popular erde score metric early detection systems examine detail drawbacks context share task illustrate slightly modify metric propose compare original score finally new word embed train large corpus domain describe task evaluate well
social media feature substantial stylistic variation raise new challenge syntactic analysis online write however variation often align author attribute age gender geography well readily available social network metadata paper report new evidence link language social network task part speech tag find tagger error rat correlate network structure high accuracy part network lower accuracy elsewhere result tagger accuracy depend train balance sample network rather train texts narrow subcommunity also describe attempt add robustness stylistic variation build mixture experts model expert associate region social network prior work find similar approach yield performance improvements sentiment analysis entity link unable obtain performance improvements part speech tag despite strong evidence link part speech error rat social network structure
primary aim project build contextual question answer model videos current methodologies provide robust model image base question answer aim generalize approach videos propose graphical representation video able handle several type query across whole video example frame image man cat sit able handle query like cat sit respect man man hold hand able answer query relate temporal relationships also
verifiability one core edit principles wikipedia editors encourage provide citations add statements statements arbitrary piece text range sentence paragraph however many case citations either outdated miss link non exist reference eg dead url move content etc total twenty case citations refer news article represent second cite source even case citations provide explicit indicators span citation give piece text addition issue relate verifiability principle many wikipedia entity page incomplete relevant information already available online news source miss even already exist citations often delay news publication time reference time thesis address aforementioned issue propose automate approach enforce verifiability principle wikipedia suggest relevant miss news reference enrich wikipedia entity page
dialogue policy transfer enable us build dialogue policies target domain little data leverage knowledge source domain plenty data dialogue sentence usually represent speech act domain slot dialogue policy transfer usually achieve assign slot map matrix base human heuristics however exist dialogue policy transfer methods transfer across dialogue domains different speech act example systems build different company also depend either common slot slot entropy available source target slot totally disjoint database available calculate slot entropy solve problem propose policy transfer across domains speech act promise model able transfer dialogue policies across domains different speech act disjoint slot promise model learn align different speech act slot simultaneously require common slot calculation slot entropy experiment real world dialogue data simulations demonstrate promise model effectively transfer dialogue policies across domains different speech act disjoint slot
continuous word representations learn separately distinct languages align word become comparable common space exist work typically solve least square regression problem learn rotation align small bilingual lexicon use retrieval criterion inference paper propose unify formulation directly optimize retrieval criterion end end fashion experiment standard benchmarks show approach outperform state art word translation biggest improvements observe distant language pair english chinese
structure data summarization involve generation natural language summaries structure input data work consider summarize structure data occur form table prevalent across wide variety domains formulate standard table summarization problem deal table conform single predefined schema end propose mix hierarchical attention base encoder decoder model able leverage structure addition content table experiment publicly available weathergov dataset show around eighteen bleu thirty improvement current state art
construct multilingual common semantic space base distributional semantics word multiple languages project share space enable knowledge resource transfer across languages beyond word alignment introduce multiple cluster level alignments enforce word cluster consistently distribute across multiple languages exploit three signal cluster one neighbor word monolingual word embed space two character level information three linguistic properties eg apposition locative suffix derive linguistic structure knowledge base available thousands languages introduce new cluster consistent correlational neural network construct common semantic space align word well cluster intrinsic evaluation monolingual multilingual qvec task show approach achieve significantly higher correlation linguistic feature state art multi lingual embed learn methods use low resource language name tag case study extrinsic evaluation approach achieve two hundred and forty-five absolute f score gain state art
build semantic parser quickly new domain fundamental challenge conversational interfaces current semantic parsers require expensive supervision lack ability generalize new domains paper introduce zero shoot approach semantic parse parse utterances unseen domains train examples source domains first map utterance abstract domain independent logical form represent structure logical form contain slot instead kb constants replace slot kb constants via lexical alignment score global inference model reach average accuracy five hundred and thirty-four seven domains overnight dataset substantially better zero shoot baselines perform good parser train thirty target domain examples
propose post process method enrich word representation also vector space use semantic lexicons call extrofitting method consist three step follow expand one dimension word vectors fill representative value ii transfer semantic knowledge average representative value synonyms fill expand dimension two step make representations synonyms close together iii project vector space use linear discriminant analysis eliminate expand dimension semantic knowledge experiment glove find method outperform faruqui retrofit word similarity task also report analysis method respect word vector dimension vocabulary size well well know pretrained word vectors eg word2vec fasttext
rapid development recently community question answer cqa satisfy users quest professional personal knowledge anything cqa one central issue find users expertise willingness answer give question expert find cqa often exhibit different challenge compare traditional methods sparse data new feature violate fundamental assumptions traditional recommendation systems paper focus review categorize current progress expert find cqa classify exist solutions four different categories matrix factorization base model mf base model gradient boost tree base model gbt base model deep learn base model dl base model rank base model r base model find mf base model outperform categories model field expert find cqa moreover use innovative diagram clarify several important concepts ensemble learn find ensemble model several specific single model boost performance compare performance different model different type match task include text vs text graph vs text audio vs text video vs text result help model selection expert find practice finally explore potential future issue expert find research cqa
state art approach embed base unsupervised semantic search exploit either compositional similarity query passage pair wise word term similarity query passage design word base approach incorporate similarity larger context query passage compositional similarity base approach usually unable take advantage important cue context paper propose new compositional similarity base approach call variable centroid vector vcvb try address limitations also present result use different type compositional similarity base approach exploit universal sentence embed provide empirical evaluation two different benchmarks
encoder decoder dialog model one prominent methods use build dialog systems complex domains yet limit output interpretable action traditional systems hinder humans understand generation process present unsupervised discrete sentence representation learn method integrate exist encoder decoder dialog model interpretable response generation build upon variational autoencoders vaes present two novel model di vae di vst improve vaes discover interpretable semantics via either auto encode context predict methods validate real world dialog datasets discover semantic representations enhance encoder decoder model interpretable generation
quality train data one crucial problems learn center approach employ paper propose new method investigate quality large corpus design recognize textual entailment rte task propose method inspire statistical hypothesis test consist two phase first phase introduce predictability textual entailment label null hypothesis extremely unacceptable target corpus hide bias second phase test null hypothesis use naive bay model experimental result stanford natural language inference snli corpus reject null hypothesis therefore indicate snli corpus hide bias allow prediction textual entailment label hypothesis sentence even context information give premise sentence paper also present performance impact nn model rte cause hide bias
distribute representation play important role deep learn base natural language process however representation sentence often vary different task usually learn scratch suffer limit amount train data paper claim good sentence representation invariant benefit various subsequent task achieve purpose propose new scheme information share multi task learn specifically task share sentence representation task select task specific information share sentence representation attention mechanism query vector task attention could either static parameters generate dynamically conduct extensive experiment sixteen different text classification task demonstrate benefit architecture
end end dialog systems become popular hold promise learn directly human human dialog interaction retrieval generative methods explore area mix result key element miss far incorporation priori knowledge task hand knowledge may exist form structure unstructured information first step towards direction present novel approach knowledge base end end memory network kb memn2n allow special handle name entities goal orient dialog task present result two datasets dstc6 challenge dataset dialog babi task
bilingual word embeddings represent word two languages space allow transfer knowledge one language without machine translation main approach train monolingual embeddings first map use bilingual dictionaries work present novel method learn bilingual embeddings base multilingual knowledge base kb wordnet method extract bilingual information multilingual wordnets via random walk learn joint embed space one go reinforce cross lingual equivalence add bilingual con straints loss function popular skipgram model experiment involve twelve cross lingual word similarity relatedness datasets six lan guage pair cover four languages show one random walk mul tilingual wordnets improve result use dictionaries two multilingual wordnets improve text base systems similarity datasets three good result consistent large wordnets eg english spanish smaller wordnets eg basque loosely align wordnets eg italian four combination wordnets text yield best result map base approach method apply richer kbs like dbpedia babel net easily extend multilingual embeddings software resources open source
present lightrel lightweight fast relation classifier goal develop high baseline different relation extraction task define data internal word level feature external knowledge source form word cluster word embeddings train fast simple linear classifier
understand characterize people interact information seek conversations crucial develop conversational search systems paper introduce new dataset design purpose use analyze information seek conversations user intent distribution co occurrence flow pattern msdialog dataset label dialog dataset question answer qa interactions information seekers providers online forum microsoft products dataset contain two thousand multi turn qa dialogs ten thousand utterances annotate user intent utterance level annotations do use crowdsourcing msdialog find highly recur pattern user intent information seek process could useful design conversational search systems make dataset freely available encourage exploration information seek conversation model
simplequestions dataset one commonly use benchmarks study single relation factoid question paper present new evidence benchmark nearly solve standard methods first show ambiguity data bound performance benchmark eight hundred and thirty-four often multiple answer disambiguate linguistic signal alone second introduce baseline set new state art performance level seven hundred and eighty-one accuracy despite use standard methods finally report empirical analysis show upperbound loose roughly third remain errors also resolvable linguistic signal together result suggest simplequestions dataset nearly solve
propose graph base mechanism extract rich emotion bear pattern foster deeper analysis online emotional expressions corpus pattern enrich word embeddings evaluate several emotion recognition task moreover conduct analysis emotion orient pattern demonstrate applicability explore properties experimental result demonstrate propose techniques outperform state art emotion recognition techniques
study problem name entity recognition ner electronic medical record one fundamental critical problems medical text mine medical record write clinicians different specialties usually contain quite different terminologies write style difference specialties cost human annotation make particularly difficult train universal medical ner system paper propose label aware double transfer learn framework la dtl cross specialty ner medical ner system design one specialty could conveniently apply another one minimal annotation efforts transferability guarantee two components propose label aware mmd feature representation transfer ii perform parameter transfer theoretical upper bind also label aware conduct extensive experiment twelve cross specialty ner task experimental result demonstrate la dtl provide consistent accuracy improvement strong baselines besides promise experimental result non medical ner scenarios indicate la dtl potential seamlessly adapt wide range ner task
many natural language process nlp task depend use name entities nes contain texts external knowledge source easy humans present neural methods rely learn word embeddings may perform well nlp task especially presence vocabulary oov rare nes paper propose solution problem present empirical evaluations structure question answer task b three relate goal orient dialog task c read comprehension task show propose method effective deal vocabulary oov nes create extend versions dialog babi task twelve four oov versions cbt test set available https githubcom ibm ne table datasets
prosody usually define term three distinct interact domains pitch intensity duration pattern generally phonological phonetic properties suprasegmentals speech segment larger consonants vowels rather take approach concept multiple time domains prosody process take methods time domain analysis discuss annotation mine time dispersion measure time tree induction oscillator model phonology phonetics finally use amplitude envelope modulation spectrum aems frequency demodulation form pitch track central issue prosodic analysis present context amplitude envelope demodulation frequency zone long time domain spectra demodulate envelope focus generalise view take oscillation iteration abstract prosodic model modulation demodulation variety rhythms speech signal
transcribe voice communications nasa launch control center important information utilization however automatic speech recognition environment particularly challenge due lack train data unfamiliar word acronyms multiple different speakers accent conversational characteristics speak use bidirectional deep recurrent neural network train test speech recognition performance show data augmentation custom language model improve speech recognition accuracy transcribe communications launch control center help machine analyze information accelerate knowledge generation
peer review central component scientific publish process present first public dataset scientific peer review available research purpose peerread v1 provide opportunity study important artifact dataset consist 147k paper draft correspond accept reject decisions top tier venues include acl nip iclr dataset also include 107k textual peer review write experts subset paper describe data collection process report interest observe phenomena peer review also propose two novel nlp task base dataset provide simple baseline model first task show simple model predict whether paper accept twenty-one error reduction compare majority baseline second task predict numerical score review aspects show simple model outperform mean baseline aspects high variance originality impact
query auto completion search engine feature whereby system suggest complete query user type recently use recurrent neural network language model suggest method generate query completions show adaptable language model use generate personalize completions model use online update make predictions users see train personalize predictions significantly better baseline use user information
non standard analysis area mathematics deal notions infinitesimal infinitely large number many statements classical analysis express naturally cheap non standard analysis introduce terence tao two thousand and twelve base idea consider property hold eventually sufficient give essence many statements provide constructivity acceptable price consider computability cheap non standard analysis prove many concepts computable analysis well several concepts computability elegantly alternatively present framework provide dual view dual proof several statements already know field
past year witness rapid advance sequence sequence seq2seq model machine translation mt classic rnn base approach mt first perform convolutional seq2seq model perform recent transformer model new approach consist fundamental architecture accompany set model train techniques principle applicable seq2seq architectures paper tease apart new architectures accompany techniques two ways first identify several key model train techniques apply rnn architecture yield new rnmt model outperform three fundamental architectures benchmark wmt fourteen english french english german task second analyze properties fundamental seq2seq architecture devise new hybrid architectures intend combine strengths hybrid model obtain improvements outperform rnmt model benchmark datasets
predict popularity social media videos publish challenge task mainly due complexity content distribution network well number factor play part process solve task provide tremendous help media content creators many successful methods propose solve problem machine learn work change viewpoint postulate predict popularity matter also maybe even importantly understand individual part influence final popularity score end propose combine grad cam visualization method soft attention mechanism preliminary result show approach allow intuitive interpretation content impact video popularity achieve competitive result term prediction accuracy
explore contemporary data drive techniques solve math word problems recent large scale datasets show well tune neural equation classifiers outperform sophisticate model sequence sequence self attention across datasets error analysis indicate fully data drive model show promise semantic world knowledge necessary advance
paper propose novel reinforcement learn base model sequence tag refer mm tag inspire success methodology alphago zero mm tag formalize problem sequence tag monte carlo tree search mcts enhance markov decision process mdp model time step correspond position word sentence leave right action correspond assign tag word two long short term memory network lstm use summarize past tag assignments word sentence base output lstms policy guide tag assignment value predict whole tag accuracy whole sentence produce policy value strengthen mcts take produce raw policy value input simulate evaluate possible tag assignments subsequent position output better search policy assign tag reinforcement learn algorithm propose train model parameters work first apply mcts enhance mdp model sequence tag task show mm tag accurately predict tag thank exploratory decision make mechanism introduce mcts experimental result show base chunk benchmark show mm tag outperform state art sequence tag baselines include crf crf lstm
work address problem fast scalable learn neuro symbolic representations general biological knowledge base recently publish comprehensive biological knowledge graph alshahrani two thousand and seventeen use demonstrate neuro symbolic representation learn show train fast one minute log linear neural embeddings entities utilize representations input machine learn classifiers enable important task biological link prediction classifiers train concatenate learn entity embeddings represent entity relations train classifiers concatenate embeddings discern true relations automatically generate negative examples simple embed methodology greatly improve classification error compare previously publish state art result yield maximum increase twenty-eight f measure twenty-two roc auc score difficult biological link prediction problem finally embed approach order magnitude faster train leq one minute vs hours much economical term embed dimension d50 vs d512 naturally encode directionality asymmetric biological relations control order concatenate embeddings
paper introduce notion demand weight completeness allow estimation completeness knowledge base respect use define entity class employ usage data predict distribution relations entity example instance person knowledge base may require birth date name nationality consider complete predict relation distributions enable detection important gap knowledge base define require facts unseen entities characterisation knowledge base also quantify usage completeness change time demonstrate method measure demand weight completeness show simple neural network model perform well prediction task
past decade steep rise data drive analysis major areas medicine clinical decision support system survival analysis patient similarity analysis image analytics etc data field well structure available numerical categorical format use experiment directly opposite end spectrum exist wide expanse data intractable direct analysis owe unstructured nature find form discharge summaries clinical note procedural note human write narrative format neither relational model standard grammatical structure important step utilization texts study transform process data retrieve structure information haystack irrelevant data use information retrieval data mine techniques address problem author present q map paper simple yet robust system sift massive datasets unregulated format retrieve structure information aggressively efficiently back effective mine technique base string match algorithm index curated knowledge source fast configurable author also briefly examine comparative performance metamap one repute tool medical concepts retrieval present advantage former display latter
recent study investigate siamese network architectures learn invariant speech representations use different side information word level investigate systematically often ignore component siamese network sample procedure pair vs different tokens select show sample strategies take account zipf law distribution speakers proportion different pair word significantly impact performance network particular show word frequency compression improve learn across large range variations number train pair effect apply extent fully unsupervised set pair different word obtain speak term discovery apply result pair word discover use unsupervised algorithm show improvement state art unsupervised representation learn use siamese network
collaborative filter base recommender systems prove extremely successful settings user preference data items abundant however collaborative filter algorithms hinder weakness item cold start problem general lack interpretability ontology base recommender systems exploit hierarchical organizations users items enhance browse recommendation profile construction ontology base approach address shortcomings collaborative filter counterparts ontological organizations items difficult obtain items mostly belong category eg television series episodes paper present ontology base recommender system integrate knowledge represent large ontology literary theme produce fiction content recommendations main novelty work ontology base method compute similarities items integration classical item knn k nearest neighbor algorithm study case evaluate propose method approach perform classical rat prediction task collection star trek television series episodes item cold start scenario transverse evaluation provide insights utility different information resources methods initial stag recommender system development find propose method convenient alternative collaborative filter approach collections mostly similar items particularly content base approach applicable otherwise unavailable aside new methods paper contribute testbed future research online framework collaboratively extend ontology literary theme cover narrative content
order expand reach increase website ad revenue media outlets start use clickbait techniques lure readers click article digital platform successfully entice user open article article fail satiate curiosity serve boost click rat initial methods task dependent feature engineer vary dataset industry systems rely exhaustive set rule get job do neural network barely explore perform task propose novel approach consider different textual embeddings news headline relate article generate sub word level embeddings title use convolutional neural network use train bidirectional lstm architecture attention layer allow calculation significance term towards nature post also generate doc2vec embeddings title article text model interact follow concatenate output previous component finally representation pass neural network obtain score headline test model two thousand, five hundred and thirty-eight post train seventeen thousand record achieve accuracy eight thousand, three hundred and forty-nine outscore previous state art approach
argue logical semantics might falter due failure distinguish two fundamentally different type concepts ontological concepts type strongly type ontology logical concepts predicate correspond properties relations object various ontological type show account differences amount integration lexical compositional semantics one coherent framework embed logical semantics strongly type ontology reflect commonsense view world way talk ordinary language show framework number challenge natural language semantics adequately systematically treat
growth social media usage social activists try leverage platform raise awareness relate social issue engage public worldwide broad use social media platforms recent years make easier people stay date news relate regional worldwide events social media namely twitter assist social movements connect people mobilize movement traditional media news article help spread news relate events broader aspect study analyze linguistic feature cue individualism vs pluralism sentiment emotion examine relationship medium discourse time conduct work specific application context black live matter blm movement compare discussions relate event social media vs news article
propose novel attention base hierarchical lstm model classify discourse act sequence social media conversations aim mine data online discussion use textual mean beyond sentence level uniqueness task complete categorization possible pragmatic roles informal textual discussions contrary extraction question answer stance detection sarcasm identification much role specific task early attempt make reddit discussion dataset train model data present test result two different datasets one reddit one facebook propose model outperform previous one term domain independence without use platform dependent structural feature hierarchical lstm word relevance attention mechanism achieve f1 score seventy-one sixty-six respectively predict discourse roles comment reddit facebook discussions efficiency recurrent convolutional architectures order learn discursive representation task present analyze different word comment embed scheme attention mechanism enable us inquire relevance order text segment accord roles discourse present human annotator experiment unveil important observations model data annotation equip text base discourse identification model inquire heterogeneous non textual feature like location time lean information etc play roles charaterizing online discussions facebook
work propose classifier distinguish device direct query background speech context interactions voice assistants applications include rejection false wake up unintended interactions well enable wake word free follow query consider example interaction computerplaymusic computerreducethevolume interaction user need repeat wake word computer second query allow natural interactions device could immediately enter listen state first query without wake word repetition accept reject potential follow device direct background speech propose model consist two long short term memory lstm neural network train acoustic feature automatic speech recognition asr one best hypotheses respectively fee forward deep neural network dnn train combine acoustic one best embeddings derive lstms feature asr decoder experimental result show asr decoder acoustic embeddings one best embeddings yield equal error rate ever ninety-three one hundred and nine two hundred and one respectively combination feature result forty-four relative improvement final ever fifty-two
paper investigate impact word base rnn language model rnn lms performance end end automatic speech recognition asr prior work propose multi level lm character base word base rnn lms combine hybrid ctc attention base asr although multi level approach achieve significant error reduction wall street journal wsj task two different lms need train use decode increase computational cost memory usage paper propose novel word base rnn lm allow us decode word base lm provide look ahead word probabilities predict next character instead character base lm lead competitive accuracy less computation compare multi level lm demonstrate efficacy word base rnn lms use larger corpus librispeech addition wsj use prior work furthermore show propose model achieve fifty-one wer wsj eval ninety-two test set vocabulary size increase best wer report end end asr systems benchmark
ticket assignment dispatch crucial part service delivery business lot scope automation optimization paper present end end automate helpdesk email ticket assignment system also offer service objective system determine nature problem mention incoming email ticket automatically dispatch appropriate resolver group team resolution propose system use ensemble classifier augment configurable rule engine design classifier accurate one main challenge also need address need design system robust adaptive change business need discuss main design challenge associate email ticket assignment automation solve design decisions system drive high accuracy coverage business continuity scalability optimal usage computational resources system deploy production three major service providers currently assign forty thousand email per month average accuracy close ninety cover least ninety email ticket translate achieve human level accuracy result net save twenty-three thousand man hours effort per annum
web era since technology revolutionize mankind life plenty data information publish internet day instance news agencies publish news websites world raw data could important resource knowledge extraction share data contain emotions ie positive neutral negative toward various topics therefore sentimental content extraction could beneficial task many aspects extract sentiment news illustrate highly valuable information events period time viewpoint media news agency events paper attempt make propose approach news analysis extract useful knowledge firstly attempt extract noise robust sentiment news document therefore news associate six countries unite state unite kingdom germany canada france australia five different news categories politics sport business entertainment technology download paper compare condition different countries five news topics base extract sentiments emotional content news document moreover propose approach reduce bulky news data extract hottest topics news title knowledge eventually generate word model map word fix size vector word2vec order understand relations word collect news database
vision common source inspiration poetry object sentimental imprint one perceive image may lead various feel depend reader paper present system poetry generation image mimic process give image first extract keywords represent object sentiments perceive image keywords expand relate ones base associations human write poems finally verse generate gradually keywords use recurrent neural network train exist poems approach evaluate human assessors compare generation baselines result show method generate poems artistic baseline methods one attempt generate poetry image deploy propose approach xiaoice already generate twelve million poems users since release july two thousand and seventeen book poems publish cheer publish claim book first ever poetry collection write ai human history
healthcare organizations continuous effort improve health outcomes reduce cost enhance patient experience care data essential measure help achieve improvements healthcare delivery consequently data influx various clinical financial operational source overtake healthcare organizations patients effective use data however major challenge clearly text important medium make data accessible financial report produce assess healthcare organizations key performance indicators steer healthcare delivery similarly clinical level data patient status convey mean textual descriptions facilitate patient review shift handover care transition likewise patients inform data health status treatments via text form report via ehealth platforms doctor unfortunately text outcome highly labour intensive process do healthcare professionals also prone incompleteness subjectivity hard scale different domains wider audiences vary communication purpose data text recent breakthrough technology artificial intelligence automatically generate natural language form text speech data chapter provide survey data text technology focus deploy healthcare set one give date synthesis data text approach two give categorize overview use case healthcare three seek make strong case evaluate implement data text healthcare set four highlight recent research challenge
knowledge graph embed kge aim represent entities relations knowledge graph low dimensional continuous vector space recent work focus incorporate structural knowledge additional information entity descriptions relation paths however common use additional information usually contain plenty noise make hard learn valuable representation paper propose new kind additional information call entity neighbor contain semantic topological feature give entity develop deep memory network model encode information neighbor employ gate mechanism representations structure neighbor integrate joint representation experimental result show model outperform exist kge methods utilize entity descriptions achieve state art metrics four datasets
take practical approach solve sequence label problem assume unavailability domain expertise scarcity informational computational resources end utilize universal end end bi lstm base neural sequence label model applicable wide range nlp task languages model combine morphological semantic structural cue extract data arrive inform predictions model performance evaluate eight benchmark datasets cover three task pos tag ner chunk four languages english german dutch spanish observe state art result four conll two thousand and twelve english ner conll two thousand and two dutch ner germeval two thousand and fourteen german ner tiger corpus german pos tag competitive performance rest
paper present three hybrid model directly combine latent dirichlet allocation word embed distinguish speakers without alzheimer disease transcripts picture descriptions two model get f score current state art use automatic methods dementiabank dataset
paper introduce embed model name capse explore capsule network model relationship triple subject relation object capse represent triple three column matrix column vector represent embed element triple three column matrix feed convolution layer multiple filter operate generate different feature map feature map reconstruct correspond capsule rout another capsule produce continuous vector length vector use measure plausibility score triple propose capse obtain better performance previous state art embed model knowledge graph completion two benchmark datasets wn18rr fb15k two hundred and thirty-seven outperform strong search personalization baselines search17
patternattribution recent method introduce vision domain explain classifications deep neural network demonstrate also generate meaningful interpretations language domain
compute universal distribute representations sentence fundamental task natural language process propose conssent simple yet surprisingly powerful unsupervised method learn representations enforce consistency constraints sequence tokens consider two class constraints sequence form sentence two sequence form sentence merge learn sentence encoders train distinguish consistent inconsistent examples latter generate randomly perturb consistent examples six different ways extensive evaluation several transfer learn linguistic probe task show improve performance strong unsupervised supervise baselines substantially surpass several case best result achieve train sentence encoders multitask set ensemble encoders train individual task
one thousand, nine hundred and ninety-three homophonic quotient group french english quotient free group generate french respectively english alphabet determine relations represent standard pronunciation rule explicitly characterize five paper apply methodology three different language systems german korean turkish argue result point interest differences three languages least current script systems
text match fundamental problem natural language process neural model use bidirectional lstms sentence encode inter sentence attention mechanisms perform remarkably well several benchmark datasets propose regmapr simple general architecture text match use inter sentence attention start siamese architecture augment embeddings word two feature base exact para phrase match word two sentence train model use three type regularization datasets textual entailment paraphrase detection semantic relate ness regmapr perform comparably better complex neural model model use large number handcraft feature regmapr achieve state art result paraphrase detection sick dataset textual entailment snli dataset among model use inter sentence attention
paper investigate diversity aspect paraphrase generation prior deep learn model employ either decode methods add random input noise vary output propose simple method diverse paraphrase generation page extend neural machine translation nmt model support generation diverse paraphrase implicit rewrite pattern experimental result two real world benchmark datasets demonstrate model generate least one order magnitude diverse output baselines term new evaluation metric jeffrey divergence also conduct extensive experiment understand various properties model focus diversity
number recent machine learn paper work automate style transfer texts counter intuition demonstrate consensus formulation nlp task different researchers propose different algorithms datasets target metrics address short opinion paper aim discuss possible formalization nlp task anticipation grow interest
design reliable natural language nl interface query table longtime goal researchers data management natural language process nlp communities interface receive input nl question translate formal query execute query return result errors translation process uncommon users typically struggle understand whether query map correctly address problem explain obtain formal query non expert users two methods query explanations present first translate query nl second method provide graphic representation query cell base provenance execution give table solution augment state art nl interface web table enhance train deployment phase experiment include user study conduct amazon mechanical turk show solution improve correctness reliability nl interface
paper investigate data drive segmentation use pair byte pair encode techniques contrast previous work primarily focus subword units machine translation interest general properties segment word level call segment r grams discuss properties effect token frequency distribution propose approach evaluate demonstrate viability embed techniques monolingual multilingual test settings also provide number qualitative examples propose methodology demonstrate viability language invariant segmentation procedure
traditional text classification techniques clinical domain heavily rely manually extract textual cue paper propose generally supervise machine learn method equally hassle free use clinical knowledge employ methods simple implement fast run yet effective paper propose novel name entity recognition ner base ensemble system capable learn keyword feature document instead merely consider whole sentence paragraph analysis ner base keyword feature stress important clinic relevant phase addition capture semantic information document fasttext feature originate document level fasttext classification result exploit
increase popularity smart devices rumor multimedia content become common social network multimedia information usually make rumor look convince therefore find automatic approach verify rumor multimedia content press task previous rumor verification research utilize multimedia input feature propose use multimedia content find external information news platforms pivot introduce new feature set cross lingual cross platform feature leverage semantic similarity rumor external information implement machine learn methods utilize feature achieve state art rumor verification result
current state art automatic speech recognition systems train work specific domains define base factor like application sample rate codec recognizers use condition match train domain performance significantly drop work explore idea build single domain invariant model vary use case combine large scale train data multiple application domains final system train use one hundred and sixty-two thousand hours speech additionally utterance artificially distort train simulate effect like background noise codec distortion sample rat result show even scale model thus train work almost well fine tune specific subsets single model robust multiple application domains variations like codecs noise importantly model generalize better unseen condition allow rapid adaptation show use little ten hours data new domain adapt domain invariant model match performance domain specific model train scratch use seventy time much data also highlight limitations model areas need address future work
represent text set word co occurrences one obtain word adjacency network reduce representation give language sample paper possibility use network representation extract information individual language style literary texts study determine select quantitative characteristics network apply machine learn algorithms possible distinguish texts different author within study set texts english polish properly rescale weight cluster coefficients weight degrees nod word adjacency network sufficient obtain authorship attribution accuracy ninety correspondence text authorship word adjacency network structure therefore find network representation allow distinguish individual language style compare way author use particular word punctuation mark present approach view generalization authorship attribution methods base simple lexical feature additionally network parameters study local global ones unweighted weight network potential capture write style diversity discuss differences languages observe
sequence generative adversarial network seqgan use improve conditional sequence generation task example chit chat dialogue generation stabilize train seqgan monte carlo tree search mcts reward every generation step regs use evaluate goodness generate subsequence mcts computationally intensive performance regs worse mcts paper propose stepwise gin stepgan discriminator modify automatically assign score quantify goodness subsequence every generation step stepgan significantly less computational cost mcts demonstrate stepgan outperform previous gin base methods synthetic experiment chit chat dialogue generation
highly regularize lstms achieve impressive result several benchmark datasets language model propose new regularization method base decode last token context use predict distribution next token bias model towards retain contextual information turn improve ability predict next token negligible overhead number parameters train time past decode regularization pdr method achieve word level perplexity five hundred and fifty-six penn treebank six hundred and thirty-five wikitext two datasets use single softmax also show gain use pdr combination mixture softmaxes achieve word level perplexity five hundred and thirty-eight six hundred and five datasets addition method achieve one thousand, one hundred and sixty-nine bits per character penn treebank character dataset character level language model result constitute new state art respective settings
everyday place descriptions often contain place name fine grain feature build businesses difficult disambiguate name refer larger place example cities natural geographic feature fine grain place often significantly frequent similar disambiguation heuristics develop larger place base population containment relationships often applicable case research address disambiguation fine grain place name everyday place descriptions purpose evaluate performance different exist cluster base approach since cluster approach require knowledge locations ambiguous place name consider approach develop specifically place name disambiguation also cluster algorithms develop general data mine could potentially leverage compare methods novel algorithm show novel algorithm outperform algorithms term disambiguation precision distance error several test datasets
task orient dialog systems become pervasive many company heavily rely complement human agents customer service call center globalization need provide cross lingual customer support become urgent ever however cross lingual support pose great challenge require large amount additional annotate data native speakers order bypass expensive human annotation achieve first step towards ultimate goal build universal dialog system set build cross lingual state track framework specifically assume exist source language dialog belief track annotations target languages annotate dialog data form pre train state tracker source language teacher able exploit easy access parallel data distill transfer knowledge student state tracker target languages specifically discuss two type common parallel resources bilingual corpus bilingual dictionary design different transfer learn strategies accordingly experimentally successfully use english state tracker teacher transfer knowledge italian german trackers achieve promise result
reinforcement learn methods use learn dialogue policies however learn effective dialogue policy frequently require prohibitively many conversations partly sparse reward dialogues successful dialogues early learn phase hindsight experience replay enable learn failures vanilla inapplicable dialogue learn due implicit goals work develop two complex methods provide different trade off complexity performance first time enable base dialogue policy learn experiment use realistic user simulator show methods perform better exist experience replay methods apply deep q network learn rate
wide variety neural network architectures propose task chinese word segmentation surprisingly find bidirectional lstm model combine standard deep learn techniques best practice achieve better accuracy many popular datasets compare model base complex neural network architectures furthermore error analysis show vocabulary word remain challenge neural network model many remain errors unlikely fix architecture change instead effort make explore resources improvement
linguistic feature show promise applications detect various cognitive impairments improve detection accuracies increase amount data number linguistic feature two applicable approach however acquire additional clinical data expensive hand craft feature burdensome paper take third approach propose consensus network cns framework classify reach agreements modalities divide linguistic feature non overlap subsets accord modalities let neural network learn low dimensional representations agree representations pass classifier network neural network optimize iteratively paper also present two methods improve performance cns present ablation study illustrate effectiveness modality division understand happen cns visualize representations train overall use four hundred and thirteen linguistic feature model significantly outperform traditional classifiers use state art paper
give text description exist semantic parsers synthesize program one shoot however quite challenge produce correct program solely base description reality often ambiguous incomplete paper investigate interactive semantic parse agent ask user clarification question resolve ambiguities via multi turn dialogue important type program call recipes develop hierarchical reinforcement learn hrl base agent significantly improve parse performance minimal question user result simulation human evaluation show agent substantially outperform non interactive semantic parsers rule base agents
propose framework automatically generate descriptive comment source code block problem study many researchers previously methods mostly base fix template achieve poor result framework rely template make use new recursive neural network call code rnn extract feature source code embed one vector vector representation input new recurrent neural network code gru overall framework generate text descriptions code accuracy rouge two value significantly higher learn base approach sequence sequence model code rnn model also use scenario representation code require
several attempt define plausible motivation chit chat dialogue agent lead engage conversations work explore new direction agent specifically focus discover information interlocutor formalize approach define quantitative metric propose algorithm agent maximize validate idea human evaluation system outperform various baselines demonstrate metric indeed correlate human judgments engagingness
exploit fact natural languages complex systems present exploratory article propose direct method base frequency distributions may useful make decision status problematic phonemes open problem linguistics main notion natural languages consider complex outlook information process machine somehow manage set appropriate level redundancy already make choice whether linguistic unit phoneme would reflect greater smoothness frequency versus rank graph particular case choose study conclude reasonable consider spanish semiconsonant w separate phoneme vowel counterpart one hand possibly also semiconsonant j separate phoneme vowel counterpart language central topic study complexity discussion grant us addition opportunity gain insight emerge properties broader complex systems debate
development information technology explosive growth number online comment concern news blog massive comment overload often contain mislead unwelcome information therefore necessary identify high quality comment filter low quality comment work introduce novel task high quality comment identification hqci aim automatically assess quality online comment first construct news comment corpus consist news comment correspond quality label second analyze dataset find quality comment measure three aspects informativeness consistency novelty finally propose novel multi target text match model measure three aspects refer news surround comment experimental result show method outperform various baselines large margin news dataset
classification sentence challenge since sentence contain limit contextual information paper propose attention gate convolutional neural network agcnn sentence classification generate attention weight feature context windows different size use specialize convolution encoders make full use limit contextual information extract enhance influence important feature predict sentence category experimental result demonstrate model achieve thirty-one higher accuracy standard cnn model gain competitive result baselines four six task besides design activation function namely natural logarithm rescale rectify linear unit nlrelu experiment show nlrelu outperform relu comparable well know activation function agcnn
neural machine translation nmt model base sequence sequence seq2seq model encoder decoder framework equip attention mechanism however conventional attention mechanism treat decode time step equally matrix problematic since softness attention different type word eg content word function word differ therefore propose new model mechanism call self adaptive control temperature sact control softness attention mean attention temperature experimental result chinese english translation english vietnamese translation demonstrate model outperform baseline model analysis case study show model attend relevant elements source side contexts generate translation high quality
generative adversarial network gans show great capacity image generation discriminative model guide train generative model construct image resemble real image recently gans extend generate image generate sequence eg poems music cod exist gans sequence generation mainly focus general sequence grammar free many real world applications however need generate sequence formal language constraint correspond grammar example test performance database one may want generate collection sql query similar query real users also follow sql syntax target database generate sequence highly challenge generator discriminator gans need consider structure sequence give grammar formal language address issue study problem syntax aware sequence generation gans collection real sequence set pre define grammatical rule give discriminator generator propose novel gin framework namely treegan incorporate give context free grammar cfg sequence generation process treegan generator employ recurrent neural network rnn construct parse tree generate parse tree translate valid sequence give grammar discriminator use tree structure rnn distinguish generate tree real tree show treegan generate sequence cfg generation fully conform give syntax experiment synthetic real data set demonstrate treegan significantly improve quality sequence generation context free languages
exist neural semantic parsers mainly utilize sequence encoder ie sequential lstm extract word order feature neglect valuable syntactic information dependency graph constituent tree paper first propose use textitsyntactic graph represent three type syntactic information ie word order dependency constituency feature employ graph sequence model encode syntactic graph decode logical form experimental result benchmark datasets show model comparable state art jobs640 atis geo880 experimental result adversarial examples demonstrate robustness model also improve encode syntactic information
design share neural architecture play important role multi task learn challenge find optimal share scheme heavily rely expert knowledge scalable large number diverse task inspire promise work neural architecture search nas apply reinforcement learn automatically find possible share architecture multi task learn specifically use controller select set shareable modules assemble task specific architecture repeat procedure task controller train reinforcement learn maximize expect accuracies task conduct extensive experiment two type task text classification sequence label demonstrate benefit approach
sequence sequence model introduce tackle many real life problems like machine translation summarization image caption etc standard optimization algorithms mainly base example example match like maximum likelihood estimation know suffer data sparsity problem present alternate view explain sequence sequence learn distribution match problem source target example view represent local latent distribution source target domain interpret sequence sequence learn learn transductive model transform source local latent distributions match correspond target distributions framework approximate source target latent distributions recurrent neural network augmenter train parallel augmenters learn better approximate local latent distributions sequence prediction model learn minimize kl divergence transform source distributions approximate target distributions algorithm alleviate data sparsity issue sequence learn locally augment unseen data pair increase model robustness experiment conduct machine translation image caption consistently demonstrate superiority propose algorithm compete algorithms
neural language model keep track number agreement subject verb show diagnostic classifiers train predict number internal state language model provide detail understand information represent moreover give us insight number information corrupt case language model end make agreement errors demonstrate causal role play representations find use agreement information influence course lstm process difficult sentence result intervention reveal large increase language model accuracy together result show diagnostic classifiers give us unrivalled detail look representation linguistic information neural model demonstrate knowledge use improve performance
various forums question answer qanda sit available online allow ubuntu users find result similar query however search result often time consume require user find specific problem instance relevant query large set question paper present automate question answer system ubuntu users call dr tux design answer user query select similar question online database prototype implement python use nltk corenlp tool natural language process data prototype take askubuntu website contain 150k question result obtain manual evaluation prototype promise also present interest opportunities improvement
paper propose extend recently introduce model agnostic meta learn algorithm maml low resource neural machine translation nmt frame low resource translation meta learn problem learn adapt low resource languages base multilingual high resource language task use universal lexical representationcitepgu2018universal overcome input output mismatch across different languages evaluate propose meta learn strategy use eighteen european languages bg cs da de el es et fr hu lt nl pl pt sk sl sv ru source task five diverse languages ro lv fi tr ko target task show propose approach significantly outperform multilingual transfer learn base approachcitepzoph2016transfer enable us train competitive nmt system fraction train examples instance propose approach achieve high two thousand, two hundred and four bleu romanian english wmt sixteen see sixteen thousand translate word six hundred parallel sentence
explore two methods represent author context textual sarcasm detection bayesian approach directly represent author propensities sarcastic dense embed approach learn interactions author text use sarc dataset reddit comment show augment bidirectional rnn representations improve performance bayesian approach suffice homogeneous contexts whereas add power dense embeddings prove valuable diverse ones
paper describe submission semeval two thousand and eighteen task seven share task semantic relation extraction classification scientific paper extend end end relation extraction model miwa bansal enhancements character level encode attention mechanism select pretrained concept candidate embeddings official submission rank second relation classification task subtask eleven subtask two senerio two first relation extraction task subtask two scenario one
sentence level representations necessary various nlp task recurrent neural network prove effective learn distribute representations train efficiently natural language inference task build top one model propose hierarchy bilstm max pool layer implement iterative refinement strategy yield state art result scitail dataset well strong result snli multinli show sentence embeddings learn way utilize wide variety transfer learn task outperform infersent seven ten skipthought eight nine senteval sentence embed evaluation task furthermore model beat infersent model eight ten recently publish senteval probe task design evaluate sentence embeddings ability capture important linguistic properties sentence
paper describe system develop amobee wassa two thousand and eighteen implicit emotions share task iest goal task predict emotion express miss word tweet without explicit mention word develop ensemble system consist language model together lstm base network contain cnn attention mechanism approach represent novel use language model specifically train large twitter dataset predict classify emotions system reach 1st place macro textf1 score seven thousand, one hundred and forty-five
present setexpander corpus base system expand seed set term amore complete set term belong semantic class setexpander implement iterative end end workflow enable users easily select seed set term expand view expand set validate expand validate set store thus simplify extraction domain specific fine grain semantic classessetexpander use successfully real life use case include integration automate recruitment system issue defect resolution system video demo setexpander available https drivegooglecom openid1e545bb87autsch36djnjhmq3hwfsd1rv image blur privacy reason
unsupervised learn syntactic structure typically perform use generative model discrete latent variables multinomial parameters case model leverage continuous word representations work propose novel generative model jointly learn discrete syntactic structure continuous word representations unsupervised fashion cascade invertible neural network structure generative prior show invertibility condition allow efficient exact inference marginal likelihood computation model long prior well behave experiment instantiate approach markov tree structure priors evaluate two task part speech pos induction unsupervised dependency parse without gold pos annotation penn treebank markov structure model surpass state art result pos induction similarly find tree structure model achieve state art performance unsupervised dependency parse difficult train condition neither gold pos annotation punctuation base constraints available
textual entailment model focus lexical gap premise text hypothesis rarely knowledge gap focus fill knowledge gap science entailment task leverage external structure knowledge base kb science facts new architecture combine standard neural entailment model knowledge lookup module facilitate lookup propose fact level decomposition hypothesis verify result sub facts textual premise structure kb model nsnet learn aggregate predictions heterogeneous data format scitail dataset nsnet outperform simpler combination two predictions three base entailment model five
exponential growth information internet big challenge information retrieval systems towards generate relevant result novel approach require reformat expand user query generate satisfactory response increase recall precision query expansion qe technique broaden users query introduce additional tokens phrase base semantic similarity metrics tradeoff add computational complexity find semantically similar word possible increase noise information retrieval despite several research efforts topic qe yet explore enough work need similarity match composition query term objective retrieve small set appropriate responses qe scalable fast robust handle complex query good response time noise ceiling paper propose xu automate qe technique use high dimensional cluster word vectors datamuse api open source query engine find semantically similar word implement xu command line tool evaluate performances use datasets contain news article human generate qes evaluation result show xu better datamuse achieve eighty-eight accuracy reference human generate qe
challenge create dataset machine read comprehension mrc collect question require sophisticate understand language answer beyond use superficial cue work investigate make question easier across recent twelve mrc datasets three question style answer extraction description multiple choice propose employ simple heuristics split dataset easy hard subsets examine performance two baseline model subsets manually annotate question sample subset validity requisite reason skills investigate skills explain difference easy hard question study observe baseline performances hard subsets remarkably degrade compare entire datasets ii hard question require knowledge inference multiple sentence reason comparison easy question iii multiple choice question tend require broader range reason skills answer extraction description question result suggest one might overestimate recent advance mrc
last several years field computer assist language learn increasingly focus computer aid question generation however approach often provide test takers exhaustive amount question design specific test purpose work present personalize computer aid question generation generate multiple choice question various difficulty level type include vocabulary grammar read comprehension order improve weaknesses test takers select question depend estimate proficiency level unclear concepts behind incorrect responses result show students personalize automatic quiz generation correct mistake frequently ones computer aid question generation moreover students demonstrate progress pretest post test correctly answer difficult question finally investigate personalize strategy find student could make significant progress propose system offer vocabulary question level proficiency level grammar read comprehension question level lower proficiency level
propose kdsl new word sense disambiguation wsd framework utilize knowledge automatically generate sense label data supervise learn first wordnet automatically construct semantic knowledge base call disdict provide refine feature word highlight differences among word sense ie synsets second automatically generate new sense label data disdict unlabeled corpora third generate data together manually label data unlabeled data feed neural framework conduct supervise unsupervised learn jointly model semantic relations among synsets feature word contexts experimental result show kdsl outperform several representative state art methods various major benchmarks interestingly perform relatively well even manually label data unavailable thus provide potential solution similar task lack manual annotations
research read comprehension focus answer question base individual document even single paragraph introduce neural model integrate reason rely information spread within document across multiple document frame inference problem graph mention entities nod graph edge encode relations different mention eg within cross document co reference graph convolutional network gcns apply graph train perform multi step reason entity gcn method scalable compact achieve state art result multi document question answer dataset wikihop welbl et al two thousand and eighteen
dialog multiple valid next utterances point present end end neural methods dialog take account learn assumption time one correct next utterance work focus problem goal orient dialog set different paths reach goal propose new method use combination supervise learn reinforcement learn approach address issue also propose new effective testbed permute babi dialog task introduce multiple valid next utterances original babi dialog task allow evaluation goal orient dialog systems realistic set show significant drop performance exist end end neural methods eight hundred and fifteen per dialog accuracy original babi dialog task three hundred and three permute babi dialog task also show propose method improve performance achieve four hundred and seventy-three per dialog accuracy permute babi dialog task
neural encoder decoder model significant empirical success text generation remain several unaddressed problems style generation encoder decoder model largely uninterpretable b difficult control term phrase content work propose neural generation system use hide semi markov model hsmm decoder learn latent discrete templates jointly learn generate show model learn useful templates templates make generation interpretable controllable furthermore show approach scale real data set achieve strong performance near encoder decoder text generation model
paper introduce task automatically generate text describe differences two similar image collect new dataset crowd source difference descriptions pair image frame extract video surveillance footage annotators ask succinctly describe differences short paragraph result novel dataset provide opportunity explore model align language vision capture visual salience dataset may also useful benchmark coherent multi sentence generation perform firstpass visual analysis expose cluster differ pixels proxy object level differences propose model capture visual salience use latent variable align cluster differ pixels output sentence find single sentence generation well multi sentence generation propose model outperform model use attention alone
study consider task machine read scale mrs wherein give question system first perform information retrieval ir task find relevant passages knowledge source carry read comprehension rc task extract answer span passages previous mrs study ir component train without consider answer span struggle accurately find small number relevant passages large set passages paper propose simple effective approach incorporate ir rc task use supervise multi task learn order ir component train consider answer span experimental result standard benchmark answer squad question use full wikipedia knowledge source show model achieve state art performance moreover thoroughly evaluate individual contributions model components new japanese dataset squad result show significant improvements ir task provide new perspective ir rc effective teach part passage answer question rather give relevance score whole passage
grow interest language develop agents interact emergent communication settings earlier study focus agents symbol usage rather representation visual input paper consider referential game lazaridou et al two thousand and seventeen investigate representations agents develop evolve interaction find agents establish successful communication induce visual representations almost perfectly align surprisingly capture conceptual properties object depict input image conclude interest develop language like communication systems must pay attention visual semantics agents associate symbols use
describe first automatic approach merge coreference annotations obtain multiple annotators single gold standard merge subject certain linguistic hard constraints optimization criteria prefer solutions minimal divergence annotators representation involve equivalence relation large number elements use answer set program describe two representations problem four objective function suitable different datasets provide two structurally different real world benchmark datasets base metu sabanci turkish treebank report experience use gringo clasp wasp tool compute optimal adjudication result datasets
hate speech offensive language sexism racism type abusive behavior become common phenomenon many online social media platforms recent years diverse abusive behaviors manifest increase frequency level intensity due openness willingness popular media platforms twitter facebook host content sensitive controversial topics however platforms adequately address problem online abusive behavior responsiveness effective detection block inappropriate behavior remain limit present paper study complex problem follow holistic approach consider various aspects abusive behavior make approach tangible focus twitter data analyze user textual properties different angle abusive post behavior propose deep learn architecture utilize wide variety available metadata combine automatically extract hide pattern within text tweet detect multiple abusive behavioral norms highly inter relate apply unify architecture seamless transparent fashion detect different type abusive behavior hate speech sexism vs racism bully sarcasm etc without need tune model architecture task test propose approach multiple datasets address different multiple abusive behaviors twitter result demonstrate largely outperform state art methods twenty-one forty-five improvement auc depend dataset
present adaptive memory network amn process input question pair dynamically construct network architecture optimize lower inference time question answer qa task amn process input story extract entities store memory bank start single bank number input entities increase amn learn create new bank entropy single bank become high hence process input question pair result network represent hierarchical structure entities store different bank distance question relevance inference one bank use create tradeoff accuracy performance amn enable dynamic network allow input dependent network creation efficiency dynamic mini batch well novel bank controller allow learn discrete decision make high accuracy result demonstrate amn learn create variable depth network depend task complexity reduce inference time qa task
certain concepts word image intuitively similar others dog vs cat dog vs spoon though quantify similarity notoriously difficult indeed kind computation likely critical part learn category boundaries word within give language use set twenty-seven items eg dog highly common infants input use image word base algorithms independently compute similarity among find three key result first pairwise item similarities derive within image space word space correlate suggest preserve structure among extremely different representational format second closest neighbor item within space show significant overlap eg find egg neighbor apple third items overlap neighbor later learn infants toddlers conclude approach rely human rat similarity may nevertheless reflect stable within class structure across two space speculate invariance might aid lexical acquisition serve informative marker category boundaries
multi document summarization receive great deal attention past couple decades several approach propose many perform equally well become creasingly difficult choose one particular system another ensemble systems able leverage strengths individual systems build better robust summary despite attempt make direction paper describe category ensemble systems use consensus candidate systems build better meta summary highlight two major shortcomings systems inability take account relative performance individual systems overlook content candidate summaries favour sentence rank propose alternate method content base weight consensus summarization address concern use pseudo relevant summaries estimate performance individual candidate systems use information generate better aggregate rank experiment duc two thousand and three duc two thousand and four datasets show propose system outperform exist consensus base techniques large margin
collective entity disambiguation aim jointly resolve multiple mention link associate entities knowledge base previous work primarily base underlie assumption entities within document highly relate however extend mention entities actually connect reality rarely study therefore raise interest research question first time show semantic relationships mention entities fact less dense expect could attribute several reason noise data sparsity knowledge base incompleteness remedy introduce mintree new tree base objective entity disambiguation problem key intuition behind mintree concept coherence relaxation utilize weight minimum span tree measure coherence entities base new objective design novel entity disambiguation algorithms call pair link instead consider give mention pair link iteratively select pair highest confidence step decision make via extensive experiment show approach accurate also surprisingly faster many state art collective link algorithms
witness medieval literary texts preserve manuscript layer object almost exclusively copy copy result multiple hard distinguish linguistic strata author scripta interact scriptae various scribe context literary write language already dialectal hybrid moreover single linguistic phenomenon allow distinguish different scriptae combination multiple characteristics likely significant nine ones common approach search feature set previously select texts suppose representative give scripta induce circularity texts use select feature turn characterise belong linguistic area counter issue paper offer unsupervised corpus base approach cluster methods apply old french corpus identify main divisions group ultimately scriptometric profile build
paper present quantitative fine grain manual evaluation approach compare performance different machine translation mt systems build upon well establish multidimensional quality metrics mqm error taxonomy implement novel method assess whether differences performance mqm error type different mt systems statistically significant conduct case study english croatian language direction involve translate morphologically rich language compare three mt systems belong different paradigms pure phrase base factor phrase base neural first design mqm compliant error taxonomy tailor relevant linguistic phenomena slavic languages make annotation process feasible accurate errors mt output annotate two annotators follow taxonomy subsequently carry statistical analysis show best perform system neural reduce errors produce worst system pure phrase base half fifty-four moreover conduct additional analysis agreement errors distinguish short phrase level long distance sentence level errors discover phrase base mt approach limit use long distance agreement phenomena neural mt find especially effective
pervasive use distributional semantic model word embeddings variety research field due remarkable ability represent mean word practical application cognitive model however little know kind information encode text base word vectors lack understand particularly problematic word vectors regard model semantic representation abstract concepts paper attempt reveal internal information distributional word vectors analysis use binder et al two thousand and sixteen brain base vectors explicitly structure conceptual representations base neurobiologically motivate attribute analysis map text base vectors brain base vectors train prediction performance evaluate compare estimate original brain base vectors analysis demonstrate social cognitive information better encode text base word vectors emotional information result discuss term embody theories abstract concepts
machine translation mt process translate text write source language text target language article present english arabic statistical machine translation system first present general process set statistical machine translation system describe tool well different corpora use build mt system system evaluate term blue score two thousand, four hundred and fifty-one
paper present praaline open source software system manage annotate analyse visualise speech corpora researchers work speech corpora often face multiple tool format need work ever increase amount data collaborative way praaline integrate extend exist time prove tool speak corpora analysis praat sonic visualiser bridge r statistical package modular system facilitate automation reuse users expose integrate user friendly interface access multiple tool corpus metadata annotations may store database locally remotely users define metadata annotation structure users may run customisable cascade analysis step base plug ins script update database result corpus database may query produce aggregate data set praaline extensible use python c plug ins praat r script may execute corpus data series visualisations editors plug ins provide praaline free software release gpl license
paper present performance parallel text process map reduce cloud platform scientific paper turkish language process use zemberek nlp library experiment run hadoop cluster compare single machine performance
ontology learn old process automatically generate ontological knowledge base plain text document paper propose new ontology learn approach tool call dlol generate knowledge base description logic dl shoqd collection factual non negative sentence english provide extensive experimental result accuracy dlol give experimental comparisons three state art exist old tool namely text2onto fred lexo use standard old accuracy measure call lexical accuracy novel old accuracy measure call instance base inference model experimental result dlol turn twenty-one forty-six respectively better best three approach
first step many research project define rank short list candidates study modern rapidity scientific progress turn automate hypothesis generation hg systems aid process systems identify implicit overlook connections within large scientific corpus importance grow alongside pace science lack thorough validation without standard numerical evaluation method many validate general purpose hg systems rediscover handful historical find wish thorough may run laboratory experiment base automatic suggestions methods expensive time consume scale thus present numerical evaluation framework purpose validate hg systems leverage thousands validation hypotheses method evaluate hg system ability rank hypotheses plausibility process reminiscent human candidate selection hg systems produce rank criteria specifically produce topic model additionally present novel metrics quantify plausibility hypotheses give topic model system output finally demonstrate propose validation method align real world research goals deploy method within moliere recent topic drive hg system order automatically generate set candidate genes relate hiv associate neurodegenerative disease hand perform laboratory experiment base candidate set discover new connection hand dead box rna helicase three ddx3 reproducibility code validation data result find sybrandtcom two thousand and eighteen validation
readability assessment deal estimate level difficulty read textsmany readability test indicate execution efficiency apply specific texts measure read grade level science textbooks paper analyze content cover elementary school turkish textbooks employ distribute parallel process framework base popular mapreduce paradigm outline architecture distribute big data process system use hadoop full text readability analysis readability score textbooks system performance measurements also give paper
important challenge human like ai compositional semantics recent research attempt address use deep neural network learn vector space embeddings sentence serve input task present new dataset one task natural language inference nli solve use word level knowledge require compositionality find performance state art sentence embeddings infersent conneau et al two thousand and seventeen new dataset poor analyze decision rule learn infersent find consistent simple heuristics ecologically valid train dataset find augment train dataset improve test performance dataset without loss performance original train dataset highlight importance structure datasets better understand improve ai systems
unspoken social rule govern choose proper discussion topic change discussion topics guide conversational behaviors propose computational model conversation follow break rule participant agents respond accordingly additionally demonstrate application model experimental social tutor est first step toward social skills train tool generate human readable conversation conversational guideline point dialogue finally discuss design result pilot study evaluate est result show model capable produce conversations follow social norms
adverse drug reactions adrs one lead cause mortality health care current adr surveillance systems often associate substantial time lag events officially publish hand online social media twitter contain information adr events real time much official report current state art methods adr mention extraction use recurrent neural network rnn typically need large label corpora towards end propose semi supervise method base co train exploit large pool unlabeled tweet augment limit supervise train data result enhance performance experiment 01m tweet show propose approach outperform state art methods adr mention extraction task five term f1 score
adverse drug reactions adrs one lead cause mortality health care current adr surveillance systems often associate substantial time lag events officially publish hand online social media twitter contain information adr events real time much official report current state art adr mention extraction use recurrent neural network rnn typically need large label corpora towards end propose multi task learn base method utilize similar auxiliary task adverse drug event detection enhance performance main task ie adr extraction furthermore absence auxiliary task dataset propose novel joint multi task learn method automatically generate weak supervision dataset auxiliary task large pool unlabeled tweet available experiment 048m tweet show propose approach outperform state art methods adr mention extraction task seventy-two term f1 score
compute systems become increasingly advance users increasingly engage technology security never greater concern malware detection static analysis method analyze potentially malicious file prominent approach approach however quickly fall short malicious program become advance adopt capabilities obfuscate binaries execute malicious function make static analysis extremely difficult newer variants approach assess paper novel dynamic malware analysis method may generalize better static analysis newer variants inspire recent successes natural language process nlp widely use document classification techniques assess detect malware analysis system call contain useful information operation program request program make kernel feature consider extract system call trace benign malicious program task classify trace treat binary document classification task system call trace system call trace process remove parameters leave system call function name feature group various n grams weight term frequency inverse document frequency paper show linear support vector machine svm optimize stochastic gradient descent traditional coordinate descent wolfe dual form svm effective approach achieve highest ninety-six accuracy ninety-five recall score additional contributions include identification significant system call sequence could avenues research
visual question answer vqa model struggle count object natural image far identify fundamental problem due soft attention model circumvent problem propose neural network component allow robust count object proposals experiment toy task show effectiveness component obtain state art accuracy number category vqa v2 dataset without negatively affect categories even outperform ensemble model single model difficult balance pair metric component give substantial improvement count strong baseline sixty-six
give target name product aspect entity identify aspect word opinion word give corpus fine grain task target base sentiment analysis tsa task challenge especially label data want perform give domain address propose general two stage approach stage one extract group target relate word call word give target relatively easy apply exist semantics base learn technique stage two separate aspect opinion word group word challenge often enough word level aspect opinion label work formulate problem pu learn set incorporate idea lifelong learn solve experimental result show effectiveness approach
work exploit translation data source semantically relevant learn signal model word representation particular exploit equivalence translation form distribute context jointly learn embed align deep generative model embedalign model embed word complete observe context learn marginalisation latent lexical alignments besides embed word posterior probability densities rather point estimate allow us compare word context use measure overlap distributions eg kl divergence investigate model performance range lexical semantics task achieve competitive result several standard benchmarks include natural language inference paraphrase text similarity
automatic phylogenetic inference play increasingly important role computational historical linguistics pertinent work currently base expert cognate judgments limit scope approach small number well study language families use machine learn techniques compile data suitable phylogenetic inference asjp database collection almost seven thousand phonetically transcribe word list forty concepts cover two third extant world wide linguistic diversity first estimate pointwise mutual information score sound class use weight sequence alignment general purpose optimization compute dissimilarity matrix asjp word list matrix suitable distance base phylogenetic inference second apply cognate cluster asjp data use supervise train svm classifier expert cognacy judgments third define two type binary character base automatically infer cognate class sound class occurrences several test report demonstrate suitability character character base phylogenetic inference
abundance digitise texts available sanskrit however word segmentation task texts challenge due issue sandhi sandhi word sentence often fuse together form single chunk text word delimiter vanish sound word boundaries undergo transformations also reflect write text propose approach use deep sequence sequence seq2seq model take sandhied string input predict unsandhied string state art model linguistically involve external dependencies lexical morphological analysis input model train overnight use production spite knowledge lean approach system preform better current state art gain percentage increase one thousand, six hundred and seventy-nine current state art
word language randomly replace time new ones long know word correspond items mean less frequently replace others usually rate replacement give item directly observable infer estimate stability contrary observable idea go back long way lexicostatistical literature nevertheless nothing ensure give correct answer family romance languages allow direct test estimate stabilities replacement rat since proto language latin know replacement rat explicitly compute output test threefoldfirst prove standard approach try infer replacement rat trough estimate stabilities sound second able rewrite fundamental formula glottochronology non universal replacement rate rate depend item third give indisputable evidence stability rank far different families languages last result also support comparison malagasy family dialects side result also provide evidence vulgar latin late classical latin root modern romance languages
distribute word representations word vectors recently apply many task natural language process lead state art performance key ingredient successful application representations train large corpora use pre train model downstream task paper describe train high quality word representations one hundred and fifty-seven languages use two source data train model free online encyclopedia wikipedia data common crawl project also introduce three new word analogy datasets evaluate word vectors french hindi polish finally evaluate pre train word vectors ten languages evaluation datasets exist show strong performance compare previous model
compute universal distribute representations sentence fundamental task natural language process propose method learn representations encode suffix word sequence sentence train stanford natural language inference snli dataset demonstrate effectiveness approach evaluate senteval benchmark improve exist approach several transfer task
many neural model new feature polynomial function exist ones use augment representations use natural language inference task example investigate use scale polynomials degree two match feature find scale degree two feature highest impact performance reduce classification error five best model
identify relationship two article eg whether two article publish different source describe break news critical many document understand task exist approach model match sentence pair perform well match longer document embody complex interactions enclose entities sentence model article pair propose concept interaction graph represent article graph concepts match pair article compare sentence enclose concept vertex series encode techniques aggregate match signal graph convolutional network facilitate evaluation long article match create two datasets consist 30k pair break news article cover diverse topics open domain extensive evaluations propose methods two datasets demonstrate significant improvements wide range state art methods natural language match
background study examine sentiment social media vary depend time location appear produce inconsistent result make hard design systems use sentiment detect localize events public health applications objective aim study measure common time location confounders explain variation sentiment twitter methods use dataset one thousand, six hundred and fifty-four million english language tweet one hundred cities post july thirteen november thirty two thousand and seventeen estimate positive negative sentiment cities use dictionary base sentiment analysis construct model explain differences sentiment use time day day week weather city interaction type conversations broadcast factor find factor independently associate sentiment result full multivariable model positive pearson r test data two hundred and thirty-six ninety-five ci two hundred and thirty-one two hundred and forty-one negative pearson r test data three hundred and six ninety-five ci three hundred and one three hundred and ten sentiment city time day explain variance weather day week model account confounders produce different distribution rank important events compare model account confounders conclusions public health applications aim detect localize events aggregate sentiment across populations twitter users worthwhile account baseline differences look unexpected change
recent interest apply cognitively empirically motivate bound recursion depth limit search space grammar induction model ponvert et al two thousand and eleven noji johnson two thousand and sixteen shain et al two thousand and sixteen work extend depth bound approach probabilistic context free grammar induction db pcfg smaller parameter space hierarchical sequence model therefore fully exploit space reductions depth bound result model grammar acquisition transcribe child direct speech newswire text exceed competitive model evaluate parse accuracy moreover gram mar acquire model demonstrate consistent use category label something demonstrate acquisition model
deep generative model enjoy success model continuous data however remain challenge capture representations discrete structure formal grammars semantics eg computer program molecular structure generate syntactically semantically correct data still remain largely open problem inspire theory compiler syntax semantics check do via syntax direct translation sdt propose novel syntax direct variational autoencoder sd vae introduce stochastic lazy attribute approach convert offline sdt check fly generate guidance constrain decoder compare state art methods approach enforce constraints output space output syntactically valid also semantically reasonable evaluate propose model applications program language molecules include reconstruction program molecule optimization result demonstrate effectiveness incorporate syntactic semantic constraints discrete generative model significantly better current state art approach
semantic composition function play pivotal role neural representation learn text sequence spite success exist model suffer underfitting problem use share compositional function position sequence thereby lack expressive power due incapacity capture richness compositionality besides composition function different task independent learn scratch paper propose new share scheme composition function across multiple task specifically use share meta network capture meta knowledge semantic composition generate parameters task specific semantic composition model conduct extensive experiment two type task text classification sequence tag demonstrate benefit approach besides show share meta knowledge learn propose model regard shelf knowledge easily transfer new task
present new data semantic parse methods problem map english sentence bash command nl2bash long term goal enable user perform operations file manipulation search application specific script simply state goals english take first step domain provide new dataset challenge commonly use bash command expert write english descriptions along baseline methods establish performance level task
task answer natural language question rdf data receive wide interest recent years particular context series qald benchmarks task consist map natural language question executable form eg sparql answer give kb extract far systems propose monolingual ii rely set hard cod rule interpret question map sparql query present first multilingual qald pipeline induce model train data map natural language question logical form probabilistic inference particular approach learn map universal syntactic dependency representations language independent logical form base dudes dependency base underspecified discourse representation structure map sparql query deterministic second step model build factor graph rely feature extract dependency graph correspond semantic representations rely approximate inference techniques markov chain monte carlo methods particular well sample rank update parameters use rank objective focus lie develop methods overcome lexical gap present novel combination machine translation word embed approach purpose proof concept approach evaluate approach qald six datasets english german spanish
text summarization interest area researchers develop new techniques provide human like summaries vast amount information summarization techniques tend focus provide accurate representation content often tone content ignore tone content set baseline reader perceive content able generate summary tone appropriate reader important work implement maximal marginal relevance mmr base multi document text summarization propose naive model change tone summarization set bias specific set word restrict word summarization output bias towards specify set word produce summary whose tone tone specify word
one thousand, eight hundred and thirty-seven return england aboard textithms beagle one thousand, eight hundred and sixty publication textitthe origin species charles darwin keep detail note book read want read note manuscripts provide information decades individual scientific practice previously train topic model full texts read apply information theoretic measure detect change read pattern coincide boundaries three major intellectual project period one thousand, eight hundred and thirty-seven one thousand, eight hundred and sixty new work apply read model five additional document four darwin first edition textitthe origin species two private essay state intermediate form theory one thousand, eight hundred and forty-two one thousand, eight hundred and forty-four third essay dispute date alfred russel wallace essay darwin receive one thousand, eight hundred and fifty-eight address three historical inquiries previously treat qualitatively one mythology darwin delay despite complete extensive draft one thousand, eight hundred and forty-four darwin wait one thousand, eight hundred and fifty-nine publish textitthe origin species due external pressure two relationship darwin wallace contemporaneous theories especially light joint presentation three date outline draft rediscover one thousand, nine hundred and seventy-five postulate first one thousand, eight hundred and thirty-nine draft precede sketch one thousand, eight hundred and forty-two interstitial draft one thousand, eight hundred and forty-two one thousand, eight hundred and forty-four essay
flood information produce daily basis global internet usage arise line interactive communications among users situation contribute significantly quality human life unfortunately involve enormous dangers since line texts high toxicity personal attack line harassment bully behaviors trigger industrial research community last years several try identify efficient model line toxic comment prediction however step still infancy new approach frameworks require parallel data explosion appear constantly make construction new machine learn computational tool manage information imperative need thankfully advance hardware cloud compute big data management allow development deep learn approach appear promise performance far text classification particular use convolutional neural network cnn recently propose approach text analytics modern manner emphasize structure word document work employ approach discover toxic comment large pool document provide current kaggle competition regard wikipedia talk page edit justify decision choose compare cnns traditional bag word approach text analysis combine selection algorithms prove effective text classification report result provide enough evidence cnn enhance toxic comment classification reinforce research interest towards direction
publications life sciences characterize large technical vocabulary many lexical semantic variations express concept towards address problem relevance biomedical literature search introduce deep learn model relevance document text keyword style query limit relatively small amount train data model use pre train word embeddings model first compute variable length delta matrix query document represent difference two texts pass deep convolution stage follow deep fee forward network compute relevance score result fast model suitable use online search engine model robust outperform comparable state art deep learn approach
describe experience implement news content organization system tencent discover events vast stream break news evolve news story structure online fashion real world system distinct requirements contrast previous study topic detection track tdt event timeline graph generation one need accurately quickly extract distinguishable events massive stream long text document cover diverse topics contain highly redundant information two must develop structure event stories online manner without repeatedly restructure previously form stories order guarantee consistent user view experience solve challenge propose story forest set online scheme automatically cluster stream document events connect relate events grow tree tell evolve stories conduct extensive evaluation base sixty gb real world chinese news data although ideas language dependent easily extend languages detail pilot user experience study result demonstrate superior capability story forest accurately identify events organize news text logical structure appeal human readers compare multiple exist algorithm frameworks
product recommendation systems important major movie studios movie greenlight process part machine learn personalization pipelines collaborative filter cf model prove effective power recommender systems online stream service explicit customer feedback data cf model perform well scenarios feedback data available cold start situations like new product launch situations markedly different customer tiers eg high frequency customers vs casual customers generative natural language model create useful theme base representations underlie corpus document use represent new product descriptions like new movie plot combine cf show increase performance cold start situations outside case though explicit customer feedback available recommender engines must rely binary purchase data materially degrade performance fortunately purchase data combine product descriptions generate meaningful representations products customer trajectories convenient product space proximity represent similarity learn measure distance point space accomplish deep neural network train customer histories dense vectorizations product descriptions develop system base collaborative deep metric learn cml predict purchase probabilities new theatrical release train evaluate model use large dataset customer histories test model set movies release outside train window initial experiment show gain relative model train collaborative preferences
verbs play important role understand natural language text paper study problem abstract subject object arguments verb set noun concepts know argument concepts set concepts whose size parameterized represent fine grain semantics verb example object enjoy abstract time hobby event etc present novel framework automatically infer human readable machine computable action concepts high accuracy
thank development semantic web lot new structure data become available web form knowledge base kbs make valuable data accessible usable end users one main goals question answer qa kbs current qa systems query one kb one language namely english exist approach design easily adaptable new kbs languages first introduce new approach translate natural language question sparql query able query several kbs simultaneously different languages easily port kbs languages evaluation impact approach prove use five different well know large kbs wikidata dbpedia musicbrainz dblp freebase well five different languages namely english german french italian spanish second show integrate approach make easily accessible research community end users summarize provide conceptional solution multilingual kb agnostic question answer semantic web provide first approximation validate concept
recent approach use sequence sequence model paraphrase generation exist sequence sequence model tend memorize word pattern train dataset instead learn mean word therefore generate sentence often grammatically correct semantically improper work introduce novel model base encoder decoder framework call word embed attention network wean propose model generate word query distribute word representations ie neural word embeddings hop capture mean accord word follow previous work evaluate model two paraphrase orient task namely text simplification short text abstractive summarization experimental result show model outperform sequence sequence baseline bleu score sixty-three fifty-five two english text simplification datasets rouge two f1 score fifty-seven chinese summarization dataset moreover model achieve state art performances three benchmark datasets
goal formalization propose paper bring together near possible theoretic linguistic problem synonym conception computer linguistic methods base generally empirical intuitive unjustified factor use word vector representation propose geometric approach mathematical model synonym set synset word embed base neural network skip gram cbow develop realize word2vec program mikolov standard cosine similarity use distance word vectors several geometric characteristics synset word introduce interior synset synset word rank centrality notions intend select significant synset word ie word sense nearest sense synset experiment propose notions base rusvectores resources represent brief description work view slide https googl k82fei
language describe network interact object different qualitative properties complexity network include semantic syntactic phonological level find provide new picture language complexity evolution general approach consider language information theory perspective incorporate speaker hearer noisy channel later often encode matrix connect signal use communication mean find real world study language evolution deal way another theoretical contraption explore outcome diverse form selection communication matrix somewhat optimize communication framework naturally introduce network mediate communicate agents systematic analysis underlie landscape possible language graph develop present detail analysis network properties generic model communication code reveal rather complex heterogeneous morphospace language network additionally use curated data english word locate evaluate real languages within language morphospace find indicate surprisingly simple structure human language unless particles introduce vocabulary ability name concept result refine first time complement empirical data last theoretical tradition around framework emphleast effort language
recent research provide evidence effective communication collaborative software development significant impact software development lifecycle although relate qualitative quantitative study point textual characteristics well form message underlie semantics intertwine linguistic structure still remain largely misinterpret ignore especially regard quality code review importance thorough feedback explicit rationale often mention rarely link relate linguistic feature first step towards address shortcoming propose ground study theories linguistics particularly focus linguistic structure coherent speech explain exploit practice reflect relate approach examine preliminary study four open source project possible link exist find directions suggest detect textual feature useful code review
large scale datasets natural language inference create present crowd workers sentence premise ask generate three new sentence hypotheses entail contradict logically neutral respect show significant portion data protocol leave clue make possible identify label look hypothesis without observe premise specifically show simple text categorization model correctly classify hypothesis alone sixty-seven snli bowman et al two thousand and fifteen fifty-three multinli williams et al two thousand and seventeen analysis reveal specific linguistic phenomena negation vagueness highly correlate certain inference class find suggest success natural language inference model date overestimate task remain hard open problem
conventional supervise train model train fit train examples however monolithic model may always best strategy examples could vary widely work explore different learn protocol treat example unique pseudo task reduce original learn problem shoot meta learn scenario help domain dependent relevance function evaluate wikisql dataset approach lead faster convergence achieve eleven fifty-four absolute accuracy gain non meta learn counterparts
extract action sequence natural language texts challenge require commonsense inferences base world knowledge although work extract action script instructions navigation action etc require either set candidate action provide advance action descriptions restrict specific form eg description templates paper aim extract action sequence texts free natural language ie without restrict templates provide candidate set action unknown propose extract action sequence texts base deep reinforcement learn framework specifically view select eliminate word texts action texts associate action state build q network learn policy extract action extract plan label texts demonstrate effectiveness approach several datasets comparison state art approach include online experiment interact humans
learn distribute sentence representations remain interest problem field natural language process nlp want learn model approximate conditional latent space representations logical antecedent give statement paper propose approach generate sentence condition input sentence logical inference label model different possibilities output sentence distribution latent representation train use adversarial objective evaluate model use two state art model recognize textual entailment rte task measure bleu score actual sentence probe diversity sentence produce model experiment result show give framework clear ways improve quality diversity generate sentence
clinical note often describe important aspects patient physiology therefore critical medical research however note typically inaccessible researchers without prior removal sensitive protect health information phi natural language process nlp task refer deidentification tool automatically de identify clinical note need difficult create without access note contain phi work present first step toward create large synthetically identify corpus clinical note correspond phi annotations order facilitate development de identification tool one tool evaluate corpus order understand advantage shortcomings approach
neural network base methods represent state art question generation text exist work focus generate question text without concern answer generation moreover analysis show handle rare word generate appropriate question give candidate answer still challenge face exist approach present novel two stage process generate question answer pair text first stage present alternatives encode span pivotal answer sentence use pointer network second stage employ sequence sequence model question generation enhance rich linguistic feature finally global attention answer encode use generate question relevant answer motivate linguistically analyze role component framework consider compositions analysis support extensive experimental evaluations use standard evaluation metrics well human evaluations experimental result validate significant improvement quality question generate framework state art technique present represent another step towards automate read comprehension assessment also present live system footnotedemo system available urlhttps wwwcseiitbacin vishwajeet autoqghtml demonstrate effectiveness approach
neural language model lms typically train use lexical feature surface form word paper argue deprive lm crucial syntactic signal detect high confidence use exist parsers present simple highly effective approach train neural lms use lexical syntactic information novel approach apply lms unparsed text use sequential monte carlo sample experiment range corpora corpus size show approach consistently outperform standard lexical lms character level language model hand word level model model par standard language model result indicate potential expand lms beyond lexical surface feature higher level nlp feature character level model
linkage imagenet wordnet synsets wikidata items leverage deep learn algorithm access rich multilingual knowledge graph describe go efforts link two resources issue face match wikidata wordnet knowledge graph show example linkage use deep learn set real time image classification label non english language discuss opportunities lie ahead
although emerge trend towards generate embeddings primarily unstructured data recently structure data systematic suite measure quality embeddings propose yet deficiency sense respect embeddings generate structure data concrete evaluation metrics measure quality encode structure well semantic pattern embed space paper introduce framework contain three distinct task concern individual aspects ontological concepts categorization aspect ii hierarchical aspect iii relational aspect scope task number intrinsic metrics propose evaluate quality embeddings furthermore wrt framework multiple experimental study run compare quality available embed model employ framework future research reduce misjudgment provide greater insight quality comparisons embeddings ontological concepts position sample data code https githubcom alshargi concept2vec gnu general public license v30
study current best model kdg question answer tabular data evaluate wikitablequestions dataset previous ablation study perform model attribute model performance certain aspects architecture paper find model performance also crucially depend certain prune data use train model disable prune step drop accuracy model four hundred and thirty-three three hundred and sixty-three large impact performance kdg model suggest prune may useful pre process step train semantic parsers well
paper present improve feedforward sequential memory network fsmn architecture namely deep fsmn dfsmn introduce skip connections memory block adjacent layer skip connections enable information flow across different layer thus alleviate gradient vanish problem build deep structure result dfsmn significantly benefit skip connections deep structure compare performance dfsmn blstm without lower frame rate lfr several large speech recognition task include english mandarin experimental result show dfsmn consistently outperform blstm dramatic gain especially train lfr use cd phone model units two thousand hours fisher fsh task propose dfsmn achieve word error rate ninety-four purely use cross entropy criterion decode three gram language model achieve fifteen absolute improvement compare blstm twenty thousand hours mandarin recognition task lfr train dfsmn achieve twenty relative improvement compare lfr train blstm moreover easily design lookahead filter order memory block dfsmn control latency real time applications
paper report modern approach information extraction ie two main sub task name entity recognition ner relation extraction basic concepts recent approach area review mainly include machine learn ml base approach recent trend deep learn dl base methods
introduce mesys mean base approach solve english math word problems mwps via understand reason paper first analyze text transform body question part correspond logic form perform inference associate context quantity represent propose role tag eg nsubj verb etc provide flexibility annotate extract math quantity associate context information ie physical mean quantity statistical model propose select operator operands noisy dataset design assess solver solve mwps mainly via understand mechanical pattern match experimental result show approach outperform exist systems benchmark datasets noisy dataset demonstrate propose approach understand mean quantity text
extract information handwritten document text transcription name entity recognition usually face separate subsequent task disadvantage errors first module affect heavily performance second module work propose task jointly use single neural network common architecture use plain text recognition experimentally work test collection historical marriage record result experiment present show effect performance different configurations different ways encode information transfer learn process text line multi line region level result comparable state art report icdar two thousand and seventeen information extraction competition even though propose technique use dictionaries language model post process
adequately model mathematical arguments analyst must able represent mathematical object discussion relationships well inferences draw object relationships discourse unfold introduce framework properties use analyse mathematical dialogues expository texts framework recover salient elements discourse within sentence level well way mathematical content connect form larger argumentative structure show framework might use support computational reason argue provide natural way examine process prove theorems lamport structure proof
infer miss link knowledge graph kg attract lot attention research community paper tackle practical query answer task involve predict relation give entity pair frame prediction problem inference problem probabilistic graphical model aim resolve variational inference perspective order model relation query entity pair assume exist underlie latent variable paths connect two nod kg carry equivalent semantics relations however due intractability connections large kgs propose use variation inference maximize evidence lower bind specifically framework textscdiva compose three modules ie posterior approximator prior path finder likelihood path reasoner use variational inference able incorporate closely unify architecture jointly optimize perform kg reason active interactions among sub modules textscdiva better handle noise cop complex reason scenarios order evaluate method conduct experiment link prediction task multiple datasets achieve state art performances datasets
paper describe methodology follow build neural machine translation system biomedical domain english catalan language pair task consider low resourced task point view domain language pair face task paper report experiment cascade pivot strategy spanish neural machine translation use english spanish scielo spanish catalan el peri odico database test final performance system create new test data set english catalan biomedical domain freely available request
knowledge graph embed methods aim represent entities relations knowledge base point vectors continuous vector space several approach use embeddings show promise result task link prediction entity recommendation question answer triplet classification however methods compute low dimensional embeddings large knowledge base without need state art computational resources paper propose kg2vec simple fast approach knowledge graph embed base skip gram model instead use predefined score function learn rely long short term memories show embeddings achieve result comparable scalable approach knowledge graph completion well new metric yet kg2vec embed large graph lesser time process two hundred and fifty million triple less seven hours common hardware
consider problem zero shoot recognition learn visual classifier category zero train examples use word embed category relationship categories visual data provide key deal unfamiliar novel category transfer knowledge obtain familiar class describe unfamiliar class paper build upon recently introduce graph convolutional network gcn propose approach use semantic embeddings categorical relationships predict classifiers give learn knowledge graph kg approach take input semantic embeddings node represent visual category series graph convolutions predict visual classifier category train visual classifiers categories give learn gcn parameters test time filter use predict visual classifiers unseen categories show approach robust noise kg importantly approach provide significant improvement performance compare current state art result two three metrics whop twenty
conversational agents become ubiquitous range goal orient systems help reservations chit chat model find modern virtual assistants survey paper explore fascinate field look pioneer work define field gradually move current state art model look statistical neural generative adversarial network base reinforcement learn base approach evolve along way discuss various challenge field face lack context utterances good quantitative metric compare model lack trust agents consistent persona etc structure paper way answer pertinent question discuss compete approach solve
word sense induction wsi ability automatically induce word sense corpora wsi task first propose overcome limitations manually annotate corpus require word sense disambiguation systems even though several work propose induce word sense exist systems still limit sense make use structure domain specific knowledge source paper devise method leverage recent find word embeddings research generate context embeddings embeddings contain information semantical context word order induce sense model set ambiguous word complex network generate network two instance nod connect respective context embeddings similar upon use well establish community detection methods cluster obtain context embeddings find propose method yield excellent performance wsi task method outperform compete algorithms baselines completely unsupervised manner without need additional structure knowledge source
propose unsupervised keyphrase extraction model encode topical information within multipartite graph structure model represent keyphrase candidates topics single graph exploit mutually reinforce relationship improve candidate rank introduce novel mechanism incorporate keyphrase selection preferences model experiment conduct three widely use datasets show significant improvements state art graph base model
effectively develop speech technology languages transcribe data available many exist approach use annotate resources yet make sense leverage information large annotate corpora languages example form multilingual bottleneck feature bnfs obtain supervise speech recognition system work evaluate benefit bnfs subword model feature extraction six unseen languages word discrimination task first establish strong unsupervised baseline combine two exist methods vocal tract length normalisation vtln correspondence autoencoder cae show bnfs train single language already beat baseline include ten languages result additional improvements match add data single language finally show cae improve bnfs high quality word pair available
many vision language task require commonsense reason beyond data drive image natural language process adopt visual question answer vqa example task system expect answer question natural language image current state art systems attempt solve task use deep neural architectures achieve promise performance however result systems generally opaque struggle understand question extra knowledge require paper present explicit reason layer top set penultimate neural network base systems reason layer enable reason answer question additional knowledge require time provide interpretable interface end users specifically reason layer adopt probabilistic soft logic psl base engine reason basket input visual relations semantic parse question background ontological knowledge word2vec conceptnet experimental analysis answer key evidential predicate generate vqa dataset validate approach
present new approach evaluate computational model task text understand mean context error detection novel design automate modification process exist large scale data source adopt vast number text understand task data thereby alter semantic level allow model test challenge set modify text passages require comprise broader narrative discourse newly introduce task target actual real world problems transcription translation systems insert authentic context errors automate modification process apply two thousand and sixteen tedtalk corpus entirely automate process allow adoption complete datasets low cost facilitate supervise learn procedures deeper network train test evaluate quality modification algorithm language model supervise binary classification model train test alter dataset human baseline evaluation examine compare result human performance outcome evaluation task indicate difficulty detect semantic errors machine learn algorithms humans show errors identify limit single sentence
keyphrase efficient representation main idea document background knowledge provide valuable information document rarely incorporate keyphrase extraction methods paper propose wikirank unsupervised method keyphrase extraction base background knowledge wikipedia firstly construct semantic graph document transform keyphrase extraction problem optimization problem graph finally get optimal keyphrase set output method obtain improvements state art model two f1 score
paper study problem parse structure knowledge graph textual descriptions particular consider scene graph representation consider object together attribute relations representation prove useful across variety vision language applications begin introduce alternative equivalent edge centric view scene graph connect dependency parse together careful redesign label action space combine two stage pipeline use prior work generic dependency parse follow simple post process one enable end end train scene graph generate learn neural dependency parser achieve f score similarity four thousand, nine hundred and sixty-seven grind truth graph evaluation set surpass best previous approach five demonstrate effectiveness learn parser image retrieval applications
domestic violence silent crisis develop underdevelop countries though develop countries also remain drown curse develop countries victims easily report ask help contrary develop underdevelop countries victims hardly report crimes notice authority become late save support victim kind problems identify begin event proper action take help victim also reduce domestic violence crimes paper propose smart system extract victim situation provide help accord among develop underdevelop countries bangladesh choose though rate report domestic violence low extreme report collect authorities high case study collect different ngo relate domestic violence study apply extract possible condition victims
english language evolve dramatically throughout lifespan extent modern speaker old english would incomprehensible without translation one concrete indicator process movement irregular regular ed form past tense verbs study quantify extent verb regularization use two vastly disparate datasets one six years publish book scan google two thousand and three two thousand and eight two decade social media message post twitter two thousand and eight two thousand and seventeen find extent verb regularization greater twitter take whole english fiction book regularization also greater tweet geotagged unite state relative american english book opposite true tweet geotagged unite kingdom relative british english book also find interest regional variations regularization across counties unite state however differences population account identify strong correlations socio demographic variables education income
web semantic extension ie link open data contain open global scale knowledge make available potentially intelligent machine want benefit nevertheless link open data lack ontological distinctions sparse axiomatisation example distinctions whether entity inherently class individual whether physical object hardly express data although largely study formalise foundational ontologies eg dolce sumo distinctions belong common sense relevant many artificial intelligence task natural language understand scene recognition like gap foundational ontologies often formalise inspire pre exist philosophical theories develop top approach link open data mostly derive exist databases crowd base effort eg dbpedia wikidata investigate whether machine learn foundational distinctions link open data entities match common sense want answer question dbpedia entity dog refer class instance report set experiment base machine learn crowdsourcing show promise result
introduce novel framework image caption produce natural language explicitly ground entities object detectors find image approach reconcile classical slot fill approach generally better ground image modern neural caption approach generally natural sound accurate approach first generate sentence template slot locations explicitly tie specific image regions slot fill visual concepts identify regions object detectors entire architecture sentence template generation slot fill object detectors end end differentiable verify effectiveness propose model different image caption task standard image caption novel object caption model reach state art coco flickr30k datasets also demonstrate model unique advantage train test distributions scene compositions hence language priors associate caption different code make available https githubcom jiasenlu neuralbabytalk
present train framework neural abstractive summarization base actor critic approach reinforcement learn traditional neural network base methods objective maximize likelihood predict summaries assessment constraints consider may generate low quality summaries even incorrect sentence alleviate problem employ actor critic framework enhance train procedure actor employ typical attention base sequence sequence seq2seq framework policy network summary generation critic combine maximum likelihood estimator well design global summary quality estimator neural network base binary classifier aim make generate summaries indistinguishable human write ones policy gradient method use conduct parameter learn alternate train strategy propose conduct joint train actor critic model extensive experiment benchmark datasets different languages show framework achieve improvements state art methods
human conversation complex mechanism subtle nuances hence ambitious goal develop artificial intelligence agents participate fluently conversation still far achieve goal recent progress visual question answer image caption visual question generation show dialog systems may realizable distant future end novel dataset introduce recently encourage result demonstrate particularly question answer paper demonstrate simple symmetric discriminative baseline apply predict answer well predict question show method perform par state art even memory net base methods addition first time visual dialog dataset assess performance system ask question demonstrate visual dialog generate discriminative question generation question answer
extract accurate attribute qualities product title vital component deliver ecommerce customers reward online shop experience via enrich faceted search demonstrate potential deep recurrent network domain primarily model bidirectional lstms bidirectional lstm crf without attention mechanism improve overall f1 score compare previous benchmarks et al least three hundred and ninety-one showcasing overall precision nine thousand, seven hundred and ninety-four recall nine thousand, four hundred and twelve f1 score nine thousand, five hundred and ninety-nine make us achieve significant coverage important facets attribute products show efficacy deep recurrent model previous machine learn benchmarks also greatly enhance overall customer experience shop online
intelligent personal assistant systems either text base voice base conversational interfaces become increasingly popular around world retrieval base conversation model advantage return fluent informative responses exist study area open domain chit chat conversations task transaction orient conversations research need information seek conversations also lack model external knowledge beyond dialog utterances among current conversational model paper propose learn framework top deep neural match network leverage external knowledge response rank information seek conversation systems incorporate external knowledge deep neural model pseudo relevance feedback qa correspondence knowledge distillation extensive experiment three information seek conversation data set include open benchmarks commercial data show methods outperform various baseline methods include several deep text match model state art method response selection multi turn conversations also perform analysis different response type model variations rank examples model research find provide new insights utilize external knowledge deep neural model response selection implications design next generation information seek conversation systems
describe effort annotate corpus natural language instructions consist six hundred and twenty-two wet lab protocols facilitate automatic semi automatic conversion protocols machine readable format benefit biological research experimental result demonstrate utility corpus develop machine learn approach shallow semantic parse instructional texts make annotate wet lab protocol corpus available research community
paper focus detection task information extraction positive instance sparsely distribute model usually evaluate use f measure positive class characteristics often result deficient performance neural network base detection model paper propose adaptive scale algorithm handle positive sparsity problem directly optimize f measure via dynamic cost sensitive learn end borrow idea marginal utility economics propose theoretical framework instance importance measure without introduce additional hyper parameters experiment show algorithm lead effective stable train neural network base detection model
study explore application word2vec doc2vec sentiment analysis clinical discharge summaries apply unsupervised learn since data set sentiment annotations note unsupervised learn realistic scenario supervise learn require access train set sentiment annotate data aim detect exist underlie bias towards certain disease use sentiwordnet establish gold sentiment standard data set evaluate performance word2vec doc2vec methods show word2vec doc2vec methods complement result sentiment analysis data set
two thousand and fifteen unicode consortium introduce five skin tone emoji use combination emoji represent human figure body part study use skin tone emoji analyze geographically large sample data twitter show value skin tone emoji country correspond approximately skin tone resident populations negative correlation exist tweet sentiment darker skin tone global level era large scale migrations continue sensitivity question skin color race understand new language elements skin tone emoji use help frame understand people represent others term salient personal appearance attribute
build intelligent agents communicate learn humans natural language great value supervise language learn limit ability capture mainly statistics train data hardly adaptive new scenarios flexible acquire new knowledge without inefficient retrain catastrophic forget highlight perspective conversational interaction serve natural interface language learn novel knowledge acquisition propose joint imitation reinforcement approach ground language learn interactive conversational game agent train approach able actively acquire information ask question novel object use learn knowledge subsequent conversations one shoot fashion result compare methods verify effectiveness propose approach
generate image natural language one primary applications recent conditional generative model besides test ability model conditional highly dimensional distributions text image synthesis many excite practical applications photo edit computer aid content creation recent progress make use generative adversarial network gans material start gentle introduction topics discuss existent state art model moreover propose wasserstein gin cls new model conditional image generation base wasserstein distance offer guarantee stability show novel loss function wasserstein gin cls use conditional progressive grow gin combination propose loss model boost seven hundred and seven best inception score caltech bird dataset model use sentence level visual semantics model perform better conditional wasserstein progressive grow gin recently propose attngan use word level visual semantics well
chinese pinyin input methods important chinese language process actually users may make typos inevitably input pinyin moreover pinyin typo correction become increasingly important task popularity smartphones mobile internet exploit knowledge users type behaviors support typo correction acronym pinyin remain challenge problem tackle challenge propose knptc novel approach base neural machine translation nmt contrast previous work knptc able integrate explicit knowledge nmt pinyin typo correction able learn correct variety typos without guidance manually select constraints languagespecific feature approach first obtain transition probabilities adjacent letter base large scale real life datasets construct grind truth alignments train sentence pair utilize probabilities furthermore alignments integrate nmt capture sensible pinyin typo correction pattern knptc apply correct typos real life datasets achieve three thousand, two hundred and seventy-seven increment average accuracy rate typo correction compare state art system
neural network equip self attention parallelizable computation light weight structure ability capture long range local dependencies expressive power performance boost use vector measure pairwise dependency require expand alignment matrix tensor result memory computation bottleneck paper propose novel attention mechanism call multi mask tensorized self attention mtsa fast memory efficient cnn significantly outperform previous cnn rnn attention base model mtsa one capture pairwise token2token global source2token dependencies novel compatibility function compose dot product additive attentions two use tensor represent feature wise alignment score better expressive power require parallelizable matrix multiplications three combine multi head multi dimensional attentions apply distinct positional mask head subspace memory computation distribute multiple head sequential information encode independently experiment show cnn rnn free model base mtsa achieve state art competitive performance nine nlp benchmarks compel memory time efficiency
present koko system take declarative information extraction new level incorporate advance natural language process techniques extraction language koko novel extraction language simultaneously support condition surface text structure dependency parse tree sentence thereby allow refine extractions koko also support condition forgive linguistic variation express concepts allow aggregate evidence entire document order filter extractions scale koko exploit multi index scheme heuristics efficient extractions extensively evaluate koko publicly available text corpora show koko indices take smallest amount space notably faster effective number prior index scheme finally demonstrate koko scale corpus five million wikipedia article
introduce novel architecture dependency parse emphstack pointer network textbftextscstackptr combine pointer networkscitepvinyals2015pointer internal stack propose model first read encode whole sentence build dependency tree top root leaf depth first fashion stack track status depth first search pointer network select one child word top stack step textscstackptr parser benefit information whole sentence previously derive subtree structure remove leave right restriction classical transition base parsers yet number step build include non projective parse tree linear length sentence transition base parsers yield efficient decode algorithm ofn2 time complexity evaluate model twenty-nine treebanks span twenty languages different dependency annotation schemas achieve state art performance twenty-one
text summarization sentiment classification aim capture main ideas text different level text summarization describe text within sentence sentiment classification regard special type summarization summarize text even abstract fashion ie sentiment class base idea propose hierarchical end end model joint learn text summarization sentiment classification sentiment classification label treat summarization text summarization output hence sentiment classification layer put upon text summarization layer hierarchical structure derive experimental result amazon online review datasets show model achieve better performance strong baseline systems abstractive summarization sentiment classification
paper present kernel entity salience model kesm improve text understand retrieval better estimate entity salience importance document kesm represent entities knowledge enrich distribute representations model interactions entities word kernels combine kernel score estimate entity salience whole model learn end end use entity salience label salience model also improve ad hoc search accuracy provide effective rank feature model salience query entities candidate document experiment two entity salience corpora two trec ad hoc search datasets demonstrate effectiveness kesm frequency base feature base methods also provide examples show kesm convey text understand ability learn entity salience search
hide structural pattern write texts subject considerable research last decades particular map text time series sentence lengths natural way investigate text structure typically sentence length quantify use measure base number word number character variations possible quantify robustness different sentence length measure analyze database contain five hundred book english book extract six distinct measure sentence length include number word number character take account lemmatization stop word removal compare six measure book use pearson coefficient investigate linear correlations ii kolmogorov smirnov test compare distributions iii detrended fluctuation analysis dfa quantify auto correlations find six measure exhibit similar behavior suggest sentence length robust measure relate text structure
present approach interactive predictive neural machine translation attempt reduce human effort three directions firstly instead require humans select correct delete segment employ idea learn human reinforcements form judgments quality partial translations secondly human effort reduce use entropy word predictions uncertainty criterion trigger feedback request lastly online update model parameters every interaction allow model adapt quickly show simulation experiment reward signal partial translations significantly improve character f score bleu compare feedback full translations human effort reduce average number five feedback request every input
since june two thousand and thirteen brazil face largest significant mass protest generation political crisis course midst crisis brazilian politicians use social media communicate electorate order retain grow political capital problem many controversial topics course deputies may prefer avoid theme message characterize behavior propose method accurately identify political non political tweet independently deputy post time post moreover collect tweet congressmen active twitter work brazilian parliament october two thousand and thirteen october two thousand and seventeen evaluate method use word cloud topic model identify main political non political latent topics parliamentarian tweet result indicate proposal able accurately distinguish political non political tweet moreover analyse reveal strike fact half message post brazilian deputies non political
automatically generate fake restaurant review threat online review systems recent research show users difficulties detect machine generate fake review hide among real restaurant review method use work char lstm one drawback difficulties stay context ie generate review specific target entity result review may contain phrase unrelated target thus increase detectability work present evaluate sophisticate technique base neural machine translation nmt generate review stay topic test multiple variants technique use native english speakers amazon mechanical turk demonstrate review generate best variant almost optimal undetectability class average f score forty-seven conduct user study skeptical users show method evade detection frequently compare state art average evasion thirty-two four vs fifteen four statistical significance level alpha one section forty-three develop effective detection tool reach average f score ninety-seven classify although fake review effective fool people effective automatic detection still feasible
embed knowledge graph kgs continuous vector space focus current research early work perform task via simple model develop kg triple recent attempt focus either design complicate triple score model incorporate extra information beyond triple paper contrast investigate potential use simple constraints improve kg embed examine non negativity constraints entity representations approximate entailment constraints relation representations former help learn compact interpretable representations entities latter encode regularities logical entailment relations distribute representations constraints impose prior beliefs upon structure embed space without negative impact efficiency scalability evaluation wordnet freebase dbpedia show approach simple yet surprisingly effective significantly consistently outperform competitive baselines constraints impose indeed improve model interpretability lead substantially increase structure embed space code data available https githubcom iieir km complex nneaer
paper provide holistic study stock price vary response financial disclosures across different topics thereby specifically would light extensive amount file priori categorization content exist purpose utilize approach data mine namely latent dirichlet allocation mean topic model technique facilitate task automatically categorize ex ante content seventy thousand regulatory eight k file yous company evaluate subsequent stock market reaction empirical evidence suggest considerable discrepancy among various type news stories term relevance impact financial market instance find statistically significant abnormal return response earn result credit rat also disclosures regard business strategy health sector well mergers acquisitions result yield find benefit managers investors policy makers indicate regulatory file structure topics likely precede change stock valuations
hypertext document web page academic paper great importance deliver information daily life although effective plain document conventional text embed methods suffer information loss directly adapt hyper document paper propose general embed approach hyper document namely hyperdoc2vec along four criteria characterize necessary information hyper document embed model preserve systematic comparisons conduct hyperdoc2vec several competitors two task ie paper classification citation recommendation academic paper domain analyse experiment validate superiority hyperdoc2vec model wrt four criteria
word embeddings widely use sentiment classification efficacy semantic representations word give review different domains exist methods word embeddings exploit sentiment information produce domain sensitive embeddings hand exist methods generate domain sensitive word embeddings distinguish word similar contexts opposite sentiment polarity propose new method learn domain sensitive sentiment aware embeddings simultaneously capture information sentiment semantics domain sensitivity individual word method automatically determine produce domain common embeddings domain specific embeddings differentiation domain common domain specific word enable advantage data augmentation common semantics multiple domains capture vary semantics specific word different domains time experimental result show model provide effective way learn domain sensitive sentiment aware word embeddings benefit sentiment classification sentence level lexicon term level
paper investigate tendency end end neural machine read comprehension mrc model match shallow pattern rather perform inference orient reason rc benchmarks aim test ability systems answer question focus referential inference propose parallelqa strategy formulate question use parallel passages also demonstrate exist neural model fail generalize well set
end end speech recognition become increasingly popular mandarin speech recognition achieve delightful performance mandarin tonal language different english require special treatment acoustic model units several different kinds model units mandarin phoneme syllable chinese character work explore two major end end model connectionist temporal classification ctc model attention base encoder decoder model mandarin speech recognition compare performance three different scale model units context dependent phonemecdp syllable tone chinese character find type model units achieve approximate character error rate cer ctc model performance chinese character attention model better syllable attention model furthermore find chinese character reasonable unit mandarin speech recognition didicallcenter task chinese character attention model achieve cer five hundred and sixty-eight ctc model get cer seven hundred and twenty-nine didireading task cer four hundred and eighty-nine five hundred and seventy-nine respectively moreover attention model achieve better performance ctc model datasets
past years distribute semantic representations prove effective flexible keepers prior knowledge integrate downstream applications survey focus representation mean start theoretical background behind word vector space model highlight one major limitations mean conflation deficiency arise represent word possible mean single vector explain deficiency address transition word level fine grain level word sense broader acceptation method model unambiguous lexical mean present comprehensive overview wide range techniques two main branch sense representation ie unsupervised knowledge base finally survey cover main evaluation procedures applications type representation provide analysis four important aspects interpretability sense granularity adaptability different domains compositionality
word embeddings effective intermediate representations capture semantic regularities word learn representations text sequence propose view text classification label word joint embed problem label embed space word vectors introduce attention framework measure compatibility embeddings text sequence label attention learn train set label sample ensure give text sequence relevant word weight higher irrelevant ones method maintain interpretability word embeddings enjoy build ability leverage alternative source information addition input text sequence extensive result several large text datasets show propose framework outperform state art methods large margin term accuracy speed
natural language inference challenge task receive substantial attention state art model achieve impressive test set performance form accuracy score go beyond single evaluation metric examine robustness semantically valid alterations input data identify three factor insensitivity polarity unseen pair compare impact three snli model variety condition result demonstrate number strengths weaknesses model ability generalise new domain instance particular strong performance possible unseen hypernyms unseen antonyms challenge model generally model suffer insensitivity certain small semantically significant alterations also often influence simple statistical correlations word train label overall show evaluations nli model benefit study influence factor intrinsic model find dataset use
present framework analyze state rnns remember input embeddings approach inspire backpropagation sense compute gradients state respect input embeddings gradient matrix decompose singular value decomposition analyze directions embed space best transfer hide state space characterize largest singular value apply approach lstm language model investigate extent long certain class word remember average certain corpus additionally extent specific property relationship remember rnn track compare vector characterize property directions embed space best preserve hide state space
common question study class context free grammars whether equivalence decidable within class answer question positively class clark congruential grammars interest grammatical inference also consider problem check whether give cfg clark congruential show decidable give cfg dcfg
many information retrieval algorithms rely notion good distance allow efficiently compare object different nature recently new promise metric call word mover distance propose measure divergence text passages paper demonstrate metric extend incorporate term weight scheme provide accurate computationally efficient match document use entropic regularization evaluate benefit extensions task cross lingual document retrieval cldr experimental result eight cldr problems suggest propose methods achieve remarkable improvements term mean reciprocal rank compare several baselines
introduce latent vector grammars lvegs new framework extend latent variable grammars nonterminal symbol associate continuous vector space represent set infinitely many subtypes nonterminal show previous model latent variable grammars compositional vector grammars interpret special case lvegs present gaussian mixture lvegs gm lvegs new special case lvegs use gaussian mixtures formulate weight production rule subtypes nonterminals major advantage use gaussian mixtures partition function expectations subtype rule compute use extension inside outside algorithm enable efficient inference learn apply gm lvegs part speech tag constituency parse show gm lvegs achieve competitive accuracies code available https githubcom zhaoyanpeng lveg
paper introduce zero shoot dialog generation zsdg step towards neural dialog systems instantly generalize new situations minimal data zsdg enable end end generative dialog system generalize new domain domain description provide train dialogs available novel learn framework action match propose algorithm learn cross domain embed space model semantics dialog responses turn let us neural dialog generation model generalize new domains evaluate methods new synthetic dialog dataset exist human human dialog dataset result show method superior performance learn dialog model rapidly adapt behavior new domains suggest promise future research
neural machine translation nmt perform poor low resource language pair xz especially z rare language introduce another rich language propose novel triangular train architecture ta nmt leverage bilingual data yz may small xy rich improve translation performance low resource pair triangular architecture z take intermediate latent variable translation model z jointly optimize unify bidirectional algorithm goal maximize translation likelihood xy empirical result demonstrate method significantly improve translation quality rare languages multiun iwslt2012 datasets achieve even better performance combine back translation methods
despite effectiveness recurrent neural network language model maximum likelihood estimation suffer two limitations treat sentence match grind truth equally poor ignore structure output space second suffer exposure bias train tokens predict give grind truth sequence test time prediction condition generate output sequence overcome limitations build upon recent reward augment maximum likelihood approach ie sequence level smooth encourage model predict sentence close grind truth accord give performance metric extend approach token level loss smooth propose improvements sequence level smooth approach experiment two different task image caption machine translation show token level sequence level loss smooth complementary significantly improve result
script event prediction require model predict subsequent event give exist event context previous model base event pair event chain make full use dense event connections may limit capability event prediction remedy propose construct event graph better utilize event network information script event prediction particular first extract narrative event chain large quantities news corpus construct narrative event evolutionary graph neeg base extract chain neeg see knowledge base describe event evolutionary principles pattern solve inference problem neeg present scale graph neural network sgnn model event interactions learn better event representations instead compute representations whole graph sgnn process concern nod time make model feasible large scale graph compare similarity input context event representations candidate event representations choose reasonable subsequent event experimental result widely use new york time corpus demonstrate model significantly outperform state art baseline methods use standard multiple choice narrative cloze evaluation
present new large scale corpus question answer drive semantic role label qa srl annotations first high quality qa srl parser corpus qa srl bank twenty consist two hundred and fifty thousand question answer pair sixty-four thousand sentence across three domains gather new crowd source scheme show high precision good recall modest cost also present neural model two qa srl subtasks detect argument span predicate generate question label semantic relationship best model achieve question accuracy eight hundred and twenty-six span level accuracy seven hundred and seventy-six human evaluation full pipelined qa srl prediction task also show use gather additional annotations low cost
motivations like domain adaptation transfer learn feature learn fuel interest induce embeddings rare unseen word n grams synsets textual feature paper introduce la carte embed simple general alternative usual word2vec base approach build representations base upon recent theoretical result glove like embeddings method rely mainly linear transformation efficiently learnable use pretrained word vectors linear regression transform applicable fly future new text feature rare word encounter even single usage example available introduce new dataset show la carte method require fewer examples word context learn high quality embeddings obtain state art result nonce task unsupervised document classification task
demand response dr scheme effective tool maintain dynamic balance energy market higher integration fluctuate renewable energy source dr scheme use harness residential devices flexibility utilize achieve social financial objectives however exist dr scheme suffer low user participation fail take account users requirements first dr scheme highly demand users users need provide direct information eg via survey energy consumption preferences second user utility model base survey hard cod adapt time third exist schedule techniques require users input energy requirements daily basis alternative paper propose dr scheme user orient direct load control residential appliances operations instead rely user survey evaluate user utility propose online data drive approach estimate user utility function purely base available load consumption data adaptively model users preference time scheme base day ahead schedule technique transparently prescribe users optimal device operation schedule take account financial benefit user perceive quality service model day ahead user energy demand flexibility propose probabilistic approach generate flexibility model uncertainty result real world simulate datasets show dr scheme provide significant financial benefit preserve user perceive quality service
analyze state art deep learn model three task question answer one image two table three passages text use notion emphattribution word importance find deep network often ignore important question term leverage behavior perturb question craft variety adversarial examples strongest attack drop accuracy visual question answer model six hundred and eleven nineteen tabular question answer model three hundred and thirty-five thirty-three additionally show attributions strengthen attack propose jia liang two thousand and seventeen paragraph comprehension model result demonstrate attributions augment standard measure accuracy empower investigation model performance model accurate wrong reason attributions surface erroneous logic model indicate inadequacies test data
present paper abstract write system base attentive neural sequence sequence model take title input automatically generate abstract design novel write edit network attend title previously generate abstract draft iteratively revise polish abstract two series turing test human judge ask distinguish system generate abstract human write ones system pass turing test junior domain experts rate thirty non expert rate eighty
propose novel data augmentation label sentence call contextual augmentation assume invariance sentence natural even word sentence replace word paradigmatic relations stochastically replace word word predict bi directional language model word position word predict accord context numerous appropriate augmentation original word furthermore retrofit language model label conditional architecture allow model augment sentence without break label compatibility experiment six various different text classification task demonstrate propose method improve classifiers base convolutional recurrent neural network
weight finite state transducers fsts frequently use language process handle task part speech tag speech recognition previous work use multiple cpu core accelerate finite state algorithms limit attention give parallel graphics process unit gpu implementations paper introduce first knowledge gpu implementation fst composition operation also discuss optimizations use achieve best performance architecture show approach obtain speedups 6x serial implementation 45x openfst
semantic similarity important application find use many downstream nlp applications though task mathematically define semantic similarity essence capture notions similarity impregnate humans machine use heuristics calculate similarity word typically corpus dependent useful specific domains difference semantic similarity semantic relatedness motivate development new algorithms human word car road probably relate car bus may case computational methods ontological methods good encode semantic similarity vector space model better encode semantic relatedness dearth methods leverage ontologies create better vector representations aim proposal explore direction hybrid method combine statistical vector space methods like word2vec ontological methods like wordnet leverage advantage provide
analogical reason effective capture linguistic regularities paper propose analogical reason task chinese delve chinese lexical knowledge sketch sixty-eight implicit morphological relations twenty-eight explicit semantic relations big balance dataset ca8 build task include seventeen thousand, eight hundred and thirteen question furthermore systematically explore influence vector representations context feature corpora analogical reason experiment ca8 prove reliable benchmark evaluate chinese word embeddings
online social media users react content base context emotions mood play significant part reactions fill platforms opinionated content different approach applications make better use data continuously develop however due nature data variety platforms dynamic online user behavior still many issue deal remain challenge properly obtain reliable emotional status user prior post comment work introduce methodology explore semi supervise multilingual emotion detection base overlap facebook reactions textual data result emotion detection system evaluate possibility use emotions user behavior feature task sarcasm detection one million english chinese comment sixty-two thousand public facebook page post collect process conduct experiment show acceptable performance metrics
work focus use acoustic cue model turn take dyadic speak dialogues previous work show speaker intentions eg ask question utter backchannel etc influence turn take behavior good predictors turn transition speak dialogues however speaker intentions readily available use automate systems run time make difficult use information anticipate turn transition end propose multi task neural approach predict turn transition speaker intentions simultaneously result show add auxiliary task speaker intention prediction improve performance turn transition prediction speak dialogues without rely additional input feature run time
stance classification target stance make define boundary task classifier usually train prediction target work explore potential generalize classifiers different target propose neural model apply learn source target destination target show model find useful information share relevant target improve generalization certain scenarios
research relatively unexplored area question answer technologies patient specific question electronic health record large dataset human expert curated question answer pair important pre requisite develop train evaluate question answer system power machine learn paper describe process create dataset question answer methodology replicable conduct medical students annotators result high inter annotator agreement seventy-one cohen kappa course eleven months eleven medical students follow annotation methodology result question answer dataset five thousand, six hundred and ninety-six question seventy-one patient record one thousand, seven hundred and forty-seven question correspond answer generate medical students
text base game suitable test bed design agents learn interaction environment form natural language text recently deep reinforcement learn base agents successfully apply play text base game paper explore possibility design single agent play several text base game expand agent vocabulary use vocabulary agents train multiple game extent explore application recently propose policy distillation method video game text base game set also use text base game test bed analyze hence understand policy distillation approach detail
word polysemous multi faceted many shade mean suggest sparse distribute representations suitable commonly use dense representations express multiple facets present category builder work system show make use sparse representations support multi faceted lexical representations argue set expansion task well suit study mean distinctions since word may belong multiple set different reason membership therefore exhibit performance category builder task show representation capture time analogy problems ganga egypt voldemort tolkien category builder show expressive lexical representation outperform dense representations word2vec analogy class despite show two three input term
study shoot learn natural language domains compare many exist work apply either metric base optimization base meta learn image domain low inter task variance consider realistic set task diverse however impose tremendous difficulties exist state art metric base algorithms since single metric insufficient capture complex task variations natural language domain alleviate problem propose adaptive metric learn approach automatically determine best weight combination set metrics obtain meta train task newly see shoot task extensive quantitative evaluations real world sentiment analysis dialog intent classification datasets demonstrate propose method perform favorably state art shoot learn algorithms term predictive accuracy make code data available study
introduce new approach tackle problem offensive language online social media approach use unsupervised text style transfer translate offensive sentence non offensive ones propose new method train encoder decoders use non parallel data combine collaborative classifier attention cycle consistency loss experimental result data twitter reddit show method outperform state art text style transfer system two three quantitative metrics produce reliable non offensive transfer sentence
robotic agents share autonomy human leverage human domain knowledge account preferences complete task extra knowledge dramatically improve plan efficiency user satisfaction gain lose communicate robot tax unnatural paper show view humanrobot language lens share autonomy explain efficiency versus cognitive load trade off humans make decide cooperative explicit make instructions
recent developments monetary policy federal reserve create need objective method communication analysisusing methods develop text analysis present novel technique analysis create semantic space define various policymakers public comment place committee consensus appropriate location possible determine member committee closely align committee consensus time create foundation actionable research
human communication include information opinions reactions reactions often capture affective message write well verbal communications work affect model extent affective content generation area affective word distributions well study synsets lexica capture semantic relationships across word model however lack encode affective emotional word interpretations propose model aff2vec provide method enrich word embeddings representative affective interpretations word aff2vec outperform state art intrinsic word similarity task use aff2vec representations outperform baseline embeddings downstream natural language understand task include sentiment analysis personality detection frustration prediction
despite substantial interest applications neural network information retrieval neural rank model apply standard ad hoc retrieval task web page newswire document paper propose mp hcnn multi perspective hierarchical convolutional neural network novel neural rank model specifically design rank short social media post identify document length informal language heterogeneous relevance signal feature distinguish document domain present model specifically design characteristics mind model use hierarchical convolutional layer learn latent semantic soft match relevance signal character word phrase level pool base similarity measurement layer integrate evidence multiple type match query social media post well urls contain post extensive experiment use twitter data trec microblog track two thousand and eleven two thousand and fourteen show model significantly outperform prior feature base well exist neural rank model best knowledge paper present first substantial work tackle search social media post use neural rank model
reproducibility report paper learn count object natural image visual questionanswering
predict congressional legislators vote important understand past future behavior however previous work roll call prediction limit single session settings thus consider generalization across sessions paper show metadata crucial model vote outcomes new contexts change sessions lead change underlie data generation process show augment bill text sponsor ideologies neural network model achieve average four boost accuracy previous state art
answer visual question need acquire daily common knowledge model semantic connection among different part image difficult vqa systems learn image supervision answer meanwhile image caption systems beam search strategy tend generate similar caption fail diversely describe image address aforementioned issue present system two task compensate capable jointly produce image caption answer visual question particular utilize question image feature generate question relate caption use generate caption additional feature provide new knowledge vqa system image caption system attain informative result term relative improvements vqa task well competitive result use automate metrics apply system vqa task result vqa v2 dataset achieve six hundred and fifty-eight use generate caption six hundred and ninety-one use annotate caption validation set six hundred and eighty-four test standard set ensemble ten model result six hundred and ninety-seven test standard split
work propose novel approach base sequence sequence seq2seq model context aware conversational systems exist ing seq2seq model show good generate natural responses data drive conversational system however still lack mechanisms incorporate previous conversation turn investigate rnn base methods efficiently integrate previous turn context generate responses overall experimental result base human judgment demonstrate feasibility effectiveness propose approach
paper contribute cross lingual image annotation retrieval term data baseline methods propose coco cn novel dataset enrich ms coco manually write chinese sentence tag effective annotation acquisition develop recommendation assist collective annotation system automatically provide annotator several tag sentence deem relevant respect pictorial content twenty thousand, three hundred and forty-two image annotate twenty-seven thousand, two hundred and eighteen chinese sentence seventy thousand, nine hundred and ninety-three tag coco cn currently largest chinese english dataset provide unify challenge platform cross lingual image tag caption retrieval develop conceptually simple yet effective methods per task learn cross lingual resources extensive experiment three task justify viability propose dataset methods data code publicly available https githubcom li xirong coco cn
paper describe formalism subsume peterson intermediate quantifier syllogistic system extend ideas van eijck aristotle logic syllogisms express concise form make use extend monotonicity calculus contradictory contrary relationships add deduction derive proposition express form negation
task like code synthesis natural language code retrieval code summarization data drive model show great promise however create model require parallel data natural language nl code fine grain alignments stack overflow promise source create data set question diverse correspond answer high quality code snippets however exist heuristic methods eg pair title post code accept answer limit coverage correctness nl code pair obtain paper propose novel method mine high quality align data use two set feature hand craft feature consider structure extract snippets correspondence feature obtain train probabilistic model capture correlation nl code use neural network feature feed classifier determine quality mine nl code pair experiment use python java test bed show propose method greatly expand coverage accuracy exist mine methods even use small number label examples find reasonable result achieve even train classifier one language test another show promise scale nl code mine wide variety program languages beyond able annotate data
ontologies cover overlap topics overlap represent use ontology alignments alignments need continuously adapt change ontologies especially large ontologies costly task often consist manual work find change lead adaption alignment potentially make process significantly easier work present approach find change base rdf embeddings common classification techniques examine feasibility approach evaluation real world dataset present evaluation best classifiers reach precision eight
problem word sense disambiguation wsd consider article give set synonyms synsets sentence synonyms necessary select mean word sentence automatically one thousand, two hundred and eighty-five sentence tag experts namely one dictionary mean select experts target word solve wsd problem algorithm base new method vector word contexts proximity calculation propose order achieve higher accuracy preliminary epsilon filter word perform sentence set synonyms extensive program experiment carry four algorithms implement include new algorithm experiment show number case new algorithm show better result develop software tag corpus open license available online wiktionary wikisource use brief description work view slide https googl 9ak6gt video lecture russian research available online https youtube dlmrkepf58
demonstration present infrastructure compute multilingual semantic relatedness correlation twelve natural languages use three distributional semantic model dsms demonsrator dinfra distributional infrastructure provide researchers developers highly useful platform process large scale corpora conduct experiment distributional semantics integrate several multilingual dsms webservice end user obtain result without worry complexities involve build dsms webservice allow users easy access wide range comparisons dsms different parameters addition users configure access dsm parameters use easy use api
dialogue state track estimate user goals request give dialogue context essential part task orient dialogue systems paper propose global locally self attentive dialogue state tracker glad learn representations user utterance previous system action global local modules model use global modules share parameters estimators different type call slot dialogue state use local modules learn slot specific feature show significantly improve track rare state achieve state art performance woz dstc2 state track task glad obtain eight hundred and eighty-one joint goal accuracy nine hundred and seventy-one request accuracy woz outperform prior work thirty-seven fifty-five dstc2 model obtain seven hundred and forty-five joint goal accuracy nine hundred and seventy-five request accuracy outperform prior work eleven ten
learn joint multilingual sentence embed use distance sentence different languages filter noisy parallel data mine parallel data large news collections able improve competitive baseline wmt fourteen english german task three bleu filter twenty-five train data approach use mine additional bitexts wmt fourteen system obtain competitive result bucc share task identify parallel sentence comparable corpora approach generic apply many language pair independent architecture machine translation system
background social media capacity afford healthcare industry valuable feedback patients reveal express medical decision make process well self report quality life indicators post treatment prior work crannell et al study active cancer patient population twitter compile set tweet describe experience disease refer online public testimonies invisible patient report outcomes ipros carry relevant indicators yet difficult capture conventional mean self report methods present study aim identify tweet relate patient experience additional informative tool monitor public health use twitter public stream api compile fifty-three million breast cancer relate tweet span september two thousand and sixteen mid december two thousand and seventeen combine supervise machine learn methods natural language process sift tweet relevant breast cancer patient experience analyze sample eight hundred and forty-five breast cancer patient survivor account responsible forty-eight thousand post investigate tweet content hedonometric sentiment analysis quantitatively extract emotionally charge topics result find positive experience share regard patient treatment raise support spread awareness discussions relate healthcare prevalent largely negative focus fear political legislation could result loss coverage conclusions social media provide positive outlet patients discuss need concern regard healthcare coverage treatment need capture ipros online communication help inform healthcare professionals lead connect personalize treatment regimens
paper present machine learn architecture snip voice platform software solution perform speak language understand microprocessors typical iot devices embed inference fast accurate enforce privacy design personal user data ever collect focus automatic speech recognition natural language understand detail approach train high performance machine learn model small enough run real time small devices additionally describe data generation procedure provide sufficient high quality train data without compromise user privacy
neural machine translation nmt systems rely large amount parallel data major challenge low resource languages build recent work unsupervised semi supervise methods present approach combine zero shoot dual learn latter rely reinforcement learn exploit duality machine translation task require monolingual data target language pair experiment show zero shoot dual system train english french english spanish outperform large margins standard nmt system zero shoot translation performance spanish french directions zero shoot dual method approach performance within twenty-two bleu point comparable supervise set method obtain improvements also set small amount parallel data zero shoot language pair available add russian extend experiment jointly model six zero shoot translation directions directions improve four fifteen bleu point reach performance near supervise set
present novel deep learn architecture address cloze style question answer task exist approach employ read mechanisms fully exploit interdependency document query paper propose novel emphdependent gate read bidirectional gru network dgr efficiently model relationship document query encode decision make evaluation show dgr obtain highly competitive performance well know machine comprehension benchmarks children book test cbt ne cbt cn wdw strict relax finally extensively analyze validate model ablation attention study
present study reinforcement learn rl human bandit feedback sequence sequence learn exemplify task bandit neural machine translation nmt investigate reliability human bandit feedback analyze influence reliability learnability reward estimator effect quality reward estimate overall rl task analysis cardinal five point rat ordinal pairwise preferences feedback show intra inter annotator alpha agreement comparable best reliability obtain standardize cardinal feedback cardinal feedback also easiest learn generalize finally improvements one bleu obtain integrate regression base reward estimator train cardinal feedback eight hundred translations rl nmt show rl possible even small amount fairly reliable human feedback point great potential applications larger scale
number miss people ie people get lose greatly increase recent years serious worldwide problem find miss people consume large amount social resources track find miss people timely data gather analysis actually play important role development social media information miss people get propagate web quickly provide promise way solve problem information online social media usually heterogeneous categories involve complex social interactions textual data diverse structure effective fusion different type information address miss people identification problem great challenge motivate multi instance learn problem exist social science theory homophily paper propose novel r instance ri learn model
inspire double temporality characteristic narrative texts propose novel approach acquire rich temporal event knowledge across sentence narrative stories double temporality state narrative story often describe sequence events follow chronological order therefore temporal order events match textual order explore narratology principles build weakly supervise approach identify 287k narrative paragraph three large text corpora extract rich temporal event knowledge narrative paragraph event knowledge show useful improve temporal relation classification outperform several recent neural network model narrative cloze task
task multi image cue story generation visual storytelling dataset vist challenge compose multiple coherent sentence give sequence image main difficulty generate image specific sentence within context overall image propose deep learn network model glac net generate visual stories combine global local glocal attention context cascade mechanisms model incorporate two level attention ie overall encode level image feature level construct image dependent sentence standard attention configuration need large number parameters glac net implement simple way via hard connections output encoders image feature onto sentence generators coherency generate story improve convey cascade information previous sentence next sentence serially evaluate performance glac net visual storytelling dataset vist achieve competitive result compare state art techniques code pre train model available
paper study problem geometric reason context question answer introduce dynamic spatial memory network dsmn new deep network architecture design answer question admit latent visual representations dsmn learn generate reason representations propose two synthetic benchmarks floorplanqa shapeintersection evaluate geometric reason capability qa systems experimental result validate effectiveness propose dsmn visual think task
goal author profile ap identify demographic aspects eg age gender give set author analyze write texts recently ap task gain interest many problems relate computer forensics psychology market specially relate social media exploitation know social media data share wide range modalities eg text image audio represent valuable information exploit extract valuable insights users nevertheless current work ap use social media data devote analyze textual information work start explore gender identification use visual information contrastingly paper focus exploit visual modality perform age gender identification social media specifically twitter goal evaluate pertinence use visual information solve ap task accordingly extend twitter corpus pan two thousand and fourteen incorporate post image users make distinction tweet retweeted image perform experiment provide interest evidence usefulness visual information comparison traditional textual representations ap task
paper present generative model generate natural language sentence describe table region eg row model map row table continuous vector generate natural language sentence leverage semantics table deal rare word appear table develop flexible copy mechanism selectively replicate content table output sequence extensive experiment demonstrate accuracy model power copy mechanism two synthetic datasets wikibio simplequestions model improve current state art bleu four score three thousand, four hundred and seventy four thousand and twenty-six three thousand, three hundred and thirty-two three thousand, nine hundred and twelve respectively furthermore introduce open domain dataset wikitabletext include thirteen thousand, three hundred and eighteen explanatory sentence four thousand, nine hundred and sixty-two table model achieve bleu four score three thousand, eight hundred and twenty-three outperform template base language model base approach
ilcm project pursue development integrate research environment analysis structure unstructured data software service architecture saas research environment address requirements quantitative evaluation large amount qualitative data text mine methods well requirements reproducibility data drive research design social sciences ilcm research environment comprise two central components first leipzig corpus miner lcm decentralize saas application analysis large amount news texts develop previous digital humanities project second text mine tool implement lcm extend open research compute orc environment executable script document call notebooks novel integration allow combine generic high performance methods process large amount unstructured text data individual program script address specific research requirements computational social science digital humanities
examine benefit visual context train neural language model perform next word prediction multi modal neural architecture introduce outperform equivalent train language alone two decrease perplexity even visual context available test fine tune embeddings pre train state art bidirectional language model bert language model framework yield thirty-five improvement advantage train visual context test without robust across different languages english german spanish different model gru lstm delta rnn well use bert embeddings thus language model perform better learn like baby ie multi modal environment find compatible theory situate cognition language inseparable physical context
programmers make rich use natural language source code write identifiers comment source code identifiers select pool tokens strongly relate mean name conventions context tokens often combine produce precise obvious designations multi part identifiers count ninety-seven name tokens public git archive largest dataset git repositories date introduce bidirectional lstm recurrent neural network detect subtokens source code identifiers train network four hundred and seventeen million distinct splittable identifiers collect one hundred and eighty-two thousand and fourteen open source project public git archive show outperform several machine learn model propose network use improve upstream model base source code identifiers well improve developer experience allow write code without switch keyboard case
effective expression draw laughter human be present paper order consider question academic standpoint generate image caption draw laugh computer system output funny caption base image caption propose computer vision field construct moreover also propose funny score flexibly give weight accord evaluation database funny score effectively bring laughter optimize model addition build self collect boketedb contain theme image funny caption text post bokete image ogiri website experiment use boketedb verify effectiveness propose method compare result obtain use propose method obtain use ms coco pre train cnnlstm baseline idiot create humans refer propose method use boketedb pre train model neural joke machine njm
visual storytelling include two important part coherence story image well story structure image text neural network model similar image sequence would provide close information story generator obtain almost identical sentence however repeatedly narrate object events undermine good story structure paper propose inter sentence diverse beam search generate expressive story compare recent model visual storytelling task generate story without consider generate sentence previous picture propose method avoid generate identical sentence even give sequence similar picture
paper present amnestic forgery ontology metaphor semantics base metanet inspire theory conceptual metaphor amnestic forgery reuse extend framester schema ideal ontology design framework deal semiotic referential aspects frame roles mappings eventually blend description resource supply discussion applications examples take metaphor generation referential problems metaphoric mappings schema data available framester sparql endpoint
enable efficient exploration web scale scientific knowledge necessary organize scientific publications hierarchical concept structure work present large scale system one identify hundreds thousands scientific concepts two tag identify concepts hundreds millions scientific publications leverage text graph structure three build six level concept hierarchy subsumption base model system build comprehensive cross domain scientific concept ontology publish date two hundred thousand concepts one million relationships
study role linguistic context predict quantifiers collect crowdsourced data human participants test various model local single sentence global context multi sentence condition model significantly perform humans former set slightly better latter human performance improve linguistic context especially proportional quantifiers model performance suffer model effective exploit lexical morpho syntactic pattern humans better genuinely understand mean global context
scene aware dialog systems able conversations users object events around progress systems make integrate state art technologies multiple research areas include end end dialog systems visual dialog video description introduce audio visual scene aware dialog avsd challenge dataset challenge one track 7th dialog system technology challenge dstc7 workshop1 task build system generate responses dialog input video
context reduce manual effort extract test case natural language requirements many approach base natural language process nlp propose literature give large amount approach area since many practitioners eager utilize techniques important synthesize provide overview state art area objective objective summarize state art nlp assist software test could benefit practitioners potentially utilize nlp base techniques moreover benefit researchers provide overview research landscape method address need conduct survey form systematic literature map classification compile initial pool ninety-five paper conduct systematic vote final pool include sixty-seven technical paper result review paper provide overview contribution type present paper type nlp approach use assist software test type require input requirements review tool support area key result detect one four thirty-eight tool eleven present paper available download two larger ratio paper thirty sixty-seven provide shallow exposure nlp aspects almost detail conclusion paper would benefit practitioners researchers serve index body knowledge area result could help practitioners utilize exist nlp base techniques turn reduce cost test case design decrease amount human resources spend test activities share review industrial collaborators initial insights show review indeed useful beneficial practitioners
development social network fake news various commercial political purpose appear large number get widespread online world deceptive word people get infect fake news easily share without fact check instance two thousand and sixteen us president election various kinds fake news candidates widely spread official news media online social network fake news usually release either smear opponents support candidate side erroneous information fake news usually write motivate voters irrational emotion enthusiasm kinds fake news sometimes bring devastate effect important goal improve credibility online social network identify fake news timely paper propose study fake news detection problem automatic fake news identification extremely hard since pure model base fact check news still open problem exist model apply solve problem thorough investigation fake news data lot useful explicit feature identify text word image use fake news besides explicit feature also exist hide pattern word image use fake news capture set latent feature extract via multiple convolutional layer model model name ti cnn text image information base convolutinal neural network propose paper project explicit latent feature unify feature space ti cnn train text image information simultaneously extensive experiment carry real world fake news datasets demonstrate effectiveness ti cnn
start idea sentiment analysis model able predict positive negative also psychological state person implement sentiment analysis model investigate relationship model emotional state first examine psychological measurements sixty-four participants ask write book report story train sentiment analysis model use crawl movie review data finally evaluate participants write use pretrained model concept transfer learn result show sentiment analysis model perform good predict score score correlation human self check sentiment
topic model widely use natural language process allow researchers estimate underlie theme collection document topic model use unsupervised methods hence require additional step attach meaningful label estimate topics process manual label scalable suffer human bias present semi automatic transfer topic label method seek remedy problems domain specific codebooks form knowledge base automate topic label demonstrate approach dynamic topic model analysis complete corpus uk house commons speeches one thousand, nine hundred and thirty-five two thousand and fourteen use cod instructions comparative agendas project label topics show method work well majority topics estimate also find institution specific topics particular subnational governance require manual input validate result use human expert cod
paper propose method obtain sentence level embeddings problem secure word level embeddings well study propose novel method obtain sentence level embeddings obtain simple method context solve paraphrase generation task use sequential encoder decoder model generate paraphrase would like generate paraphrase semantically close original sentence one way ensure add constraints true paraphrase embeddings close unrelated paraphrase candidate sentence embeddings far ensure use sequential pair wise discriminator share weight encoder train suitable loss function loss function penalize paraphrase sentence embed distance large loss use combination sequential encoder decoder network also validate method evaluate obtain embeddings sentiment analysis task propose method result semantic embeddings outperform state art paraphrase generation sentiment analysis task standard datasets result also show statistically significant
develop family techniques align word embeddings derive different source datasets create use different mechanisms eg glove word2vec methods simple close form optimally rotate translate scale minimize root mean square errors maximize average cosine similarity two embeddings vocabulary dimensional space methods extend approach know absolute orientation popular align object three dimension generalize approach smith etal iclr two thousand and seventeen prove new result optimal scale maximize cosine similarity demonstrate evaluate similarity embeddings different source mechanisms certain properties like synonyms analogies preserve across embeddings enhance simply align average ensembles embeddings
software categorization task organize software group broadly describe behavior software editors science categorization play important role several maintenance task repository navigation feature elicitation current approach attempt cast problem text classification make use rich body literature nlp domain however show paper text classification algorithms generally applicable shelf source code find work well high level project descriptions available suffer large performance penalties classify source code comment propose set adaptations state art neural classification algorithm perform two evaluations one reference data debian end user program one set c c libraries hire professional programmers annotate show propose approach achieve performance exceed previous software classification techniques well state art neural text classification technique
navigation guide natural language instructions present challenge reason problem instruction followers natural language instructions typically identify high level decisions landmarks rather complete low level motor behaviors much miss information must infer base perceptual context machine learn settings doubly challenge difficult collect enough annotate data enable learn reason process scratch also difficult implement reason process use generic sequence model describe approach vision language navigation address issue embed speaker model use speaker model one synthesize new instructions data augmentation two implement pragmatic reason evaluate well candidate action sequence explain instruction step support panoramic action space reflect granularity human generate instructions experiment show three components approach speaker drive data augmentation pragmatic reason panoramic action space dramatically improve performance baseline instruction follower double success rate best exist approach standard benchmark
function gain recognition important indicator global health remain study medical natural language process research present first analysis automatically extract descriptions patient mobility use recently develop dataset free text electronic health record frame task name entity recognition ner problem investigate applicability ner techniques mobility extraction text corpora focus patient function scarce explore domain adaptation word embeddings use recurrent neural network ner system find embeddings train small domain corpus perform nearly well learn large domain corpora domain adaptation techniques yield additional improvements precision recall analysis identify several significant challenge extract descriptions patient mobility include length complexity annotate entities high linguistic variability mobility descriptions
embeddings medical concepts medication procedure diagnosis cod electronic medical record emrs central healthcare analytics previous work medical concept embed take medical concepts emrs word document respectively nevertheless model miss temporal nature emr data one hand two consecutive medical concepts indicate temporally close correlations reveal time gap hand temporal scopes medical concepts often vary greatly eg textitcommon cold textitdiabetes paper propose incorporate temporal information embed medical cod base continuous bag word model employ attention mechanism learn soft time aware context window medical concept experiment public proprietary datasets cluster nearest neighbour search task demonstrate effectiveness model show outperform five state art baselines
large proportion online comment present public domains constructive however significant proportion toxic nature comment contain lot typos increase number feature manifold make ml model difficult train consider fact data scientists spend approximately eighty time collect clean organize data one explore much effort invest preprocessing transformation raw comment feed state art classification model help four model jigsaw toxic comment classification data demonstrate train model without transformation produce relatively decent model apply even basic transformations case lead worse performance apply caution
paper describe textent neural network model learn distribute representations entities document directly knowledge base kb give document kb consist word entity annotations train model predict entity document describe map document target entity close continuous vector space model train use large number document extract wikipedia performance propose model evaluate use two task namely fine grain entity type multiclass text classification result demonstrate model achieve state art performance task code train representations make available online academic research
child forensic interview fi present challenge effective information retrieval decision make high stake associate process demand expert legal interviewers able effectively establish channel communication elicit substantive knowledge child client minimize potential experience trauma first step toward computationally model produce quality speak interview strategies generalize understand interview dynamics propose novel methodology computationally model effectiveness criteria apply summarization topic model techniques objectively measure rank responsiveness conversational productivity child fi score information retrieval construct agenda represent general topics interest measure alignment give response leverage lexical entrainment responsiveness comparison present methods along traditional metrics evaluation discuss use prior information generate situational awareness
read comprehension model base recurrent neural network sequentially process document tokens interest turn answer complex question longer document sequential read large portion text become substantial bottleneck inspire humans use document structure propose novel framework read comprehension represent document tree model agent learn interleave quick navigation document tree expensive answer extraction encourage exploration document tree propose new algorithm base deep q network dqn strategically sample tree nod train time empirically find algorithm improve question answer performance compare dqn strong information retrieval ir baseline ensembling model ir baseline result gain performance
document give knowledge orient analysis twenty interest recognize textual entailment rte examples draw two thousand and five rte5 competition test set analysis ignore shallow statistical match techniques h rather ask would take reasonably infer imply h world knowledge would need task although knowledge intensive techniques much success rte evaluations ultimately intelligent system expect know deploy kind world knowledge require perform kind reason select examples typically ones rte system call blue get wrong ones require world knowledge answer particular analysis cover case near perfect lexical overlap h yet entailment ie examples likely current rte systems get wrong nice example three hundred and forty-one page twenty-six require infer river flood river overflow bank seem easy right enjoy
propose end end character base recurrent neural network extract disease name entities japanese medical text simultaneously judge modality either positive negative ie mention disease symptom affirm negate motivation adopt neural network learn effective lexical structural representation feature entity recognition also positive negative classification annotate corpora without explicitly provide rule base manual feature set confirm superiority method previous char base crf svm methods result
paper focus multimodal language understand method carry place task domestic service robots address case ambiguous instructions target area specify instance put away milk cereal natural instruction ambiguity regard target area consider environments daily life conventionally instruction disambiguate dialogue system cost time cumbersome interaction instead propose multimodal approach instructions disambiguate use robot state environment context develop multi modal classifier generative adversarial network mmc gin predict likelihood different target areas consider robot physical limitation target clutter approach mmc gin significantly improve accuracy compare baseline methods use instructions simple deep neural network
many popular form factor digital assistants amazon echo apple homepod google home enable user hold conversation systems base speech modality lack screen present unique challenge satisfy information need user presentation answer need optimize voice interactions paper propose task evaluate usefulness audio transformations ie prosodic modifications voice question answer introduce crowdsourcing setup evaluate quality propose modifications along multiple dimension correspond informativeness naturalness ability user identify key part answer offer set prosodic modifications highlight potentially important part answer use various acoustic cue experiment show prosodic modifications lead better comprehension expense slightly degrade naturalness audio
neural language model nlms recently gain renew interest achieve state art performance across many natural language process nlp task however nlms computationally demand largely due computational cost softmax layer large vocabulary observe decode many nlp task probabilities top k hypotheses need calculate preciously k often much smaller vocabulary size paper propose novel softmax layer approximation algorithm call fast graph decoder fgd quickly identify give context set k word likely occur accord nlm demonstrate fgd reduce decode time order magnitude attain close full softmax baseline accuracy neural machine translation language model task also prove theoretical guarantee softmax approximation quality
natural language text exhibit hierarchical structure variety respect ideally could incorporate prior knowledge hierarchical structure unsupervised learn algorithms work text data recent work nickel kiela two thousand and seventeen propose use hyperbolic instead euclidean embed space represent hierarchical data demonstrate encourage result embed graph work extend method parameterization technique allow us learn hyperbolic embeddings arbitrarily parameterized object apply framework learn word sentence embeddings hyperbolic space unsupervised manner text corpora result embeddings seem encode certain intuitive notions hierarchy word context frequency phrase constituency however implicit continuous hierarchy learn hyperbolic space make interrogate model learn hierarchies difficult model learn explicit edge items learn hyperbolic embeddings show improvements euclidean embeddings downstream task suggest hierarchical organization useful task others
generate humor quote challenge problems field computational linguistics often tackle separately paper present control long short term memory lstm architecture train categorical data like joke quote together pass category input along sequence word idea single neural net learn structure joke quote generate demand accord input category importantly believe neural net knowledge train different datasets hence enable generate creative joke quote mixture information may network generate funny inspirational joke
introduce novel meme generation system give image produce humorous relevant caption furthermore system condition image also user define label relate meme template give handle user meme content system use pretrained inception v3 network return image embed pass attention base deep layer lstm model produce caption inspire widely recognise show tell model implement modify beam search encourage diversity caption evaluate quality model use perplexity human assessment quality memes generate whether differentiate real ones model produce original memes whole differentiate real ones
sentiment analysis widely study nlp task goal determine opinions emotions evaluations users towards product entity service review one biggest challenge sentiment analysis highly language dependent word embeddings sentiment lexicons even annotate data language specific optimize model language time consume labor intensive especially recurrent neural network model resource perspective challenge collect data different languages paper look answer follow research question sentiment analysis model train language reuse sentiment analysis languages russian spanish turkish dutch data limit goal build single model language largest dataset available task reuse languages limit resources purpose train sentiment analysis model use recurrent neural network review english translate review languages reuse model evaluate sentiments experimental result show robust approach single model train english review statistically significantly outperform baselines several different languages
order computer systems human like higher emotional quotient need able process understand intrinsic human language phenomena like humour paper consider subtype humour pun common type wordplay base joke particular consider code mix pun become increasingly mainstream social media informal conversations advertisements aim build system automatically identify pun location recover target pun first study classify code mix pun two categories namely intra sentential intra word propose four step algorithm recover pun target pun belong intra sentential category algorithm use language model phonetic similarity base feature get desire result test approach small set code mix pun advertisements observe system successfully able recover target sixty-seven pun
comment software critical maintenance reuse apart prescriptive advice little practical support quantitative understand make comment useful paper introduce task identify comment uninformative code mean document address problem introduce notion comment entailment code high entailment indicate comment natural language semantics infer directly code although entail comment low quality comment easily infer example comment restate code widely discourage authorities software style base develop tool call craic score method level comment redundancy highly redundant comment expand alternately remove developer craic use deep language model exploit large software corpora without require expensive manual annotations entailment show craic perform comment entailment task good agreement human judgements find also implications documentation tool example find common tag javadoc least two time predictable code non javadoc sentence suggest javadoc tag less informative free form comment
visual question answer vqa require joint comprehension image natural language question many question directly clearly answer visual content require reason structure human knowledge confirmation visual content paper propose visual knowledge memory network vkmn address issue seamlessly incorporate structure human knowledge deep visual feature memory network end end learn framework compare exist methods leverage external knowledge support vqa paper stress two miss mechanisms first mechanism integrate visual content knowledge facts vkmn handle issue embed knowledge triple subject relation target deep visual feature jointly visual knowledge feature second mechanism handle multiple knowledge facts expand question answer pair vkmn store joint embed use key value pair structure memory network easy handle multiple facts experiment show propose method achieve promise result vqa v10 v20 benchmarks outperform state art methods knowledge reason relate question
openedgar open source python framework design rapidly construct research databases base electronic data gather analysis retrieval edgar system operate us securities exchange commission sec openedgar build django application framework support distribute compute across one servers include functionality retrieve parse index file data edgar ii build table key metadata like form type filer iii retrieve parse update cik ticker industry mappings iv extract content metadata file document v search file document content openedgar design use academic research industrial applications distribute mit license https githubcom lexpredict openedgar
recent work consider image pair speech use supervision build speech systems transcriptions available ask whether visual ground use cross lingual keyword spot give text keyword one language task retrieve speak utterances contain keyword another language could enable search speech low resource language use text query high resource language proof concept use english speech german query use german visual tagger add keyword label train image train neural network map english speech german keywords without see parallel speech transcriptions translations model achieve precision ten fifty-eight show erroneous retrievals contain equivalent semantically relevant keywords exclude would improve p10 ninety-one
paper target problem speech act detection conversations bug repair conduct wizard oz experiment thirty professional programmers programmers fix bug two hours use simulate virtual assistant help use open cod manual annotation procedure identify speech act type conversations finally train evaluate supervise learn algorithm automatically detect speech act type conversations thirty two hour conversations make two thousand, four hundred and fifty-nine annotations uncover twenty-six speech act type automate detection achieve sixty-nine precision fifty recall key application work advance state art virtual assistants software engineer virtual assistant technology grow rapidly though applications software engineer behind areas largely due lack relevant data experiment paper target problem area developer q conversations bug repair
automatic language identification natural language process problem try determine natural language give content paper present statistical method automatic language identification write text use dictionaries contain stop word diacritics propose different approach combine two dictionaries accurately determine language textual corpora method choose stop word diacritics specific language although languages similar word special character common languages take account romance languages similar usually hard distinguish computational point view test method use twitter corpus news article corpus corpora consist utf eight encode text diacritics could take account case text diacritics stop word use determine language text experimental result show propose method accuracy ninety small texts nine hundred and ninety-eight
paper present two ways deal scarce data semantic decode use n best speech recognition hypotheses first learn feature use deep learn architecture weight unknown know categories jointly optimise second unsupervised method use tune weight share weight inject prior knowledge unknown categories unsupervised tune ie risk minimisation improve f measure recognise nearly zero shoot data dstc3 corpus unsupervised method apply subject two assumptions rank class marginal assume know class conditional score classifier assume follow gaussian distribution
word semantics substantially change across communities contexts capture domain specific word semantics important challenge propose semaxis simple yet powerful framework characterize word semantics use many semantic ax word vector space beyond sentiment demonstrate semaxis capture nuanced semantic representations multiple online communities also show sentiment axis examine semaxis outperform state art approach build domain specific sentiment lexicons
capture semantic relations sentence entailment long stand challenge computational semantics logic base model analyse entailment term possible worlds interpretations situations premise p entail hypothesis h iff worlds p true h also true statistical model view relationship probabilistically address term whether human would likely infer h p paper wish bridge two perspectives argue visually ground version textual entailment task specifically ask whether model perform better addition p h also image correspond relevant world situation use multimodal version snli dataset bowman et al two thousand and fifteen compare blind visually augment model textual entailment show visual information beneficial also conduct depth error analysis reveal current multimodal model perform ground optimal fashion
investigate birth diffusion lexical innovations large dataset online social communities build sociolinguistic theories focus relation spread novel term social role individuals use uncover characteristics innovators adopters finally perform prediction task allow us anticipate whether innovation successfully spread within community
multimodal sentiment analysis actively grow field research promise area opportunity field improve multimodal fusion mechanism present novel feature fusion strategy proceed hierarchical fashion first fuse modalities two two fuse three modalities multimodal sentiment analysis individual utterances strategy outperform conventional concatenation feature one amount five reduction error rate utterance level multimodal sentiment analysis multi utterance video clip current state art techniques incorporate contextual information utterances clip hierarchical fusion give twenty-four almost ten error rate reduction currently use concatenation implementation method publicly available form open source code
survey discuss recent developments multimodal process facilitate conceptual ground language categorize information flow multimodal process respect cognitive model human information process analyze different methods combine multimodal representations base methodological inventory discuss benefit multimodal ground variety language process task challenge arise particularly focus multimodal ground verbs play crucial role compositional power language
rapid growth text sentiment analysis demand automatic classification electronic document increase leap bind paradigm text classification text mine subject many research work recent time paper propose technique text sentiment classification use term frequency inverse document frequency tf idf along next word negation nwn also compare performances binary bag word model tf idf model tf idf next word negation tf idf nwn model text classification propose model apply three different text mine algorithms find linear support vector machine lsvm appropriate work propose model achieve result show significant increase accuracy compare earlier methods
conversational systems become increasingly popular way humans interact computers able provide intelligent responses conversational systems must correctly model structure semantics conversation introduce task measure semantic incoherence conversation respect background knowledge rely identification semantic relations concepts introduce conversation propose evaluate graph base machine learn base approach measure semantic coherence use knowledge graph vector space embeddings word embed model source background knowledge demonstrate approach able uncover different coherence pattern conversations ubuntu dialogue corpus
multilingual knowledge graph kg embeddings provide latent semantic representations entities structure knowledge cross lingual inferences benefit various knowledge drive cross lingual nlp task however precisely learn cross lingual inferences usually hinder low coverage entity alignment many kgs since many multilingual kgs also provide literal descriptions entities paper introduce embed base approach leverage weakly align multilingual kg semi supervise cross lingual learn use entity descriptions approach perform co train two embed model ie multilingual kg embed model multilingual literal description embed model model train large wikipedia base trilingual dataset entity alignment unknown train experimental result show performance propose approach entity alignment task improve iteration co train eventually reach stage significantly surpass previous approach also show approach promise abilities zero shoot entity alignment cross lingual kg completion
present first attempt perform attentional word segmentation directly speech signal final goal automatically identify lexical units low resource unwritten language ul methodology assume pair record ul translations well resourced language use acoustic unit discovery aud convert speech sequence pseudo phone segment use neural soft alignments produce neural machine translation model evaluation use actual bantu ul mboshi comparisons monolingual bilingual baselines illustrate potential attentional word segmentation language documentation
task question answer gain prominence past decades test ability machine understand natural language large datasets machine read lead development neural model cater deeper language understand compare information retrieval task different components neural architectures intend tackle different challenge first step towards achieve generalization across multiple domains attempt understand compare peculiarities exist end end neural model stanford question answer dataset squad perform quantitative well qualitative analysis result attain observe prediction errors reflect certain model specific bias discuss paper
traditional neural language model tend generate generic reply poor logic emotion paper syntactically constrain bidirectional asynchronous approach emotional conversation generation e scba propose address issue model pre generate emotion keywords topic keywords asynchronously introduce process decode much different exist methods generate reply first word last experiment result indicate approach improve diversity reply gain boost logic emotion compare baselines
k fold cross validation cv popular method estimate true performance machine learn model allow model selection parameter tune however process cv require random partition data performance estimate fact stochastic variability substantial natural language process task demonstrate unstable estimate rely upon effective parameter tune result tune parameters highly sensitive data partition mean often select sub optimal parameter choices serious reproducibility issue instead propose use less variable j k fold cv j independent k fold cross validations use assess performance main contributions extend j k fold cv performance estimation parameter tune investigate choose j k argue variability important bias effective tune advocate lower choices k typically see nlp literature instead use save computation increase j demonstrate generality recommendations investigate wide range case study sentiment classification general target specific part speech tag document classification
protect user privacy data analysis state art strategy differential privacy scientific noise inject real analysis output noise mask individual sensitive information contain dataset however determine amount noise key challenge since much noise destroy data utility little noise increase privacy risk though previous research work design mechanisms protect data privacy different scenarios exist study assume uniform privacy concern individuals consequently put equal amount noise individuals lead insufficient privacy protection users protect others address issue propose self adaptive approach privacy concern detection base user personality experimental study demonstrate effectiveness address suitable personalize privacy protection cold start users ie without privacy concern information train data
propose scalable computerize approach large scale inference liver image report data system li rads final assessment categories narrative ultrasound us report although model train report create use li rads template also able infer li rads score unstructured report create li rads guidelines establish human label data require step study train li rads score automatically extract report contain structure li rads score translate derive knowledge reason unstructured radiology report provide automate li rads categorization approach may enable standardize screen recommendations treatment plan patients risk hepatocellular carcinoma may facilitate ai base healthcare research us image offer large scale text mine data gather opportunities standard hospital clinical data repositories
paper present novel model entity disambiguation combine local contextual information global evidence limit discrepancy search lds give input document start complete solution construct local model conduct search space possible corrections improve local solution global view point search utilize heuristic function focus least confident local decisions prune function score global solutions base local fitness global coherences among predict entities experimental result conll two thousand and three tac two thousand and ten benchmarks verify effectiveness model
semantic parse task transduce natural language nl utterances formal mean representations mrs commonly represent tree structure annotate nl utterances correspond mrs expensive time consume thus limit availability label data often become bottleneck data drive supervise model introduce structvae variational auto encode model semisupervised semantic parse learn limit amount parallel data readily available unlabeled nl utterances structvae model latent mrs observe unlabeled data tree structure latent variables experiment semantic parse atis domain python code generation show extra unlabeled data structvae outperform strong supervise model
survey paper evaluate several recent deep neural network dnn architectures timit phone recognition task choose timit corpus due popularity broad availability community also simulate low resource scenario helpful minor languages also prefer phone recognition task much sensitive acoustic model quality large vocabulary continuous speech recognition lvcsr task recent years many dnn publish paper report result timit however report phone error rat pers often much higher per simple fee forward ff dnn main motivation paper provide baseline dnns open source script easily replicate baseline result future paper lowest possible pers accord knowledge best achieve per survey better best publish per date
increase number texts make available internet many applications rely text mine tool tackle diversity problems relevant model represent texts call word adjacency co occurrence representation know capture mainly syntactical feature textsin study introduce novel network representation consider semantic similarity paragraph two main properties paragraph network consider ability incorporate characteristics discriminate real artificial shuffle manuscripts ii ability capture syntactical semantic textual feature result reveal real texts organize communities turn important feature discriminate artificial texts interestingly also find differently traditional co occurrence network adopt representation able capture semantic feature additionally propose framework employ analyze voynich manuscript find compatible texts write natural languages take together find suggest propose methodology combine traditional network model improve text classification task
evaluate adversarial examples become standard procedure measure robustness deep learn model due difficulty create white box adversarial examples discrete text input analyse robustness nlp model do black box adversarial examples investigate adversarial examples character level neural machine translation nmt contrast black box adversaries novel white box adversary employ differentiable string edit operations rank adversarial change propose two novel type attack aim remove change word translation rather simply break nmt demonstrate white box adversarial examples significantly stronger black box counterparts different attack scenarios show serious vulnerabilities previously know addition perform adversarial train take three time longer regular train improve model robustness significantly
extensive expansion growth social network sit allow people share view experience freely peer internet due huge amount data generate everyday basis use opinion mine extract view people particular field opinion mine find applications many areas tourism politics education entertainment etc extensively implement area education system paper discuss malpractices present examination system present scenario opinion mine vastly use decision make author paper design framework apply nai bay approach education dataset various phase nai bay approach include three step conversion data frequency table make class dataset apply nai bay algorithm equation calculate probabilities class finally highest probability class outcome prediction predictions use make improvements education system help provide better education
many nlp applications frame graph sequence learn problem previous work propose neural architectures set obtain promise result compare grammar base approach still rely linearisation heuristics standard recurrent network achieve best performance work propose new model encode full structural information contain graph architecture couple recently propose gate graph neural network input transformation allow nod edge hide representations tackle parameter explosion problem present previous work experimental result show model outperform strong baselines generation amr graph syntax base neural machine translation
automatic speech recognition asr systems recurrent neural network language model rnnlm use rescore word lattice n best hypotheses list due expensive train rnnlm vocabulary set accommodate small shortlist frequent word lead suboptimal performance input speech contain many shortlist oos word effective solution increase shortlist size retrain entire network highly inefficient therefore propose efficient method expand shortlist set pretrained rnnlm without incur expensive retrain use additional train data method exploit structure rnnlm decouple three part input projection layer middle layer output projection layer specifically method expand word embed matrices projection layer keep middle layer unchanged approach functionality pretrained rnnlm correctly maintain long oos word properly model two embed space propose model oos word borrow linguistic knowledge appropriate shortlist word additionally propose generate list oos word expand vocabulary unsupervised manner automatically extract asr output
study problem ground distributional representations texts visual domain namely visual semantic embeddings vse short begin insightful adversarial attack vse embeddings show limitation current frameworks image text datasets eg ms coco quantitatively qualitatively large gap number possible constitutions real world semantics size parallel data large extent restrict model establish link textual semantics visual concepts alleviate problem augment ms coco image caption datasets textual contrastive adversarial sample sample synthesize use linguistic rule wordnet knowledge base construction procedure syntax semantics aware sample enforce model grind learn embeddings concrete concepts within image simple powerful technique bring noticeable improvement baselines diverse set downstream task addition defend know type adversarial attack release cod https githubcom explorerfreda vse c
problem associate propagation fake news continue grow alarm scale trend generate much interest politics academia industry alike propose framework detect classify fake news message twitter post use hybrid convolutional neural network long short term recurrent neural network model propose work use deep learn approach achieve eighty-two accuracy approach intuitively identify relevant feature associate fake news stories without previous knowledge domain
propose recurrent rl agent episodic exploration mechanism help discover good policies text base game environments show promise result set generate text base game vary difficulty goal collect coin locate end chain room contrast previous text base rl approach observe agent learn policies generalize unseen game greater difficulty
automatic text summarization strategies successfully employ digest text collections extract essential content usually summaries generate use textual corpora belong domain area summary use nonetheless special case find enough textual source one possible alternative generate summary different domain one manner summarize texts consist use graph model model allow give importance word correspond main concepts target domain find summarize text give reader overview main text concepts well relationships however kind summarization present significant number repeat term compare human generate summaries paper present approach produce graph model extractive summaries texts meet target domain exigences treat term repetition problem evaluate proposition perform series experiment show propose approach statistically improve performance model base graph centrality achieve better coverage accuracy recall
apply techniques natural language process computational linguistics machine learn investigate paper hep th four relate section arxiv hep ph hep lat gr qc math ph title paper section inception arxiv end two thousand and seventeen extract treat corpus use train neural network word2vec comparative study common n grams linear syntactical identities word cloud word similarities carry find notable scientific sociological differences field conjunction support vector machine also show syntactic structure title different sub field high energy mathematical physics sufficiently different neural network perform binary classification formal versus phenomenological section eight hundred and seventy-one accuracy perform finer five fold classification across section six hundred and fifty-one accuracy
neural network base language model deal data sparsity problems map large discrete space word smaller continuous space real value vectors learn distribute vector representations word train sample inform neural network model combinatorial number pattern paper exploit sparsity natural language even encode unique input word use fix sparse random representation sparse cod project onto smaller embed space allow encode word occurrences possibly unknown vocabulary along creation compact language model use reduce number parameters investigate properties encode mechanism empirically evaluate performance widely use penn treebank corpus show guarantee approximately equidistant nearly orthogonal vector representations unique discrete input enough provide neural network model enough information learn make use distribute representations input
new whole sentence language model neural trans dimensional random field language model neural trf lm sentence model collection random field potential function define neural network introduce successfully train noise contrastive estimation nce paper extend nce propose dynamic noise contrastive estimation dnce solve two problems observe nce train first dynamic noise distribution introduce train simultaneously converge data distribution help significantly cut noise sample number use nce reduce train cost second dnce discriminate sentence generate noise distribution sentence generate interpolation data distribution noise distribution alleviate overfitting problem cause sparseness train set dnce successfully efficiently train neural trf lms large corpus eight billion word large vocabulary five hundred and sixty-eight k word neural trf lms perform good lstm lms less parameters 5x114x faster rescoring sentence interpolate neural trf lms lstm lms n gram lms reduce error rat
last decade video blog vlogs become extremely popular method people express sentiment ubiquitousness videos increase importance multimodal fusion model incorporate video audio feature traditional text feature automatic sentiment detection multimodal fusion offer unique opportunity build model learn full depth expression available human viewers detection sentiment videos acoustic video feature provide clarity otherwise ambiguous transcripts paper present multimodal fusion model exclusively use high level video audio feature analyze speak sentence sentiment discard traditional transcription feature order minimize human intervention maximize deployability model scale real world data select high level feature model successful nonaffect domains order test generalizability sentiment detection domain train test model newly release cmu multimodal opinion sentiment emotion intensity cmumosei dataset obtain f1 score eight thousand and forty-nine validation set f1 score six thousand, three hundred and twenty-five hold challenge test set
neural sequence sequence seq2seq approach prove successful grammatical error correction gec base seq2seq framework propose novel fluency boost learn inference mechanism fluency boost learn generate diverse error correct sentence pair train enable error correction model learn improve sentence fluency instance fluency boost inference allow model correct sentence incrementally multiple inference step combine fluency boost learn inference convolutional seq2seq model approach achieve state art performance seven thousand, five hundred and seventy-two f05 conll two thousand and fourteen ten annotation dataset six thousand, two hundred and forty-two gleu jfleg test set respectively become first gec system reach human level performance seven thousand, two hundred and fifty-eight conll six thousand, two hundred and thirty-seven jfleg benchmarks
three contributions work one explore utility stack denoising autoencoder paragraph vector model learn task independent dense patient representations directly clinical note analyze representations transferable across task evaluate multiple supervise setups predict patient mortality primary diagnostic procedural category gender compare performance sparse representations obtain bag word model observe learn generalize representations significantly outperform sparse representations positive instance learn absence strong lexical feature two compare model performance feature set construct bag word obtain medical concepts latter case concepts represent problems treatments test find concept identification improve classification performance three propose novel techniques facilitate model interpretability understand interpret representations explore best encode feature within patient representations obtain autoencoder model calculate feature sensitivity across two network identify significant input feature different classification task use pretrained representations supervise input successfully extract influential feature pipeline use technique
paper study problem data augmentation language understand task orient dialogue system contrast previous work augment utterance without consider relation utterances propose sequence sequence generation base data augmentation framework leverage one utterance semantic alternatives train data novel diversity rank incorporate utterance representation make model produce diverse utterances diversely augment utterances help improve language understand module experimental result airline travel information system dataset newly create semantic frame annotation stanford multi turn multidomain dialogue dataset show framework achieve significant improvements six hundred and thirty-eight one thousand and four f score respectively train set hundreds utterances represent case study also confirm method generate diverse utterances
present end end approach take unstructured textual input generate structure output compliant give vocabulary inspire recent successes neural machine translation treat triple within give knowledge graph independent graph language propose encoder decoder framework attention mechanism leverage knowledge graph embeddings model learn map natural language text triple representation form subject predicate object use select knowledge graph vocabulary experiment three different data set show achieve competitive f1 measure baselines use simple yet effective approach demo video include
increasingly complex approach question answer qa propose true gain systems particularly respect expensive train requirements inflate compare adequate baselines propose unsupervised simple fast alignment information retrieval baseline incorporate two novel contributions textitone many alignment query document term textitnegative alignment proxy discriminative information approach outperform conventional baselines well many supervise recurrent neural network also approach state art supervise systems three qa datasets three hyperparameters achieve forty-seven p1 8th grade science qa dataset three hundred and twenty-nine p1 yahoo answer qa dataset sixty-four map wikiqa also achieve two thousand, six hundred and fifty-six five thousand, eight hundred and thirty-six arc challenge easy dataset respectively addition include additional arc result version paper arc easy set also experiment one additional parameter number justifications retrieve
knowledge protein protein interactions essential understand biological process metabolic pathways dna replication transcription etc however majority exist protein protein interaction ppi systems dependent primarily scientific literature yet accessible structure database thus efficient information extraction systems require identify ppi information large collection biomedical texts exist systems model ppi extraction task classification problem tailor handcraft feature set include domain dependent feature paper present novel method base deep bidirectional long short term memory b lstm technique exploit word sequence dependency path relate information identify ppi information text model leverage joint model proteins relations single unify framework name shortest dependency path b lstm sdplstm model perform experiment two popular benchmark ppi datasets namely aim bioinfer evaluation show f1 score value eight thousand, six hundred and forty-five seven thousand, seven hundred and thirty-five aim bioinfer respectively comparisons exist systems show propose approach attain state art performance
much business literature address issue consumer centric design businesses design customize service products accurately reflect consumer preferences paper use data science natural language process methodology explore whether extent emotions shape consumer preferences media entertainment content use unique filter dataset six thousand, one hundred and seventy-four movie script generate map screen content capture emotional trajectory motion picture combine obtain mappings cluster represent group consumer emotional journey cluster use predict overall success parameters movies include box office revenues viewer satisfaction level capture imdb rat award well number viewers critics review find like book movie stories dominate six basic shape highest box offices associate man hole shape characterize emotional fall follow emotional rise shape result financially successful movies irrespective genre production budget yet man hole succeed produce like movies generate talk movies interestingly carefully choose combination production budget genre may produce financially successful movie emotional shape implications analysis generate demand content drive business model innovation entertainment industries discuss
rise digital age explosion information form news article social media much data lie unstructured form manually manage effectively make use tedious bore labor intensive explosion information need sophisticate efficient information handle tool give rise information extractionie information retrievalir technology information extraction systems take natural language text input produce structure information specify certain criteria relevant particular application various sub task ie name entity recognition coreference resolution name entity link relation extraction knowledge base reason form build block various high end natural language process nlp task machine translation question answer system natural language understand text summarization digital assistants like siri cortana google paper introduce information extraction technology various sub task highlight state art research various ie subtasks current challenge future research directions
deep learn model often easily adaptable new task require task specific adjustments differentiable neural computer dnc memory augment neural network design general problem solver use wide range task reality hard apply model new task analyze dnc identify possible improvements within application question answer motivate robust scalable dnc rsdnc objective precondition keep general character model intact make application reliable speed require train time rsdnc distinguish robust train slim memory unit bidirectional architecture achieve new state art performance babi task also minimize performance variance different initializations furthermore demonstrate simplify applicability rsdnc new task passable result cnn rc task without adaptions
paper discuss method identify seed word would best represent class name entities graphical representation word similarities word network word graph representations vectorized text nod word encounter corpus weight edge incident nod represent similar word intend build bilingual word graph identify seed word community analysis would best use segment graph accord name entities therefore provide unsupervised way tag name entities bilingual language base
use natural language give instructions robots challenge since natural language understand still largely open problem paper address problem restrict attention command model one action plus arguments also know slot action detection also call intent detection slot fill various architectures recurrent neural network long short term memory lstm network evaluate lstms achieve superior accuracy action request may fall within robots capabilities support vector machinesvm use determine whether input neural network several word embed algorithms compare finally implement system robot ros package create use smach state machine propose system evaluate use well know datasets benchmarks context domestic service robots
learn representations knowledge base entities concepts become increasingly important nlp applications however recent entity embed methods rely structure resources expensive create new domains corpora present distantly supervise method jointly learn embeddings entities text unnanotated corpus use list mappings entities surface form learn embeddings open domain biomedical corpora compare prior methods rely human annotate text large knowledge graph structure embeddings capture entity similarity relatedness better prior work exist biomedical datasets new wikipedia base dataset release community result analogy completion entity sense disambiguation indicate entities word capture complementary information effectively combine downstream use
investigate difficulty level question read comprehension datasets squad propose new question generation set name difficulty controllable question generation dqg take input sentence read comprehension paragraph text fragment ie answer want ask question dqg method need generate question give text fragment answer meanwhile generation control specify difficulty label output question satisfy specify difficulty much possible solve task propose end end framework generate question designate difficulty level explore important intuitions evaluation prepare first dataset read comprehension question difficulty label result show question generate framework better quality metrics like bleu also comply specify difficulty label
explosion video data internet require effective efficient technology generate caption automatically people able watch videos despite great progress video caption research particularly video feature encode language decoder still largely base prevail rnn decoder lstm tend prefer frequent word align video paper propose boundary aware hierarchical language decoder video caption consist high level gru base language decoder work global caption level language model low level gru base language decoder work local phrase level language model importantly introduce binary gate low level gru language decoder detect language boundaries together advance components include joint video prediction share soft attention boundary aware video encode integrate video caption framework discover hierarchical language information distinguish subject object sentence usually confuse language generation extensive experiment two widely use video caption datasets msr video text msr vtt citexu2016msr youtube text msvd citechen2011collecting show method highly competitive compare state art methods
eighty today data unstructured nature unstructured datasets evolve time large part datasets text document generate media outlets scholarly article digital libraries find scientific professional communities social media vector space model develop analyze text data use data mine machine learn algorithms ample vector space model exist text data evolutionary aspect ever change text corpora still miss vector base representations advent word embeddings enable us create contextual vector space embeddings fail consider temporal aspects feature space successfully paper present approach include temporal aspects feature space inclusion time aspect feature space provide vectors every natural language element word entities every timestamp temporal word vectors allow us track mean word change time study change neighborhood moreover time reflective text representation pave way new set text analytic abilities involve time series text collections paper present time reflective vector space model temporal text data able capture short long term change mean word compare approach limit literature dynamic embeddings present qualitative quantitative evaluations use track semantic evolution target application
investigative journalism recent years confront two major challenge one vast amount unstructured data originate large text collections leak answer freedom information request two multi lingual data due intensify global cooperation communication politics business civil society face challenge journalists increasingly cooperate international network support collaborations present new version new leak twenty open source software content base search leak include three novel main feature one automatic language detection language dependent information extraction forty languages two entity keyword visualization efficient exploration three decentral deployment analysis confidential data various format illustrate new analysis capabilities exemplary case study
image classification ongoing research challenge available research focus image classification english language however little research image classification arabic language expand image classification arabic several applications present study investigate method generate arabic label image object method use study involve direct english arabic translation label currently available imagenet database commonly use image classification research purpose study test accuracy method study two thousand, eight hundred and eighty-seven label image randomly select imagenet label translate english arabic use google translate accuracy translations evaluate result indicate six hundred and fifty-six arabic label accurate study make three important contributions image classification literature one determine baseline level accuracy algorithms provide arabic label image two provide one thousand, eight hundred and ninety-five image tag accurate arabic label three provide accuracy translations image label english arabic
search typically rely keyword query often semantically ambiguous propose overcome offer users natural language question base keyword query disambiguate intent keyword question task may address use neural machine translation techniques neural translation model however require massive amount train data keyword question pair unavailable task main idea paper generate large amount synthetic train data small seed set hand label keyword question pair since natural language question available large quantities develop model automatically generate correspond keyword query introduce various filter mechanisms ensure synthetic train data high quality demonstrate feasibility approach use automatic manual evaluation extend version article publish title proceed ictir eighteen
text search base lexical match keywords satisfactory due polysemous synonymous word semantic search exploit word mean general improve search performance paper survey wordnet base information retrieval systems employ word sense disambiguation method process query document problem many case word one possible direct sense pick one may give wrong sense word moreover previous systems use word form represent word sense hypernyms propose novel approach use specific common hypernym remain undisambiguated multi sense word well combine wordnet feature represent word mean experiment benchmark dataset show term map measure search engine one hundred and seventy-seven better lexical search least ninety-four better survey search systems use wordnet keywords ontology word sense disambiguation semantic annotation semantic search
present late algorithm asynchronous variant earley algorithm parse context free grammars earley algorithm naturally task base difficult parallelize dependencies task present late algorithm use additional data structure maintain information state parse work items may process order property allow late algorithm speed use task parallelism show late algorithm achieve 120x speedup earley algorithm natural language task
last several years twitter adopt company alternative platform interact customers address concern abundance unconventional conversation resources push develop effective virtual agents ever address challenge better understand customer service conversations require lately several work propose novel taxonomy fine grain dialogue act well develop algorithms automatic detection act outcomes work provide step stone ultimate goal build efficient effective virtual agents none work consider handle notion negation propose algorithms work develop svm base dialogue act prediction algorithm twitter customer service conversations negation handle integral part end end solution negation handle propose several efficient heuristics well adopt recent state art third party machine learn base solutions empirically show model performance gain handle negation compare experiment show informal text tweet heuristic base approach effective
report present find benchmarking experiment information extraction historical handwritten marriage record esposalles iehhr icdar two thousand and seventeen robust read competition information extraction model semantic label sequence across two set label achieve sequentially jointly apply handwritten text recognition htr name entity recognition ner deploy pipeline approach first use state art htr use output input ner show give low resource setup simple structure record high performance htr ensure overall high performance explore various configurations conditional random field neural network benchmark ner give certain noisy input best model ten fold cross validation well blind test data use n gram feature bidirectional long short term memory
paper present portable phenotyping system capable integrate rule base statistical machine learn base approach system utilize umls extract clinically relevant feature unstructured text facilitate portability across different institutions data systems incorporate ohdsi omop common data model cdm standardize necessary data elements system also store key components rule base systems eg regular expression match format omop cdm thus enable reuse adaptation extension many exist rule base clinical nlp systems experiment system corpus i2b2 obesity challenge pilot study system facilitate portable phenotyping obesity fifteen comorbidities base unstructured patient discharge summaries achieve performance often rank among top ten challenge participants standardization enable consistent application numerous rule base machine learn base classification techniques downstream
article quantitatively analyze term fake news shape news media recent years study perception conceptualization term traditional media use eight years data collect news outlets base twenty countries result corroborate previous indications high increase usage expression fake news also show contextual change around expression unite state presidential election two thousand and sixteen among result find change relate vocabulary mention entities surround topics contextual polarity around term fake news suggest expression undergo change perception conceptualization two thousand and sixteen outcomes expand understand usage term fake news help comprehend accurately characterize relevant social phenomenon link misinformation manipulation
embed model typically associate word single real value vector represent different properties evaluation methods therefore need analyze accuracy completeness properties embeddings require fine grain analysis embed subspaces multi label classification appropriate way propose new evaluation method word embeddings base multi label classification give word embed task use fine grain name type give large corpus find type name refer base name embed give scale entities knowledge base build datasets task complementary current embed evaluation datasets large contain fine grain class allow direct evaluation embeddings without confound factor like sentence context
predict mental health smartphone social media data longitudinal basis recently attract great interest promise result report across many study approach potential revolutionise mental health assessment development evaluation follow real world deployment set work take closer look state art approach use different mental health datasets indicators different feature source multiple simulations order assess ability generalise demonstrate pragmatic evaluation framework none approach deliver even approach report performances fact show current state art approach barely outperform nai baselines real world set pose serious question deployment ability also contribution derive feature mental health assessment task make better use data future
multilingual nature internet increase complications cybersecurity community ongoing efforts strategically mine threat intelligence osint data web osint source social media blog dark web vulnerability market exist diverse languages hinder security analysts unable draw conclusions intelligence languages understand although third party translation engines grow stronger unsuited private security environments first sensitive intelligence permit input third party engines due privacy confidentiality policies addition third party engines produce generalize translations tend lack exclusive cybersecurity terminology paper address issue describe system enable threat intelligence understand across unfamiliar languages create neural network base system take cybersecurity data different language output respective english translation english translation understand analyst also serve input ai base cyber defense system take mitigative action proof concept create pipeline take russian threats generate correspond english rdf vectorized representations network optimize translations specifically cybersecurity data
categorical compositional semantics natural language one study functors category grammatical derivations lambek pregroup semantic category real vector space compositionally build game theoretic semantics sentence take semantic category category whose morphisms open game require modifications grammar category compensate failure open game form compact close category illustrate theory use simple examples wittgenstein language game
sentiment analysis large scale social media data important bridge gap social media content real world activities include political election prediction individual public emotional status monitor analysis although textual sentiment analysis well study base platforms twitter instagram analysis role extensive emoji use sentiment analysis remain light paper propose novel scheme twitter sentiment analysis extra attention emojis first learn bi sense emoji embeddings positive negative sentimental tweet individually train sentiment classifier attend bi sense emoji embeddings attention base long short term memory network lstm experiment show bi sense embed effective extract sentiment aware embeddings emojis outperform state art model also visualize attentions show bi sense emoji embed provide better guidance attention mechanism obtain robust understand semantics sentiments
machine comprehension question answer find answer question give passage involve high level reason process understand track relevant content across various semantic units word phrase sentence document paper propose novel question aware sentence gate network directly incorporate sentence level information word level encode process end model first learn question aware sentence representations dynamically combine word level representations result semantically meaningful word representations qa task experimental result demonstrate approach consistently improve accuracy exist baseline approach various qa datasets bear wide applicability neural network base qa model
scoutbot dialogue interface physical simulate robots support collaborative exploration environments demonstration allow users issue unconstrained speak language command scoutbot scoutbot prompt clarification user instruction need additional input train human robot dialogue collect wizard oz experiment robot responses initiate human wizard previous interactions demonstration show simulate grind robot clearpath jackal simulate environment support ros robot operate system
neural model combine representation learn reason end end trainable manner receive increase interest however use severely limit computational complexity render unusable real world datasets focus neural theorem prover ntp model propose rocktaschel riedel two thousand and seventeen continuous relaxation prolog backward chain algorithm unification term replace similarity embed representations answer give query model need consider possible proof paths aggregate result quickly become infeasible even small knowledge base kbs observe accurately approximate inference process model consider proof paths associate highest proof score enable inference learn previously impracticable kbs
free form open end visual question answer systems solve problem provide accurate natural language answer question pertain image current vqa systems evaluate pose question relevant input image hence provide nonsensical answer pose irrelevant question image paper solve problem identify relevance pose question image address problem two sub problems first identify question visual question visual determine relevant image second problem generate large dataset exist visual question answer datasets order enable train complex architectures model relevance visual question image also compare result long short term memory recurrent neural network base model logistic regression xgboost multi layer perceptron base approach problem
consider multilingual bottleneck feature bnfs nearly zero resource keyword spot form part unite nations effort use keyword spot support humanitarian relief program part africa languages severely resourced use one thousand, nine hundred and twenty isolate keywords forty type thirty-four minutes exemplars dynamic time warp dtw template match perform much larger body untranscribed speech dtw cost use target convolutional neural network cnn keyword spotter give much faster system direct dtw consider available data well resourced languages improve cnn dtw approach show multilingual bnfs train ten languages improve area roc curve cnn dtw system one hundred and nine absolute relative mfcc baseline combine low resource dtw base supervision information well resourced languages cnn dtw competitive option low resource keyword spot
present first efforts build automatic speech recognition system somali resourced language use one hundred and fifty-seven hrs annotate speech acoustic model train system part ongoing effort unite nations un implement keyword spot systems support humanitarian relief program part africa languages severely resourced evaluate several type acoustic model include recent neural architectures language model data augmentation use combination recurrent neural network rnn long short term memory neural network lstms well perturbation acoustic data also consider find type data augmentation beneficial performance best system use combination convolutional neural network cnns time delay neural network tdnns bi directional long short term memory blstms achieve word error rate five thousand, three hundred and seventy-five
image caption open research issue evolve progress deep neural network convolutional neural network cnns recurrent neural network rnns employ compute image feature generate natural language descriptions research previous work caption involve semantic description generate apply additional information rnns approach propose distinctive attribute extraction dae explicitly encourage significant mean generate accurate caption describe overall mean image unique situation specifically caption train image analyze term frequency inverse document frequency tf idf analyze semantic information train extract distinctive attribute infer caption propose scheme evaluate challenge data improve objective performance describe image detail
human annotation syntactic parse expensive large resources available fraction languages question ask whether one leverage abundant unlabeled texts improve syntactic parsers beyond use texts obtain generalisable lexical feature ie beyond word embeddings end propose novel latent variable generative model semi supervise syntactic dependency parse exact inference intractable introduce differentiable relaxation obtain approximate sample compute gradients respect parser parameters method differentiable perturb parse rely differentiable dynamic program stochastically perturb edge score demonstrate effectiveness approach experiment english french swedish
present setexpander corpus base system expand seed set term complete set term belong semantic class setexpander implement iterative end end workflow term set expansion enable users easily select seed set term expand view expand set validate expand validate set store thus simplify extraction domain specific fine grain semantic class setexpander use solve real life use case include integration automate recruitment system issue defect resolution system video demo setexpander available https drivegooglecom openid1e545bb87autsch36djnjhmq3hwfsd1rv image blur privacy reason
presence gender stereotype many aspects society well know phenomenon paper focus study quantify stereotype bias man bookers prize win fiction consider two hundred and seventy-five book shortlist man bookers prize one thousand, nine hundred and sixty-nine two thousand and seventeen gender bias analyze semantic model book descriptions goodreads reveal pervasiveness gender bias stereotype book different feature like occupation introductions action associate character book
sequence label task assign categorical label data sequence natural language process sequence label apply various fundamental problems part speech pos tag name entity recognition ner chunk study propose method add various linguistic feature neural sequence framework improve sequence label besides word level knowledge sense embeddings add provide semantic information additionally selective read character embeddings add capture contextual well morphological feature word sentence compare previous methods add linguistic feature allow us design concise model perform efficient train propose architecture achieve state art result benchmark datasets pos ner chunk moreover convergence rate model significantly better previous state art model
scientific discipline research find strong impact society reduce amount time take understand synthesize exploit research invaluable topic model effective technique summarize collection document find main theme among classify document similar mixture co occur word show ground topic model ontology extract glossary important domain phrase improve topics generate make easier understand apply evaluate method climate science domain result improve topics generate support faster research understand discovery social network among researchers automatic ontology generation
one billion monthly viewers millions users discuss share opinions comment youtube videos rich source data opinion mine sentiment analysis introduce youtube av 50k dataset freely available collections fifty thousand youtube comment metadata autonomous vehicle av relate videos describe creation process content data format discuss possible usages especially case study first self drive car fatality evaluate dataset show use dataset better understand public attitudes toward self drive cars public reactions accident future developments dataset also discuss
ensure satisfactory user experience dialog systems must able determine whether input sentence domain id domain ood assume id sentence available train data collect enough ood sentence unbiased way laborious time consume job paper propose novel neural sentence embed method represent sentence low dimensional continuous vector space emphasize aspects distinguish id case ood case first use large set unlabeled text pre train word representations use initialize neural sentence embed use domain category analysis auxiliary task train neural sentence embed ood sentence detection sentence representations learn use train autoencoder aim ood sentence detection evaluate method experimentally compare state art methods eight domain dialog system propose method achieve highest accuracy test
graph embed model produce embed vectors entities relations knowledge graph often without take literal properties account show initial idea base combination global graph structure additional information provide textual information properties initial experiment show approach might useful clearly outperform earlier approach evaluate machine learn task
exist work automate hate speech detection typically focus binary classification differentiate among small set categories paper propose novel method fine grain hate speech classification task focus differentiate among forty hate group thirteen different hate group categories first explore conditional variational autoencoder cvae discriminative model extend hierarchical architecture utilize additional hate category information accurate prediction experimentally show incorporate hate category information train significantly improve classification performance propose model outperform commonly use discriminative model
multilingual neural machine translation show share single translation model multiple languages achieve competitive performance sometimes even lead performance gain bilingually train model however improvements uniform often multilingual parameter share result decrease accuracy due translation model able accommodate different languages limit parameter space work examine parameter share techniques strike happy medium full share individual train specifically focus self attentional transformer model find full parameter share approach lead increase bleu score mainly target languages similar language family however even case target languages different families full parameter share lead noticeable drop bleu score propose methods partial share parameters lead substantial improvements translation accuracy
chinese pinyin input method engine ime convert pinyin character chinese character conveniently inputted computer common keyboard imes work rely core component pinyin character conversion p2c usually chinese imes simply predict list character sequence user choice accord user pinyin input turn however chinese inputting multi turn online procedure suppose exploit user experience promote paper thus first time introduce sequence sequence model gate attention mechanism core task imes propose neural p2c model learn encode previous input utterance extra context enable ime capable predict character sequence incomplete pinyin input model evaluate different benchmark datasets show great user experience improvement compare traditional model demonstrate first engineer practice build chinese aid ime
automatic image caption generation aim produce accurate description image natural language automatically however bangla fifth widely speak language world lag considerably research development domain besides many establish data set relate image annotation english resource exist bangla yet hence paper outline development chittron automatic image caption system bangla moreover address data set availability issue collection sixteen thousand bangladeshi contextual image accumulate manually annotate bangla data set use train model integrate pre train vgg16 image embed model stack lstm layer model train predict caption input image one word time result show model successfully able learn work language model generate caption image quite accurately many case result evaluate mainly qualitatively however bleu score also report expect better result obtain bigger vary data set
user intent detection play critical role question answer dialog systems previous work treat intent detection classification problem utterances label predefined intents however labor intensive time consume label users utterances intents diversely express novel intents continually involve instead study zero shoot intent detection problem aim detect emerge user intents label utterances currently available propose two capsule base architectures intent capsnet extract semantic feature utterances aggregate discriminate exist intents intentcapsnet zsl give intentcapsnet zero shoot learn ability discriminate emerge intents via knowledge transfer exist intents experiment two real world datasets show model better discriminate diversely express exist intents also able discriminate emerge intents label utterances available
cluster news across languages enable efficient media monitor aggregate article multilingual source coherent stories online set allow scalable process massive news stream end describe novel method cluster incoming stream multilingual document monolingual crosslingual story cluster unlike typical cluster approach consider small know number label tackle problem discover ever grow number cluster label online fashion use real news datasets multiple languages method simple implement computationally efficient produce state art result datasets german english spanish
train agents communicate one another give task base supervision attract considerable attention recently due grow interest develop model human agent interaction prior work topic focus simple environments train use policy gradient feasible despite non stationarity agents train paper present challenge environment test emergence communication raw pixels train use policy gradient fail propose new model train algorithm utilize structure learn representation space produce consistent speakers initial phase train stabilize learn empirically show algorithm substantially improve performance compare policy gradient also propose new alignment base metric measure context independence emerge communication find method increase context independence compare policy gradient competitive baselines
identify salience ie importance discourse units important task language understand events play important roles text document little research exist analyze saliency status paper empirically study event salience task propose two salience detection model base content similarities discourse relations first feature base salience model incorporate similarities among discourse units second neural model capture complex relations discourse units test new large scale event salience corpus methods significantly outperform strong frequency baseline neural model improve feature base one large margin analyse demonstrate neural model capture interest connections salience discourse unit relations eg script frame structure
open domain question answer qa evolve complex pipelined systems end end deep neural network specialize neural model develop extract answer either text alone knowledge base kbs alone paper look practical set namely qa combination kb entity link text appropriate incomplete kb available large text corpus build recent advance graph representation learn propose novel model graft net extract answer question specific subgraph contain text kb entities relations construct suite benchmark task problem vary difficulty question amount train data kb completeness show graft net competitive state art test use either kbs text alone vastly outperform exist methods combine set source code available https githubcom oceanskysun graftnet
paper propose new kernel base co occurrence measure apply sparse linguistic expressions eg sentence short learn time alternative pointwise mutual information pmi well derive pmi mutual information derive new measure hilbert schmidt independence criterion hsic thus call new measure pointwise hsic phsic phsic interpret smooth variant pmi allow various similarity metrics eg sentence embeddings plug kernels moreover phsic estimate simple fast linear size data matrix calculations regardless whether use linear nonlinear kernels empirically dialogue response selection task phsic learn thousands time faster rnn base pmi outperform pmi accuracy addition also demonstrate phsic beneficial criterion data selection task machine translation owe ability give high low score consistent inconsistent pair pair
understand reason cook recipes fruitful research direction towards enable machine interpret procedural text work introduce recipeqa dataset multimodal comprehension cook recipes comprise approximately 20k instructional recipes multiple modalities title descriptions align set image 36k automatically generate question answer pair design set comprehension reason task require joint understand image text capture temporal flow events make sense procedural knowledge preliminary result indicate recipeqa serve challenge test bed ideal benchmark evaluate machine comprehension systems data leaderboard available http hucvlgithubio recipeqa
propose new type representation learn method model word phrase sentence seamlessly method depend word segmentation human annotate resources eg word dictionaries yet effective noisy corpora write unsegmented languages chinese japanese main idea method ignore word boundaries completely ie segmentation free construct representations character n grams raw corpus embeddings compositional sub n grams although idea simple experiment various benchmarks real world datasets show efficacy proposal
word sense disambiguation wsd well research problem computational linguistics different research work approach problem different ways state art result achieve problem supervise model term accuracy often fall behind flexible knowledge base solutions use engineer feature well human annotators disambiguate every target word work focus bridge gap use neural sequence model incorporate well know attention mechanism main gist work combine multiple attentions different linguistic feature weight provide unify framework weight attention allow model easily disambiguate sense ambiguous word attend suitable portion sentence extensive experiment show multiple attention enable versatile encoder decoder model lead state art result
paper propose text2scene model generate various form compositional scene representations natural language descriptions unlike recent work method use generative adversarial network gans text2scene instead learn sequentially generate object attribute location size appearance etc every time step attend different part input text current status generate scene show minor modifications propose framework handle generation different form scene representations include cartoon like scenes object layouts correspond real image synthetic image method competitive compare state art gin base methods use automatic metrics superior base human judgments also advantage produce interpretable result
localize moments longer video via natural language query new challenge task intersection language video understand though moment localization natural language similar language vision task like natural language object retrieval image moment localization offer interest opportunity model temporal dependencies reason text propose new model explicitly reason different temporal segment video show temporal context important localize phrase include temporal language benchmark whether model recent video localization model effectively reason temporal language collect novel temporal reason video language tempo dataset dataset consist two part dataset real videos template sentence tempo template language allow control study temporal language human language dataset consist temporal sentence annotate humans tempo human language
paper present system use implicit wassa two thousand and eighteen implicit emotion share task task predict emotion tweet explicit mention emotion term remove idea come model ability implicitly identify emotion express give context word use gate recurrent neural network gru capsule network base model task pre train word embeddings utilize incorporate contextual knowledge word model gru layer learn latent representations use input word embeddings subsequent capsule network layer learn high level feature hide representation propose model manage achieve macro f1 score six hundred and ninety-two
discourse segmentation aim segment elementary discourse units edus fundamental task discourse analysis chinese previous research identify edus discriminate function punctuations paper argue chinese edus may end punctuation position follow definition edu rst dt definition conduct chinese discourse segmentation help english label datausing discourse commonality english chinese design adversarial neural network framework extract common language independent feature language specific feature useful discourse segmentation small scale chinese label data available experiment discourse segmentation demonstrate model leverage common feature bilingual data learn efficient chinese specific feature small amount chinese label data outperform baseline model
medical image analysis practitioners embrace big data methodologies create need large annotate datasets source big data typically large image collections clinical report record image many case however build algorithms aim segmentation detection disease require train dataset mark areas interest image match describe anomalies process annotation expensive need involvement clinicians work propose two separate deep neural network architectures automatic mark region interest roi image best represent find location give textual report set keywords one architecture consist lstm cnn components train end end image match text mark rois image output layer estimate coordinate vertices polygonal region second architecture use network pre train large dataset image type learn feature representations find interest show variety find chest x ray image propose architectures learn estimate roi validate clinical annotations clear advantage obtain architecture pre train image network centroids rois mark network average distance equivalent fifty-one image width centroids grind truth rois
explore several new model document relevance rank build upon deep relevance match model drmm guo et al two thousand and sixteen unlike drmm use context insensitive encode term query document term interactions inject rich context sensitive encode throughout model inspire pacrr hui et al two thousand and seventeen convolutional n gram match feature extend several ways include multiple view query document input test model datasets bioasq question answer challenge tsatsaronis et al two thousand and fifteen trec robust two thousand and four voorhees two thousand and five show outperform bm25 base baselines drmm pacrr
aim automatically generate natural language descriptions input structure knowledge base kb build generation framework base pointer network copy facts input kb add two attention mechanisms slot aware attention capture association slot type correspond slot value ii new emphtable position self attention capture inter dependencies among relate slot evaluation besides standard metrics include bleu meteor rouge propose kb reconstruction base metric extract kb generation output compare input kb also create new data set include one hundred and six thousand, two hundred and sixteen pair structure kbs correspond natural language descriptions two distinct entity type experiment show approach significantly outperform state art methods reconstruct kb achieve six hundred and eighty-eight seven hundred and twenty-six f score
diversity long study topic information retrieval usually refer requirement retrieve result non repetitive cover different aspects conversational set additional dimension diversity matter engage response generation system able output responses diverse interest sequence sequence seq2seq model show effective response generation however dialogue responses generate seq2seq model tend low diversity paper review know source exist approach low diversity problem also identify source low diversity little study far namely model confidence sketch several directions tackle model confidence hence low diversity problem include confidence penalties label smooth
work focus build language model lms code switch text propose two techniques significantly improve lms one novel recurrent neural network unit dual components focus language code switch text separately two pretraining lm use synthetic text generative model estimate use train data demonstrate effectiveness propose techniques report perplexities mandarin english task derive significant reductions perplexity
multi hop read comprehension focus one type factoid question system need properly integrate multiple piece evidence correctly answer question previous work approximate global evidence local coreference information encode coreference chain dag style gru layer within gate attention reader however coreference limit provide information rich inference introduce new method better connect global evidence form complex graph compare dags perform evidence integration graph investigate two recent graph neural network namely graph convolutional network gcn graph recurrent network grn experiment two standard datasets show richer global information lead better answer method perform better publish result datasets
present two categories model agnostic adversarial strategies reveal weaknesses several generative task orient dialogue model change strategies evaluate sensitivity small semantics preserve edit well change strategies test model stable subtle yet semantics change modifications next perform adversarial train strategy employ max margin approach negative generative examples make target dialogue model robust adversarial input also help perform significantly better original input moreover train strategies combine achieve improvements achieve new state art performance original task also verify via human evaluation addition adversarial train also address robustness task model level feed subword units input output show result model equally competitive require one four original vocabulary size robust one adversarial strategies original model vulnerable even without adversarial train
despite continuously improve performance contemporary image caption model prone hallucinate object actually scene one problem standard metrics measure similarity grind truth caption may fully capture image relevance work propose new image relevance metric evaluate current model veridical visual label assess rate object hallucination analyze caption model architectures learn objectives contribute object hallucination explore hallucination likely due image misclassification language priors assess well current sentence metrics capture object hallucination investigate question standard image caption benchmark mscoco use diverse set model analysis yield several interest find include model score best standard sentence metrics always lower hallucination model hallucinate tend make errors drive language priors
recently grow concern machine bias train statistical model grow reflect controversial societal asymmetries gender racial bias significant number ai tool recently suggest harmfully bias towards minority report racist criminal behavior predictors iphone x fail differentiate two asian people google photos mistakenly classify black people gorillas although systematic study bias difficult believe automate translation tool exploit gender neutral languages yield window phenomenon gender bias ai paper start comprehensive list job position yous bureau labor statistics bls use build sentence constructions like engineer twelve different gender neutral languages hungarian chinese yoruba several others translate sentence english use google translate api collect statistics frequency female male gender neutral pronouns translate output show gt exhibit strong tendency towards male default particular field link unbalance gender distribution stem job run statistics bls data frequency female participation job position show gt fail reproduce real world distribution female workers provide experimental evidence even one expect principle five thousand and fifty pronominal gender distribution gt yield male default much frequently would expect demographic data alone hopeful work ignite debate need augment current statistical translation tool debiasing techniques already find scientific literature
present data resource useful research purpose language ground task context geographical refer expression generation resource compose two data set encompass twenty-five different geographical descriptors set associate graphical representations draw polygons map two group human subject teenage students expert meteorologists
novel logographic subword model propose reinterpret logograms abstract subwords neural machine translation approach drastically reduce size artificial neural network maintain comparable bleu score attain baseline rnn cnn seq2seq model smaller model size also lead shorter train inference time experiment demonstrate task english chinese chinese english translation reduction aspects eleven high seventy-seven compare previous subword model abstract subwords apply various logographic languages consider logographic languages ancient low resource languages advantage desirable archaeological computational linguistic applications resource limit offline hand hold demotic english translator
relational database management systems rdbmss powerful able optimize answer query relational database natural language interface nli database hand tailor support specific database work introduce general purpose transfer learnable nli goal learn one model use nli relational database adopt data management principle separate data schema additional support idiosyncrasy complexity natural languages specifically introduce automatic annotation mechanism separate schema data schema also cover knowledge natural language furthermore propose customize sequence model translate annotate natural language query sql statements show experiment approach outperform previous nli methods wikisql dataset model learn apply another benchmark dataset overnight without retrain
topic model evaluate base ability describe document well ie low perplexity produce topics carry coherent semantic mean topic model far perplexity direct optimization target however topic coherence owe challenge computation optimize evaluate train work neural variational inference framework propose methods incorporate topic coherence objective train process demonstrate coherence aware topic model exhibit similar level perplexity baseline model achieve substantially higher topic coherence
nature language inference nli task predictive task determine inference relationship pair natural language sentence increase popularity nli many state art predictive model propose impressive performances however several work notice statistical irregularities collect nli data set may result estimate performance model propose remedy paper investigate statistical irregularities refer confound factor nli data set belief nli label preserve swap operations propose simple yet effective way swap two text fragment evaluate nli predictive model naturally mitigate observe problems continue train predictive model swap manner propose use deviation model evaluation performances different percentages train text fragment swap describe robustness predictive model evaluation metrics lead interest understand recent publish nli methods finally also apply swap operation nli model see effectiveness straightforward method mitigate confound factor problems train generic sentence embeddings nlp transfer task
recent neural model data text generation mostly base data drive end end train encoder decoder network even though generate texts mostly fluent informative often generate descriptions consistent input structure data critical issue especially domains require inference calculations raw data paper attempt improve fidelity neural data text generation utilize pre execute symbolic operations propose framework call operation guide attention base sequence sequence network opatt specifically design gate mechanism well quantization module operation result utilize information pre execute operations experiment two sport datasets show propose method clearly improve fidelity generate texts input structure data
improve train efficiency hierarchical recurrent model without compromise performance propose strategy name lower simpler simplify baseline model make lower layer simpler upper layer carry strategy simplify two typical hierarchical recurrent model namely hierarchical recurrent encoder decoder hred r net whose basic build block gru specifically propose scalar gate unit sgu simplify variant gru use replace grus middle layer hred r net besides also use fix size ordinally forget encode fofe efficient encode method without trainable parameter replace grus bottom layer hred r net experimental result show simplify hred simplify r net contain significantly less trainable parameters consume significantly less train time achieve slightly better performance baseline model
ai systems ability explain reason critical utility trustworthiness deep neural network enable significant progress many challenge problems visual question answer vqa however opaque black box limit explanatory capability paper present novel approach develop high perform vqa system elucidate answer integrate textual visual explanations faithfully reflect important aspects underlie reason capture style comprehensible human explanations extensive experimental evaluation demonstrate advantage approach compare compete methods automatic evaluation metrics human evaluation metrics
news article capture variety topics society reflect socioeconomic activities happen physical world also culture human interest public concern exist perceptions people cities frequently mention news article two cities may co occur article co occurrence often suggest certain relatedness mention cities relatedness may different topics depend content news article consider relatedness different topics semantic relatedness read news article one grasp general semantic relatedness cities yet give hundreds thousands news article difficult impossible anyone manually read paper propose computational framework read large number news article extract semantic relatedness cities framework base natural language process model employ machine learn process identify main topics news article describe overall structure framework individual modules apply experimental dataset five hundred thousand news article cover top one hundred yous cities span ten year period perform exploratory visualization extract semantic relatedness different topics multiple years also analyze impact geographic distance semantic relatedness find vary distance decay effect propose framework use support large scale content analysis city network research
although neural machine translation achieve promise result suffer slow translation speed direct consequence trade make translation quality speed thus performance come full play apply cube prune popular technique speed dynamic program neural machine translation speed translation construct equivalence class similar target hide state combine lead less rnn expansion operations target side less mathrmsoftmax operations large target vocabulary experiment show even better translation quality method translate faster compare naive beam search 33times gpus 35times cpus
research link prediction knowledge graph mainly focus static multi relational data work consider temporal knowledge graph relations entities may hold time interval specific point time line previous work static knowledge graph propose address problem learn latent entity relation type representations incorporate temporal information utilize recurrent neural network learn time aware representations relation type use conjunction exist latent factorization methods propose approach show robust common challenge real world kgs sparsity heterogeneity temporal expressions experiment show benefit approach four temporal kgs data set available permissive bsd three license one
propose ground dialogue state encoder address foundational issue integrate visual ground dialogue system components test bed focus guesswhat game two player game goal identify object complex visual scene ask sequence yes question visually ground encoder leverage synergies guess ask question train jointly use multi task learn enrich model via cooperative learn regime show introduction joint architecture cooperative learn lead accuracy improvements baseline system compare approach alternative system extend baseline reinforcement learn depth analysis show linguistic skills two model differ dramatically despite approach comparable performance level point importance analyze linguistic output compete systems beyond numeric comparison solely base task success
bridge gap machine read comprehension mrc model human be mainly reflect hunger data robustness noise paper explore integrate neural network mrc model general knowledge human be one hand propose data enrichment method use wordnet extract inter word semantic connections general knowledge give passage question pair hand propose end end mrc model name knowledge aid reader kar explicitly use extract general knowledge assist attention mechanisms base data enrichment method kar comparable performance state art mrc model significantly robust noise subset twenty eighty train examples available kar outperform state art mrc model large margin still reasonably robust noise
paper introduce vsts new dataset measure textual similarity sentence use multimodal information dataset comprise image along respectively textual caption describe dataset quantitatively qualitatively claim valid gold standard measure automatic multimodal textual similarity systems also describe initial experiment combine multimodal information
approach problem generalize pre train word embeddings beyond fix size vocabularies without use additional contextual information propose subword level word vector generation model view word bag character n grams model simple fast train provide good vectors rare unseen word experiment show model achieve state art performances english word similarity task joint prediction part speech tag morphosyntactic attribute twenty-three languages suggest model ability capture relationship word textual representations embeddings
word embeddings widely adopt across several nlp applications exist word embed methods utilize sequential context word learn embed attempt utilize syntactic context word methods result explosion vocabulary size paper overcome problem propose syngcn flexible graph convolution base method learn word embeddings syngcn utilize dependency context word without increase vocabulary size word embeddings learn syngcn outperform exist methods various intrinsic extrinsic task provide advantage use elmo also propose semgcn effective framework incorporate diverse semantic knowledge enhance learn word representations make source code model available encourage reproducible research
exquisite concise literary form poetry gem human culture automatic poetry generation essential step towards computer creativity recent years several neural model design task however among line whole poem coherence mean topics still remain big challenge paper inspire theoretical concept cognitive psychology propose novel work memory model poetry generation different previous methods model explicitly maintain topics informative limit history neural memory generation process model read relevant part memory slot generate current line line generate write salient part previous line memory slot dynamic manipulation memory model keep coherent information flow learn express topic flexibly naturally experiment three different genres chinese poetry quatrain iambic chinoiserie lyric automatic human evaluation result show model outperform current state art methods
precious part human cultural heritage chinese poetry influence people generations automatic poetry composition challenge ai recent years significant progress make area benefit development neural network however coherence mean theme even artistic conception generate poem whole still remain big problem paper propose novel salient clue mechanism chinese poetry generation different previous work try exploit context information model select salient character automatically far generate line gradually form salient clue utilize guide successive poem generation process eliminate interruptions improve coherence besides model flexibly extend control generate poem different aspects example poetry style enhance coherence experimental result show model effective outperform three strong baselines
paper study novel task learn compose music natural language give lyric input propose melody composition model generate lyric conditional melody well exact alignment generate melody give lyric simultaneously specifically develop melody composition model base sequence sequence framework consist two neural encoders encode current lyric context melody respectively hierarchical decoder jointly produce musical note correspond alignment experimental result lyric melody pair eighteen thousand, four hundred and fifty-one pop songs demonstrate effectiveness propose methods addition apply sing voice synthesizer software synthesize sing lyric melodies human evaluation result indicate generate melodies melodious tuneful compare baseline method
propose novel framework controllable natural language transformation realize requirement parallel corpus practically unsustainable controllable generation task unsupervised train scheme introduce crux framework deep neural encoder decoder reinforce text transformation knowledge auxiliary modules call scorers scorers base shelf language process tool decide learn scheme encoder decoder base action apply framework text transformation task formalize input text improve readability grade degree require formalization control user run time experiment public datasets demonstrate efficacy model towards transform give text formal style b introduce appropriate amount formalness output text pertain input control code datasets release academic use
good neural sequence sequence summarization model strong encoder distill memorize important information long input texts decoder generate salient summaries base encoder memory paper aim improve memorization capabilities encoder pointer generator model add additional close book decoder without attention pointer mechanisms decoder force encoder selective information encode memory state decoder rely extra information provide attention possibly copy modules hence improve entire model cnn daily mail dataset two decoder model outperform baseline significantly term rouge meteor metrics cross entropy reinforce setups human evaluation moreover model also achieve higher score test duc two thousand and two generalizability setup present memory ability test two saliency metrics well several sanity check ablations base fix encoder gradient flow cut model capacity prove encoder two decoder model fact learn stronger memory representations baseline encoder
transfer representations large supervise task downstream task show promise result ai field computer vision natural language process nlp parallel recent progress machine translation mt enable one train multilingual neural mt nmt systems translate multiple languages also capable perform zero shoot translation however little attention pay leverage representations learn multilingual nmt system enable zero shoot multilinguality nlp task paper demonstrate simple framework multilingual encoder classifier cross lingual transfer learn reuse encoder multilingual nmt system stitch task specific classifier component propose model achieve significant improvements english setup three benchmark task amazon review sst snli system perform classification new language classification data see train show zero shoot classification possible remarkably competitive order understand underlie factor contribute find conduct series analyse effect share vocabulary train data type nmt classifier complexity encoder representation power model generalization zero shoot performance result provide strong evidence representations learn multilingual nmt systems widely applicable across languages task
commonsense knowledge paramount enable intelligent systems typically characterize implicit ambiguous hinder thereby automation acquisition address challenge paper present semantically enhance model enable reason resolve part commonsense ambiguity propose model enhance knowledge graph embed kge framework knowledge base completion experimental result show effectiveness new semantic model commonsense reason
recent rise metoo increase number personal stories sexual harassment sexual abuse share online order push forward fight harassment abuse present task automatically categorize analyze various form sexual harassment base stories share online forum safecity label grope ogle comment single label cnn rnn model achieve accuracy eight hundred and sixty-five multi label model achieve ham score eight hundred and twenty-five furthermore present analysis use lime first derivative saliency heatmaps activation cluster embed visualization interpret neural model predictions demonstrate extract feature help automatically fill incident report identify unsafe areas avoid unsafe practice pin creep
character level pattern widely use feature english name entity recognition ner systems however date direct investigation inherent differences name non name tokens text whether property hold across multiple languages paper analyze capabilities corpus agnostic character level language model clms binary task distinguish name tokens non name tokens demonstrate clms provide simple powerful model capture differences identify name entity tokens diverse set languages close performance full ner systems moreover add simple clm base feature significantly improve performance shelf ner system multiple languages
previous work approach sql text generation task use vanilla seq2seq model may fully capture inherent graph structure information sql query paper first introduce strategy represent sql query direct graph employ graph sequence model encode global structure information node embeddings model effectively learn correlation sql query pattern interpretation experimental result wikisql dataset stackoverflow dataset show model significantly outperform seq2seq tree2seq baselines achieve state art performance
use connectionist approach conversational agents progress rapidly due availability large corpora however current generative dialogue model often lack coherence content poor work propose architecture incorporate unstructured knowledge source enhance next utterance prediction chit chat type generative dialogue model focus sequence sequence seq2seq conversational agents train reddit news dataset consider incorporate external knowledge wikipedia summaries well nell knowledge base experiment show faster train time improve perplexity leverage external knowledge
many datasets nowadays contain link geographic locations natural language texts link geotags geotagged tweet geotagged wikipedia page location coordinate explicitly attach texts link also place mention news article travel blog historical archive texts implicitly connect mention place kind data refer geo text data availability large amount geo text data bring challenge opportunities one hand challenge automatically process kind data due unstructured texts complex spatial footprints place hand geo text data offer unique research opportunities rich information contain texts special link texts geography result geo text data facilitate various study especially data drive geospatial semantics paper discuss geo text data relate concepts focus data drive research paper systematically review large number study discover multiple type knowledge geo text data base literature review generalize workflow extract key challenge future work discuss
text classification important classical problem natural language process number study apply convolutional neural network convolution regular grid eg sequence classification however limit number study explore flexible graph convolutional neural network convolution non grid eg arbitrary graph task work propose use graph convolutional network text classification build single text graph corpus base word co occurrence document word relations learn text graph convolutional network text gcn corpus text gcn initialize one hot representation word document jointly learn embeddings word document supervise know class label document experimental result multiple benchmark datasets demonstrate vanilla text gcn without external word embeddings knowledge outperform state art methods text classification hand text gcn also learn predictive word document embeddings addition experimental result show improvement text gcn state art comparison methods become prominent lower percentage train data suggest robustness text gcn less train data text classification
open domain question answer qa important problem ai nlp emerge bellwether progress generalizability ai methods techniques much progress open domain qa systems realize advance information retrieval methods corpus construction paper focus recently introduce arc challenge dataset contain two thousand, five hundred and ninety multiple choice question author grade school science exams question select challenge current qa systems current state art performance slightly better random chance present system rewrite give question query use retrieve support text large corpus science relate text rewriter able incorporate background knowledge conceptnet tandem generic textual entailment system train scitail identify support retrieve result outperform several strong baselines end end qa task despite train identify essential term original source question use generalizable decision methodology retrieve evidence answer candidates select best answer combine query rewrite background knowledge textual entailment system able outperform several strong baselines arc dataset
sentiment analysis sa product review user product information prove useful current task handle user profile product information unify model may able learn salient feature users products effectively work propose dual user product memory network dupmn model learn user profile product review use separate memory network two representations use jointly sentiment prediction use separate model aim capture user profile product information effectively compare state art unify prediction model evaluations three benchmark datasets imdb yelp13 yelp14 show dual learn model give performance gain six twelve nine respectively improvements also deem significant measure p value
word embeddings show benefit ensambling several word embed source often carry use straightforward mathematical operations set word vectors recently self supervise learn use find lower dimensional representation similar size individual word embeddings within ensemble however methods use available manually label datasets often use solely purpose evaluation propose reconstruct ensemble word embeddings auxiliary task regularise main task task share learn meta embed layer carry intrinsic evaluation six word similarity datasets three analogy datasets extrinsic evaluation four downstream task intrinsic task evaluation supervision come various label word similarity datasets experimental result show performance improve word similarity datasets compare self supervise learn methods mean increase one thousand, one hundred and thirty-three spearman correlation specifically propose method show best performance four six word similarity datasets use cosine reconstruction loss brier word similarity loss moreover improvements also make perform word meta embed reconstruction sequence tag sentence meta embed sentence classification
responses generate neural conversational model tend lack informativeness diversity present adversarial information maximization aim adversarial learn strategy address two relate distinct problems foster response diversity leverage adversarial train allow distributional match synthetic real responses improve informativeness framework explicitly optimize variational lower bind pairwise mutual information query response empirical result automatic human evaluations demonstrate methods significantly boost informativeness diversity
read comprehension qa task see recent surge popularity yet work focus fact find extractive qa instead focus challenge multi hop generative task narrativeqa require model reason gather synthesize disjoint piece information within context generate answer type multi step reason also often require understand implicit relations humans resolve via external background commonsense knowledge first present strong generative baseline use multi attention mechanism perform multiple hop reason pointer generator decoder synthesize answer model perform substantially better previous generative model competitive current state art span prediction model next introduce novel system select ground multi hop relational commonsense information conceptnet via pointwise mutual information term frequency base score function finally effectively use extract commonsense information fill gap reason context hop use selectively gate attention mechanism boost model performance significantly also verify via human evaluation establish new state art task also show promise initial result generalizability background knowledge enhancements demonstrate improvement qangaroo wikihop another multi hop reason dataset
present aueb submissions bioasq six document snippet retrieval task part task 6b phase model use novel extensions deep learn architectures operate solely text query candidate document snippets systems score top near top batch challenge highlight effectiveness deep learn task
misinformation fake news one big challenge society research automate fact check propose methods base supervise learn approach consider external evidence apart label train instance recent approach counter deficit consider external source relate claim however methods require substantial feature model rich lexicons paper overcome limitations prior work end end model evidence aware credibility assessment arbitrary textual claim without human intervention present neural network model judiciously aggregate signal external evidence article language article trustworthiness source also derive informative feature generate user comprehensible explanations make neural network predictions transparent end user experiment four datasets ablation study show strength method
propose triad base neural network system generate affinity score entity mention coreference resolution system simultaneously accept three mention input take mutual dependency logical constraints three mention account thus make accurate predictions traditional pairwise approach depend system choices affinity score use cluster mention rank experiment show standard hierarchical cluster use score produce state art result gold mention english portion conll two thousand and twelve share task model rely many handcraft feature easy train use triads also easily extend polyads higher order knowledge first neural network system model mutual dependency two members mention level
automatic judgment prediction aim predict judicial result base case materials study several decades mainly lawyers judge consider novel prospective application artificial intelligence techniques legal field exist methods follow text classification framework fail model complex interactions among complementary case materials address issue formalize task legal read comprehension accord legal scenario follow work protocol human judge lrc predict final judgment result base three type information include fact description plaintiffs pleas law article moreover propose novel lrc model autojudge capture complex semantic interactions among facts pleas laws experiment construct real world civil case dataset lrc experimental result dataset demonstrate model achieve significant improvement state art model publish source cod datasets work urlgithubcom research
semantic frame parse crucial component speak language understand slu build speak dialog systems two main task intent detection slot fill although state art approach show good result require large annotate train data long train time paper aim alleviate drawbacks semantic frame parse utilize ubiquitous user information design novel coarse fine deep neural network model incorporate prior knowledge user information intermediately better quickly train semantic frame parser due lack benchmark dataset real user information synthesize simplest type user information location time atis benchmark data result show approach leverage simple user information outperform state art approach twenty-five intent detection thirty-one slot fill use standard train data use smaller train data performance improvement intent detection slot fill reach one hundred and thirty-five one hundred and twenty respectively also show approach achieve similar performance state art approach use less eighty annotate train data moreover train time achieve similar performance also reduce sixty
recent rapid increase generation clinical data rapid development computational science make us able extract new insights massive datasets healthcare industry oncological clinical note create rich databases document patients history potentially contain lot pattern could help better management disease however pattern lock within free text unstructured portion clinical document consequence limit health professionals extract useful information finally perform query answer qa process accurate way information extraction ie process require natural language process nlp techniques assign semantics pattern therefore paper analyze design annotators specific lung cancer concepts integrate apache unstructured information management architecture uima framework addition explain detail generation storage annotation outcomes
conversational agents gain popularity increase ubiquity smart devices however train agents data drive manner challenge due lack suitable corpora paper present novel method gather topical unstructured conversational data efficient way self dialogues crowd source alongside paper include corpus thirty-six million word across twenty-three topics argue utility corpus compare self dialogues standard two party conversations well data corpora
wikipedia strong norm write neutral point view npov article violate norm tag editors encourage make corrections impact tag system quantitatively measure npov tag help article converge desire style npov corrections encourage editors adopt style study question use corpus npov tag article set lexicons associate bias language interrupt time series analysis show article tag npov significant decrease bias language article measure several lexicons however individual editors npov corrections talk page discussions yield significant change usage word lexicons include wikipedia list word watch suggest npov tag discussion improve content less success enculturating editors site linguistic norms
propose multi task learn framework learn joint machine read comprehension mrc model apply wide range mrc task different domains inspire recent ideas data selection machine translation develop novel sample weight scheme assign sample specific weight loss empirical study show approach apply many exist mrc model combine contextual representations pre train language model elmo achieve new state art result set mrc benchmark datasets release code https githubcom xycforgithub multitask mrc
probabilistic context free grammars pcfg core probabilistic reason base parsers several years especially context nlp multi entity bayesian network mebn first order logic probabilistic reason methodology widely adopt use method uncertainty reason upper ontology like probabilistic ontology web language pr owl build use mebn take care probabilistic ontologies model capture uncertainties inherent domain semantic information paper attempt establish link probabilistic reason pcfg mebn propose formal description pcfg drive mebn lead usage pr owl model ontologies pcfg parsers furthermore paper outline approach resolve prepositional phrase pp attachment ambiguity use propose map pcfg mebn
background find biomedical name entities one essential task biomedical text mine recently deep learn base approach apply biomedical name entity recognition bioner show promise result however deep learn approach need abundant amount train data lack data hinder performance bioner datasets scarce resources dataset cover small subset entity type furthermore many bio entities polysemous one major obstacles name entity recognition result address lack data entity type misclassification problem propose collabonet utilize combination multiple ner model collabonet model train different dataset connect target model obtain information collaborator model reduce false positives every model expert target entity type take turn serve target collaborator model train time experimental result show collabonet use greatly reduce number false positives misclassified entities include polysemous word collabonet achieve state art performance term precision recall f1 score conclusions demonstrate benefit combine multiple model bioner model successfully reduce number misclassified entities improve performance leverage multiple datasets annotate different entity type give state art performance model believe collabonet improve accuracy downstream biomedical text mine applications bio entity relation extraction
paper design integrate algorithm evaluate sentiment chinese market firstly help web browser automation crawl lot news comment several influential financial websites automatically secondly use techniques natural language processingnlp chinese context include tokenization word2vec word embed semantic database wordnet compute senti score news comment construct sentimental factor build finance specific sentimental lexicon sentimental factor reflect sentiment financial market general sentiments happiness sadness etc thirdly also implement adjustment standard sentimental factor experimental performance show significant correlation standard sentimental factor chinese market adjust factor even informative stronger correlation chinese market therefore sentimental factor important reference make investment decisions especially chinese market crash two thousand and fifteen pearson correlation coefficient adjust sentimental factor sse five thousand, eight hundred and forty-four suggest model provide solid guidance especially special period market influence greatly public sentiment
problems intersection language vision like visual question answer recently gain lot attention field multi modal machine learn computer vision research move beyond traditional recognition task recent success visual question answer use deep neural network model use linguistic structure question dynamically instantiate network layouts process convert question network layout question simplify result loss information model paper enrich image information textual data use image caption external knowledge base generate coherent answer achieve five hundred and seventy-one overall accuracy test dev open end question visual question answer vqa ten real image dataset
paper propose name entity recognition ner system identify film title podcast audio take inspiration ner systems noisy text social media implement two stage approach robust computer transcription errors require significant computational expense accommodate new film title release evaluate diverse set podcast demonstrate twenty increase f1 score across three baseline approach combine fuzzy match linear model aware film specific metadata
propose new model speaker name movies leverage visual textual acoustic modalities unify optimization framework evaluate performance model introduce new dataset consist six episodes big bang theory tv show eighteen full movies cover different genres experiment show multimodal model significantly outperform several competitive baselines average weight f score metric demonstrate effectiveness framework design end end memory network model leverage speaker name model achieve state art result subtitle task movieqa two thousand and seventeen challenge
neural cache language model lms extend idea regular cache language model make cache probability dependent similarity current context context word cache make extensive comparison regular cache model neural cache model term perplexity wer rescoring first pass asr result furthermore propose two extensions neural cache model make use content value information weight word firstly combine cache probability lm probability information weight interpolation secondly selectively add content word cache obtain two hundred and ninety-nine three hundred and twenty-one validation test set relative improvement perplexity respect baseline lstm lm wikitext two dataset outperform previous work neural cache lms additionally observe significant wer reductions respect baseline model wsj asr task
present spider large scale complex cross domain semantic parse text sql dataset annotate eleven college students consist ten thousand, one hundred and eighty-one question five thousand, six hundred and ninety-three unique complex sql query two hundred databases multiple table cover one hundred and thirty-eight different domains define new complex cross domain semantic parse text sql task different complex sql query databases appear train test set way task require model generalize well new sql query new database schemas spider distinct previous semantic parse task use single database exact program train set test set experiment various state art model best model achieve one hundred and twenty-four exact match accuracy database split set show spider present strong challenge future research dataset task publicly available https yale lilygithubio spider
automatically predict level non native english speakers give write essay interest machine learn problem work present system balikasg achieve state art performance cap two thousand and eighteen data science challenge among fourteen systems detail feature extraction feature engineer model selection step evaluate decisions impact system performance paper conclude remark future work
build reference task open information extraction five document tentatively resolve number issue arise include inference granularity seek better pinpoint requirements task produce annotation guidelines specify correct extract turn use reference score exist open ie systems address non trivial problem evaluate extractions produce systems reference tuples share evaluation script among seven compare extractors find minie system perform best
order satisfy consumers increase personalize service demand intelligent service arise user service intention recognition important challenge intelligent service system provide precise service difficult intelligent system understand semantics user demand lead poor recognition effect noise user requirement descriptions therefore hybrid neural network classification model base bilstm cnn propose recognize users service intentions model fuse temporal semantics spatial semantics user descriptions experimental result show model achieve better effect compare model reach ninety-four f1 score
faceted classifications define dimension type entities include effect facets provide ontological commitment compare faceted thesaurus art architecture thesaurus aat ontologies derive basic formal ontology bfo2 upper formal ontology widely use describe entities biomedicine consider aat bfo2 base ontologies could coordinate integrate human activity infrastructure foundry haif extend aat enable coordination integration describe wider range relationships among term could introduce use extensions explore richer model topics aat deal technology finally consider ontology base frame semantic role frame integrate make rich semantic statements change world
question answer qa knowledge graph gain significant momentum past five years due increase availability large knowledge graph rise importance question answer user interaction dbpedia prominently use knowledge graph set approach currently use pipeline process step connect sequence components article analyse micro evaluate behaviour twenty-nine available qa components dbpedia knowledge graph release research community since two thousand and ten result provide perspective collective failure case suggest characteristics qa components prevent perform better provide future challenge research directions field
despite recent progress computer vision fine grain interpretation satellite image remain challenge lack label train data overcome limitation propose use wikipedia previously untapped source rich georeferenced textual information global coverage construct novel large scale multi modal dataset pair geo reference wikipedia article satellite imagery correspond locations prove efficacy dataset focus african continent train deep network classify image base label extract article fine tune model human annotate dataset demonstrate weak form supervision drastically reduce quantity human annotate label time require downstream task
paper introduce sentence vector encode framework suitable advance natural language process latent representation show encode sentence common semantic information similar vector representations vector representation extract encoder decoder model train sentence paraphrase pair demonstrate application sentence representations two different task sentence paraphrase paragraph summarization make attractive commonly use recurrent frameworks process text experimental result help gain insight vector representations suitable advance language embed
neural language model nlms exist accuracy efficiency tradeoff space better perplexity typically come cost greater computation complexity software keyboard application mobile devices translate higher power consumption shorter battery life paper represent first attempt knowledge explore accuracy efficiency tradeoffs nlms build quasi recurrent neural network qrnns apply prune techniques provide knob select different operate point addition propose simple technique recover perplexity use negligible amount memory empirical evaluations consider perplexity well energy consumption raspberry pi demonstrate methods provide best perplexity power consumption operate point one operate point one techniques able provide energy save forty state art seventeen relative increase perplexity
data intensive science communities progressively adopt fair practice enhance visibility scientific breakthroughs enable reuse core movement research object contain describe scientific information resources way compliant fair principles sustain development key infrastructure tool paper provide account challenge experience solutions involve adoption fair around research object several earth science discipline journey work comprehensive outcomes include extend research object model adapt need earth scientists provision digital object identifiers doi enable persistent identification give due credit author generation content base semantically rich research object metadata natural language process enhance visibility reuse recommendation systems third party search engines various type checklists provide compact representation research object quality key enabler scientific reuse result integrate rohub platform provide research object management functionality wealth applications interfaces across different scientific communities monitor quantify community uptake research object define indicators obtain measure via rohub also discuss herein
cross situational word learn wherein learner combine information possible mean word across multiple exposures previously show powerful strategy acquire large lexicon short time however success may derive idealizations make model word learn process particular earlier model assume learner could perfectly recall previous instance word use inferences draw mean work relax assumption determine performance model cross situational learner forget word mean associations time main find possible learner acquire human scale lexicon adulthood word exposure memory decay rat consistent empirical research childhood word learn long degree referential uncertainty high learner employ mutual exclusivity constraint find therefore suggest successful word learn necessarily demand either highly accurate long term track word mean statistics hypothesis test strategies
inspire success self attention mechanism transformer architecture sequence transduction image generation applications propose novel self attention base architectures improve performance adversarial latent code base scheme text generation adversarial latent code base text generation recently gain lot attention due promise result paper take step fortify architectures use setups specifically aae arae benchmark two latent code base methods aae arae design base adversarial setups experiment google sentence compression dataset utilize compare method methods use various objective subjective measure experiment demonstrate propose self attention base model outperform state art adversarial code base text generation
winograd schema ws challenge propose al ternative turing test become new standard evaluate progress natural language understand nlu paper however concern challenge might address instead aim threefold first formally situate ws challenge data information knowledge continuum suggest continuum good ws reside ii show ws special case general phenomenon language understand namely miss text phenomenon henceforth mtp particular argue usually call think process language understand involve discover significant amount miss text text explicitly state often implicitly assume share background knowledge iii conclude brief discussion mtp inconsistent data drive machine learn approach language understand
propose neural entity reasoner ne reasoner framework introduce global consistency recognize entities neural reasoner name entity recognition ner task give input sentence ne reasoner layer infer multiple entities increase global consistency output label transfer entities input next layer ne reasoner inherit develop feature neural reasoner one symbolic memory allow exchange entities layer two specific interaction pool mechanism allow connect local word multiple global entities three deep architecture allow bootstrap recognize entity set coarse fine like human be ne reasoner able accommodate ambiguous word name entities rarely never meet despite symbolic information model introduce ne reasoner still train effectively end end manner via parameter share strategy ne reasoner outperform conventional ner model case english chinese ner datasets example achieve state art conll two thousand and three english ner dataset
recently open domain question answer qa combine machine comprehension model find answer large knowledge source open domain qa require retrieve relevant document text corpora answer question performance largely depend performance document retrievers however since traditional information retrieval systems effective obtain document high probability contain answer lower performance qa systems simply extract document increase number irrelevant document also degrade performance qa systems paper introduce paragraph ranker rank paragraph retrieve document higher answer recall less noise show rank paragraph aggregate answer use paragraph ranker improve performance open domain qa pipeline four open domain qa datasets seventy-eight average
talaia platform monitor social media digital press configurable crawler gather content respect user define domains topics crawl data process mean elixa sentiment analysis system django power interface provide data visualization user base analysis data paper present architecture system describe detail different components prove validity approach two real use case account one cultural domain one political domain evaluation sentiment analysis task scenarios also provide show capacity domain adaptation
automatic spell grammatical correction systems one widely use tool within natural language applications thesis assume task error correction type monolingual machine translation source sentence potentially erroneous target sentence correct form input main focus project build neural network model task error correction particular investigate sequence sequence attention base model recently show higher performance state art many language process problems demonstrate neural machine translation model successfully apply task error correction experiment research perform arabic corpus methods thesis easily apply language
propose end end deep learn model translate free form natural language instructions high level plan behavioral robot navigation use attention model connect information user instructions topological representation environment evaluate model performance new dataset contain ten thousand and fifty pair navigation instructions model significantly outperform baseline approach furthermore result suggest possible leverage environment map relevant knowledge base facilitate translation free form navigational instruction
sequence sequence seq2seq model become overwhelmingly popular build end end trainable dialogue systems though highly efficient learn backbone human computer communications suffer problem strongly favor short generic responses paper argue good response smoothly connect precede dialogue history follow conversations strengthen connection mutual information maximization sidestep non differentiability discrete natural language tokens introduce auxiliary continuous code space map code space learnable prior distribution generation purpose experiment two dialogue datasets validate effectiveness model generate responses closely relate dialogue context lead interactive conversations
despite many recent advance design dialogue systems true bottleneck remain acquisition data require train components unlike many language process applications dialogue systems require interactions users therefore complex develop pre record data build previous work line learn pursue convenient way address issue data collection annotation use learn algorithms perform single process main difficulties bootstrap initial basic system control level additional cost user side consider well perform solutions use directly shelf speech recognition synthesis study focus learn speak language understand dialogue management modules several variants joint learn investigate test user trials confirm overall line learn obtain hundred train dialogues overstep expert base system
causal understand essential many kinds decision make causal inference observational data typically apply structure low dimensional datasets text classifiers produce low dimensional output use causal inference previously study facilitate causal analyse base language data consider role text classifiers play causal inference establish model mechanisms causality literature miss data measurement error demonstrate conduct causal analyse use text classifiers simulate yelp data discuss opportunities challenge future work use text data causal inference
grow abundance unlabeled data real world task researchers rely predictions give black box computational model however often neglect fact model may score high accuracy wrong reason paper present practical impact analysis enable model transparency various presentation form purpose develop environment empower non computer scientists become practice data scientists research field demonstrate gradually increase understand journalism historians real world use case study automatic genre classification newspaper article study first step towards trust usage machine learn pipelines responsible way
user comment become essential part online journalism however newsrooms often overwhelm vast number diverse comment manual analysis barely feasible identify meta comment address mention newsrooms individual journalists moderators may call reactions particularly critical paper present automate approach identify classify meta comment compare comment classification base manually extract feature end end learn approach develop optimize evaluate multiple classifiers comment dataset large german online newsroom spiegel online one million post corpus der standard austrian newspaper optimize classification approach achieve encourage f05 value seventy-six ninety-one report significant classification feature result qualitative analysis discuss work contribute make participation online journalism constructive
modern neural machine translation nmt systems rely presegmented input segmentation granularity importantly determine input output sequence lengths hence model depth source target vocabularies turn determine model size computational cost softmax normalization handle vocabulary word however current practice use static heuristic base segmentations fix nmt train beg question whether choose segmentation optimal translation task overcome suboptimal segmentation choices present algorithm dynamic segmentation base adaptative computation time algorithm grave two thousand and sixteen trainable end end drive nmt objective evaluation four translation task find give freedom navigate different segmentation level model prefer operate almost character level provide support purely character level nmt model novel angle
present tranx transition base neural semantic parser map natural language nl utterances formal mean representations mrs tranx use transition system base abstract syntax description language target mr give two major advantage one highly accurate use information syntax target mr constrain output space model information flow two highly generalizable easily apply new type mr write new abstract syntax description correspond allowable structure mr experiment four different semantic parse code generation task show system generalizable extensible effective register strong result compare exist neural semantic parsers
fundamental big five personality traits eg extraversion facets eg activity know correlate broad range linguistic feature accordingly recognition personality traits text well know natural language process task label text data facets information however may require use lengthy personality inventory perhaps reason exist computational model kind usually limit recognition fundamental traits base observations paper investigate issue personality facets recognition text label information available shorter personality inventory provide low cost model recognition certain personality facets present reference result study field
analyse call computable laws ie laws enforce automatic procedures laws logically perfect unambiguous sometimes use regulation road transport illustrate issue show fragment regulation would look like rewrite image logic propose desiderata fulfil computable laws provide critical platform assess exist laws guideline compose future ones
paper introduce first geolocation inference approach reddit social media platform user pseudonymity thus far make supervise demographic inference difficult implement validate particular design text base heuristic schema generate grind truth location label reddit users absence explicitly geotagged data evaluate accuracy label procedure train test several geolocation inference model across reddit data set three benchmark twitter geolocation data set ultimately show geolocation model train apply domain substantially outperform model attempt transfer train data across domains even reddit platform specific interest group metadata use improve inferences
previous traditional approach unsupervised chinese word segmentation cws roughly classify discriminative generative model former use carefully design goodness measure candidate segmentation latter focus find optimal segmentation highest generative probability however exist trivial way extend discriminative model neural version use neural language model generative ones non trivial paper propose segmental language model slms cws approach explicitly focus segmental nature chinese well preserve several properties language model slms context encoder encode previous context segment decoder generate segment incrementally far know first propose neural model unsupervised cws achieve competitive performance state art statistical model four different datasets sighan two thousand and five bakeoff
explore active learn al improve accuracy new domains natural language understand nlu system propose algorithm call majority crf use ensemble classification model guide selection relevant utterances well sequence label model help prioritize informative examples experiment three domains show majority crf achieve sixty-six nine relative error rate reduction compare random sample annotation budget statistically significant improvements compare al approach additionally case study human loop al six new domains show forty-six nine improvement exist nlu system
paper develop low character feature embed call radical embed apply lstm model sentence segmentation pre modern chinese texts datasets include one hundred and fifty classical chinese book three different dynasties contain different literary style lstm crf model state art method sequence label problem new model add component radical embed lead improve performances experimental result base aforementioned chinese book demonstrate better accuracy earlier methods sentence segmentation especial tang epitaph texts
modern nlp applications enjoy great boost utilize neural network model deep neural model however applicable human languages due lack annotate train data various nlp task cross lingual transfer learn cltl viable method build nlp model low resource target language leverage label data source languages work focus multilingual transfer set train data multiple source languages leverage boost target language performance unlike exist methods rely language invariant feature cltl approach coherently utilize language invariant language specific feature instance level model leverage adversarial network learn language invariant feature mixture experts model dynamically exploit similarity target language individual source language enable model learn effectively share various languages multilingual setup moreover couple unsupervised multilingual embeddings model operate zero resource set neither target language train data cross lingual resources available model achieve significant performance gain prior art show extensive set experiment multiple text classification sequence tag task include large scale industry dataset
several lexica sentiment analysis develop make available nlp community come word polarity annotations eg positive negative attempt build lexica finer grain emotion analysis eg happiness sadness recently attract significant attention lexica often exploit build block process develop learn model emotion recognition need use baselines compare performance model work contribute two new resources community extension exist widely use emotion lexicon english b novel version lexicon target italian furthermore show simple techniques use supervise unsupervised experimental settings boost performances datasets task vary degree domain specificity
rapid development knowledge basequestion answer base knowledge base hot research issue paper focus answer singlerelation factoid question base knowledge base build question answer system study effect context information fact selection entity notable typeoutdegree experimental result show context information improve result simple question answer
work propose analysis presence gender bias associate professions portuguese word embeddings objective work study gender implications relate stereotype professions women men context portuguese language
learn generate fluent natural language structure data neural network become common approach nlg problem challenge form structure data vary examples paper present survey several extensions sequence sequence model account latent content selection process particularly variants copy attention coverage decode propose train method base diverse ensembling encourage model learn distinct sentence templates train empirical evaluation techniques show increase quality generate text across five automate metrics well human evaluation
exist study text sql task require generate complex sql query multiple clauses sub query generalize new unseen databases paper propose syntaxsqlnet syntax tree network address complex cross domain text sql generation task syntaxsqlnet employ sql specific syntax tree base decoder sql generation path history table aware column attention encoders evaluate syntaxsqlnet spider text sql task contain databases multiple table complex sql query multiple sql clauses nest query use database split set databases test set unseen train experimental result show syntaxsqlnet handle significantly greater number complex sql examples prior work outperform previous state art model seventy-three exact match accuracy also show syntaxsqlnet improve performance additional seventy-five use cross domain augmentation method result one hundred and forty-eight improvement total knowledge first study complex cross domain text sql task
different texts shall nature correspond different number keyphrases desideratum largely miss exist neural keyphrase generation model study address problem model evaluation perspectives first propose recurrent generative model generate multiple keyphrases delimiter separate sequence generation diversity enhance two novel techniques manipulate decoder hide state contrast previous approach model capable generate diverse keyphrases control number output propose two evaluation metrics tailor towards variable number generation also introduce new dataset stackex expand beyond exist genre ie academic write keyphrase generation task previous new evaluation metrics model outperform strong baselines datasets
image caption generation systems typically evaluate reference output show possible predict output quality without generate caption base probability assign neural model reference caption pre gen metrics strongly correlate standard evaluation metrics
paper address sensitivity neural image caption generators visual input sensitivity analysis omission analysis base image foil report show extent image caption architectures retain sensitive visual information vary depend type word generate position caption whole motivate work context broader goals field achieve explainability ai
past years witness flourish crowdsourced medical question answer qanda websites patients medical information demand tend post question health condition crowdsourced qanda websites get answer users however observe large portion new medical question answer time receive answer websites hand notice solve question great potential solve challenge motivate propose end end system automatically find similar question unsolved medical question learn vector presentation unsolved question candidate similar question propose system output similar question accord similarity vector representations vector representation similar question find question level diversity medical question expression issue address handle two important issue ie train data generation issue efficiency issue associate lstm train procedure retrieval candidate similar question effectiveness propose system validate large scale real world dataset collect crowdsourced maternal infant qanda website
knowledge base completion kbc aim determine miss relations entity pair receive increase attention recent years exist kbc methods focus either embed knowledge base kb specific semantic space leverage joint probability random walk rws multi hop paths unify model take semantic path relate feature consideration adequacy paper propose novel method explore intrinsic relationship single relation ie one hop path multi hop paths pair entities use hierarchical attention network hans select important relations multi hop paths encode low dimensional vectors treat relations multi hop paths two different input source use feature extractor share two downstream components ie relation classifier source discriminator capture share similar information joint adversarial train encourage model extract feature multi hop paths representative relation completion apply train model except source discriminator several large scale kbs relation completion experimental result show method outperform exist path information base approach since sub module model well interpret model apply large number relation learn task
machine read comprehension unanswerable question new challenge task natural language process key subtask reliably predict whether question unanswerable paper propose unify model call net three important components answer pointer answer pointer answer verifier introduce universal node thus process question context passage single contiguous sequence tokens universal node encode fuse information question passage play important role predict whether question answerable also greatly improve conciseness net different state art pipeline model net learn end end fashion experimental result squad twenty dataset show net effectively predict unanswerability question achieve f1 score seven hundred and seventeen squad twenty
recently neural network show promise result name entity recognition ner need number label data model train meet new domain target domain ner label data make domain ner much difficult ner research long time similar domain already well label data source domain therefore paper focus domain ner study utilize label data similar source domain new target domain design kernel function base instance transfer strategy get similar label sentence source domain moreover propose enhance recurrent neural network ernn add additional layer combine source domain label data traditional rnn structure comprehensive experiment conduct two datasets comparison result among hmm crf rnn show rnn perform bette others label data domain target compare directly use source domain label data without select transfer instance enhance rnn approach get improvement eight thousand and fifty-two nine thousand, three hundred and twenty-eight term f1 measure
user profile mean exploit technology machine learn predict attribute users demographic attribute hobby attribute preference attribute etc powerful data support precision market exist methods mainly study network behavior personal preferences post texts build user profile data analysis micro blog find females show positive richer emotions males online social platform difference conducive distinction genders therefore argue sentiment context important well user profilingthis paper focus exploit microblog user post predict one demographic label gender propose sentiment representation learn base multi layer perceptronsrl mlp model classify gender first build sentiment polarity classifier advance train long short term memorylstm model e commerce review corpus next transfer sentiment representation basic mlp network last conduct experiment gender classification sentiment representation experimental result show approach improve gender classification accuracy five hundred and fifty-three eight thousand, four hundred and twenty eight thousand, nine hundred and seventy-three
conversational machine comprehension require understand conversation history previous question answer pair document context current question enable traditional single turn model encode history comprehensively introduce flow mechanism incorporate intermediate representations generate process answer previous question alternate parallel process structure compare approach concatenate previous question answer input flow integrate latent semantics conversation history deeply model flowqa show superior performance two recently propose conversational challenge seventy-two f1 coqa forty quac effectiveness flow also show task reduce sequential instruction understand conversational machine comprehension flowqa outperform best model three domains scone eighteen forty-four improvement accuracy
study compare media discourse euroscepticism two thousand and fourteen across six countries unite kingdom ireland france spain sweden denmark assess extent mass media report euroscepticism indicate europeanization public spheres use mix methods approach combine lda topic model qualitative cod find approximately seventy per cent print article mention euroscepticism eurosceptic frame non domestic ie european context five six case study article exhibit european context strikingly similar content british case exception however coverage british euroscepticism drive europeanization member state bivariate logistic regressions reveal three macro level structural variables significantly correlate europeanize media discourse newspaper type tabloid broadsheet presence strong eurosceptic party relationship eu budget net contributor receiver eu fund
use medical data also know electronic health record research help develop advance medical science however protect patient confidentiality identity use medical data analysis crucial medical data form tabular structure ie table free form narratives image study focus medical data free form longitudinal text de identification electronic health record provide opportunity use data research without affect patient privacy avoid need individual patient consent recent years increase interest develop accurate robust adaptable automatic de identification system electronic health record mainly due dilemma availability abundance health data inability use data research due legal ethical restrictions de identification track competitions two thousand and fourteen i2b2 uthealth two thousand and sixteen cegs n grid share task provide great platform advance area primary reason include open source nature dataset fact raw psychiatric data use two thousand and sixteen competitions study focus noticeable trend change techniques use development automatic de identification longitudinal clinical narratives specifically shift use conditional random field crf base systems rule regular expressions dictionary combinations base systems hybrid model combine crf rule recently deep learn base systems review literature result arise two thousand and fourteen two thousand and sixteen competitions discuss outcomes systems also provide list research question emerge survey
previous research treat name entity extraction classification end end task argue two sub task address separately entity extraction lie level syntactic analysis entity classification lie level semantic analysis accord noam chomsky syntactic structure pp ninety-three ninety-four chomsky one thousand, nine hundred and fifty-seven syntax appeal semantics semantics affect syntax analyze two benchmark datasets characteristics name entities find uncommon word distinguish name entities common text uncommon word word hardly appear common text mainly proper nouns experiment validate lexical syntactic feature achieve state art performance entity extraction semantic feature improve extraction performance model state art baselines chomsky view also explain failure joint syntactic semantic parse work
vocal age universal process human age largely affect one language use possibly include subtle acoustic feature one utterances like voice onset time figure time effect queen elizabeth christmas speeches document analyze long term trend build statistical model time dependence voice onset time control wide range fix factor present annual variations simulate trajectory reveal variation range voice onset time narrow fifty years slight reduction mean value possibly effect diminish exertion result subdue muscle contraction transcend non linguistic factor form voice onset time pattern long time
past decades importance soft skills labour market outcomes grow substantially carry implications labour market inequality since previous research show soft skills value equally across race gender work explore role soft skills job advertisements draw methods computational science well theoretical empirical insights economics sociology psychology present semi automatic approach base crowdsourcing text mine extract list soft skills find soft skills crucial component job ads especially low pay job job female dominate professions work show soft skills serve partial predictors gender composition job categories soft skills receive equal wage return labour market especially female skills frequently associate wage penalties result expand grow literature association soft skills wage inequality highlight importance occupational gender segregation labour market
paper present first attempt towards unsupervised neural text simplification rely unlabeled text corpora core framework compose share encoder pair attentional decoders gain knowledge simplification discrimination base losses denoising framework train use unlabeled text collect en wikipedia dump analysis quantitative qualitative involve human evaluators public test data show propose model perform text simplification lexical syntactic level competitive exist supervise methods addition label pair also improve performance
suggest new nlg task context discourse generation pipeline computational storytelling systems task textual embellishment define take text input generate semantically equivalent output increase lexical syntactic complexity ideally would allow author computational storytellers implement lightweight nlg systems use domain independent embellishment module translate output literary text present promise first result task use lstm encoder decoder network train wikilarge dataset furthermore introduce compile computer tales corpus computationally generate stories use test capabilities embellishment algorithms
allow humans interactively train artificial agents understand language instructions desirable practical scientific reason give poor data efficiency current learn methods goal may require substantial research efforts introduce babyai research platform support investigations towards include humans loop ground language learn babyai platform comprise extensible suite nineteen level increase difficulty level gradually lead agent towards acquire combinatorially rich synthetic language proper subset english platform also provide heuristic expert agent purpose simulate human teacher report baseline result estimate amount human involvement would require train neural network base agent babyai level put forward strong evidence current deep learn methods yet sufficiently sample efficient come learn language compositional properties
mean may arise element role interactions within larger system example hit nail central people concept hammer particular material composition intrinsic feature likewise importance web page may result link page rather solely content one example mean arise extrinsic relationships approach extract mean word concepts co occurrence pattern large text corpora success methods suggest human activity pattern may reveal conceptual organization however texts directly reflect human activity instead serve communicative function usually highly curated edit suit audience apply methods devise text data source directly reflect thousands individuals activity pattern namely supermarket purchase use product co occurrence data nearly 13m shop baskets train topic model learn twenty-five high level concepts topics topics find comprehensible coherent retail experts consumers topics range specific eg ingredients stir fry general eg cook scratch topics tend goal direct situational consistent notion human conceptual knowledge tailor support action individual differences topics sample predict basic demographic characteristics result suggest human activity pattern reveal conceptual organization may give rise
dropout crucial regularization technique recurrent neural network rnn model natural language inference nli however dropout evaluate effectiveness different layer dropout rat nli model paper propose novel rnn model nli empirically evaluate effect apply dropout different layer model also investigate impact vary dropout rat layer empirical evaluation large stanford natural language inference snli small scitail dataset suggest dropout fee forward connection severely affect model accuracy increase dropout rat also show regularize embed layer efficient snli whereas regularize recurrent layer improve accuracy scitail model achieve accuracy eight thousand, six hundred and fourteen snli dataset seven thousand, seven hundred and five scitail
present experimental system identify annotate metaphor corpora design plug metacorps experimental web app annotate metaphor metacorps users annotate metaphors system use user annotations train data system confident suggest identification annotation approve user become train data naturally allow transfer learn system know degree reliability classify one class metaphor train another class metaphor example metaphorical violence project metaphors may classify network observe grammatical subject object violence metaphor violent word use hit attack beat etc
current state art read comprehension model rely heavily recurrent neural network explore entirely different approach question answer convolutional model nature convolutional model fast train capture local dependencies well though struggle longer range dependencies thus require augmentation achieve comparable performance rnn base model conduct two dozen control experiment convolutional model various kernel attention regularization scheme determine precise performance gain strategy maintain focus speed ultimately ensembled three model crossconv five thousand, three hundred and ninety-eight dev f1 attnconv five thousand, six hundred and sixty-five maybeconv five thousand, two hundred and eighty-five ensembled model able achieve six thousand, two hundred and thirty-eight f1 score use official squad evaluation script individual convolutional model crossconv able exceed performance rnn plus attention baseline twenty-five train six time faster
entity type et problem assign label give entity mention sentence exist work et require knowledge domain target label set give test instance et absence knowledge novel problem address et wild hypothesize solution problem build supervise model generalize better et task whole rather specific dataset direction propose collective learn framework clf enable learn diverse datasets unify way clf first create unify hierarchical label set uhls label map aggregate label information available datasets build single neural network classifier use uhls label map partial loss function single classifier predict finest possible label across available domains even though label may present domain specific dataset also propose set evaluation scheme metrics evaluate performance model novel problem extensive experimentation seven diverse real world datasets demonstrate efficacy clf
input method essential service every mobile desktop devices provide text suggestions convert sequential keyboard input character target language indispensable japanese chinese users due critical resource constraints limit network bandwidth target devices apply neural model input method well explore work apply lstm base language model input method evaluate performance prediction conversion task japanese bccwj corpus articulate bottleneck slow softmax computation conversion solve issue propose incremental softmax approximation approach compute softmax select subset vocabulary fix stale probabilities vocabulary update future step refer method incremental selective softmax result show two order speedup softmax computation convert japanese input sequence large vocabulary reach real time speed commodity cpu also exploit model compress potential achieve ninety-two model size reduction without lose accuracy
violence epidemic brazil problem rise world wide mobile devices provide communication technologies use monitor alert violent situations however current solutions like panic button safe word might increase loss life violent situations propose embed artificial intelligence solution use natural language speech process technology silently alert someone help situation corpus use contain four hundred positive phrase eight hundred negative phrase total one thousand, two hundred sentence classify use two well know extraction methods natural language process task bag word word embeddings classify support vector machine describe proof concept product development promise result indicate path towards commercial product importantly show model improvements via word embeddings data augmentation techniques provide intrinsically robust model final embed solution also small footprint less ten mb
natural language hierarchically structure smaller units eg phrase nest within larger units eg clauses larger constituent end smaller constituents nest within must also close standard lstm architecture allow different neurons track information different time scale explicit bias towards model hierarchy constituents paper propose add inductive bias order neurons vector master input forget gate ensure give neuron update neurons follow order also update novel recurrent architecture order neurons lstm lstm achieve good performance four different task language model unsupervised parse target syntactic evaluation logical inference
recurrent neural network state art natural language process build rich contextual representations process texts arbitrary length however recent developments attention mechanisms equip feedforward network similar capabilities hence enable faster computations due increase number operations parallelize explore new type architecture domain question answer propose novel approach call fully attention base information retriever fabir show fabir achieve competitive result stanford question answer dataset squad fewer parameters faster learn inference rival methods
paper describe general framework parameters read write network prawn systematically analyze current neural model multi task learn find exist model expect disentangle feature different space feature learn practice still entangle share space leave potential hazard train unseen task propose alleviate problem incorporate inductive bias process multi task learn task keep inform knowledge store task way task maintain knowledge practice achieve inductive bias allow different task communicate pass hide variables gradients explicitly experimentally evaluate propose methods three group task two type settings textscin task textscout task quantitative qualitative result show effectiveness
topological data analysis tda widely use make progress number problems however seem tda application natural language process nlp infancy paper try bridge gap argue tda tool natural choice come analyse word embed data describe parallelisable unsupervised learn algorithm base local homology datapoints show experimental result word embed data see local homology datapoints word embed data contain information potentially use solve word sense disambiguation problem
image may elements contain text bound box associate example text identify via optical character recognition computer screen image natural image label object present end end trainable architecture incorporate information elements image segment identify part image natural language expression refer calculate embed element project onto correspond location ie associate bound box image feature map show architecture give improvement resolve refer expressions use image methods incorporate element information demonstrate experimental result refer expression datasets base coco webpage image refer expression dataset develop
advance neural machine translation nmt model generally implement encoder decoder multiple layer allow systems model complex function capture complicate linguistic structure however top layer encoder decoder leverage subsequent process miss opportunity exploit useful information embed layer work propose simultaneously expose signal layer aggregation multi layer attention mechanisms addition introduce auxiliary regularization term encourage different layer capture diverse information experimental result widely use wmt14 english german wmt17 chinese english translation data demonstrate effectiveness universality propose approach
self attention network prove profound value strength capture global dependencies work propose model localness self attention network enhance ability capture useful local context cast localness model learnable gaussian bias indicate central scope local region pay attention bias incorporate original attention distribution form revise distribution maintain strength capture long distance dependencies enhance ability capture short range dependencies apply localness model lower layer self attention network quantitative qualitative analyse chinese english english german translation task demonstrate effectiveness universality propose approach
multi head attention appeal ability jointly attend information different representation subspaces different position work introduce disagreement regularization explicitly encourage diversity among multiple attention head specifically propose three type disagreement regularization respectively encourage subspace attend position output representation associate attention head different head experimental result widely use wmt14 english german wmt17 chinese english translation task demonstrate effectiveness universality propose approach
investigate extent compositional vector space model use account scope ambiguity quantify sentence form every man love woman sentence contain two quantifiers introduce two read direct scope read inverse scope read ambiguity treat vector space model use bialgebras hedge sadrzadeh two thousand and sixteen sadrzadeh two thousand and sixteen though without explanation mechanism ambiguity arise combine polarise focus sequent calculus non associative lambek calculus nl describe moortgat moot two thousand and eleven vector base approach quantifier scope ambiguity particular establish procedure obtain vector space model quantifier scope ambiguity derivational way
knowledge base kbs paramount nlp employ multiview learn increase accuracy coverage entity type information kbs rely two metaviews language representation language consider high resource low resource languages wikipedia representation consider representations base context distribution entity ie embed entity name ie surface form description wikipedia two metaviews language representation freely combine pair language representation eg german embed english description spanish name distinct view experiment entity type fine grain class demonstrate effectiveness multiview learn release mvet large multiview particular multilingual entity type dataset create mono multilingual fine grain entity type systems evaluate dataset
paper examine effect two side argumentation perceive helpfulness online consumer review contrast previous work analysis thereby shed light reception review language base perspective purpose propose intrigue text analysis approach base distribute text representations multi instance learn operationalize two sidedness argumentation review texts subsequent empirical analysis use large corpus amazon review suggest two side argumentation review significantly increase helpfulness find effect stronger positive review negative review whereas higher degree emotional language weaken effect find immediate implications retailer platforms utilize result optimize customer feedback system present useful product review
vector space embed model like word2vec glove fasttext elmo extremely popular representations natural language process nlp applications present magnitude fast lightweight tool utilize process embeddings magnitude open source python package compact vector storage file format allow efficient manipulation huge number embeddings magnitude perform common operations sixty six thousand time faster gensim magnitude introduce several novel feature improve robustness like vocabulary lookups
sentence simplification aim reduce complexity sentence retain original mean current model sentence simplification adopt ideas chine translation study implicitly learn simplification map rule normal simple sentence pair paper explore novel model base multi layer multi head attention architecture pro pose two innovative approach integrate simple ppdb paraphrase database simplification external paraphrase knowledge base simplification cover wide range real world simplification rule experiment show integration provide two major benefit one integrate model outperform multiple state art baseline model sentence simplification literature two analysis rule utilization model seek select accurate simplification rule code model use paper available https githubcom sanqiang textsimplification
address problem american sign language fingerspell recognition wild use videos collect websites introduce largest data set available far problem fingerspell recognition first use naturally occur video data use data set present first attempt recognize fingerspell sequence challenge set unlike prior work video data extremely challenge due low frame rat visual variability tackle visual challenge train special purpose sign hand detector use small subset data give hand detector output sequence model decode hypothesize fingerspell letter sequence sequence model explore attention base recurrent encoder decoders ctc base approach first attempt fingerspell recognition wild work intend serve baseline future work sign language recognition realistic condition find expect letter error rat much higher previous work control data analyze source error effect model variants
imbalanced dataset occur due uneven distribution data available real world disposition complaints government offices bandung consequently multi label text categorization algorithms may produce best performance classifiers tend weigh majority data ignore minority paper bag adaptive boost algorithms employ handle issue improve performance text categorization result evaluate four evaluation metrics ham loss subset accuracy example base accuracy micro average f measure bag ml lp smo weak classifier best performer term subset accuracy example base accuracy bag ml br smo weak classifier best micro average f measure among hand adaboost mh j48 weak classifier lowest ham loss value thus algorithms high potential boost performance text categorization certain weak classifiers however bag potential adaptive boost increase accuracy minority label
consider problem automatically generate textual paraphrase modify attribute properties focus set without parallel data hu et al two thousand and seventeen shen et al two thousand and seventeen set pose challenge evaluation show metric post transfer classification accuracy insufficient propose additional metrics base semantic preservation fluency well way combine single overall score contribute new loss function train strategies address different metrics semantic preservation address add cyclic consistency loss loss base paraphrase pair fluency improve integrate losses base style specific language model experiment yelp sentiment dataset new literature dataset propose use multiple model extend prior work shen et al two thousand and seventeen demonstrate metrics correlate well human judgments sentence level system level automatic manual evaluation also show large improvements baseline method shen et al two thousand and seventeen hope propose metrics speed system development new textual transfer task also encourage community address three complementary aspects transfer quality
multimodal search base dialogue challenge new task extend visually ground question answer systems multi turn conversations access external database address new challenge learn neural response generation system recently release multimodal dialogue mmd dataset saha et al two thousand and seventeen introduce knowledge ground multimodal conversational model encode knowledge base kb representation append decoder input model substantially outperform strong baselines term text base similarity measure nine bleu point three solely due use additional information kb
semantic representations form direct acyclic graph dags introduce recent years model need probabilistic model dags one model attract attention dag automaton study probabilistic model show dag automata make useful probabilistic model nearly universal strategy assign weight transition problem affect single root multi root unbounded degree variants dag automata appear pervasive affect planar variants problematic reason
language model methods rely large scale data statistically learn sequential pattern word paper argue word atomic language units necessarily atomic semantic units inspire hownet use sememes minimum semantic units human languages represent implicit semantics behind word language model name sememe drive language model sdlm specifically predict next word sdlm first estimate sememe distribution give textual context afterward regard sememe distinct semantic expert experts jointly identify probable sense correspond word way sdlm enable language model work beyond word level manipulation fine grain sememe level semantics offer us powerful tool fine tune language model improve interpretability well robustness language model experiment language model downstream application headline gener ation demonstrate significant effect sdlm source code data use experiment access https githubcom thunlp sdlm pytorch
recent paper neural machine translation propose strict use attention mechanisms previous standards recurrent convolutional neural network rnns cnns propose run traditionally stack encode branch encoder decoder attention focus architectures parallel even sequential operations remove model thereby decrease train time particular modify recently publish attention base architecture call transformer google replace sequential attention modules parallel ones reduce amount train time substantially improve bleu score time experiment english german english french translation task show model establish new state art
code switch speech recognition attract increase interest recently need expert linguistic knowledge always big issue end end automatic speech recognition asr simplify build asr systems considerably predict graphemes character directly acoustic input mean time need expert linguistic knowledge also eliminate make attractive choice code switch asr paper present hybrid ctc attention base end end mandarin english code switch cs speech recognition system study effect hybrid ctc attention base model different model units inclusion language identification different decode strategies task code switch asr seame corpus system achieve mix error rate mer three thousand, four hundred and twenty-four
research show neural model implicitly encode linguistic feature research show emphhow encode arise model train present first study learn dynamics neural language model use simple flexible analysis method call singular vector canonical correlation analysis svcca enable us compare learn representations across time across model without need evaluate directly annotate data probe evolution syntactic semantic topic representations find part speech learn earlier topic recurrent layer become similar tagger train embed layer less similar result methods could inform better learn algorithms nlp model possibly incorporate linguistic information effectively
recent advance deep learn computer vision cv natural language process nlp provide us new way understand semantics deal challenge task automatic description generation natural image challenge encoder decoder framework achieve promise performance convolutional neural network cnn use image encoder recurrent neural network rnn decoder paper introduce sequential guide network guide decoder word generation new model extension encoder decoder framework attention additional guide long short term memory lstm train end end manner use image descriptions pair validate approach conduct extensive experiment benchmark dataset ie ms coco caption propose model achieve significant improvement compare state art deep learn model
introduce new dataset joint reason natural language image focus semantic diversity compositionality visual reason challenge data contain one hundred and seven thousand, two hundred and ninety-two examples english sentence pair web photograph task determine whether natural language caption true pair photograph crowdsource data use set visually rich image compare contrast task elicit linguistically diverse language qualitative analysis show data require compositional joint reason include quantities comparisons relations evaluation use state art visual reason methods show data present strong challenge
dominant approach unsupervised style transfer text base idea learn latent representation independent attribute specify style paper show condition necessary always meet practice even domain adversarial train explicitly aim learn disentangle representations thus propose new model control several factor variation textual data condition disentanglement replace simpler mechanism base back translation method allow control multiple attribute like gender sentiment product type etc fine grain control trade content preservation change style pool operator latent space experiment demonstrate fully entangle model produce better generations even test new challenge benchmarks comprise review multiple sentence multiple attribute
different languages might different word order paper investigate cross lingual transfer posit order agnostic model perform better transfer distant foreign languages test hypothesis train dependency parsers english corpus evaluate transfer performance thirty languages specifically compare encoders decoders base recurrent neural network rnns modify self attentive architectures former rely sequential information latter flexible model word order rigorous experiment detail analysis show rnn base architectures transfer well languages close english self attentive model better overall cross lingual transferability perform especially well distant languages
neural information retrieval neu ir model derive query document rank score base term level match inspire tilebars classical term distribution visualization method paper propose novel neu ir model handle query document match subtopic higher level system first split document topical segment visualize match query segment feed interaction matrix neu ir model deeptilebars obtain final rank score deeptilebars model relevance signal occur different granularities document topic hierarchy better capture discourse structure document thus match pattern although design implementation light weight deeptilebars outperform state art neu ir model benchmark datasets include text retrieval conference trec two thousand and ten two thousand and twelve web track letor forty
one fundamental requirements model semantic process dialogue incrementality model must reflect people interpret generate language least word word basis handle phenomena fragment incomplete jointly produce utterances show incremental word word parse process dynamic syntax ds assign compositional distributional semantics composition operator ds correspond general operation tensor contraction multilinear algebra provide abstract semantic decorations nod ds tree term vectors tensors sum thereof use latter model underspecified elements crucial assign partial representations incremental process work example give instantiation theory use plausibility tensors compositional distributional semantics show framework incrementally assign semantic plausibility measure parse phrase sentence
consistency long stand issue face dialogue model paper frame consistency dialogue agents natural language inference nli create new natural language inference dataset call dialogue nli propose method demonstrate model train dialogue nli use improve consistency dialogue model evaluate method human evaluation automatic metrics suite evaluation set design measure dialogue model consistency
question answer qa achieve promise progress recently however answer question real world scenarios like medical domain still challenge due requirement external knowledge insufficient quantity high quality train data light challenge study task generate medical qa pair paper insight medical question consider sample latent distribution question give answer propose automate medical qa pair generation framework consist unsupervised key phrase detector explore unstructured material validity generator involve multi pass decoder integrate structural knowledge diversity series experiment conduct real world dataset collect national medical license examination china automatic evaluation human annotation demonstrate effectiveness propose method investigation show incorporate generate qa pair train significant improvement term accuracy achieve examination qa system
acquire large vocabulary important aspect human intelligence onecommon approach human populate vocabulary learn word duringreading listen use write speak ability totransfer input output natural human difficult machineshuman spontaneously perform knowledge transfer complicate multimodaltasks visual question answer vqa order approach human levelartificial intelligence hope equip machine ability therefore toaccelerate research propose newzero shoot transfer vqazst vqadataset reorganize exist vqa v10 dataset way duringtraining word appear one module ie question theother ie answer set intelligent model understand andlearn concepts one module ie question test time transfer themto ie predict concepts answer conduct evaluation thisnew dataset use three exist state art vqa neural model experimentalresults show significant drop performance dataset indicate existingmethods address zero shoot transfer problem besides analysis findsthat may cause implicit bias learn train
machine translation systems base deep neural network expensive train curriculum learn aim address issue choose order sample present train help train better model faster adopt probabilistic view curriculum learn let us us flexibly evaluate impact curricula design perform extensive exploration german english translation task result show possible improve convergence time loss translation quality however result highly sensitive choice sample difficulty criteria curriculum schedule hyperparameters
fake news detection critical yet challenge problem natural language process nlp rapid rise social network platforms yield vast increase information accessibility also accelerate spread fake news thus effect fake news grow sometimes extend offline world threaten public safety give massive amount web content automatic fake news detection practical nlp problem useful online content providers order reduce human time effort detect prevent spread fake news paper describe challenge involve fake news detection also describe relate task systematically review compare task formulations datasets nlp solutions develop task also discuss potentials limitations base insights outline promise research directions include fine grain detail fair practical detection model also highlight difference fake news detection relate task importance nlp solutions fake news detection
current methods sequence tag core task nlp data hungry motivate use crowdsourcing cheap way obtain label data however annotators often unreliable current aggregation methods capture common type span annotation errors address propose bayesian method aggregate sequence tag reduce errors model sequential dependencies annotations well grind truth label take bayesian approach account uncertainty model due annotator errors lack data model annotators complete task evaluate model crowdsourced data name entity recognition information extraction argument mine show sequential model outperform previous state art also find approach reduce crowdsourcing cost effective active learn better capture uncertainty sequence label annotations
propose paper combine model long short term memory convolutional neural network lstm cnn exploit word embeddings positional embeddings cross sentence n ary relation extraction propose model bring together properties lstms cnns simultaneously exploit long range sequential information capture informative feature essential cross sentence n ary relation extraction lstm cnn model evaluate standard dataset cross sentence n ary relation extraction significantly outperform baselines cnns lstms also combine cnn lstm model paper also show lstm cnn model outperform current state art methods cross sentence n ary relation extraction
paper propose new method query expansion use farsnet persian wordnet find similar tokens relate query expand semantic mean query purpose use synonymy relations farsnet extract relate synonyms query word algorithm use enhance information retrieval systems improve search result overall evaluation system comparison baseline method without use query expansion show improvement nine percent mean average precision map
investigate impact search strategies neural dialogue model first compare two standard search algorithms greedy beam search well newly propose iterative beam search produce diverse set candidate responses evaluate strategies realistic full conversations humans propose model base bayesian calibration address annotator bias conversations analyze use two automatic metrics log probabilities assign model utterance diversity experiment reveal better search algorithms lead higher rat conversations however find optimal selection mechanism choose diverse set candidates still open question
recently several deep learn base model propose end end learn dialogs model train data without need additional annotations hard interpret hand exist traditional state base dialog systems state dialog discrete hence easy interpret however state need handcraft annotate data achieve best worlds propose latent state track network lstn use learn interpretable model unsupervised manner model define discrete latent variable turn conversation take finite set value since discrete variables present train data use algorithm train model unsupervised manner experiment show lstn help achieve interpretability dialog model without much decrease performance compare end end approach
consider problem align continuous word representations learn multiple languages common space recently show case two languages possible learn map without supervision paper extend line work problem align multiple languages common space solution independently map languages pivot language unfortunately degrade quality indirect word translation thus propose novel formulation ensure composable mappings lead better alignments evaluate method jointly align word vectors eleven languages show consistent improvement indirect mappings maintain competitive performance direct word translation
propose novel path base reason approach multi hop read comprehension task system need combine facts multiple passages answer question although inspire multi hop reason knowledge graph propose approach operate directly unstructured text generate potential paths passages score without direct path supervision propose model name pathnet attempt extract implicit relations text entity pair representations compose encode path capture additional context pathnet also compose passage representations along path compute passage base representation unlike previous approach model able explain reason via explicit paths passages show approach outperform prior model multi hop wikihop dataset also generalize apply openbookqa dataset match state art performance
identify extract data elements study descriptors publication full texts critical yet manual labor intensive step require number task paper address question identify data elements unsupervised manner specifically provide set criteria describe specific study parameters species route administration dose regimen develop unsupervised approach identify text segment sentence relevant criteria binary classifier train identify publications meet criteria perform better train candidate sentence train sentence randomly pick text support intuition method able accurately identify study descriptors
biomedical association study increasingly do use clinical concepts particular diagnostic cod clinical data repositories phenotypes clinical concepts represent meaningful vector space use word embed model embeddings allow comparison clinical concepts straightforward input machine learn model use traditional approach good representations require high dimensionality make downstream task visualization difficult apply poincar e embeddings two dimensional hyperbolic space large scale administrative claim database show performance comparable one hundred dimensional embeddings euclidean space examine disease relationships different disease contexts better understand potential phenotypes
automatically annotate column type knowledge base kb concepts critical task gain basic understand web table current methods rely either table metadata like column name entity correspondences cells kb may fail deal grow web table incomplete meta information paper propose neural network base column type annotation framework name colnet able integrate kb reason lookup machine learn automatically train convolutional neural network prediction prediction model consider contextual semantics within cell use word representation also embed semantics column learn locality feature multiple cells method evaluate dbpedia two different web table datasets t2dv2 general web limaye wikipedia page achieve higher performance state art approach
knowledge graph embed aim model entities relations low dimensional vectors previous methods require entities see train unpractical real world knowledge graph new entities emerge daily basis recent efforts issue suggest train neighborhood aggregator conjunction conventional entity relation embeddings may help embed new entities inductively via exist neighbor however neighborhood aggregators neglect unordered unequal natures entity neighbor end summarize desire properties may lead effective neighborhood aggregators also introduce novel aggregator namely logic attention network lan address properties aggregate neighbor rule network base attention weight compare conventional aggregators two knowledge graph completion task experimentally validate lan superiority term desire properties
paper introduce takefive new semantic role label method transform text frame orient knowledge graph perform dependency parse identify word evoke lexical frame locate roles fillers frame run coercion techniques formalise result knowledge graph formal representation comply frame semantics use framester factual linguistic link data resource obtain precision recall f1 value indicate takefive competitive exist methods semafor pikes pathlstm fred finally discuss combine takefive fred obtain higher value precision recall f1
ubiquitous task process electronic medical data assignment standardize cod represent diagnose procedures free text document medical report difficult natural language process task require parse long heterogeneous document select set appropriate cod tens thousands possibilities many positive train sample present deep learn system advance state art mimic iii dataset achieve new best micro f1 measure five thousand, five hundred and eighty-five significantly outperform previous best result mullenbach et al two thousand and eighteen achieve number enhancements include two major novel contributions multi view convolutional channel effectively learn adjust kernel size throughout input attention regularization mediate natural language code descriptions help overcome sparsity thousands uncommon cod modifications select address difficulties inherent automate cod specifically deep learn generally finally investigate accuracy result detail individually measure impact contributions point way towards future algorithmic improvements
extreme multi label text classification xmtc important problem era big data tag give text relevant multiple label extremely large scale label set xmtc find many applications item categorization web page tag news annotation traditionally methods use bag word bow input ignore word context well deep semantic information recent attempt overcome problems bow deep learn still suffer one fail capture important subtext label two lack scalability huge number label propose new label tree base deep learn model xmtc call attentionxml two unique feature one multi label attention mechanism raw text input allow capture relevant part text label two shallow wide probabilistic label tree plt allow handle millions label especially tail label empirically compare performance attentionxml eight state art methods six benchmark datasets include amazon 3m around three million label attentionxml outperform compete methods experimental settings experimental result also show attentionxml achieve best performance tail label among label tree base methods code datasets available http githubcom yourh attentionxml
many text classification task major problem pose lack label data target domain although classifiers target domain train label text data relate source domain accuracy classifiers usually lower cross domain set recently string kernels obtain state art result various text classification task native language identification automatic essay score moreover classifiers base string kernels find robust distribution gap different domains paper formally describe algorithm compose two simple yet effective transductive learn approach improve result string kernels cross domain settings adapt string kernels test set without use grind truth test label report significantly better accuracy rat cross domain english polarity classification
equip sl software properly need input system represent manipulate sign content way every day software allow process write text refute claim video good enough medium serve purpose propose build representation editable queryable synthesisable user friendly define term upfront issue functionally conceptually link write study exist write systems namely use vocal languages design propose sls spontaneous ways sl users put language write observe paradigm turn move propose new approach satisfy goals integration software finally open prospect proposition use outside restrict scope write system compare properties write systems present
language use reveal information feel1 three one pioneer text analysis walter weintraub manually count type word people use medical interview show frequency first person singular pronouns ie reliable indicator depression depress people use often people depressed4 several study demonstrate language use also differ truthful deceptive statements5 seven differences consistent across people contexts make prediction difficult8 show well linguistic deception detection perform individual level develop model tailor single individual current us president use tweet fact check independent third party washington post find substantial linguistic differences factually correct incorrect tweet develop quantitative model base differences next predict whether sample tweet either factually correct incorrect achieve seventy-three overall accuracy result demonstrate power linguistic analysis real world deception research apply individual level provide evidence factually incorrect tweet random mistake sender
pattern base label methods achieve promise result alleviate inevitable label noise distantly supervise neural relation extraction however methods require significant expert labor write relation specific pattern make sophisticate generalize quicklyto ease labor intensive workload pattern write enable quick generalization new relation type propose neural pattern diagnosis framework diag nre automatically summarize refine high quality relational pattern noise data human experts loop demonstrate effectiveness diag nre apply two real world datasets present significant interpretable improvements state art methods
consider problem learn knowledge graph kg embeddings entity alignment ea current methods use embed model mainly focus triple level learn lack ability capture long term dependencies exist kgs consequently embed base ea methods heavily rely amount prior know alignment due identity information prior alignment efficiently propagate one kg another paper propose rsn4ea recurrent skip network ea leverage bias random walk sample generate long paths across kgs model paths novel recurrent skip network rsn rsn integrate conventional recurrent neural network rnn residual learn largely improve convergence speed performance parameters evaluate rsn4ea series datasets construct real world kgs experimental result show outperform number state art embed base ea methods also achieve comparable performance kg completion
keyword spot kws provide start signal asr problem thus essential ensure high recall rate however real time property require low computation complexity contradiction inspire people find suitable model small enough perform well multi environments deal contradiction implement hierarchical neural networkhnn prove effective many speech recognition problems hnn outperform traditional dnn cnn even though model size computation complexity slightly less also simple topology structure make easy deploy device
abundance text data produce modern age make increasingly important intuitively group categorize classify text data theme efficient retrieval search yet high dimensionality imprecision text data generally language whole prove challenge attempt perform unsupervised document cluster thesis present two novel methods improve unsupervised document cluster classification theme first improve document representations look exploit term neighborhoods blur semantic weight across neighbor term neighborhoods locate semantic space afford word embeddings second method cluster revision base deem stochastic barcoding barcode pattern text data inherently high dimensional yet cluster typically take place low dimensional representation space method utilize lower dimension cluster result initial cluster configurations iteratively revise configuration high dimensional space show experimental result two methods improve quality document cluster thesis elaborate two new conceptual contributions joint thesis david yan detail feature transformation software architecture develop unsupervised document classification
pervasive belief regard differences human language animal vocal sequence song belong different class computational complexity animal song belong regular languages whereas human language superregular argument however lack empirical evidence since superregular analyse animal song understudy goal paper perform superregular analysis animal song use data gibbons case study demonstrate superregular analysis effectively use non human data key find superregular analysis increase explanatory power rather provide compact analysis fewer grammatical rule necessary superregularity allow pattern analogous previous computational analysis human language accordingly null hypothesis human language animal song govern type grammatical systems reject
automatic speech recognition asr recurrent neural language model rnnlm typically use refine hypotheses form lattices n best list generate beam search decoder weaker language model rnnlms usually train generatively use perplexity ppl criterion large corpora grammatically correct text however hypotheses noisy rnnlm always make choices minimise metric optimise word error rate wer address mismatch propose use task specific loss train rnnlm discriminate multiple hypotheses within lattice rescoring scenario fine tune rnnlm lattices average edit distance loss show obtain nineteen relative improvement word error rate purely generatively train model
generate high quality text sufficient diversity essential wide range natural language generation nlg task maximum likelihood mle model train teacher force consistently report weak baselines poor performance attribute exposure bias bengio et al two thousand and fifteen ranzato et al two thousand and fifteen inference time model feed prediction instead grind truth token lead accumulate errors poor sample line reason lead outbreak adversarial base approach nlg account gans suffer exposure bias work make several surprise observations contradict common beliefs first revisit canonical evaluation framework nlg point fundamental flaw quality evaluation show one outperform metrics use simple well know temperature parameter artificially reduce entropy model conditional distributions second leverage control quality diversity trade give parameter evaluate model whole quality diversity spectrum find mle model constantly outperform propose gin variants whole quality diversity space result several implications one impact exposure bias sample quality less severe previously think two temperature tune provide better quality diversity trade adversarial train easier train easier cross validate less computationally expensive code reproduce experiment available githubcom pclucas14 gansfallingshort
due popularity availability social media data may present new way identify individuals experience mental illness analyse blog content study aim investigate associations linguistic feature symptoms depression generalise anxiety suicidal ideation study utilise longitudinal study design individuals blogged invite participate study complete fortnightly mental health questionnaires include phq9 gad7 period thirty-six weeks linguistic feature extract blog data use liwc tool bivariate multivariate analyse perform investigate correlations linguistic feature mental health score subject use multivariate regression model predict longitudinal change mood within subject total one hundred and fifty-three participants consent take part thirty-eight participants complete require number questionnaires blog post study period subject analysis reveal several linguistic feature include tentativeness non fluencies significantly associate depression anxiety symptoms suicidal thoughts within subject analysis show robust correlations linguistic feature change mental health score study provide support relationship linguistic feature within social media data symptoms depression anxiety lack robust within subject correlations indicate relationship observe group level may generalise individual change time
neural network natural language reason largely focus extractive fact base question answer qa common sense inference however also crucial understand extent neural network perform relational reason combinatorial generalization natural language abilities often obscure annotation artifacts dominance language model standard qa benchmarks work present novel benchmark dataset language understand isolate performance relational reason also present neural message pass baseline show model incorporate relational inductive bias superior combinatorial generalization compare traditional recurrent neural network approach
customer support central objective square help us build maintain great relationships sellers order provide best experience strive deliver accurate quasi instantaneous responses question regard products work introduce attention fusion network model combine signal extract seller interactions square product ecosystem along submit email question predict relevant solution seller inquiry show innovative combination two different data source rarely use together use state art deep learn systems outperform candidate model train single source
paper pregroup model natural languages relate explicitly categorical use pregroups compositional distributional semantics natural language process categorical interpretations make certain assumptions nature natural languages state formally may see impose strong restrictions pregroup grammars natural languages formalize hypothesis form pregroup model natural languages must take demonstrate artificial language example restrictions impose pregroup axioms compare contrast artificial language examples natural languages use welsh language noun type take primitive illustrative example hypothesis simply must exist causal connection information flow word sentence language whose purpose communicate information necessarily case formal languages simply generate series mean free rule impose restrictions type pregroup grammars expect find natural languages formalize algebraic categorical graphical term take preliminary step provide condition ensure pregroup model satisfy conjecture properties discuss general form hypothesis may take
paper aim apply notions quantum geometry correlation typification semantic relations couple keywords different document particular analyse texts classify hate non hate speeches contain keywords women white black paper compare approach cosine similarity classical methodology cast light notion similar mean
text classification fundamental task nlp applications latest research field largely divide two major sub field learn representations one sub field learn deeper model sequential convolutional connect back representation side posit idea stronger representation simpler classifier model need achieve higher performance paper propose completely novel direction text classification research wherein convert text representation similar image deep network able handle image equally able handle text take deeper look representation document image subsequently utilize simple convolution base model take computer vision domain image crop scale sample augment like image work state art large convolution base model design handle large image datasets show impressive result latest benchmarks relate field perform transfer learn experiment text text domain also image text domain believe paradigm shift way document understand text classification traditionally do drive numerous novel research ideas community
paper present discovery length entities various datasets follow family scale free power law distributions concept entity broadly include name entity entity mention time expression aspect term domain specific entity well investigate natural language process relate areas entity length denote number word entity power law distributions entity length possess scale free property well define mean finite variances explain phenomenon power laws entity length principle least effort communication preferential mechanism
query expansion method alleviate vocabulary mismatch problem present information retrieval task previous work show term select query expansion traditional methods pseudo relevance feedback always helpful retrieval process paper show also true recently propose embed base query expansion methods introduce artificial neural network classifier predict usefulness query expansion term classifier use term word embeddings input perform experiment four trec newswire web collections show use term select classifier expansion significantly improve retrieval performance compare competitive baselines result also show robust baselines
word embeddings key component high perform natural language process nlp systems remain challenge learn good representations novel word fly ie word occur train data general problem set word embeddings induce unlabeled train corpus model train embed novel word induce embed space currently two approach learn embeddings novel word exist learn embed novel word surface form eg subword n grams ii learn embed context occur paper propose architecture leverage source information surface form context show result large increase embed quality architecture obtain state art result definitional nonce contextual rare word datasets input require embed set unlabeled corpus train architecture produce embeddings appropriate induce embed space thus model easily integrate exist nlp system enhance capability handle novel word
exist methods determine relation type entities recognize thus interaction relation type entity mention fully model paper present novel paradigm deal relation extraction regard relate entities arguments relation apply hierarchical reinforcement learn hrl framework paradigm enhance interaction entity mention relation type whole extraction process decompose hierarchy two level rl policies relation detection entity extraction respectively feasible natural deal overlap relations model evaluate public datasets collect via distant supervision result show gain better performance exist methods powerful extract overlap relations
article propose adversarially train normalize noisy feature auto encoder atnnfae byte level text generation atnnfae consist auto encoder internal code normalize unit sphere corrupt additive noise simultaneously replica decoder share parameters ae decoder use generator feed random latent vectors adversarial discriminator train distinguish train sample reconstruct ae sample produce random input generator make entire generator discriminator path differentiable discrete data like text combine effect noise injection code share weight decoder generator prevent mode collapse phenomenon commonly observe gans since perplexity apply non sequential text generation propose new evaluation method use total variance distance frequencies hash cod byte level n grams ngtvd ngtvd single benchmark characterize quality diversity generate texts experiment offer six large scale datasets arabic chinese english comparisons n gram baselines recurrent neural network rnns ablation study noise level discriminator perform find rnns trouble compete n gram baselines atnnfae result generally competitive
pinyin character p2c conversion core component pinyin base chinese input method engine ime however conversion seriously compromise ambiguities chinese character correspond pinyin well predefined fix vocabularies alleviate inconvenience propose neural p2c conversion model augment online update vocabulary sample mechanism support open vocabulary learn ime work experiment show propose method outperform commercial imes state art traditional model standard corpus true inputting history dataset term multiple metrics thus online update vocabulary indeed help ime effectively follow user inputting behavior
knowledge graph embed active research topic knowledge base completion progressive improvement initial transe transh distmult et al current state art conve conve use 2d convolution embeddings multiple layer nonlinear feature model knowledge graph model efficiently train scalable large knowledge graph however structure enforcement embed space conve recent graph convolutional network gcn provide another way learn graph node embed successfully utilize graph connectivity structure work propose novel end end structure aware convolutional network sacn take benefit gcn conve together sacn consist encoder weight graph convolutional network wgcn decoder convolutional network call conv transe wgcn utilize knowledge graph node structure node attribute edge relation type learnable weight adapt amount information neighbor use local aggregation lead accurate embeddings graph nod node attribute graph represent additional nod wgcn decoder conv transe enable state art conve translational entities relations keep link prediction performance conve demonstrate effectiveness propose sacn standard fb15k two hundred and thirty-seven wn18rr datasets give ten relative improvement state art conve term hits1 hits3 hits10
concepts represent group different instance share common properties essential information knowledge representation conventional knowledge embed methods encode entities concepts instance relations vectors low dimensional semantic space equally ignore difference concepts instance paper propose novel knowledge graph embed model name transc differentiate concepts instance specifically transc encode concept knowledge graph sphere instance vector semantic space use relative position model relations concepts instance ie instanceof relations concepts sub concepts ie subclassof evaluate model link prediction triple classification task dataset base yago experimental result show transc outperform state art methods capture semantic transitivity instanceof subclassof relation cod datasets obtain https githubcom davidlvxin transc
bipolar disorder illness characterize manic depressive episodes affect sixty million people worldwide present preliminary study bipolar disorder prediction user generate text reddit rely users self report label benchmark classifiers bipolar disorder prediction outperform baselines reach accuracy f1 score eighty-six feature analysis show interest differences language use users bipolar disorder control group include differences use emotion expressive word
subword model zero resource languages aim learn low level representations speech audio without use transcriptions resources target language text corpora pronunciation dictionaries good representation capture phonetic content abstract away type variability speaker differences channel noise previous work area primarily focus unsupervised learn target language data evaluate intrinsically directly compare multiple methods include use target language speech data use transcribe speech non target languages evaluate use two intrinsic measure well downstream unsupervised word segmentation cluster task find combine two exist target language methods yield better feature either method alone nevertheless even better result obtain extract target language bottleneck feature use model train languages cross lingual train use one language enough provide benefit multilingual train help even addition result hold across intrinsic measure extrinsic task discuss qualitative differences different type learn feature
provide approach generate beautiful poetry sonnet generation algorithm include several novel elements improve state art lead metrical rhyme poetry many human like qualities novel elements include line punctuation part speech restrictions appropriate train corpora work winner two thousand and eighteen poetix literary turing test award computer generate poetry
question convey information questioner namely one know paper propose novel approach allow learn agent ask consider tricky predict course produce final output analyze ask make model transparent interpretable first develop idea propose general framework deep neural network ask question call ask network specific architecture train process ask network propose task colorization exemplar one many task thus task ask question helpful perform task accurately result show model learn generate meaningful question ask difficult question first utilize provide hint efficiently baseline model conclude propose ask framework make learn agent reveal weaknesses pose promise new direction develop interpretable interactive model
recurrent neural network rnns widely use process natural language task achieve huge success traditional rnns usually treat token sentence uniformly equally however may miss rich semantic structure information sentence useful understand natural languages since semantic structure word dependence pattern parameterized challenge capture leverage structure information paper propose improve variant rnn multi channel rnn mc rnn dynamically capture leverage local semantic structure information concretely mc rnn contain multiple channel represent local dependence pattern time attention mechanism introduce combine pattern step accord semantic information parameterize structure information adaptively select appropriate connection structure among channel way diverse local structure dependence pattern sentence well capture mc rnn verify effectiveness mc rnn conduct extensive experiment typical natural language process task include neural machine translation abstractive summarization language model experimental result task show significant improvements mc rnn current top systems
first robotic platforms slowly approach everyday life imagine near future service robots easily accessible non expert users vocal interfaces capability manage natural language would indeed speed process integrate platform ordinary life semantic parse fundamental task natural language understand process allow extract mean user utterance use machine paper present preliminary study semantically parse user vocal command house service robot use multi layer long short term memory neural network attention mechanism system train human robot interaction corpus preliminarily compare previous approach
translate natural language sql query table base question answer challenge problem receive significant attention research community work extend pointer generator investigate order matter problem semantic parse sql even though model straightforward extension general purpose pointer generator outperform early work wikisql remain competitive concurrently introduce complex model moreover provide deeper investigation potential order matter problem could arise due multiple correct decode paths investigate use reinforce well dynamic oracle context
neural network representation learn frameworks recently show highly effective wide range task range radiography interpretation via data drive diagnostics clinical decision support often superior performance come price dramatically increase train data requirements satisfy every give institution scenario mean counter data sparsity effect distant supervision alleviate need scarce domain data rely relate resource rich task train study present end end neural clinical decision support system recommend relevant literature individual patients available resources via distant supervision well know mimic iii collection abundant resource experiment show significant improvements retrieval effectiveness traditional statistical well purely locally supervise retrieval model
semantic annotation process identify key phrase texts link concepts knowledge base important basis semantic information retrieval semantic web uptake despite emergence semantic annotation systems comparative study publish performance paper provide evaluation performance exist systems three task full semantic annotation name entity recognition keyword detection specifically spot capability recognition relevant surface form text evaluate three task whereas disambiguation correctly associate entity wikipedia dbpedia spot surface form evaluate first two task evaluation twofold first compute standard precision recall output semantic annotators diverse datasets best suit one identify task second build statistical model use logistic regression identify significant performance differences result show systems provide full annotation perform better name entities annotators keyword extractors three task however still much room improvement identification relevant entities describe text
extract valuable facts informative summaries multi dimensional table ie insight mine important task data analysis business intelligence however rank importance insights remain challenge unexplored task main challenge explicitly score insight give rank require thorough understand table cost lot manual efforts lead lack available train data insight rank problem paper propose insight rank model consist two part neural rank model explore data characteristics header semantics data statistical feature memory network model introduce table structure context information rank process also build dataset text assistance experimental result show approach largely improve rank precision report multi evaluation metrics
distant supervise relation extraction successfully apply large corpus thousands relations however inevitable wrong label problem distant supervision hurt performance relation extraction paper propose method neural noise converter alleviate impact noisy data conditional optimal selector make proper prediction noise converter learn structure transition matrix logit level capture property distant supervise relation extraction dataset conditional optimal selector hand help make proper prediction decision entity pair even group sentence overwhelm relation sentence conduct experiment widely use dataset result show significant improvement competitive baseline methods
global popularity microblogs lead increase accumulation large volumes text data microblogging platforms twitter corpora untapped resources understand social expressions diverse subject microblog analysis aim unlock value expressions discover insights events significance hide among swathe text besides velocity diversity content brevity absence structure time sensitivity key challenge microblog analysis paper propose unsupervised incremental machine learn event detection technique address challenge propose technique separate microblog discussion topics address key problem diversity maintain record evolution topic time brevity time sensitivity unstructured nature address individual topic pathways contribute generate temporal topic drive structure microblog discussion propose event detection method continuously monitor topic pathways use multiple domain independent event indicators events significance autonomous nature topic separation topic pathway generation new topic identification event detection appropriate propose technique extensive applications microblog analysis demonstrate capabilities tweet contain microsoft tweet contain obama
categorical compositional distributional semantics ccds allow one compute mean phrase sentence mean constituent word type structure carry traditional categorial model grammar la lambek become wire structure mediate interaction word mean however ccds much richer logical structure plain categorical semantics certain word also give internal wire either provide entire mean reduce size mean space previous examples internal wire include relative pronouns intersective adjectives establish large class well behave transitive verbs refer cartesian verbs reduce mean space ternary tensor unary one experimental evidence also provide
development e commerce review website comment information influence people life users share consumption experience evaluate quality commodity comment people make decision refer comment dependency comment make fake comment appear fake comment profit bad motivation business fabricate untrue consumption experience preach slander products fake comment easy mislead users opinion decision accuracy humans identify fake comment low meaningful detect fake comment use natural language process technology people get true comment information paper use sentimental analysis detect fake comment
paper describe submission e2e nlg challenge recently neural seq2seq approach become mainstream nlg often resort pre respectively post process delexicalization relexicalization step word level handle rare word contrast train simple character level seq2seq model require pre post process delexicalization tokenization even lowercasing surprisingly good result improvement explore two rank approach score candidates also introduce synthetic dataset creation procedure open new way create artificial datasets natural language generation
paper present lisp architecture portable nlp system term lapnlp process clinical note lapnlp integrate multiple standard customize house develop nlp tool system facilitate portability across different institutions data systems incorporate enrich common data model cdm standardize necessary data elements utilize umls perform domain adaptation integrate generic domain nlp tool also feature stand annotations specify positional reference original document build interval tree base search engine efficiently query retrieve stand annotations specify positional requirements also develop utility convert inline annotation format stand annotations enable reuse clinical text datasets inline annotations experiment system several nlp facilitate task include computational phenotyping lymphoma patients semantic relation extraction clinical note experiment showcased broader applicability utility lapnlp
automatic development phenotype algorithms electronic health record data machine learn ml techniques great interest give current practice time consume resource intensive extraction design pattern phenotype algorithms essential understand rationale standard great potential automate development process pilot study perform network visualization design pattern associations phenotypes sit classify design pattern use fragment previously annotate phenotype algorithms grind truth classification performance use proxy coherence attribution level bag word representation knowledge base feature generate good performance classification task seventy-nine macro f1 score good classification accuracy simple feature demonstrate attribution coherence feasibility automatic identification design pattern result point feasibility challenge automatic identification phenotyping design pattern would power automatic development phenotype algorithms
logic base approach reason task recognize textual entailment rte important system large amount knowledge data however tradeoff add knowledge data improve rte performance maintain efficient rte system big database problematic term memory usage computational complexity work show process time state art logic base rte system significantly reduce replace search base axiom injection abduction mechanism base knowledge base completion kbc integrate mechanism coq plugin provide proof automation tactic natural language inference additionally show empirically add new knowledge data contribute better rte performance harm process speed framework
recent speech synthesis systems base sample autoregressive neural network model generate speech almost undistinguishable human record however model require large amount data paper show lack data one speaker compensate data speakers naturalness tacotron2 like model train blend 5k utterances seven speakers better speaker dependent model train 15k utterances term stability multi speaker model always stable also demonstrate model mix one thousand, two hundred and fifty utterances target speaker 5k utterances another six speakers produce significantly better quality state art dnn guide unit selection systems train ten time data target speaker
since advent web amount data wen increase several million fold recent years web data generate data store years one important data format text answer user query internet overcome problem information overload one possible solution text document summarization reduce query access time also optimize document result accord specific users requirements summarization text document categorize abstractive extractive work do direction extractive summarization extractive summarize result subset original document objective content coverage lea redundancy work base extractive approach first approach use statistical feature semantic base feature include sentiment feature idea cache view emotion play important role effectively convey message may play vital role text document summarization
duplicate question detection ongoing challenge community question answer semantically equivalent question significantly different word structure addition identification duplicate question reduce resources require retrieval question repeat study compare performance deep neural network gradient tree boost explore possibility domain adaptation transfer learn improve perform target domains text pair duplicate classification task use three heterogeneous datasets general purpose quora technical ask ubuntu academic english stack exchange ultimately study expose alternative hypothesis mean duplicate inherently general purpose rather dependent domain learn hence reduce chance transfer learn adapt domain
neural conversation model attractive one train model directly dialog examples minimal label small amount data however often fail generalize test data since tend capture spurious feature instead semantically meaningful domain knowledge address issue propose novel approach allow human teachers transfer domain knowledge conversation model form natural language rule test method three different dialog datasets improve performance across domains demonstrate efficacy propose method
success natural language inference nli require model understand lexical compositional semantics however adversarial evaluation find several state art model diverse architectures rely former fail use latter compositionality unawareness reflect via standard evaluation current datasets show remove rnns exist model shuffle input word train induce large performance loss despite explicit removal compositional information therefore propose compositionality sensitivity test setup analyze model natural examples exist datasets solve via lexical feature alone ie bag word model give high probability one wrong label hence reveal model actual compositionality awareness show setup highlight limit compositional ability current nli model also differentiate model performance base design eg separate shallow bag word model deeper linguistically ground tree base model evaluation setup important analysis tool complement currently exist adversarial linguistically drive diagnostic evaluations expose opportunities future work evaluate model compositional understand
increase concern misinformation stimulate research efforts automatic fact check recently release fever dataset introduce benchmark fact verification task system ask verify claim use evidential sentence wikipedia document paper present connect system consist three homogeneous neural semantic match model conduct document retrieval sentence selection claim verification jointly fact extraction verification evidence retrieval document retrieval sentence selection unlike traditional vector space ir model query source match pre design term vector space develop neural model perform deep semantic match raw textual input assume intermediate term representation access structure external knowledge base also show pageview frequency also help improve performance evidence retrieval result later match use neural semantic match network claim verification unlike previous approach simply fee upstream retrieve evidence claim natural language inference nli model enhance nli model provide internal semantic relatedness score hence integrate evidence retrieval modules ontological wordnet feature experiment fever dataset indicate one neural semantic match method outperform popular tf idf encoder model significant margins evidence retrieval metrics two additional relatedness score wordnet feature improve nli model via better semantic awareness three formalize three subtasks similar semantic match problem improve three stag complete model able achieve state art result fever test set
anonymity form integral important part digital life enable us express true selves without fear judgment paper investigate different aspects anonymity social qanda site quora choice quora motivate fact one rare social qanda sit allow users explicitly post anonymous question activity forum become normative rather taboo analysis fifty-one million question observe global scale almost difference manifest linguistic structure anonymous non anonymous question find topical mix global scale primary reason absence however differences start feature deep dive topically cluster question compare cluster high volumes anonymous question low volumes anonymous question particular observe choice post question anonymous dependent user perception anonymity often choose speak depression anxiety social tie personal issue guise anonymity perform personality trait analysis observe anonymous group users positive correlation extraversion agreeableness negative correlation openness subsequently gain insights build anonymity grid identify differences perception anonymity user post question community users answer also look first response time question observe lowest topics talk personal sensitive issue hint toward higher degree community support user engagement
paper introduce novel pattern match neural network architecture use neighbor similarity score feature eliminate need feature engineer disfluency detection task evaluate approach disfluency detection four different speech genres show approach effective hand engineer pattern match feature use domain data achieve superior performance cross domain scenarios
body literature demonstrate users mental health condition depression anxiety predict social media language still gap scientific understand psychological stress express social media stress one primary underlie cause correlate chronic physical illnesses mental health condition paper explore language psychological stress dataset six hundred and one social media users answer perceive stress scale questionnaire also consent share facebook twitter data firstly find stress users post exhaustion lose control increase self focus physical pain compare post breakfast family time travel users stress secondly find facebook language predictive stress twitter language thirdly demonstrate language base model thus develop adapt scale measure county level trend since county level language easily available twitter use stream api explore multiple domain adaptation algorithms adapt user level facebook model twitter language find domain adapt scale social media base measurements stress outperform sociodemographic variables age gender race education income grind truth survey base stress measurements user county level yous twitter language score higher stress also predictive poorer health less access facilities lower socioeconomic status counties conclude discussion implications use social media new tool monitor stress level individuals counties
paper introduce temporal framework detect cluster emergent viral topics social network endogenous exogenous influence develop viral content explore use cluster method base user behavior social network dataset twitter api result discuss introduce metrics popularity burstiness relevance score result show clear distinction characteristics develop content two class users
rank function information retrieval often use search engines recommend relevant answer query paper make use notion information retrieval apply onto problem domain cognate detection main contributions paper one positional segmentation incorporate sequential notion two graphical error model deduce transformations current research work focus classification problem distinguish whether pair word cognates paper focus harder problem whether could predict possible cognate give input study show language model smooth methods apply retrieval function use conjunction positional segmentation error model give better result compete baselines classification prediction cognates source code https githubcom pranav ust cognates
although neural machine translation nmt model advance state art performance machine translation face problems like inadequate translation attribute standard maximum likelihood estimation mle judge real translation quality due several limitations work propose adequacy orient learn mechanism nmt cast translation stochastic policy reinforcement learn rl reward estimate explicitly measure translation adequacy benefit sequence level train rl strategy accurate reward design specifically translation model outperform multiple strong baselines include one standard coverage augment attention model mle base train two advance reinforcement adversarial train strategies reward base word level bleu character level chrf3 quantitative qualitative analyse different language pair nmt architectures demonstrate effectiveness universality propose approach
joint embeddings medical image modalities associate radiology report potential offer significant benefit clinical community range cross domain retrieval conditional generation report broader goals multimodal representation learn work establish baseline joint embed result measure via local global retrieval methods soon release mimic cxr dataset consist chest x ray image associate radiology report examine supervise unsupervised methods task show document retrieval task learn representations limit amount supervision need yield result comparable fully supervise methods
humans convey intentions usage verbal nonverbal behaviors face face communication speaker intentions often vary dynamically depend different nonverbal contexts vocal pattern facial expressions result model human language essential consider literal mean word also nonverbal contexts word appear better model human language first model expressive nonverbal representations analyze fine grain visual acoustic pattern occur word segment addition seek capture dynamic nature nonverbal intents shift word representations base accompany nonverbal behaviors end propose recurrent attend variation embed network raven model fine grain structure nonverbal subword sequence dynamically shift word representations base nonverbal cue propose model achieve competitive performance two publicly available datasets multimodal sentiment analysis emotion recognition also visualize shift word representations different nonverbal contexts summarize common pattern regard multimodal variations word representations
entity type classification define task assign category label entity mention document neural network recently improve classification general entity mention pattern match systems continue use classify personal data entities eg classify organization media company government institution gdpr hipaa compliance propose neural model expand class personal data entities classify fine grain level use output exist pattern match systems additional contextual feature introduce new resources personal data entities hierarchy one hundred and thirty-four type two datasets wikipedia page elect representatives enron email hope resource aid research area personal data discovery effect provide baseline result datasets compare method state art model ontonotes dataset
text classification one fundamental task natural language process recently deep neural network achieve promise performance text classification task compare shallow model despite significance deep model ignore fine grain match signal word class classification clue since classifications mainly rely text level representations address problem introduce interaction mechanism incorporate word level match signal text classification task particular design novel framework explicit interaction model dub exam equip interaction mechanism justify propose approach several benchmark datasets include multi label multi class text classification task extensive experimental result demonstrate superiority propose method byproduct release cod parameter settings facilitate research
paper focus sentiment mine sentiment correlation analysis web events although neural network model contribute lot mine text information little attention pay analysis inter sentiment correlations paper fill gap sentiment calculation inter sentiment correlations paper social emotion divide six categories love joy anger sadness fear surprise two deep neural network model present sentiment calculation three datasets title body comment news article collect cover objective subjective texts vary lengths long short dataset three kinds feature extract explicit expression implicit expression alphabet character performance two model analyze respect three kinds feature controversial phenomenon interpretation anger fn love gd subjective text emotions easily consider anger contrast objective news body title easy regard text cause love gd mean journalist may want arouse emotion love write news anger news publish result reflect sentiment complexity unpredictability
use future contextual information typically show helpful acoustic model recently propose rnn model call minimal gate recurrent unit input projection mgruip context module namely temporal convolution specifically design model future context model mgruip context module mgruip ctx show able utilize future context effectively meanwhile quite low model latency computation cost paper continue improve mgruip ctx two revisions apply bn methods enlarge model context experimental result two mandarin asr task eight thousand, four hundred hours 60k hours show revise mgruip ctx outperform lstm large margin eleven thirty-eight even perform slightly better superior blstm 8400h task 33m less parameters 290ms model latency
many dialogue management frameworks allow system designer directly define belief rule implement efficient dialog policy rule directly define components say hand craft dialogues become complex number state transition policy decisions become large facilitate dialog policy design process propose approach automatically learn belief rule use supervise machine learn approach validate ideas student advisor conversation domain extract latent beliefs like student curious confuse neutral etc also perform epistemic reason help tailor dialog accord student emotional state hence improve overall effectiveness dialog system latent belief identification approach show accuracy eighty-seven result efficient meaningful dialog management
gas measurement unit computational effort take execute every single operation take part ethereum blockchain platform instruction execute ethereum virtual machine evm associate gas consumption specify ethereum transaction exceed amount gas allot user know gas limit gas exception raise wide family contract vulnerabilities due gas behaviours report design implementation gastap gas aware smart contract analysis platform take input smart contract either evm disassemble evm solidity source code automatically infer sound gas upper bound public function bound ensure gas limit pay user higher infer gas bound contract free gas vulnerabilities
diagnostic reason key component many professions improve students diagnostic reason skills educational psychologists analyse give feedback epistemic activities use students diagnose particular hypothesis generation evidence generation evidence evaluation draw conclusions however manual analysis highly time consume aim enable large scale adoption diagnostic reason analysis feedback automate epistemic activity identification create first corpus task comprise diagnostic reason self explanations students two domains annotate epistemic activities base insights corpus creation task characteristics discuss three challenge automatic identification epistemic activities use ai methods correct identification epistemic activity span reliable distinction similar epistemic activities detection overlap epistemic activities propose separate performance metric challenge thus provide evaluation framework future research indeed evaluation various state art recurrent neural network architectures reveal current techniques fail address challenge
current caption approach describe image use black box architectures whose behavior hardly controllable explainable exterior image describe infinite ways depend goal context hand higher degree controllability need apply caption algorithms complex scenarios paper introduce novel framework image caption generate diverse descriptions allow ground controllability give control signal form sequence set image regions generate correspond caption recurrent architecture predict textual chunk explicitly ground regions follow constraints give control experiment conduct flickr30k entities coco entities extend version coco add ground annotations collect semi automatic manner result demonstrate method achieve state art performances controllable image caption term caption quality diversity code annotations publicly available https githubcom aimagelab show control tell
embed model deterministic knowledge graph kg extensively study purpose capture latent semantic relations entities incorporate structure knowledge machine learn however many kgs model uncertain knowledge typically model inherent uncertainty relations facts confidence score embed uncertain knowledge represent unresolved challenge capture uncertain knowledge benefit many knowledge drive applications question answer semantic search provide natural characterization knowledge paper propose novel uncertain kg embed model ukge aim preserve structural uncertainty information relation facts embed space unlike previous model characterize relation facts binary classification techniques ukge learn embeddings accord confidence score uncertain relation facts enhance precision ukge also introduce probabilistic soft logic infer confidence score unseen relation facts train propose evaluate two variants ukge base different learn objectives experiment conduct three real world uncertain kgs via three task ie confidence prediction relation fact rank relation fact classification ukge show effectiveness capture uncertain knowledge achieve promise result task consistently outperform baselines task
study problem provide recommend responses customer service agents live chat dialogue systems smart reply systems widely apply real world applications eg gmail linkedin message successfully recommend reactive responses however observe major limitation current methods generally difficulties suggest proactive investigation act eg perhaps another account us due lack long term context information indeed act critical step customer service agents collect information resolve customers issue thus work propose end end method special focus suggest proactive investigative question customer agents airbnb customer service live chat system effectiveness propose method validate qualitative quantitative result
visual understand go well beyond object recognition one glance image effortlessly imagine world beyond pixels instance infer people action goals mental state task easy humans tremendously difficult today vision systems require higher order cognition commonsense reason world formalize task visual commonsense reason give challenge question image machine must answer correctly provide rationale justify answer next introduce new dataset vcr consist 290k multiple choice qa problems derive 110k movie scenes key recipe generate non trivial high quality problems scale adversarial match new approach transform rich annotations multiple choice question minimal bias experimental result show humans find vcr easy ninety accuracy state art vision model struggle forty-five move towards cognition level understand present new reason engine recognition cognition network r2c model necessary layer inferences ground contextualization reason r2c help narrow gap humans machine sixty-five still challenge far solve provide analysis suggest avenues future work
question answer qa systems provide easy access vast amount knowledge without know underlie complex structure knowledge research community provide ad hoc solutions key qa task include name entity recognition disambiguation relation extraction query build furthermore integrate compose components implement many task automatically efficiently however general exist solutions limit simple short question still address complex question compose several sub question exploit answer complex question challenge require integrate knowledge unstructured data source ie textual corpus well structure data source ie knowledge graph paper approach hcqa introduce deal complex question require federate knowledge hybrid heterogeneous data source structure unstructured contribute develop decomposition mechanism extract sub question potentially long complex input question ii novel comprehensive schema first kind extract annotate relations iii approach execute aggregate answer sub question evaluation hcqa show superior accuracy fundamental task relation extraction well federation task
cryptocurrency attract attention blockchain technology ethereum gain significant popularity blockchain community mainly due fact design way enable developers write smart contract decentralize applications dapps many kinds cryptocurrency information social network risk fraud problems behind push many countries include unite state south korea china make warn set correspond regulations however security ethereum smart contract gain much attention deep learn approach propose method sentiment analysis ethereum community comment research first collect users cryptocurrency comment social network feed lstm cnn model train make prediction sentiment analysis research result demonstrate precision recall sentiment analysis achieve eighty importantly deploy sentiment analysis1 ratingtoken coin master mobile application cheetah mobile blockchain security center23 effectively provide detail information resolve risk fake fraud problems
unsupervised neural machine translation unmt recently achieve remarkable result large monolingual corpora language however uncertainty associate target source sentence make unmt theoretically ill pose problem work investigate possibility utilize image disambiguation improve performance unmt assumption intuitively base invariant property image ie description visual content different languages approximately similar propose unsupervised multi modal machine translation umnmt framework base language translation cycle consistency loss conditional image target learn bidirectional multi modal translation simultaneously alternate train multi modal uni modal inference model translate without image widely use multi30k dataset experimental result approach significantly better text unmt two thousand and sixteen test dataset
work consider medical concept normalization problem ie problem map disease mention free form text concept control vocabulary usually standard thesaurus unify medical language system umls task challenge since medical terminology different come health care professionals general public form social media texts approach sequence learn problem recurrent neural network train obtain semantic representations one multi word expressions develop end end neural architectures tailor specifically medical concept normalization include bidirectional lstm gru attention mechanism additional semantic similarity feature base umls evaluation standard benchmark show model improve state art baseline classification base cnns
machine learn base dialogue managers able learn complex behaviors order complete task straightforward extend capabilities new domains investigate different policies ability handle uncooperative user behavior well expertise complete one task restaurant reservations reapplied learn new one eg book hotel introduce recurrent embed dialogue policy redp embed system action dialogue state vector space redp contain memory component attention mechanism base modify neural turing machine significantly outperform baseline lstm classifier task also show architecture baseline solve babi dialogue task achieve one hundred test accuracy
hypoglycemia common potentially dangerous among treat diabetes electronic health record ehrs important resources hypoglycemia surveillance study report development evaluation deep learn base natural language process systems automatically detect hypoglycemia events ehr narratives experts public health annotate five hundred ehr note patients diabetes use annotate dataset train evaluate hype supervise nlp systems hypoglycemia detection experiment convolutional neural network model yield promise performance precision096 pm three recall086 pm three f1091 pm three ten fold cross validation set despite annotate data highly imbalanced cnn base hype system still achieve high performance hypoglycemia detection hype could use ehr base hypoglycemia surveillance facilitate clinicians timely treatment high risk patients
numerous model ground language understand recently propose include generic model easily adapt give task ii intuitively appeal modular model require background knowledge instantiate compare type model much lend particular form systematic generalization use synthetic vqa test evaluate model capable reason possible object pair train small subset find show generalization modular model much systematic highly sensitive module layout ie exactly modules connect furthermore investigate modular model generalize well could make end end learn layout parametrization find end end methods prior work often learn inappropriate layouts parametrizations facilitate systematic generalization result suggest addition modularity systematic generalization language understand may require explicit regularizers priors
goal orient dialogue systems typically rely components specifically develop single task domain limit systems two different ways update task domain dialogue system usually need update completely train also harder extend dialogue systems different multiple domains dialogue state tracker conventional dialogue systems one component usually design fit well define application domain example common state variable categorical distribution manually predefined set entities henderson et al two thousand and thirteen result inflexible hard extend dialogue system paper propose new approach dialogue state track generalize well multiple domains without incorporate domain specific knowledge framework discrete dialogue state variables learn independently information predefined set possible value dialogue state variables require furthermore enable add arbitrary dialogue context feature allow multiple value associate single state variable characteristics make much easier expand dialogue state space evaluate framework use widely use dialogue state track challenge data set dstc2 show framework yield competitive result state art result despite incorporate little domain knowledge also show framework benefit widely available external resources pre train word embeddings
discourse structure beneficial various nlp task dialogue understand question answer sentiment analysis paper present deep sequential model parse discourse dependency structure multi party dialogues propose model aim construct discourse dependency tree predict dependency relations construct discourse structure jointly alternately make sequential scan elementary discourse units edus dialogue edu model decide previous edu current one link correspond relation type predict link relation type use build discourse structure incrementally structure encoder link prediction relation classification model utilize local information represent concern edus also global information encode edu sequence discourse structure already build current step experiment show propose model outperform state art baselines
order bring artificial agents live need go beyond supervise learn close datasets ability continuously expand knowledge inspire student learn classroom present agent continuously learn pose natural language question humans agent compose three interact modules one perform caption another generate question decision maker learn ask question implicitly reason uncertainty agent expertise teacher compare current active learn methods query image full caption agent able ask point question improve generate caption agent train improve caption expand knowledge show approach achieve better performance use less human supervision baselines challenge mscoco dataset
amount dialogue history include conversational agent often underestimate set empirical thus possibly naive way suggest principled investigations optimal context windows urgently need give amount dialogue history correspond representations play important role overall performance conversational system paper study amount history require conversational agents reliably predict dialogue reward task dialogue reward prediction choose investigate effect vary amount dialogue history impact system performance experimental result use dataset 18k human human dialogues report lengthy dialogue histories least ten sentence prefer twenty-five sentence best experiment short ones lengthy histories useful train dialogue reward predictors strong positive correlations target dialogue reward predict ones
detect controversy general web page daunt task increasingly essential efficiently moderate discussions effectively filter problematic content unfortunately controversies occur across many topics domains great change time paper investigate neural classifiers robust methodology controversy detection general web page current model often cast controversy detection general web page wikipedia link exact lexical match task diverse change nature controversies suggest semantic approach better able detect controversy train neural network capture semantic information texts use weak signal data leverage semantic properties word embeddings robustly improve exist controversy detection methods evaluate model stability time unseen topics asses model performance vary train condition test cross temporal cross topic cross domain performance annotator congruence demonstrate weak signal base neural approach closer human estimate controversy robust inherent variability controversies
user representations routinely use recommendation systems platform developers target advertisements marketers public policy researchers gauge public opinion across demographic group computer scientists consider problem infer user representations abstractly one extract stable user representation effective many downstream task medium noisy complicate social media quality user representation ultimately task dependent eg improve classifier performance make accurate recommendations recommendation system proxies less sensitive specific task representation predictive latent properties person demographic feature socioeconomic class mental health state predictive user future behavior thesis begin show user representations learn multiple type user behavior social media apply several extensions generalize canonical correlation analysis learn representations evaluate three task predict future hashtag mention friending behavior demographic feature show user feature employ distant supervision improve topic model fit finally show user feature integrate improve exist classifiers multitask learn framework treat user representations grind truth gender mental health feature auxiliary task improve mental health state prediction also use distribute user representations learn first chapter improve tweet level stance classifiers show distant user information inform classification task granularity single message
review radiology report emergency departments essential laborious task timely follow patients abnormal case radiology report may dramatically affect patient outcome especially discharge different initial diagnosis machine learn approach devise expedite process detect case demand instant follow however approach require large amount label data train reliable predictive model prepare large dataset need manually annotate health professionals costly time consume paper investigate semi supervise learn framework radiology report classification across three hospitals main goal leverage clinical unlabeled data order augment learn process limit label data available improve classification performance also integrate transfer learn technique semi supervise learn pipeline experimental find show one convolutional neural network cnns independent problem specific feature engineer achieve significantly higher effectiveness compare conventional supervise learn approach two leverage unlabeled data train cnn base classifier reduce dependency label data fifty reach performance fully supervise cnn three transfer knowledge gain available label data external source hospital significantly improve performance semi supervise cnn model fully supervise counterparts target hospital
quantify differences terminologies various academic domains longstanding problem yet solve propose computational approach analyze linguistic variation among scientific research field capture semantic change term base neural language model model train large collection literature five computer science research field obtain field specific vector representations key term global vector representations word several quantitative approach introduce identify term whose semantics drastically change remain unchanged across different research field also propose metric quantify overall linguistic variation research field quantitative evaluation human annotate data qualitative comparison methods show model improve cross disciplinary data collaboration identify term potentially induce confusion interdisciplinary study
paper describe tartan conversational agent build two thousand and eighteen alexa prize competition tartan non goal orient socialbot focus around provide users engage fluent casual conversation tartan key feature include emphasis structure conversation base flexible finite state model approach focus understand use conversational act provide engage conversations tartan blend script like yet dynamic responses data base generative retrieval model unique tartan dialog manager model dynamic finite state machine knowledge conversational agent implementation follow specific structure
zipf law predict power law relationship word rank frequency language communication systems widely report variety natural language process applications however emergence natural language often model function bias speaker listener interest lack direct way relate information theoretic bias zipfian rank function bias also serve unintuitive interpretation communicative effort exchange speaker listener counter shortcomings propose novel integral transform kernel map communicative bias function correspond word frequency rank representations arbitrary phase transition point result direct way link communicative effort model speaker listener bias specific vocabulary use represent word rank demonstrate practical utility integral transform show change bias rank result greater accuracy performance image classification task assign word label image randomly subsampled cifar10 model task reinforcement learn game speaker listener compare relative impact bias zipfian word rank communicative performance accuracy two agents
today digital age dawn era big data analytics information link information entities action define discourse textual data either available internet line like newspaper data wikipedia dump etc basically connect information treat isolate wholesome semantics need automate retrieval process proper information extraction structure data relevant fast text analytics first big challenge conversion unstructured textual data structure data unlike databases graph databases handle relationships connections elegantly project aim develop graph base information extraction retrieval system
medical synonym identification important part medical natural language process nlp however field chinese medical synonym identification problems like low precision low recall rate solve problem paper propose method identify chinese medical synonyms first select thirteen feature include chinese english feature study synonym identification result feature alone different combinations feature comparison among identification result present optimal combination feature chinese medical synonym identification experiment show select feature achieve nine thousand, seven hundred and thirty-seven precision rate nine thousand, six hundred recall rate nine thousand, seven hundred and thirty-three f1 score
present medsim novel semantic similarity method base public well establish bio medical knowledge graph kgs large scale corpus study therapeutic substitution antibiotics besides hierarchy corpus kgs medsim interpret medicine characteristics construct multi dimensional medicine specific feature vectors dataset five hundred and twenty-eight antibiotic pair score doctor apply evaluation medsim produce statistically significant improvement semantic similarity methods furthermore promise applications medsim drug substitution drug abuse prevention present case study
ontology use interpretation natural language construct anti infective drug ontology one need design deploy methodological step carry entity discovery link medical synonym resources important part medical natural language process nlp however problems low precision low recall rate study nlp approach adopt generate candidate entities open ontology analyze extract semantic relations six word vector feature word level feature select perform entity link extraction result synonyms single feature different combinations feature study experiment show select feature achieve precision rate eight thousand, six hundred and seventy-seven recall rate eight thousand, nine hundred and three f1 score eight thousand, seven hundred and eighty-nine paper finally present structure propose ontology relevant statistical data
name entity discovery link fundamental core component question answer question entity discovery link qedl problem traditional methods challenge multiple entities one short question difficult discover entirely incomplete information short text make entity link hard implement overcome difficulties propose knowledge graph base solution qedl develop system consist question entity discovery qed module entity link el module method qed module tradeoff ensemble two methods one method base knowledge graph retrieval could extract entities question guarantee recall rate method base conditional random field crf improve precision rate el module treat rank problem learn rank ltr method feature semantic similarity text similarity entity popularity utilize extract make full use information short texts official dataset share qedl evaluation task approach could obtain six thousand, four hundred and forty-four f1 score qed six thousand, four hundred and eighty-six accuracy el rank 2nd place indicate practical use qedl problem
end end model e2e automatic speech recognition asr blend components traditional speech recognition system unify model although simplify train decode pipelines unify model hard adapt mismatch exist train test data work focus contextual speech recognition particularly challenge e2e model introduce significant mismatch train test data improve performance presence complex contextual information propose use class base language modelsclm populate class contextdependent information real time enable approach scale large number class members minimize search errors propose token pass decoder efficient token recombination e2e systems first time evaluate propose system general contextual asr achieve relative sixty-two word error ratewer reduction contextual asr without hurt performance general asr show propose method perform well without modification decode hyper parameters across task make general solution e2e asr
extraction relevant pathological term radiology report important correct image label generation disease population study letter compare performance know application program interface apis task thoracic abnormality extraction radiology report explore several medical domain specific annotation tool like medical text indexermti non medline mesh demandmod options generic natural language understand nlu api provide ibm cloud result show although mti mod intend extract medical term performance worst compare generic extraction api like ibm nlu finally train dnn base name entity recognition ner model extract key concept word radiology report model outperform medical specific generic api performance large margin result demonstrate inadequacy generic apis pathology extraction task establish importance domain specific model train improve result hope result motivate research community release larger de identify radiology report corpus build high accuracy machine learn model important task pathology extraction
task answer natural language question knowledge base receive wide attention recent years various deep learn architectures propose task however architectural design choices typically systematically compare evaluate condition paper contribute better understand impact architectural design choices evaluate four different architectures condition address task answer simple question consist predict subject predicate triple give question order provide fair comparison different architectures evaluate strategy infer subject compare different architectures infer predicate architecture infer subject base standard lstm model train recognize span subject question link component link subject span entity knowledge base architectures predicate inference base standard softmax classifier range predicate output iii model predict low dimensional encode property give entity representation question iii model learn score pair subject predicate give question well iv model base well know fasttext model comparison architectures show fasttext provide better result architectures
objective electronic health record ehr represent rich resource conduct observational study support clinical trials however much relevant information store unstructured format make difficult use natural language process approach attempt automatically classify data depend vectorization algorithms impose structure text algorithms design unique characteristics ehr propose new algorithm structure call free text may help researchers make better use ehr call method relevant word order vectorization rwov materials methods proof concept attempt classify hormone receptor status breast cancer patients treat university kansas medical center recent year unstructured text pathology report approach attempt account semi structure way healthcare providers often enter information compare approach ngrams word2vec methods result approach result consistently high accuracy measure f1 score area receiver operate characteristic curve auc discussion result suggest methods structure free text take account context may show better performance approach promise conclusion use method account fact healthcare providers tend use certain key word repetitively order key word important show improve performance methods
nowadays thank web twenty technologies people possibility generate spread content different social media easy way context evaluation quality information available online become crucial issue fact constant flow content generate every day often unknown source certify traditional authoritative entities require development appropriate methodologies evaluate systematic way content base objective aspects connect would help individuals nowadays tend increasingly form opinions base read online social media come contact information actually useful verify wikipedia nowadays one biggest online resources users rely source information amount collaboratively generate content send online encyclopedia every day let possible creation low quality article consequently misinformation properly monitor revise reason paper problem automatically assess quality wikipedia article consider particular focus analysis hand craft feature employ supervise machine learn techniques perform classification wikipedia article qualitative base respect prior literature wider set characteristics connect wikipedia article take account illustrate detail evaluations perform consider label dataset provide prior work different supervise machine learn algorithms produce encourage result respect consider feature
machine learn ml natural language process nlp achieve remarkable success many field bring new opportunities high expectation analyse medical data common type medical data massive free text electronic medical record emr widely regard mine massive data bring important information improve medical practice well possible new discoveries complex diseases however free emr texts lack consistent standards rich private information limit availability also accumulate everyday practice often hard balance number sample type diseases study problems hinder development ml nlp methods emr data analysis tackle problems develop model generate synthetic text emrs call medical text generative adversarial network mtgan base gin framework train reinforce algorithm take disease feature input generate synthetic texts emrs correspond diseases evaluate model micro level macro level application level chinese emr text dataset result show method good capacity fit real data generate realistic diverse emr sample provide novel way avoid potential leakage patient privacy still supply sufficient well control cohort data develop downstream ml nlp methods also use data augmentation method assist study base real emr data
mathematical expressions generate evaluate use train neural network model base transformer architecture expressions target analyze character level sequence transduction task encoder decoder build attention mechanisms three model train understand evaluate symbolic variables expressions mathematics one self attentive fee forward transformer without recurrence convolution two universal transformer recurrence three adaptive universal transformer recurrence adaptive computation time model respectively achieve test accuracies high seven hundred and sixty-one seven hundred and eighty-eight eight hundred and forty-nine evaluate expressions match target value case infer incorrectly result differ target one two character model notably learn add subtract multiply positive negative decimal number variable digits assign symbolic variables
work herein describe system automatic news category keyphrase label present context motivation improve speed user find relevant interest content within aggregation platform set twelve discrete categories apply five hundred thousand news article train neural network use facilitate depth task extract significant keyphrases latter do use three methods statistical graphical numerical use pre identify category label improve relevance extract phrase result present demo article pre populate via news api upon select category keyphrase label compute via methods explain herein
paper introduce chat crowd interactive environment visual layout composition via conversational interactions chat crowd support multiple agents two conversational roles agents play role designer charge place object editable canvas accord instructions command issue agents director role system integrate crowdsourcing platforms synchronous asynchronous data collection equip comprehensive quality control performance type agents expect system useful build multimodal goal orient dialog task require spatial geometric reason
stock market forecast important plan business activities stock price prediction attract many researchers multiple discipline include computer science statistics economics finance operations research recent study show vast amount online information public domain wikipedia usage pattern news stories mainstream media social media discussions observable effect investors opinions towards financial market reliability computational model stock market prediction important sensitive economy directly lead financial loss paper retrieve extract analyze effect news sentiments stock market main contributions include development sentiment analysis dictionary financial sector development dictionary base sentiment analysis model evaluation model gauge effect news sentiments stock pharmaceutical market use news sentiments achieve directional accuracy seven thousand and fifty-nine predict trend short term stock price movement
recent advancements area computer vision state art neural network give boost optical character recognition ocr accuracies however extract character text alone often insufficient relevant information extraction document also visual structure capture ocr extract information table chart footnote box head retrieve correspond structure representation document remain challenge find application large number real world use case paper propose novel enterprise base end end framework call deepreader facilitate information extraction document image via identification visual entities populate meta relational model across different entities document image model schema allow easy understand abstraction entities detect deep vision model relationships deepreader suite state art vision algorithms apply recognize handwritten print text eliminate noisy effect identify type document detect visual entities like table line box deep reader map extract entities rich relational schema capture relevant relationships entities word textboxes line etc detect document relevant information field extract document write sql query top relationship table natural language base interface add top relationship schema non technical user specify query natural language fetch information minimal effort paper also demonstrate many different capabilities deep reader report result real world use case
practice common find oneself far little text data train deep neural network big data wall represent challenge minority language communities internet organizations laboratories company compete gafam google amazon facebook apple microsoft research effort text data augmentation aim long term goal find end end learn solutions equivalent use neural network fee neural network engineer work focus use practical robust scalable easy implement data augmentation pre process techniques similar successful computer vision several text augmentation techniques experiment exist ones test comparison purpose noise injection use regular expressions others modify improve techniques like lexical replacement finally innovative ones generation paraphrase use back translation transformation syntactic tree base robust scalable easy use nlp cloud apis text augmentation techniques study amplification factor five increase accuracy result range forty-three two hundred and sixteen significant statistical fluctuations standardize task text polarity prediction standard deep neural network architectures test multilayer perceptron mlp long short term memory recurrent network lstm bidirectional lstm bilstm classical xgboost algorithm test twenty-five improvements
robots widely collaborate human users diferent task require high level cognitive function make able discover surround environment difcult challenge briefy highlight short paper infer latent grammatical structure language include ground part speech eg verbs nouns adjectives prepositions visual perception induction combinatory categorial grammar ccg phrase pave way towards ground phrase make robot able understand human instructions appropriately interaction
many researchers make use wikipedia network relatedness similarity task however approach use recent information historical change network provide analysis entity relatedness use temporal graph base approach different versions wikipedia article link network dbpedia open source knowledge base extract wikipedia consider create wikipedia article link network union intersection edge multiple time point present novel variation jaccard index weight edge base transience evaluate result kore dataset create two thousand and ten show use two thousand and ten wikipedia article link network produce strongest result suggest semantic similarity time sensitive show integrate multiple time frame methods give better overall similarity demonstrate temporal evolution important effect entity relatedness
present variational aspect base latent topic allocation valta family autoencoding topic model learn aspect base representations review valta define user item encoder map bag word vectors combine review associate pair user item onto structure embeddings turn define per aspect topic weight model individual review structure manner infer aspect assignment sentence give review per aspect topic weight obtain user item encoder serve define mixture topics condition aspect result autoencoding neural topic model review train fully unsupervised manner learn topics structure aspects experimental evaluation large number datasets demonstrate aspects interpretable yield higher coherence score non structure autoencoding topic model variants utilize perform aspect base comparison genre discovery
recent years recurrent neural network rnns base model apply slot fill problem speak language understand achieve state art performances paper investigate effect incorporate pre train language model rnn base slot fill model evaluation airline travel information system atis data corpus show significantly reduce size label train data achieve level slot fill performance incorporate extra word embed language model embed layer pre train unlabeled corpora
extract appropriate feature represent corpus important task textual mine previous attention base work usually enhance feature lexical level lack exploration feature augmentation sentence level paper exploit dynamic feature generation network dfgn solve problem specifically dfgn generate feature base variety attention mechanisms attach feature sentence representation thresholder design filter mine feature automatically dfgn extract significant characteristics datasets keep practicability robustness experimental result multiple well know answer selection datasets show propose approach significantly outperform state art baselines give detail analysis experiment illustrate dfgn provide excellent retrieval interpretative ability
neural abstractive summarization field conventional sequence sequence base model often suffer summarize wrong aspect document respect main aspect tackle problem propose task reader aware abstractive summary generation utilize reader comment help model produce better summary main aspect unlike traditional abstractive summarization task reader aware summarization confront two main challenge one comment informal noisy two jointly model news document reader comment challenge tackle challenge design adversarial learn model name reader aware summary generator rasg consist four components one sequence sequence base summary generator two reader attention module capture reader focus aspects three supervisor model semantic gap generate summary reader focus aspects four goal tracker produce goal generation step supervisor goal tacker use guide train framework adversarial manner extensive experiment conduct large scale real world text summarization dataset result show rasg achieve state art performance term automatic metrics human evaluations experimental result also demonstrate effectiveness module framework release large scale dataset research
significant progress make image caption task video description still infancy due complex nature video data generate multi sentence descriptions long videos even challenge among main issue fluency coherence generate descriptions relevance video recently reinforcement adversarial learn base methods explore improve image caption model however type methods suffer number issue eg poor readability high redundancy rl stability issue gans work instead propose apply adversarial techniques inference design discriminator encourage better multi sentence video description addition find multi discriminator hybrid design discriminator target one aspect description lead best result specifically decouple discriminator evaluate three criteria one visual relevance video two language diversity fluency three coherence across sentence approach result accurate diverse coherent multi sentence video descriptions show automatic well human evaluation popular activitynet caption dataset
network model increasingly use past years support summarization analysis narratives famous tv series book news inspire social network analysis model focus character play network model well capture character interactions give broad picture narration content work go beyond introduce additional semantic elements always capture single layer network contrast introduce work multilayer network model capture elements narration movie script people locations semantic elements model enable new measure insights movies demonstrate model two popular movies
e commerce platforms categorize products multi level taxonomy tree thousands leaf categories conventional methods product categorization typically base machine learn classification algorithms algorithms take product information input eg title descriptions classify product leaf category paper propose new paradigm base machine translation approach translate product natural language description sequence tokens represent root leaf path product taxonomy experiment two large real world datasets show approach achieve better predictive accuracy state art classification system product categorization addition demonstrate machine translation model propose meaningful new paths previously unconnected nod taxonomy tree thereby transform taxonomy direct acyclic graph dag discuss resultant taxonomy dag promote user friendly navigation adaptable new products
state art study demonstrate superiority joint model pipeline implementation medical name entity recognition normalization due mutual benefit two process exploit benefit sophisticate way propose novel deep neural multi task learn framework explicit feedback strategies jointly model recognition normalization one hand method benefit general representations task provide multi task learn hand method successfully convert hierarchical task parallel multi task set maintain mutual support task aspects improve model performance experimental result demonstrate method perform significantly better state art approach two publicly available medical literature datasets
many conversational agents market today follow standard bot development framework require train intent classifiers recognize user input need create proper set train examples often bottleneck development process many occasion agent developers access historical chat log provide good quantity well coverage train examples however cost label tens hundreds intents often prohibit take full advantage chat log paper present framework call textitsearch label propagate slp bootstrapping intents exist chat log use weak supervision framework reduce hours days label effort minutes work use search engine find examples rely data program approach automatically expand label report user study show positive user feedback new approach build conversational agents demonstrate effectiveness use data program auto label system develop train conversational agents framework broader application significantly reduce label effort train text classifiers
embeddings entities large knowledge base eg wikipedia highly beneficial solve various natural language task involve real world knowledge paper present wikipedia2vec python base open source tool learn embeddings word entities wikipedia propose tool enable users learn embeddings efficiently issue single command wikipedia dump file argument also introduce web base demonstration tool allow users visualize explore learn embeddings experiment tool achieve state art result kore entity relatedness dataset competitive result various standard benchmark datasets furthermore tool use key component various recent study publicize source code demonstration pretrained embeddings twelve languages https wikipedia2vecgithubio
progress image caption gradually get complex researchers try generalize model define representation visual feature natural language process work try define kind relationship form representation call tensor product representation tpr generalize scheme language model structure linguistic attribute relate grammar part speech language provide much better structure grammatically correct sentence tpr enable better unique representation structure feature space enable better sentence composition representations large part different ways define improve tpr discuss performance respect traditional procedures feature representations evaluate image caption application new model achieve considerable improvement correspond previous architectures
online proliferation hate speech urgent need systems detect harmful content paper present machine learn model develop automatic misogyny identification ami share task evalita two thousand and eighteen generate three type feature sentence embeddings tf idf vectors bow vectors represent tweet feature concatenate feed machine learn model model come first english subtask fifth english subtask b release win model public use available https githubcom punyajoy hateminers evalita
understand audio visual content ability informative conversation challenge areas intelligent systems audio visual scene aware dialog avsd challenge organize track dialog system technology challenge seven dstc7 propose combine task system answer question pertain video give dialogue previous question answer pair video propose task hierarchical encoder decoder model compute multi modal embed dialogue context first embed dialogue history use two lstms extract video audio frame regular intervals compute semantic feature use pre train i3d vggish model respectively summarize modalities fix length vectors use lstms use film block condition embeddings current question allow us reduce dimensionality considerably finally use lstm decoder train schedule sample evaluate use beam search compare modality fuse baseline model release avsd challenge organizers model achieve relative improvements sixteen score thirty-six bleu four thirty-three score nine hundred and ninety-seven cider
mobile keyboard suggestion typically regard word level language model problem centralize machine learn technique require massive user data collect train may impose privacy concern sensitive personal type data users federate learn fl provide promise approach learn private language model intelligent personalize keyboard suggestion train model distribute clients rather train central server obtain global model prediction exist fl algorithms simply average client model ignore importance client model aggregation furthermore optimization learn well generalize global model central server solve problems propose novel model aggregation attention mechanism consider contribution clients model global model together optimization technique server aggregation propose attentive aggregation method minimize weight distance server model client model iterative parameters update attend distance server model client model experiment two popular language model datasets social media dataset propose method outperform counterparts term perplexity communication cost settings comparison
speaker diarization audio stream turn particularly challenge apply fictional film many character talk various acoustic condition background music sound effect variations intonation despite acoustic variability movies exhibit specific visual pattern particularly within dialogue scenes paper introduce two step method achieve speaker diarization tv series speaker diarization first perform locally within scenes visually identify dialogues hypothesize local speakers compare second cluster process order detect recur speakers second stage cluster subject constraint different speakers involve dialogue assign different cluster performances approach compare obtain standard speaker diarization tool apply data
speaker diarization may difficult achieve apply narrative film speakers usually talk adverse acoustic condition background music sound effect wide variations intonation may hide inter speaker variability make audio base speaker diarization approach error prone hand fictional movies exhibit strong regularities image level particularly within dialogue scenes paper propose perform speaker diarization within dialogue scenes tv series combine audio video modalities speaker diarization first perform use modality two result partition instance set optimally match remain instance correspond case disagreement modalities finally process result obtain apply multi modal approach fictional film turn outperform obtain rely single modality
speaker diarization usually denote speak task turn particularly challenge apply fictional film many character talk various acoustic condition background music sound effect despite acoustic variability movies exhibit specific visual pattern dialogue scenes paper introduce two step method achieve speaker diarization tv series speaker diarization first perform locally scenes detect dialogues hypothesize local speakers merge second agglomerative cluster process constraint speakers locally hypothesize distinct must assign cluster performances approach compare obtain standard speaker diarization tool apply data
document information extraction task perform humans create data consist pdf document image input extract string output end end data naturally consume produce perform task valuable naturally available additional cost unfortunately state art word classification methods information extraction use data instead require word level label expensive create consequently available many real life task paper propose attend copy parse architecture deep neural network model train directly end end data bypass need word level label evaluate propose architecture large diverse set invoice outperform state art production system base word classification believe propose architecture use many real life information extraction task word classification use due lack require word level label
paper present publicly available corpus french encyclopedic history texts annotate accord berkeley framenet formalism main difference approach compare previous work semantic parse framenet interest full text parse rather partial parse goal select framenet resources minimal set frame go useful applicative framework target case information extraction encyclopedic document approach leverage manual annotation larger corpora obtain full text parse therefore open door alternative methods frame parse use far framenet fifteen benchmark corpus approach compare study rely integrate sequence label model jointly optimize frame identification semantic role segmentation identification model compare crfs multitasks bi lstms
recommendation systems important place help online users internet society recommendation systems computer science practical use days various aspects internet portals social network library websites several approach implement recommendation systems latent dirichlet allocation lda one popular techniques topic model recently researchers propose many approach base recommendation systems lda accord importance subject paper discover trend topics find relationship lda topics scholar context document fact apply probabilistic topic model base gibbs sample algorithms semantic mine six conference publications computer science dblp dataset accord experimental result semantic framework effective help organizations better organize conferences cover future research topics
paper present algorithm enumerate bias word embeddings algorithm expose large number offensive associations relate sensitive feature race gender publicly available embeddings include supposedly debiased embed bias concern light widespread use word embeddings associations identify geometric pattern word embeddings run parallel people name common lower case tokens algorithm highly unsupervised even require sensitive feature pre specify desirable many form discrimination racial discrimination link social construct may vary depend context rather categories fix definitions b make easier identify bias intersectional group depend combinations sensitive feature input algorithm list target tokens eg name word embed output number word embed association test weats capture various bias present data illustrate utility approach publicly available word embeddings list name evaluate output use crowdsourcing also show remove name may remove potential proxy bias
cross domain natural language generation nlg still difficult task within speak dialogue model give semantic representation provide dialogue manager language generator generate sentence convey desire information traditional template base generators produce sentence necessary information sentence sufficiently diverse rnn base model diversity generate sentence high however process information lose work improve rnn base generator consider latent information sentence level generation use conditional variational autoencoder architecture demonstrate model outperform original rnn base generator yield highly diverse sentence addition model perform better train data limit
paper describe semantic frame parse system base sequence label methods precisely bilstm model highway connections perform information extraction corpus french encyclopedic history texts annotate accord berkeley framenet formalism approach propose study rely integrate sequence label model jointly optimize frame identification semantic role segmentation identification purpose study analyze task complexity highlight factor make semantic frame parse difficult task provide detail evaluations performance different type frame sentence
project work speech recognition specifically predict individual word base video frame audio empower convolutional neural network recent speech recognition lip read model comparable human level performance implement make derivations state art model conduct rich experiment include effectiveness attention mechanism accurate residual network backbone pre train weight sensitivity model respect audio input without noise
text classification must sometimes apply low resource language label train data however train data may available relate language investigate whether character level knowledge transfer relate language help text classification present cross lingual document classification framework caco exploit cross lingual subword similarity jointly train character base embedder word base classifier embedder derive vector representations input word write form classifier make predictions base word vectors use joint character representation source language target language allow embedder generalize knowledge source language word target language word similar form propose multi task objective improve model additional cross lingual monolingual resources available experiment confirm character level knowledge transfer data efficient word level transfer relate languages
sentiment analysis popular technique opinion mine use software engineer research community task assess app review developer emotions issue trackers developer opinions apis past research indicate state art sentiment analysis techniques poor performance se data sentiment analysis tool often design work non technical document movie review study attempt solve issue exist sentiment analysis techniques se texts propose hierarchical model base convolutional neural network cnn long short term memory lstm train top pre train word vectors assess model performance reliability compare number frequently use sentiment analysis tool five gold standard datasets result show model push state art datasets term accuracy also show possible get better accuracy label small sample dataset train model rather use unsupervised classifier
non autoregressive translation nat model remove dependence previous target tokens input decoder achieve significantly inference speedup cost inferior accuracy compare autoregressive translation model previous work show quality input decoder important largely impact model accuracy paper propose two methods enhance decoder input improve nat model first one directly leverage phrase table generate conventional smt approach translate source tokens target tokens feed decoder input second one transform source side word embeddings target side word embeddings sentence level alignment word level adversary learn feed transform word embeddings decoder input experimental result show method largely outperform nat baselinecitepgu2017non five hundred and eleven bleu score wmt14 english german task four hundred and seventy-two bleu score wmt16 english romanian task
previous work neural sequence model show improve significantly external prior knowledge provide instance allow model access embeddings explicit feature train inference work propose different point view incorporate prior knowledge principled way use moment match framework approach standard local cross entropy train sequential model combine moment match train mode encourage equality expectations certain predefined feature model distribution empirical distribution particular show derive unbiased estimate stochastic gradients central train compare framework formally relate one policy gradient train reinforcement learn point important differences term kinds prior assumptions approach initial result promise show effectiveness propose framework
neural machine translation nmt model generally adopt encoder decoder architecture model entire translation process encoder summarize representation input sentence scratch potentially problem sentence ambiguous translate text humans often create initial understand source sentence incrementally refine along translation target side start intuition propose novel encoder refiner decoder framework dynamically refine source representations base generate target side information decode step since refine operations time consume propose strategy leverage power reinforcement learn model decide refine specific decode step experimental result chinese english english german translation task show propose approach significantly consistently improve translation performance standard encoder decoder framework furthermore refine strategy apply result still show reasonable improvement baseline without much decrease decode speed
paper investigate feasibility apply shoot learn algorithms speech task formulate user define scenario speak term classification shoot learn problem shoot learn study assume n class new n way problem suggest assumption relax define nm way problem n number new class fix class respectively propose modification model agnostic meta learn maml algorithm solve problem experiment google speech command dataset show approach outperform conventional supervise learn approach original maml
past decade dbpedia community put significant amount effort develop technical infrastructure methods efficient extraction structure information wikipedia efforts primarily focus harvest refinement publish semi structure information find wikipedia article information infoboxes categorization information image wikilinks citations nevertheless still vast amount valuable information contain unstructured wikipedia article texts paper present dbpedia nif large scale multilingual knowledge extraction corpus aim dataset two fold dramatically broaden deepen amount structure information dbpedia provide large scale multilingual language resource development various nlp ir task dataset provide content article one hundred and twenty-eight wikipedia languages describe dataset creation process nlp interchange format nif use model content link structure information wikipedia article dataset enrich twenty-five link select partition publish link data finally describe maintenance sustainability plan select use case dataset textext knowledge extraction challenge
recently hyperbolic geometry prove effective build embeddings encode hierarchical entailment information make particularly suit model complex asymmetrical relationships chinese character word paper first train large scale hyperboloid skip gram model chinese corpus apply character embeddings downstream hyperbolic transformer model derive principles gyrovector space poincare disk model experiment character base transformer outperform word base euclidean equivalent best knowledge first time chinese nlp character base model outperform word base counterpart allow circumvention challenge domain dependent task chinese word segmentation cws
cross lingual speech emotion recognition important task practical applications performance automatic speech emotion recognition systems degrade cross corpus scenarios particularly scenarios involve multiple languages previously unseen language urdu limit data available study investigate problem cross lingual emotion recognition urdu language contribute urdu first ever spontaneous urdu language speech emotion database evaluations perform use three different western languages urdu experimental result different possible scenarios suggest various interest aspects design adaptive emotion recognition system limit languages result select train instance multiple languages deliver comparable result baseline augmentation fraction test language data train help boost accuracy speech emotion recognition urdu data publicly available research
distant supervision leverage knowledge base automatically label instance thus allow us train relation extractor without human annotations however generate train data typically contain massive noise may result poor performances vanilla supervise learn paper propose conduct multi instance learn novel cross relation cross bag selective attention c2sa lead noise robust train distant supervise relation extractor specifically employ sentence level selective attention reduce effect noisy mismatch sentence correlation among relations capture improve quality attention weight moreover instead treat entity pair equally try pay attention entity pair higher quality similarly adopt selective attention mechanism achieve goal experiment two type relation extractor demonstrate superiority propose approach state art ablation study verify intuitions demonstrate effectiveness propose two techniques
paper introduce methodology predict intent slot query chatbot answer career relate query take multi stag approach process intent classification slot tag inform decision make different stag model break problem stag solve one problem time pass relevant result current stage next thereby reduce search space subsequent stag eventually make classification tag viable stage also observe relax rule fuzzy entity match slot tag stage maintain separate name entity tagger per stage help us improve performance although slight cost false positives model achieve state art performance f1 score seven thousand, seven hundred and sixty-three intent classification eight thousand, two hundred and twenty-four slot tag dataset would publicly release along paper
understand structure interaction process help us improve information seek dialogue systems analyze interaction process boil discover pattern sequence alternate utterances exchange user agent process mine techniques successfully apply analyze structure event log discover underlie process model evaluate whether observe behavior conformance know process paper apply process mine techniques discover pattern conversational transcripts extract new model information seek dialogues qrfa query request feedback answer result ground empirical evaluation across multiple conversational datasets different domains never attempt show qrfa model better reflect conversation flow observe real information seek conversations model propose previously moreover qrfa allow us identify malfunction dialogue system transcripts deviations expect conversation flow describe model via conformance analysis
build open domain conversational systems allow users engage conversations topics choice challenge task alexa prize launch two thousand and sixteen tackle problem achieve natural sustain coherent engage open domain dialogs second iteration competition two thousand and eighteen university team advance state art use context dialog model leverage knowledge graph language understand handle complex utterances build statistical hierarchical dialog managers leverage model drive signal user responses two thousand and eighteen competition also include provision suite tool model competitors include cobot conversational bot toolkit topic dialog act detection model conversation evaluators sensitive content detection model compete team could focus build knowledge rich coherent engage multi turn dialog systems paper outline advance develop university team well alexa prize team achieve common goal advance science conversational ai address several key open end problems conversational speech recognition open domain natural language understand commonsense reason statistical dialog management dialog evaluation collaborative efforts drive improve experience alexa users average rat three hundred and sixty-one median duration two mins eighteen second average turn one hundred and forty-six increase fourteen ninety-two fifty-four respectively since launch two thousand and eighteen competition conversational speech recognition improve relative word error rate fifty-five relative entity error rate thirty-four since launch alexa prize socialbots improve quality significantly rapidly two thousand and eighteen part due release cobot toolkit
clickbait grow become nuisance social media users social media operators alike malicious content publishers misuse social media manipulate many users possible visit websites use clickbait message machine learn technology may help handle problem give rise automatic clickbait detection accelerate progress direction organize clickbait challenge two thousand and seventeen share task invite submission clickbait detectors comparative evaluation total thirteen detectors submit achieve significant improvements previous state art term detection performance also many submit approach publish open source render reproducible good start point newcomers two thousand and seventeen challenge pass maintain evaluation system answer new registrations support ongoing research better clickbait detectors
sentence formation highly structure history dependent sample space reduce ssr process first word sentence choose entire vocabulary typically freedom choose subsequent word get constrain grammar context sentence progress sample space reduce property offer natural explanation zipf law word frequencies however fail capture structure word word transition probability matrices english text adopt view grammatical constraints subject predicate object locally order word order sentence sample ssr word generation process demonstrate superimpose grammatical structure local word order permutation process sample space reduce process sufficient explain word frequencies word word transition probabilities compare quality grammatically order ssr model reproduce several test statistics real texts text generation model bernoulli model simon model monkey typewrite model
propose neural network model joint extraction name entities relations without hand craft feature key contribution model extend bilstm crf base entity recognition model deep biaffine attention layer model second order interactions latent feature relation classification specifically attend role entity directional relationship benchmark relation entity recognition dataset conll04 experimental result show model outperform previous model produce new state art performances
capsule group neurons whose activity vector represent instantiation parameters specific type entity paper explore capsule network use relation extraction multi instance multi label learn framework propose novel neural approach base capsule network attention mechanisms evaluate method different benchmarks demonstrate method improve precision predict relations particularly show capsule network improve multiple entity pair relation extraction
volume scholarly publications increase frenetic pace access consume useful candidate paper large digital libraries become essential challenge task scholars unfortunately language barrier scientists especially junior ones graduate students master languages efficiently locate publications host foreign language repository study propose novel solution cross language citation recommendation via hierarchical representation learn heterogeneous graph hrlhg address new problem hrlhg learn representation function map publications multilingual repositories low dimensional joint embed space various kinds vertexes relations heterogeneous graph leverage global task specific plus local task independent information well novel supervise hierarchical random walk algorithm propose method optimize publication representations maximize likelihood locate important cross language neighborhoods graph experiment result show propose method outperform state art baseline model also improve interpretability representation model cross language citation recommendation task
able automatically discover synonymous entities open world set benefit various task entity disambiguation knowledge graph canonicalization exist work either utilize entity feature rely structure annotations single piece context entity mention leverage diverse contexts entities mention paper generalize distributional hypothesis multi context set propose synonym discovery framework detect entity synonyms free text corpora considerations effectiveness robustness one key components synonym discovery introduce neural network model synonymnet determine whether two give entities synonym instead use entities feature synonymnet make use multiple piece contexts entity mention compare context level similarity via bilateral match schema experimental result demonstrate propose model able detect synonym set observe train generic domain specific datasets wikifreebase pubmedumls medbookmkg four hundred and sixteen improvement term area curve three hundred and nineteen term mean average precision compare best baseline method
text representations use neural word embeddings prove effective many nlp applications recent research adapt traditional word embed model learn vectors multiword expressions concepts entities however methods limit textual knowledge base eg wikipedia paper propose novel simple technique integrate knowledge concepts two large scale knowledge base different structure wikipedia probase order learn concept representations adapt efficient skip gram model seamlessly learn knowledge wikipedia text probase concept graph evaluate concept embed model two task one analogical reason achieve state art performance ninety-one semantic analogies two concept categorization achieve state art performance two benchmark datasets achieve categorization accuracy one hundred one ninety-eight additionally present case study evaluate model unsupervised argument type identification neural semantic parse demonstrate competitive accuracy unsupervised method ability better generalize vocabulary entity mention compare tedious error prone methods depend gazetteers regular expressions
recurrent neural network nowadays successfully use abundance applications go text speech image process recommender systems backpropagation time algorithm commonly use train network specific task many deep learn frameworks implementation train sample procedures recurrent neural network fact multiple possibilities choose parameters tune exist literature often overlook ignore paper therefore give overview possible train sample scheme character level recurrent neural network solve task predict next token give sequence test different scheme variety datasets neural network architectures parameter settings formulate number take home recommendations choice train sample scheme turn subject number trade off train stability sample time model performance implementation effort largely independent data perhaps surprise result transfer hide state correctly initialize model subsequences often lead unstable train behavior depend dataset
investigate train end end speech recognition model recurrent neural network transducer rnn stream neural sequence sequence architecture jointly learn acoustic language model components transcribe acoustic data explore various model architectures demonstrate model improve additional text pronunciation data available model consist encoder initialize connectionist temporal classification base ctc acoustic model decoder partially initialize recurrent neural network language model train text data alone entire neural network train rnn loss directly output recognize transcript sequence graphemes thus perform end end speech recognition find performance improve use sub word units wordpieces capture longer context significantly reduce substitution errors best rnn system twelve layer lstm encoder two layer lstm decoder train thirty thousand wordpieces output target achieve word error rate eighty-five voice search fifty-two voice dictation task comparable state art baseline eighty-three voice search fifty-four voice dictation
languages share people differ different regions base accent pronunciation word usages era share language take place mainly social media blog every second swing micro post exist induce need process micro post order extract knowledge knowledge extraction differ respect application research cognitive science feed necessities work move forward research extract semantic information stream batch data applications like name entity recognition author profile case name entity recognition context single micro post utilize context lie pool micro post utilize identify sociolect aspects author micro post work conditional random field utilize entity recognition novel approach propose find sociolect aspects author gender age group
paper explore possibility use alternative data artificial intelligence techniques trade stock efficacy daily twitter sentiment predict stock return examine use machine learn methods reinforcement learningq learn apply generate optimal trade policy base sentiment signal predict power sentiment signal significant stock price drive expectation company growth company major event draw public attention optimal trade strategy base reinforcement learn outperform trade strategy base machine learn prediction
crowd power conversational assistants show robust automate systems cost higher response latency monetary cost promise direction combine two approach high quality low latency low cost solutions paper introduce evorus crowd power conversational assistant build automate time allow new chatbots easily integrate automate scenarios ii reuse prior crowd answer iii learn automatically approve response candidates five month long deployment eighty participants two hundred and eighty-one conversations show evorus automate without compromise conversation quality crowd ai architectures long propose way reduce cost latency crowd power systems evorus demonstrate automation introduce successfully deploy system architecture allow future researchers make innovation underlie automate components context deploy open domain dialog system
paper propose novel lifelong learn approach sentiment classification mimic human continuous learn process ie retain knowledge learn past task use help future learn paper first discuss general sentiment classification particular propose approach adopt bayesian optimization framework base stochastic gradient descent experimental result show propose method outperform baseline methods significantly demonstrate lifelong learn promise research direction
amount publicly available biomedical literature grow rapidly recent years yet question answer systems still struggle exploit full potential source data preliminary process step many question answer systems rely retrieval model identify relevant document passages paper propose weight cosine distance retrieval scheme base neural network word embeddings experiment base publicly available data task bioasq biomedical question answer challenge demonstrate significant performance gain wide range state art model
topical stance detection problem address detect stance text content respect give topic whether sentiment give text content favor positive negative none neutral towards give topic use concept attention develop two phase solution first phase classify subjectivity whether give tweet neutral subjective respect give topic second phase classify sentiment subjective tweet ignore neutral tweet whether give subjective tweet favor stance towards topic propose long short term memory lstm base deep neural network phase embed attention phase semeval two thousand and sixteen stance detection twitter task dataset obtain best case macro f score six thousand, eight hundred and eighty-four best case accuracy six hundred and two outperform exist deep learn base solutions framework pan first topical stance detection literature use deep learn within two phase architecture
natural language process often involve computations semantic syntactic graph facilitate sophisticate reason base structural relationships convolution kernels provide powerful tool compare graph structure base node word level relationships difficult customize computationally expensive propose generalization convolution kernels nonstationary model better expressibility natural languages supervise settings scalable learn parameters introduce model propose novel algorithm leverage stochastic sample k nearest neighbor graph along approximations base locality sensitive hash demonstrate advantage approach challenge real world structure inference problem automatically extract biological model text scientific paper
although various techniques propose generate adversarial sample white box attack text little attention pay black box attack realistic scenarios paper present novel algorithm deepwordbug effectively generate small text perturbations black box set force deep learn classifier misclassify text input employ novel score strategies identify critical tokens modify classifier make incorrect prediction simple character level transformations apply highest rank tokens order minimize edit distance perturbation yet change original classification evaluate deepwordbug eight real world text datasets include text classification sentiment analysis spam detection compare result deepwordbug two baselines random black box gradient white box experimental result indicate deepwordbug reduce prediction accuracy current state art deep learn model include decrease sixty-eight average word lstm model forty-eight average char cnn model
paper address important problem discern hateful content social media propose detection scheme ensemble recurrent neural network rnn classifiers incorporate various feature associate user relate information users tendency towards racism sexism data feed input classifiers along word frequency vectors derive textual content approach evaluate publicly available corpus 16k tweet result demonstrate effectiveness comparison exist state art solutions specifically scheme successfully distinguish racism sexism message normal text achieve higher classification quality current state art algorithms
text mine field aim extract information textual data one challenge field study come pre process stage vector structure representation extract unstructured data common extraction create large sparse vectors represent importance term document usually lead curse dimensionality plague machine learn algorithms cope issue paper propose new supervise feature extraction reduction algorithm name dcdistance create feature base distance document representative class label propose technique reduce feature set ninety-nine original set additionally algorithm also capable improve classification accuracy set benchmark datasets compare traditional state art feature selection algorithms
project explore several machine learn methods predict movie genres base plot summaries naive bay word2vecxgboost recurrent neural network use text classification k binary transformation rank method probabilistic classification learn probability threshold employ multi label problem involve genre tag taskexperiments two hundred and fifty thousand movies show employ gate recurrent units gru neural network probabilistic classification learn probability threshold approach achieve best result test set model attain jaccard index five hundred f score fifty-six hit rate eight hundred and five
topic model enable exploration compact representation corpus caringbridge cb dataset massive collection journals write patients caregivers health crisis topic model cb dataset however challenge due asynchronous nature multiple author write health journey overcome challenge introduce dynamic author persona topic model dap probabilistic graphical model design temporal corpora multiple author novelty dap model lie representation author persona personas capture propensity write certain topics time present regularize variational inference algorithm use encourage dap model personas distinct result show significant improvements compete topic model particularly regularization highlight dap model unique ability capture common journey share different author
target task study ground language understand domestic service robots dsrs particular focus instruction understand short sentence verbs miss task critical importance build communicative dsrs manipulation essential dsrs exist instruction understand methods usually estimate miss information non ground knowledge therefore whether predict action physically executable unclear paper present ground instruction understand method estimate appropriate object give instruction situation extend generative adversarial net gin build gin base classifier use latent representations quantitatively evaluate propose method develop data set base standard data set use visual qa experimental result show propose method give better result baseline methods
let finite countable alphabet let theta literal anti automorphism onto definition correspondence determinated permutation alphabet paper deal set invariant theta theta invariant short languages l theta l subset lwe establish extension famous defect theorem regard call notion completeness provide series examples finite complete theta invariant cod moreover establish formula allow embed non complete theta invariant code complete one consequence family call thin theta invariant cod maximality completeness two equivalent notions
drive force behind recent success lstms ability learn complex non linear relationships consequently inability describe relationships lead lstms characterize black box end introduce contextual decomposition cd interpretation algorithm analyse individual predictions make standard lstms without change underlie model decompose output lstm cd capture contributions combinations word variables final prediction lstm task sentiment analysis yelp sst data set show cd able reliably identify word phrase contrast sentiment combine yield lstm final prediction use phrase level label sst also demonstrate cd able successfully extract positive negative negations lstm something previously do
social media offer great communication opportunities also increase vulnerability young people threaten situations online recent study report cyberbullying constitute grow problem among youngsters successful prevention depend adequate detection potentially harmful message information overload web require intelligent systems identify potential risk automatically focus paper automatic cyberbullying detection social media text model post write bully victims bystanders online bully describe collection fine grain annotation train corpus english dutch perform series binary classification experiment determine feasibility automatic cyberbullying detection make use linear support vector machine exploit rich feature set investigate information source contribute particular task experiment holdout test set reveal promise result detection cyberbullying relate post optimisation hyperparameters classifier yield f1 score sixty-four sixty-one english dutch respectively considerably outperform baseline systems base keywords word unigrams
train multi task autoencoders linguistic task analyze learn hide sentence representations representations change significantly translation part speech decoders add decoders model employ better cluster sentence accord syntactic similarity representation space become less entangle explore structure representation space interpolate sentence yield interest pseudo english sentence many recognizable syntactic structure lastly point interest property model difference vector two sentence add change third sentence similar feature meaningful way
inductive transfer learn greatly impact computer vision exist approach nlp still require task specific modifications train scratch propose universal language model fine tune ulmfit effective transfer learn method apply task nlp introduce techniques key fine tune language model method significantly outperform state art six text classification task reduce error eighteen twenty-four majority datasets furthermore one hundred label examples match performance train scratch 100x data open source pretrained model code
train task completion dialogue agent via reinforcement learn rl costly require many interactions real users one common alternative use user simulator however user simulator usually lack language complexity human interlocutors bias design may tend degrade agent address issue present deep dyna q knowledge first deep rl framework integrate plan task completion dialogue policy learn incorporate dialogue agent model environment refer world model mimic real user response generate simulate experience dialogue policy learn world model constantly update real user experience approach real user behavior turn dialogue agent optimize use real experience simulate experience effectiveness approach demonstrate movie ticket book task simulate human loop settings
textcnn convolutional neural network text useful deep learn algorithm sentence classification task sentiment analysis question classification however neural network long know black box interpret challenge task researchers develop several tool understand cnn image classification deep visualization research deep textcnns still insufficient paper try understand textcnn learn two classical nlp datasets work focus function different convolutional kernels correlations convolutional kernels
social media grow crucial information source pharmacovigilance study increase number people post adverse reactions medical drug previously unreported aim effectively monitor various aspects adverse drug reactions adrs diversely express social medical post propose multi task neural network framework learn several task associate adr monitor different level supervisions collectively besides able correctly classify adr post accurately extract adr mention online post propose framework also able understand reason drug take know indication give social media post coverage base attention mechanism adopt framework help model properly identify phrasal adrs indications attentive multiple word post framework applicable situations limit parallel data different pharmacovigilance task availablewe evaluate propose framework real world twitter datasets propose model outperform state art alternatives individual task consistently
harassment cyberbullies significant phenomenon social media exist work cyberbullying detection least one follow three bottleneck first target one particular social media platform smp second address one topic cyberbullying third rely carefully handcraft feature data show deep learn base model overcome three bottleneck knowledge learn model one dataset transfer datasets perform extensive experiment use three real world datasets formspring 12k post twitter 16k post wikipedia100k post experiment provide several useful insights cyberbullying detection best knowledge first work systematically analyze cyberbullying detection various topics across multiple smps use deep learn base model transfer learn
adversarial sample image extensively study literature among many attack methods gradient base methods effective easy compute work propose framework adapt gradient attack methods image text domain main difficulties generate adversarial texts gradient methods input space discrete make difficult accumulate small noise directly input ii measurement quality adversarial texts difficult tackle first problem search adversarials embed space reconstruct adversarial texts via nearest neighbor search latter problem employ word mover distance wmd quantify quality adversarial texts extensive experiment three datasets imdb movie review reuters two reuters five newswires show framework leverage gradient attack methods generate high quality adversarial texts word different original texts many case change one word alter label whole piece text successfully incorporate fgm deepfool framework addition empirically show wmd closely relate quality adversarial texts
neural network methods achieve great success review sentiment classification recently work achieve improvement incorporate user product information generate review representation however review observe word sentence show strong user preference others tend indicate product characteristic two kinds information play different roles determine sentiment label review therefore reasonable encode user product information together one representation paper propose novel framework encode user product information firstly apply two individual hierarchical neural network generate two representations user attention product attention design combine strategy make full use two representations train final prediction experimental result show model obviously outperform state art methods imdb yelp datasets visualization attention word relate user product validate observation mention
paper investigate evaluate support vector machine active learn algorithms use imbalanced datasets commonly arise many applications information extraction applications algorithms base closest hyperplane selection query committee selection combine methods address imbalance positive amplification base prevalence statistics initial random sample three algorithms closestpa qbagpa qboostpa present carefully evaluate datasets text classification relation extraction closestpa algorithm show consistently outperform two variety ways insights provide case
deep learn emerge powerful machine learn technique learn multiple layer representations feature data produce state art prediction result along success deep learn many application domains deep learn also popularly use sentiment analysis recent years paper first give overview deep learn provide comprehensive survey current applications sentiment analysis
use active learn smaller batch size typically efficient learn efficiency perspective however practice due speed human annotator considerations use larger batch size necessary past work show larger batch size decrease learn efficiency learn curve perspective remain open question batch size impact methods stop active learn find large batch size degrade performance lead stop method degradation result reduce learn efficiency analyze degradation find mitigate change window size parameter many past iterations learn take account make stop decision find use larger batch size stop methods effective smaller window size use
ultraslow diffusion ie logarithmic diffusion extensively study theoretically hardly observe empirically paper firstly find ultraslow like diffusion time series word count already popular word analyse three different nationwide language databases newspaper article japanese ii blog article japanese iii page view wikipedia english french chinese japanese secondly use theoretical analysis show diffusion basically explain random walk model power law forget exponent beta approx five relate fractional langevin equation exponent beta characterise speed forget beta approx five correspond border thresholds stationary nonstationary ii right middle dynamics iid noise beta1 normal random walk beta0 thirdly generative model time series word count already popular word kind poisson process poisson parameter sample mention random walk model almost reproduce empirical mean square displacement also power spectrum density probability density function
paper address refer expression comprehension localize image region describe natural language expression recent work treat expressions single unit propose decompose three modular components relate subject appearance location relationship object allow us flexibly adapt expressions contain different type information end end framework model call modular attention network mattnet two type attention utilize language base attention learn module weight well word phrase attention module focus visual attention allow subject relationship modules focus relevant image components module weight combine score three modules dynamically output overall score experiment show mattnet outperform previous state art methods large margin bound box level pixel level comprehension task demo code provide
many recent state art recommender systems att transnet deepconn exploit review representation learn paper propose new neural architecture recommendation review model operate multi hierarchical paradigm base intuition review create equal ie select important importance however dynamically infer depend current target end propose review review pointer base learn scheme extract important review subsequently match word word fashion enable informative review utilize prediction also deeper word level interaction pointer base method operate novel gumbel softmax base pointer mechanism enable incorporation discrete vectors within differentiable neural architectures pointer mechanism co attentive nature learn pointers co dependent user item relationships finally propose multi pointer learn scheme learn combine multiple view interactions user item overall demonstrate effectiveness propose model via extensive experiment textbf24 benchmark datasets amazon yelp empirical result show approach significantly outperform exist state art nineteen seventy-one relative improvement compare transnet deepconn respectively study behavior multi pointer learn mechanism shed light evidence aggregation pattern review base recommender systems
different structural feature human language change different rat thus exhibit different temporal stabilities exist methods linguistic stability estimation depend upon prior genealogical classification world languages language families methods result unreliable stability estimate feature sensitive horizontal transfer families whenever data aggregate families divergent time depths overcome problems describe method stability estimation without family classifications base mathematical model analysis contemporary geospatial distributions linguistic feature regress estimate produce model genealogical method report broad agreement also important differences particular show approach liable false positives false negative incur genealogical method result suggest historical evolution linguistic feature leave footprint global geospatial distribution rat evolution recover distributions treat language dynamics spatially extend stochastic process
relational data source still one popular ways store enterprise web data however issue relational schema lack well define semantic description common ontology provide way represent mean relational schema facilitate integration heterogeneous data source within domain semantic label achieve map attribute data source class properties ontology formulate problem multi class classification problem previously label data source use learn rule label new data source majority exist approach semantic label focus data integration challenge name conflict semantic heterogeneity addition machine learn approach typically issue around class imbalance lack label instance relative importance attribute address issue develop new machine learn model engineer feature well two deep learn model require extensive feature engineer evaluate new approach state art
motivation state art biomedical name entity recognition bioner systems often require handcraft feature specific entity type genes chemicals diseases although recent study explore use neural network model bioner free experts manual feature engineer performance remain limit available train data entity type result propose multi task learn framework bioner collectively use train data different type entities improve performance experiment fifteen benchmark bioner datasets multi task model achieve substantially better performance compare state art bioner systems baseline neural sequence label model analysis show large performance gain come share character word level information among relevant biomedical entities across differently label corpora
public dataset variety properties suitable sentiment analysis one event prediction trend detection text mine applications need order able successfully perform analysis study vast majority data social media text base possible directly apply machine learn process raw data since several different process require prepare data implementation algorithms example different misspell word enlarge word vector space unnecessarily thereby lead reduce success algorithm increase computational power requirement paper present improve turkish dataset effective spell correction algorithm base hadoop two collect data record hadoop distribute file system text base data process mapreduce program model method suitable storage process large size text base social media data study movie review automatically record apache manifoldcf mcf three data cluster create various methods compare levenshtein fuzzy string match propose create public dataset collect data experimental result show propose algorithm use open source dataset sentiment analysis study perform successfully detection correction spell errors
technical report describe general class monoids subsequential rational characterise term congruence relation flavour myhill nerode relation class monoids consider describe term natural algebraic axioms contain free monoids group tropical monoid close cartesian
past decade emoji emerge new widespread form digital communication span diverse social network speak languages propose treat ideograms new modality right distinct semantic structure text often embed well image resemble new modality emoji present rich novel possibilities representation interaction paper explore challenge arise naturally consider emoji modality lens multimedia research specifically ways emoji relate common modalities text image first present large scale dataset real world emoji usage collect twitter dataset contain examples text emoji image emoji relationships present baseline result challenge predict emoji text image use state art neural network offer first consideration problem account new unseen emoji relevant issue emoji vocabulary continue expand yearly basis finally present result multimedia retrieval use emoji query
present end end method transform audio one style another case speech condition speaker identities train single model transform word speak multiple people multiple target voice case music specify musical instrument achieve result architecturally method fully differentiable sequence sequence model base convolutional hierarchical recurrent neural network design capture long term acoustic dependencies require minimal post process produce realistic audio transform ablation study confirm model separate speaker instrument properties acoustic content different receptive field empirically method achieve competitive performance community standard datasets
revisit skip gram negative sample sgns one popular neural network base approach learn distribute word representation first point ambiguity issue undermine sgns model sense word vectors entirely distort without change objective value resolve issue investigate intrinsic structure solution good word embed model deliver motivate rectify sgns model quadratic regularization show simple modification suffice structure solution desire manner theoretical justification present provide novel insights quadratic regularization preliminary experiment also conduct google analytical reason task support modify sgns model
ability extract insights new data set critical decision make visual interactive tool play important role data exploration since provide non technical users effective way visually compose query comprehend result natural language recently gain traction alternative query interface databases potential enable non expert users formulate complex question information need efficiently effectively however understand natural language question translate accurately sql challenge task thus natural language interfaces databases nlidbs yet make way practical tool commercial products paper present dbpal novel data exploration tool natural language interface dbpal leverage recent advance deep model make query understand robust follow ways first dbpal use deep model translate natural language statements sql make translation process robust paraphrase linguistic variations second support users phrase question without know database schema query feature dbpal provide learn auto completion model suggest partial query extensions users query formulation thus help write complex query
although voice conversion vc algorithms achieve remarkable success along development machine learn superior performance still difficult achieve use nonparallel data paper propose use cycle consistent adversarial network cyclegan nonparallel data base vc train cyclegan generative adversarial network gin originally develop unpaired image image translation subjective evaluation inter gender conversion demonstrate propose method significantly outperform method base merlin open source neural network speech synthesis system parallel vc system adapt setup gin base parallel vc system first research show performance nonparallel vc method exceed state art parallel vc methods
teacher student learn show effective unsupervised domain adaptation one form transfer learn term transfer recognition decisions knowledge posteriori probabilities source domain evaluate teacher model learn handle speaker environment variability inherent restrict speech signal target domain without proactively address robustness likely condition performance degradation may thus ensue work advance learn propose adversarial learn explicitly achieve condition robust unsupervised domain adaptation method student acoustic model condition classifier jointly optimize minimize kullback leibler divergence output distributions teacher student model simultaneously min maximize condition classification loss condition invariant deep feature learn adapt student model procedure propose multi factorial adversarial learn suppress condition variabilities cause multiple factor simultaneously evaluate noisy chime three test set propose methods achieve relative word error rate improvements four thousand, four hundred and sixty five hundred and thirty-eight respectively clean source model strong learn baseline model
propose novel adversarial multi task learn scheme aim actively curtail inter talker feature variability maximize senone discriminability enhance performance deep neural network dnn base asr system call scheme speaker invariant train sit sit dnn acoustic model speaker classifier network jointly optimize minimize senone tie triphone state classification loss simultaneously mini maximize speaker classification loss speaker invariant senone discriminative deep feature learn adversarial multi task learn sit canonical dnn acoustic model significantly reduce variance output probabilities learn explicit speaker independent si transformations speaker specific representations use train test evaluate chime three dataset sit achieve four hundred and ninety-nine relative word error rate wer improvement conventional si acoustic model additional unsupervised speaker adaptation speaker adapt sa sit model achieve four hundred and eighty-six relative wer gain sa si acoustic model
celebrate sequence sequence learn seq2seq technique numerous variants achieve excellent performance many task however many machine learn task input naturally represent graph exist seq2seq model face significant challenge achieve accurate conversion graph form appropriate sequence address challenge introduce novel general end end graph sequence neural encoder decoder model map input graph sequence vectors use attention base lstm method decode target sequence vectors method first generate node graph embeddings use improve graph base neural network novel aggregation strategy incorporate edge direction information node embeddings introduce attention mechanism align node embeddings decode sequence better cope large graph experimental result babi shortest path natural language generation task demonstrate model achieve state art performance significantly outperform exist graph neural network seq2seq tree2seq model use propose bi directional node embed aggregation strategy model converge rapidly optimal performance
paper propose attention base classifier predict multiple emotions give sentence model imitate human two step procedure sentence understand effectively represent classify sentence emoji mean preprocessing extra lexicon utilization improve model performance train evaluate model data provide semeval two thousand and eighteen task one five sentence several label among eleven give sentiments model achieve five th one th rank english spanish respectively
paper propose method generate speech filterbank mel frequency cepstral coefficients mfcc widely use speech applications asr generally consider unusable speech synthesis first predict fundamental frequency voice information mfccs autoregressive recurrent neural net second spectral envelope information contain mfccs convert pole filter pitch synchronous excitation model match filter train finally introduce generative adversarial network base noise model add realistic high frequency stochastic component model excitation signal result show high quality speech reconstruction obtain give mfcc information test time
proliferation fake news filter bubble make increasingly difficult form unbiased balance opinion towards topic ameliorate propose 360deg stance detection tool aggregate news multiple perspectives topic present spectrum range support opposition enable user base opinion multiple piece diverse evidence
recent work richardson kuhn 2017ab richardson et al two thousand and eighteen look semantic parser induction question answer domain source code libraries apis brief note formalize representations learn study introduce simple domain specific language systematic translation language first order logic recast target representations term classical logic aim broaden applicability exist code datasets investigate complex natural language understand reason problems software domain
paper address problem predict duration unplanned power outages use historical outage record train series neural network predictors initial duration prediction make base environmental factor update base incoming field report use natural language process automatically analyze text experiment use fifteen years outage record show good initial result improve performance leverage text case study show language process identify phrase point outage cause repair step
paper explore neural network model learn associate segment speak audio caption semantically relevant portion natural image refer demonstrate audio visual associative localizations emerge network internal representations learn product train perform image audio retrieval task model operate directly image pixels speech waveform rely conventional supervision form label segmentations alignments modalities train perform analysis use place two hundred and five ade20k datasets demonstrate model implicitly learn semantically couple object word detectors
word embeddings popular approach unsupervised learn word relationships widely use natural language process article present new set embeddings medical concepts learn use extremely large collection multimodal medical data lean recent theoretical insights demonstrate insurance claim database sixty million members collection twenty million clinical note seventeen million full text biomedical journal article combine embed concepts common space result largest ever set embeddings one hundred and eight thousand, four hundred and seventy-seven medical concepts evaluate approach present new benchmark methodology base statistical power specifically design test embeddings medical concepts approach call cui2vec attain state art performance relative previous methods instance finally provide downloadable set pre train embeddings researchers use well online tool interactive exploration cui2vec embeddings
several work propose learn two path neural network map image texts respectively share euclidean space geometry capture useful semantic relationships multi modal embed train use various task notably image caption present work introduce new architecture type visual path leverage recent space aware pool mechanisms combine textual path jointly train scratch semantic visual embed offer versatile model train supervision caption image yield new state art performance cross modal retrieval also allow localization new concepts embed space input image deliver state art result visual ground phrase
ability identify sentiment text refer sentiment analysis one natural adult humans task however one computer perform default identify sentiments automate algorithmic manner useful capability business research search understand consumers think products service understand human sociology propose two new genetic algorithms gas task automate text sentiment analysis gas learn whether word occur text corpus either sentiment amplifier word correspond magnitude sentiment word horrible add linearly final sentiment amplifier word contrast typically adjectives adverbs like multiply sentiment follow word increase decrease negate sentiment follow word sentiment full text sum term approach grow sentiment amplifier dictionary reuse purpose feed machine learn algorithms report result multiple experiment conduct large amazon data set result reveal propose approach able outperform several public commercial sentiment analysis algorithms
recent advance neural autoregressive model improve performance speech synthesis ss however lack ability model global characteristics speech speaker individualities speak style particularly characteristics label make neural autoregressive ss systems expressive still open issue paper propose combine voiceloop autoregressive ss model variational autoencoder vae approach unlike traditional autoregressive ss systems use vae model global characteristics explicitly enable expressiveness synthesize speech control unsupervised manner experiment use vctk blizzard2012 datasets show vae help voiceloop generate higher quality speech control expressions synthesize speech incorporate global characteristics speech generate process
deep neural network dnn acoustic model often use discriminative sequence train optimise objective function better approximate word error rate wer frame base train sequence train normally implement use stochastic gradient descent sgd hessian free hf train paper propose alternative batch style optimisation framework employ natural gradient ng approach traverse parameter space correct gradient accord local curvature kl divergence ng optimisation process converge quickly hf furthermore propose ng approach apply sequence discriminative train criterion efficacy ng method show use experiment multi genre broadcast mgb transcription task demonstrate computational efficiency accuracy result dnn model
currency trade forex largest world market term volume analyze trade tweet eur usd currency pair period three years first large number tweet manually label twitter stance classification model construct model classify tweet trade stance signal buy hold sell eur vs usd twitter stance compare actual currency rat apply event study methodology well know financial economics turn large differences twitter stance distribution potential trade return four group twitter users trade robots spammers trade company individual traders additionally observe attempt reputation manipulation post festum removal tweet poor predictions delete reposting identical tweet increase visibility without taint one twitter timeline
one distinguish aspects human language compositionality allow us describe complex environments limit vocabulary previously show neural network agents learn communicate highly structure possibly compositional language base disentangle input eg hand engineer feature humans however learn communicate base well summarize feature work train neural agents simultaneously develop visual perception raw image pixels learn communicate sequence discrete symbols agents play image description game image contain factor color shape train agents use obverter technique agent introspect generate message maximize understand qualitative analysis visualization zero shoot test show agents develop raw image pixels language compositional properties give proper pressure environment
propose quootstrap method extract quotations well name speakers utter large news corpora whereas prior work address problem primarily supervise machine learn approach follow fully unsupervised bootstrapping paradigm leverage redundancy present large news corpora precisely fact quotation often appear across multiple news article slightly different contexts start seed pattern q say method extract set quotation speaker pair q turn use discover new pattern express quotations process repeat larger pattern set algorithm highly scalable demonstrate run large icwsm two thousand and eleven spinn3r corpus validate result crowdsourced grind truth obtain ninety precision forty recall use single seed pattern significantly higher recall value frequently report thus likely interest quotations finally showcase usefulness algorithm output computational social science analyze sentiment express extract quotations
recent advance speech synthesis suggest limitations lossy nature amplitude spectrum minimum phase approximation smooth effect acoustic model overcome use advance machine learn approach paper build framework fairly compare new vocoding acoustic model techniques conventional approach mean large scale crowdsourced evaluation result acoustic model show generative adversarial network autoregressive ar model perform better normal recurrent network ar model perform best evaluation vocoders use ar acoustic model demonstrate wavenet vocoder outperform classical source filter base vocoders particularly generate speech waveforms combination ar acoustic model wavenet vocoder achieve similar score speech quality vocoded speech
generative adversarial network gans promise field image generation however hard train language generation gans originally design output differentiable value discrete language generation challenge cause high level instability train gans consequently past work resort pre train maximum likelihood train gans without pre train wgan objective gradient penalty study present comparison approach furthermore present result experiment indicate better train convergence wasserstein gans wgans weaker regularization term enforce lipschitz constraint
develop android smartophone application software tourist information system especially agent system recommend sightsee spot local hospitality correspond current feel system concierge estimate user emotion mood emotion generate calculations mental state transition network paper system decide next candidates spot foods reason fuzzy petri net order make smooth communication human smartphone system develop hiroshima tourist information describe hospitality concierge system
recently cycle consistent adversarial network cycle gin successfully apply voice conversion different speaker without parallel data although approach individual model need target speaker paper propose adversarial learn framework voice conversion single model train convert voice many different speakers without parallel data separate speaker characteristics linguistic content speech signal autoencoder first train extract speaker independent latent representations speaker embed separately use another auxiliary speaker classifier regularize latent representation decoder take speaker independent latent representation target speaker embed input generate voice target speaker linguistic content source utterance quality decoder output improve patch residual signal produce another pair generator discriminator target speaker set size twenty test preliminary experiment good voice quality obtain conventional voice conversion metrics report also show speaker information properly reduce latent representations
paper explore learn neural network embeddings natural image speech waveforms describe content image embeddings learn directly waveforms without use linguistic transcriptions conventional speech recognition technology prior work investigate set monolingual case use english speech data work represent first effort apply techniques languages beyond english use speak caption collect english hindi show model architecture successfully apply languages demonstrate train multilingual model simultaneously languages offer improve performance monolingual model finally show model capable perform semantic cross lingual speech speech retrieval
optimization patient throughput wait time emergency departments ed important task hospital systems reason emergency severity index esi system patient triage introduce help guide manual estimation acuity level use nurse rank patients organize hospital resources however despite improvements bring manage medical resources triage system greatly depend nurse subjective judgment thus prone human errors propose novel deep model base word attention mechanism design predict number resources ed patient would need approach incorporate routinely available continuous nominal structure data medical text unstructured data include patient chief complaint past medical history medication list nurse assessment collect three hundred and thirty-eight thousand, five hundred ed visit three years large urban hospital use structure unstructured data propose approach achieve auc sim eighty-eight task identify resource intensive patients binary classification accuracy sim forty-four predict exact category number resources multi class classification task give estimate lift nurse performance sixteen accuracy furthermore attention mechanism propose model provide interpretability assign attention score nurse note crucial decision make implementation approach real systems work human health
rapid growth knowledge base kbs question answer knowledge base aka kbqa draw huge attention recent years exist kbqa methods follow call encoder compare framework map question kb facts common embed space similarity question vector fact vectors conveniently compute however inevitably lose original word interaction information preserve original information propose attentive recurrent neural network similarity matrix base convolutional neural network ar smcnn model able capture comprehensive hierarchical information utilize advantage rnn cnn use rnn capture semantic level correlation sequential model nature use attention mechanism keep track entities relations simultaneously meanwhile use similarity matrix base cnn two directions pool extract literal level word interaction match utilize cnns strength model spatial correlation among data moreover develop new heuristic extension method entity detection significantly decrease effect noise method outperform state arts simplequestion benchmark accuracy efficiency
information extraction ie refer automatically extract structure relation tuples unstructured texts common ie solutions include relation extraction open ie systems hardly handle cross sentence tuples severely restrict limit relation type well informal relation specifications eg free text base relation tuples order overcome weaknesses propose novel ie framework name qa4ie leverage flexible question answer qa approach produce high quality relation triple across sentence base framework develop large ie benchmark high quality human evaluation benchmark contain 293k document 2m golden relation triple six hundred and thirty-six relation type compare system ie baselines benchmark result show system achieve great improvements
variational autoencoders vae combine hierarchical rnns emerge powerful framework conversation model however suffer notorious degeneration problem decoders learn ignore latent variables reduce vanilla rnns empirically show degeneracy occur mostly due two reason first expressive power hierarchical rnn decoders often high enough model data use decode distributions without rely latent variables second conditional vae structure whose generation process condition context make range train target sparse rnn decoders easily overfit train data ignore latent variables solve degeneration problem propose novel model name variational hierarchical conversation rnns vhcr involve two key ideas one use hierarchical structure latent variables two exploit utterance drop regularization evaluations two datasets cornell movie dialog ubuntu dialog corpus show vhcr successfully utilize latent variables outperform state art model conversation generation moreover perform several new utterance control task thank hierarchical latent structure
social media become increasingly important data source learn break news follow latest developments ongoing news part possible thank existence mobile devices allow anyone access internet post update anywhere lead turn grow presence citizen journalism consequently social media become go resource journalists process newsgathering use social media newsgathering however challenge suitable tool need order facilitate access useful information report paper provide overview research data mine natural language process mine social media newsgathering discuss five different areas researchers work mitigate challenge inherent social media newsgathering news discovery curation news validation verification content newsgathering dashboards task outline progress make far field summarise current challenge well discuss future directions use computational journalism assist social media newsgathering review relevant computer scientists research news social media well interdisciplinary researchers interest intersection computer science journalism
imagine scene describe natural language realistic layout appearance entities ultimate test spatial visual semantic world knowledge towards goal present composition retrieval fusion network craft model capable learn knowledge video caption data apply generate videos novel caption craft explicitly predict temporal layout mention entities character object retrieve spatio temporal entity segment video database fuse generate scene videos contributions include sequential train components craft jointly model layout appearances losses encourage learn compositional representations retrieval evaluate craft semantic fidelity caption composition consistency visual quality craft outperform direct pixel generation approach generalize well unseen caption unseen video databases text annotations demonstrate craft flintstones new richly annotate video caption dataset twenty-five thousand videos glimpse videos generate craft see https youtube 688vv86n0z8
paper study generative model sequential discrete data tackle exposure bias problem inherent maximum likelihood estimation mle generative adversarial network gans introduce penalize unrealistic generate sample exploit supervision signal discriminator previous model leverage reinforce address non differentiable problem sequential discrete data however unstable property train signal dynamic process adversarial train effectiveness reinforce case hardly guarantee deal problem propose novel approach call cooperative train cot improve train sequence generative model cot transform min max game gans joint maximization framework manage explicitly estimate optimize jensen shannon divergence moreover cot work without necessity pre train via mle crucial success previous methods experiment compare exist state art methods cot show superior least competitive performance sample quality diversity well train stability
multi agent reinforcement learn offer way study communication could emerge communities agents need solve specific problems paper study emergence communication negotiation environment semi cooperative model agent interaction introduce two communication protocols one ground semantics game one textita priori ungrounded form cheap talk show self interest agents use pre ground communication channel negotiate fairly unable effectively use ungrounded channel however prosocial agents learn use cheap talk find optimal negotiate strategy suggest cooperation necessary language emerge also study communication behaviour set one agent interact agents community different level prosociality show agent identifiability aid negotiation
ability algorithms evolve learn compositional communication protocols traditionally study language evolution literature use emergent communication task scale research use contemporary deep learn methods train reinforcement learn neural network agents referential communication game extend previous work agents train symbolic environments develop agents able learn raw pixel data challenge realistic input representation find degree structure find input data affect nature emerge protocols thereby corroborate hypothesis structure compositional language likely emerge agents perceive world structure
acoustically express emotions make communication robot efficient detect emotions like anger could provide clue robot indicate unsafe undesired situations recently several deep neural network base model propose establish new state art result affective state evaluation model typically start process end utterance require mechanism detect end utterance also make difficult use real time communication scenario eg human robot interaction propose emorl model trigger emotion classification soon gain enough confidence listen person speak result minimize need segment audio signal classification achieve lower latency audio signal process incrementally method competitive accuracy strong baseline model allow much earlier prediction
paper study users perception regard controversial product namely self drive autonomous cars find people opinion regard new technology use annotate twitter dataset extract topics positive negative tweet use unsupervised probabilistic model know topic model later use topics well linguist twitter specific feature classify sentiment tweet regard opinions result analysis show people optimistic excite future technology time find dangerous reliable classification task find twitter specific feature hashtags well linguistic feature emphatic word among top attribute classify sentiment tweet
infer socioeconomic attribute social media users occupation income important problem computational social science automate inference characteristics applications personalise recommender systems target computational advertise online political campaign previous work show language feature reliably predict socioeconomic attribute twitter employ information come users social network yet explore complex user characteristics paper describe method predict occupational class income twitter users give information extract extend network learn low dimensional vector representation users ie graph embeddings use representation train predictive model occupational class income result two publicly available datasets show method consistently outperform state art methods task also obtain significant improvements combine graph embeddings textual feature demonstrate social network language information complementary
understand narrative content become increasingly popular topic nonetheless research identify common type narrative character personae impede lack automatic broad coverage evaluation methods argue computationally model actors provide benefit include novel evaluation mechanisms personae specifically propose two actor model task cast prediction versatility rank capture complementary aspects relation actors character portray actor model present technique embed actors movies character roles genres descriptive keywords gaussian distributions translation vectors gaussian variance correspond actors versatility empirical result indicate one technique considerably outperform transe bordes et al two thousand and thirteen ablation baselines two automatically identify persona topics bamman connor smith two thousand and thirteen yield statistically significant improvements task whereas simplistic persona descriptors include age gender perform inconsistently validate prior research
give complexity human mind behavioral flexibility require sophisticate data analysis sift large amount human behavioral evidence model human mind predict human behavior people currently spend significant amount time social media twitter facebook thus many aspects live behaviors digitally capture continuously archive platforms make social media great source large rich diverse human behavioral evidence paper survey recent work apply machine learn infer human traits behavior social media data also point several future research directions
topic model discover latent topic probability give text document generate meaningful topic better represent give document propose new feature extraction technique use data preprocessing stage method consist three step first generate word word pair every single document second apply two way tf idf algorithm word word pair semantic filter third use k mean algorithm merge word pair similar semantic mean experiment carry open movie database omdb reuters dataset 20newsgroup dataset mean average precision score use evaluation metric compare result state art topic model latent dirichlet allocation traditional restrict boltzmann machine propose data preprocessing improve generate topic accuracy one thousand, two hundred and ninety-nine
skip gram negative sample popular variant word2vec originally design tune create word embeddings natural language process use create item embeddings successful applications recommendation field share type data neither evaluate task recommendation applications tend use already tune hyperparameters value even optimal hyperparameters value often know data task dependent thus investigate marginal importance hyperparameter recommendation set large hyperparameter grid search various datasets result reveal optimize neglect hyperparameters namely negative sample distribution number epochs subsampling parameter window size significantly improve performance recommendation task increase order magnitude importantly find optimal hyperparameters configurations natural language process task recommendation task noticeably different
present voice conversion challenge two thousand and eighteen design follow two thousand and sixteen edition aim provide common framework evaluate compare different state art voice conversion vc systems objective challenge perform speaker conversion ie transform vocal identity source speaker target speaker maintain linguistic information update previous challenge consider parallel non parallel data form hub speak task respectively total twenty-three team around world submit systems eleven additionally participate optional speak task large scale crowdsourced perceptual evaluation carry rate submit convert speech term naturalness similarity target speaker identity paper present brief summary state art techniques vc follow detail explanation challenge task result obtain
fundamental problem short text classification emphfeature sparseness lack feature overlap train model test instance classify propose emphclassinet network classifiers train predict miss feature give instance overcome feature sparseness problem use set unlabeled train instance first learn binary classifiers feature predictors predict whether particular feature occur give instance next feature predictor represent vertex vi classinet one one correspondence exist feature predictors vertices weight direct edge eij connect vertex vi vertex vj represent conditional probability give vi exist instance vj also exist instance show classinets generalize word co occurrence graph consider implicit co occurrences feature extract numerous feature train classinet overcome feature sparseness particular give instance vecx find similar feature classinet appear vecx append feature representation vecx moreover propose method base graph propagation find feature indirectly relate give short text evaluate classinets several benchmark datasets short text classification experimental result show use classinet statistically significantly improve accuracy short text classification task without use external resources thesauri find relate feature
speak content process retrieval browse mature sing content still almost completely leave songs human voice carry plenty semantic information speech may consider special type speech highly flexible prosody various problems song audio example significantly change phone duration highly flexible pitch contour make recognition lyric song audio much difficult paper report initial attempt towards goal collect music remove version english songs directly commercial sing content best result obtain tdnn lstm data augmentation three fold speed perturbation plus special approach wer achieve seven thousand, three hundred and ninety significantly lower baseline nine thousand, six hundred and twenty-one still relatively high
major challenge video caption combine audio visual cue exist multi modal fusion methods show encourage result video understand however temporal structure multiple modalities different granularities rarely explore selectively fuse multi modal representations different level detail remain uncharted paper propose novel hierarchically align cross modal attention haca framework learn selectively fuse global local temporal dynamics different modalities furthermore first time validate superior performance deep audio feature video caption task finally haca model significantly outperform previous best systems achieve new state art result widely use msr vtt dataset
collaborative filter cf key technique recommender systems pure cf approach exploit user item interaction data eg click like view suffer sparsity issue items usually associate content information unstructured text eg abstract article review products cf extend leverage text paper develop unify neural framework exploit interaction data content information seamlessly propose framework call lcmr base memory network consist local centralize memories exploit content information interaction data respectively model content information local memories lcmr attentively learn exploit guidance user item interaction real world datasets lcmr show better performance compare various baselines term hit ratio ndcg metrics conduct analyse understand local centralize memories work propose framework
abstractive text summarization task compress rewrite long document short summary maintain saliency direct logical entailment non redundancy work address three important aspects good summary via reinforcement learn approach two novel reward function rougesal entail top coverage base baseline rougesal reward modify rouge metric weight salient phrase word detect via keyphrase classifier entail reward give high length normalize score logically entail summaries use entailment classifier show superior performance improvement reward combine traditional metric rouge base reward via novel effective multi reward approach optimize multiple reward simultaneously alternate mini batch method achieve new state art result include human evaluation cnn daily mail dataset well strong improvements test transfer setup duc two thousand and two
introduce unspeech embeddings base unsupervised learn context feature representations speak language embeddings train nine thousand, five hundred hours crawl english speech data without transcriptions speaker information use straightforward learn objective base context non context discrimination negative sample use siamese convolutional neural network architecture train unspeech embeddings evaluate speaker comparison utterance cluster context feature tdnn hmm acoustic model train ted lium compare vector baselines particularly decode domain speech data recently release common voice corpus show consistent wer reductions release source code pre train unspeech model permissive open source license
multimodal machine learn algorithms aim learn visual textual correspondences previous work suggest concepts concrete visual manifestations may easier learn concepts abstract ones give algorithm automatically compute visual concreteness word topics within multimodal datasets apply approach four settings range image caption image text scrap historical book addition enable explorations concepts multimodal datasets concreteness score predict capacity machine learn algorithms learn textual visual relationships find one concrete concepts indeed easier learn two large number algorithms consider similar failure case three precise positive relationship concreteness performance vary datasets conclude recommendations use concreteness score facilitate future multimodal research
visual reason compositional natural language instructions eg base newly release cornell natural language visual reason nlvr dataset challenge task model need ability create accurate map diverse phrase several object place complex arrangements image map need process answer question statement give order relationship object across three similar image paper propose novel end end neural model nlvr task first use joint bidirectional attention build two way condition visual information language phrase next use rl base pointer network sort process vary number unordered object match order statement phrase three image pool three decisions model achieve strong improvements four six absolute state art structure representation raw image versions dataset
compromise account social network regular user account take entity malicious intent since adversary exploit already establish trust compromise account crucial detect account limit damage propose novel general framework semantic analysis text message come account detect compromise account framework build observation normal users use language measurably different language adversary would use account compromise propose use difference language model users adversaries define novel interpretable semantic feature measure semantic incoherence message stream study effectiveness propose semantic feature use twitter data set evaluation result show propose framework effective discover compromise account social network kl divergence base language model feature work best
work focus task generate natural language descriptions structure table facts contain field nationality occupation etc value indian actor director etc one simple choice treat table sequence field value use standard seq2seq model task however model generic exploit task specific characteristics example generate descriptions table human would attend information two level field macro level ii value within field micro level human would continue attend field timesteps till information field render never return back field nothing leave say capture behavior use fuse bifocal attention mechanism exploit combine micro macro level information ii gate orthogonalization mechanism try ensure field remember time step forget experiment recently release dataset contain fact table people correspond one line biographical descriptions english addition also introduce two similar datasets french german experiment show propose model give twenty-one relative improvement recently propose state art method ten relative improvement basic seq2seq model code datasets develop part work publicly available
develop agents engage complex goal orient dialogues challenge partly main learn signal sparse long conversations paper propose divide conquer approach discover exploit hide structure task enable efficient policy learn first give successful example dialogues propose subgoal discovery network sdn divide complex goal orient task set simpler subgoals unsupervised fashion use subgoals learn multi level policy hierarchical reinforcement learn demonstrate method build dialogue agent composite task travel plan experiment simulate real users show approach perform competitively state art method require human define subgoals moreover show learn subgoals often human comprehensible
pachinko allocation machine pam deep topic model allow represent rich correlation structure among topics direct acyclic graph topics flexibility model however approximate inference difficult perhaps reason small number potential pam architectures explore literature paper present efficient flexible amortize variational inference method pam use deep inference network parameterize approximate posterior distribution manner similar variational autoencoder inference method produce coherent topics state art inference methods pam order magnitude faster allow exploration wider range pam architectures previously study
contemporary era social media influence people make decisions proliferation online review diversify verbose content often cause problems inaccurate decision make since online review impact people walk life take decisions choose appropriate review base podsolization consist important since rely use micro review consistency evaluate review set section micro review concise directly talk product service instead unnecessary verbose content thus micro review help choose review base personalize consistency relate directly indirectly main profile review personalize review selection highly relevant high personalize coverage term match micro review main problem consider paper furthermore personalization user preferences make review selection also consider base personalize users profile towards end propose framework know perview personalize review selection use micro review base propose evaluation metric approach consider two main factor personalize match score subset size personalize review selection algorithm prsa propose make use multiple similarity measure merge highly efficient personalize review match function selection experimental result base use review dataset collect yelpcom micro review dataset obtain foursqurecom show personalize review selection empirical case study
automatically extract useful information electronic medical record along conduct disease diagnose promise task clinical decision supportcds neural language processingnlp exist systems base artificially construct knowledge base auxiliary diagnosis do rule match study present clinical intelligent decision approach base convolutional neural networkscnn automatically extract high level semantic information electronic medical record perform automatic diagnosis without artificial construction rule knowledge base use collect eighteen thousand, five hundred and ninety copy real world clinical electronic medical record train test propose model experimental result show propose model achieve nine thousand, eight hundred and sixty-seven accuracy nine thousand, six hundred and two recall strongly support use convolutional neural network automatically learn high level semantic feature electronic medical record conduct assist diagnosis feasible effective
annotate temporal relations temprel events describe natural language know labor intensive partly total number temprels quadratic number events result small number document typically annotate limit coverage various lexical semantic phenomena order improve exist approach one possibility make use readily available partially annotate data p partial cover document however miss annotations p know hurt rather help exist systems work case study explore various usages p temprel extraction result show despite miss annotations p still useful supervision signal task within constrain bootstrapping learn framework system describe system publicly available
voice conversion vc aim conversion speaker characteristic without alter content due train data limitations model imperfections difficult achieve believable speaker mimicry without introduce process artifacts performance assessment vc therefore usually involve speaker similarity quality evaluation human panel time consume expensive non reproducible process hinder rapid prototyping new vc technology address artifact assessment use alternative objective approach leverage prior work spoof countermeasures cms automatic speaker verification therein cms use reject fake input replay synthetic convert speech potential automatic speech artifact assessment remain unknown study serve fill gap supplement subjective result two thousand and eighteen voice conversion challenge vcc eighteen data configure standard constant q cepstral coefficient cm quantify extent process artifacts equal error rate ever cm confusability index vc sample real human speech serve artifact measure two cluster vcc eighteen entries identify low quality ones detectable artifacts low eers higher quality ones less artifacts none vcc eighteen systems however perfect eers thirty ideal value would fifty preliminary find suggest potential cms outside original application supplemental optimization benchmarking tool enhance vc technology
work focus problem ground language train agent follow set natural language instructions navigate target object environment agent receive visual information raw pixels natural language instruction tell task need achieve train end end way develop attention mechanism multi modal fusion visual textual modalities allow agent learn complete task achieve language ground experimental result show attention mechanism outperform exist multi modal fusion mechanisms propose 2d 3d environments order solve mention task term speed success rate show learn textual representations semantically meaningful follow vector arithmetic embed space effectiveness attention approach contemporary fusion mechanisms also highlight textual embeddings learn different approach also show model generalize effectively unseen scenarios exhibit zero shoot generalization capabilities 2d 3d environments code 2d environment well model develop 2d 3d available https githubcom rl lang ground rl lang grind
false information create spread easily web social media platforms result widespread real world impact characterize false information proliferate social platforms succeed deceive readers critical develop efficient detection algorithms tool early detection recent surge research area aim address key issue use methods base feature engineer graph mine information model majority research primarily focus two broad categories false information opinion base eg fake review fact base eg false news hoax therefore work present comprehensive survey span diverse aspects false information namely actors involve spread false information ii rationale behind successfully deceive readers iii quantify impact false information iv measure characteristics across different dimension finally iv algorithms develop detect false information create unify framework describe recent methods highlight number important directions future research
entrainment know adaptation mechanism cause interaction participants adapt synchronize acoustic characteristics understand interlocutors tend adapt speak style entrainment involve measure range acoustic feature compare via multiple signal comparison methods work present turn level distance measure obtain unsupervised manner use deep neural network dnn model call neural entrainment distance ned metric establish framework learn embed population wide entrainment unlabeled train corpus use framework set acoustic feature validate measure experimentally show efficacy distinguish real conversations fake ones create randomly shuffle speaker turn moreover show real world evidence validity propose measure find high value ned associate high rat emotional bond suicide assessment interview consistent prior study
exist applications include huge amount knowledge reach deep neural network paper present novel approach integrate call exist applications deep learn architectures use approach estimate application functionality estimator implement deep neural network dnn estimator embed base network direct comply application interface end end optimization process inference time replace estimator exist application counterpart let base network solve task interact exist application use estimate replace method able train dnn end end less data outperform match dnn interact external application
monitor biomedical literature case adverse drug reactions adrs critically important time consume task pharmacovigilance development computer assist approach aid process different form subject many recent work one particular area show promise use deep neural network particular convolutional neural network cnns detection adr relevant sentence use token level convolutions general purpose word embeddings architecture show good performance relative traditional model well long short term memory lstm model work evaluate compare two different cnn architectures use ade corpus addition show de duplicate adr relevant sentence greatly reduce overoptimism classification result finally evaluate use word embeddings specifically develop biomedical text show lead better performance task
though impressive result achieve visual caption task generate abstract stories photo stream still little tap problem different caption stories expressive language style contain many imaginary concepts appear image thus pose challenge behavioral clone algorithms furthermore due limitations automatic metrics evaluate story quality reinforcement learn methods hand craft reward also face difficulties gain overall performance boost therefore propose adversarial reward learn arel framework learn implicit reward function human demonstrations optimize policy search learn reward function though automatic eval uation indicate slight performance boost state art sota methods clone expert behaviors human evaluation show approach achieve significant improvement generate human like stories sota systems
paper summarize recent progress make deep learn base acoustic model motivation insights behind survey techniques first discuss acoustic model effectively exploit variable length contextual information recurrent neural network rnns convolutional neural network cnns various combination model describe acoustic model optimize end end emphasis feature representations learn jointly rest system connectionist temporal classification ctc criterion attention base sequence sequence model illustrate robustness issue speech recognition systems discuss acoustic model adaptation speech enhancement separation robust train strategies also cover model techniques lead efficient decode discuss possible future directions acoustic model research
neural sequence sequence model prove accurate robust many sequence prediction task become standard approach automatic translation text model work five stage blackbox process involve encode source sequence vector space decode new target sequence process standard like many deep learn methods remain quite difficult understand debug work present visual analysis tool allow interaction train sequence sequence model stage translation process aim identify pattern learn detect model errors demonstrate utility tool several real world large scale sequence sequence use case
novel neural model propose recent years learn domain shift model however evaluate single task proprietary datasets compare weak baselines make comparison model difficult paper evaluate classic general purpose bootstrapping approach context neural network domain shift vs recent neural approach propose novel multi task tri train method reduce time space complexity classic tri train extensive experiment two benchmarks negative novel method establish new state art sentiment analysis fare consistently best importantly arrive somewhat surprise conclusion classic tri train additions outperform state art conclude classic approach constitute important strong baseline
current end end machine read question answer qanda model primarily base recurrent neural network rnns attention despite success model often slow train inference due sequential nature rnns propose new qanda architecture call qanet require recurrent network encoder consist exclusively convolution self attention convolution model local interactions self attention model global interactions squad dataset model 3x 13x faster train 4x 9x faster inference achieve equivalent accuracy recurrent model speed gain allow us train model much data hence combine model data generate backtranslation neural machine translation model squad dataset single model train augment data achieve eight hundred and forty-six f1 score test set significantly better best publish f1 score eight hundred and eighteen
transcription sub title open domain videos still challenge domain automatic speech recognition asr due data challenge acoustics variable signal process essentially unrestricted domain data previous work show visual channel specifically object scene feature help adapt acoustic model language model lm recognizer expand work end end approach case connectionist temporal classification ctc base approach retain separation lm sequence sequence s2s approach information source adapt together single model paper also analyze behavior ctc s2s model noisy video data corpus compare result clean wall street journal wsj corpus provide insight robustness approach
represent word probability densities rather point vectors probabilistic word embeddings capture rich interpretable semantic information uncertainty uncertainty information particularly meaningful capture entailment relationships whereby general word entity correspond broad distributions encompass specific word animal instrument introduce density order embeddings learn hierarchical representations encapsulation probability densities particular propose simple yet effective loss function distance metrics well graph base scheme select negative sample better learn hierarchical density representations approach provide state art performance wordnet hypernym relationship prediction task challenge hyperlex lexical entailment dataset retain rich interpretable density representation
investigate deep neural network performance textindependent speaker recognition task demonstrate use angular softmax activation last classification layer classification neural network instead simple softmax activation allow train generalize discriminative speaker embed extractor cosine similarity effective metric speaker verification embed space also address problem choose architecture extractor find deep network residual frame level connections outperform wide relatively shallow architectures paper also propose several improvements previous dnn base extractor systems increase speaker recognition accuracy show discriminatively train similarity metric learn approach outperform standard lda plda method embed backend result obtain speakers wild nist sre two thousand and sixteen evaluation set demonstrate robustness propose systems deal close real life condition
present sound board social chatbot two thousand and seventeen amazon alexa prize system architecture consist several components include speak language process dialogue management language generation content management emphasis user centric content drive design also share insights gain large scale online log base one hundred and sixty thousand conversations real world users
sequence sequence attention base model recently show promise result automatic speech recognition asr task integrate acoustic pronunciation language model single neural network model transformer new sequence sequence attention base model rely entirely self attention without use rnns convolutions achieve new single model state art bleu neural machine translation nmt task since outstanding performance transformer extend speech concentrate basic architecture sequence sequence attention base model mandarin chinese asr task furthermore investigate comparison syllable base model context independent phoneme ci phoneme base model transformer mandarin chinese additionally greedy cascade decoder transformer propose map ci phoneme sequence syllable sequence word sequence experiment hkust datasets demonstrate syllable base model transformer perform better ci phoneme base counterpart achieve character error rate cer emph2877 competitive state art cer two hundred and eighty joint ctc attention base encoder decoder network
motivation ontologies widely use biology data annotation integration analysis addition formally structure axioms ontologies contain meta data form annotation axioms provide valuable piece information characterize ontology class annotations commonly use ontologies include class label descriptions synonyms despite rich source semantic information ontology meta data generally unexploited ontology base analysis methods semantic similarity measure result propose novel method opa2vec generate vector representations biological entities ontologies combine formal ontology axioms annotation axioms ontology meta data apply word2vec model pre train pubmed abstract produce feature vectors collect data validate method two different ways first use obtain vector representations proteins similarity measure predict protein protein interaction ppi two different datasets second evaluate method predict gene disease associations base phenotype similarity generate vector representations genes diseases use phenotype ontology apply obtain vectors predict gene disease associations two experiment illustration possible applications method opa2vec use produce vector representations biomedical entity give type biomedical ontology availability https githubcom bio ontology research group opa2vec contact roberthoehndorfkaustedusa xingaokaustedusa
work study credit assignment problem reward augment maximum likelihood raml learn establish theoretical equivalence token level counterpart raml entropy regularize reinforcement learn inspire connection propose two sequence prediction algorithms one extend raml fine grain credit assignment improve actor critic systematic entropy regularization two benchmark datasets show propose algorithms outperform raml actor critic respectively provide new alternatives sequence prediction
documentation errors increase healthcare cost unnecessary patient deaths standard language diagnose bill icd cod serve foundation medical documentation worldwide despite prevalence electronic medical record hospitals still witness high level icd miscoding paper propose automatically document icd cod far field speech recognition far field speech occur microphone locate several meter source common smart home security systems method combine acoustic signal process recurrent neural network recognize document icd cod real time evaluate model collect far field speech dataset icd ten cod find model achieve eighty-seven accuracy bleu score eighty-five sample unsupervised medical language model method able outperform exist methods overall work show potential automatic speech recognition provide efficient accurate cost effective healthcare documentation
language recognition system typically train directly optimize classification error target language label without use external meta information estimation model parameters however label independent dependency enforce example language family affect negatively classification external information source eg audio encode telephony video speech also decrease classification accuracy paper attempt solve issue construct deep hierarchical neural network different level meta information encapsulate attentive prediction units also embed train progress propose method learn auxiliary task obtain robust internal representation construct variant attentive units within hierarchical model final result structural prediction target language closely relate language family algorithm reflect staircase way learn architecture train advance fundamental audio encode language family level finally target language level process improve generalization also tackle issue imbalanced class priors channel variability deep neural network model experimental find show propose architecture outperform state art vector approach small big language corpora significant margin
design powerful tool support cook activities rapidly gain popularity due massive amount available data well recent advance machine learn capable analyze paper propose cross modal retrieval model align visual textual data like picture dish recipes share representation space describe effective learn scheme capable tackle large scale problems validate recipe1m dataset contain nearly one million picture recipe pair show effectiveness approach regard previous state art model present qualitative result computational cook use case
twitter one popular microblogging service world great amount information within twitter make important information channel people learn share news twitter hashtag popular feature view human label information people use identify topic tweet many researchers propose event detection approach monitor twitter data determine whether special events accidents extreme weather earthquakes crimes take place although many approach use hashtags one feature explicitly focus effectiveness use hashtags event detection study propose event detection approach utilize hashtags tweet adopt feature extraction use streamcube apply cluster k mean approach experiment demonstrate k mean approach perform better streamcube cluster result discussion optimal k value k mean approach also provide
text generation crucial task nlp recently several adversarial generative model propose improve exposure bias problem text generation though model gain great success still suffer problems reward sparsity mode collapse order address two problems paper employ inverse reinforcement learn irl text generation specifically irl framework learn reward function train data optimal policy maximum expect total reward similar adversarial model reward policy function irl optimize alternately method two advantage one reward function produce dense reward signal two generation policy train entropy regularize policy gradient encourage generate diversify texts experiment result demonstrate propose method generate higher quality texts previous methods
key aspect vqa model interpretable ability grind answer relevant regions image current approach capability rely supervise learn human annotate ground train attention mechanisms inside vqa architecture unfortunately obtain human annotations specific visual ground difficult expensive work demonstrate effectively train vqa architecture ground supervision automatically obtain available region descriptions object annotations also show model train mine supervision generate visual ground achieve higher correlation respect manually annotate ground meanwhile achieve state art vqa accuracy
study evaluate performances lstm network detect extract intent content com mands financial chatbot present two techniques sequence sequence learn multi task learn might improve previous task
accurate device keyword spot kws low false accept false reject rate crucial customer experience far field voice control conversational agents particularly challenge maintain low false reject rate real world condition ambient noise external source tv household appliances speech direct device b imperfect cancellation audio playback device result residual echo process acoustic echo cancellation aec system paper propose data augmentation strategy improve keyword spot performance challenge condition train set audio artificially corrupt mix music tv movie audio different signal interference ratios result show get around thirty forty-five relative reduction false reject rat range false alarm rat audio playback devices
investigate impact noisy linguistic feature performance japanese speech synthesis system base neural network use wavenet vocoder compare ideal system use manually correct linguistic feature include phoneme prosodic information train test set systems use corrupt linguistic feature subjective objective result demonstrate corrupt linguistic feature especially test set affect ideal system performance significantly statistical sense due mismatch condition train test set interestingly utterance level turing test show listeners difficult time differentiate synthetic speech natural speech indicate add noise linguistic feature train set partially reduce effect mismatch regularize model help system perform better linguistic feature test set noisy
explosion amount news journalistic content generate across globe couple extend instantaneous access information online media make difficult time consume monitor news developments opinion formation real time increase need tool pre process analyse classify raw text extract interpretable content specifically identify topics content drive group article present methodology bring together powerful vector embeddings natural language process tool graph theory exploit diffusive dynamics graph reveal natural partition across scale framework use recent deep neural network text analysis methodology doc2vec represent text vector form apply multi scale community detection method markov stability partition similarity graph document vectors method allow us obtain cluster document similar content different level resolution unsupervised manner showcase approach analysis corpus nine thousand news article publish vox media one year result show consistent group document accord content without priori assumptions number type cluster find multilevel cluster reveal quasi hierarchy topics subtopics increase intelligibility improve topic coherence compare external taxonomy service standard topic detection methods
recent work show train convolutional neural network cnns rapidly large image datasets transfer knowledge gain model variety task follow radford two thousand and seventeen work demonstrate similar scalability transfer recurrent neural network rnns natural language task utilize mix precision arithmetic 32k batch size distribute across one hundred and twenty-eight nvidia tesla v100 gpus able train character level four thousand and ninety-six dimension multiplicative lstm mlstm unsupervised text reconstruction three epochs forty gb amazon review dataset four hours runtime compare favorably previous work take one month train size configuration one epoch dataset converge large batch rnn model challenge recent work suggest scale learn rate function batch size find simply scale learn rate function batch size lead either significantly worse convergence immediate divergence problem provide learn rate schedule allow model converge 32k batch size since model converge amazon review dataset hours compute requirement one hundred and twenty-eight tesla v100 gpus substantial commercially available work open large scale unsupervised nlp train commercial applications deep learn researchers model train public private text datasets overnight
automatic speech process systems speaker diarization crucial front end component separate segment different speakers inspire recent success deep neural network dnns semantic inferencing triplet loss base architectures successfully use problem however exist work utilize conventional vectors input representation build simple fully connect network metric learn thus fully leverage model power dnn architectures paper investigate importance learn effective representations sequence directly metric learn pipelines speaker diarization specifically propose employ attention model learn embeddings metric jointly end end fashion experiment conduct callhome conversational speech corpus diarization result demonstrate besides provide unify model propose approach achieve improve performance compare exist approach
recurrent neural network rnns temporal network cumulative nature show promise result various natural language process task despite success still remain challenge understand hide behavior work analyze interpret cumulative nature rnn via propose technique name layer wise semantic accumulation lisa explain decisions detect likely ie saliency pattern network rely decision make demonstrate one lisa rnn accumulate build semantics sequential process give text example expect response two example2pattern saliency pattern look like category data accord network decision make analyse sensitiveness rnns different input check increase decrease prediction score extract saliency pattern learn network employ two relation classification datasets semeval ten task eight tac kbp slot fill explain rnn predictions via lisa example2pattern
previous similar case common law systems use reference respect current case identical situations treat similarly every case however current approach judgment document similarity computation fail capture core semantics judgment document therefore suffer lower accuracy higher computation complexity paper knowledge block summarization base machine learn approach propose compute semantic similarity chinese judgment document utilize domain ontologies judgment document core semantics chinese judgment document summarize base knowledge block wmd algorithm use calculate similarity knowledge block last relate experiment make illustrate approach effective efficient achieve higher accuracy faster computation speed comparison traditional approach
train deep recurrent neural network rnn architectures complicate due increase network complexity disrupt learn higher order abstract use deep rnn case fee forward network train deep structure simple faster learn long term temporal information possible paper propose residual memory neural network rmn architecture model short time dependencies use deep fee forward layer residual time delay connections residual connection pave way construct deeper network enable unhindered flow gradients time delay units capture temporal information share weight number layer rmn signify hierarchical process depth temporal depth computational complexity train rmn significantly less compare deep recurrent network rmn extend bi directional rmn brmn capture past future information experimental analysis do ami corpus substantiate capability rmn learn long term information hierarchical information recognition performance rmn train three hundred hours switchboard corpus compare various state art lvcsr systems result indicate rmn brmn gain six thirty-eight relative improvement lstm blstm network
audio tag aim predict one several label audio clip many previous work use weakly label data wld audio tag presence absence sound events know order sound events unknown use order information sound events propose sequential label data sld presence absence order information sound events know utilize sld audio tag propose convolutional recurrent neural network follow connectionist temporal classification crnn ctc objective function map audio clip spectrogram sld experiment show crnn ctc obtain area curve auc score nine hundred and eighty-six audio tag outperform baseline crnn nine hundred and eight eight hundred and fifteen max pool average pool respectively addition show crnn ctc ability predict order sound events audio clip
yang et al two thousand and sixteen hierarchical attention network han create document classification attention layer use visualize text influential classify document thereby explain model prediction successfully apply han sequential analysis task form real time monitor turn take conversations however discover instance attention weight uniform stop point indicate turn equivalently influential classifier prevent meaningful visualization real time human review classifier improvement observe attention weight turn fluctuate conversations progress indicate turn vary influence base conversation state leverage observation develop method create informative real time visuals confirm human reviewers case uniform attention weight use change turn importance conversation progress time
paper propose novel question guide hybrid convolution qghc network visual question answer vqa state art vqa methods fuse high level textual visual feature neural network abandon visual spatial information learn multi modal featuresto address problems question guide kernels generate input question design convolute visual feature capture textual visual relationship early stage question guide convolution tightly couple textual visual information also introduce parameters learn kernels apply group convolution consist question independent kernels question dependent kernels reduce parameter size alleviate fit hybrid convolution generate discriminative multi modal feature fewer parameters propose approach also complementary exist bilinear pool fusion attention base vqa methods integrate method could boost performance extensive experiment public vqa datasets validate effectiveness qghc
learn disentangle representations high dimensional data currently active research area however compare field computer vision less work do speech process paper provide review two representative efforts topic propose novel concept fine grain disentangle speech representation learn
identify interactions proteins important understand underlie biological process extract protein protein interaction ppi raw text often difficult previous supervise learn methods use handcraft feature human annotate data set paper propose novel tree recurrent neural network structure attention architecture ppi architecture achieve state art result precision recall f1 score aim bioinfer benchmark data set moreover model achieve significant improvement previous best model without explicit feature extraction experimental result show traditional recurrent network inferior performance compare tree recurrent network supervise ppi problem
technical compute challenge application area program languages address evince unusually large number specialize languages area eg matlab r complexity common software stack often involve multiple languages custom code generators believe ultimately due key characteristics domain highly complex operators need extensive code specialization performance desire permissive high level program style allow productive experimentation julia language attempt provide effective structure kind program allow programmers express complex polymorphic behaviors use dynamic multiple dispatch parametric type form extension reuse permit paradigm prove valuable technical compute report approach allow domain experts express useful abstractions simultaneously provide natural path better performance high level technical code
present lemmatag featureless neural network architecture jointly generate part speech tag lemmas sentence use bidirectional rnns character level word level embeddings demonstrate task benefit share encode part network predict tag subcategories use tagger output input lemmatizer evaluate model across several languages complex morphology surpass state art accuracy part speech tag lemmatization czech german arabic
bilingual word embeddings widely use capture similarity lexical semantics different human languages however many applications cross lingual semantic search question answer largely benefit cross lingual correspondence sentence lexicons bridge gap propose neural embed model leverage bilingual dictionaries propose model train map literal word definitions cross lingual target word explore different sentence encode techniques enhance learn process limit resources model adopt several critical learn strategies include multi task learn different bridge languages joint learn dictionary model bilingual word embed model experimental evaluation focus two applications result cross lingual reverse dictionary retrieval task show model promise ability comprehend bilingual concepts base descriptions highlight effectiveness propose learn strategies improve performance meanwhile model effectively address bilingual paraphrase identification problem significantly outperform previous approach
last decade variety topic model propose text engineer however except probabilistic latent semantic analysis plsa latent dirichlet allocation lda exist topic model seldom apply consider industrial scenarios phenomenon cause fact convenient tool support topic model far intimidate demand expertise labor design implement parameter inference algorithms software engineer prone simply resort plsa lda without consider whether proper problem hand paper propose configurable topic model framework name familia order bridge huge gap academic research fruit current industrial practice familia support important line topic model widely applicable text engineer scenarios order relieve burden software engineer without knowledge bayesian network familia able conduct automatic parameter inference variety topic model simply change data organization familia software engineer able easily explore broad spectrum exist topic model even design topic model find one best suit problem hand superior extendability familia novel sample mechanism strike balance effectiveness efficiency parameter inference furthermore familia essentially big topic model framework support parallel parameter inference distribute parameter storage utilities necessity familia demonstrate real life industrial applications familia would significantly enlarge software engineer arsenal topic model pave way utilize highly customize topic model real life problems
context information around word help determine actual mean example network use contexts artificial neural network biological neuron network generative topic model infer topic word distributions take little context account extend neural autoregressive topic model exploit full context information around word document language model fashion result improve performance term generalization interpretability applicability apply model approach seven data set various domains demonstrate approach consistently outperform stateof art generative topic model learn representations show average gain ninety-six fifty-seven vs fifty-two precision retrieval fraction two seventy-two five hundred and eighty-two vs five hundred and forty-three f1 text categorization
technical report present jldadmm easy use java toolkit conventional topic model jldadmm release provide alternatives topic model normal short texts provide implementations latent dirichlet allocation topic model one topic per document dirichlet multinomial mixture model ie mixture unigrams use collapse gibbs sample addition jldadmm supply document cluster evaluation compare topic model jldadmm open source available download https githubcom datquocnguyen jldadmm
deep learn model achieve remarkable success natural language inference nli task model widely explore hard interpret often unclear actually work paper take step toward explain deep learn base model case study popular neural model nli particular propose interpret intermediate layer nli model visualize saliency attention lstm gate signal present several examples methods able reveal interest insights identify critical information contribute model decisions
generate natural question image semantic task require use visual language modality learn multimodal representations image multiple visual language contexts relevant generate question namely place caption tag paper propose use exemplars obtain relevant context obtain use multimodal differential network produce natural engage question generate question show remarkable similarity natural question validate human study observe propose approach substantially improve state art benchmarks quantitative metrics bleu meteor rouge cider
humans imagine scene sound want machine use conditional generative adversarial network gans apply techniques include spectral norm projection discriminator auxiliary classifier compare naive conditional gin model generate image better quality term subjective objective evaluations almost three fourth people agree model ability generate image relate sound inputting different volumes sound model output different scale change base volumes show model truly know relationship sound image extent
work address problem author name homonymy web science aim efficient simple straightforward solution introduce novel probabilistic similarity measure author name disambiguation base feature overlap use researcher id available subset web science evaluate application measure context agglomeratively cluster author mention focus concise evaluation show clearly problem setups time cluster process approach work best contrast work field sceptical towards performance author name disambiguation methods general compare approach trivial single cluster baseline result present separately correct cluster size explain treat case together trivial baseline sophisticate approach hardly distinguishable term evaluation result model show state art performance correct cluster size without discriminative train tune one convergence parameter
ensembling word embeddings improve distribute word representations show good success natural language process task recent years approach either carry straightforward mathematical operations set vectors use unsupervised learn find lower dimensional representation work compare meta embeddings train different losses namely loss function account angular distance reconstruct embed target account normalize distance base vector length argue meta embeddings better treat ensemble set equally unsupervised learn respective quality embed unknown upstream task prior meta embed show normalization methods account cosine kl divergence objectives outperform meta embed train standard ell1 ell2 loss textitdefacto word similarity relatedness datasets find outperform exist meta learn strategies
task conduct visually ground dialog involve learn goal orient cooperative dialog autonomous agents exchange information scene several round question answer natural language posit require artificial agents adhere rule human language also require maximize information exchange dialog ill pose problem observe humans stray common language social creatures live communities communicate many people everyday far easier stick common language even cost efficiency loss use inspiration propose evaluate multi agent community base dialog framework agent interact learn multiple agents show community enforce regularization result relevant coherent dialog judge human evaluators without sacrifice task performance judge quantitative metrics
lstms rnn variants show strong performance character level language model model typically train use truncate backpropagation time common assume success stem ability remember long term contexts paper show deep sixty-four layer transformer model fix context outperform rnn variants large margin achieve state art two popular benchmarks one hundred and thirteen bits per character text8 one hundred and six enwik8 get good result depth show important add auxiliary losses intermediate network layer intermediate sequence position
recent breakthroughs computer vision natural language process spur interest challenge multi modal task visual question answer visual dialogue task one successful approach condition image base convolutional network computation language via feature wise linear modulation film layer ie per channel scale shift propose generate parameters film layer go hierarchy convolutional network multi hop fashion rather prior work alternate attend language input generate film layer parameters approach better able scale settings longer input sequence dialogue demonstrate multi hop film generation achieve state art short input sequence task referit par single hop film generation also significantly outperform prior state art single hop film generation guesswhat visual dialogue task
text image translation active area research recent past ability network learn mean sentence generate accurate image depict sentence show ability model think like humans popular methods text image translation make use generative adversarial network gans generate high quality image base text input generate image always reflect mean sentence give model input address issue use caption network caption generate image exploit distance grind truth caption generate caption improve network show extensive comparisons method exist methods
binary code analysis allow analyze binary code without access correspond source code binary disassembly express assembly language inspire us approach binary analysis leverage ideas techniques natural language process nlp rich area focus process text various natural languages notice binary code analysis nlp share lot analogical topics semantics extraction summarization classification work utilize ideas address two important code similarity comparison problems give pair basic block different instruction set architectures isas determine whether semantics similar ii give piece code interest determine contain another piece assembly code different isa solutions two problems many applications cross architecture vulnerability discovery code plagiarism detection implement prototype system innereye perform comprehensive evaluation comparison approach exist approach problem show system outperform term accuracy efficiency scalability case study utilize system demonstrate solution problem ii effective moreover research showcases apply ideas techniques nlp large scale binary code analysis
many recent paper address read comprehension examples consist question passage answer tuples presumably model must combine information question passages predict correspond answer however despite intense interest topic hundreds publish paper vie leaderboard dominance basic question difficulty many popular benchmarks remain unanswered paper establish sensible baselines babi squad cbt cnn datasets find question passage model often perform surprisingly well fourteen twenty babi task passage model achieve greater fifty accuracy sometimes match full model interestingly cbt provide twenty sentence stories last need comparably accurate prediction comparison squad cnn appear better construct
early detection preventable diseases important better disease management improve inter ventions efficient health care resource allocation various machine learn approacheshave develop utilize information electronic health record ehr task majorityof previous attempt however focus structure field lose vast amount information inthe unstructured note work propose general multi task framework disease onsetprediction combine free text medical note structure information compareperformance different deep learn architectures include cnn lstm hierarchical modelsin contrast traditional text base prediction model approach require disease specificfeature engineer handle negations numerical value exist text ourresults cohort one million patients show model use text outperform modelsusing structure data model capable use numerical value negations thetext addition raw text improve performance additionally compare differentvisualization methods medical professionals interpret model predictions
accurate time series forecast vital numerous areas application transportation energy finance economics etc however modern techniques able explore large set temporal data build forecast model typically neglect valuable information often available form unstructured text although data radically different format often contain contextual explanations many pattern observe temporal data paper propose two deep learn architectures leverage word embeddings convolutional layer attention mechanisms combine text information time series data apply approach problem taxi demand forecast event areas use publicly available taxi data new york empirically show fuse two complementary cross modal source information propose model able significantly reduce error forecast
several recent paper investigate active learn al mitigate data dependence deep learn natural language process however applicability al real world problems remain open question supervise learn practitioners try many different methods evaluate validation set select model al afford luxury course one al run agent annotate dataset exhaust label budget thus give new task active learner opportunity compare model acquisition function paper provide large scale empirical study deep active learn address multiple task multiple datasets multiple model full suite acquisition function find across settings bayesian active learn disagreement use uncertainty estimate provide either dropout bay backprop significantly improve iid baselines usually outperform classic uncertainty sample
requirements elicitation require extensive knowledge deep understand problem domain final system situate however many software development project analysts require elicit requirements unfamiliar domain often cause communication barriers analysts stakeholders paper propose requirements elicitation aid tool elica help analysts better understand target application domain dynamic extraction label requirements relevant knowledge extract relevant term leverage flexibility power weight finite state transducers wfsts dynamic model natural language process task addition information convey text elica capture process non linguistic information intention speakers confidence level analytical tone emotions extract information make available analysts set label snippets highlight relevant term also export artifact requirements engineer process application usefulness elica demonstrate case study study show pre exist relevant information application domain information capture elicitation meet conversation stakeholders intentions capture use support analysts achieve task
follow particular news story online important difficult task relevant information often scatter across different domains source eg news article blog comment tweet present various format language style may overlap thousands stories work join areas topic track entity disambiguation propose framework name story disambiguation cross domain story track approach build real time entity disambiguation learn rank framework represent update rich semantic structure news stories give target news story specify seed set document goal effectively select new story relevant document incoming document stream represent stories entity graph model story track problem learn rank task enable us track content high accuracy multiple domains real time study range text entity graph base feature understand type feature effective represent stories propose new semi supervise learn techniques automatically update story representation time empirical study show outperform accuracy state art methods track mix domain document stream require fewer label data seed track stories particularly case local news stories easily shadow trend stories complex news stories ambiguous content noisy stream environments
whatsapp two thousand and eighteen significant component global information communication infrastructure especially develop countries however probably due strong end end encryption whatsapp become attractive place dissemination misinformation extremism form undesirable behavior paper investigate public perception whatsapp lens media analyze two large datasets news show kind content associate whatsapp different regions world time analyse include examination name entities general vocabulary topics address news article mention whatsapp well polarity texts among result demonstrate vocabulary topics around term whatsapp media change years two thousand and eighteen concentrate matter relate misinformation politics criminal scam generally find useful understand impact tool like whatsapp play contemporary society see communities
twitter provide great opportunity public libraries disseminate information variety purpose twitter data apply different domains health politics history thousands public libraries us study yet investigate content social media post like tweet find interest moreover traditional content analysis twitter content efficient task explore thousands tweet therefore need automatic methods overcome limitations manual methods paper propose computational approach collect analyze use twitter application program interfaces api investigate one hundred and thirty-eight thousand tweet forty-eight us west coast libraries use topic model find twenty topics assign five categories include public relations book event train social good result show us west coast libraries interest use twitter public relations book relate events research practical theoretical applications libraries well organizations explore social media actives customer
although millions transgender people world lack information exist health issue issue consequences medical field nascent understand identify meet population health relate need social media sit like twitter provide new opportunities transgender people overcome barriers share personal health experience research employ computational framework collect tweet self identify transgender users detect health relate identify information need framework significant provide macro scale perspective issue lack investigation national demographic level find identify fifty-four distinct health relate topics group seven broader categories find linguistic topical differences health relate information share transgender men tm com par transgender women tw find help inform medical policy base strategies health interventions within transgender communities also propose approach inform development computational strategies identify health relate information need marginalize populations
recurrent neural network rnns prove effective model sequential data apply boost variety task document classification speech recognition machine translation exist rnn model design sequence assume identically independently distribute iid however many real world applications sequence naturally link example web document connect hyperlinks genes interact one hand link sequence inherently iid pose tremendous challenge exist rnn model hand link sequence offer link information addition sequential information enable unprecedented opportunities build advance rnn model paper study problem rnn link sequence particular introduce principled approach capture link information propose link recurrent neural network linkedrnn model sequential link information coherently conduct experiment real world datasets multiple domains experimental result validate effectiveness propose framework
paper propose new architecture speaker adaptation multi speaker neural network speech synthesis systems unseen speaker voice build use relatively small amount speech data without transcriptions sometimes call unsupervised speaker adaptation specifically concatenate layer audio input perform unsupervised speaker adaptation concatenate text input synthesize speech text two new train scheme new architecture also propose paper train scheme limit speech synthesis applications suggest experimental result show propose model enable adaptation unseen speakers use untranscribed speech also improve performance multi speaker model speaker adaptation use transcribe audio file
recent advance representation learn adversarial train seem succeed remove unwanted feature learn representation show demographic information author encode recover intermediate representations learn text base neural classifiers implication decisions classifiers train textual data agnostic likely condition demographic attribute attempt remove demographic information use adversarial train find adversarial component achieve chance level development set accuracy train post hoc classifier train encode sentence first part still manage reach substantially higher classification accuracies data behavior consistent across several task demographic properties datasets explore several techniques improve effectiveness adversarial component main conclusion cautionary one rely adversarial train achieve invariant representation sensitive feature
paper study product title summarization problem e commerce applications display mobile devices compare conventional sentence summarization product title summarization extra essential constraints example factual errors loss key information intolerable e commerce applications therefore abstract two constraints product title summarization introduce irrelevant information ii retain key information eg brand name commodity name address issue propose novel multi source pointer network add new knowledge encoder pointer network first constraint handle pointer mechanism second constraint restore key information copy word knowledge encoder help soft gate mechanism evaluation build large collection real world product title along human write short title experimental result demonstrate model significantly outperform baselines finally online deployment propose model yield significant business impact measure click rate
present quac dataset question answer context contain 14k information seek qa dialogs 100k question total dialogs involve two crowd workers one student pose sequence freeform question learn much possible hide wikipedia text two teacher answer question provide short excerpt text quac introduce challenge find exist machine comprehension datasets question often open end unanswerable meaningful within dialog context show detail qualitative evaluation also report result number reference model include recently state art read comprehension architecture extend model dialog context best model underperform humans twenty f1 suggest significant room future work data dataset baseline leaderboard available http quacai
humans gather information engage conversations involve series interconnect question answer machine assist information gather therefore essential enable answer conversational question introduce coqa novel dataset build conversational question answer systems dataset contain 127k question answer obtain 8k conversations text passages seven diverse domains question conversational answer free form text correspond evidence highlight passage analyze coqa depth show conversational question challenge phenomena present exist read comprehension datasets eg coreference pragmatic reason evaluate strong conversational read comprehension model coqa best system obtain f1 score six hundred and fifty-four two hundred and thirty-four point behind human performance eight hundred and eighty-eight indicate ample room improvement launch coqa challenge community http stanfordnlpgithubio coqa
extractive summarization model require sentence level label usually create heuristically eg rule base methods give summarization datasets document summary pair since label might suboptimal propose latent variable extractive model sentence view latent variables sentence activate variables use infer gold summaries train loss come emphdirectly gold summaries experiment cnn dailymail dataset show model improve strong extractive baseline train heuristically approximate label also perform competitively several recent model
study focus extract knowledgeable snippets annotate knowledgeable document web corpus consist document social media media informally knowledgeable snippets refer text describe concepts properties entities relations among entities knowledgeable document ones enough knowledgeable snippets knowledgeable snippets document could helpful multiple applications knowledge base construction knowledge orient service previous study extract knowledgeable snippets use pattern base method propose semantic base method task specifically cnn base model develop extract knowledgeable snippets annotate knowledgeable document simultaneously additionally low level share high level split structure cnn design handle document different content domains compare build multiple domain specific cnns joint model critically save train time also improve prediction accuracy visibly superiority propose method demonstrate real dataset wechat public platform
paper propose dynamic self attention dsa new self attention mechanism sentence embed design dsa modify dynamic rout capsule network sabouretal2017 natural language process dsa attend informative word dynamic weight vector achieve new state art result among sentence encode methods stanford natural language inference snli dataset least number parameters show comparative result stanford sentiment treebank sst dataset
current state art nmt model rnn seq2seq transformers possess large number parameters still shallow comparison convolutional model use text vision applications work attempt train significantly two 3x deeper transformer bi rnn encoders machine translation propose simple modification attention mechanism ease optimization deeper model result consistent gain seven eleven bleu benchmark wmt fourteen english german wmt fifteen czech english task architectures
twenty question q20 game well know game encourage deductive reason creativity game answerer first think object famous person kind animal questioner try guess object ask twenty question q20 game system user consider answerer system act questioner require good strategy question selection figure correct object win game however optimal policy question selection hard derive due complexity volatility game environment paper propose novel policy base reinforcement learn rl method enable questioner agent learn optimal policy question selection continuous interactions users facilitate train also propose use reward network estimate informative reward compare previous methods rl method robust noisy answer rely knowledge base object experimental result show rl method clearly outperform entropy base engineer system competitive performance noisy free simulation environment
paper address problem map natural language text knowledge base entities map process approach composition phrase sentence point multi dimensional entity space obtain knowledge graph compositional model lstm equip dynamic disambiguation mechanism input word embeddings multi sense lstm address polysemy issue knowledge base space prepare collect random walk graph enhance textual feature act set semantic bridge text knowledge base entities ideas work demonstrate large scale text entity map entity classification task state art result
neural language model critical component state art systems machine translation summarization audio transcription task language model almost universally autoregressive nature generate sentence one token time leave right paper study influence token generation order model quality via novel two pass language model produce partially fill sentence templates fill miss tokens compare various strategies structure two pass observe surprisingly large variation model quality find effective strategy generate function word first pass follow content word second believe experimental result justify extensive investigation generation order neural language model
sequence generation task many work use policy gradient model optimization tackle intractable backpropagation issue maximize non differentiable evaluation metrics fool discriminator adversarial learn paper replace policy gradient proximal policy optimization ppo prove efficient reinforcement learn algorithm propose dynamic approach ppo ppo dynamic demonstrate efficacy ppo ppo dynamic conditional sequence generation task include synthetic experiment chit chat chatbot result show ppo ppo dynamic beat policy gradient stability performance
dropout use avoid overfitting randomly drop units neural network train inspire dropout paper present gi dropout novel dropout method integrate global information improve neural network text classification unlike traditional dropout method units drop randomly accord probability aim use explicit instructions base global information dataset guide train process gi dropout model suppose pay attention inapparent feature pattern experiment demonstrate effectiveness dropout global information seven text classification task include sentiment analysis topic classification
propose generic interpretable learn framework build robust text classification model achieve accuracy comparable full model test time budget constraints approach learn selector identify word relevant prediction task pass classifier process selector train jointly classifier directly learn incorporate classifier propose data aggregation scheme improve robustness classifier learn framework general incorporate type text classification model real world data show propose approach improve performance give classifier speed model mere loss accuracy performance
measure entity relatedness fundamental task many natural language process information retrieval applications prior work often study entity relatedness static settings unsupervised manner however entities real world often involve many different relationships consequently entity relations dynamic time work propose neural networkbased approach dynamic entity relatedness leverage collective attention supervision model capable learn rich different entity representations joint framework extensive experiment large scale datasets demonstrate method achieve better result competitive baselines
propose simple modification exist neural machine translation nmt model enable use single universal model translate multiple languages allow language specific parameterization also use domain adaptation approach require change model architecture standard nmt system instead introduce new component contextual parameter generator cpg generate parameters system eg weight neural network parameter generator accept source target language embeddings input generate parameters encoder decoder respectively rest model remain unchanged share across languages show simple modification enable system use monolingual data train also perform zero shoot translation show able surpass state art performance iwslt fifteen iwslt seventeen datasets learn language embeddings able uncover interest relationships languages
model user vote intention social media important research area applications analyse electorate behaviour online political campaign advertise previous approach mainly focus predict national general elections regularly schedule data past result opinion poll available however evidence model would perform sudden vote time constrain circumstances pose challenge task compare traditional elections due spontaneous nature paper focus two thousand and fifteen greek bailout referendum aim nowcast daily basis vote intention two thousand, one hundred and ninety-seven twitter users propose semi supervise multiple convolution kernel learn approach leverage temporally sensitive text network information evaluation real time simulation framework demonstrate effectiveness robustness approach competitive baselines achieve significant twenty increase f score compare solely text base model
adversarial examples input machine learn model design model make mistake useful understand shortcomings machine learn model interpret result regularisation nlp however example generation strategies produce input text use know pre specify semantic transformations require significant manual effort depth understand problem domain paper investigate problem automatically generate adversarial examples violate set give first order logic constraints natural language inference nli reduce problem identify adversarial examples combinatorial optimisation problem maximise quantity measure degree violation constraints use language model generate linguistically plausible examples furthermore propose method adversarially regularise neural nli model incorporate background knowledge result show propose method always improve result snli multinli datasets significantly consistently increase predictive accuracy adversarially craft datasets seven hundred and ninety-six relative improvement drastically reduce number background knowledge violations furthermore show adversarial examples transfer among model architectures propose adversarial train procedure improve robustness nli model adversarial examples
gans show perform exceedingly well task pertain image generation style transfer field language model word embeddings glove word2vec state art methods apply neural network model textual data attempt make utilize gans word embeddings text generation study present approach text generation use skip think sentence embeddings gans base gradient penalty function f measure propose architecture aim reproduce write style generate text model way expression sentence level across work author extensive experiment run different embed settings variety task include conditional text generation language generation model outperform baseline text generation network across several automate evaluation metrics like bleu n meteor rouge wide applicability effectiveness real life task demonstrate human judgement score
induce sparseness train neural network show yield model lower memory footprint similar effectiveness dense model however sparseness typically induce start dense model thus advantage hold train propose techniques enforce sparseness upfront recurrent sequence model nlp applications also benefit train first language model show increase hide state size recurrent layer without increase number parameters lead expressive model second sequence label show word embeddings predefined sparseness lead similar performance dense embeddings fraction number trainable parameters
propose novel geometric approach learn bilingual mappings give monolingual embeddings bilingual dictionary approach decouple learn transformation source language target language learn rotations language specific embeddings align common space b learn similarity metric common space model similarities embeddings model bilingual map problem optimization problem smooth riemannian manifold show approach outperform previous approach bilingual lexicon induction cross lingual word similarity task also generalize framework represent multiple languages common latent space particular latent space representations several languages learn jointly give bilingual dictionaries multiple language pair illustrate effectiveness joint learn multiple languages zero shoot word translation set implementation available https githubcom anoopkunchukuttan geomm
best systems semeval sixteen semeval seventeen community question answer share task task amount question relevancy rank involve complex pipelines manual feature engineer despite many still fail beat ir baseline ie rank provide google search engine present strong baseline question relevancy rank train simple multi task fee forward network bag fourteen distance measure input question pair baseline model fast train use language independent feature outperform best share task systems task retrieve relevant previously ask question
present neural framework opinion summarization online product review knowledge lean require light supervision eg form product domain label user provide rat method combine two weakly supervise components identify salient opinions form extractive summaries multiple review aspect extractor train multi task objective sentiment predictor base multiple instance learn introduce opinion summarization dataset include train set product review six diverse domains human annotate development test set gold standard aspect annotations salience label opinion summaries automatic evaluation show significant improvements baselines large scale study indicate opinion summaries prefer human judge accord multiple criteria
introduce novel discriminative latent variable model task bilingual lexicon induction model combine bipartite match dictionary prior haghighi et al two thousand and eight state art embed base approach train model derive efficient viterbi algorithm provide empirical improvements six language pair two metrics show prior theoretically empirically help mitigate hubness problem also demonstrate previous work may view similarly fashion latent variable model albeit different prior
paper present discriminative deep dyna q d3q approach improve effectiveness robustness deep dyna q ddq recently propose framework extend dyna q algorithm integrate plan task completion dialogue policy learn obviate ddq high dependency quality simulate experience incorporate rnn base discriminator d3q differentiate simulate experience real user experience order control quality train data experiment show d3q significantly outperform ddq control quality simulate experience use plan effectiveness robustness d3q demonstrate domain extension set agent capability adapt change environment test
character level feature currently use different neural network base natural language process algorithms however little know character level pattern model learn moreover model often compare quantitatively qualitative analysis miss paper investigate character level pattern neural network learn pattern coincide manually define word segmentations annotations end extend contextual decomposition technique murdoch et al two thousand and eighteen convolutional neural network allow us compare convolutional neural network bidirectional long short term memory network evaluate compare model task morphological tag three morphologically different languages show model implicitly discover understandable linguistic rule implementation find https githubcom fredericgodin contextualdecomposition nlp
subset computational problems computable polynomial time exist algorithm may complete due lack high performance technology mission field define subclass deterministic polynomial time complexity class call mission class many polynomial problems computable mission time focus subclass languages context successful military applications also discuss computational communicational constraints investigate feasible nonlinear model minimize energy maximize memory efficiency computational power also provide approximate solution obtain within pre determine length computation time use limit resources optimal solution language could determine
network embeddings learn low dimensional representations vertex large scale network receive considerable attention recent years wide range applications vertices network typically accompany rich textual information user profile paper abstract etc propose incorporate semantic feature network embeddings match important word text sequence pair vertices introduce word word alignment framework measure compatibility embeddings word pair adaptively accumulate alignment feature simple yet effective aggregation function experiment evaluate propose framework three real world benchmarks downstream task include link prediction multi label vertex classification result demonstrate model outperform state art network embed methods large margin
effective approach non parallel voice conversion vc utilize deep neural network dnns specifically variational auto encoders vaes model latent structure speech unsupervised manner previous study confirm ef fectiveness vae use straight spectra vc ever vae use type spectral feature mel cepstral coefficients mccs relate human per ception widely use vc prop erly investigate instead use one specific type spectral feature expect vae may benefit use multi ple type spectral feature simultaneously thereby improve capability vae vc end propose novel vae framework call cross domain vae cdvae vc specifically propose framework utilize straight spectra mccs explicitly regularize multiple objectives order constrain behavior learn encoder de coder experimental result demonstrate propose cd vae framework outperform conventional vae framework term subjective test
question categorization expert retrieval methods crucial information organization accessibility community question answer cqa platforms research area however deal text modality increase multimodal nature web content focus extend methods cqa question accompany image specifically leverage success representation learn text image visual question answer vqa domain adapt underlie concept architecture automate category classification expert retrieval image base question post yahoo chiebukuro japanese counterpart yahoo answer best knowledge first work tackle multimodality challenge cqa adapt vqa model task ecologically valid source visual question analysis differences visual qa community qa data drive proposal novel augmentations attention method tailor cqa use auxiliary task learn better ground feature final model markedly outperform text vqa model baselines task classification expert retrieval real world multimodal cqa data
propose method perform automatic document summarisation without use reference summaries instead method interactively learn users preferences merit preference base interactive summarisation preferences easier users provide reference summaries exist preference base interactive learn methods suffer high sample complexity ie need interact oracle many round order converge work propose new objective function enable us leverage active learn preference learn reinforcement learn techniques order reduce sample complexity simulation real user experiment suggest method significantly advance state art source code freely available https githubcom ukplab emnlp2018 april
present framework build unsupervised representations entities compositions entity view probability distribution rather vector embed particular distribution support contexts co occur entity embed suitable low dimensional space enable us consider representation learn perspective optimal transport take advantage tool wasserstein distance barycenters elaborate method apply obtain unsupervised representations text illustrate performance quantitatively well qualitatively task measure sentence similarity word entailment similarity empirically observe significant gain eg forty-one relative improvement sent2vec gensen key benefit propose approach include capture uncertainty polysemy via model entities distributions b utilize underlie geometry particular task grind cost c simultaneously provide interpretability notion optimal transport contexts easy applicability top exist point embed methods code well prebuilt histograms available https githubcom context mover
understand behavior train network find explanations output important improve network performance generalization ability ensure trust automate systems several approach previously propose identify visualize important feature analyze train network however relations different feature class lose case propose technique induce set else rule capture relations globally explain predictions network first calculate importance feature train network weigh original input feature importance score simplify transform input space finally fit rule induction model explain model predictions find output rule set explain predictions neural network train four class text classification twenty newsgroups dataset macro average f score eighty make code available https githubcom clip interpretwithrules
neural network show impressive performance large datasets apply model task little data available remain challenge problem paper propose use feature transfer zero shoot experimental set task semantic parse first introduce new method learn share space multiple domains base prediction domain label example experiment support superiority method zero shoot experimental set term accuracy metrics compare state art techniques second part paper study impact individual domains examples semantic parse performance use influence function aim investigate sensitivity domain label classification loss example find reveal cross domain adversarial attack identify useful examples train even domains least similar target domain augment train data influential examples boost accuracy token sequence level
enable users heavily specify criteria database query user friendly way paper describe general framework conversational bot extract meaningful information user sentence ask subsequent question complete miss information adjust question information extraction parameters later conversations depend users behavior additionally provide comparison exist tool give novel techniques implement framework finally exemplify framework bot query movies database whose code available microsoft employees
answer compositional question require multi step reason challenge introduce end end differentiable model interpret question knowledge graph kg inspire formal approach semantics span text represent denotation kg vector capture ungrounded aspects mean learn composition modules recursively combine constituent span culminate ground complete sentence answer question example interpret green model represent green set kg entities trainable ungrounded vector use vector parameterize composition function perform complement operation sentence build parse chart subsume possible parse allow model jointly learn composition operators output structure gradient descent end task supervision model learn variety challenge semantic operators quantifiers disjunctions compose relations infer latent syntactic structure also generalize well longer question see train data contrast rnn tree base variants semantic parse baselines
active learn identify data point label expect useful improve supervise model opportunistic active learn incorporate active learn interactive task constrain possible query interactions prior work show opportunistic active learn use improve ground natural language descriptions interactive object retrieval task work use reinforcement learn object retrieval task learn policy effectively trade task completion model improvement would benefit future task
paper present adaptive computation step acs algo rithm enable end end speech recognition model dy namically decide many frame process predict linguistic output model apply acs algorithm follow encoder decoder framework unlike attention base mod els produce alignments independently encoder side use correlation adjacent frame thus predictions make soon sufficient acoustic information receive make model applicable online case besides small change make decode stage encoder decoder framework allow prediction exploit bidirectional contexts verify acs algorithm mandarin speech corpus aishell one achieve three hundred and twelve cer online occasion compare three hundred and twenty-four cer attention base model fully demonstrate advantage acs algorithm offline experiment conduct acs model achieve one hundred and eighty-seven cer outperform attention base counterpart cer two hundred and twenty
although end end text speech tts model tacotron show excellent result typically require sizable set high quality pair train expensive collect paper propose semi supervise train framework improve data efficiency tacotron idea allow tacotron utilize textual acoustic knowledge contain large publicly available text speech corpora importantly external data unpaired potentially noisy specifically first embed word input text word vectors condition tacotron encoder use unpaired speech corpus pre train tacotron decoder acoustic domain finally fine tune model use available pair data demonstrate propose framework enable tacotron generate intelligible speech use less half hour pair train data
level assessment foreign language students necessary put right level group furthermore interview students time consume task propose automate evaluation speaker fluency level implement machine learn techniques work present audio process system capable classify level fluency non native english speakers use five different machine learn model first step build dataset consist label audio conversations english people range different fluency domains class low intermediate high segment audio conversations 5s non overlap audio clip perform feature extraction start extract mel cepstral coefficients audios select twenty coefficients appropriate quantity data thereafter extract zero cross rate root mean square energy spectral flux feature prove improve model performance total one thousand, four hundred and twenty-four audio segment seventy train data thirty test data one train model support vector machine achieve classification accuracy nine thousand, four hundred and thirty-nine whereas four model pass eighty-nine classification accuracy threshold
multi hop reason effective approach query answer qa incomplete knowledge graph kgs problem formulate reinforcement learn rl setup policy base agent sequentially extend inference path reach target however incomplete kg environment agent receive low quality reward corrupt false negative train data harm generalization test time furthermore since golden action sequence use train agent mislead spurious search trajectories incidentally lead correct answer propose two model advance address issue one reduce impact false negative supervision adopt pretrained one hop embed model estimate reward unobserved facts two counter sensitivity spurious paths policy rl force agent explore diverse set paths use randomly generate edge mask approach significantly improve exist path base kgqa model several benchmark datasets comparable better embed base model
neural network base methods abstractive summarization produce output fluent techniques poor content selection work propose simple technique address issue use data efficient content selector determine phrase source document part summary use selector bottom attention step constrain model likely phrase show approach improve ability compress text still generate fluent summaries two step process simpler higher perform end end content selection model lead significant improvements rouge cnn dm nyt corpus furthermore content selector train little one thousand sentence make easy transfer train summarizer new domain
state art english automatic speech recognition systems typically use phonetic rather graphemic lexicons graphemic systems know perform less well english map write form speak form complicate however recent years representational power deep learn base acoustic model improve raise interest graphemic acoustic model english due simplicity generate lexicon paper phonetic graphemic model compare english multi genre broadcast transcription task range acoustic model base lattice free mmi train construct use phonetic graphemic lexicons task find long span temporal history reduce difference performance two form model addition system combination examine use parameter smooth hypothesis combination combination approach become complicate difference phonetic graphemic systems decrease finally configurations examine combination phonetic graphemic systems yield consistent gain
investigate automatic classification patient discharge note standard disease label find convolutional neural network attention outperform previous algorithms use task suggest areas improvement
paper contribute emerge literature model vote text tandem better understand polarization express preferences introduce new approach estimate preference polarization multidimensional settings international relations base developments natural language process network science literatures namely word embeddings retain valuable syntactical qualities human language community detection multilayer network locate densely connect actors across multiple complex network find employment tool tandem help better estimate state foreign policy preferences express un vote speeches beyond permit vote alone utility locate affinity blocs demonstrate application conflict onset international relations though tool interest scholars face measurement preferences polarization multidimensional settings
human face face communication complex multimodal signal use word language modality gesture vision modality change tone acoustic modality convey intentions humans easily process understand face face communication however comprehend form communication remain significant challenge artificial intelligence ai ai must understand modality interactions shape human communication paper present novel neural architecture understand human communication call multi attention recurrent network marn main strength model come discover interactions modalities time use neural component call multi attention block mab store hybrid memory recurrent component call long short term hybrid memory lsthm perform extensive comparisons six publicly available datasets multimodal sentiment analysis speaker trait recognition emotion recognition marn show state art performance datasets
increase popularity video share websites youtube facebook multimodal sentiment analysis receive increase attention scientific community contrary previous work multimodal sentiment analysis focus holistic information speech segment bag word representations average facial expression intensity develop novel deep architecture multimodal sentiment analysis perform modality fusion word level paper propose gate multimodal embed lstm temporal attention gme lstma model compose two modules gate multimodal embed alleviate difficulties fusion noisy modalities lstm temporal attention perform word level fusion finer fusion resolution input modalities attend important time step result gme lstma able better model multimodal structure speech time perform better sentiment comprehension demonstrate effectiveness approach publicly available multimodal corpus sentiment intensity subjectivity analysis cmu mosi dataset achieve state art sentiment classification regression result qualitative analysis model emphasize importance temporal attention layer sentiment prediction additional acoustic visual modalities noisy also demonstrate effectiveness gate multimodal embed selectively filter noisy modalities result analysis open new areas study sentiment analysis human communication provide new model multimodal fusion
personality find predict many life outcomes huge interest automatic personality recognition speaker utterance previously achieve accuracies thirty-seven forty-four three way classification high medium low big five personality traits openness experience conscientiousness extraversion agreeableness neuroticism show improve performance task account heterogeneity gender l1 data english speech female male native speakers chinese standard american english sae experiment personalize model l1 gender normalize feature speaker l1 group gender
build virtual agent learn language 2d maze like world agent see image surround environment listen virtual teacher take action receive reward interactively learn teacher language scratch base two language use case sentence direct navigation question answer learn simultaneously visual representations world language action control disentangle language ground computational routines share concept detection function language ground prediction agent reliably interpolate extrapolate interpret sentence contain new word combinations new word miss train sentence new word transfer answer language prediction language ability train evaluate population sixteen million distinct sentence consist one hundred and nineteen object word eight color word nine spatial relation word fifty grammatical word propose model significantly outperform five comparison methods interpret zero shoot sentence addition demonstrate human interpretable intermediate output model appendix
attention base sequence sequence model prove successful neural machine translation nmt however attention without consideration decode history include past information decoder attention mechanism often cause much repetition address problem propose decode history base adaptive control attention aca nmt model aca learn control attention keep track decode history current information memory vector model take translate content current information consideration experiment chinese english translation english vietnamese translation demonstrate model significantly outperform strong baselines analysis show model capable generate translation less repetition higher accuracy code available https githubcom lancopku
introduce texygen benchmarking platform support research open domain text generation model texygen implement majority text generation model also cover set metrics evaluate diversity quality consistency generate texts texygen platform could help standardize research text generation facilitate share fine tune open source implementations among researchers work consequence would help improve reproductivity reliability future research work text generation
variational encoder decoders veds show promise result dialogue generation however latent variable distributions usually approximate much simpler model powerful rnn structure use encode decode yield kl vanish problem inconsistent train objective paper separate train step two phase first phase learn autoencode discrete texts continuous embeddings second phase learn generalize latent representations reconstruct encode embed case latent variables sample transform gaussian noise multi layer perceptrons train separate ved model potential realize much flexible distribution compare model current popular model experiment demonstrate substantial improvement metric base human evaluations
new text data techniques offer great promise ability inductively discover measure useful test social science theories interest large collections text introduce conceptual framework make causal inferences discover measure treatment outcome framework enable researchers discover high dimensional textual interventions estimate ways observe treatments affect text base outcomes argue nearly text base causal inferences depend upon latent representation text provide framework learn latent representation estimate latent representation show create new risk may introduce identification problem overfit address risk describe split sample framework apply estimate causal effect experiment immigration attitudes study bureaucratic response work provide rigorous foundation text base causal inferences
amortize variational inference avi replace instance specific local inference global inference network avi enable efficient train deep generative model variational autoencoders vae recent empirical work suggest inference network produce suboptimal variational parameters propose hybrid approach use avi initialize variational parameters run stochastic variational inference svi refine crucially local svi procedure differentiable inference network generative model train end end gradient base optimization semi amortize approach enable use rich generative model without experience posterior collapse phenomenon common train vaes problems like text generation experiment show approach outperform strong autoregressive variational baselines standard text image datasets
privacy policies primary channel company inform users data collection share practice policies often long difficult comprehend short notice base information extract privacy policies show useful face significant scalability hurdle give number policies evolution time company users researchers regulators still lack usable scalable tool cope breadth depth privacy policies address hurdle propose automate framework privacy policy analysis polisis enable scalable dynamic multi dimensional query natural language privacy policies core polisis privacy centric language model build 130k privacy policies novel hierarchy neural network classifiers account high level aspects fine grain detail privacy practice demonstrate polisis modularity utility two applications support structure free form query structure query application automate assignment privacy icons privacy policies polisis achieve accuracy eight hundred and eighty-four task second application pribot first freeform question answer system privacy policies show pribot produce correct answer among top three result eighty-two test question use mturk user study seven hundred participants show least one pribot top three answer relevant users eighty-nine test question
automatic speech recognition asr systems often make unrecoverable errors due subsystem prune acoustic language pronunciation model example prune word due acoustics use short term context prior rescoring long term context base linguistics work model asr phrase base noisy transformation channel propose error correction system learn aggregate errors independent modules constitute asr attempt invert propose system exploit long term context use neural network language model better choose exist asr output possibilities well introduce previously prune unseen vocabulary phrase provide corrections poorly perform asr condition without degrade accurate transcriptions corrections greater top domain mismatch data asr system consistently provide improvements baseline asr even baseline optimize recurrent neural network language model rescoring demonstrate asr improvements exploit independently propose system potentially still provide benefit highly optimize asr finally present extensive analysis type errors correct system
performance automatic speech recognition systems degrade increase mismatch train test scenarios differences speaker accent significant source mismatch traditional approach deal multiple accent involve pool data several accent train build single model multi task fashion task correspond individual accent paper explore alternate model jointly learn accent classifier multi task acoustic model experiment american english wall street journal british english cambridge corpora demonstrate joint model outperform strong multi task acoustic model baseline obtain five hundred and ninety-four relative improvement word error rate british english nine hundred and forty-seven relative improvement american english illustrate jointly model accent information improve acoustic model performance
people use rich prior knowledge world order efficiently learn new concepts priors also know inductive bias pertain space internal model consider learner help learner make inferences go beyond observe data recent study find deep neural network optimize object recognition develop shape bias ritter et al two thousand and seventeen inductive bias possess children play important role early word learn however network use unrealistically large quantities train data condition require bias develop well understand moreover unclear learn dynamics network relate developmental process childhood investigate development influence shape bias neural network use control datasets abstract pattern synthetic image allow us systematically vary quantity form experience provide learn algorithms find simple neural network develop shape bias see three examples four object categories development bias predict onset vocabulary acceleration network consistent developmental process children
incipient internet largely text base modern digital world become increasingly multi modal examine multi modal classification one modality discrete eg text continuous eg visual representations transfer convolutional neural network particular focus scenarios able classify large quantities data quickly investigate various methods perform multi modal fusion analyze trade off term classification accuracy computational efficiency find indicate inclusion continuous information improve performance text range multi modal classification task even simple fusion methods addition experiment discretizing continuous feature order speed simplify fusion process even result show fusion discretized feature outperform text classification fraction computational cost full multi modal fusion additional benefit improve interpretability
develop methods automate inference able provide users compel human readable justifications answer question correct critical domains science medicine user trust detect costly errors limit factor adoption one central barriers train question answer model explainable inference task lack gold explanations serve train data paper present corpus explanations standardize science exams recent challenge task question answer manually construct corpus detail explanations nearly publicly available standardize elementary science question approximately one thousand, six hundred and eighty 3rd 5th grade question represent explanation graph set lexically overlap sentence describe arrive correct answer question combination domain world knowledge also provide explanation center tablestore collection semi structure table contain knowledge construct elementary science explanations together two knowledge resources map substantial portion knowledge require answer explain elementary science exams provide structure free text train data explainable inference task
speak dialogue systems aim deploy artificial intelligence build automate dialogue agents converse humans part effort policy optimisation task attempt find policy describe respond humans form function take current state dialogue return response system paper investigate deep reinforcement learn approach solve problem particular attention give actor critic methods policy reinforcement learn experience replay various methods aim reduce bias variance estimators combine methods result previously propose acer algorithm give competitive result game environments environments however fully observable relatively small action set paper examine application acer dialogue policy optimisation show method beat current state art deep learn approach speak dialogue systems lead sample efficient algorithm train faster also allow us apply algorithm difficult environments thus experiment learn large action space two order magnitude action previously consider find acer train significantly faster current state art
goal orient dialog give attention due numerous applications artificial intelligence goal orient dialogue task occur questioner ask action orient question answerer respond intent let questioner know correct action take ask adequate question deep learn reinforcement learn recently apply however approach struggle find competent recurrent neural questioner owe complexity learn series sentence motivate theory mind propose answerer questioner mind aqm novel information theoretic algorithm goal orient dialog aqm questioner ask infer base approximate probabilistic model answerer questioner figure answerer intention via select plausible question explicitly calculate information gain candidate intentions possible answer question test framework two goal orient visual dialog task mnist count dialog guesswhat experiment aqm outperform comparative algorithms large margin
structure prediction require search combinatorial number structure tackle introduce sparsemap new method sparse structure inference natural loss function sparsemap automatically select global structure situate map inference pick single structure marginal inference assign probability mass structure include implausible ones importantly sparsemap compute use call map oracle make applicable problems intractable marginal inference eg linear assignment sparsity make gradient backpropagation efficient regardless structure enable us augment deep neural network generic sparse structure hide layer experiment dependency parse natural language inference reveal competitive accuracy improve interpretability ability capture natural language ambiguities attractive pipeline systems
present neural program search algorithm generate program natural language description small number input output examples algorithm combine methods deep learn program synthesis field design rich domain specific language dsl define efficient search algorithm guide seq2tree model evaluate quality approach also present semi synthetic dataset descriptions test examples correspond program show algorithm significantly outperform sequence sequence model attention baseline
learn walk graph towards target node give query source node important problem applications knowledge base completion kbc formulate reinforcement learn rl problem know state transition model overcome challenge sparse reward develop graph walk agent call walk consist deep recurrent neural network rnn monte carlo tree search mcts rnn encode state ie history walk path map separately policy q value order effectively train agent sparse reward combine mcts neural policy generate trajectories yield positive reward trajectories network improve policy manner use q learn modify rnn policy via parameter share propose rl algorithm repeatedly apply policy improvement step learn model test time mcts combine neural policy predict target node experimental result several graph walk benchmarks show walk able learn better policies rl base methods mainly base policy gradients walk also outperform traditional kbc baselines
work present weakly supervise sentence extraction technique identify important sentence scientific paper worthy inclusion abstract propose new attention base deep learn architecture jointly learn identify important content well cue phrase indicative summary worthy sentence propose new context embed technique determine focus give paper use topic model use jointly lstm base sequence encoder learn attention weight across sentence word use collection article publicly available acl anthology experiment system achieve performance better term several rouge metrics compare several state art extractive techniques also generate coherent summaries preserve overall structure document
grow importance massive datasets use deep learn make robustness label noise critical property classifiers source label noise include automatic label non expert label label corruption data poison adversaries numerous previous work assume source label trust relax assumption assume small subset train data trust enable substantial label corruption robustness performance gain addition particularly severe label noise combat use set trust data clean label utilize trust data propose loss correction technique utilize trust examples data efficient manner mitigate effect label noise deep neural network classifiers across vision natural language process task experiment various label noise several strengths show method significantly outperform exist methods
present neural transducer model visual attention learn generate latex markup real world math formula give image apply sequence model transduction techniques successful across modalities natural language image handwrite speech audio construct image markup model learn produce syntactically semantically correct latex markup code one hundred and fifty word long achieve bleu score eighty-nine improve upon previous state art im2latex problem also demonstrate heat map visualization attention help interpret model pinpoint detect localize symbols image accurately despite train without bound box data
work design neural network recognize emotions speech use iemocap dataset follow latest advance audio analysis use architecture involve convolutional layer extract high level feature raw spectrograms recurrent ones aggregate long term dependencies examine techniques data augmentation vocal track length perturbation layer wise optimizer adjustment batch normalization recurrent layer obtain highly competitive result six hundred and forty-five weight accuracy six hundred and seventeen unweighted accuracy four emotions
many text classification task know highly domain dependent unfortunately availability train data vary drastically across domains worse still domains may annotate data work propose multinomial adversarial network man tackle text classification problem real world multidomain set mdtc provide theoretical justifications man framework prove different instance man essentially minimizers various f divergence metrics ali silvey one thousand, nine hundred and sixty-six among multiple probability distributions man thus theoretically sound generalization traditional adversarial network discriminate two distributions specifically mdtc task man learn feature invariant across multiple domains resort ability reduce divergence among feature distributions domain present experimental result show man significantly outperform prior art mdtc task also show man achieve state art performance domains label data
clinical note text document create clinicians patient encounter typically accompany medical cod describe diagnosis treatment annotate cod labor intensive error prone furthermore connection cod text annotate obscure reason detail behind specific diagnose treatments present attentional convolutional network predict medical cod clinical text method aggregate information across document use convolutional neural network use attention mechanism select relevant segment thousands possible cod method accurate achieve precision8 seventy-one micro f1 fifty-four better prior state art furthermore interpretability evaluation physician show attention mechanism identify meaningful explanations code assignment
paper explore use multi view feature discriminative transform convolutional deep neural network cnn architecture continuous large vocabulary speech recognition task mel filterbank energies perceptually motivate force damp oscillator coefficient doc feature use feature space maximum likelihood linear regression fmllr transform combine feed multi view feature single cnn acoustic model use multi view feature representation demonstrate significant reduction word error rat wers compare use individual feature addition articulatory information use additional input fuse deep neural network dnn cnn acoustic model find demonstrate reduction wer switchboard subset callhome subset contain partly non native accent speech nist two thousand conversational telephone speech test set reduce error rate twelve relative baseline case work show multi view feature association articulatory information improve speech recognition robustness spontaneous non native speech
sequence sequence attentional base neural network architectures show provide powerful model machine translation speech recognition recently several work attempt extend model end end speech translation task however usefulness model investigate language pair similar syntax word order eg english french english spanish work focus end end speech translation task syntactically distant language pair eg english japanese require distant word reorder guide encoder decoder attentional model learn difficult problem propose structure base curriculum learn strategy unlike conventional curriculum learn gradually emphasize difficult data examples formalize learn strategies easier network structure difficult network structure start train end end encoder decoder speech recognition text base machine translation task gradually move end end speech translation task experiment result show propose approach could provide significant improvements comparison one without curriculum learn
voice clone highly desire feature personalize speech interfaces neural network base speech synthesis show generate high quality speech large number speakers paper introduce neural voice clone system take audio sample input study two approach speaker adaptation speaker encode speaker adaptation base fine tune multi speaker generative model clone sample speaker encode base train separate model directly infer new speaker embed clone audios use multi speaker generative model term naturalness speech similarity original speaker approach achieve good performance even clone audios speaker adaptation achieve better naturalness similarity clone time require memory speaker encode approach significantly less make favorable low resource deployment
chaos game representation method create image nucleotide sequence modify make image chunk text document machine learn methods apply train classifiers base authorship experiment conduct several benchmark data set english include widely use federalist paper one portuguese validation result train classifiers competitive best methods prior literature methodology also successfully apply text categorization encourage result one classifier method moreover see hold promise task digital fingerprint
although chatbots popular recent years still serious weaknesses limit scope applications one major weakness learn new knowledge conversation process ie knowledge fix beforehand expand update conversation paper propose build general knowledge learn engine chatbots enable continuously interactively learn new knowledge conversations time go become knowledgeable better better learn conversation model task open world knowledge base completion problem propose novel technique call lifelong interactive learn inference lili solve lili work imitate humans acquire knowledge perform inference interactive conversation experimental result show lili highly promise
sentiment analysis evolve past decades work revolve around textual sentiment analysis text mine techniques audio sentiment analysis still nascent stage research community propose research perform sentiment analysis speaker discriminate speech transcripts detect emotions individual speakers involve conversation analyze different techniques perform speaker discrimination sentiment analysis find efficient algorithms perform task
mild cognitive impairment mci prodromal phase progression normal age dementia especially alzheimers disease even though mild cognitive decline mci patients normal overall cognition thus challenge distinguish normal age use transcribe data obtain record conversational interactions participants train interviewers apply supervise learn model data recent clinical trial show promise result differentiate mci normal age however substantial amount interactions medical staff still incur significant medical care expense practice paper propose novel reinforcement learn rl framework train efficient dialogue agent exist transcripts clinical trials specifically agent train sketch disease specific lexical probability distribution thus converse way maximize diagnosis accuracy minimize number conversation turn evaluate performance propose reinforcement learn framework mci diagnosis real clinical trial result show use turn conversation framework significantly outperform state art supervise learn approach
social media major platform communication information exchange rich repository opinions sentiments twenty-three billion users vast spectrum topics sense certain social user demand cultural drive interest however knowledge embed eighteen billion picture upload daily public profile start exploit since process typically text base follow trend visual base social analysis present novel methodology base deep learn build combine image text base personality trait model train image post together word find highly correlate specific personality traits key contribution explore whether ocean personality trait model address base image call emphmindpics appear certain tag psychological insights find correlation post image accompany texts successfully model use deep neural network personality estimation experimental result consistent previous cyber psychology result base texts image addition classification result traits show pattern emerge set image correspond specific text essence represent abstract concept result open new avenues research refine propose personality model supervision psychology experts
unseen data degrade performance deep neural net acoustic model cope unseen data adaptation techniques deploy unlabeled unseen data one must generate hypothesis give exist model use label model adaptation however assess goodness hypothesis difficult erroneous hypothesis lead poorly train model case strategy select data reliable hypothesis ensure better model adaptation work propose data selection strategy dnn model adaptation dnn output layer activations use ascertain goodness generate hypothesis dnn acoustic model output layer activations use generate target class probabilities unseen data condition difference probable target next probable target decrease compare see data indicate model may uncertain generate hypothesis work propose strategy assess model performance analyze output layer activations use distance measure likely target next likely target use data selection perform unsupervised adaptation
present new algorithm identify transition emission probabilities hide markov model hmm emit data expectation maximization become computationally prohibitive long observation record often require identification new algorithm particularly suitable case available sample size large enough accurately estimate second order output probabilities higher order ones show one able obtain reliable estimate pairwise co occurrence probabilities emissions still possible uniquely identify hmm emission probability emphsufficiently scatter apply method hide topic markov model demonstrate learn topics higher quality document model observations hmms share emission topic probability compare simple widely use bag word model
propose conditional non autoregressive neural sequence model base iterative refinement propose model design base principles latent variable model denoising autoencoders generally applicable sequence generation task extensively evaluate propose model machine translation en de en ro image caption generation observe significantly speed decode maintain generation quality comparable autoregressive counterpart
order improve performance far field speech recognition paper propose distill knowledge close talk model far field model use parallel data close talk model call teacher model far field model call student model student model train imitate output distributions teacher model constraint realize minimize kullback leibler kl divergence output distribution student model teacher model experimental result ami corpus show best student model achieve forty-seven absolute word error rate wer reduction compare conventionally train baseline model
paper propose new architecture attentive tensor product learn atpl represent grammatical structure deep learn model atpl new architecture bridge gap exploit tensor product representations tpr structure neural symbolic model develop cognitive science aim integrate deep learn explicit language structure rule key ideas atpl one unsupervised learn role unbind vectors word via tpr base deep neural network two employ attention modules compute tpr three integration tpr typical deep learn architectures include long short term memory lstm feedforward neural network ffnn novelty approach lie ability extract grammatical structure sentence use role unbind vectors obtain unsupervised manner atpl approach apply one image caption two part speech pos tag three constituency parse sentence experimental result demonstrate effectiveness propose approach
techniques multi lingual cross lingual speech recognition help low resource scenarios bootstrap systems enable analysis new languages domains end end approach particular sequence base techniques attractive simplicity elegance possible integrate traditional multi lingual bottleneck feature extractors front end show end end multi lingual train sequence model effective context independent model train use connectionist temporal classification ctc loss show model improve performance babel languages six absolute term word phoneme error rate compare mono lingual systems build set languages also show train model adapt cross lingually unseen language use twenty-five target data show train multiple languages important low resource cross lingual target scenarios multi lingual test scenarios appear beneficial include large well prepare datasets
learn speaker specific feature vital many applications like speaker recognition diarization speech recognition paper provide novel approach term neural predictive cod npc learn speaker specific characteristics completely unsupervised manner large amount unlabeled train data even contain many non speech events multi speaker audio stream npc framework exploit propose short term active speaker stationarity hypothesis assume two temporally close short speech segment belong speaker thus common representation encode commonalities segment capture vocal characteristics speaker train convolutional deep siamese network produce speaker embeddings learn separate vs different speaker pair generate unlabeled data audio stream two set experiment do different scenarios evaluate strength npc embeddings compare state art domain supervise methods first two speaker identification experiment different context lengths perform scenario comparatively limit within speaker channel variability npc embeddings find perform best short duration experiment provide complementary information vectors full utterance experiment second large scale speaker verification task wide range within speaker channel variability adopt upper bind experiment comparisons draw domain supervise methods
address task generate query suggestions task base search current state art rely heavily suggestions provide major search engine paper solve task without reliance search engines specifically focus first step two stage pipeline approach dedicate generation query suggestion candidates present three methods generate candidate suggestions apply multiple information source use purpose build test collection find methods able generate high quality suggestion candidates
entity orient search deal wide variety information need display direct answer interact service work aim understand prominent entity orient search intents fulfil develop scheme entity intent categories use annotate sample query specifically annotate unique query refiners level entity type observe average half refiners seek interact service quarter refiners search information may look knowledge base
deep model effective explainable desirable many settings prior explainable model unimodal offer either image base visualization attention weight text base generation post hoc justifications propose multimodal approach explanation argue two modalities provide complementary explanatory strengths collect two new datasets define evaluate task propose novel model provide joint textual rationale generation attention visualization datasets define visual textual justifications classification decision activity recognition task act x visual question answer task vqa x quantitatively show train textual explanations yield better textual justification model also better localize evidence support decision also qualitatively show case visual explanation insightful textual explanation vice versa support thesis multimodal explanation model offer significant benefit unimodal approach
study algorithms automatically answer visual question currently motivate visual question answer vqa datasets construct artificial vqa settings propose vizwiz first goal orient vqa dataset arise natural vqa set vizwiz consist thirty-one thousand visual question originate blind people take picture use mobile phone record speak question together ten crowdsourced answer per visual question vizwiz differ many exist vqa datasets one image capture blind photographers often poor quality two question speak conversational three often visual question answer evaluation modern algorithms answer visual question decide visual question answerable reveal vizwiz challenge dataset introduce dataset encourage larger community develop generalize algorithms assist blind people
present content base method recommend citations academic paper draft embed give query document vector space use nearest neighbor candidates rerank candidates use discriminative model train distinguish observe unobserved citations unlike previous work method require metadata author name miss eg peer review process without use metadata method outperform best report result pubmed dblp datasets relative improvements eighteen f120 twenty-two mrr show empirically although add metadata improve performance standard metrics favor self citations less useful citation recommendation setup release online portal http labssemanticscholarorg citeomatic citation recommendation base method new dataset opencorpus seven million research article facilitate future research task
vanish long term gradients major issue train standard recurrent neural network rnns alleviate long short term memory lstm model memory cells however extra parameters associate memory cells mean lstm layer four time many parameters rnn hide vector size paper address vanish gradient problem use high order rnn hornn additional connections multiple previous time step speech recognition experiment use british english multi genre broadcast mgb3 data show propose hornn architectures rectify linear unit sigmoid activation function reduce word error rat wer forty-two sixty-three correspond rnns give similar wers project lstm use twenty fifty recurrent layer parameters computation
propose several ways reuse subword embeddings weight subword aware neural language model propose techniques benefit competitive character aware model improve performance syllable morpheme aware model show significant reductions model size discover simple hand principle multi layer input embed model layer tie consecutively bottom reuse output best morpheme aware model properly reuse weight beat competitive word level model large margin across multiple languages twenty eighty-seven fewer parameters
understand visualize human discourse long challenge task although recent work argument mine show success classify role various sentence task recognize concepts understand ways discuss remain challenge give email thread transcript group discussion task extract relevant concepts understand reference reference throughout discussion present work present preliminary approach extract visualize group discourse adapt wikipedia category hierarchy external concept ontology user study find method achieve better result four strong alternative approach illustrate visualization method base extract discourse flow
political speeches debate play important role shape image politicians public often rely media outlets select bits political communication large pool utterances important research question understand factor impact selection process quantitatively explore selection process build three decade dataset presidential debate transcripts post debate coverage first examine effect word propose binary classification framework control speaker debate situation find crowdworkers achieve accuracy sixty task indicate media choices entirely obvious classifiers outperform crowdworkers average mainly primary debate also compare important factor crowdworkers free form explanations data drive methods find interest differences crowdworkers mention context matter whereas data show well quote sentence distinct previous utterance speaker less quote sentence finally examine aggregate effect media preferences towards different word understand extent fragmentation among media outlets analyze bipartite graph build quote behavior data observe decrease trend bipartisan coverage
due recent technical scientific advance wealth information hide unstructured text data offline online narratives research article clinical report mine data properly attributable innate ambiguity word sense disambiguation wsd algorithm avoid number difficulties natural language process nlp pipeline however consider large number ambiguous word one language technical domain may encounter limit constraints proper deployment exist wsd model paper attempt address problem one classifier per one word wsd algorithms propose single bidirectional long short term memory blstm network consider sense context sequence work ambiguous word collectively evaluate senseval three benchmark show result model comparable top perform wsd algorithms also discuss apply additional modifications alleviate model fault need train data
embeddings machine learn low dimensional representations complex input pattern property simple geometric operations like euclidean distance dot products use classification comparison task propose meta embeddings special embeddings live general inner product space design propagate uncertainty final output speaker recognition similar applications familiar gaussian plda model gplda formulate extractor gaussian meta embeddings gmes likelihood ratio score give hilbert space inner products gaussian likelihood function gmes extract gplda model fix precisions propagate uncertainty show generalization heavy tail plda give gmes variable precisions propagate uncertainty experiment nist sre two thousand and ten two thousand and sixteen show propose method apply vectors without length normalization twenty accurate gplda apply length normalize ivectors
combine multi task learn semi supervise learn induce joint embed space disparate label space learn transfer function label embeddings enable us jointly leverage unlabelled data auxiliary annotate datasets evaluate approach variety sequence classification task disparate label space outperform strong single multi task baselines achieve new state art topic base sentiment analysis
paper explore vector semantics problem perspective almost orthogonal property high dimensional random vectors show intrigue property use memorize random vectors simply add provide efficient probabilistic solution set membership problem also discuss several applications word context vector embeddings document sentence similarity spam filter
alignment heterogeneous sequential data video text important challenge problem standard techniques task include dynamic time warp dtw conditional random field crfs suffer inherent drawbacks mainly markov assumption imply give immediate past future alignment decisions independent history separation similarity computation alignment decision also prevent end end train paper propose end end neural architecture alignment action implement move data stack long short term memory lstm block flexible architecture support large variety alignment task include one one one many skip unmatched elements extensions non monotonic alignment extensive experiment semi synthetic real datasets show algorithm outperform state art baselines
automatic deception detection important task gain momentum computational linguistics due potential applications paper propose simple yet tough beat multi modal neural model deception detection combine feature different modalities video audio text along micro expression feature show detect deception real life videos accurate experimental result dataset real life deception videos show model outperform exist techniques deception detection accuracy nine thousand, six hundred and fourteen roc auc nine thousand, seven hundred and ninety-nine
children comprise significant proportion tv viewers worthwhile customize experience however identify child audience challenge task identify gender age audio command well study problem still challenge get good accuracy utterances typically couple second long present initial study novel method combine utterances user metadata particular develop ensemble different machine learn techniques different subsets data improve child detection initial result show ninety-two absolute improvement baseline lead state art performance
thank grow availability spoof databases rapid advance use systems detect voice spoof attack become capable error rat close zero reach asvspoof2015 database however speech synthesis voice conversion paradigms consider asvspoof2015 database appear examples include direct waveform model generative adversarial network also need investigate feasibility train spoof systems use low quality find data purpose develop generative adversarial network base speech enhancement system improve quality speech data find publicly available source use enhance data train state art text speech voice conversion model evaluate term perceptual speech quality speaker similarity result show enhancement model significantly improve snr low quality degrade data find publicly available source significantly improve perceptual cleanliness source speech without significantly degrade naturalness voice however result also show limitations generate speech low quality find data
various informative factor mix speech signal lead great difficulty decode factor intuitive idea factorize speech frame individual informative factor though turn highly difficult recently find speaker traits assume long term distributional properties actually short time pattern learn carefully design deep neural network dnn discovery motivate cascade deep factorization cdf framework present paper propose framework infer speech factor sequential way factor previously infer use conditional variables infer factor show approach effectively factorize speech signal use factor original speech spectrum recover high accuracy factorization reconstruction approach provide potential value many speech process task eg speaker recognition emotion recognition demonstrate paper
deep learn practitioners sequence model synonymous recurrent network yet recent result indicate convolutional architectures outperform recurrent network task audio synthesis machine translation give new sequence model task dataset architecture one use conduct systematic evaluation generic convolutional recurrent architectures sequence model model evaluate across broad range standard task commonly use benchmark recurrent network result indicate simple convolutional architecture outperform canonical recurrent network lstms across diverse range task datasets demonstrate longer effective memory conclude common association sequence model recurrent network reconsider convolutional network regard natural start point sequence model task assist relate work make code available http githubcom locuslab tcn
work first analyze memory behavior three recurrent neural network rnn cells namely simple rnn srn long short term memory lstm gate recurrent unit gru memory define function map previous elements sequence current output study show three suffer rapid memory decay alleviate effect introduce trainable scale factor act like attention mechanism adjust memory decay adaptively new design call extend lstm elstm finally design system robust previous erroneous predictions propose dependent bidirectional recurrent neural network dbrnn extensive experiment conduct different language task demonstrate superiority propose elstm dbrnn solutions eltsm achieve thirty increase label attachment score las compare lstm gru dependency parse dp task model also outperform state art model bi attention convolutional sequence sequence convseq2seq close ten las code release open source https githubcom yuanhangsu elstm dbrnn
evaluation summarization task extremely crucial determine quality machine generate summaries last decade rouge become standard automatic evaluation measure evaluate summarization task rouge show effective capture n gram overlap system human compose summaries several limitations exist rouge measure term capture synonymous concepts coverage topics thus often time rouge score reflect true quality summaries prevent multi faceted evaluation summaries ie topics overall content coverage etc paper introduce rouge twenty several update measure rouge rouge nsynonyms rouge topic rouge topicsynonyms rouge topicuniq rouge topicuniqsynonyms improvements core rouge measure
autonomous systems remote locations high degree autonomy need explain order increase transparency maintain trust describe natural language chat interface enable vehicle behaviour query user obtain interpretable model autonomy expert speak loud provide explanations mission approach agnostic type autonomy model expert operator user group predict explanations align well operator mental model increase transparency assist operator train
present flipper natural language interface describe high level task specifications robots compile robot action flipper start formal core language task plan allow express rich temporal specifications use semantic parser provide natural language interface flipper provide immediate visual feedback execute automatically construct plan task graphical user interface allow user resolve potentially ambiguous interpretations flipper extend via naturalization users add definitions utterances flipper induce new rule add core language gradually grow natural task specification language flipper improve naturalization generalize definition provide users unlike task specification systems flipper enable natural language interactions maintain expressive power formal precision program language show initial user study natural language interactions generalization considerably ease description task moreover time users employ concepts outside initial core language extensions available flipper community users use concepts others define
examine algebraic geometric properties uni directional gru word embeddings train end end text classification task hyperparameter search word embed dimension gru hide dimension linear combination gru output perform conclude word naturally embed lie group rnns form nonlinear representation group appeal result propose novel class recurrent like neural network word embed scheme
work propose simple efficient framework learn sentence representations unlabelled data draw inspiration distributional hypothesis recent work learn sentence representations reformulate problem predict context sentence appear classification problem give sentence context classifier distinguish context sentence contrastive sentence base vector representations allow us efficiently learn different type encode function show model learn high quality sentence representations demonstrate sentence representations outperform state art unsupervised supervise representation learn methods several downstream nlp task involve understand sentence semantics achieve order magnitude speedup train time
behavior users certain service could clue use infer preferences may use make recommendations service never use however cross domain relationships items user consumption pattern simple especially common users items across domains address problem propose content base cross domain recommendation method cold start users require user item overlap formulate recommendation extreme multi class classification label items correspond users predict formulation problem reduce domain adaptation set classifier train source domain adapt target domain construct neural network combine architecture domain adaptation domain separation network denoising autoencoder item representation assess performance approach experiment pair data set collect movie news service yahoo japan show approach outperform several baseline methods include cross domain collaborative filter method
reinforcement learn rl promise approach solve dialogue policy optimisation traditional rl algorithms however fail scale large domains due curse dimensionality propose novel dialogue management architecture base feudal rl decompose decision two step first step master policy select subset primitive action second step primitive action choose select subset structural information include domain ontology use abstract dialogue state space take decisions step use different part abstract state combine information share mechanism slot increase scalability large domains show implementation approach base deep q network significantly outperform previous state art several dialogue domains environments without need additional reward signal
expert find important task industry academia challenge rank candidates appropriate expertise various query addition different type object interact one another naturally form heterogeneous information network study task expert find heterogeneous bibliographical network base two aspects textual content analysis authority rank regard textual content analysis propose new method query expansion via locally train embed learn concept hierarchy guidance particularly tailor specific query narrow semantic mean compare global embed learn locally train embed learn project term latent semantic space constrain relevant topics therefore preserve precise subtle information specific query consider candidate rank heterogeneous information network structure largely ignore previous study expert find provide additional information specifically different type interactions among object play different roles propose rank algorithm estimate authority object network treat strongly type edge type individually demonstrate effectiveness propose framework apply propose method large scale bibliographical dataset two million entries one million researcher candidates experiment result show propose framework outperform exist methods general specific query
structure prediction energy network spens belanger mccallum two thousand and sixteen use neural network architectures define energy function capture arbitrary dependencies among part structure output prior work use gradient descent inference relax structure output set continuous variables optimize energy respect replace use gradient descent neural network train approximate structure argmax inference inference network output continuous value treat output structure develop large margin train criteria joint train structure energy function inference network multi label classification report speed up ten 60x compare belanger et al two thousand and seventeen also improve accuracy sequence label simple structure energies approach perform comparably exact inference much faster test time demonstrate improve accuracy augment energy label language model score entire output label sequence show improve handle long distance dependencies part speech tag finally show inference network replace dynamic program test time inference conditional random field suggestive general use fast inference structure settings
propose novel online learn algorithm call spcoslam twenty spatial concepts lexical acquisition high accuracy scalability previously propose spcoslam online learn algorithm base unsupervised bayesian probabilistic model integrate multimodal place categorization lexical acquisition slam however original algorithm limit estimation accuracy owe influence early stag learn increase computational complexity add train data therefore introduce techniques fix lag rejuvenation reduce calculation time maintain accuracy higher original algorithm result show term estimation accuracy propose algorithm exceed original algorithm comparable batch learn addition calculation time propose algorithm depend amount train data become constant step scalable algorithm approach contribute realization long term spatial language interactions humans robots
demonstrate large texts represent human english russian ukrainian artificial c java languages display quantitative pattern characterize benford like zipf laws frequency word follow zipf law inversely proportional rank whereas total number certain word appear text generate uneven benford like distribution lead number exclude popular word essentially improve correlation actual textual data zipfian distribution whereas benford distribution lead number arise overall amount certain word insensitive elimination procedure calculate value moduli slop double logarithmical plot artificial languages c java markedly larger human ones
users organize communities web platforms communities interact one another often lead conflict toxic interactions however little know mechanisms interactions communities impact users study intercommunity interactions across thirty-six thousand communities reddit examine case users one community mobilize negative sentiment comment another community show conflict tend initiate handful communities less one communities start seventy-four conflict conflict tend initiate highly active community members carry significantly less active members find conflict mark formation echo chamber users primarily talk users community long term conflict adverse effect reduce overall activity users target communities analysis user interactions also suggest strategies mitigate negative impact conflict increase direct engagement attackers defenders accurately predict whether conflict occur create novel lstm model combine graph embeddings user community text feature model use toreate early warn systems community moderators prevent conflict altogether work present data drive view community interactions conflict pave way towards healthier online communities
past years witness renew interest nlp task interface vision language one intensively study problem automatically generate text image paper extend problem specific domain face description unlike scene descriptions face descriptions fine grain rely attribute extract image rather object relations give data exist task present ongoing crowdsourcing study collect corpus descriptions face image take wild gain better understand variation find face description possible issue may raise also conduct annotation study subset corpus primarily find descriptions refer mixture attribute physical also emotional inferential bind create challenge current image text methods
develop system automatically detect online jihadist hate speech eighty accuracy use techniques natural language process machine learn system train corpus forty-five thousand subversive twitter message collect october two thousand and fourteen december two thousand and sixteen present qualitative quantitative analysis jihadist rhetoric corpus examine network twitter users outline technical procedure use train system discuss examples use
translate program write one program language another useful software development task need functionality implementations different languages although past study consider problem may either specific language grammars specific certain kinds code elements eg tokens phrase api use paper propose new approach automatically learn cross language representations various kinds structural code elements may use program translation key idea two fold first normalize enrich code token stream additional structural semantic information train cross language vector representations tokens aka share embeddings base word2vec neural network base technique produce word embeddings second hierarchically bottom construct share embeddings code elements higher level granularity eg expressions statements methods embeddings constituents build mappings among code elements across languages base similarities among embeddings preliminary evaluations forty thousand java c source file nine software project show approach automatically learn share embeddings various code elements different languages identify cross language mappings reasonable mean average precision score compare exist tool map library api methods approach identify many mappings accurately map result code access https githubcom bdqnghi hierarchical program language map believe idea learn cross language vector representations code structural information useful step towards automate program translation
present novel architecture database entity link idel integrate analytics optimize rdbms monetdb neural text mine abilities system design abstract core task neural entity link systems monetdb best knowledge first defacto implement system integrate entity link database leverage ability monetdb support database analytics user define function udfs implement python function call machine learn libraries neural text mine tensorflow system achieve zero cost data ship transformation utilize monetdb ability embed python process database kernel exchange data numpy array idel represent text relational data joint vector space neural embeddings compensate errors ambiguous entity representations detect match entities propose novel similarity function base joint neural embeddings learn via minimize pairwise contrastive rank loss function utilize high dimensional index structure fast retrieval match entities first implementation experiment use webnlg corpus show effectiveness potentials idel
background music social interaction settings hinder conversation yet little know specific properties music impact speech process paper address knowledge gap investigate one whether mask effect background music lyric larger music without lyric two whether mask effect larger complex music answer question word identification experiment run dutch participants listen dutch cvc word embed stretch background music two condition without lyric three snrs three songs use different genres complexities music stretch without lyric sample song order control factor beyond presence lyric result show clear negative impact presence lyric background music speak word recognition impact independent complexity result suggest social space eg restaurants caf es bar make careful choices music promote conversation open path future work
social media become increasingly important source information public mood regard issue elections brexit stock market etc paper focus sentiment classification twitter data construction sentiment classifiers standard text mine task address question properly evaluate settle way sentiment class order unbalance twitter produce stream time order data problem address concern procedures use obtain reliable estimate performance measure whether temporal order train test data matter collect large set fifteen million tweet thirteen european languages create one hundred and thirty-eight sentiment model sample datasets use gold standard evaluations correspond one hundred and thirty-eight sample datasets use empirically compare six different estimation procedures three variants cross validation three variants sequential validation test set always follow train set find significant difference best cross validation sequential validation however observe cross validation variants tend overestimate performance sequential methods tend underestimate standard cross validation random selection examples significantly worse block cross validation use evaluate classifiers time order data scenarios
present new question set text corpus baselines assemble encourage ai research advance question answer together constitute ai2 reason challenge arc require far powerful knowledge reason previous challenge squad snli arc question set partition challenge set easy set challenge set contain question answer incorrectly retrieval base algorithm word co occurence algorithm dataset contain natural grade school science question author human test largest public domain set kind seven thousand, seven hundred and eighty-seven question test several baselines challenge set include lead neural model squad snli task find none able significantly outperform random baseline reflect difficult nature task also release arc corpus corpus 14m science sentence relevant task implementations three neural baseline model test model perform better pose arc challenge community
transformation machine learn ml boutique science generally accept technology increase importance reproduction transportability ml study current work investigate corpus characteristics textual data set correspond text classification result work two data set gather sub forums online health relate forum empirical result obtain multi class sentiment analysis application
answer complex question time consume activity humans require reason integration information recent work read comprehension make headway answer simple question tackle complex question still ongoing research challenge conversely semantic parsers successful handle compositionality information reside target knowledge base paper present novel framework answer broad complex question assume answer simple question possible use search engine read comprehension model propose decompose complex question sequence simple question compute final answer sequence answer illustrate viability approach create new dataset complex question complexwebquestions present model decompose question interact web compute answer empirically demonstrate question decomposition improve performance two hundred and eight precision1 two hundred and seventy-five precision1 new dataset
paper present systematic survey recent development neural text generation model specifically start recurrent neural network language model traditional maximum likelihood estimation train scheme point shortcoming text generation thus introduce recently propose methods text generation base reinforcement learn parametrization trick generative adversarial net gin techniques compare different properties model correspond techniques handle common problems gradient vanish generation diversity finally conduct benchmarking experiment different type neural text generation model two well know datasets discuss empirical result along aforementioned model properties
tensor2tensor library deep learn model well suit neural machine translation include reference implementation state art transformer model
compile baselines along dataset split multimodal sentiment analysis paper explore three different deep learn base architectures multimodal sentiment classification improve upon previous evaluate architectures multiple datasets fix train test partition also discuss major issue frequently ignore multimodal sentiment analysis research eg role speaker exclusive model importance different modalities generalizability framework illustrate different facets analysis consider perform multimodal sentiment analysis hence serve new benchmark future research emerge field
visual question answer vqa increasingly popular topic deep learn research require coordination natural language process computer vision modules single architecture build upon model place first vqa challenge develop thirteen new attention mechanisms introduce simplify classifier perform three hundred gpu hours extensive hyperparameter architecture search able achieve evaluation score six thousand, four hundred and seventy-eight outperform exist state art single model validation score six thousand, three hundred and fifteen
exist research study vision language ground robot navigation focus improve model free deep reinforcement learn drl model synthetic environments however model free drl model consider dynamics real world environments often fail generalize new scenes paper take radical approach bridge gap synthetic study real world practice propose novel plan ahead hybrid reinforcement learn model combine model free model base reinforcement learn solve real world vision language navigation task look ahead module tightly integrate look ahead policy model environment model predict next state reward experimental result suggest propose method significantly outperform baselines achieve best real world room room dataset moreover scalable method generalizable transfer unseen environments
recently increase attention direct study speech emotion recognition global acoustic feature utterance mostly use eliminate content differences however expression speech emotion dynamic process reflect dynamic durations energies prosodic information one speak paper novel local dynamic pitch probability distribution feature obtain draw histogram propose improve accuracy speech emotion recognition compare previous work use global feature propose method take advantage local dynamic information convey emotional speech several experiment berlin database emotional speech conduct verify effectiveness propose method experimental result demonstrate local dynamic information obtain propose method effective speech emotion recognition traditional global feature
many lead approach language model introduce novel complex specialize architectures take exist state art word level language model base lstms qrnns extend larger vocabularies well character level granularity properly tune lstms qrnns achieve state art result character level penn treebank enwik8 word level wikitext one hundred and three datasets respectively result obtain twelve hours wikitext one hundred and three two days enwik8 use single modern gpu
interventional cancer clinical trials generally restrictive patients often exclude basis comorbidity past concomitant treatments fact certain age efficacy safety new treatments patients characteristics therefore define work build model automatically predict whether short clinical statements consider inclusion exclusion criteria use protocols cancer clinical trials available public registries last eighteen years train word embeddings construct adataset 6m short free texts label eligible eligible text classifier train use deep neural network pre train word embeddings input predict whether short free text statements describe clinical information consider eligible additionally analyze semantic reason word embed representations obtain able identify equivalent treatments type tumor analogous drug use treat tumors show representation learn use deep neural network successfully leverage extract medical knowledge clinical trial protocols potentially assist practitioners prescribe treatments
rohingya movement crisis cause huge uproar political economic state bangladesh refugee movement recur event large amount data form opinions remain social media facebook little analysis do themto analyse comment base rohingya relate post create modify classifier base support vector machine algorithm code implement python use scikit learn library dataset rohingya analysis currently available use data set two thousand, five hundred positive two thousand, five hundred negative comment specifically use support vector machine linear kernel previous experiment perform us dataset use naive bay algorithm yield impressive result
statistical language model powerful tool use many task within natural language process recently use sequential data source coderay et al two thousand and fifteen show possible train n gram source code language mode use predict buggy line code determine unnatural line via entropy respect language model work propose use advance language model technique long short term memory recurrent neural network model source code classify buggy line base entropy show method slightly outperform n gram model buggy line classification task use auc
domain unsupervised learn work speech focus discover low level construct phoneme inventory word like units contrast write language large body work unsupervised induction semantic representations word whole sentence longer texts study examine challenge adapt approach write speak language conjecture unsupervised learn semantics speak language become feasible abstract surface variability simulate set dataset utterances speak realistic uniform synthetic voice evaluate two simple unsupervised model vary degrees success learn semantic representations speech fragment finally present inconclusive result human speech discuss challenge inherent learn distributional semantic representations unrestricted natural speak language
automate techniques model check use verify model robotic mission plan base markov decision process mdps generate counterexamples may help diagnose requirement violations however artifacts may complex humans understand exist representations counterexamples typically include large number paths complex automaton help improve interpretability counterexamples define notion explainable counterexample include set structure natural language sentence describe robotic behavior lead requirement violation mdp model robotic mission plan propose approach base mix integer linear program generate explainable counterexamples minimal sound complete demonstrate usefulness propose approach via case study warehouse robots plan
work propose global style tokens gsts bank embeddings jointly train within tacotron state art end end speech synthesis system embeddings train explicit label yet learn model large range acoustic expressiveness gsts lead rich set significant result soft interpretable label generate use control synthesis novel ways vary speed speak style independently text content also use style transfer replicate speak style single audio clip across entire long form text corpus train noisy unlabeled find data gsts learn factorize noise speaker identity provide path towards highly scalable robust speech synthesis
present extension tacotron speech synthesis architecture learn latent embed space prosody derive reference acoustic representation contain desire prosody show condition tacotron learn embed space result synthesize audio match prosody reference signal fine time detail even reference synthesis speakers different additionally show reference prosody embed use synthesize text different reference utterance define several quantitative subjective metrics evaluate prosody transfer report result accompany audio sample single speaker forty-four speaker tacotron model prosody transfer task
propose mru multi range reason units new fast compositional encoder machine comprehension mc propose mru encoders characterize multi range gate execute series parameterized contract expand layer learn gate vectors benefit long short term dependencies aim approach follow one learn representations concurrently aware long short term context two model relationships intra document block three fast efficient sequence encode show propose encoder demonstrate promise result standalone encoder well complementary build block conduct extensive experiment three challenge mc datasets namely race searchqa narrativeqa achieve highly competitive performance race benchmark model outperform dfn dynamic fusion network fifteen six without use recurrent convolution layer similarly achieve competitive performance relative amanda searchqa benchmark bidaf narrativeqa benchmark without use lstm gru layer finally incorporate mru encoders standard bilstm architectures improve performance achieve state art result
present unsupervised approach discover semantic representations mathematical equations equations challenge analyze unique nearly unique method call equation embeddings find good representations equations use representations surround word use equation embeddings analyze four collections scientific article arxiv cover four computer science domains nlp ir ai ml sim985k equations quantitatively find equation embeddings provide better model compare exist word embed approach qualitatively find equation embeddings provide coherent semantic representations equations capture semantic similarity equations word
recommender systems rss provide effective way alleviate information overload problem select personalize items different users latent factor base collaborative filter cf become popular approach rss due accuracy scalability recently online social network user generate content provide diverse source recommendation beyond rat although social matrix factorization social mf topic matrix factorization topic mf successfully exploit social relations item review respectively ignore useful information paper investigate effective data fusion combine aforementioned approach first propose novel model mboxmr3 jointly model three source information ie rat item review social relations effectively rat prediction align latent factor hide topics second incorporate implicit feedback rat propose model enhance capability demonstrate flexibility achieve accurate rat prediction real life datasets various state art methods furthermore measure contribution three data source impact implicit feedback rat follow sensitivity analysis hyperparameters empirical study demonstrate effectiveness efficacy propose model extension
develop state art approach specific task major drive force research community depend prestige task publish come along lot visibility question arise reliable evaluation methodologies compare approach one common methodology identify state art partition data train development test set researchers train tune approach part dataset select model work best development set final evaluation unseen test data test score different approach compare performance differences test statistical significance publication show high risk statistical significance type evaluation due superior learn approach instead high risk difference due chance example conll two thousand and three ner dataset observe twenty-six case type errors false positives threshold p five ie falsely conclude statistically significant difference two identical approach prove evaluation setup unsuitable compare learn approach formalize alternative evaluation setups base score distributions
task speech enhancement local learn objectives agnostic phonetic structure helpful speech recognition propose add global criterion ensure de noise speech useful downstream task like asr first train spectral classifier clean speech predict senone label spectral classifier join speech enhancer noisy speech recognizer model teach imitate output spectral classifier alone clean speech textitmimic loss combine traditional local criterion train speech enhancer produce de noise speech feed de noise speech shelf kaldi train recipe chime two corpus show significant improvements wer
investigate use generative adversarial network gans speech dereverberation robust speech recognition gans recently study speech enhancement remove additive noise still lack work examine ability speech dereverberation advantage use gans fully establish paper provide deep investigations use gin base dereverberation front end asr first study effectiveness different dereverberation network generator gin find lstm lead significant improvement compare fee forward dnn cnn dataset second add residual connections deep lstms boost performance well finally find success gin important update generator discriminator use mini batch data train moreover use reverberant spectrogram condition discriminator suggest previous study may degrade performance summary gin base dereverberation front end achieve fourteen nineteen relative cer reduction compare baseline dnn dereverberation network test strong multi condition train acoustic model
speaker adaptation aim estimate speaker specific acoustic model speaker independent one minimize mismatch train test condition arise speaker variabilities variety neural network adaptation methods propose since deep learn model become main stream still lack experimental comparison different methods especially dnn base acoustic model advance greatly paper aim close gap provide empirical evaluation three typical speaker adaptation methods lin lhuc kld adaptation experiment different size adaptation data conduct strong tdnn lstm acoustic model challengingly source target concern standard mandarin speaker model accent mandarin speaker model compare performances different methods combinations speaker adaptation performance also examine speaker accent degree
present new end end architecture automatic speech recognition asr train use emphsymbolic input addition traditional acoustic input architecture utilize two separate encoders one acoustic input another symbolic input share attention decoder parameters call architecture multi modal data augmentation network mmda support multi modal acoustic symbolic input enable seamless mix large text datasets significantly smaller transcribe speech corpora train study different ways transform large text corpora symbolic form suitable train mmda network best mmda setup obtain small improvements character error rate cer much seven ten relative word error rate wer improvement baseline without external language model
previous work develop close loop speech chain model base deep learn architecture enable automatic speech recognition asr text speech synthesis tts components mutually improve performance accomplish two part teach use label unlabeled data approach could significantly improve model performance within single speaker speech dataset slight increase could gain multi speaker task furthermore model still unable handle unseen speakers paper present new speech chain mechanism integrate speaker recognition model inside loop also propose extend capability tts handle unseen speakers implement one shoot speaker adaptation enable tts mimic voice characteristics one speaker another one shoot speaker sample even text without speaker information speech chain loop mechanism asr also benefit ability learn arbitrary speaker characteristics generate speech waveform result significant improvement recognition rate
paper propose attention base end end neural approach small footprint keyword spot kws aim simplify pipelines build production quality kws system model consist encoder attention mechanism encoder transform input signal high level representation use rnns attention mechanism weight encoder feature generate fix length vector finally linear transformation softmax function vector become score use keyword detection also evaluate performance different encoder architectures include lstm gru crnn experiment real world wake data show approach outperform recent deep kws approach large margin best performance achieve crnn specific 84k parameters attention base model achieve one hundred and two false rejection rate frr ten false alarm fa per hour
paper study image caption conditional gin train propose context aware lstm captioner co attentive discriminator enforce semantic alignment image caption empirically focus viability two train methods self critical sequence train scst gumbel straight st demonstrate scst show stable gradient behavior improve result gumbel st even without access discriminator gradients directly also address problem automatic evaluation caption model introduce new semantic score show correlation human judgement evaluation paradigm argue important criterion captioner ability generalize compositions object usually co occur together end introduce small caption context ooc test set ooc set combine semantic score propose new diagnosis tool caption community evaluate ooc ms coco benchmarks show scst base train strong performance semantic score human evaluation promise valuable new approach efficient discrete gin train
dialogue management dm decide next action dialogue system accord current dialogue state thus play central role task orient dialogue systems since dialogue management require access local utterances also global semantics entire dialogue session model long range history information critical issue end propose novel memory augment dialogue management model mad employ memory controller two additional memory structure ie slot value memory external memory slot value memory track dialogue state memorize update value semantic slot instance cuisine price location external memory augment representation hide state traditional recurrent neural network store context information update dialogue state efficiently also propose slot level attention user utterances extract specific semantic information slot experiment show model obtain state art performance outperform exist baselines
use explicit object detectors intermediate step image caption use constitute essential stage early work often bypass currently dominant end end approach language model condition directly mid level image embed argue explicit detections provide rich semantic information thus use interpretable representation better understand end end image caption systems work well provide depth analysis end end image caption explore variety cue derive object detections study reveal end end image caption systems rely match image representations generate caption encode frequency size position object complementary play role form good image representation also reveal different object categories contribute different ways towards image caption
image description task invariably examine static manner qualitative presumptions hold universally applicable regardless scope target description practice however different viewers may pay attention different aspects image yield different descriptions interpretations various contexts diversity perspectives difficult derive conventional image description techniques paper propose customize image narrative generation task users interactively engage generation process provide answer question attempt learn user interest via repeat interactive stag automatically reflect interest descriptions new image experimental result demonstrate model generate variety descriptions single image cover wider range topics conventional model customizable target user interaction
propose end end model base convolutional recurrent neural network speech enhancement model purely data drive make assumptions type stationarity noise contrast exist methods use multilayer perceptrons mlps employ convolutional recurrent neural network architectures thus approach allow us exploit local structure frequency temporal domains incorporate prior knowledge speech signal design model structure build model data efficient achieve better generalization see unseen noise base experiment synthetic data demonstrate model outperform exist methods improve pesq six see noise sixty-four unseen noise
paper architecture base long short term memory network propose text independent scenario aim capture temporal speaker relate information operate traditional speech feature speaker verification first background model must create speaker representation enrollment stage speaker model create base enrollment utterances work model train end end fashion combine first two stag main goal end end train model optimize consistent speaker verification protocol end end train jointly learn background speaker model create representation space lstm architecture train create discrimination space validate match non match pair speaker verification propose architecture demonstrate superiority text independent compare traditional methods
integration information across multiple modalities across time promise way enhance emotion recognition performance affective systems much previous work focus instantaneous emotion recognition two thousand and eighteen one minute gradual emotion recognition omg emotion challenge hold conjunction ieee world congress computational intelligence encourage participants address long term emotion recognition integrate cue multiple modalities include facial expression audio language intuitively multi modal inference network able leverage information modality correlations improve recognition achievable single modality network describe multi modal neural architecture integrate visual information time use lstm combine utterance level audio text cue recognize human sentiment multimodal clip model outperform unimodal baseline achieve concordance correlation coefficients ccc four hundred arousal task three hundred and fifty-three valence task
recent advance machine learn community allow different use case emerge association domains like cook create computational cuisine paper tackle picture recipe alignment problem target application large scale retrieval task find recipe give picture vice versa approach validate recipe1m dataset compose one million image recipe pair additional class information achieve state art result
continuous dimensional emotion model arousal valence depict complex change emotions paper present work arousal valence predictions one minute gradual omg emotion challenge multimodal representations first extract videos use variety acoustic video textual model support vector machine svm use fusion multimodal signal make final predictions solution achieve concordant correlation coefficient ccc score three hundred and ninety-seven five hundred and twenty arousal valence respectively validation dataset outperform baseline systems best ccc score fifteen twenty-three arousal valence large margin
knowledge base kb use real world applications book movie restaurant reservation keep change time end end neural network train task orient dialogs expect immune change kb however exist approach breakdown ask handle change propose encoder decoder architecture bossnet novel bag sequence boss memory facilitate disentangle learn response language model knowledge incorporation consequently kb modify new knowledge without drop interpretability find bossnet outperform state art model considerable improvements ten babi oov test set human human datasets also systematically modify exist datasets measure disentanglement show bossnet robust kb modifications
counterfactual learn human bandit feedback describe scenario user feedback quality output historic system log use improve target system show apply learn framework neural semantic parse machine learn perspective key challenge lie proper reweighting estimator avoid know degeneracies counterfactual learn still applicable stochastic gradient optimization conduct experiment human users devise easy use interface collect human feedback semantic parse work first show semantic parsers improve significantly counterfactual learn log human feedback data
report describe approach achieve fifty-three unweighted accuracy seven emotions five nine mean square errors arousal valence omg emotion recognition challenge result obtain ensemble single modality model train voice face data video separately consider stream sequence frame next estimate feature frame handle recurrent neural network audio frame mean short four second spectrogram interval feature estimation face picture use resnet neural network pretrained affectnet database short spectrogram consider picture process convolutional network base audio model use resnet pretrained speaker recognition task predictions modalities fuse decision level improve single channel approach percent
paper address problem automatic emotion recognition scope one minute gradual emotional behavior challenge omg emotion challenge underlie objective challenge automatic estimation emotion expressions two dimensional emotion representation space ie arousal valence adopt methodology weight ensemble several model video text modalities video base recognition two different type visual cue ie face facial landmarks consider fee multi input deep neural network regard text modality sequential model base simple recurrent architecture implement addition also introduce model base high level feature order embed domain knowledge learn process experimental result omg emotion validation set demonstrate effectiveness implement ensemble model clearly outperform current baseline methods
attention gradient base visualization techniques use infer token level label binary sequence tag problems use network train sentence level label construct neural network architecture base soft attention train binary sentence classifier evaluate token level annotation four different datasets infer token label network provide method quantitatively evaluate model learn along generate useful feedback assistance systems result indicate attention base methods able predict token level label accurately compare gradient base methods sometimes even rival supervise oracle network
bi directional lstms powerful tool text representation hand show suffer various limitations due sequential nature investigate alternative lstm structure encode text consist parallel state word recurrent step use perform local global information exchange word simultaneously rather incremental read sequence word result various classification sequence label benchmarks show propose model strong representation power give highly competitive performances compare stack bilstm model similar parameter number
sarcasm sophisticate speech act commonly manifest social communities twitter reddit prevalence sarcasm social web highly disruptive opinion mine systems due tendency polarity flip also usage figurative language sarcasm commonly manifest contrastive theme either positive negative sentiments literal figurative scenarios paper revisit notion model contrast order reason sarcasm specifically propose attention base neural model look instead across enable explicitly model contrast incongruity conduct extensive experiment six benchmark datasets twitter reddit internet argument corpus propose model achieve state art performance datasets also enjoy improve interpretability
softmax function ubiquitous machine learn multiple previous work suggest faster alternatives paper propose way compute classical softmax fewer memory access hypothesize reduction memory access improve softmax performance actual hardware benchmarks confirm hypothesis softmax accelerate 13x softmaxtopk combine fuse 5x
follow great success image process field idea adversarial train apply task natural language process nlp field one promise approach directly apply adversarial train develop image process field input word embed space instead discrete input space texts however approach abandon interpretability generate adversarial texts significantly improve performance nlp task paper restore interpretability methods restrict directions perturbations toward exist word input embed space result straightforwardly reconstruct input perturbations actual text consider perturbations replacement word sentence maintain even improve task performance
fundamental frequency f0 contour speech key aspect represent speech prosody find use speech speak language analysis voice conversion speech synthesis well speaker language identification work propose new methods estimate f0 contour speech use deep neural network dnns recurrent neural network rnns train use supervise learn grind truth f0 contour latest prior research address problem first frame frame classification problem follow sequence track use deep neural network hide markov model dnn hmm hybrid architecture study however tackle problem regression problem instead order obtain f0 contour higher frequency resolution clean noisy speech experiment use ptdb tug corpus contaminate additive noise noisex ninety-two show propose method improve gross pitch error gpe twenty-five signal noise ratios snrs ten db ten db compare one noise robust f0 trackers pefac furthermore performance fine pitch error fpe improve approximately twenty state art dnn hmm base approach
stylistic dialogue response generation valuable applications personality base conversational agents challenge task response need fluent contextually relevant well paralinguistically accurate moreover parallel datasets regular stylistic pair usually unavailable present three weakly supervise model generate diverse polite rude dialogue responses without parallel data late fusion model fusion merge decoder encoder attention decoder dialogue model language model train stand alone polite utterances label fine tune lft model prepends source sequence politeness score scale label predict state art politeness classifier train test time able generate polite neutral rude responses simply scale label embed correspond score reinforcement learn model polite rl encourage politeness generation assign reward proportional politeness classifier score sample response also present two retrieval base polite dialogue model baselines human evaluation validate fusion retrieval base model achieve politeness poorer context relevance lft polite rl model produce significantly polite responses without sacrifice dialogue quality
sequence sequence attention base model subword units allow simple open vocabulary end end speech recognition work show model achieve competitive result switchboard 300h librispeech 1000h task particular report state art word error rat wer three hundred and fifty-four dev clean three hundred and eighty-two test clean evaluation subsets librispeech introduce new pretraining scheme start high time reduction factor lower train crucial convergence final performance experiment also use auxiliary ctc loss function help convergence addition train long short term memory lstm language model subword units shallow fusion report twenty-seven relative improvements wer attention baseline without language model
children speech recognition challenge mainly due inherent high variability children physical articulatory characteristics expressions variability manifest acoustic construct linguistic usage due rapidly change developmental stage children life part challenge due lack large amount available children speech data efficient model work attempt address key challenge use transfer learn adult model children model deep neural network dnn framework children automatic speech recognition asr task evaluate multiple children speech corpora large vocabulary paper present systematic extensive analysis propose transfer learn technique consider key factor affect children speech recognition prior literature evaluations present comparisons earlier gmm hmm newer dnn model ii effectiveness standard adaptation techniques versus transfer learn iii various adaptation configurations tackle variabilities present children speech term acoustic spectral variability b pronunciation variability linguistic constraints analysis span number dnn model parameters adaptation ii amount adaptation data iii age children iv age dependent independent adaptation finally provide recommendations favorable strategies various aforementioned analyze parameters ii potential future research directions relevant challenge problems persist dnn base asr children speech
online review play important role influence buyers daily purchase decisions however fake meaningless review reflect users genuine purchase experience opinions widely exist web pose great challenge users make right choices thereforeit desirable build fair model evaluate quality products distinguish spamming review present end end trainable unify model leverage appeal properties autoencoder random forest stochastic decision tree model implement guide global parameter learn process extensive experiment conduct large amazon review dataset propose model consistently outperform series compare methods
experimental evidence indicate simple model outperform complex deep network many unsupervised similarity task provide simple yet rigorous explanation behaviour introduce concept optimal representation space semantically close symbols map representations close similarity measure induce model objective function addition present straightforward procedure without retrain architectural modifications allow deep recurrent model perform equally well sometimes better compare shallow model validate analysis conduct set consistent empirical evaluations introduce several new sentence embed model process even though work present within context natural language process insights readily applicable domains rely distribute representations transfer task
paper propose deep learn approach tackle automatic summarization task incorporate topic information convolutional sequence sequence convs2s model use self critical sequence train scst optimization jointly attend topics word level alignment approach improve coherence diversity informativeness generate summaries via bias probability generation mechanism hand reinforcement train like scst directly optimize propose model respect non differentiable metric rouge also avoid exposure bias inference carry experimental evaluation state art methods gigaword duc two thousand and four lcsts datasets empirical result demonstrate superiority propose method abstractive summarization
unsupervised machine translation ie assume cross lingual supervision signal whether dictionary translations comparable corpora seem impossible nevertheless lample et al two thousand and eighteen recently propose fully unsupervised machine translation mt model model rely heavily adversarial unsupervised alignment word embed space bilingual dictionary induction conneau et al two thousand and eighteen examine result identify limitations current unsupervised mt unsupervised bilingual dictionary induction perform much worse morphologically rich languages dependent mark monolingual corpora different domains different embed algorithms use show simple trick exploit weak supervision signal identical word enable robust induction establish near perfect correlation unsupervised bilingual dictionary induction performance previously unexplored graph similarity metric
learn contrast positive negative sample general strategy adopt many methods noise contrastive estimation nce word embeddings translate embeddings knowledge graph examples nlp employ approach work view contrastive learn abstraction methods augment negative sampler mixture distribution contain adversarially learn sampler result adaptive sampler find harder negative examples force main model learn better representation data evaluate proposal learn word embeddings order embeddings knowledge graph embeddings observe faster convergence improve result multiple metrics
understand customer sentiments paramount importance market strategies today give company insight customers perceive products service also give idea improve offer paper attempt understand correlation different variables customer review women clothe e commerce classify review whether recommend review product whether consist positive negative neutral sentiment achieve goals employ univariate multivariate analyse dataset feature except review title review texts implement bidirectional recurrent neural network rnn long short term memory unit lstm recommendation sentiment classification result show recommendation strong indicator positive sentiment score vice versa hand rat product review fuzzy indicators sentiment score also find bidirectional lstm able reach f1 score eighty-eight recommendation classification ninety-three sentiment classification
lstms introduce combat vanish gradients simple rnns augment gate additive recurrent connections present alternative view explain success lstms gate versatile recurrent model provide representational power previously appreciate decouple lstm gate embed simple rnn produce new class rnns recurrence compute element wise weight sum context independent function input ablations range problems demonstrate gate mechanism alone perform well lstm settings strongly suggest gate much practice alleviate vanish gradients
neural abstractive summarization conventional sequence sequence seq2seq model often suffer repetition semantic irrelevance tackle problem propose global encode framework control information flow encoder decoder base global information source context consist convolutional gate unit perform global encode improve representations source side information evaluations lcsts english gigaword demonstrate model outperform baseline model analysis show model capable reduce repetition
exist attention mechanisms either attend local image grid object level feature visual question answer vqa motivate observation question relate object instance part propose novel attention mechanism jointly consider reciprocal relationships two level visual detail bottom attention thus generate coalesce top information focus scene elements relevant give question design hierarchically fuse multi modal information ie language object gird level feature efficient tensor decomposition scheme propose model improve state art single model performances six hundred and seventy-nine six hundred and eighty-two vqav1 six hundred and fifty-seven six hundred and seventy-four vqav2 demonstrate significant boost
human label data time consume expensive yet many case critical success learn process order minimize human label efforts propose novel active learn solution rely exist source unlabeled data use small amount label data core set synthesis useful membership query mqs unlabeled instance generate algorithm human label solution use modification operators function modify instance extent apply operators small set instance core set create set new membership query use framework look instance space search space apply search algorithms order generate new examples highly relevant learner implement framework textual domain test several text classification task show improve classifier performance mqs label incorporate train set best knowledge first work membership query textual domain
research hate speech become relevant every day still focus hate speech detection attempt replicate hate speech detection experiment perform exist twitter corpus annotate hate speech highlight issue arise research field hate speech essentially still infancy take critical look train corpus order understand bias also use venture beyond hate speech detection investigate whether use would light facets research popularity hate tweet
consider problem learn textual entailment model limit supervision 5k 10k train examples present two complementary approach first propose knowledge guide adversarial example generators incorporate large lexical resources entailment model via handful rule templates second make entailment model discriminator robust propose first gin style approach train use natural language example generator iteratively adjust base discriminator performance demonstrate effectiveness use two entailment datasets propose methods increase accuracy forty-seven scitail twenty-eight one train sub sample snli notably even single hand write rule negate improve accuracy negation examples snli sixty-one
recurrent neural network rnns famously know turing complete rely infinite precision state unbounded computation time consider case rnns finite precision whose computation time linear input length limitations show different rnn variants different computational power particular show lstm elman rnn relu activation strictly stronger rnn squash activation gru achieve lstms relu rnns easily implement count behavior show empirically lstm indeed learn effectively use count mechanism
compare fast train decode speed returnn attention model translation due fast cuda lstm kernels fast pure tensorflow beam search decoder show layer wise pretraining scheme recurrent attention model give one bleu improvement absolute allow train deeper recurrent encoder network promise preliminary result max expect bleu train present able train state art model translation end end model speech recognition show result wmt two thousand and seventeen switchboard flexibility returnn allow fast research feedback loop experiment alternative architectures generality allow use wide range applications
semantic hash become powerful paradigm fast similarity search many information retrieval systems fairly successful previous techniques generally require two stage train binary constraints handle ad hoc paper present end end neural architecture semantic hash nash binary hash cod treat bernoulli latent variables neural variational inference framework propose train gradients directly back propagate discrete latent variable optimize hash function also draw connections propose method rate distortion theory provide theoretical foundation effectiveness propose framework experimental result three public datasets demonstrate method significantly outperform several state art model unsupervised supervise scenarios
past decade track health trend use social media data show great promise due powerful combination massive adoption social media around world increasingly potent hardware software enable us work new big data stream time many challenge problems identify first often mismatch rapidly online data change rapidly algorithms update mean limit reusability algorithms train past data performance decrease time second much work focus specific issue specific past period time even though public health institutions would need flexible tool assess multiple evolve situations real time third tool provide capabilities proprietary systems little algorithmic data transparency thus little buy global public health research community introduce crowdbreaks open platform allow track health trend make use continuous crowdsourced label public social media content system build way automatize typical workflow data collection filter label train machine learn classifiers therefore greatly accelerate research process public health domain work introduce technical aspects platform explore future use case
furui first demonstrate identity consonant vowel perceive c v transition later stevens propose acoustic landmarks primary cue speech perception steady state regions secondary supplemental acoustic landmarks perceptually salient even language one speak demonstrate non speakers language identify feature primary articulator landmark factor suggest strategy develop language independent automatic speech recognition landmarks potentially learn suitably label corpus rapidly apply many languages paper propose enhance cross lingual portability neural network use landmarks secondary task multi task learn mtl network train well resourced source language phone landmark label english adapt resourced target language word label iban landmark task mtl reduce source language phone error rate twenty-nine relative reduce target language word error rate nineteen fifty-nine depend amount target language train data result suggest landmark task mtl cause dnn learn hide node feature useful cross lingual adaptation
recent research ai focus towards generate narrative stories visual scenes potential achieve human like understand basic description generation image sequence work propose solution generate stories image sequence base sequence sequence model novelty encoder model compose two separate encoders one model behaviour image sequence model sentence story generate previous image sequence image use image sequence encoder capture temporal dependencies image sequence sentence story use previous sentence story encoder achieve better story flow solution generate long human like stories describe visual context image sequence also contain narrative evaluative language obtain result confirm manual human evaluation
process collectively invent new word new concepts population conflict quickly become numerous form synonymy homonymy remember could cost much memory remember may slow overall process efficient behavior could help balance two name game multi agent computational model emergence language focus negotiation new lexical conventions common lexicon self organize go phase high complexity previous work do control complexity growth particular model allow agents actively choose talk however strategies rely ad hoc heuristics highly dependent fine tune parameters define new principled measure new strategy base beliefs agent global state population measure rely heavy computation cognitively plausible new strategy yield efficient control complexity growth along faster agreement process also show short term memory enough build relevant beliefs global lexicon
recently grow interest multi speaker speech recognition utterances multiple speakers recognize mixture promise techniques propose task earlier work require additional train data isolate source signal senone alignments effective learn paper propose new sequence sequence framework directly decode multiple label sequence single speech sequence unify source separation speech recognition function end end manner propose new objective function improve contrast hide vectors avoid generate similar hypotheses experimental result show model directly able learn map speech mixture multiple label sequence achieve eight hundred and thirty-one relative improvement compare model train without propose objective interestingly result comparable produce previous end end work feature explicit separation recognition modules
recurrent convolutional neural network comprise two distinct families model prove useful encode natural language utterances paper present sopa new model aim bridge two approach sopa combine neural representation learn weight finite state automata wfsas learn soft version traditional surface pattern show sopa extension one layer cnn cnns equivalent restrict version sopa accordingly restrict form wfsa empirically three text classification task sopa comparable better bilstm rnn baseline cnn baseline particularly useful small data settings
understand follow directions provide humans enable robots navigate effectively unknown situations present follownet end end differentiable neural architecture learn multi modal navigation policies follownet map natural language instructions well visual depth input locomotion primitives follownet process instructions use attention mechanism condition visual depth input focus relevant part command perform navigation task deep reinforcement learn rl sparse reward learn simultaneously state representation attention function control policies evaluate agent dataset complex natural language directions guide agent rich realistic dataset simulate home show follownet agent learn execute previously unseen instructions describe similar vocabulary successfully navigate along paths encounter train agent show thirty improvement baseline model without attention mechanism fifty-two success rate novel instructions
choice model units critical automatic speech recognition asr task conventional asr systems typically choose context dependent state cd state context dependent phonemes cd phonemes model units however challenge sequence sequence attention base model integrate acoustic pronunciation language model single neural network english asr task previous attempt already show model unit graphemes outperform phonemes sequence sequence attention base model paper concern model units mandarin chinese asr task use sequence sequence attention base model transformer five model units explore include context independent phonemes ci phonemes syllables word sub word character experiment hkust datasets demonstrate lexicon free model units outperform lexicon relate model units term character error rate cer among five model units character base model perform best establish new state art cer two thousand, six hundred and sixty-four hkust datasets without hand design lexicon extra language model integration correspond forty-eight relative improvement exist best cer two hundred and eighty joint ctc attention base encoder decoder network
recent approach dialogue act recognition show context precede utterances important classify subsequent one show performance improve rapidly context take account propose utterance level attention base bidirectional recurrent neural network utt att birnn model analyze importance precede utterances classify current one setup birnn give input set current precede utterances model outperform previous model use precede utterances context use corpus another contribution article discover amount information utterance classify subsequent one show context base learn improve performance also achieve higher confidence classification use character word level feature represent utterances result present character word feature representations ensemble model representations find classify short utterances closest precede utterances contribute higher degree
dialogue act recognition important part natural language understand investigate way dialogue act corpora annotate learn approach use far find dialogue act context sensitive within conversation class nevertheless previous model dialogue act classification work utterance level consider context propose novel context base learn method classify dialogue act use character level language model utterance representation notice significant improvement evaluate method switchboard dialogue act corpus result show consideration precede utterances context current utterance improve dialogue act detection
time critical situations natural disasters rapid classification data post social network affect people useful humanitarian organizations gain situational awareness plan response efforts however scarcity label data early hours crisis hinder machine learn task thus delay crisis response work propose use inductive semi supervise technique utilize unlabeled data often abundant onset crisis event along fewer label data specif ically adopt graph base deep learn framework learn inductive semi supervise model use two real world crisis datasets twitter evaluate propose approach result show significant improvements use unlabeled data compare use label data
recent work manage learn cross lingual word embeddings without parallel data map monolingual embeddings share space adversarial train however evaluation focus favorable condition use comparable corpora closely relate languages show often fail realistic scenarios work propose alternative approach base fully unsupervised initialization explicitly exploit structural similarity embeddings robust self learn algorithm iteratively improve solution method succeed test scenarios obtain best publish result standard datasets even surpass previous supervise systems implementation release open source project https githubcom artetxem vecmap
report experiment train deep neural network automatically translate informalized latex write mizar texts formal mizar language best knowledge first time neural network adopt formalization mathematics use luong et al neural machine translation model nmt test align informal formal corpora various hyperparameters evaluate result experiment show best perform model configurations able generate correct mizar statements six thousand, five hundred and seventy-three inference data union model cover seven thousand, nine hundred and seventeen result indicate formalization artificial neural network promise approach automate formalization mathematics present several case study illustrate result
traditional elm improve versions suffer problems outliers noise due overfitting imbalance due distribution propose novel hybrid adaptive fuzzy elmha felm introduce fuzzy membership function traditional elm method deal problems define fuzzy membership function base distance sample center class also density among sample base quantum harmonic oscillator model propose fuzzy membership function overcome shortcoming traditional fuzzy membership function could make adjust accord specific distribution different sample adaptively experiment show propose ha felm produce better performance svm elm relm text classification
paper propose novel approach base cost sensitive ensemble weight extreme learn machine call approach ae1 welm apply approach text classification ae1 welm algorithm include balance imbalanced multiclassification text classification weight elm assign different weight different sample improve classification accuracy certain extent weight elm consider differences sample different categories ignore differences sample within categories measure importance document sample information entropy generate cost sensitive matrix factor base document importance embed cost sensitive weight elm adaboostm1 framework seamlessly vector space modelvsm text representation produce high dimension sparse feature increase burden elm overcome problem develop text classification framework combine word vector ae1 welm experimental result show method provide accurate reliable effective solution text classification
address task detect foil image caption ie identify whether caption contain word deliberately replace semantically similar word thus render inaccurate respect image describe solve problem principle require fine grain understand image detect linguistically valid perturbations caption contexts encode sufficiently descriptive image information become key challenge paper demonstrate possible solve task use simple interpretable yet powerful representations base explicit object information model achieve state art performance standard dataset score exceed achieve humans task also measure upper bind performance model use gold standard annotations analysis reveal simpler model perform well even without image information suggest dataset contain strong linguistic bias
emotion recognition become popular topic interest especially field human computer interaction previous work involve unimodal analysis emotion recent efforts focus multi modal emotion recognition vision speech paper propose new method learn hide representations speech text data use convolutional attention network compare shallow model employ simple concatenation feature vectors propose attention model perform much better classify emotion speech text data contain cmu mosei dataset
goal explore abilities bring dialogue manager include end end visually ground conversational agents make initial step towards general goal augment task orient visual dialogue model decision make component decide whether ask follow question identify target referent image stop conversation make guess analyse show add decision make component produce dialogues less repetitive include fewer unnecessary question thus potentially lead efficient less unnatural interactions
user simulators one major tool enable offline train task orient dialogue systems task agenda base user simulator abus often use abus base hand craft rule output semantic form issue arise properties limit diversity inability interface text level belief tracker paper introduce neural user simulator nus whose behaviour learn corpus generate natural language hence need less label dataset simulators generate semantic output comparison much past work topic evaluate user simulators corpus base metrics use nus train policy reinforcement learn base speak dialogue system nus compare abus evaluate policies train use simulators cross model evaluation perform ie train one simulator test furthermore train policies test real users evaluation task nus outperform abus
use future contextual information typically show helpful acoustic model however recurrent neural network rnn easy model future temporal context effectively meanwhile keep lower model latency paper attempt design rnn acoustic model capable utilize future context effectively directly model latency computation cost low possible propose model base minimal gate recurrent unit mgru input projection layer insert two context modules temporal encode temporal convolution specifically design architecture model future context experimental result switchboard task internal mandarin asr task show propose model perform much better long short term memory lstm mgru model whereas enable online decode maximum latency one hundred and seventy ms model even outperform strong baseline tdnn lstm smaller model latency almost half less parameters
recurrent neural network become ubiquitous compute representations sequential data especially textual data natural language process particular bidirectional lstms heart several neural model achieve state art performance wide variety task nlp however bilstms know suffer sequential bias contextual representation token heavily influence tokens close sentence propose general effective improvement bilstm model encode suffix prefix sequence tokens forward reverse directions call model suffix bidirectional lstm subilstm introduce alternate bias favor long range dependencies apply subilstms several task require sentence model demonstrate use subilstm instead bilstm exist model lead improvements performance learn general sentence representations text classification textual entailment paraphrase detection use subilstm achieve new state art result fine grain sentiment classification question classification
multi view learn provide self supervision different view available data distributional hypothesis provide another form useful self supervision adjacent sentence plentiful large unlabelled corpora motivate asymmetry two hemispheres human brain well observation different learn architectures tend emphasise different aspects sentence mean create unify multi view sentence representation learn framework one view encode input sentence recurrent neural network rnn view encode simple linear model train objective maximise agreement specify adjacent context information two view show train vectors produce multi view train provide improve representations single view train combination different view give representational improvement demonstrate solid transferability standard downstream task
recent research show word embed space learn text corpora different languages align without parallel data supervision inspire success unsupervised cross lingual word embeddings paper target learn cross modal alignment embed space speech text learn corpora respective modalities unsupervised fashion propose framework learn individual speech text embed space attempt align two space via adversarial train follow refinement procedure show framework could use perform speak word classification translation result two task demonstrate performance unsupervised alignment approach comparable supervise counterpart framework especially useful develop automatic speech recognition asr speech text translation systems low zero resource languages little parallel audio text data train modern supervise asr speech text translation model account majority languages speak across world
motivate problem automate repair software vulnerabilities propose adversarial learn approach map one discrete source domain another target domain without require pair label examples source target domains bijections demonstrate propose adversarial learn approach effective technique repair software vulnerabilities perform close seq2seq approach require label pair propose generative adversarial network approach application agnostic apply problems similar code repair grammar correction sentiment translation
fee forward network widely use cross modal applications bridge modalities map distribute vectors one modality share space predict vectors use perform eg retrieval label thus success whole system rely ability map make neighborhood structure ie pairwise similarities predict vectors akin target vectors however whether achieve investigate yet propose new similarity measure two ad hoc experiment would light issue three cross modal benchmarks learn large number language vision vision language neural network mappings five layer use rich diversity image text feature loss function result reveal surprisingly neighborhood structure predict vectors consistently resemble input vectors target vectors second experiment show untrained net significantly disrupt neighborhood ie semantic structure input vectors
attention network multimodal learn provide efficient way utilize give visual information selectively however computational cost learn attention distributions every pair multimodal input channel prohibitively expensive solve problem co attention build two separate attention distributions modality neglect interaction multimodal input paper propose bilinear attention network ban find bilinear attention distributions utilize give vision language information seamlessly ban consider bilinear interactions among two group input channel low rank bilinear pool extract joint representations pair channel furthermore propose variant multimodal residual network exploit eight attention map ban efficiently quantitatively qualitatively evaluate model visual question answer vqa twenty flickr30k entities datasets show ban significantly outperform previous methods achieve new state arts datasets
numeracy ability understand work number necessary skill compose understand document clinical scientific technical domains paper explore different strategies model numerals language model memorisation digit digit composition propose novel neural architecture use continuous probability density function model numerals open vocabulary evaluation clinical scientific datasets show use hierarchical model distinguish numerals word improve perplexity metric subset numerals two four order magnitude respectively non hierarchical model combination strategies improve perplexity continuous probability density function model reduce mean absolute percentage errors eighteen fifty-four comparison second best strategy dataset respectively
recently rise interest train agents embody virtual environments perform language direct task deep reinforcement learn paper propose simple effective neural language ground module embody agents train end end scratch take raw pixels unstructured linguistic command sparse reward input model language ground process language guide transformation visual feature latent sentence embeddings use transformation matrices several language direct navigation task feature challenge partial observability require simple reason module significantly outperform state art also release xworld3d easy customize 3d environment potentially modify evaluate variety embody agents
last decade applications signal process drastically improve deep learn however areas affect compute emotional speech synthesis emotion recognition speak language remain challenge paper investigate use neural automatic speech recognition asr feature extractor emotion recognition show feature outperform egemaps feature set predict valence arousal emotional dimension mean audio text map learn asr system contain information relate emotional dimension spontaneous speech also examine relationship first layer closer speech last layer closer text asr valence arousal
show dropout train best understand perform map estimation concurrently family conditional model whose objectives lower bound original dropout objective discovery allow us pick model family train lead substantial improvement regularisation heavy language model family include model compute power mean sample dropout mask less stochastic subvariants tighter higher lower bound fully stochastic dropout objective argue since deterministic subvariant bind equal objective highest amongst model predominant view good approximation mc average mislead rather deterministic dropout best available approximation true objective
present supervise directional similarity network sdsn novel neural architecture learn task specific transformation function top general purpose word embeddings rely limit amount supervision task specific score subset vocabulary architecture able generalise transform general purpose distributional vector space model relation lexical entailment experiment show excellent performance score grade lexical entailment raise state art hyperlex dataset approximately twenty-five
neural network model successfully apply domains require substantial generalisation skills recent study imply struggle solve task train require infer underlie compositional structure paper introduce attentive guidance mechanism direct sequence sequence model equip attention find compositional solutions test two task devise precisely assess compositional capabilities neural model show vanilla sequence sequence model attention overfit train distribution guide versions come compositional solutions fit train test distributions almost equally well moreover learn solutions generalise even case train test distributions strongly diverge way demonstrate sequence sequence model capable find compositional solutions without require extra components result help disentangle cause lack systematic compositionality neural network turn fuel future work
every year thousands people receive consumer product relate injuries research indicate online customer review process autonomously identify product safety issue early identification safety issue lead earlier recall thus fewer injuries deaths dataset product review amazoncom compile along emphsaferproductsgov complaints recall descriptions consumer product safety commission cpsc european commission rapid alert system system build clean collect text extract relevant feature dimensionality reduction perform compute feature relevance random forest discard feature low information gain various classifiers analyze include logistic regression svms naive bay random forest ensemble classifier experimentation various feature classifier combinations result logistic regression model sixty-six precision top fifty review surface classifier outperform benchmarks set relate literature consumer product safety professionals
guide troubleshoot inherent task domain technical support service customer experience issue function technical service product expert user help guide customer set step comprise troubleshoot procedure objective identify source problem set diagnostic step observations arrive resolution procedures contain set diagnostic step observations response different problems common artifacts body technical support documentation ability use machine learn linguistics understand leverage procedures applications like intelligent chatbots robotic process automation crucial exist research question answer intelligent chatbots look within procedures deep understand paper outline system mine procedures technical support document create model solve important subproblems like extraction procedures identify decision point within procedures identify block instructions correspond decision point map instructions within decision block also release dataset contain manual annotations publicly available support document promote research problem
many deep learn architectures propose model compositionality text sequence require substantial number parameters expensive computations however rigorous evaluation regard add value sophisticate compositional function paper conduct point point comparative study simple word embed base model swems consist parameter free pool operations relative word embed base rnn cnn model surprisingly swems exhibit comparable even superior performance majority case consider base upon understand propose two additional pool strategies learn word embeddings max pool operation improve interpretability ii hierarchical pool operation preserve spatial n gram information within text sequence present experiment seventeen datasets encompass three task long document classification ii text sequence match iii short text task include classification tag source code datasets obtain https githubcom dinghanshen swem
textual network embed leverage rich text information associate network learn low dimensional vectorial representations vertices rather use typical natural language process nlp approach recent research exploit relationship texts edge graphically embed text however model neglect measure complete level connectivity two texts graph present diffusion map textual network embed dmte integrate global structural information graph capture semantic relatedness texts diffusion convolution operation apply text input addition new objective function design efficiently preserve high order proximity use graph diffusion experimental result show propose approach outperform state art methods vertex classification link prediction task
introduce groundnet neural network refer expression recognition task localize ground image object refer natural language expression approach task first rely syntactic analysis input refer expression order inform structure computation graph give parse tree input expression explicitly map syntactic constituents relationships present tree compose graph neural modules define architecture perform localization syntax base approach aid localization textitboth target object auxiliary support object mention expression result groundnet interpretable previous methods one determine phrase refer expression point object image two track localization target object determine network study property empirically introduce new set annotations googleref dataset evaluate localization support object experiment show groundnet achieve state art accuracy identify support object maintain comparable performance localization target object
convolutional neural network modern model efficient many classification task originally create image process purpose trials perform use different domains like natural language process artificial intelligence systems like humanoid robots often base embed systems constraints memory power consumption etc therefore convolutional neural network memory capacity reduce map give hardware paper result present compress efficient convolutional neural network sentiment analysis main step quantization prune process method responsible map compress network fpga result implementation present describe simulations show five bite width enough drop accuracy float point version network additionally significant memory footprint reduction achieve eighty-five ninety-three
paper propose interactive text2pickup it2p network human robot collaboration enable effective interaction human user despite ambiguity user command focus task robot expect pick object instruct human interact human give instruction vague propose network understand command human user estimate position desire object first handle inherent ambiguity human language command suitable question resolve ambiguity generate user answer question combine initial command give back network result accurate estimation experiment result show give unambiguous command propose method estimate position request object accuracy nine thousand, eight hundred and forty-nine base test dataset give ambiguous language command show accuracy pick task increase one hundred and ninety-four time incorporate information obtain interaction
process translation ambiguous typically many valid trans lations give sentence give rise significant variation parallel cor pora however current model machine translation account variation instead treat prob lem deterministic process end present deep generative model machine translation incorporate chain latent variables order ac count local lexical syntactic varia tion parallel corpora provide depth analysis pitfalls encounter variational inference train deep generative model experiment sev eral different language pair demonstrate model consistently improve strong baselines
accurate abstractive summary document contain salient information logically entail input document improve important aspects abstractive summarization via multi task learn auxiliary task question generation entailment generation former teach summarization model look salient question worthy detail latter teach model rewrite summary direct logical subset input document also propose novel multi task architectures high level semantic layer specific share across multiple encoder decoder layer three task well soft share mechanisms show performance ablations analysis examples contribution overall achieve statistically significant improvements state art cnn dailymail gigaword datasets well duc two thousand and two transfer setup also present several quantitative qualitative analysis study model learn saliency entailment skills
inspire humans summarize long document propose accurate fast summarization model first select salient sentence rewrite abstractively ie compress paraphrase generate concise overall summary use novel sentence level policy gradient method bridge non differentiable computation two neural network hierarchical way maintain language fluency empirically achieve new state art metrics include human evaluation cnn daily mail dataset well significantly higher abstractiveness score moreover first operate sentence level word level enable parallel decode neural generative model result substantially faster ten 20x inference speed well 4x faster train convergence previous long paragraph encoder decoder model also demonstrate generalization model test duc two thousand and two dataset achieve higher score state art model
although neural machine translation encoder decoder framework achieve great success recently still suffer drawbacks forget distant information inherent disadvantage recurrent neural network structure disregard relationship source word encode step whereas practice former information relationship often useful current step target solve problems thus introduce relation network learn better representations source relation network able facilitate memorization capability recurrent neural network via associate source word would also help retain relationships source representations relations feed attention component together decode main encoder decoder framework unchanged experiment several datasets show method improve translation performance significantly conventional encoder decoder model even outperform approach involve supervise syntactic knowledge
consider task align two set point high dimension many applications natural language process computer vision example recently show possible infer bilingual lexicon without supervise data align word embeddings train monolingual data recent advance base adversarial train learn map two embeddings paper propose use alternative formulation base joint estimation orthogonal matrix permutation matrix problem convex propose initialize optimization algorithm use convex relaxation traditionally consider graph isomorphism problem propose stochastic algorithm minimize cost function large scale problems finally evaluate method problem unsupervised word translation align word embeddings train monolingual data task method obtain state art result require less computational resources compete approach
paper propose improvement exist data drive neural belief track nbt framework dialogue state track dst exist nbt model use hand craft belief state update mechanism involve expensive manual retuning step whenever model deploy new dialogue domain show update mechanism learn jointly semantic decode context model part nbt model eliminate last rule base module dst framework propose two different statistical update mechanisms show dialogue dynamics model small number additional model parameters dst evaluation three languages show model achieve competitive performance provide robust framework build resource light dst model
date romantic relationships play huge role personal live also collectively influence shape society today many romantic partnerships originate internet signify importance technology web modern date paper present text base computational approach estimate relationship compatibility two users social media unlike many previous work propose reciprocal recommender systems online date websites devise distant supervision heuristic obtain real world couple social platforms twitter approach couplenet end end deep learn base estimator analyze social profile two users subsequently perform similarity match users intuitively approach perform user profile match make within unify end end framework couplenet utilize hierarchical recurrent neural model learn representations user profile subsequently couple attention mechanisms fuse information aggregate two users best knowledge approach first data drive deep learn approach novel relationship recommendation problem benchmark couplenet several machine learn deep learn baselines experimental result show approach outperform approach significantly term precision qualitative analysis show model capable also produce explainable result users
present empirical analysis state art systems refer expression recognition task identify object image refer natural language expression goal gain insight systems reason language vision surprisingly find strong evidence even sophisticate linguistically motivate model task may ignore linguistic structure instead rely shallow correlations introduce unintended bias data selection annotation process example show system train test input image textitwithout input refer expression achieve precision seven hundred and twelve top two predictions furthermore system predict object category give input achieve precision eight hundred and forty-two top two predictions surprisingly positive result deficient prediction scenarios suggest careful analysis model learn data construct critical seek make substantive progress ground language task
word2vec w2v glove popular fast efficient word embed algorithms embeddings widely use perform well variety natural language process task moreover w2v recently adopt field graph embed underpin several lead algorithms however despite ubiquity relatively simple model architecture theoretical understand embed parameters w2v glove learn useful downstream task lack show different interactions pmi vectors reflect semantic word relationships similarity paraphrase encode low dimensional word embeddings suitable projection theoretically explain embeddings w2v glove work consequence also reveal interest mathematical interconnection consider semantic relationships
variational autoencodersvaes show promise data drive conversation model however vae conversation model match approximate posterior distribution latent variables simple prior standard normal distribution thereby restrict generate responses relatively simple eg unimodal scope paper propose dialogwae conditional wasserstein autoencoderwae specially design dialogue model unlike vaes impose simple distribution latent variables dialogwae model distribution data train gin within latent variable space specifically model sample prior posterior distributions latent variables transform context dependent random noise use neural network minimize wasserstein distance two distributions develop gaussian mixture prior network enrich latent space experiment two popular datasets show dialogwae outperform state art approach generate coherent informative diverse responses
ai2 reason challenge arc new benchmark dataset question answer qa recently release arc contain natural science question author human exams hard answer require advance logic reason arc challenge set exist state art qa systems fail significantly outperform random baseline reflect difficult nature task paper propose novel framework answer science exam question mimic human solve process open book exam address reason challenge construct contextual knowledge graph respectively question support sentence model learn reason neural embeddings knowledge graph experiment arc challenge set show model outperform previous state art qa systems
neural machine translation nmt deep learn base approach machine translation yield state art translation performance scenarios large scale parallel corpora available although high quality domain specific translation crucial real world domain specific corpora usually scarce nonexistent thus vanilla nmt perform poorly scenarios domain adaptation leverage domain parallel corpora well monolingual corpora domain translation important domain specific translation paper give comprehensive survey state art domain adaptation techniques nmt
recent work clark et al introduce ai2 reason challenge arc associate arc dataset partition open domain complex science question easy set challenge set paper include analysis one hundred question respect type knowledge reason require answer however include clear definitions type offer information quality label propose comprehensive set definitions knowledge reason type necessary answer question arc dataset use ten annotators sophisticate annotation interface analyze distribution label across challenge set statistics relate additionally demonstrate although naive information retrieval methods return sentence irrelevant answer query sufficient support text often present arc corpus evaluate human select relevant sentence improve performance neural machine comprehension model forty-two point
exploit sparsity enable hardware systems run neural network faster energy efficiently however prior sparsity centric optimization techniques accelerate forward pass neural network usually require even longer train process iterative prune retrain observe artificially induce sparsity gradients gate lstm cell little impact train quality enforce structure sparsity gate gradients make lstm backward pass forty-five faster state art dense approach one hundred and sixty-eight faster state art sparsifying method modern gpus though structure sparsifying method impact accuracy model performance gap eliminate mix sparse train method standard dense train method experimental result show mix method achieve comparable result shorter time span use purely dense train
present gpu base locality sensitive hash lsh algorithm speed beam search sequence model utilize winner take wta hash base relative rank order hide dimension thus resilient perturbations numerical value algorithm design fully consider underling architecture cuda enable gpus algorithm architecture co design one parallel cuckoo hash table apply lsh code lookup guarantee of1 lookup time two candidate list share across beam maximize parallelism three top frequent word merge candidate list improve performance experiment four large scale neural machine translation model demonstrate algorithm achieve 4x speedup softmax module 2x overall speedup without hurt bleu gpu
advance community detection reveal new insights multiplex multilayer network less work however investigate relationship communities outcomes social systems leverage advance would light relationship cooperative mesostructure international system onset interstate conflict detect communities base upon weaker signal affinity express unite nations vote speeches well stronger signal observe across multiple layer bilateral cooperation communities diplomatic affinity display expect negative relationship conflict onset tie communities base upon observe cooperation however display effect standard model specification positive relationship conflict alternative specification result align extant hypotheses also point paucity understand relationship community structure behavioral outcomes network
paper propose novel framework recurrent neural network rnns inspire human memory model field cognitive neuroscience enhance information process transmission adjacent rnns units propose framework rnns consist three stag work memory forget long term store first stage include take input data sensory memory transfer work memory preliminary treatment second stage mainly focus proactively forget secondary information rather primary work memory finally get long term store normally use kind rnn unit framework generalize simple evaluate six datasets fall three different task correspond text classification image classification language model experiment reveal framework obviously improve performance traditional recurrent neural network exploratory task show ability framework correctly forget secondary information
present neural model generate short stories image sequence extend image description model vinyals et al vinyals et al two thousand and fifteen extension rely encoder lstm compute context vector story image sequence context vector use first state multiple independent decoder lstms generate portion story correspond image sequence take image embed first input model show competitive result meteor metric human rat internal track visual storytelling challenge two thousand and eighteen
attention typically use select informative sub phrase use prediction paper investigate novel use attention form feature augmentation ie cast attention propose multi cast attention network mcan new attention mechanism general model architecture potpourri rank task conversational model question answer domains approach perform series soft attention operations time cast scalar feature upon inner word embeddings key idea provide real value hint feature subsequent encoder layer target improve representation learn process several advantage design eg allow arbitrary number attention mechanisms cast allow multiple attention type eg co attention intra attention attention variants eg alignment pool max pool mean pool execute simultaneously eliminate costly need tune nature co attention layer also provide greater extents explainability practitioners via extensive experiment four well know benchmark datasets show mcan achieve state art performance ubuntu dialogue corpus mcan outperform exist state art model nine mcan also achieve best perform score date well study trecqa dataset
extraction miss attribute value find value describe attribute interest free text input past relate work extraction miss attribute value work close world assumption possible set value know beforehand use dictionaries value hand craft feature discover new attribute value never see limit human annotation supervision study problem context product catalog often miss value many attribute interest work leverage product profile information title descriptions discover miss value product attribute develop novel deep tag model opentag extraction problem follow contributions one formalize problem sequence tag task propose joint model exploit recurrent neural network specifically bidirectional lstm capture context semantics conditional random field crf enforce tag consistency two develop novel attention mechanism provide interpretable explanation model decisions three propose novel sample strategy explore active learn reduce burden human annotation opentag use dictionary hand craft feature prior work extensive experiment real life datasets different domains show opentag active learn strategy discover new attribute value one hundred and fifty annotate sample reduction 33x amount annotation effort high f score eighty-three outperform state art model
instructional systems design practice create instructional experience make acquisition knowledge skill efficient effective appeal specifically design course hour train material require thirty five hundred hours effort source organize reference data use preparation course material paper present first system kind help reduce effort associate source reference material course creation present algorithms document chunk automatic generation learn objectives content create descriptive content metadata improve content discoverability unlike exist methods learn objectives generate system incorporate pedagogically motivate bloom verbs demonstrate usefulness methods use real world data bank industry live deployment large pharmaceutical company
variety methods exist generate synthetic electronic health record ehrs capable generate unstructured text like emergency department ed chief complaints history present illness progress note use encoder decoder model deep learn algorithm feature many contemporary machine translation systems generate synthetic chief complaints discrete variables ehrs like age group gender discharge diagnosis train end end authentic record model generate realistic chief complaint text preserve much epidemiological information original data side effect model optimization goal synthetic chief complaints also free relatively uncommon abbreviation misspell include none personally identifiable information pii train data suggest may use support de identification text ehrs combine algorithms like generative adversarial network gans model could use generate fully synthetic ehrs facilitate data share healthcare providers researchers improve ability develop machine learn methods tailor information healthcare data
state mind base experience people tell us may result conflict information uncertainty alternative facts present robot model relativity knowledge perception within social interaction follow principles theory mind utilize vision speech capabilities pepper robot build interaction model store interpretations perceptions conversations combination provenance source robot learn directly people tell possibly relation perception demonstrate robot communication drive hunger acquire knowledge people object resolve uncertainties conflict share awareness per ceived environment likewise robot make reference world knowledge world encounter people yield knowledge
paper explore use adversarial examples train speech recognition systems increase robustness deep neural network acoustic model train fast gradient sign method use generate adversarial examples augment original train data different conventional data augmentation base data transformations examples dynamically generate base current acoustic model parameters assess impact adversarial data augmentation experiment aurora four chime four single channel task show improve robustness noise channel variation improvement obtain combine adversarial examples teacher student train lead twenty-three relative word error rate reduction aurora four
commonsense reason long stand challenge deep learn example difficult use neural network tackle winograd schema dataset levesque et al two thousand and eleven paper present simple method commonsense reason neural network use unsupervised learn key method use language model train massive amount unlabled data score multiple choice question pose commonsense reason test pronoun disambiguation winograd schema challenge model outperform previous state art methods large margin without use expensive annotate knowledge base hand engineer feature train array large rnn language model operate word character level lm one billion commoncrawl squad gutenberg book customize corpus task show diversity train data play important role test performance analysis also show system successfully discover important feature context decide correct answer indicate good grasp commonsense knowledge
sentiment classification involve quantify affective reaction human document media item event although researchers investigate several methods reliably infer sentiment lexical speech body language cue train model small set label datasets still challenge instance expand sentiment analysis new languages culture may always possible obtain comprehensive label datasets paper investigate application semi supervise transfer learn methods improve performances low resource sentiment classification task experiment extract dense feature representations pre train manifold regularization enhance performance sentiment classification systems goal coherent implementation methods evaluate gain achieve methods match set involve train test single corpus set well two cross corpora settings case experiment demonstrate propose methods significantly enhance model performance purely supervise approach particularly case involve handful train data
introduce probabilistic fasttext new model word embeddings capture multiple word sense sub word structure uncertainty information particular represent word gaussian mixture density mean mixture component give sum n grams representation allow model share statistical strength across sub word structure eg latin root produce accurate representations rare misspell even unseen word moreover component mixture capture different word sense probabilistic fasttext outperform fasttext probabilistic model dictionary level probabilistic embeddings incorporate subword structure several word similarity benchmarks include english rareword foreign language datasets also achieve state art performance benchmarks measure ability discern different mean thus propose model first achieve multi sense representations enrich semantics rare word
many structure prediction problems particularly vision language domains ambiguous multiple output correct input eg many ways describe image multiple ways translate sentence however exhaustively annotate applicability possible output intractable due exponentially large output space eg english sentence practice problems cast multi class prediction likelihood sparse set annotations maximize unfortunately penalize place beliefs plausible unannotated output make test follow hypothesis give input annotations neighbor may serve additional supervisory signal specifically propose objective transfer supervision neighbor examples first study properties develop method control toy setup report result multi label classification two image ground sequence model task caption question generation evaluate use standard task specific metrics measure output diversity find consistent improvements standard maximum likelihood train baselines
long short term memory lstm one widely use recurrent structure sequence model aim use gate control information flow eg whether skip information recurrent computations although practical implementation base soft gate partially achieve goal paper propose new way lstm train push output value gate towards zero one better control information flow gate mostly open close instead middle state make result interpretable empirical study show one although seem restrict model capacity performance drop achieve better comparable performances due better generalization ability two output gate sensitive input easily compress lstm unit multiple ways eg low rank approximation low precision approximation compress model even better baseline model without compression
text classification become indispensable due rapid increase text digital form past three decades efforts make approach task use various learn algorithms statistical model base bag word bow feature despite simple implementation bow feature lack semantic mean representation solve problem neural network start employ learn word vectors word2vec word2vec embed word semantic structure vectors angle vectors indicate meaningful similarity word measure similarity texts propose novel concept word subspace represent intrinsic variability feature set word vectors concept possible model text word vectors hold semantic information incorporate word frequency directly subspace model extend word subspace term frequency tf weight word subspace base new concepts text classification perform mutual subspace method msm framework validity model show experiment reuters text database compare result various state art algorithms
lexnlp open source python package focus natural language process machine learn legal regulatory text package include functionality segment document ii identify key text title section head iii extract eighteen type structure information like distance date iv extract name entities company geopolitical entities v transform text feature model train vi build unsupervised supervise model word embed tag model lexnlp include pre train model base thousands unit test draw real document available sec edgar database well various judicial regulatory proceed lexnlp design use academic research industrial applications distribute https githubcom lexpredict lexpredict lexnlp
great proportion sequence sequence seq2seq model neural machine translation nmt adopt recurrent neural network rnn generate translation word word follow sequential order study linguistics prove language linear word sequence sequence complex structure translation step condition whole target side context tackle problem propose new nmt model decode sequence guidance structural prediction context target sequence model generate translation base structural prediction target side context translation free bind sequential order experimental result demonstrate model competitive compare state art methods analysis reflect model also robust translate sentence different lengths also reduce repetition instruction target side context decode
paper present ingress robot system follow human natural language instructions pick place everyday object core issue ground refer expressions infer object relationships input image language expressions ingress allow unconstrained object categories unconstrained language expressions ask question disambiguate refer expressions interactively achieve take approach ground generation propose two stage neural network model ground first stage use neural network generate visual descriptions object compare input language expression identify set candidate object second stage use another neural network examine pairwise relations candidates infer likely refer object neural network use ground question generation disambiguation experiment show ingress outperform state art method refcoco dataset robot experiment humans
work propose novel constituency parse scheme model predict vector real value scalars name syntactic distance split position input sentence syntactic distance specify order split point select recursively partition input top fashion compare traditional shift reduce parse scheme approach free potential problem compound errors faster easier parallelize model achieve competitive performance amongst single model discriminative parsers ptb dataset outperform previous model ctb dataset
stochastic zeroth order szo gradient free optimization allow optimize arbitrary function rely function evaluations parameter perturbations however iteration complexity szo methods suffer factor proportional dimensionality perturb function show scenarios natural sparsity pattern structure prediction applications factor reduce expect number active feature input output pair give general proof apply sparse szo optimization lipschitz continuous nonconvex stochastic objectives present experimental evaluation linear bandit structure prediction task sparse word base feature representations confirm theoretical result
autoregressive feedback consider necessity successful unconditional text generation use stochastic sequence model however feedback know introduce systematic bias train process obscure principle generation commit global information forget local nuances show non autoregressive deep state space model clear separation global local uncertainty build two ingredients independent noise source deterministic transition function recent advance flow base variational inference use train evidence lower bind without resort anneal auxiliary losses similar measure result highly interpretable generative model par comparable auto regressive model task word generation
describe neural network base system text speech tts synthesis able generate speech audio voice many different speakers include unseen train system consist three independently train components one speaker encoder network train speaker verification task use independent dataset noisy speech thousands speakers without transcripts generate fix dimensional embed vector second reference speech target speaker two sequence sequence synthesis network base tacotron two generate mel spectrogram text condition speaker embed three auto regressive wavenet base vocoder convert mel spectrogram sequence time domain waveform sample demonstrate propose model able transfer knowledge speaker variability learn discriminatively train speaker encoder new task able synthesize natural speech speakers see train quantify importance train speaker encoder large diverse speaker set order obtain best generalization performance finally show randomly sample speaker embeddings use synthesize speech voice novel speakers dissimilar use train indicate model learn high quality speaker representation
speech recognizers train close talk speech generalize distant speech word error rate degradation large forty absolute study focus tackle distant speech recognition separate problem leave little effort adapt close talk speech recognizers distant speech work review several approach domain adaptation perspective approach include speech enhancement multi condition train data augmentation autoencoders involve transformation data domains conduct experiment ami data set approach realize control set approach lead different amount improvement respective assumptions purpose paper quantify characterize performance gap two domains set basis study adaptation speech recognizers close talk speech distant speech result also implications improve distant speech recognition
sequence sequence attention base model integrate acoustic pronunciation language model single neural network make suitable multilingual automatic speech recognition asr paper concern multilingual speech recognition low resource languages single transformer one sequence sequence attention base model sub word employ multilingual model unit without use pronunciation lexicon first show single multilingual asr transformer perform well low resource languages despite language confusion look incorporate language information model insert language symbol begin end original sub word sequence condition language information know train experiment callhome datasets demonstrate multilingual asr transformer language symbol end perform better obtain relatively one hundred and five average word error rate wer reduction compare shl mlstm residual learn go show assume language information know train test relatively one hundred and twenty-four average wer reduction observe compare shl mlstm residual learn give language symbol sentence start token
introduce generative neural machine translation gnmt latent variable architecture design model semantics source target sentence modify encoder decoder translation model add latent variable language agnostic representation encourage learn mean sentence gnmt achieve competitive bleu score pure translation task superior miss word source sentence augment model facilitate multilingual translation semi supervise learn without add parameters framework significantly reduce overfitting limit pair data available effective translate pair languages see train
dispute human brain represent conceptual knowledge argue many scientific field brain image study show spatial pattern neural activation brain correlate think different semantic categories word example tool animals build view relate picture paper present computational model learn predict neural activation capture functional magnetic resonance image fmri data test word unlike model hand craft feature use literature paper propose novel approach wherein decode model build feature extract popular linguistic encode word2vec glove meta embeddings conjunction empirical fmri data associate view several dozen concrete nouns compare model several model use word feature extract fasttext randomly generate feature mitchell twenty-five feature one experimental result show predict fmri image use meta embeddings meet state art performance although model feature glove word2vec predict fmri image similar state art model model feature meta embeddings predict significantly better propose scheme use popular linguistic encode offer simple easy approach semantic decode fmri experiment
introduce attentive unsupervised text writer autr word level generative model natural language use recurrent neural network dynamic attention canvas memory mechanism iteratively construct sentence view state memory intermediate stag model place attention gain insight construct sentence demonstrate autr learn meaningful latent representation sentence achieve competitive log likelihood lower bound whilst computationally efficient effective generate reconstruct sentence well impute miss word
two thousand and seventeen fake news challenge stage one fnc one share task address stance classification task crucial first step towards detect fake news date depth analysis paper critically discuss fnc one experimental setup reproduce result draw conclusions next generation stance classification methods paper provide depth analysis three top perform systems first find fnc one propose evaluation metric favor majority class easily classify thus overestimate true discriminative power methods therefore propose new f1 base metric yield change system rank next compare feature architectures use lead novel feature rich stack lstm model perform par best systems superior predict minority class understand methods ability generalize derive new dataset perform domain cross domain experiment qualitative quantitative study help interpret original fnc one score understand feature help improve performance new dataset source code use reproduction study publicly available future research
parallel sentence extraction task address data sparsity problem find multilingual natural language process applications propose bidirectional recurrent neural network base approach extract parallel sentence collections multilingual texts experiment noisy parallel corpora show achieve promise result competitive baseline remove need specific feature engineer additional external resources justify utility approach extract sentence pair wikipedia article train machine translation systems show significant improvements translation performance
modern deep transfer learn approach mainly focus learn generic feature vectors one task transferable task word embeddings language pretrained convolutional feature vision however approach usually transfer unary feature largely ignore structure graphical representations work explore possibility learn generic latent relational graph capture dependencies pair data units eg word pixels large scale unlabeled data transfer graph downstream task propose transfer learn framework improve performance various task include question answer natural language inference sentiment analysis image classification also show learn graph generic enough transfer different embeddings graph train include glove embeddings elmo embeddings task specific rnn hide unit embed free units image pixels
lack comprehensive high quality health data develop nations create roadblock combat impact disease one key challenge understand health information need people nations without understand people everyday need concern misconceptions health organizations policymakers lack ability effectively target education program efforts paper propose bottom approach use search data individuals uncover gain insight health information need africa analyze bing search relate hiv aid malaria tuberculosis fifty-four african nations disease automatically derive set common search theme topics reveal wide spread interest various type information include disease symptoms drug concern breastfeed well stigma beliefs natural cure topics may hard uncover traditional survey expose different pattern emerge health information need demographic group age sex country also uncover discrepancies quality content return search engines users topic combine result suggest search data help illuminate health information need africa inform discussions health policy target education efforts offline
learn multimodal representations fundamentally complex research problem due presence multiple heterogeneous source information although presence multiple modalities provide additional valuable information two key challenge address learn multimodal data one model must learn complex intra modal cross modal interactions prediction two model must robust unexpected miss noisy modalities test paper propose optimize joint generative discriminative objective across multimodal data label introduce model factorize representations two set independent factor multimodal discriminative modality specific generative factor multimodal discriminative factor share across modalities contain joint multimodal feature require discriminative task sentiment prediction modality specific generative factor unique modality contain information require generate data experimental result show model able learn meaningful multimodal representations achieve state art competitive performance six multimodal datasets model demonstrate flexible generative capabilities condition independent factor reconstruct miss modalities without significantly impact performance lastly interpret factorize representations understand interactions influence multimodal learn
regional language extraction natural scene image always challenge proposition due dependence text information extract image text extraction hand vary different light condition arbitrary orientation inadequate text information heavy background influence text change text appearance paper present novel unify method tackle challenge propose work use image correction segmentation technique exist text detection pipeline efficient accurate scene text detector east east use standard pvanet architecture select feature non maximal suppression detect text image text recognition do use combine architecture maxout convolution neural network cnn bidirectional long short term memory lstm network recognize text use deep learn base approach native languages translate english tokenized use standard text tokenizers tokens likely represent location use find global position system gps coordinate location subsequently regional languages speak location extract propose method test self generate dataset collect government india dataset experiment standard dataset evaluate performance propose technique comparative study state art methods text detection recognition extraction regional language image show propose method outperform exist methods
many modern artificial intelligence ai systems make use data embeddings particularly domain natural language process nlp embeddings learn data gather wild find contain unwanted bias paper make three contributions towards measure understand remove problem present rigorous way measure bias base use word list create social psychology applications observe gender bias occupations reflect actual gender bias occupations real world finally demonstrate simple projection significantly reduce effect embed bias part ongoing effort understand trust build ai systems
end end model show superiority automatic speech recognition asr time capacity stream recognition become grow requirement end end model follow trend encoder decoder recurrent neural network call recurrent neural aligner rna freshly propose show competitiveness two english asr task however clear rna improve apply speak language work explore applicability rna mandarin chinese present four effective extensions encoder redesign temporal sample introduce powerful convolutional structure decoder utilize regularizer smooth output distribution conduct joint train language model two mandarin chinese conversational telephone speech recognition mts datasets extend rna obtain promise performance particularly achieve two hundred and seventy-seven character error rate cer superior current state art result popular hkust task
sememes minimum semantic units concepts human languages word sense compose one multiple sememes word usually manually annotate sememes linguists form linguistic common sense knowledge base widely use various nlp task recently lexical sememe prediction task introduce consist automatically recommend sememes word expect improve annotation efficiency consistency however exist methods lexical sememe prediction typically rely external context word represent mean usually fail deal low frequency vocabulary word address issue chinese propose novel framework take advantage internal character information external context information word experiment hownet chinese sememe knowledge base demonstrate framework outperform state art baselines large margin maintain robust performance even low frequency word
gate key technique use integrate information multiple source long short term memory lstm model recently also apply model highway network although gate powerful rather expensive term computation storage gate unit use separate full weight matrix issue severe since several gate use together eg lstm cell paper propose semi tie unit stu approach solve efficiency issue use one share weight matrix replace units layer approach term semi tie since extra parameters use separately scale share output value extra scale factor associate network activation function result use parameterised sigmoid hyperbolic tangent rectify linear unit function speech recognition experiment use british english multi genre broadcast data show use stus reduce calculation storage cost factor three highway network four lstms give similar word error rat original model
work focus combine nonparametric topic model auto encode variational bay aevb specifically first propose itm vae topics treat trainable parameters document specific topic proportion obtain stick break construction inference itm vae model neural network compute simple fee forward manner also describe introduce hyper prior itm vae model uncertainty prior parameter actually hyper prior technique quite general show apply aevb base model alleviate collapse prior problem elegantly moreover also propose hitm vae document specific topic distributions generate hierarchical manner hitm vae even flexible generate topic distributions better variability experimental result 20news reuters rcv1 v2 datasets show propose model outperform state art baselines significantly advantage hyper prior technique hierarchical model construction also confirm experiment
model compression essential serve large deep neural net devices limit resources applications require real time responses case study state art neural language model usually consist one recurrent layer sandwich embed layer use represent input tokens softmax layer generate output tokens problems large vocabulary size embed softmax matrices account half model size instance biglstm model achieve state art performance one billion word obw dataset around 800k vocabulary word embed softmax matrices use 6gbytes space responsible ninety model parameters paper propose groupreduce novel compression method neural language model base vocabulary partition block base low rank matrix approximation inherent frequency distribution tokens power law distribution word experimental result show method significantly outperform traditional compression methods low rank approximation prune obw dataset method achieve sixty-six time compression rate embed softmax matrices combine quantization method achieve twenty-six time compression rate translate factor one hundred and twenty-eight time compression entire model little degradation perplexity
confidential text corpora exist many form allow arbitrary share explore use private corpora use privacy preserve text analytics construct typical text process applications use appropriate privacy preservation techniques include homomorphic encryption rademacher operators secure computation set preliminary materials rademacher operators binary classifiers construct basic text process approach match binary classifiers
identify crime forensic investigate team crimes involve people different nationals challenge paper propose new method ethnicity nationality identification base cloud line distribution cold feature handwrite components propose method first explore tangent angle contour pixels row mean intensity value row image segment text line segment text line use tangent angle direction base line remove rule line image use polygonal approximation find dominant point contour edge components propose method connect nearest dominant point every dominant point result line segment dominant point pair line segment propose method estimate angle length give point polar domain line segment propose method generate dense point polar domain result cold distribution character component shape change accord nationals shape distribution change observation extract base distance pixels distribution principal axis distribution feature subject svm classifier identify nationals experiment conduct complex dataset show propose method effective outperform exist method
state art speech recognition systems rely fix hand craft feature mel filterbanks preprocess waveform train pipeline paper study end end systems train directly raw waveform build two alternatives trainable replacements mel filterbanks use convolutional architecture first one inspire gammatone filterbanks hoshen et al two thousand and fifteen sainath et al two thousand and fifteen second one scatter transform zeghidour et al two thousand and seventeen propose two modifications architectures systematically compare mel filterbanks wall street journal dataset first modification addition instance normalization layer greatly improve gammatone base trainable filterbanks speed train scatter base filterbanks second one relate low pass filter use approach modifications consistently improve performances approach remove need careful initialization scatter base trainable filterbanks particular show consistent improvement word error rate trainable filterbanks relatively comparable mel filterbanks first time end end model train raw signal significantly outperform mel filterbanks large vocabulary task clean record condition
sentence simplification aim improve readability understandability base several operations split deletion paraphrase however valid simplify sentence also logically entail input sentence work first present strong pointer copy mechanism base sequence sequence sentence simplification model improve entailment paraphrase capabilities via multi task learn relate auxiliary task entailment paraphrase generation moreover propose novel multi level layer soft share approach auxiliary task share different higher versus lower level layer sentence simplification model depend task semantic versus lexico syntactic nature also introduce novel multi arm bandit base train approach dynamically learn effectively switch across task multi task learn experiment multiple popular datasets demonstrate model outperform competitive simplification systems sari fkgl automatic metrics human evaluation present several ablation analyse alternative layer share methods soft versus hard share dynamic multi arm bandit sample approach model learn entailment paraphrase skills
paper present context multi channel asr method adapt mask base statistically optimal beamforming approach speaker interest beamforming vector statistically optimal beamformer compute utilize speech noise mask estimate neural network propose adaptation approach base integration beamformer include mask estimation network acoustic model asr system allow propagation train error acoustic model cost function way beamforming operation mask estimation network use result first pass recognition keep parameters fix mask estimation network therefore fine tune retrain utterances speaker interest thus use two pass approach optimize beamforming speech characteristics specific speaker show approach improve asr performance state art multi channel asr system chime four data furthermore effect adaptation estimate speech mask discuss
twitter stream become large source information many people magnitude tweet noisy nature content make harvest knowledge twitter challenge task researchers long time aim overcome main challenge extract hide information tweet stream work propose new approach real time detection news events twitter stream divide approach three step first step use neural network deep learn detect news relevant tweet stream second step apply novel stream data cluster algorithm detect news tweet form news events third final step rank detect events base size event cluster growth speed tweet frequencies evaluate propose system large publicly available corpus annotate news events twitter part evaluation compare approach relate state art solution overall experiment user base evaluation show approach detect current real news events deliver state art performance
online news media sometimes use mislead headline lure users open news article catchy headline attract users disappoint end call clickbaits importance automatic clickbait detection online medias lot machine learn methods propose employ find clickbait headline research model use deep learn methods propose find clickbaits clickbait challenge two thousand and seventeen dataset propose model gain first rank clickbait challenge two thousand and seventeen term mean square error also data analytics visualization techniques employ explore discover provide dataset get insight data
inspire recent work fink reeve palma farr two thousand and seventeen innovation language gastronomy technology study new symbol discovery manifest term additional word vocabulary available dictionaries generate finite number symbols several distinct dictionary generation model investigate use numerical simulation emphasis scale knowledge dictionary generators parameters vary role order symbols discover
field natural language process nlp revisit well know word embed algorithm word2vec word embeddings identify word vectors word distributional similarity capture unexpectedly besides semantic similarity even relational similarity show capture word embeddings generate word2vec whence two question arise firstly kind relations representable continuous space secondly relations build order tackle question propose bottom point view call generate input text word2vec output target relations solve corpus replication task deem generalizations approach set relations possible expect solve corpus replication task provide partial answer question
scientific digital libraries paper different research communities describe community dependent keywords even share semantically similar topic article tag enough keyword variations poorly index information retrieval system limit potentially fruitful exchange scientific discipline paper introduce novel experimentally design pipeline multi label semantic base tag develop open access metadata digital libraries approach start learn standard scientific categorization sample topic tag article find semantically relevant article enrich metadata accordingly propose pipeline aim enable researchers reach article various discipline tend use different terminologies allow retrieve semantically relevant article give limit know variation search term addition achieve accuracy higher expand query base method use topic synonym set extract semantic network experiment also show higher computational scalability versus comparable techniques create new benchmark extract open access metadata scientific digital library publish along experiment code allow research topic
dialog systems need understand dynamic visual scenes order conversations users object events around scene aware dialog systems real world applications could develop integrate state art technologies multiple research areas include end end dialog technologies generate system responses use model train dialog data visual question answer vqa technologies answer question image use learn image feature video description technologies descriptions caption generate videos use multimodal information introduce new dataset dialogs videos human behaviors dialog type conversation consist sequence ten question answerqa pair two amazon mechanical turk amt workers total collect dialogs roughly nine thousand videos use new dataset audio visual scene aware dialog avsd train end end conversation model generate responses dialog video experiment demonstrate use multimodal feature develop multimodal attention base video description enhance quality generate dialog dynamic scenes videos dataset model code pretrained model publicly available new video scene aware dialog challenge
variational autoencoder vae impose probabilistic distribution typically gaussian latent space penalize kullback leibler kl divergence posterior prior nlp vaes extremely difficult train due problem kl collapse zero one implement various heuristics kl weight anneal word dropout carefully engineer manner successfully train vae text paper propose use wasserstein autoencoder wae probabilistic sentence generation encoder could either stochastic deterministic show theoretically empirically original wae stochastically encode gaussian distribution tend become dirac delta function propose variant wae encourage stochasticity encoder experimental result show latent space learn wae exhibit properties continuity smoothness vaes simultaneously achieve much higher bleu score sentence reconstruction
propose approach train speaker identification model weakly supervise manner concentrate set train data consist set audio record speaker annotation provide record level method use speaker diarization find unique speakers record vectors project speech speaker fix dimensional vector neural network train map vectors speakers use special objective function allow optimize model use record level speaker label report experiment two different real world datasets voxceleb dataset method provide nine hundred and forty-six accuracy close set speaker identification task surpass baseline performance large margin estonian broadcast news dataset method provide sixty-six time weight speaker identification recall ninety-three precision
many machine read natural language understand task require read support text order answer question example question answer support text newswire wikipedia article natural language inference premise see support text hypotheses question provide set useful primitives operate single framework relate task would allow expressive model easier model comparison replication end present jack reader jack framework machine read allow quick model prototyping component reuse evaluation new model exist datasets well integrate new datasets apply grow set implement baseline model jack currently support limit three task question answer natural language inference link prediction develop aim increase research efficiency code reuse
deep learn improve performance many natural language process nlp task individually however general nlp model emerge within paradigm focus particularities single metric dataset task introduce natural language decathlon decanlp challenge span ten task question answer machine translation summarization natural language inference sentiment analysis semantic role label zero shoot relation extraction goal orient dialogue semantic parse commonsense pronoun resolution cast task question answer context furthermore present new multitask question answer network mqan jointly learn task decanlp without task specific modules parameters multitask set mqan show improvements transfer learn machine translation name entity recognition domain adaptation sentiment analysis natural language inference zero shoot capabilities text classification demonstrate mqan multi pointer generator decoder key success performance improve anti curriculum train strategy though design decanlp mqan also achieve state art result wikisql semantic parse task single task set also release code procure process data train evaluate model reproduce experiment decanlp
recurrent neural network rnns draw much attention great success many applications like speech recognition neural machine translation long short term memory lstm one popular rnn units deep learn applications lstm transform input previous hide state next state affine transformation multiplication operations nonlinear activation function make good data representation give task affine transformation include rotation reflection change semantic syntactic information dimension hide state however consider model interpret output sequence lstm whole input sequence dimension state need keep type semantic syntactic information regardless location sequence paper propose simple variant lstm unit persistent recurrent unit pru dimension hide state keep persistent information across time space keep mean whole sequence addition improve nonlinear transformation power add feedforward layer pru structure experiment evaluate propose methods three different task result confirm methods better performance conventional lstm
emerge technique deep learn widely apply many different areas however adopt certain specific domain technique combine domain knowledge improve efficiency accuracy particular analyze applications deep learn sentiment analysis find current approach suffer follow drawbacks exist work pay much attention importance different type sentiment term important concept area ii loss function currently employ well reflect degree error sentiment misclassification overcome problem propose combine domain knowledge deep learn proposal include use sentiment score learn quadratic program augment train data introduce penalty matrix enhance loss function cross entropy experiment achieve significant improvement classification result
current approach speech emotion recognition focus speech feature capture emotional content speech signal mel frequency cepstral coefficients mfccs one commonly use representations audio speech recognition classification paper propose gammatone frequency cepstral coefficients gfccs potentially better representation speech signal emotion recognition effectiveness mfcc gfcc representations compare evaluate emotion intensity classification task fully connect recurrent neural network architectures result provide evidence gfccs outperform mfccs speech emotion recognition
informative evaluation must measure well systems generalize realistic unseen data identify limitations propose improvements current evaluations text sql systems first compare human generate automatically generate question characterize properties query necessary real world applications facilitate evaluation multiple datasets release standardize improve versions seven exist datasets one new text sql dataset second show current division data train test set measure robustness variations way question ask partially test well systems generalize new query therefore propose complementary dataset split evaluation future work finally demonstrate common practice anonymizing variables evaluation remove important challenge task observations highlight key difficulties methodology enable effective measurement future development
paper address scalability challenge architecture search formulate task differentiable manner unlike conventional approach apply evolution reinforcement learn discrete non differentiable search space method base continuous relaxation architecture representation allow efficient search architecture use gradient descent extensive experiment cifar ten imagenet penn treebank wikitext two show algorithm excel discover high performance convolutional architectures image classification recurrent architectures language model order magnitude faster state art non differentiable techniques implementation make publicly available facilitate research efficient architecture search algorithms
present prototype news search engine present balance viewpoints across liberal conservative article goal de polarize content allow users escape filter bubble balance do accord flexible user define constraints leverage recent advance constrain bandit optimization showcase balance news fee display side side news fee produce traditional polarize fee
paper propose single channel speech dereverberation system deregat base convolutional bidirectional long short term memory deep fee forward neural network cbldnn generative adversarial train gat order obtain better speech quality instead minimize mean square error mse gat employ make dereverberated speech indistinguishable form clean sample besides system deal wide range reverberation well adapt variant environments experimental result show propose model outperform weight prediction error wpe deep neural network base systems addition deregat extend online speech dereverberation scenario report comparable performance offline case
work present hierarchical deep learn natural language parser fashion proposal intend recognize fashion domain entities also expose syntactic morphologic insights leverage usage architecture specialist model one different task parse entity recognition architecture render hierarchical model able capture nuances fashion language natural language parser able deal textual ambiguities leave unresolved currently exist solution empirical result establish robust baseline justify use hierarchical architectures deep learn model open new research avenues explore
paper present database emotional speech intend open source use synthesis generation purpose contain data male female actors english male actor french database cover five emotion class could suitable build synthesis voice transformation systems potential control emotional dimension continuous way show data efficiency build simple mlp system convert neutral angry speech style evaluate via cmos perception test even though system simple one test show efficiency data promise future work
paper attempt provide state art trend prediction use news headline present research do predict djia trend use natural language process explain different algorithms use well various embed techniques attempt rely statistical deep learn model order extract information corpuses
map translate professional arcane clinical jargons consumer language essential improve patient clinician communication researchers use exist biomedical ontologies consumer health vocabulary dictionary translate languages however approach limit expert efforts manually build dictionary hard generalize scalable work utilize embeddings alignment method word map unparalleled clinical professional consumer language embeddings map semantically similar word two different word embeddings first independently train word embeddings corpus abundant clinical professional term mainly healthcare consumer term align embeddings procrustes algorithm also investigate approach adversarial train refinement evaluate quality alignment similar word retrieval compute model precision well judge qualitatively human show procrustes algorithm performant professional consumer language embeddings alignment whereas adversarial train refinement may find relations two languages
online environment provide great opportunity insurance policyholders share complaints respect different service complaints reveal valuable information insurance company seek improve service however analyze huge number online complaints complicate task human must involve computational methods create efficient process research propose computational approach characterize major topics large number online complaints approach base use topic model approach disclose latent semantic complaints propose approach deploy thousands geico negative review analyze one thousand, three hundred and seventy-one geico complaints indicate thirty major complain four categories one customer service two insurance coverage paperwork policy report three legal issue four cost estimate payments research approach use applications explore large number review
broad set deep generative model dgms achieve remarkable advance however often difficult incorporate rich structure domain knowledge end end dgms posterior regularization pr offer principled framework impose structure constraints probabilistic model limit applicability diverse dgms lack bayesian formulation even explicit density evaluation pr also require constraints fully specify priori impractical suboptimal complex knowledge learnable uncertain part paper establish mathematical correspondence pr reinforcement learn rl base connection expand pr learn constraints extrinsic reward rl result algorithm model agnostic apply dgms flexible adapt arbitrary constraints model jointly experiment human image generation templated sentence generation show model learn knowledge constraints algorithm greatly improve base generative model
classic topic model build bag word assumption word position ignore simplicity besides symmetric priors typically use applications order easily learn topics different properties among corpus propose new line work paragraph structure exploit proposal base follow assumption many text document corpora formal constraints share across collection eg section assumption satisfy paragraph may relate general concepts share document corpus others would contain genuine description document assume paragraph semantically general specific hybrid look ways measure transfer distinction topics able learn call specific general topics experiment show proper methodology highlight certain paragraph structure document time learn interest diverse topics
paper present new method text independent speaker verification combine segmental dynamic time warp sdtw vector approach vectors generate fee forward deep neural network train distinguish speakers use feature perform alignment hence calculate overall distance enrolment test utteranceswe present result nist two thousand and eight data set speaker verification propose method outperform conventional vector baseline plda score outperform vector approach local distance base cosine plda score also score combination vector plda baseline lead significant gain methods
research question answer knowledge base recently see increase use deep architectures extend abstract study application neural machine translation paradigm question parse employ sequence sequence model learn graph pattern sparql graph query language compositions instead induce program question answer pair expect semi supervise approach alignments question query build templates argue coverage language utterances expand use late notable work natural language generation
speak language understand one key factor dialogue system context conversation play important role understand current utterance work demonstrate importance context within dialogue neural network model online web interface live demo develop two different neural model model use context context base model context model classify dialogue act utterance level whereas context base model take precede utterances account make train neural model available live demo call discourse wizard use modular server architecture live demo provide easy use interface conversational analysis discover deep discourse structure conversation
introduce textworld sandbox learn environment train evaluation rl agents text base game textworld python library handle interactive play text game well backend function like state track reward assignment come curated list game whose feature challenge analyze significantly enable users handcraft automatically generate new game generative mechanisms give precise control difficulty scope language construct game use relax challenge inherent commercial text game like partial observability sparse reward generate set vary similar game textworld also use study generalization transfer learn cast text base game reinforcement learn formalism use framework develop set benchmark game evaluate several baseline agents set curated list
descriptive title provide crucial context interpret table extract web page key component table base web applications prior approach attempt produce title select exist text snippets associate table approach however limit dependence suitable title exist priori user study observe relevant information title tend scatter across page often eighty time appear verbatim anywhere page propose instead application sequence sequence neural network model generalizable mean generate high quality title accomplish extract many text snippets potentially relevant information table encode input sequence use copy generation mechanisms decoder balance relevance readability generate title validate approach human evaluation sample web table report sequence model copy mechanism generation mechanism easily outperform simple selection base baselines model capabilities outperform approach quality crowdsourced title train fewer ten thousand examples best knowledge propose technique first consider text generation methods table title establish new state art
topic discovery witness significant growth field data mine large particular time evolve topic discovery evolution topic take account instrumental understand historical context emerge topic dynamic corpus traditionally time evolve topic discovery focus notion time however especially settings content contribute community crowd orthogonal notion time one pertain level expertise content creator experience creator advance topic paper propose novel time evolve topic discovery method addition extract topics able identify evolution topic time well level difficulty topic infer level expertise main contributors method base novel formulation constrain couple matrix tensor factorization adopt constraints well motivate demonstrate essential high quality topic discovery qualitatively evaluate approach use real data physics also program stack exchange forum able identify topics vary level difficulty link external events announcement gravitational wave ligo lab physics forum provide quantitative evaluation method conduct user study experts ask judge coherence quality extract topics finally propose method implications automatic curriculum design use extract topics notion level difficulty necessary proper model prerequisites advance concepts
measure similarity basic task information retrieval often build block complex arguments cultural change measure textual similarity distance really correspond evidence cultural proximity differentiation explore question empirically paper compare textual social measure similarities genres english language fiction exist measure textual similarity cosine similarity tf idf vectors topic vectors also compare new strategies use supervise learn anchor textual measurement social context
large size model implement recently asr system deal complex speech recognition problems num ber parameters model make hard deploy especially resource short devices car tablet besides time asr system use deal real time problem keyword spot kws contradictory fact large model require long com putation time deal problem apply sparse algo rithms reduce number parameters widely use model deep neural network dnn kws require real short computation time prune ninety even ninety-five parameters model tiny effect decline sparse model perform better baseline model order number parameters besides sparse algorithm lead us find rational model size au tomatically certain problem without concern choose original model size
manually label corpora expensive create often available low resource languages domains automatic label approach alternative way obtain label data quicker cheaper way however label often contain errors deteriorate classifier performance train data propose noise layer add neural network architecture allow model noise train combination clean noisy data show low resource ner task improve performance thirty-five use additional noisy data handle noise
fundamental frequency f0 represent pitch speech determine prosodic characteristics speech need various task speech analysis synthesis despite decades research topic f0 estimation low signal noise ratios snrs unexpected noise condition remain difficult work propose new approach noise robust f0 estimation use recurrent neural network rnn train supervise manner recent study employ deep neural network dnns f0 track frame frame classification task quantise frequency state propose waveform sinusoid regression instead achieve noise robustness accurate estimation increase frequency resolution experimental result ptdb tug corpus contaminate additive noise noisex ninety-two demonstrate propose method improve gross pitch error gpe rate fine pitch error fpe thirty-five snrs ten db ten db compare well know noise robust f0 tracker pefac furthermore propose method also outperform state art dnn base approach fifteen term fpe gpe rate precede snr range
paper explore ways improve pos tag use various type auxiliary losses different word representations baseline utilize bilstm tagger able achieve state art result sequence label task develop new method character level word representation use feedforward neural network representation give us better result term speed performance model also apply novel technique pretraining word representations exist word vectors finally design new variant auxiliary loss sequence label task additional prediction neighbour label loss force model learn dependencies side sequence label accelerate process train test methods english russian languages
work present simple grapheme base system low resource speech recognition use babel data turkish spontaneous speech eighty hours investigate different neural network architectures performance include fully convolutional recurrent resnet gru different feature normalization techniques compare well also propose ctc loss modification use segmentation train lead improvement decode small beam size best model achieve word error rate four hundred and fifty-eight best report result end end systems use domain data task accord knowledge
company look provide delightful user experience paramount importance take care customer issue paper propose cota system improve speed reliability customer support end users automate ticket classification answer selection support representatives two machine learn natural language process techniques demonstrate one rely feature engineer cota v1 exploit raw signal deep learn architectures cota v2 cota v1 employ new approach convert multi classification task rank problem demonstrate significantly better performance case thousands class cota v2 propose encoder combiner decoder novel deep learn architecture allow heterogeneous input output feature type injection prior knowledge network architecture choices paper compare model variants task ticket classification answer selection show model cota v2 outperform cota v1 analyze inner work shortcomings finally b test conduct production set validate real world impact cota reduce issue resolution time ten percent without reduce customer satisfaction
natural language process make significant inroads learn semantics word distributional approach however representations learn via methods fail capture certain kinds information implicit real world particular spatial relations encode way inconsistent human spatial reason lack invariance viewpoint change present system capable capture semantics spatial relations behind leave etc natural language key contributions novel multi modal objective base generate image scenes textual descriptions new dataset train demonstrate internal representations robust mean preserve transformations descriptions paraphrase invariance viewpoint invariance emergent property system
multilingual speech recognition one costly ai problems language seven thousand even different accent require acoustic model obtain best recognition performance even though use phoneme symbols language accent impose color twang many adaptive approach propose require train additional data generally inferior monolingually train model paper propose different approach use large multilingual model emphmodulated cod generate ancillary network learn code useful differences twang human language use meta pi network one network language code net gate activity neurons another acoustic model net result show recognition multilingual meta pi network quickly adapt proper language color without retrain new data perform better monolingually train network model evaluate train acoustic model net modulate language code net jointly optimize best recognition performance
early years text classification typically accomplish feature base machine learn model recently deep neural network powerful learn machine make possible work raw input text stand however exit end end neural network lack explicit interpretation prediction paper propose novel framework jumper inspire cognitive process text read model text classification sequential decision process basically jumper neural system scan piece text sequentially make classification decisions time wish classification result make classification part decision process control policy network train reinforcement learn experimental result show properly train jumper follow properties one make decisions whenever evidence enough therefore reduce total text read thirty forty often find key rationale prediction two achieve classification accuracy better comparable state art model several benchmark industrial datasets
present memory augment policy optimization mapo simple novel way leverage memory buffer promise trajectories reduce variance policy gradient estimate mapo applicable deterministic environments discrete action structure prediction combinatorial optimization task express expect return objective weight sum two term expectation high reward trajectories inside memory buffer separate expectation trajectories outside buffer make efficient algorithm mapo propose one memory weight clip accelerate stabilize train two systematic exploration discover high reward trajectories three distribute sample inside outside memory buffer scale train mapo improve sample efficiency robustness policy gradient especially task sparse reward evaluate mapo weakly supervise program synthesis natural language semantic parse wikitablequestions benchmark improve state art twenty-six achieve accuracy four hundred and sixty-three wikisql benchmark mapo achieve accuracy seven hundred and forty-nine weak supervision outperform several strong baselines full supervision source code available https githubcom crazydonkey200 neural symbolic machine
neural machine translation nmt widely adopt recently due advantage compare traditional statistical machine translation smt however nmt system still often produce translation failures due complexity natural language sophistication design neural network house black box system test base reference translations ie examples valid translations common practice nmt quality assurance increasingly critical industrial practice name vivo test expose unseen type instance translation failures real users use deploy industrial nmt system fill gap lack test oracle vivo test nmt system paper propose new approach automatically identify translation failures without require reference translations translation task approach directly serve test oracle vivo test approach focus properties natural language translation check systematically use information test input ie texts translate test output ie translations inspection nmt system evaluation conduct real world datasets show approach effectively detect target property violations translation failures experience deploy approach production development environments wechat messenger app one billion monthly active users demonstrate high effectiveness approach along high industry impact
domestic violence dv consider big social issue exist strong relationship dv health impact public exist research study focus social media track analyse real world events like emerge trend natural disasters user sentiment analysis political opinions health care however less attention give social welfare issue like dv impact public health recently victims dv turn social media platforms express feel form post seek social emotional support sympathetic encouragement show compassion empathy among public difficult mine actionable knowledge large conversational datasets social media due characteristics high dimension short noisy huge volume high velocity hence paper propose novel framework model discover various theme relate dv public domain propose framework would possibly provide unprecedentedly valuable information public health researchers national family health organizations government public data enrichment consolidation improve social welfare community thus provide actionable knowledge monitor analyse continuous rich user generate content
web load textual content natural language process standout amongst vital field machine learn data huge simple machine learn algorithms able handle deep learn come play base neural network however since neural network process raw text change diverse strategies word embed paper demonstrate distinctive word embed strategies implement amazon review dataset two sentiments classify happy unhappy base numerous customer review moreover demonstrate distinction accuracy discourse word embed apply
goal industrial ticket system retrieve relevant solution input query match historical ticket store knowledge base query comprise subject description historical ticket consist subject description solution retrieve relevant solution use textual similarity paradigm learn similarity query historical ticket task challenge due significant term mismatch query ticket pair asymmetric lengths subject short text description solution multi sentence texts present novel replicate siamese lstm model learn similarity asymmetric text pair give twenty-two seven gain accuracy10 retrieval task respectively unsupervised supervise baselines also show topic distribute semantic feature short long texts improve similarity learn retrieval
paper describe apply self attention relative positional encode task relation extraction propose use self attention encoder layer together additional position aware attention layer take account position query object sentence self attention encoder also use custom implementation relative positional encode allow word sentence take account leave right context evaluation model do tacred dataset propose model rely attention recurrent convolutional layer use improve performance wrt previous state art
introduce talk walk first large scale dialogue dataset ground action perception task involve two agents guide tourist communicate via natural language order achieve common goal tourist navigate give target location task dataset describe detail challenge full solution open problem pose community focus task tourist localization develop novel mask attention spatial convolutions masc mechanism allow ground tourist utterances guide map ii show yield significant improvements emergent natural language communication iii use method establish non trivial baselines full task
recurrent neural network dominant model many speech language process task however understand little behavior class function recurrent network realize moreover heuristics use train complicate analyse paper study recurrent network ability learn long term dependency context speech recognition consider two decode approach online batch decode show class function decode approach correspond draw connection batch decode popular train approach recurrent network truncate backpropagation time change decode approach restrict amount past history recurrent network use prediction allow us analyze ability remember empirically utilize long term dependency subphonetic state phonemes word show design decisions decode approach lookahead context frame consecutive prediction characterize behavior recurrent network finally draw connection markov process vanish gradients result implications study long term dependency speech data properties learn recurrent network
neural attention become central many state art model natural language process relate domains attention network easy train effective method softly simulate alignment however approach marginalize latent alignments probabilistic sense property make difficult compare attention alignment approach compose probabilistic model perform posterior inference condition observe data relate latent approach hard attention fix issue generally harder train less accurate work consider variational attention network alternatives soft hard attention learn latent variable alignment model tighter approximation bound base amortize variational inference propose methods reduce variance gradients make approach computationally feasible experiment show machine translation visual question answer inefficient exact latent variable model outperform standard neural attention gain go away use hard attention base train hand variational attention retain performance gain train speed comparable neural attention
recurrent neural network rnns sequentially process data update state new data point long de facto choice sequence model task however inherently sequential computation make slow train fee forward convolutional architectures recently show achieve superior result sequence model task machine translation add advantage concurrently process input sequence lead easy parallelization faster train time despite successes however popular fee forward sequence model like transformer fail generalize many simple task recurrent model handle ease eg copy string even simple logical inference string formula lengths exceed observe train time propose universal transformer ut parallel time self attentive recurrent sequence model cast generalization transformer model address issue uts combine parallelizability global receptive field fee forward sequence model like transformer recurrent inductive bias rnns also add dynamic per position halt mechanism find improve accuracy several task contrast standard transformer certain assumptions uts show turing complete experiment show uts outperform standard transformers wide range algorithmic language understand task include challenge lambada language model task uts achieve new state art machine translation uts achieve nine bleu improvement transformers wmt14 en de dataset
multimodal machine learn core research area span language visual acoustic modalities central challenge multimodal learn involve learn representations process relate information multiple modalities paper propose two methods unsupervised learn joint multimodal representations use sequence sequence seq2seq methods textitseq2seq modality translation model textithierarchical seq2seq modality translation model also explore multiple different variations multimodal input output seq2seq model experiment multimodal sentiment analysis use cmu mosi dataset indicate methods learn informative multimodal representations outperform baselines achieve improve performance multimodal sentiment analysis specifically bimodal case model able improve f1 score twelve point also discuss future directions multimodal seq2seq methods
scarcity label data one frequent problems face machine learn particularly true relation extraction text mine large corpora texts exist many application domains label text data require expert invest much time read document overall state art model like convolutional neural network use paper achieve great result train large enough amount label data however practical point view question arise whether efficient approach one take manual effort expert account paper report alternative approach first construct relation extraction model use distant supervision later make use domain expert refine result distant supervision provide mean label data give know relations knowledge base suffer noisy label introduce active learn base extension allow neural network incorporate expert feedback report first result complex data set
text classification problem overfitting arise due high dimensionality make regularization essential although classic regularizers provide sparsity fail return highly accurate model contrary state art group lasso regularizers provide better result expense low sparsity paper apply greedy variable selection algorithm call orthogonal match pursuit text classification task also extend standard group omp introduce overlap group omp handle overlap group feature empirical analysis verify omp overlap gomp constitute powerful regularizers able produce effective sparse model code data available online https githubcom y3nk0 omp text classification
variational autoencoders learn distributions high dimensional data model data deep latent variable model fit model maximize lower bind log marginal likelihood vaes capture complex distributions also suffer issue know latent variable collapse especially likelihood model powerful specifically lower bind involve approximate posterior latent variables posterior collapse set equal prior ie approximate posterior independent data vaes learn good generative model latent variable collapse prevent learn useful representations paper propose simple new way avoid latent variable collapse include skip connections generative model connections enforce strong link latent variables likelihood function study generative skip model theoretically empirically theoretically prove skip model increase mutual information observations infer latent variables empirically study image mnist omniglot text yahoo compare exist vae architectures show generative skip model maintain similar predictive performance lead less collapse provide meaningful representations data
introduce new entity type task give sentence entity mention goal predict set free form phrase eg skyscraper songwriter criminal describe appropriate type target entity formulation allow us use new type distant supervision large scale head word indicate type noun phrase appear show ultra fine type crowd source introduce new evaluation set much diverse fine grain exist benchmarks present model predict open type train use multitask objective pool new head word supervision prior supervision entity link experimental result demonstrate model effective predict entity type vary granularity achieve state art performance exist fine grain entity type benchmark set baselines newly introduce datasets data model download http nlpcswashingtonedu entitytype
paper present end end automatic speech recognition system successfully employ subword units hybrid ctc attention base system subword units obtain byte pair encode bpe compression algorithm compare use word model units use character subword units suffer vocabulary oov problem furthermore use subword units offer capability model longer context use character evaluate different systems librispeech 1000h dataset subword base hybrid ctc attention system obtain sixty-eight word error rate wer testclean subset without dictionary external language model represent significant improvement one hundred and twenty-eight wer relative reduction character base hybrid ctc attention system
implicit discourse relation recognition challenge task relation prediction without explicit connectives discourse parse need understand text span easily derive surface feature input sentence pair thus properly represent text crucial task paper propose model augment different grain text representations include character subword word sentence sentence pair level propose deeper model evaluate benchmark treebank achieve state art accuracy greater forty-eight eleven way f1 score greater fifty four way classifications first time accord best knowledge
chinese word segmentation cws important task chinese nlp recently many neural network base methods propose cws however methods require large number label sentence model train usually utilize useful information chinese dictionary paper propose two methods exploit dictionary information cws first one base pseudo label data generation second one base multi task learn experimental result two benchmark datasets validate approach effectively improve performance chinese word segmentation especially train data insufficient
paper time delay neural network tdnn base acoustic model propose implement fast converge acoustic model korean speech recognition tdnn advantage fast convergence amount train data limit due subsampling exclude duplicate weight tdnn show absolute improvement two hundred and twelve term character error rate compare fee forward neural network ffnn base model korean speech corpora propose model converge one hundred and sixty-seven time faster ffnn base model
measure similarities string central many establish fast grow research areas include information retrieval biology natural language process traditional approach string similarity measurements define metric word space quantify sum differences character two string state art area surprisingly evolve much last decades majority metrics base simple comparison character character distributions without consideration context word paper propose string metric encompass similarities string base one character similarities word include non standard standard spell word two context word proposal neural network compose denoising autoencoder call context encoder specifically design find similarities word base context experimental result show result metrics succeed eight hundred and fifty-four case find correct version non standard spell among closest word compare six hundred and thirty-two establish normalise levenshtein distance besides show word use similar context approach calculate similar word different contexts desirable property miss establish string metrics
recently recurrent neural network become state art acoustic model automatic speech recognition long short term memory lstm units popular ones however alternative units like gate recurrent unit gru modifications outperform lstm publications paper compare five neural network nn architectures various adaptation feature normalization techniques evaluate feature space maximum likelihood linear regression five variants vector adaptation two variants cepstral mean normalization adaptation normalization techniques develop fee forward nns accord result paper work also rnns experiment choose well know available timit phone recognition task phone recognition much sensitive quality large vocabulary task complex language model also publish open source script easily replicate result help continue development
despite rapid advance speech recognition current model remain brittle superficial perturbations input small amount noise destroy performance otherwise state art model harden model background noise practitioners often perform data augmentation add artificially noise examples train set carry original label paper hypothesize clean example superficially perturb counterparts merely map class map representation propose invariant representation learn irl train iteration train examplewe sample noisy counterpart apply penalty term coerce match representations layer choose layer key result demonstrate librispeech dataset follow irl significantly reduce character error rat cer clean thirty-three vs sixty-five one hundred and ten vs one hundred and eighty-one test set ii several domain noise settings different see train irl benefit even pronounce careful ablations confirm result simply due shrink activations choose layer
paper propose forward attention method sequenceto sequence acoustic model speech synthesis method motivate nature monotonic alignment phone sequence acoustic sequence alignment paths satisfy monotonic condition take consideration decoder timestep modify attention probabilities timestep compute recursively use forward algorithm transition agent forward attention propose help attention mechanism make decisions whether move forward stay decoder timestep experimental result show propose forward attention method achieve faster convergence speed higher stability baseline attention method besides method forward attention transition agent also help improve naturalness synthetic speech control speed synthetic speech effectively
important task recommender system provide interpretable explanations user important credibility system current interpretable recommender systems tend focus certain feature know important user offer explanations structure form well know user generate review feedback reviewers strong leverage users decisions hand recent text generation work show generate text similar quality human write text aim show generate text successfully use explain recommendations paper propose framework consist popular review orient generation model aim create personalise explanations recommendations interpretations generate character word level build dataset contain reviewers feedback amazon book review dataset cross domain experiment design bridge natural language process recommender system domain besides language model evaluation methods employ deepconn novel review orient recommender system use deep neural network evaluate recommendation performance generate review root mean square error rmse demonstrate synthetic personalise review better recommendation performance human write review knowledge present first machine generate natural language explanations rat prediction
paper address problem stylize text generation multilingual setup version language model base long short term memory lstm artificial neural network extend phonetic semantic embeddings use stylize poetry generation quality result poems generate network estimate bilingual evaluation understudy bleu survey new cross entropy base metric suggest problems type experiment show propose model consistently outperform random sample vanilla lstm baselines humans also tend associate machine generate texts target author
propose network independent hand hold system translate disambiguate foreign restaurant menu items real time system base use portable multimedia device smartphones pda accurate fast translation obtain use machine translation engine context specific corpora apply two pre process step call translation standardization n gram consolidation phrase table generate order magnitude lighter ones commonly use market applications thus make translations computationally less expensive decrease battery usage translation ambiguities mitigate use multimedia information include image dish ingredients along ingredient list implement prototype system ipod touch second generation english speakers travel spain test indicate translation method yield higher accuracy translation engines google translate almost instantaneously memory requirements application include database image also well within limit device combine database nutritional information propose system use help individuals follow medical diet maintain diet travel
study problem learn similarity function large corpora use neural network embed model model typically train use sgd sample random observe unobserved pair number sample grow quadratically corpus size make expensive scale large corpora propose new efficient methods train model without sample unobserved pair inspire matrix factorization approach rely add global quadratic penalty pair examples express term matrix inner product two generalize gramians show gradient term efficiently compute maintain estimate gramians develop variance reduction scheme improve quality estimate conduct large scale experiment show significant improvement train time generalization quality compare traditional sample methods
study open domain dialogue generation dialogue act design explain people engage social chat imitate human behavior propose manage flow human machine interactions dialogue act policies policies response generation jointly learn human human conversations former optimize reinforcement learn approach dialogue act achieve significant improvement state art methods response quality give contexts dialogue length machine machine simulation human machine conversation
systematic compositionality ability recombine meaningful units regular predictable outcomes see key humans capacity generalization language recent work study systematic compositionality modern seq2seq model use generalization novel navigation instructions ground environment probe tool require model quickly bootstrap mean new word extend framework settings model need recombine well train functional word around right novel contexts find confirm strengthen earlier ones seq2seq model impressively good generalize novel combinations previously see input receive extensive train specific pattern generalize eg generalize many examples x around right jump around right fail generalization require novel application compositional rule eg infer mean around right right around
employers actively look talents specific hard skills also various soft skills analyze soft skill demand job market important able detect soft skill phrase job advertisements automatically however naive match soft skill phrase lead false positive match soft skill phrase friendly use describe company team another entity rather desire candidate paper propose phrase match base approach differentiate soft skill phrase refer candidate vs something else disambiguation formulate binary text classification problem prediction make potential soft skill base context occur inform model soft skill prediction make develop several approach include soft skill mask soft skill tag compare several neural network base approach include cnn lstm hierarchical attention model propose tag base input representation use lstm achieve highest recall eight thousand, three hundred and ninety-two job dataset fix precision ninety-five
social media increasingly use humans express feel opinions form short text message detect sentiments text wide range applications include identify anxiety depression individuals measure well mood community sentiments express many ways see facial expression gesture speech write text sentiment analysis text document essentially content base classification problem involve concepts domains natural language process well machine learn paper sentiment recognition base textual data techniques use sentiment analysis discuss
name entities ne object refer name people organizations locations name entities keywords important mean document propose generalize vector space model combine name entities keywords model take account different ontological feature name entities namely aliases class identifiers moreover use entity class represent latent information interrogative word wh query ignore traditional keyword base search implement test propose model trec dataset present discuss paper
offline handwritten text recognition image important problem enterprises attempt digitize large volumes handmarked scan document report deep recurrent model multi dimensional lstms show yield superior performance traditional hide markov model base approach suffer markov assumption therefore lack representational power rnns paper introduce novel approach combine deep convolutional network recurrent encoder decoder network map image sequence character correspond text present image entire model train end end use focal loss improvement standard cross entropy loss address class imbalance problem inherent text recognition enhance decode capacity model beam search algorithm employ search best sequence set hypotheses base joint distribution individual character model take input downsampled version original image thereby make computationally memory efficient experimental result benchmarked two publicly available datasets iam rim surpass state art word level accuracy evaluation set datasets thirty-five eleven respectively
people live cities witness decline exposure nature grow body research demonstrate association nature contact improve mood use twitter hedonometer world analysis tool investigate sentiment estimate happiness word people write vary visit san francisco urban park system find sentiment substantially higher park visit remain elevate several hours follow visit leverage differences vegetative cover across park type explore different type outdoor public space may contribute subjective well tweet visit regional park greener greater vegetative cover exhibit larger increase sentiment tweet visit civic plazas square finally analyze word frequencies explore several mechanisms theorize link nature exposure mental cognitive benefit negation word decrease frequency visit urban park result use urban planners public health officials better target nature contact recommendations grow urban populations
paper identify stylistic differences instruction give observe corpus human robot dialogue differences verbosity structure ie single intent vs multi intent instructions arise naturally without restrictions prior guidance users speak robot different style find produce different rat miscommunication correlations find style differences individual user variation trust interaction experience robot understand potential consequences factor influence style inform design dialogue systems robust natural variation human users
word embed word2vec successful offer semantics text word learn context word audio word2vec show offer phonetic structure speak word signal segment word learn signal within speak word paper propose two stage framework perform phonetic semantic embed speak word consider context speak word stage one perform phonetic embed speaker characteristics disentangle stage two perform semantic embed addition propose evaluate phonetic semantic nature audio embeddings obtain stage two parallelize text embeddings general phonetic structure semantics inevitably disturb example word brother sister close semantics different phonetic structure word brother bother way around phonetic semantic embed attractive show initial experiment speak document retrieval speak document include speak query retrieve base phonetic structure speak document semantically relate query include query also retrieve base semantics
sequence sequence model neural network module map two sequence different lengths sequence sequence model three core modules encoder decoder attention attention bridge connect encoder decoder modules improve model performance many task paper propose two ideas improve sequence sequence model performance enhance attention module first maintain history location expect context several previous time step second apply multiscale convolution several previous attention vectors current decoder state utilize propose framework sequence sequence speech recognition text speech systems result reveal propose extension could improve performance significantly compare standard attention baseline
knowledge graph emerge important model study complex multi relational data give rise construction numerous large scale incomplete knowledge graph encode information extract various resources effective scalable approach jointly learn multiple graph eventually construct unify graph crucial next step success knowledge base inference many downstream applications end propose linknbed deep relational learn framework learn entity relationship representations across multiple graph identify entity linkage across graph vital component achieve goal design novel objective leverage entity linkage build efficient multi task train procedure experiment link prediction entity linkage demonstrate substantial improvements state art relational learn approach
gang violence severe issue major cities across yous recent study patton et al two thousand and seventeen find evidence social media communications link violence communities high rat exposure gang activity paper partner computer scientists social work researchers domain expertise gang violence analyze public tweet image post youth mention gang associations twitter leverage automatically detect psychosocial factor condition could potentially assist social workers violence outreach workers prevention early intervention program end develop rigorous methodology collect annotate tweet gather one thousand, eight hundred and fifty-one tweet accompany annotations relate visual concepts psychosocial cod aggression loss substance use cod relevant social work interventions represent possible pathways violence social media compare various methods classify tweet three class use text tweet image tweet modalities input classifier particular analyze usefulness mid level visual concepts role different modalities tweet classification task experiment show individually text information dominate classification performance loss class image information dominate aggression substance use class multimodal approach provide promise improvement eighteen relative mean average precision best single modality approach finally also illustrate complexity understand social media data elaborate open challenge
exist knowledge graph kgs academic domains suffer problems insufficient multi relational information name ambiguity improper data format large scale machine process paper present acekg new large scale kg academic domain acekg provide clean academic information also offer large scale benchmark dataset researchers conduct challenge data mine project include link prediction community detection scholar classification specifically acekg describe three hundred and thirteen billion triple academic facts base consistent ontology include necessary properties paper author field study venues institute well relations among enrich propose knowledge graph also perform entity alignment exist databases rule base inference base acekg conduct experiment three typical academic data mine task evaluate several state art knowledge embed network representation learn approach benchmark datasets build acekg finally discuss several promise research directions benefit acekg
extract textual feature tweet challenge process due noisy nature content weak signal word use paper propose use singular value decomposition svd cluster enhance signal textual feature tweet improve correlation events propose technique apply svd time series vector feature factorize matrix feature day count order ensure independence feature vectors afterwards k mean cluster apply build look table map members cluster cluster centroid lookup table use map feature original data centroid cluster calculate sum term frequency vectors feature cluster term frequency vector cluster centroid test technique calculate correlations cluster centroids golden standard record gsr vector sum vectors cluster members centroid vector propose method apply multiple correlation techniques include pearson spearman distance correlation kendal tao experiment also consider different word form lengths feature include keywords n grams skip grams bag word correlation result enhance significantly highest correlation score increase three six average correlation score increase three four
acoustic word recognition provide straightforward solution end end speech recognition without need external decode language model score lexicon character base model offer natural solution vocabulary problem word model simpler decode may also able directly recognize semantically meaningful units present effective methods train sequence sequence model direct word level recognition character level recognition show absolute improvement forty-four fifty word error rate switchboard corpus compare prior work addition promise result word base model interpretable character model compose word use separate decode step analyze encoder hide state attention behavior show location aware attention naturally represent word single speech word vector despite span multiple frame input finally show acoustic word model also learn segment speech word mean standard deviation three frame compare human annotate force alignments switchboard corpus
recently talmor berant two thousand and eighteen introduce complexwebquestions dataset focus answer complex question decompose sequence simpler question extract answer retrieve web snippets work author use pre train read comprehension rc model salant berant two thousand and eighteen extract answer web snippets short note show train rc model directly train data complexwebquestions reveal leakage train set test set allow obtain unreasonably high performance solution construct new partition complexwebquestions suffer leakage publicly release also perform empirical evaluation two datasets show train rc model train data substantially improve state art performance
different language pair word level neural machine translation nmt model fix size vocabulary suffer problem represent vocabulary oov word common practice usually replace rare unknown word token limit translation performance extent recent work handle problem split word character specially extract subword units enable open vocabulary translation byte pair encode bpe one successful attempt show extremely competitive provide effective subword segmentation nmt systems paper extend bpe style segmentation general unsupervised framework three statistical measure frequency frq accessor variety av description length gain dlg test approach two translation task german english chinese english experimental result show av dlg enhance systems outperform frq baseline frequency weight scheme different significant level
understand large structure document like scholarly article request proposals business report complex difficult task involve discover document overall purpose subject understand function mean section subsections extract low level entities facts research present deep learn base document ontology capture general purpose semantic structure domain specific semantic concepts large number academic article business document ontology able describe different functional part document use enhance semantic index better understand human be machine evaluate model extensive experiment datasets scholarly article arxiv request proposal document
study improve performance neural name entity recognition margin eleven f score example low resource language like german thereby outperform exist baselines establish new state art single open source dataset rather design deeper wider hybrid neural architectures gather available resources perform detail optimization grammar dependent morphological process consist lemmatization part speech tag prior expose raw data train process test approach threefold monolingual experimental setup single b joint c optimize train would light dependency downstream task size corpora use compute word embeddings
attention base recurrent neural encoder decoder model present elegant solution automatic speech recognition problem approach fold acoustic model pronunciation model language model single network require parallel corpus speech text train however unlike conventional approach combine separate acoustic language model clear use additional unpaired text previous work methods address problem thorough comparison among methods still lack paper compare suite past methods propose methods use unpaired text data improve encoder decoder model evaluation use medium size switchboard data set large scale google voice search dictation data set result confirm benefit use unpaired text across range methods data set surprisingly first pass decode rather simple approach shallow fusion perform best across data set however google data set find cold fusion lower oracle error rate outperform approach second pass rescoring google voice search data set
develop practical speech recognizer low resource language challenge potentially unknown properties language also test data may domain available train data paper focus latter challenge ie domain mismatch systems train use sequence base criterion demonstrate effectiveness use pre train english recognizer robust mismatch condition domain normalize feature extractor low resource language example use turkish conversational speech broadcast news data enable rapid development speech recognizers new languages easily adapt domain test various cross domain scenarios achieve relative improvements around twenty-five phoneme error rate improvements around fifty domains
nowadays lot people use social media opinions make decision buy products service opinion spam detection hard problem fake review make organizations well individuals different purpose write fake review mislead readers automate detection system promote demote target products promote damage reputations paper pro pose new approach use knowledge base ontology detect opinion spam high accuracy higher seventy-five keywords opinion spam fake review e commercial ontology
proposal introduce dialogue challenge build end end task completion dialogue systems goal encourage dialogue research community collaborate benchmark standard datasets unify experimental environment special session release human annotate conversational data three domains movie ticket book restaurant reservation taxi book well experiment platform build simulators domain train evaluation purpose final submit systems evaluate simulate set human judge
paper investigate use adversarial learn unsupervised adaptation unseen record condition specifically single microphone far field speech adapt neural network base acoustic model train close talk clean speech new record condition use untranscribed adaptation data experimental result italian speecon data set show propose method achieve one hundred and ninety-eight relative word error rate wer reduction compare unadapted model furthermore adaptation method beneficial even perform data another language ie french give one hundred and twenty-six relative wer reduction
present new recurrent neural network topology enhance state art machine learn systems incorporate broader context approach overcome recent limitations extend narratives multi layer computational approach generate abstract context representation therefore develop system capture narrative word level sentence level context level hierarchical set propose model summarize salient information level create abstract representation extend context subsequently use representation enhance neural language process systems task semantic error detection show potential newly introduce topology compare approach context agnostic set include standard neural language model supervise binary classification network performance measure error detection task show advantage hierarchical context aware topologies improve baseline one thousand, two hundred and seventy-five relative unsupervised model two thousand and thirty-seven relative supervise model
recent neural network wavenet samplernn learn directly speech waveform sample achieve high quality synthetic speech term naturalness speaker similarity even multi speaker text speech synthesis systems neural network use alternative vocoders hence often call neural vocoders neural vocoder use acoustic feature local condition parameters parameters need accurately predict another acoustic model however yet clear train acoustic model problematic final quality synthetic speech significantly affect performance acoustic model significant degradation happen especially predict acoustic feature mismatch characteristics compare natural ones order reduce mismatch characteristics natural generate acoustic feature propose frameworks incorporate either conditional generative adversarial network gin variant wasserstein gin gradient penalty wgan gp multi speaker speech synthesis use wavenet vocoder also extend gin frameworks use discretized mixture logistic loss well train wavenet addition mean square error adversarial losses part objective function experimental result show acoustic model train use wgan gp framework use back propagate discretized mixture logistics dml loss achieve highest subjective evaluation score term quality speaker similarity
nowadays editors tend separate different subtopics long wiki pedia article multiple sub article separation seek improve human readability however also deleterious effect many wikipedia base task rely article concept assumption require entity concept describe solely one article underlie assumption significantly simplify knowledge representation extraction vital many exist technologies automate knowledge base construction cross lingual knowledge alignment semantic search data lineage wikipedia entities paper provide approach match scatter sub article back correspond main article intent facilitate automate wikipedia curation process propose model adopt hierarchical learn structure combine multiple variants neural document pair encoders comprehensive set explicit feature large crowdsourced dataset create support evaluation feature extraction task base large dataset propose model achieve promise result cross validation significantly outperform previous approach large scale serve entire english wikipedia also prove practicability scalability propose model effectively extract vast collection newly pair main sub article
speak language convenient interface command mobile robot yet work number base term must ground perceptual motor skills detail language process use robot eli explain ground perform interact user gesture handle phenomena anaphora importantly however certain concepts robot preprogrammed name various object household nature specific task may request perform case vital exist method extend ground essentially learn tell describe successfully implement learn new nouns verbs tabletop set create language learn kernel may last explicit program robot ever need core mechanism could eventually use impart vast amount knowledge much child learn parent teachers
measure domain relevance data identify select well fit domain data machine translation mt well study topic denoising yet denoising concern different type data quality try reduce negative impact data noise mt train particular neural mt nmt train paper generalize methods measure select data domain mt apply denoising nmt train propose approach use trust data denoising curriculum realize online data selection intrinsic extrinsic evaluations approach show significant effectiveness nmt train data severe noise
structure scene descriptions image useful automatic process query large image databases show combination semantic visual statistical model improve task map image associate scene description paper consider scene descriptions represent set triple subject predicate object triple consist pair visual object appear image relationship eg man rid elephant man wear hat combine standard visual model object detection base convolutional neural network latent variable model link prediction apply multiple state art link prediction methods compare capability visual relationship detection one main advantage link prediction methods also generalize triple never observe train data experimental result recently publish stanford visual relationship dataset challenge real world dataset show integration semantic model use link prediction methods significantly improve result visual relationship detection combine approach achieve superior performance compare state art method stanford computer vision group
inspire previous attempt answer crossword question use neural network hill cho korhonen bengio two thousand and fifteen dissertation implement extensions improve performance exist definition model task answer crossword question discussion evaluation original implementation find ways recurrent neural model could extend insights relate field neural language model neural machine translation provide justification mean require extensions two extensions apply lstm encoder first take average lstm state across sequence secondly use bidirectional lstm implementations serve improve model performance definitions crossword test set order improve performance crossword question train data increase include crossword question answer serve improve result definitions well crossword question final experiment conduct use sub word unit segmentation first source side later preliminary experimentation conduct facilitate character level output initially exact reproduction baseline result prove unsuccessful despite extensions improve performance allow definition model surpass performance recurrent neural network variants previous work hill et al two thousand and fifteen
address problem construct knowledge base entity orient search intents search intents define level entity type comprise high level intent category property website service along cluster query term use express intent machine readable statements leverage various applications eg generate entity card query recommendations structure service orient search intents take one step towards make entities actionable main contribution paper pipeline components develop construct knowledge base entity intents evaluate performance component wise end end demonstrate approach able generate high quality data
coherence text important attribute measure manually automatically generate discourse well define quantitative metrics still elusive paper present metric score topical coherence input paragraph real value scale analyze underlie topical structure first extract possible topics sentence paragraph text relate coherence text measure compute degree uncertainty topics respect paragraph b relatedness topics components modular framework rely unlabeled data wordnet thus make completely unsupervised important feature general purpose usage metric experiment conduct two datasets publicly available dataset essay grade represent human discourse synthetic dataset construct mix content multiple paragraph cover diverse topics evaluation show measure coherence score positively correlate grind truth datasets validation coherence score provide conduct human evaluation synthetic data show significant agreement seven hundred and ninety-three
introduce new measure unsupervised hypernym detection directionality motivation keep measure computationally light portatable across languages show relative physical location word explanatory article capture directionality property phrase section title article word capture semantic similarity need hypernym detection task experimentally show combination feature come two simple measure suffice produce result comparable best unsupervised measure term average precision
growth internet number fake news online proliferate every year consequences phenomena manifold range lousy decision make process bully violence episodes therefore fact check algorithms become valuable asset aim important step detect fake news access credibility score give information source however widely use web indicators either shut public eg google pagerank free use alexa rank exist databases short manually curated list online source scale finally research topic theoretical base explore confidential data restrict simulation environment paper explore current research highlight challenge propose solutions tackle problem classify websites credibility scale propose model automatically extract source reputation cue compute credibility factor provide valuable insights help belittle dubious confirm trustful unknown websites experimental result outperform state art two class five class set
paper describe defactonlp system design fever two thousand and eighteen share task aim task conceive system automatically assess veracity claim also retrieve evidence support assessment wikipedia approach wikipedia document whose term frequency inverse document frequency tfidf vectors similar vector claim document whose name similar name entities nes mention claim identify document might contain evidence sentence document supply textual entailment recognition module module calculate probability sentence support claim contradict claim provide relevant information assess veracity claim various feature compute use probabilities finally use random forest classifier determine overall truthfulness claim sentence support classification return evidence approach achieve four thousand, two hundred and seventy-seven evidence f1 score five thousand, one hundred and thirty-six label accuracy three thousand, eight hundred and thirty-three fever score
deep nlp model benefit underlie structure data eg parse tree typically extract use shelf parsers recent attempt jointly learn latent structure encounter tradeoff either make factorization assumptions limit expressiveness sacrifice end end differentiability use recently propose sparsemap inference retrieve sparse distribution latent structure propose novel approach end end learn latent structure predictors jointly downstream predictor best knowledge method first enable unrestricted dynamic computation graph construction global latent structure maintain differentiability
paper explore relational syllogistic logics family logical systems relate reason relations extensions classical syllogistic decidable logical systems prove completeness theorems complexity result natural subfamily relational syllogistic logics parametrized constructors term sentence
increase digitization political speech open door study new dimension political behavior use text analysis work investigate value word level statistical data us congressional record contain full text speeches make us congress study ideological position behavior senators apply machine learn techniques use data automatically classify senators accord party obtain accuracy seventy ninety-five range depend specific method use also show use text predict dw nominate score common proxy ideology improve upon already successful result classification deteriorate apply text sessions congress four years remove train set point need part voters dynamically update heuristics use evaluate party base political speech text base predictions less accurate base vote behavior support theory roll call vote represent greater commitment part politicians thus accurate reflection ideological preferences however overall success machine learn approach study demonstrate political speeches highly predictive partisan affiliation addition find work also introduce computational tool methods relevant use political speech data
introduce texar open source toolkit aim support broad set text generation task transform input natural language machine translation summarization dialog content manipulation forth design goals modularity versatility extensibility mind texar extract common pattern underlie diverse task methodologies create library highly reusable modules allow arbitrary model architectures algorithmic paradigms texar model architecture inference learn process properly decompose modules high concept level freely assemble plug swap toolkit also support rich set large scale pretrained model texar thus particularly suitable researchers practitioners fast prototyping experimentation versatile toolkit also foster technique share across different text generation task texar support tensorflow pytorch release apache license twenty https wwwtexario
recursive neural network widely use researchers handle applications recursively hierarchically structure data however embed control flow deep learn frameworks tensorflow theano caffe2 mxnet fail efficiently represent execute neural network due lack support recursion paper add recursion program model exist frameworks complement design recursive execution dataflow graph well additional apis recursive definitions unlike iterative implementations understand topological index node recursive data structure recursive implementation able exploit recursive relationships nod efficient execution base parallel computation present implementation tensorflow evaluation result various recursive neural network model show recursive implementation convey recursive nature recursive neural network better implementations also use give resources effectively reduce train inference time
quantification supervise learn task consist predict give set class c set unlabelled items prevalence relative frequency pcd class c c quantification principle solve classify unlabelled items count many attribute class however classify count approach show yield suboptimal quantification accuracy establish quantification task give rise number methods specifically devise propose recurrent neural network architecture quantification call quanet observe classification predictions learn higher order quantification embeddings refine incorporate quantification predictions simple classify count like methods test quanet sentiment quantification text show substantially outperform several state art baselines
sentence classification task context form sentence adjacent sentence classify provide important information classification context however often ignore methods make use context small amount consider make difficult scale present new method sentence classification context lstm cnn make use potentially large contexts method also utilize long range dependencies within sentence classify use lstm short span feature use stack cnn experiment demonstrate approach consistently improve previous methods two different datasets
many complex generative systems use languages create structure object consider model random languages define weight context free grammars distribution grammar weight broaden transition find random phase sentence indistinguishable noise organize phase nontrivial information carry mark emergence deep structure language understand competition energy entropy
recent advance deep learn bring fore model make multiple computational step service complete task capable describ ing long term dependencies sequential data novel recurrent attention model possibly large external memory modules constitute core mechanisms enable capabilities work address learn subtler complex underlie temporal dynamics language model task deal sparse sequential data end improve upon recent advance adopt concepts field bayesian statistics namely variational inference propose approach consist treat network parameters latent variables prior distribution impose statistical assumptions go beyond standard practice postulate gaussian priors indeed allow handle outliers prevalent long observe sequence multivariate data multivariate exponential distributions impose basis proceed infer correspond posteriors use inference prediction test time way account uncertainty available sparse train data specifically allow approach best exploit merit exponential family method consider new divergence measure generalize concept kullback leibler divergence perform extensive experimental evaluation approach use challenge language model benchmarks illustrate superiority exist state art techniques
modern machine translation rely large parallel corpora recent line work manage train neural machine translation nmt systems monolingual corpora artetxe et al 2018c lample et al two thousand and eighteen despite potential approach low resource settings exist systems far behind supervise counterparts limit practical interest paper propose alternative approach base phrase base statistical machine translation smt significantly close gap supervise systems method profit modular architecture smt first induce phrase table monolingual corpora cross lingual embed mappings combine n gram language model fine tune hyperparameters unsupervised mert variant addition iterative backtranslation improve result yield instance one thousand, four hundred and eight two thousand, six hundred and twenty-two bleu point wmt two thousand and fourteen english german english french respectively improvement seven ten bleu point previous unsupervised systems close gap supervise smt moses train europarl two five bleu point implementation available https githubcom artetxem monoses
online calendar service gain popularity worldwide calendar data become one richest context source understand human behavior however event schedule still time consume even development online calendar although machine learn base event schedule model automate schedule process extent often fail understand subtle user preferences complex calendar contexts event title write natural language paper propose neural event schedule assistant nesa learn user preferences understand calendar contexts directly raw online calendar fully automate highly effective event schedule leverage 593k calendar events nesa learn schedule personal events utilize nesa multi attendee event schedule nesa successfully incorporate deep neural network bidirectional long short term memory convolutional neural network highway network learn preferences user understand calendar context base natural languages experimental result show nesa significantly outperform previous baseline model term various evaluation metrics personal multi attendee event schedule task qualitative analysis demonstrate effectiveness layer nesa learn user preferences
represent entities relations embed space well study approach machine learn relational data exist approach however primarily focus simple link structure finite set entities ignore variety data type often use knowledge base text image numerical value paper propose multimodal knowledge base embeddings mkbe use different neural encoders variety observe data combine exist relational model learn embeddings entities multimodal data use learn embed different neural decoders introduce novel multimodal imputation model generate miss multimodal value like text image information knowledge base enrich exist relational datasets create two novel benchmarks contain additional information textual descriptions image original entities demonstrate model utilize additional information effectively provide accurate link prediction achieve state art result considerable gap five seven exist methods evaluate quality generate multimodal value via user study release datasets open source implementation model https githubcom pouyapez mkbe
portable document format pdf file format increase popularity research analyse structure text extraction analysis necessary detect head crucial component classify extract meaningful data research involve train supervise learn model detect head feature carefully select recursive feature elimination best perform classifier accuracy nine thousand, six hundred and ninety-five sensitivity nine hundred and eighty-six specificity nine hundred and fifty-three research head detection contribute field pdf base text extraction apply automation large scale pdf text analysis variety professional policy base contexts
deep neural network gain increase popularity classic text classification task due strong expressive power less requirement feature engineer despite attractiveness neural text classification model suffer lack train data many real world applications although many semi supervise weakly supervise text classification model exist easily apply deep neural model meanwhile support limit supervision type paper propose weakly supervise method address lack train data neural text classification method consist two modules one pseudo document generator leverage seed information generate pseudo label document model pre train two self train module bootstrap real unlabeled data model refinement method flexibility handle different type weak supervision easily integrate exist deep neural model text classification perform extensive experiment three real world datasets different domains result demonstrate propose method achieve inspire performance without require excessive train data outperform baseline methods significantly
fact extraction verification fever share task launch support development systems able verify claim extract support refute facts raw text share task organizers provide large scale dataset consecutive step involve claim verification particular document retrieval fact extraction claim classification paper present claim verification pipeline approach accord preliminary result score third share task twenty-three compete systems document retrieval implement new entity link approach order able rank candidate facts classify claim basis several select facts introduce two extensions enhance lstm esim
work machine read focus question answer problems answer directly express text read however many real world question answer problems require read text contain literal answer contain recipe derive answer together reader background knowledge one example task interpret regulations answer question work canada carry pay uk national insurance read uk government website topic task require interpretation rule application background knowledge complicate due fact practice question underspecified human assistant regularly ask clarification question long work abroad answer directly derive question text paper formalise task develop crowd source strategy collect 32k task instance base real world rule crowd generate question scenarios analyse challenge task assess difficulty evaluate performance rule base machine learn baselines observe promise result background knowledge necessary substantial room improvement whenever background knowledge need
search orient conversational systems rely information need express natural language nl focus understand nl expressions build keyword base query propose reinforcement learn drive translation model framework able one learn translation nl expressions query supervise way two overcome lack large scale dataset frame translation model word selection approach inject relevance feedback learn process experiment carry two trec datasets outline effectiveness approach
word embed model become fundamental component wide range natural language process nlp applications however embeddings train human generate corpora demonstrate inherit strong gender stereotype reflect social construct address concern paper propose novel train procedure learn gender neutral word embeddings approach aim preserve gender information certain dimension word vectors compel dimension free gender influence base propose method generate gender neutral variant glove gn glove quantitative qualitative experiment demonstrate gn glove successfully isolate gender information without sacrifice functionality embed model
recent work demonstrate embeddings tree like graph hyperbolic space surpass euclidean counterparts performance large margin inspire result scale free structure word co occurrence graph present algorithm learn word embeddings hyperbolic space free text objective function base hyperbolic distance derive include skip gram negative sample architecture word2vec hyperbolic word embeddings evaluate word similarity analogy benchmarks result demonstrate potential hyperbolic word embeddings particularly low dimension though without clear superiority euclidean counterparts discuss subtleties formulation analogy task curve space
introduce adversarial method produce high recall explanations neural text classifier decisions build exist architecture extractive explanations via hard attention add adversarial layer scan residual attention remain predictive signal motivate important domain detect personal attack social media comment additionally demonstrate importance manually set semantically appropriate default behavior model explicitly manipulate bias term develop validation set human annotate personal attack evaluate impact change
text normalization important enable technology several nlp task recently neural network base approach outperform well establish model task however languages english little exploration direction scarcity annotate data complexity language increase difficulty problem address challenge use sequence sequence model character base attention addition self learn character embeddings use word embeddings pre train approach also model subword information provide neural model access linguistic information especially suitable text normalization without large parallel corpora show provide model word level feature bridge gap neural network approach achieve state art f1 score standard arabic language correction share task dataset
recent years witness increase interest image base question answer qa task however due data limitations much less work video base qa paper present tvqa large scale video qa dataset base six popular tv show tvqa consist one hundred and fifty-two thousand, five hundred and forty-five qa pair twenty-one thousand, seven hundred and ninety-three clip span four hundred and sixty hours video question design compositional nature require systems jointly localize relevant moments within clip comprehend subtitle base dialogue recognize relevant visual concepts provide analyse new dataset well several baselines multi stream end end trainable neural network framework tvqa task dataset publicly available http tvqacsuncedu
efficient distribute numerical word representation model word embeddings combine modern machine learn algorithms recently yield considerable improvement automatic document classification task however effectiveness techniques assess hierarchical text classification htc yet study investigate application model algorithms specific problem mean experimentation analysis train classification model prominent machine learn algorithm implementations fasttext xgboost svm keras cnn noticeable word embeddings generation methods glove word2vec fasttext publicly available data evaluate measure specifically appropriate hierarchical context fasttext achieve lcaf1 eight hundred and ninety-three single label version rcv1 dataset analysis indicate use word embeddings flavor promise approach htc
noise contrastive estimation nce powerful parameter estimation method log linear model avoid calculation partition function derivatives train step computationally demand step many case closely relate negative sample methods widely use nlp paper consider nce base estimation conditional model conditional model frequently encounter practice however rigorous theoretical analysis nce set argue subtle important question generalize nce conditional case particular analyze two variants nce conditional model one base classification objective base rank objective show rank base variant nce give consistent parameter estimate weaker assumptions classification base method analyze statistical efficiency rank base classification base variants nce finally describe experiment synthetic data language model show effectiveness trade off methods
visual dialog entail answer series question ground image use dialog history context addition challenge find visual question answer vqa see one round dialog visual dialog encompass several focus one problem call visual coreference resolution involve determine word typically noun phrase pronouns co refer entity object instance image crucial especially pronouns eg dialog agent must first link previous coreference eg boat rely visual ground coreference boat reason pronoun prior work visual dialog model visual coreference resolution either implicitly via memory network history b coarse level entire question explicitly phrase level granularity work propose neural module network architecture visual dialog introduce two novel modules refer exclude perform explicit ground coreference resolution finer word level demonstrate effectiveness model mnist dialog visually simple yet coreference wise complex dataset achieve near perfect accuracy visdial large challenge visual dialog dataset real image model outperform approach interpretable ground consistent qualitatively
visual reason special visual question answer problem multi step compositional nature also require intensive text vision interactions propose cmm cascade mutual modulation novel end end visual reason model cmm include multi step comprehension process question image step use feature wise linear modulation film technique enable textual visual pipeline mutually control experiment show cmm significantly outperform relate model reach state arts two visual reason benchmarks clevr nlvr collect synthetic natural languages ablation study confirm multistep framework visual guide language modulation critical task code available https githubcom flaminghorizon cmm vr
three modalities read comprehension set question answer context task question answer question generation aim infer answer question give counterpart base context present novel two way neural sequence transduction model connect three modalities allow learn two task simultaneously mutually benefit one another train model receive question context answer triplets input capture cross modal interaction via hierarchical attention process unlike previous joint learn paradigms leverage duality question generation question answer data level solve dual task architecture level mirror network structure partially share components different layer enable knowledge transfer one task another help model find general representation modality evaluation four public datasets show dual learn model outperform mono learn counterpart well state art joint model question answer question generation task
follow recent success word embeddings argue thing ideal representation word different model tend capture divergent often mutually incompatible aspects like semantics syntax similarity relatedness paper show embed model capture information directly apparent linear transformation adjust similarity order model without external resource tailor achieve better result aspects provide new perspective embeddings encode divergent linguistic information addition explore relation intrinsic extrinsic evaluation effect transformations downstream task higher unsupervised systems supervise ones
feature map deep neural network commonly use single channel speech enhancement feature map network directly transform noisy feature correspond enhance ones train minimize mean square errors enhance clean feature paper propose adversarial feature map afm method speech enhancement advance feature map approach adversarial learn additional discriminator network introduce distinguish enhance feature real clean ones two network jointly optimize minimize feature map loss simultaneously mini maximize discrimination loss distribution enhance feature push towards clean feature adversarial multi task train achieve better performance asr task senone aware sa afm propose acoustic model network jointly train feature map discriminator network optimize senone classification loss addition afm losses evaluate chime three dataset propose afm achieve one thousand, six hundred and ninety-five five hundred and twenty-seven relative word error rate wer improvements real noisy data feature map baseline respectively sa afm achieve nine hundred and eighty-five relative wer improvement multi conditional acoustic model
feature map use deep neural network effective approach single channel speech enhancement noisy feature transform enhance ones map network mean square errors enhance clean feature minimize paper propose cycle consistent speech enhancement cse additional inverse map network introduce reconstruct noisy feature enhance ones cycle consistent constraint enforce minimize reconstruction loss similarly backward cycle mappings perform opposite direction network losses cycle consistency speech structure well preserve enhance feature noise effectively reduce feature map network generalize better unseen data case unparalleled noisy clean data available train two discriminator network use distinguish enhance noise feature clean noisy ones discrimination losses jointly optimize reconstruction losses adversarial multi task learn evaluate chime three dataset propose cse achieve one thousand, nine hundred and sixty six hundred and sixty-nine relative word error rate improvements respectively use without use parallel clean noisy speech data
propose unsupervised method obtain cross lingual embeddings without parallel data pre train word embeddings propose model call multilingual neural language model take sentence multiple languages input propose model contain bidirectional lstms perform forward backward language model network share among languages parameters ie word embeddings linear transformation hide state output specific language share lstms capture common sentence structure among languages accordingly word embeddings language map common latent space make possible measure similarity word across multiple languages evaluate quality cross lingual word embeddings word alignment task experiment demonstrate model obtain cross lingual embeddings much higher quality exist unsupervised model small amount monolingual data ie 50k sentence available domains monolingual data different across languages
ever grow number extractive summarization techniques propose less clarity ever good system compare rest several study highlight variance performance systems change datasets even across document within corpus effective way counter variance make systems robust could use input multiple systems generate summary present work define novel way create ensemble exploit similarity content candidate summaries estimate reliability define globalrank capture performance candidate system overall corpus localrank estimate performance give document cluster use two score assign weight individual systems use generate new aggregate rank experiment duc2003 duc two thousand and four datasets show significant improvement term rouge score exist sate art techniques
neural question generation nqg task generate question give passage deep neural network previous nqg model suffer problem significant proportion generate question include word question target result generation unintended question paper propose answer separate seq2seq better utilize information passage target answer replace target answer original passage special token model learn identify interrogative word use also propose new module term keyword net help model better capture key information target answer generate appropriate question experimental result demonstrate answer separation method significantly reduce number improper question include answer consequently model significantly outperform previous state art nqg model
question generation task automatically create question textual input work present new attentional encoder decoder recurrent neural network model automatic question generation model incorporate linguistic feature additional sentence embed capture mean sentence word level linguistic feature design capture information relate name entity recognition word case entity coreference resolution addition model use copy mechanism special answer signal enable generation numerous diverse question give sentence model achieve state art result one thousand, nine hundred and ninety-eight bleu4 benchmark question generation dataset outperform previously publish result significant margin human evaluation also show add feature improve quality generate question
encoder decoder model unsupervised sentence representation learn tend discard decoder train large unlabelled corpus since encoder need map input sentence vector representation however parameters learn decoder also contain useful information language order utilise decoder learn present two type decode function whose inverse easily derive without expensive inverse calculation therefore inverse decode function serve another encoder produce sentence representations show careful design decode function model learn good sentence representations ensemble representations produce encoder inverse decoder demonstrate even better generalisation ability solid transferability
paper analyze behavior stack augment recurrent neural network rnn model due architectural similarity stack rnns pushdown transducers train stack rnn model number task include string reversal context free language model cumulative xor evaluation examine behavior network show stack augment rnns discover intuitive stack base strategies solve task however stack rnns difficult train classical architectures lstms rather employ stack base strategies complex network often find approximate solutions use stack unstructured memory
film model achieve close perfect performance diagnostic clevr dataset distinguish model comparatively simple easily transferable architecture paper investigate detail ability film learn various linguistic constructions main result show film able learn relational statements straight away except simple instance b train broader set instance well pretraining simpler instance type help alleviate learn difficulties c mix less robust pretraining sensitive compositional structure dataset overall result suggest approach big encompass datasets paradigm effectiveness data may fundamental limitations
recognize sarcasm often require deep understand multiple source information include utterance conversational context real world facts current sarcasm detection systems consider utterance isolation limit attempt toward take account conversational context paper propose interpretable end end model combine information utterance conversational context detect sarcasm demonstrate effectiveness empirical evaluations also study behavior propose model provide explanations model decisions importantly model capable determine impact utterance conversational context model decisions finally provide ablation study illustrate impact different components propose model
build systems communicate humans core problem artificial intelligence work propose novel neural network architecture response selection end end multi turn conversational dialogue set architecture apply context level attention incorporate additional external knowledge provide descriptions domain specific word use bi directional gate recurrent unit gru encode context responses learn attend context word give latent response representation vice versain addition incorporate external domain specific information use another gru encode domain keyword descriptions allow better representation domain specific keywords responses hence improve overall performance experimental result show model outperform state art methods response selection multi turn conversations
case law significant impact proceed legal case therefore information obtain previous court case valuable lawyers legal officials perform duties paper describe methodology apply discourse relations sentence process text document relate legal domain study develop mechanism classify relationships observe among sentence transcripts unite state court case first define relationship type observe sentence court case transcripts classify pair sentence accord relationship type combine machine learn model rule base approach result obtain system evaluate use human judge best knowledge first study discourse relationships sentence use determine relationships among sentence legal court case transcripts
depth scene descriptions question answer task greatly increase scope today definition scene understand task principle open end current formulations primarily focus describe current state scenes consideration contrast paper focus future state scenes also condition action posit question answer task answer give future scene state give observations current scene question include hypothetical action solution hybrid model integrate physics engine question answer architecture order anticipate future scene state result object object interactions cause action demonstrate first result challenge new problem compare baselines outperform fully data drive end end learn approach
industry datasets use text classification rarely create purpose case data target predictions product accumulate historical data typically fraught noise present text base document well target label work address question well performance metrics compute noisy historical data reflect performance intend future machine learn model input result demonstrate utility dirty train datasets use build prediction model cleaner different prediction input
forecast multivariate time series data prediction electricity consumption solar power production polyphonic piano piece numerous valuable applications however complex non linear interdependencies time step series complicate task obtain accurate prediction crucial model long term dependency time series data achieve good extent recurrent neural network rnn attention mechanism typical attention mechanism review information previous time step select relevant information help generate output fail capture temporal pattern across multiple time step paper propose use set filter extract time invariant temporal pattern similar transform time series data frequency domain propose novel attention mechanism select relevant time series use frequency domain information forecast apply propose model several real world task achieve state art performance one exception
introduce mass simple evaluation metric task visual question answer vqa standard form vqa task operationalized follow give image open end question natural language systems require provide suitable answer currently model performance evaluate mean somehow simplistic metric predict answer choose least three human annotators ten one hundred correct though intuitively valuable metric important limitations first ignore whether predict answer one select majority annotators second account quantitative subjectivity answer sample dataset third information semantic similarity ses responses completely neglect base limitations propose multi component metric account issue show metric effective provide fine grain evaluation quantitative qualitative level
paper explore use factorize hierarchical variational autoencoder fhvae model learn unsupervised latent representation dialect identification fhvae learn latent space separate static attribute within utterance dynamic attribute encode two different set latent variables useful factor dialect identification phonetic linguistic content encode segmental latent variable irrelevant factor relatively constant within sequence channel speaker information encode sequential latent variable disentanglement property make segmental latent variable less susceptible channel speaker variation thus reduce degradation channel domain mismatch demonstrate fully supervise task end end model train feature extract fhvae model achieve best performance compare model train conventional acoustic feature vector base system moreover also show propose approach leverage large amount unlabeled data fhvae train learn domain invariant feature significantly improve performance low resource condition label domain data available
visual qa pivotal challenge higher level reason require understand language vision relationships many object scene although datasets like clevr design unsolvable without complex relational reason surprisingly simple fee forward holistic model recently show strong performance dataset model lack kind explicit iterative symbolic reason procedure hypothesize necessary count object narrow set relevant object base several attribute etc reason strong performance poorly understand hence work analyze model find minor architectural elements crucial performance particular find textitearly fusion language vision provide large performance improvements contrast late fusion approach popular dawn visual qa propose simple module call multimodal core hypothesize perform fundamental operations multimodal task believe understand elements important complex question answer aid design better perform algorithms visual qa minimize hand engineer effort
speech activity detection sad play important role current speech process systems include automatic speech recognition asr sad particularly difficult environments acoustic noise practical solution incorporate visual information increase robustness sad approach audiovisual system advantage robust different speech modes eg whisper speech background noise recent advance audiovisual speech process use deep learn open opportunities capture principled way temporal relationships acoustic visual feature study explore idea propose emphbimodal recurrent neural network brnn framework sad approach model temporal dynamic sequential audiovisual data improve accuracy robustness propose sad system instead estimate hand craft feature study investigate end end train approach acoustic visual feature directly learn raw data train experimental evaluation consider large audiovisual corpus six hundred and eight hours record collect one hundred and five speakers result demonstrate propose framework lead absolute improvements twelve practical scenarios vad baseline use audio implement deep neural network dnn propose approach achieve nine hundred and twenty-seven f1 score evaluate use sensors portable tablet noisy acoustic environment ten lower performance obtain ideal condition eg clean speech obtain high definition camera close talk microphone
current dialogue systems focus textual speech context knowledge usually base two speakers recent work investigate static image base dialogue however several real world human interactions also involve dynamic visual context similar videos well dialogue exchange among multiple speakers move closer towards multimodal conversational skills visually situate applications introduce new video context many speaker dialogue dataset base live broadcast soccer game videos chat twitchtv challenge testbed allow us develop visually ground dialogue model generate relevant temporal spatial event language live video also relevant chat history strong baselines also present several discriminative generative model eg base tridirectional attention flow tridaf evaluate model via retrieval rank recall automatic phrase match metrics well human evaluation study also present dataset analyse model ablations visualizations understand contribution different modalities model components
propose novel wasserstein method distillation mechanism yield joint learn word embeddings topics propose method base fact euclidean distance word embeddings may employ underlie distance wasserstein topic model word distributions topics optimal transport word distributions document embeddings word learn unify framework learn topic model leverage distil underlie distance matrix update topic distributions smoothly calculate correspond optimal transport strategy provide update word embeddings robust guidance improve algorithmic convergence application focus patient admission record propose method embed cod diseases procedures learn topics admissions obtain superior performance clinically meaningful disease network construction mortality prediction function admission cod procedure recommendation
state art natural language process systems rely supervision form annotate data learn competent model model generally train data single language usually english directly use beyond language since collect data every language realistic grow interest cross lingual language understand xlu low resource cross language transfer work construct evaluation set xlu extend development test set multi genre natural language inference corpus multinli fifteen languages include low resource languages swahili urdu hope dataset dub xnli catalyze research cross lingual sentence understand provide informative standard evaluation task addition provide several baselines multilingual sentence understand include two base machine translation systems two use parallel data train align multilingual bag word lstm encoders find xnli represent practical challenge evaluation suite directly translate test data yield best performance among available baselines
present sequence action parse approach natural language sql task incrementally fill slot sql query feasible action pre define inventory account fact typically multiple correct sql query similar semantics draw inspiration syntactic parse techniques propose train sequence action model non deterministic oracles evaluate model wikisql dataset achieve execution accuracy eight hundred and thirty-seven test set twenty-one absolute improvement model train traditional static oracles assume single correct target sql query combine execution guide decode strategy model set new state art performance execution accuracy eight hundred and seventy-one
abstract machine understand question tightly relate recognition articulation context computational capabilities underlie process algorithm paper mathematical model capture distinguish latent structure articulation question present propose objective drive approach represent latent structure show approach beneficial examples complementary objectives available show latent structure represent system maximize cost function relate underlie objective show optimization formulation approximate build memory pattern represent train neural auto encoder experimental evaluation use many cluster question relate objective show eighty recognition accuracy negligible false positive across cluster question extend memory relate task goal iteratively refine dataset question base latent articulation also demonstrate refinement scheme call k fingerprint achieve nearly one hundred recognition negligible false positive across different cluster question
natural language inference nli fundamental many natural language process nlp applications include semantic search question answer nli problem gain significant attention thank release large scale challenge datasets present approach problem largely focus learn base methods use textual information order classify whether give premise entail contradict neutral respect give hypothesis surprisingly use methods base structure knowledge central topic artificial intelligence receive much attention vis vis nli problem many open knowledge base contain various type reason information use nli well explore address present combination techniques harness knowledge graph improve performance nli problem science question domain present result apply techniques text graph text graph base model discuss implications use external knowledge solve nli problem model achieve new state art performance nli problem scitail science question dataset
health professionals use natural language process nlp technologies review electronic health record ehr machine learn free text classifiers help identify problems make critical decisions aim develop deep learn neural network algorithms identify ehr progress note pertain diabetes validate algorithms two institutions data use two thousand ehr progress note retrieve patients diabetes note annotate manually diabetic non diabetic several deep learn classifiers develop performances evaluate area roc curve auc convolutional neural network cnn model separable convolution layer accurately identify diabetes relate note brigham womens hospital test set highest auc nine hundred and seventy-five deep learn classifiers use identify ehr progress note pertain diabetes particular cnn base classifier achieve higher auc svm base classifier
task multi step ahead prediction language model challenge consider discrepancy train test test time language model require make predictions give past predictions input instead past target provide train difference know exposure bias lead compound errors along generate sequence test time order improve generalization neural language model address compound errors propose curriculum learn base method gradually change initially deterministic teacher policy gradually stochastic policy refer textitnearest neighbor replacement sample choose input give timestep replace sample nearest neighbor past target truncate probability proportional cosine similarity original word top k similar word allow teacher explore alternatives teacher provide sub optimal policy initial policy difficult learner model propose strategy straightforward online require little additional memory requirements report main find two language model benchmarks find propose approach perform particularly well use conjunction schedule sample attempt mitigate compound errors language model
classic supervise learn make close world assumption mean class see test must see train however dynamic world new unseen class examples may appear constantly model work environment must able reject unseen class see use train enough data collect unseen class system incrementally learn accept classify learn paradigm call open world learn owl exist owl methods need form train accept include new class overall model paper propose meta learn approach problem key novelty need train meta classifier continually accept new class enough label data meta classifier use also detect reject future unseen class train meta classifier new overall classifier cover old new class need test method use examples see class include newly add class fly classification rejection experimental result demonstrate effectiveness new approach
visual relationship detection bridge gap computer vision natural language scene understand image different pure object recognition task relation triplets subject predicate object lie extreme diversity space textitperson behind person textitcar behind build suffer problem combinatorial explosion paper propose context dependent diffusion network cddn framework deal visual relationship detection capture interactions different object instance two type graph word semantic graph visual scene graph construct encode global context interdependency semantic graph build language priors model semantic correlations across object whilst visual scene graph define connections scene object utilize surround scene information graph structure data design diffusion network adaptively aggregate information contexts effectively learn latent representations visual relationships well cater visual relationship detection view isomorphic invariance graph experiment two widely use datasets demonstrate propose method effective achieve state art performance
exist work image description focus generate expressive descriptions work dedicate generate stylish eg romantic lyric etc descriptions suffer limit style variation content digression address limitations propose controllable stylish image description generation model learn generate stylish image descriptions relate image content train arbitrary monolingual corpus without collect new pair image stylish descriptions moreover enable users generate various stylish descriptions plug style specific parameters include new style exist model achieve capability via novel layer normalization layer design refer domain layer norm dln extensive experimental validation user study various stylish image description generation task conduct show competitive advantage propose model
address two challenge topic model one context information around word help determine actual mean eg network use contexts artificial neural network vs biological neuron network generative topic model infer topic word distributions take little context account extend neural autoregressive topic model exploit full context information around word document language model fashion propose model name idocnade two due small number word occurrences ie lack context short text data sparsity corpus document application topic model challenge texts therefore propose simple efficient way incorporate external knowledge neural autoregressive topic model use embeddings distributional prior propose variants name docnadee idocnadee present novel neural autoregressive topic model variants consistently outperform state art generative topic model term generalization interpretability topic coherence applicability retrieval classification seven long text eight short text datasets diverse domains
continuous word representation aka word embed basic build block many neural network base model use natural language process task although widely accept word similar semantics close embed space find word embeddings learn several task bias towards word frequency embeddings high frequency low frequency word lie different subregions embed space embed rare word popular word far even semantically similar make learn word embeddings ineffective especially rare word consequently limit performance neural network model paper develop neat simple yet effective way learn emphfrequency agnostic word embed frage use adversarial train conduct comprehensive study ten datasets across four natural language process task include word similarity language model machine translation text classification result show frage achieve higher performance baselines task
learn visual feature representations video analysis daunt task require large amount train sample proper generalization framework many current state art methods video caption movie description rely simple encode mechanisms recurrent neural network encode temporal visual information extract video data paper introduce novel multitask encoder decoder framework automatic semantic description caption video sequence contrast current approach method rely distinct decoders train visual encoder multitask fashion system depend solely multiple label allow lack train data work even datasets one single annotation viable per video method show improve performance current state art methods several metrics multi caption single caption datasets best knowledge method first method use multitask approach encode video feature method demonstrate robustness large scale movie description challenge lsmdc two thousand and seventeen method movie description task result rank among competitors helpful visually impair
service robots need show appropriate social behaviour order deploy social environments healthcare education retail etc main capabilities robots navigation conversational skills person impatient person might want robot navigate faster vice versa linguistic feature indicate politeness provide social cue person patient impatient behaviour novelty present paper dynamically incorporate politeness robotic dialogue systems navigation understand politeness users speech use modulate robot behaviour responses therefore develop dialogue system navigate indoor environment produce different robot behaviours responses base users intention degree politeness deploy test system pepper robot adapt change user politeness
paper propose deep globally normalize topic model incorporate structural relationships connect document socially generate corpora online forums model one capture discursive interactions along observe reply link addition traditional topic information two incorporate latent distribute representations arrange deep architecture enable gpu base mean field inference procedure scale efficiently large data apply model new social media dataset consist 13m comment mine popular internet forum reddit domain pose significant challenge model account relationships connect user comment evaluate exist methods across multiple metrics include perplexity metadata prediction qualitatively analyze learn interaction pattern
syntactic theory traditionally adopt constructivist approach set atomic elements manipulate combinatory operations yield derive complex elements syntactic structure thus see result discrete recursive combinatorics lexical items get assemble phrase combine form sentence view common european american structuralism eg benveniste one thousand, nine hundred and seventy-one hockett one thousand, nine hundred and fifty-eight different incarnations generative grammar transformational non transformational chomsky one thousand, nine hundred and fifty-six one thousand, nine hundred and ninety-five kaplan bresnan one thousand, nine hundred and eighty-two gazdar one thousand, nine hundred and eighty-two since least uriagereka two thousand and two attention pay fact syntactic operations must apply somewhere particularly copy movement operations consider contemporary syntactic theory thus somewhat acknowledge importance formalize aspects space elements manipulate still vastly underexplored area paper explore consequences conceptualize syntax set topological operations apply space rather discrete elements argue empirical advantage view treatment long distance dependencies cross derivational dependencies constraints possible configurations emerge dynamics system
propose video story question answer qa architecture multimodal dual attention memory mdam key idea use dual attention mechanism late fusion mdam use self attention learn latent concepts scene frame caption give question mdam use second attention latent concepts multimodal fusion perform dual attention process late fusion use process pipeline mdam learn infer high level vision language joint representation abstraction full video content evaluate mdam pororoqa movieqa datasets large scale qa annotations cartoon videos movies respectively datasets mdam achieve new state art result significant margins compare runner model confirm best performance dual attention mechanism combine late fusion ablation study also perform qualitative analysis visualize inference mechanisms mdam
ask question pervasive human activity little understand make difficult answer analysis pair large databases new york time crosswords question quiz show jeopardy establish two orthogonal dimension question difficulty obscurity rarity answer opacity indirectness question cue operationalized word2vec importance opacity role synergistic information resolve suggest account difficulty term prior expectations capture part question ask process regression analysis show presence additional dimension question ask question complexity answer local network density cue intersection presence signal word work show question askers help interlocutors use contextual cue conversely particular kind unfamiliarity domain question make harder individuals learn others take together result suggest bayesian model question difficulty supplement process model account heuristics individuals use navigate conceptual space
eu general data protection regulation gdpr one demand comprehensive privacy regulations time year go effect study impact landscape privacy policies online conduct first longitudinal depth scale assessment privacy policies gdpr gauge complete consumption cycle policies first user impressions compliance assessment create diverse corpus two set six thousand, two hundred and seventy-eight unique english language privacy policies inside outside eu cover pre gdpr post gdpr versions result test analyse suggest gdpr catalyst major overhaul privacy policies inside outside eu overhaul policies manifest extensive textual change especially eu base websites come mix benefit users privacy policies become considerably longer user study four hundred and seventy participants amazon mturk indicate significant improvement visual representation privacy policies users perspective eu websites develop new workflow automate assessment requirements privacy policies use workflow show privacy policies cover data practice consistent seven compliance requirements post gdpr also assess transparent organizations privacy practice perform specificity analysis analysis find evidence positive change trigger gdpr specificity level improve average still find landscape privacy policies transitional phase many policies still meet several key gdpr requirements improve coverage come reduce specificity
zipf law main regularity quantitative linguistics despite many work devote foundations law still unclear whether statistical regularity deeper relations information carry structure text question relate distinguish meaningful text write unknown system meaningless set symbols mimic statistical feature text contribute resolve question compare feature first half text begin middle second half comparison uncover hide effect halve value many parameters style genre author vocabulary etc study texts saw first half zipf law apply smaller rank second half ie law apply better first half also word hold zipf law first half distribute homogeneously text feature allow distinguish meaningful text random sequence word find correlate number textual characteristics hold case study first half lexically richer longer less repetitive word shorter sentence punctuation sign paragraph differences halve indicate higher hierarchic level text organization far go unnoticed text linguistics relate validity zipf law textual information complete description effect require new model though one exist model account aspects
bilingual student learn solve word problems math expect student able solve problem languages student fluent ineven math lessons teach one language however current representations machine learn language dependent work present method decouple language problem learn language agnostic representations therefore allow train model one language apply different one zero shoot fashion learn representations take inspiration linguistics formalize universal grammar optimization process chomsky two thousand and fourteen montague one thousand, nine hundred and seventy demonstrate capabilities representations show model train single language use language agnostic representations achieve similar accuracies languages
communication standard tool central bank monetary policy toolkit theoretically communication provide central bank opportunity guide public expectations show empirically central bank communication lead financial market fluctuations however little research dimension topics information important cause fluctuations develop semi automatic methodology summarize fomc statements main theme automatically select best model base coherency assess whether significant impact theme shape yous treasury yield curve use topic model methods machine learn literature find suggest fomc statements decompose three topics information relate economic condition mandate ii information relate monetary policy tool intermediate target iii information relate financial market financial crisis find statements influential financial crisis effect mostly present curvature yield curve information relate financial theme
develop new model algorithms learn temporal dynamics topic polytopes relate geometric object arise topic model base inference model nonparametric bayesian correspond inference algorithm able discover new topics time progress exploit connection model topic polytope evolution beta bernoulli process hungarian match algorithm method show several order magnitude faster exist topic model approach demonstrate experiment work several million document two dozens minutes
introduce novel type text representation preserve 2d layout document achieve encode document page two dimensional grid character base representation present generic document understand pipeline structure document pipeline make use fully convolutional encoder decoder network predict segmentation mask bound box demonstrate capabilities information extraction task invoice show significantly outperform approach base sequential text document image
automate score engines increasingly use score free form text responses students give question engines design appropriately deal responses human reader would find alarm indicate intention self harm harm others responses allude drug abuse sexual abuse response would elicit concern student write response neural network model design help identify anomalous responses large collection typical responses students give responses identify neural network assess urgency severity validity quickly team reviewers otherwise possible give anomalous nature type responses goal maximize chance flag responses review give constraint fix percentage responses viably assess team reviewers
paper propose end end short utterances speech language identificationsld approach base long short term memory lstm neural network special suitable sld application intelligent vehicles feature use lstm learn generate transfer learn method bottle neck feature deep neural network dnn train mandarin acoustic phonetic classification use lstm train order improve sld accuracy short utterances phase vocoder base time scale modificationtsm method use reduce increase speech rat test utterance splice normal speech rate reduce increase utterances extend length test utterances improve improve performance sld system experimental result ap17 olr database show propose methods improve performance sld especially short utterance 1s 3s durations
past years adversarial train become extremely active research topic successfully apply various artificial intelligence ai domains potentially crucial technique development next generation emotional ai systems herein provide comprehensive overview application adversarial train affective compute sentiment analysis various representative adversarial train algorithms explain discuss accordingly aim tackle diverse challenge associate emotional ai systems highlight range potential future research directions expect overview help facilitate development adversarial train affective compute sentiment analysis academic industrial communities
conventional speak language understand systems consist two main components automatic speech recognition module convert audio transcript natural language understand module transform result text top n hypotheses set domains intents arguments modules typically optimize independently paper formulate audio semantic understand sequence sequence problem one propose compare various encoder decoder base approach optimize modules jointly end end manner evaluations real world task show one intermediate text representation crucial quality predict semantics especially intent arguments two jointly optimize full system improve overall accuracy prediction compare independently train model best jointly train model achieve similar domain intent prediction f1 score improve argument word error rate eighteen relative
image caption demonstrate model capable generate plausible text give input image videos recent work image generation show significant improvements image quality text use prior work tie concepts together create architecture enable bidirectional generation image text call network multi modal vector representation mmvr along mmvr propose two improvements text condition image generation firstly n gram metric base cost function introduce generalize caption respect image secondly multiple semantically similar sentence show help generate better image qualitative quantitative evaluations demonstrate mmvr improve upon exist text condition image generation result twenty integrate visual text modalities
paper study consistency kernel base neural rank model k nrm recent state art neural ir model important reproducible research deployment industry find k nrm low variance relevance base metrics across experimental trials spite low variance overall performance different trials produce different document rank individual query main source variance experiment find different latent match pattern capture k nrm ir customize word embeddings learn k nrm query document word pair follow two different match pattern equally effective align word pair differently embed space different latent match pattern enable simple yet effective approach construct ensemble rankers improve k nrm effectiveness generalization abilities
paper attempt classify lindenmayer systems base properties set rule kind string rule generate classification refer parametrization l space l space phase space possible l developments represent space infinite halt algorithm l grammars also subject hard condition grammars developments possible state l system well know example space normal grammars space normal grammars parametrized regular context free context sensitive unrestricted proper containment relations hold among see chomsky one thousand, nine hundred and fifty-nine theorem one contend l space rich landscape grammars cluster kinds mutually translatable
present operational component real world patient triage system give specific patient presentation system able assess level medical urgency issue appropriate recommendation term best point care time treat use attention base convolutional neural network architecture train six hundred thousand doctor note german compare two approach one use full text medical note one use select list medical entities extract text approach achieve seventy-nine sixty-six precision respectively confidence threshold six precision increase eighty-five seventy-five respectively addition method detect warn symptoms implement render classification task transparent medical perspective method base learn attention score method automatic validation use data
complex textual information extraction task often pose sequence label emphshallow parse field extract use local label make consistent probabilistic inference graphical model constrain transition recently become common locally parametrize model use rich feature extract recurrent neural network lstm enforce consistent output simple linear chain model represent markovian dependencies successive label however simple graphical model structure belie often complex non local constraints output label example many field first name occur fix number time presence field rnns provide increasingly powerful context aware local feature sequence tag yet integrate global graphical model similar expressivity output distribution model go beyond linear chain crf incorporate multiple hide state per output label parametrizes transition parsimoniously low rank log potential score matrices effectively learn embed space hide state augment latent space inference variables complement rich feature representation rnn allow exact global inference obey complex learn non local output constraints experiment several datasets show model outperform baseline crfrnn model global output constraints necessary inference time explore interpretable latent structure
task speak pass phrase verification decide whether test utterance contain phrase give enrollment utterances beside applications pass phrase verification complement independent speaker verification subsystem text dependent speaker verification also use liven detection verify user able correctly respond randomly prompt phrase paper build previous work vector base text dependent speaker verification show vectors extract use phrase specific hide markov model hmms use deep neural network dnn base bottle neck bn feature help reject utterances wrong pass phrase apply vector extraction techniques stand alone task speaker independent speak pass phrase classification verification experiment rsr2015 reddots databases show simple score techniques eg cosine distance score apply vectors provide result superior previously publish data
neural approach sequence label often use conditional random field crf model output dependencies recurrent neural network rnn use purpose task set establish rnns attractive alternative crfs sequence label address one rnn prominent shortcomings fact expose errors maximum likelihood train frame prediction output sequence sequential decision make process train network adjust actor critic algorithm ac rnn comprehensively compare strategy maximum likelihood train rnns crfs three structure output task propose ac rnn efficiently match performance crf ner ccg tag outperform machine transliteration also show train strategy significantly better techniques address rnn exposure bias schedule sample self critical policy train
short commentary trinh le two thousand and eighteen simple method commonsense reason outline three serious flaw cite paper discuss data drive approach consider serious model commonsense reason need natural language understand general reference resolution particular
automatic measurement semantic text similarity important task natural language process paper evaluate performance different vector space model perform task address real world problem model patent patent similarity compare tfidf relate extensions topic model eg latent semantic index neural model eg paragraph vectors contrary expectations add computational cost text embed methods justify one target text condense two similarity comparison trivial otherwise tfidf perform surprisingly well case particular longer technical texts make finer grain distinctions nearest neighbor unexpectedly extensions tfidf method add noun phrase calculate term weight incrementally helpful context
grammatical error correction like machine learn task greatly benefit large quantities high quality train data typically expensive produce write program automatically generate realistic grammatical errors would difficult one could learn distribution naturallyoccurring errors attempt introduce datasets initial work induce errors way use statistical machine translation show promise investigate cheaply construct synthetic sample give small corpus human annotate data use rack attentive sequence sequence model straight forward post process procedure approach yield error fill artificial data help vanilla bi directional lstm outperform previous state art grammatical error detection previously introduce model gain improvements five f05 score attempt determine give sentence synthetic human annotator best achieve three thousand, nine hundred and thirty-nine f1 score indicate model generate mostly human like instance
recent advance speak language technologies introduction many customer face products give rise wide customer reliance smart personal assistants many daily task paper present system reduce users cognitive load extend personal assistants long term personal memory users store retrieve voice arbitrary piece information problem frame neural retrieval base question answer system answer select previously store user memories propose directly optimize end end retrieval performance measure f1 score use reinforcement learn lead better performance experimental test set
human mind powerful multifunctional knowledge storage management system perform generalization type inference anomaly detection stereotype task dynamic kr system appropriately profile sparse input provide complete expectations unknown facets help task paper introduce task profile inspire theories find social psychology potential profile reason information process describe two generic state art neural architectures easily instantiate profile machine generate expectations apply kind knowledge fill gap evaluate methods wikidata crowd expectations compare result gain insight nature knowledge capture various profile methods make code data available facilitate future research
consensus maximisation learn provide self supervision different view available data distributional hypothesis provide another form useful self supervision adjacent sentence plentiful large unlabelled corpora motivate observation different learn architectures tend emphasise different aspects sentence mean present new self supervise learn framework learn sentence representations minimise disagreement two view sentence one view encode sentence recurrent neural network rnn view encode sentence simple linear model learn individual view network result higher quality sentence representations single view learn counterparts learn use distributional hypothesis judge performance standard downstream task ensemble view provide even better generalisation supervise unsupervised downstream task also importantly ensemble view train consensus maximisation two different architectures perform better downstream task analogous ensemble make single view train counterparts
enormous online textual information provide intrigue opportunities understand social economic semantics paper propose novel text regression model base conditional generative adversarial network gin attempt associate textual data social outcomes semi supervise manner besides promise potential predict capabilities superiorities twofold model work unbalance datasets limit label data align real world scenarios ii predictions obtain end end framework without explicitly select high level representations finally point relate datasets experiment future research directions
paper concern train recurrent neural network goal orient dialog agents use reinforcement learn train agents policy gradients typically require large amount sample however collection require data form conversations chat bots human agents time consume expensive mitigate problem describe efficient policy gradient method use positive memory retention significantly increase sample efficiency show method ten time sample efficient policy gradients extensive experiment new synthetic number guess game moreover real word visual object discovery game propose method twice sample efficient policy gradients show state art performance
present optimal completion distillation ocd train procedure optimize sequence sequence model base edit distance ocd efficient hyper parameters require pretraining joint optimization conditional log likelihood give partial sequence generate model first identify set optimal suffix minimize total edit distance use efficient dynamic program algorithm position generate sequence use target distribution put equal probability first token optimal suffix ocd achieve state art performance end end speech recognition wall street journal librispeech datasets achieve ninety-three wer forty-five wer respectively
identify classify toxic online commentary modern tool data science transform raw text key feature either thresholding learn algorithms make predictions monitor offensive conversations systematically evaluate sixty-two classifiers represent nineteen major algorithmic families feature extract jigsaw dataset wikipedia comment compare classifiers base statistically significant differences accuracy relative execution time among classifiers identify toxic comment tree base algorithms provide transparently explainable rule rank order predictive contribution feature among twenty-eight feature syntax sentiment emotion outlier word dictionaries simple bad word list prove predictive offensive commentary
propose categorial grammar base classical multiplicative linear logic see extension abstract categorial grammars acg least expressive however constituents linear logic grammars llg abstract lambda term simply tuples word label endpoints call multiwords least give concrete intuitive representation acg key observation class multiwords fundamental algebraic structure namely multiwords organize category similar category topological cobordisms category symmetric monoidal close compact close thus model linear lambda calculus classical linear logic think category interest right particular might provide categorical representation formalisms hand many model language semantics base commutative logic generally symmetric monoidal close categories category word cobordisms category language elements symmetric monoidal close independent grammar thus might prove useful understand language semantics well
paper report set experiment different word embeddings initialize state art bi lstm crf network event detection classification italian follow eventi evaluation exercise net work obtain new state art result improve f1 score detection thirteen point sixty-five point classification use single step approach result also provide evidence embeddings major impact performance architectures
marry two powerful ideas deep representation learn visual recognition language understand symbolic program execution reason neural symbolic visual question answer ns vqa system first recover structural scene representation image program trace question execute program scene representation obtain answer incorporate symbolic structure prior knowledge offer three unique advantage first execute program symbolic space robust long program trace model solve complex reason task better achieve accuracy nine hundred and ninety-eight clevr dataset second model data memory efficient perform well learn small number train data also encode image compact representation require less storage exist methods offline question answer third symbolic program execution offer full transparency reason process thus able interpret diagnose execution step
study leverage shelf visual linguistic data cope vocabulary answer visual question answer task exist large scale visual datasets annotations image class label bound box region descriptions good source learn rich diverse visual concepts however straightforward visual concepts capture transfer visual question answer model due miss link question dependent answer model visual data without question tackle problem two step one learn task conditional visual classifier capable solve diverse question specific visual recognition task base unsupervised task discovery two transfer task conditional visual classifier visual question answer model specifically employ linguistic knowledge source structure lexical database eg wordnet visual descriptions unsupervised task discovery transfer learn task conditional visual classifier answer unit visual question answer model empirically show propose algorithm generalize vocabulary answer successfully use knowledge transfer visual dataset
many service perform information retrieval point interest poi utilize lucene base setup spatial filter type system easy implement make use semantics rely direct word match query review lead loss precision recall study challenge task semantically enrich pois unstructured data order support open domain search question answer qa introduce new dataset poireviewqa consist 20k question egis restaurant dog friendly one thousand and twenty-two yelp business type question sample ten review annotate sentence review whether answer question correspond answer test system ability understand text adopt information retrieval evaluation rank review sentence question base likelihood answer question build lucene base baseline model achieve seven hundred and seventy auc four hundred and eighty-eight map sentence embed base model achieve seven hundred and ninety-two auc four hundred and eighteen map indicate dataset present challenge problem future research gir community result technology help exploit thematic content web document social media characterisation locations
learn match function two text sequence long stand problem nlp research task enable many potential applications question answer paraphrase identification paper propose co stack residual affinity network csran new universal neural architecture problem csran deep architecture involve stack multi layer recurrent encoders stack deep architectures traditionally difficult train due inherent weaknesses difficulty feature propagation vanish gradients csran incorporate two novel components take advantage stack architecture firstly introduce new bidirectional alignment mechanism learn affinity weight fuse sequence pair across stack hierarchies secondly leverage multi level attention refinement component stack recurrent layer key intuition leverage information across network hierarchies improve gradient flow also improve overall performance conduct extensive experiment six well study text sequence match datasets achieve state art performance
sentiment polarity tweet blog post product review become highly attractive utilize recommender systems market predictions business intelligence deep learn techniques become top performers analyze texts however several problems need solve efficient use deep neural network text mine text polarity analysis first deep neural network need feed data set big size well properly label second various uncertainties regard use word embed vectors generate data set use train model better source big popular collections third simplify model creation convenient generic neural network architectures effective adapt various texts encapsulate much design complexity thesis address problems provide methodological practical insights utilize neural network sentiment analysis texts achieve state art result regard first problem effectiveness various crowdsourcing alternatives explore two medium size emotion label song data set create utilize social tag address second problem series experiment large text collections various content domains conduct try word embeddings various parameters regard third problem series experiment involve convolution max pool neural layer conduct combine convolutions word bigrams trigrams regional max pool layer couple stack produce best result derive architecture achieve competitive performance sentiment polarity analysis movie business product review
text generate social media platforms essentially mix lingual text mix language form produce considerable amount difficulty language process systems moreover advancements language process research depend upon availability standard corpora development mix lingual indian name entity recognition ner systems face obstacles due unavailability standard evaluation corpora corpora may mix lingual nature text write use multiple languages predominantly use single script motivation work emphasize automatic generation kind corpora order encourage mix lingual indian ner paper present preparation cross script hindi english corpora wikipedia category page corpora successfully annotate use standard conll two thousand and three categories per loc org misc evaluation carry variety machine learn algorithms favorable result achieve
paper try explore evolution language case calculations first choose novels eleven british writers one thousand, four hundred two thousand and five find correspond work use natural language process tool construct correspond eleven corpora calculate respective word vectors one hundred high frequency word eleven corpora next corpus concatenate one hundred word vectors begin end one finally use similarity comparison hierarchical cluster method generate relationship tree combine eleven word vectors tree represent relationship eleven corpora find tree generate cluster distance corpus year correspond corpus basically mean discover specific language evolution tree verify stability versatility method add three theme dickens eight work 19th century poets work art criticism recent sixty years four theme test different parameters time span corpus time interval corpora dimension word vector number high frequency public word result show fairly stable versatile
task event detection involve identify categorize event trigger contextual information show effective task however exist methods utilize contextual information process context argue context better exploit process context multiple time allow model perform complex reason generate better context representation thus improve overall performance meanwhile dynamic memory network dmn demonstrate promise capability capture contextual information apply successfully various task light multi hop mechanism dmn model context propose trigger detection dynamic memory network td dmn tackle event detection problem perform five fold cross validation ace two thousand and five dataset experimental result show multi hop mechanism improve performance propose model achieve best f1 score compare state art methods
sequence sequence seq2seq approach low resource asr relatively new direction speech research approach benefit perform model train without use lexicon alignments however pose new problem require data compare conventional dnn hmm systems work attempt use data ten babel languages build multi lingual seq2seq model prior model port towards four babel languages use transfer learn approach also explore different architectures improve prior multilingual seq2seq model paper also discuss effect integrate recurrent neural network language model rnnlm seq2seq model decode experimental result show transfer learn approach multilingual model show substantial gain monolingual model across four babel languages incorporate rnnlm also bring significant improvements term wer achieve recognition performance comparable model train twice train data
text analytics base supervise machine learn classifiers show great promise multitude domains yet apply seismology test various standard model naive bay k nearest neighbor support vector machine random forest seismological corpus one hundred article relate topic precursory accelerate seismicity span one thousand, nine hundred and eighty-eight two thousand and ten corpus label mignan two thousand and eleven precursor whether explain critical process ie cascade trigger process signature main fault load investigate rather classification process automatize help analyze larger corpora order better understand trend earthquake predictability research find naive bay model perform best agreement machine learn literature case small datasets cross validation accuracies eighty-six binary classification refine multiclass classification non critical process agnostic critical process assume critical process demonstrate obtain seventy-eight accuracy prediction dozen article publish since two thousand and eleven show however weak generalization f1 score sixty slightly better random classifier explain change authorship use different terminologies yet model show f1 score greater eighty two multiclass extremes non critical process versus critical process demonstrate fall random classifier result around twenty-five paper label agnostic critical process assume result encourage view small size corpus high degree abstraction label domain knowledge engineer remain essential make transparent investigation naive bay keyword posterior probabilities
goal work develop meet transcription system recognize speech even utterances different speakers overlap speech overlap regard major obstacle accurately transcribe meet traditional beamformer single output exclusively use previously propose speech separation techniques critical constraints application real meet paper propose new signal process module call unmixing transducer describe implementation use windowed blstm unmixing transducer fix number say j output channel j may different number meet attendees transform input multi channel acoustic signal j time synchronous audio stream utterance meet separate emit one output channel output signal simply feed speech recognition back end segmentation transcription meet transcription system use unmixing transducer outperform system base state art neural mask base beamformer one hundred and eight significant improvements observe overlap segment best knowledge first report apply overlap speech recognition unconstrained real meet audio
address two challenge probabilistic topic model order better estimate probability word give context ie pwordcontext one language structure context probabilistic topic model ignore word order summarize give context bag word consequently semantics word context lose lstm lm learn vector space representation word account word order local collocation pattern model complex characteristics language eg syntax semantics tm simultaneously learn latent representation entire document discover underlie thematic structure unite two complementary paradigms learn mean word occurrences combine tm eg docnade lm unify probabilistic framework name ctx docnade two limit context smaller train corpus document settings small number word occurrences ie lack context short text data sparsity corpus document application tms challenge address challenge incorporate external knowledge neural autoregressive topic model via language model approach use word embeddings input lstm lm aim improve word topic map smaller short text corpus propose docnade extension name ctx docnadee present novel neural autoregressive topic model variants couple neural lms embeddings priors consistently outperform state art generative tms term generalization perplexity interpretability topic coherence applicability retrieval classification six long text eight short text datasets diverse domains
grammar base fuzzing technique use find software vulnerabilities inject well form input generate follow rule encode application semantics grammar base fuzzers network protocols rely human experts manually specify rule work study automate learn protocol rule textual specifications ie rfcs evaluate automatically extract protocol rule apply state art fuzzer transport protocols show lead smaller number test case find attack system use manually specify rule
past work relation extraction mostly focus binary relation entity pair within single sentence recently nlp community gain interest relation extraction entity pair span multiple sentence paper propose novel architecture task inter sentential dependency base neural network idepnn idepnn model shortest augment dependency paths via recurrent recursive neural network extract relationships within intra across inter sentence boundaries compare svm neural network baselines idepnn robust false positives relationships span sentence evaluate model four datasets newswire muc6 medical bionlp share task domains achieve state art performance show better balance precision recall inter sentential relationships perform better eleven team participate bionlp share task two thousand and sixteen achieve gain fifty-two five hundred and eighty-seven vs five hundred and fifty-eight f1 win team also release crosssentence annotations muc6
high degree topical diversity often consider important characteristic interest text document recent proposal measure topical diversity identify three distributions assess diversity document distributions word within document word within topics topics within document topic model play central role approach hence quality crucial efficacy measure topical diversity quality topic model affect two cause generality impurity topics general topics include common information background corpus assign document impure topics contain word relate topic impurity lower interpretability topic model impure topics likely get assign document erroneously propose hierarchical estimation process aim remove generality impurity approach three estimation components one document estimation remove general word document two topic estimation estimate distribution word topic three topic assignment estimation estimate document distributions topics measure topical diversity text document hitr approach improve state art measure pubmed dataset
probabilistic topic model widely use discover latent topics document collections latent feature vector representations word use obtain high performance many nlp task paper extend two different dirichlet multinomial topic model incorporate latent feature vector representations word train large corpora improve word topic map learn smaller corpus experimental result show use information external corpora new model produce significant improvements topic coherence document cluster document classification task especially datasets short document
text style transfer aim modify style sentence keep content unchanged recent style transfer systems often fail faithfully preserve content change style paper propose structure content preserve model leverage linguistic information structure fine grain supervisions better preserve style independent content style transfer particular achieve goal devise rich model objectives base sentence lexical information language model condition content result model therefore encourage retain semantic mean target sentence perform extensive experiment compare model exist approach task sentiment political slant transfer model achieve significant improvement term content preservation style transfer automatic human evaluation
address problem efficient acoustic model refinement continuous retrain use semi supervise active learn low resource indian language wherein low resource constraints small label corpus train baseline seed acoustic model ii large train corpus without orthographic label perform data selection manual label low cost propose semi supervise learn decode unlabeled large train corpus use seed model various protocols select decode utterances high reliability use confidence level correlate wer decode utterances iterative bootstrapping propose active learn protocol use confidence level base metric select decode utterances large unlabeled corpus label semi supervise learn protocols offer wer reduction poorly train seed model much fifty best wer reduction realizable seed model wer large corpus label use acoustic model train active learn protocols allow sixty entire train corpus manually label reach performance entire data
generative adversarial network gans experience recent surge popularity perform competitively variety task especially computer vision however gin train show limit success natural language process largely sequence text discrete thus gradients propagate discriminator generator recent solutions use reinforcement learn propagate approximate gradients generator inefficient train propose utilize autoencoder learn low dimensional representation sentence gin train generate vectors space decode realistic utterances report random interpolate sample generator visualization sentence vectors indicate model correctly learn latent space autoencoder human rat bleu score show model generate realistic text competitive baselines
deep learn methods often difficult apply legal domain due large amount label data require deep learn methods recent new trend deep learn community application multi task model enable single deep neural network perform one task time example classification translation task powerful novel model capable transfer knowledge among different task train set therefore could open legal domain many deep learn applications paper investigate transfer learn capabilities multi task model classification task publicly available kaggle toxic comment dataset classify illegal comment report promise result
deep neural network data hungry model thus face difficulties attempt train small text datasets transfer learn potential solution effectiveness text domain explore areas image analysis paper study problem transfer learn text summarization discuss exist state art model fail generalize well unseen datasets propose reinforcement learn framework base self critic policy gradient approach achieve good generalization state art result variety datasets extensive set experiment also show ability propose framework fine tune text summarization model use train sample best knowledge first work study transfer learn text summarization provide generic solution work well unseen data
legal domain important differentiate word general afterwards link occurrences entities topic solve challenge call name entity link nel current supervise neural network design nel use publicly available datasets train test however paper focus especially aspect apply transfer learn approach use network train nel legal document experiment show consistent improvement legal datasets create european union law scope research use transfer learn approach reach f1 score nine thousand, eight hundred and ninety nine thousand, eight hundred and one legal small large test dataset
present trellis network new architecture sequence model one hand trellis network temporal convolutional network special structure characterize weight tie across depth direct injection input deep layer hand show truncate recurrent network equivalent trellis network special sparsity structure weight matrices thus trellis network general weight matrices generalize truncate recurrent network leverage connections design high perform trellis network absorb structural algorithmic elements recurrent convolutional model experiment demonstrate trellis network outperform current state art methods variety challenge benchmarks include word level language model character level language model task stress test design evaluate long term memory retention code available https githubcom locuslab trellisnet
idioms pose problems almost machine translation systems type language frequent day day language use simply ignore recent interest memory augment model field language model aid systems achieve good result bridge long distance dependencies paper explore use techniques neural machine translation system help translation idiomatic language
paper propose neural sequence sequence text speech tts model control latent attribute generate speech rarely annotate train data speak style accent background noise record condition model formulate conditional generative model base variational autoencoder vae framework two level hierarchical latent variables first level categorical variable represent attribute group eg clean noisy provide interpretability second level condition first multivariate gaussian variable characterize specific attribute configurations eg noise level speak rate enable disentangle fine grain control attribute amount use gaussian mixture model gmm latent distribution extensive evaluation demonstrate ability control aforementioned attribute particular train high quality controllable tts model real find data capable infer speaker style attribute noisy utterance use synthesize clean speech controllable speak style
give vector representations individual word necessary compute vector representations sentence many applications compositional manner often use artificial neural network relatively little work explore internal structure properties sentence vectors paper explore properties sentence vectors context automatic summarization particular show cosine similarity sentence vectors document vectors strongly correlate sentence importance vector semantics identify correct gap sentence choose far document addition identify specific dimension link effective summaries knowledge first time specific dimension sentence embeddings connect sentence properties also compare feature different methods sentence embeddings many insights applications use sentence embeddings far beyond summarization
digitalization legal domain ongoing couple years process application different machine learn ml techniques crucial task classification legal document contract clauses well translation highly relevant side digitize document barely accessible field particularly germany today deep learn dl one hot topics many publications various applications sometimes provide result outperform human level hence technique may feasible legal domain well however dl require thousands sample provide decent result potential solution problem multi task dl enable transfer learn approach may able overcome data scarcity problem legal domain specifically german language apply state art multi task model three task translation summarization multi label classification experiment conduct legal document corpora utilize several task combinations well various model parameters goal find optimal configuration task hand within legal domain multi task dl approach outperform state art result three task open new direction integrate dl technology efficiently legal domain
selection west java governor one event seize attention public exception social media users public opinion prospective regional leader help predict electability tendency voters data use opinion mine process obtain twitter data vary form unstructured must manage uninformed use data pre process techniques semi structure data semi structure information follow classification stage categorize opinion negative positive opinions research methodology use literature study research examine previous research similar topic purpose study find right architecture develop application twitter opinion mine know public sentiments toward election governor west java result research twitter opinion mine part text mine opinions twitter want classify must go preprocessing text stage first preprocessing step require twitter data cleanse case fold pos tag stem result text mine architecture architecture use text mine research different topics
recently due increase popularity social media necessity extract information informal text type microblog texts gain significant attention study focus name entity recognition ner problem informal text type turkish utilize semi supervise learn approach base neural network apply fast unsupervised method learn continuous representations word vector space make use obtain word embeddings together language independent feature engineer work better informal text type generate turkish ner system microblog texts evaluate turkish ner system twitter message achieve better f score performances publish result previously propose ner systems turkish tweet since employ language dependent feature believe method easily adapt microblog texts morphologically rich languages
many challenge natural language process require generate text include language translation dialogue generation speech recognition problems text generation become difficult text become longer current language model often struggle keep track coherence long piece text attempt model construct use outline text generate keep focus find usage outline improve perplexity find use outline improve human evaluation simpler baseline reveal discrepancy perplexity human perception similarly hierarchical generation find improve human evaluation score
sentence embeddings become essential part today natural language process nlp systems especially together advance deep learn methods although pre train sentence encoders available general domain none exist biomedical texts date work introduce biosentvec first open set sentence embeddings train thirty million document scholarly article pubmed clinical note mimic iii clinical database evaluate biosentvec embeddings two sentence pair similarity task different text genres benchmarking result demonstrate biosentvec embeddings better capture sentence semantics compare competitive alternatives achieve state art performance task expect biosentvec facilitate research development biomedical text mine complement exist resources biomedical word embeddings biosentvec publicly available https githubcom ncbi nlp biosentvec
sequence sequence model recently gain state art performance summarization however many large scale high quality datasets available almost available ones mainly news article specific write style moreover abstractive human style systems involve description content deeper level require data higher level abstraction paper present wikihow dataset two hundred and thirty thousand article summary pair extract construct online knowledge base write different human author article span wide range topics therefore represent high diversity style evaluate performance exist methods wikihow present challenge set baselines improve
paper introduce pydci new implementation distributional correspondence index dci write python dci transfer learn method cross domain cross lingual text classification provide implementation call jadci build top jatecs java framework text classification pydci stand alone version dci exploit scikit learn scipy stack report new experiment carry order test pydci use baselines new high perform methods appear dci originally propose experiment show thank subtle ways improve dci pydci outperform jadci mention high perform methods deliver best know result two popular benchmarks test dci ie multidomainsentiment aka mds cross domain adaptation webis cls ten cross lingual adaptation pydci together code allow replicate experiment available https githubcom alexmoreo pydci
convolutional neural network successfully apply various nlp task however obvious whether model different linguistic pattern negation intensification clause compositionality help decision make process paper apply visualization techniques observe model capture different linguistic feature feature affect performance model later try identify model errors source believe interpret cnns first step understand underlie semantic feature raise awareness improve performance explainability cnn model
deep learn model exhibit state art performance many predictive healthcare task use electronic health record ehr data model typically require train data volume exceed capacity healthcare systems external resources medical ontologies use bridge data volume constraint approach often directly applicable useful inconsistencies terminology solve data insufficiency challenge leverage inherent multilevel structure ehr data particular encode relationships among medical cod propose multilevel medical embed mime learn multilevel embed ehr data jointly perform auxiliary prediction task rely inherent ehr structure without need external label conduct two prediction task heart failure prediction sequential disease prediction mime outperform baseline methods diverse evaluation settings particular mime consistently outperform baselines predict heart failure datasets different volumes especially demonstrate greatest performance improvement fifteen relative gain pr auc best baseline smallest dataset demonstrate ability effectively model multilevel structure ehr data
document cluster text mine technique use provide better document search browse digital libraries online corpora lot research do biomedical document cluster base use exist ontology associations co occurrences medical concepts well represent use ontology research vector representation concepts diseases similarity measurement concepts propose identify closest concepts diseases context corpus document represent use vector space model weight scheme propose consider local content associations concepts self organize map use document cluster algorithm vector projection visualization feature som enable visualization analysis cluster distributions relationships two dimensional space experimental result show propose document cluster framework generate meaningful cluster facilitate visualization cluster base concepts diseases
mainstream caption model often follow sequential structure generate caption lead issue introduction irrelevant semantics lack diversity generate caption inadequate generalization performance paper present alternative paradigm image caption factorize caption procedure two stag one extract explicit semantic representation give image two construct caption base recursive compositional procedure bottom manner compare conventional ones paradigm better preserve semantic content explicit factorization semantics syntax use compositional generation procedure caption construction follow recursive structure naturally fit properties human language moreover propose compositional procedure require less data train generalize better yield diverse caption
introduce preco large scale english dataset coreference resolution dataset design embody core challenge coreference entity representation alleviate challenge low overlap train test set enable separate analysis mention detection mention cluster strengthen train test overlap collect large corpus 38k document 124m word mostly vocabulary english speak preschoolers experiment show higher train test overlap error analysis preco efficient one ontonotes popular exist dataset furthermore annotate singleton mention make possible first time quantify influence mention detector make coreference resolution performance dataset freely available https preschool labgithubio preco
exist attention mechanisms train attend individual items collection memory predefined fix granularity eg word token image grid propose area attention way attend areas memory area contain group items structurally adjacent eg spatially 2d memory image temporally 1d memory natural language sentence importantly shape size area dynamically determine via learn enable model attend information vary granularity area attention easily work exist model architectures multi head attention simultaneously attend multiple areas memory evaluate area attention two task neural machine translation character token level image caption improve upon strong state art baselines case improvements obtainable basic form area attention parameter free
present shoot relation classification dataset fewrel consist seventy zero sentence one hundred relations derive wikipedia annotate crowdworkers relation sentence first recognize distant supervision methods filter crowdworkers adapt recent state art shoot learn methods relation classification conduct thorough evaluation methods empirical result show even competitive shoot learn model struggle task especially compare humans also show range different reason skills need solve task result indicate shoot relation classification remain open problem still require research detail analysis point multiple directions future research detail resources dataset baselines release http zhuhaome fewrel
universal language model fine tune arxiv180106146 ulmfit one first nlp methods efficient inductive transfer learn unsupervised pretraining result improvements many nlp task english paper describe new method use subword tokenization adapt ulmfit languages high inflection approach result new state art polish language take first place task three poleval eighteen train final model outperform second best model thirty-five open source pretrained model code
word frequency base methods extractive summarization easy implement yield reasonable result across languages however significant limitations ignore role context offer uneven coverage topics document sometimes disjoint hard read use simple premise linguistic typology english sentence complete descriptors potential interactions entities usually order subject verb object address subset difficulties develop hybrid model extractive summarization combine word frequency base keyword identification information automatically generate entity relationship graph select sentence summaries comparative evaluation word frequency topic word base methods show propose method competitive conventional rouge standards yield moderately informative summaries average assess large panel n94 human raters
standard image caption task coco flickr30k factual neutral tone human state obvious eg man play guitar task useful verify machine understand content image engage humans caption mind define new task personality caption goal engage humans possible incorporate controllable style personality traits collect release large dataset two hundred and one thousand, eight hundred and fifty-eight caption condition two hundred and fifteen possible traits build model combine exist work sentence representations mazare et al two thousand and eighteen transformers train seventeen billion dialogue examples ii image representations mahajan et al two thousand and eighteen resnets train thirty-five billion social media image obtain state art performance flickr30k coco strong performance new task finally online evaluations validate task model engage humans best model close human performance
natural language process nlp important detect relationship two sequence generate sequence tokens give another observe sequence call type problems model sequence pair sequence sequence seq2seq map problems lot research devote find ways tackle problems traditional approach rely combination hand craft feature alignment model segmentation heuristics external linguistic resources although great progress make traditional approach suffer various drawbacks complicate pipeline laborious feature engineer difficulty domain adaptation recently neural network emerge promise solution many problems nlp speech recognition computer vision neural model powerful train end end generalise well unseen examples framework easily adapt new domain aim thesis advance state art seq2seq map problems neural network explore solutions three major aspects investigate neural model represent sequence model interactions sequence use unpaired data boost performance neural model aspect propose novel model evaluate efficacy various task seq2seq map
natural language process lot task successfully solve recurrent neural network model huge number parameters majority parameters often concentrate embed layer size grow proportionally vocabulary length propose bayesian sparsification technique rnns allow compress rnn dozens hundreds time without time consume hyperparameters tune also generalize model vocabulary sparsification filter unnecessary word compress rnn even show choice keep word interpretable code available github https githubcom tipt0p sparsebayesianrnn
present modular approach learn policies navigation long plan horizons language input hierarchical policy operate multiple timescales higher level master policy propose subgoals execute specialize sub policies choice subgoals compositional semantic ie sequentially combine arbitrary order assume human interpretable descriptions eg exit room find kitchen find refrigerator etc use imitation learn warm start policies level hierarchy dramatically increase sample efficiency follow reinforcement learn independent reinforcement learn level hierarchy enable sub policies adapt consequences action recover errors subsequent joint hierarchical train enable master policy adapt sub policies challenge eqa das et al two thousand and eighteen benchmark house3d wu et al two thousand and eighteen require navigate diverse realistic indoor environments approach outperform prior work significant margin term navigation question answer
tune machine learn model particularly deep learn architectures complex process automate hyperparameter tune algorithms often depend specific optimization metrics however many situations developer trade one metric another accuracy versus overfitting precision versus recall smaller model accuracy etc deep learn model representations opaque model behavior parameters knobs change may also unpredictable thus pick best model often require time consume model comparison work introduce lamvi two visual analytics system support developer compare hyperparameter settings outcomes focus word embed model deep learn text integrate view compare high level statistics well internal model behaviors eg compare word distance demonstrate developers work lamvi two quickly accurately narrow appropriate effective application specific model
widespread use internet size text data increase day day poems give example grow text study aim classify poetry accord poet firstly data set consist three different poetry poets write english construct text categorization techniques implement chi square technique use feature selection addition five different classification algorithms try algorithms sequential minimal optimization naive bay c45 decision tree random forest k nearest neighbor although classifier show different result seventy classification success rate take sequential minimal optimization technique
typical speak language understand systems provide narrow semantic parse use domain specific ontology parse contain intents slot directly consume downstream domain applications work discuss expand systems handle compound entities intents introduce domain agnostic shallow parser handle linguistic coordination show model parse coordination learn domain independent slot independent feature able segment conjunct boundaries many different phrasal categories also show use adversarial train effective improve generalization across different slot type coordination parse
belong first ten word use children embody first active form linguistic negation despite early occurrence detail acquisition process remain largely unknown circumstance construe label perceptible object events put outside scope modern account language acquisition moreover symbol ground architectures struggle grind word due non referential character experimental study involve child like humanoid robot icub design illuminate acquisition process negation word robot deploy several round speech wise unconstrained interaction nai participants act language teachers result corroborate hypothesis affect volition play pivotal role socially distribute acquisition process negation word prosodically salient within prohibitive utterances negative intent interpretations easily isolate teacher speech signal word subsequently may ground negative affective state however observations nature prohibitive act temporal relationships linguistic extra linguistic components raise serious question suitability hebbian type algorithms language ground
work aim improve semi supervise learn neural network architecture introduce hybrid supervise unsupervised cost function unsupervised component train use differentiable estimator maximum mean discrepancy mmd distance network output target dataset introduce notion n channel network several methods improve performance net base supervise pre initialization multi scale kernels work investigate effectiveness methods language translation quality translations know textita priori also present thorough investigation hyper parameter space method synthetic data
paper propose new loss use short time fourier transform stft spectra aim train high performance neural speech waveform model predict raw continuous speech waveform sample directly amplitude spectra also phase spectra obtain generate speech waveforms use calculate propose loss also mathematically show train waveform model basis propose loss interpret maximum likelihood train assume amplitude phase spectra generate speech waveforms follow gaussian von mises distributions respectively furthermore paper present simple network architecture speech waveform model compose uni directional long short term memories lstms auto regressive structure experimental result show propose neural model synthesize high quality speech waveforms
end end speech synthesis promise approach directly convert raw text speech although show tacotron2 outperform classical pipeline systems regard naturalness english applicability languages still unknown japanese could one difficult languages achieve end end speech synthesis largely due character diversity pitch accent therefore state art systems still base traditional pipeline framework require separate text analyzer duration model towards end end japanese speech synthesis extend tacotron systems self attention capture long term dependencies relate pitch accent compare audio quality classical pipeline systems various condition show pros con large scale listen test investigate impact presence accentual type label use force predict alignments acoustic feature use local condition parameters wavenet vocoder result reveal although propose systems still match quality top line pipeline system japanese show important step stone towards end end japanese speech synthesis
convert n dimensional vector probability distribution n object commonly use component many machine learn task like multiclass classification multilabel classification attention mechanisms etc several probability map function propose employ literature softmax sum normalization spherical softmax sparsemax little understand term relate none formulations offer explicit control degree sparsity address develop unify framework encompass formulations special case framework ensure simple close form solutions existence sub gradients suitable learn via backpropagation within framework propose two novel sparse formulations sparsegen lin sparsehourglass seek provide control degree desire sparsity develop novel convex loss function help induce behavior aforementioned formulations multilabel classification set show improve performance also demonstrate empirically propose formulations use compute attention weight achieve better comparable performance standard seq2seq task like neural machine translation abstractive summarization
automatic speech recognition asr task resolve end end deep learn model benefit us less preparation raw data easier transformation languages propose novel end end deep learn model architecture namely cascade cnn resbilstm ctc propose model add residual block bilstm layer extract sophisticate phoneme semantic information together apply cascade structure pay attention mine information hard negative sample apply simple fast fourier transform fft technique n gram language model lm rescoring method manage achieve word error rate wer three hundred and forty-one librispeech test clean corpora furthermore propose new batch vary method speed train process length vary task result twenty-five less train time
currently increase interest text speech tts synthesis use sequence sequence model attention model end end mean learn co articulation duration properties directly text speech since model entirely data drive need large amount data generate synthetic speech good quality however challenge speak style lombard speech difficult record sufficiently large speech corpora therefore study propose transfer learn method adapt sequence sequence base tts system normal speak style lombard style moreover experiment wavenet vocoder synthesis lombard speech conduct subjective evaluations assess performance adapt tts systems subjective evaluation result indicate adaptation system wavenet vocoder clearly outperform conventional deep neural network base tts system synthesis lombard speech
patient summarization essential clinicians provide coordinate care practice effective communication automate summarization potential save time standardize note aid clinical decision make reduce medical errors provide upper bind extractive summarization discharge note develop lstm model sequentially label topics history present illness note achieve f1 score eight hundred and seventy-six indicate model employ create dataset evaluation extractive summarization methods
question answer qa significantly benefit deep learn techniques recent years however domain specific qa remain challenge due significant amount data require train neural network paper study answer sentence selection task bible domain answer question select relevant verse bible purpose create new dataset bibleqa base bible trivia question propose three neural network model task pre train model large scale qa dataset squad investigate effect transfer weight model accuracy furthermore also measure model accuracies different answer context lengths different bible translations affirm transfer learn noticeable improvement model accuracy achieve relatively good result shorter context lengths whereas longer context lengths decrease model accuracy also find use modern bible translation dataset positive effect task
rich line research attempt make deep neural network transparent generate human interpretable explanations decision process especially interactive task like visual question answer vqa work analyze exist explanations indeed make vqa model responses well failures predictable human surprisingly find hand find human loop approach treat model black box
neural language model widely use various nlp task include machine translation next word prediction conversational agents however challenge deploy model mobile devices due slow prediction speed bottleneck compute top candidates softmax layer paper introduce novel softmax layer approximation algorithm exploit cluster structure context vectors algorithm use light weight screen model predict much smaller set candidate word base give context conduct exact softmax within subset train procedure end end challenge traditional cluster methods discrete non differentiable thus unable use back propagation train process use gumbel softmax able train screen model end end train set exploit data distribution algorithm achieve order magnitude faster inference original softmax layer predict top k word various task beam search machine translation next word prediction example machine translation task german english dataset around 25k vocabulary achieve two hundred and four time speed nine hundred and eighty-nine precision1 nine hundred and ninety-three precision5 original softmax layer prediction state art citepmsrprediction achieve 67x speedup nine hundred and eighty-seven precision1 nine hundred and eighty-one precision5 task
produce large amount annotate speech data train asr systems remain difficult ninety-five languages world low resourced however note human baby start learn language sound small number exemplar word without hear large amount data initiate preliminary work direction paper audio word2vec use obtain embeddings speak word carry phonetic information extract signal autoencoder use generate embeddings text word base articulatory feature phoneme sequence set embeddings speak text word describe similar phonetic structure among word respective latent space map relation audio embeddings text embeddings actually give word level asr learn align small number speak word correspond text word embed space initial experiment two hundred annotate speak word one hour speech data without annotation give word accuracy two hundred and seventy-five low good start point
mac net compositional attention network design visual question answer propose modify mac net architecture natural language question answer question answer typically require language understand multi step reason mac net unique architecture separation memory control facilitate data drive iterative reason make ideal candidate solve task involve logical reason experiment twenty babi task demonstrate value mac net data efficient interpretable architecture natural language question answer transparent nature mac net provide highly granular view reason step take network answer query
consider problem perform speak language understand slu small devices typical iot applications contributions twofold first outline design embed private design slu system show performance par cloud base commercial solutions second release datasets use experiment interest reproducibility hope prove useful slu community
attention important cognition process humans help humans concentrate critical information perception learn however although many machine learn model remember information data attention mechanism example long short term memory lstm network able remember sequential information pay special attention part sequence paper present novel model call long short term attention lsta seamlessly integrate attention mechanism inner cell lstm process long short term dependencies lsta focus important information sequence attention mechanism extensive experiment demonstrate lsta outperform lstm relate model sequence learn task
recurrent neural network rnn successfully apply many sequence learn problems handwrite recognition image description natural language process video motion analysis years development researchers improve internal structure rnn introduce many variants among others gate recurrent unit gru one widely use rnn model however gru lack capability adaptively pay attention certain regions locations may information redundancy loss lean paper propose rnn model call recurrent attention unit rau seamlessly integrate attention mechanism interior gru add attention gate attention gate enhance gru ability remember long term memory help memory cells quickly discard unimportant content rau capable extract information sequential data adaptively select sequence regions locations pay attention select regions learn extensive experiment image classification sentiment classification language model show rau consistently outperform gru baseline methods
evidence base medicine ebm define clinical question term specific patient problem aid physicians efficiently identify appropriate resources search best available evidence medical treatment order formulate well define focus clinical question framework call pico widely use identify sentence give medical text belong four components typically report clinical trials participants problem p intervention comparison c outcome work propose novel deep learn model recognize pico elements biomedical abstract base previous state art bidirectional long short term memory bilstm plus conditional random field crf architecture add another layer bilstm upon sentence representation vectors contextual information surround sentence gather help infer interpretation current one addition propose two methods generalize improve model adversarial train unsupervised pre train large corpora test propose approach two benchmark datasets one pubmed pico dataset best result outperform previous best fifty-five seventy-nine fifty-eight p elements term f1 score respectively dataset name nicta piboso improvements p elements twenty-four one hundred and thirty-six ten f1 score respectively overall propose deep learn model obtain unprecedented pico element detection accuracy avoid need manual feature selection
use simple command recognition devices smart routers mobile phone keyword spot systems everywhere ubiquitous well web applications grow popularity complexity last decade significant improvements usability cross platform condition however despite obvious advantage natural language interaction voice enable web applications still far work attempt bridge gap bring keyword spot capabilities directly browser knowledge first demonstrate fully functional implementation convolutional neural network pure javascript run standards compliant browser also apply network slimming model compression technique explore accuracy efficiency tradeoffs report latency measurements range devices software overall robust cross device implementation keyword spot realize new paradigm serve neural network applications one slim model reduce latency sixty-six minimal decrease accuracy four ninety-four ninety
ideological lean individual often gauge sentiment one express different issue propose simple framework represent political ideology distribution sentiment polarities towards set topics representation use detect ideological lean document speeches news article etc base sentiments express towards different topics experiment perform use widely use dataset show promise propose approach achieve comparable performance methods despite much simpler interpretable
pseudo relevance feedback prf commonly use boost performance traditional information retrieval ir model use top rank document identify weight new query term thereby reduce effect query document vocabulary mismatch neural retrieval model recently demonstrate strong result ad hoc retrieval combine prf straightforward due incompatibilities exist prf approach neural architectures bridge gap propose end end neural prf framework use exist neural ir model embed different neural model build block extensive experiment two standard test collections confirm effectiveness propose nprf framework improve performance two state art neural ir model
standard approach mitigate errors make automatic speech recognition system use confidence score associate predict word simplest case score word posterior probabilities whilst complex scheme utilise bi directional recurrent neural network birnn model number upstream downstream applications however rely confidence score assign one best hypotheses word find confusion network lattices include limit speaker adaptation semi supervise train information retrieval although word posteriors could use applications confidence score know reliability issue make improve confidence score generally available paper show birnns extend one best sequence confusion network lattice structure experiment conduct use one cambridge university submissions iarpa openkws two thousand and sixteen competition result show confusion network lattice base birnns provide significant improvement confidence estimation
standard approach assess reliability automatic speech transcriptions use confidence score accurate score provide flexible mechanism flag transcription errors upstream downstream applications one challenge type errors recognisers make deletions errors account standard confidence estimation scheme hard rectify upstream downstream process high deletion rat prominent limit resource highly mismatch train test condition study iarpa babel material program paper look use bidirectional recurrent neural network yield confidence estimate predict well delete word several simple scheme examine combination assess usefulness approach combine confidence score examine untranscribed data selection favour transcriptions lower deletion errors experiment conduct use iarpa babel material program languages
attacker may use variety techniques fool automatic speaker verification system accept genuine user anti spoof methods meanwhile aim make system robust attack asvspoof two thousand and seventeen challenge focus specifically replay attack intention measure limit replay attack detection well develop countermeasures work propose replay attack detection system attentive filter network compose attention base filter mechanism enhance feature representations frequency time domains resnet base classifier show network enable us visualize automatically acquire feature representations helpful spoof detection attentive filter network attain evaluation ever eight hundred and ninety-nine asvspoof two thousand and seventeen version twenty dataset system fusion best system obtain thirty relative improvement asvspoof two thousand and seventeen enhance baseline system
recent research show attention base sequence sequence model listen attend spell las yield comparable result state art asr systems various task paper describe development system demonstrate performance two task first achieve new state art word error rate three hundred and forty-three test clean subset librispeech english data second non native english speech include read speech spontaneous speech obtain competitive result compare conventional system build update kaldi recipe
speech chain mechanism integrate automatic speech recognition asr text speech synthesis tts modules single cycle train previous work apply speech chain mechanism semi supervise learn provide ability asr tts assist receive unpaired data let infer miss pair optimize model reconstruction loss speech without transcription asr generate likely transcription speech data tts use generate transcription reconstruct original speech feature however previous paper limit back propagation closest module tts part one reason back propagate error asr challenge due output asr discrete tokens create non differentiability tts asr paper address problem describe thoroughly train speech chain end end reconstruction loss use straight estimator st experimental result reveal sample st gumbel softmax able update asr parameters improve asr performances eleven relative cer reduction compare baseline
acoustics word model end end speech recognizers use word target without rely pronunciation dictionaries graphemes model notoriously difficult train due lack linguistic knowledge also unclear amount train data impact optimization generalization model work study optimization generalization acoustics word model different amount train data addition study three type inductive bias leverage pronunciation dictionary word boundary annotations constraints word durations find constrain word durations lead improvement finally analyze word embed space learn model find space structure dominate pronunciation word suggest contexts word instead phonetic structure future focus inductive bias acoustics word model
data sequential nature arise many application domains form eg textual data dna sequence software execution trace different research discipline develop methods learn sequence model datasets machine learn field methods hide markov model recurrent neural network develop successfully apply wide range task ii process mine process discovery techniques aim generate human interpretable descriptive model iii grammar inference field focus find descriptive model form formal grammars despite different focus field share common goal learn model accurately describe behavior underlie data sequence model generative ie predict elements likely occur give unfinished sequence far field develop mainly isolation comparison exist paper present interdisciplinary experimental evaluation compare sequence model techniques task next element prediction four real life sequence datasets result indicate machine learn techniques generally aim interpretability term accuracy outperform techniques process mine grammar inference field aim yield interpretable model
introduce improve variational autoencoder vae text model topic information explicitly model dirichlet latent variable provide propose model topic awareness superior reconstruct input texts furthermore due inherent interactions newly introduce dirichlet variable conventional multivariate gaussian variable model less prone kl divergence vanish derive variational lower bind new model conduct experiment four different data set result show propose model superior text reconstruction across latent space classifications learn representations higher test accuracies
introduce new method dolores learn knowledge graph embeddings effectively capture contextual cue dependencies among entities relations first note short paths knowledge graph comprise chain entities relations encode valuable information regard contextual usage operationalize notion represent knowledge graph collection triple collection entity relation chain learn embeddings entities relations use deep neural model capture contextual usage particular model base bi directional lstms learn deep representations entities relations construct entity relation chain show representations easily incorporate exist model significantly advance state art several knowledge graph prediction task like link prediction triple classification miss relation type prediction case least ninety-five
build explainable systems critical problem field natural language process nlp since machine learn model provide explanations predictions exist approach explainable machine learn systems tend focus interpret output connections input output however fine grain information often ignore systems explicitly generate human readable explanations better alleviate problem propose novel generative explanation framework learn make classification decisions generate fine grain explanations time specifically introduce explainable factor minimum risk train approach learn generate reasonable explanations construct two new datasets contain summaries rat score fine grain reason conduct experiment datasets compare several strong neural network baseline systems experimental result show method surpass baselines datasets able generate concise explanations time
many knowledge graph embed methods operate triple therefore implicitly limit local view entire knowledge graph present new framework mohone effectively model higher order network effect knowledge graph thus enable one capture vary degrees network connectivity local global framework generic explicitly model network scale capture two different aspects similarity network share local neighborhood b structural role base similarity first introduce methods learn network representations entities knowledge graph capture vary aspects similarity propose fast efficient method incorporate information capture network representations exist knowledge graph embeddings show method consistently significantly improve performance link prediction several different knowledge graph embed methods include transe transd distmult complexby least four point seventeen case
paper address problem incremental domain adaptation ida natural language process nlp assume domain come one another could access data current domain goal ida build unify model perform well domains encounter adopt recurrent neural network rnn widely use nlp augment directly parameterized memory bank retrieve attention mechanism step rnn transition memory bank provide natural way ida adapt model new domain progressively add new slot memory bank increase number parameters thus model capacity learn new memory slot fine tune exist parameters back propagation experimental result show approach achieve significantly better performance fine tune alone compare expand hide state approach robust old domains show empirical theoretical result model also outperform previous work ida include elastic weight consolidation progressive neural network experiment
investigate unsupervised model map variable duration speech segment fix dimensional representation settings unlabelled speech available resource acoustic word embeddings form basis zero resource speech search discovery index systems exist unsupervised embed methods still use supervision word phoneme boundaries propose encoder decoder correspondence autoencoder encdec cae instead true word segment use automatically discover segment unsupervised term discovery system find pair word unknown type encdec cae train reconstruct one word give input compare standard encoder decoder autoencoder ae variational ae prior latent embed downsampling encdec cae outperform closest competitor twenty-four relative average precision two languages word discrimination task
help enforce data protection regulations gdpr detect unauthorized use personal data develop new emphmodel audit technique help users check data use train machine learn model focus audit deep learn model generate natural language text include word prediction dialog generation model core popular online service often train personal data users message search chat comment design evaluate black box audit method detect query model particular user texts use train among thousands users empirically show method successfully audit well generalize model overfitted train data also analyze text generation model memorize word sequence explain memorization make amenable audit
deep learn model become state art natural language process nlp task however deploy model production system pose significant memory constraints exist compression methods either lossy introduce significant latency propose compression method leverage low rank matrix factorization trainingto compress word embed layer represent size bottleneck nlp model model train compress train downstream task recover accuracy maintain reduce size empirically show propose method achieve ninety compression minimal impact accuracy sentence classification task outperform alternative methods like fix point quantization offline word embed compression also analyze inference time storage space method flop calculations show compress dnn model configurable ratio regain accuracy loss without introduce additional latency compare fix point quantization finally introduce novel learn rate schedule cyclically anneal learn rate calr empirically demonstrate outperform popular adaptive learn rate algorithms sentence classification benchmark
deep neural network remarkably strong generalization performances usually parameterized despite explicit regularization strategies use practitioners avoid fit impact often small theoretical study analyze implicit regularization effect stochastic gradient descent sgd simple machine learn model certain assumptions however behave practically state art model real world datasets still unknown bridge gap study role sgd implicit regularization deep learn systems show pure sgd tend converge minimas better generalization performances multiple natural language process nlp task phenomenon coexist dropout explicit regularizer addition neural network finite learn capability impact intrinsic nature sgd implicit regularization effect specifically limit train sample certain corrupt label implicit regularization effect remain strong analyze stability vary weight initialization range corroborate experimental find decision boundary visualization use three layer neural network interpretation altogether work enable deepen understand implicit regularization affect deep learn model shed light future study parameterized model generalization ability
build accurate automatic speech recognition asr system require large dataset contain many hours label speech sample produce diverse set speakers lack open free datasets one main issue prevent advancements asr research address problem propose augment natural speech dataset synthetic speech train large end end neural speech recognition model use librispeech dataset augment synthetic speech new model achieve state art word error rate wer character level base model without external language model
recently considerable research effort devote develop deep architectures topic model learn topic structure although several deep model propose learn better topic proportion document leverage benefit deep structure learn word distributions topics yet rigorously study propose new multi layer generative process word distributions topics layer consist set topics topic draw mixture topics layer topics layer directly interpret word propose model able discover interpretable topic hierarchies self contain module model flexibly adapt different kinds topic model improve model accuracy interpretability extensive experiment text corpora demonstrate advantage propose model
volume text base document increase day day medical document locate within grow text document study techniques use text classification apply medical document evaluate classification performance use data set multi class multi label chi square chi technique use feature selection also smo nb c45 rf knn algorithms use classification aim study success various classifiers evaluate multi class multi label data set consist medical document first four hundred feature successful knn classifier feature number four hundred smo become successful classifier
answer question people often draw upon rich world knowledge addition particular context recent work focus primarily answer question give relevant document context require little general background investigate question answer prior knowledge present commonsenseqa challenge new dataset commonsense question answer capture common sense beyond associations extract conceptnet speer et al two thousand and seventeen multiple target concepts semantic relation single source concept crowd workers ask author multiple choice question mention source concept discriminate turn target concepts encourage workers create question complex semantics often require prior knowledge create twelve thousand, two hundred and forty-seven question procedure demonstrate difficulty task large number strong baselines best baseline base bert large devlin et al two thousand and eighteen obtain fifty-six accuracy well human performance eighty-nine
paper carry empirical analysis various dropout techniques language model bernoulli dropout gaussian dropout curriculum dropout variational dropout concrete dropout moreover propose extension variational dropout concrete dropout curriculum dropout vary schedule find extensions perform well compare standard dropout approach particularly variational curriculum dropout linear schedule largest performance increase make apply dropout decoder layer lastly analyze errors occur test time post analysis step determine well know problem compound errors apparent end propose methods mitigate issue dataset report result two hide layer lstm gru highway network embed dropout dropout gate hide layer output projection layer model report result penn treebank wikitext two word level language model datasets former reduce long tail distribution preprocessing one preserve rare word train test set
recurrent neural network rnns theoretically turing complete establish dominant model language process yet still remain uncertainty regard language learn capabilities paper empirically evaluate inductive learn capabilities long short term memory network popular extension simple rnns learn simple formal languages particular anbn anbncn anbncndn investigate influence various aspects learn train data regimes model capacity generalization unobserved sample find strike differences model performances different train settings highlight need careful analysis assessment make claim learn capabilities neural network model
work address problem modify textual attribute sentence give input sentence set attribute label attempt generate sentence compatible condition information ensure model generate content compatible sentence introduce reconstruction loss interpolate auto encode back translation loss components propose adversarial loss enforce generate sample attribute compatible realistic quantitative qualitative human evaluations demonstrate model capable generate fluent sentence better reflect condition information compare prior methods demonstrate model capable simultaneously control multiple attribute
machine translation highly sensitive size quality train data lead increase interest collect filter large parallel corpora paper propose new method task base multilingual sentence embeddings contrast previous approach rely nearest neighbor retrieval hard threshold cosine similarity propose method account scale inconsistencies measure consider margin give sentence pair closest candidates instead experiment show large improvements exist methods outperform best publish result bucc mine task un reconstruction task ten f1 thirty precision point respectively filter english german paracrawl corpus approach obtain three hundred and twelve bleu point newstest2014 improvement one point best official filter version
paper address task relation mention extraction noisy data extract representative phrase particular relation noisy sentence collect via distant supervision despite significance value many downstream applications task less study noisy data major challenge exist one lack annotation mention phrase severely two handle noisy sentence express relation address two challenge formulate task semi markov decision process propose novel hierarchical reinforcement learn model model consist top level sentence selector remove noisy sentence low level mention extractor extract relation mention reward estimator provide signal guide data denoising mention extraction without explicit annotations experimental result show model effective extract relation mention noisy data
introduce simplervoice key message visual description generator system help low literate adults navigate information dense world confidence simplervoice automatically generate sensible sentence describe unknown object extract semantic mean object usage form query string represent string multiple type visual guidance picture pictographs etc demonstrate simplervoice system case study generate grocery products manuals mobile application evaluate conduct user study simplervoice generate description comparison information interpret users methods original product package search engines top result simplervoice achieve highest performance score four hundred and eighty-two five point mean opinion score scale result show simplervoice able provide low literate end users simple yet informative components help understand use grocery products system may potentially provide benefit real world use case
adversarial examples define input model induce mistake model output different oracle perhaps surprise malicious ways original model adversarial attack primarily study context classification computer vision task several attack propose natural language process nlp settings often vary define parameters attack successful attack would look like goal work propose unify model adversarial examples suitable nlp task generative classification settings define notion adversarial gain base control theory measure change output system relative perturbation input cause call adversary present learner definition show use different feature space distance condition determine attack defense effectiveness across different intuitive manifold notion adversarial gain provide useful way evaluate adversaries defenses act build block future work robustness adversaries due root nature stability manifold theory
present framework build speech text translation st systems use monolingual speech text corpora word speech utterances source language independent text target language oppose traditional cascade systems end end architectures system require label data ie transcribe source audio parallel source target text corpora train make especially applicable language pair even zero bilingual resources framework initialize st system cross modal bilingual dictionary infer monolingual corpora map every source speech segment correspond speak word target text translation unseen source speech utterances system first perform word word translation speech segment utterance translation improve leverage language model sequence denoising autoencoder provide prior knowledge target language experimental result show unsupervised system achieve comparable bleu score supervise end end model despite lack supervision also provide ablation analysis examine utility component system
conditional random field crfs show one successful approach sequence label various linear chain neural crfs ncrfs develop implement non linear node potentials crfs still keep linear chain hide structure paper propose ncrf transducers consist two rnns one extract feature observations capture theoretically infinite long range dependencies label different sequence label methods evaluate pos tag chunk ner english dutch experiment result show ncrf transducers achieve consistent improvements linear chain ncrfs rnn transducers across four task improve state art result
paper present method train end end automatic speech recognition asr model use unpaired data although end end approach eliminate need expert knowledge pronunciation dictionaries build asr systems still require large amount pair data ie speech utterances transcriptions cycle consistency losses recently propose way mitigate problem limit pair data approach compose reverse operation give transformation eg text speech tts asr build loss require unsupervised data speech example apply cycle consistency asr model trivial since fundamental information speaker traits lose intermediate text bottleneck solve problem work present loss base speech encoder state sequence instead raw speech signal achieve train text encoder model define loss base encoder reconstruction error experimental result librispeech corpus show propose cycle consistency train reduce word error rate one hundred and forty-seven initial model train one hundred hour pair data use additional three hundred and sixty hours audio data without transcriptions also investigate use text data mainly language model improve performance unpaired data train scenario
describe approach grammatical error correction gec effective make use model train large amount weakly supervise bitext train transformer sequence sequence model 4b tokens wikipedia revisions employ iterative decode strategy tailor loosely supervise nature wikipedia train corpus finetuning lang eight corpus ensembling yield f05 five hundred and eighty-three conll fourteen benchmark gleu six hundred and twenty-four jfleg combination weakly supervise train iterative decode obtain f05 four hundred and eighty-two conll fourteen even without use label gec data
celebrate word2vec technique yield semantically rich representations individual word relatively less success extend generate unsupervised sentence document embeddings recent work demonstrate distance measure document call emphword mover distance wmd align semantically similar word yield unprecedented knn classification accuracy however wmd expensive compute hard extend use beyond knn classifier paper propose emphword mover embed wme novel approach build unsupervised document sentence embed pre train word embeddings experiment nine benchmark text classification datasets twenty-two textual similarity task propose technique consistently match outperform state art techniques significantly higher accuracy problems short length
introduce new benchmark coreference resolution nli knowref target common sense understand world knowledge previous coreference resolution task largely solve exploit number gender antecedents handcraft reflect diversity naturally occur text present corpus eight thousand annotate text passages ambiguous pronominal anaphora instance challenge realistic show various coreference systems whether rule base feature rich neural perform significantly worse task humans display high inter annotator agreement explain performance gap show empirically state art model often fail capture context instead rely gender number candidate antecedents make decision use problem specific insights propose data augmentation trick call antecedent switch alleviate tendency model finally show antecedent switch yield promise result task well use achieve state art result gap coreference task
recent study significantly improve state art common sense reason csr benchmarks like winograd schema challenge wsc swag question ask paper whether improve performance benchmarks represent genuine progress towards common sense enable systems make case study benchmarks design protocols clarify qualify result previous work analyze threats validity previous experimental design protocols account several properties prevalent common sense benchmarks include size limitations structural regularities variable instance difficulty
summarization long sequence concise statement core problem natural language process require non trivial understand input base promise result graph neural network highly structure data develop framework extend exist sequence encoders graph component reason long distance relationships weakly structure data text extensive evaluation show result hybrid sequence graph model outperform pure sequence model well pure graph model range summarization task
increase need automate system log analysis tool large scale online system timely manner however conventional way monitor classify log output base keyword list scale well complex system cod contribute large group developers diverse ways encode error message often mislead pre set label paper propose design large scale online log analysis follow least prior knowledge principle unsupervised semi supervise solution minimal prior knowledge log encode directly thereby report experience design two stage machine learn base method system log regard output quasi natural language pre filter perplexity score threshold undergo fine grain classification procedure test empirical data show method obvious advantage regard process speed classification accuracy
classification task usually analyse improve new model architectures hyperparameter optimisation underlie properties datasets discover ad hoc basis errors occur however understand properties data crucial perfect model paper analyse exactly characteristics dataset best determine difficult dataset task text classification propose intuitive measure difficulty text classification datasets simple fast calculate show measure generalise unseen data compare state art datasets result measure use analyse precise source errors dataset allow fast estimation difficult dataset learn search measure train twelve classical neural network base model seventy-eight real world datasets use genetic algorithm discover best measure difficulty difficulty calculate code https githubcom wluper edm datasets http datawlupercom publicly available
extract common narratives multi author dynamic text corpora require complex model dynamic author persona dap topic model however model complex struggle scale large corpora often challenge non conjugate term overcome challenge paper adapt new ideas approximate inference dap model result dap perform exceedingly rapidly dapper topic model specifically develop conjugate computation variational inference cvi base variational expectation maximization learn model yield fast close form update document replace iterative optimization earlier work result show significant improvements model fit train time without need compromise model temporal structure application regularize variation inference rvi demonstrate scalability effectiveness dapper model extract health journey caringbridge corpus collection nine million journals write two hundred thousand author health crises
end end speech translation st model many potential advantage compare cascade automatic speech recognition asr text machine translation mt model include lower inference latency avoidance error compound however quality end end st often limit paucity train data since difficult collect large parallel corpora speech translate transcript pair previous study propose use pre train components multi task learn order benefit weakly supervise train data speech transcript text foreign text pair paper demonstrate use pre train mt text speech tts synthesis model convert weakly supervise data speech translation pair st train effective multi task learn furthermore demonstrate high quality end end st model train use weakly supervise datasets synthetic data source unlabeled monolingual text speech use improve performance finally discuss methods avoid overfitting synthetic speech quantitative ablation study
recently end end model become popular approach alternative traditional hybrid model automatic speech recognition asr multi speaker speech separation recognition task central task cocktail party problem paper present state art monaural multi speaker end end automatic speech recognition model contrast previous study monaural multi speaker speech recognition end end framework train recognize multiple label sequence completely scratch system require speech mixture correspond label sequence without need indeterminate supervisions obtain non mixture speech correspond label alignments moreover exploit use individual attention module separate speaker schedule sample improve performance finally evaluate propose model two speaker mix speech generate wsj corpus wsj0 2mix dataset speech separation recognition benchmark experiment demonstrate propose methods improve performance end end model separate overlap speech recognize separate stream result propose model lead one hundred relative performance gain term cer wer respectively
recently speaker embeddings extract deep neural network become state art method speaker verification paper aim facilitate implementation generic toolkit kaldi anticipate enable improvements method examine several trick train effect normalize input feature pool statistics different methods prevent overfitting well alternative non linearities use instead rectifier linear units addition investigate difference performance tdnn cnn two type attention mechanism experimental result speaker wild sre two thousand and sixteen sre two thousand and eighteen datasets demonstrate effectiveness propose implementation
propose prosody embeddings emotional expressive speech synthesis network propose methods introduce temporal structure embed network thus enable fine grain control speak style synthesize speech temporal structure design either speech side text side lead different control resolutions time prosody embed network plug end end speech synthesis network train without supervision except target speech synthesize demonstrate prosody embed network learn extract prosodic feature adjust learn prosody feature could change pitch amplitude synthesize speech frame level phoneme level also introduce temporal normalization prosody embeddings show better robustness speaker perturbations prosody transfer task
paper propose neural phrase phrase machine translation np2mt model use phrase attention mechanism discover relevant input source segment use decoder generate output target phrase also design efficient dynamic program algorithm decode segment allow model train faster exist neural phrase base machine translation method huang et al two thousand and eighteen furthermore method naturally integrate external phrase dictionaries decode empirical experiment show method achieve comparable performance state art methods benchmark datasets however train test data different distributions domains method perform better
many speech enhancement methods try learn relationship noisy clean speech obtain use acoustic room simulator point several limitations enhancement methods rely clean speech target goal work propose alternative learn algorithm call acoustic adversarial supervision aas aas make enhance output maximize likelihood transcription pre train acoustic model general characteristics clean speech improve generalization unseen noisy speeches employ connectionist temporal classification unpaired conditional boundary equilibrium generative adversarial network loss function aas aas test two datasets include additive noise without reverberation librispeech demand chime four visualize enhance speech different loss combinations demonstrate role supervision aas achieve lower word error rate state art methods use clean speech target datasets
paper address problem enhance speech speaker interest cocktail party scenario visual information speaker interest available contrary previous study learn visual feature typically small audio visual datasets use already available face landmark detector train separate image dataset landmarks use lstm base model generate time frequency mask apply acoustic mix speech spectrogram result show landmark motion feature effective feature task ii similarly previous work reconstruction target speaker spectrogram mediate mask significantly accurate direct spectrogram reconstruction iii best mask depend motion landmark feature input mix speech spectrogram best knowledge propose model first model train evaluate limit size grid tcd timit datasets achieve speaker independent speech enhancement multi talker set
ability compose part form complex whole analyze whole combination elements desirable across discipline workshop bring together researchers apply compositional approach physics nlp cognitive science game theory within nlp long stand aim represent word combine form phrase sentence within framework distributional semantics word represent vectors vector space categorical model coecke et al two thousand and ten inspire quantum protocols provide convince account compositionality vector space model nlp furthermore history vector space model cognitive science theories categorization develop nosofsky one thousand, nine hundred and eighty-six smith et al one thousand, nine hundred and eighty-eight utilise notions distance feature vectors recently gardenfors two thousand and four two thousand and fourteen develop model concepts conceptual space provide geometric structure information represent point vectors regions vector space compositional approach apply formalism give conceptual space theory richer model compositionality previously bolt et al two thousand and eighteen compositional approach also apply study strategic game nash equilibria contrast classical game theory game study monolithically one global object compositional game theory work bottom build large complex game smaller components approach inherently difficult since interaction game consider research categorical compositional methods field recently begin ghani et al two thousand and eighteen moreover interaction three discipline cognitive science linguistics game theory fertile grind research game theory cognitive science well establish area camerer two thousand and eleven similarly game theoretic approach apply linguistics jager two thousand and eight lastly study linguistics cognitive science intimately intertwine smolensky legendre two thousand and six jackendoff two thousand and seven physics supply compositional approach via vector space categorical quantum theory allow interplay three discipline examine
casual conversations involve multiple speakers noise surround devices common everyday environments degrade performances automatic speech recognition systems challenge characteristics environments target chime five challenge employ convolutional neural network cnn base multichannel end end speech recognition system study attempt overcome present difficulties everyday environments system comprise attention base encoder decoder neural network directly generate text output sound input multichannel cnn encoder use residual connections batch renormalization train augment data include white noise injection experimental result show word error rate reduce eighty-five six absolute single channel end end best baseline lf mmi tdnn chime five corpus respectively
although promise result achieve video caption exist model limit fix inventory activities train corpus generalize open vocabulary scenarios introduce novel task zero shoot video caption aim describe domain videos unseen activities videos different activities usually require different caption strategies many aspects ie word selection semantic construction style expression etc pose great challenge depict novel activities without pair train data meanwhile similar activities share aspects common therefore propose principled topic aware mixture experts tamoe model zero shoot video caption learn compose different experts base different topic embeddings implicitly transfer knowledge learn see activities unseen ones besides leverage external topic relate text corpus construct topic embed activity embody relevant semantic vectors within topic empirical result validate effectiveness method utilize semantic knowledge video caption also show strong generalization ability describe novel activities
paper present promise accurate prefix boost papb discriminative train technique attention base sequence sequence seq2seq asr papb devise unify train test scheme effective manner train procedure involve maximize score partial correct sequence obtain beam search compare hypotheses train objective also include minimization token character error rate papb show efficacy achieve one hundred and eight thirty-eight wer without rnnlm respectively wall street journal dataset
deep autoregressive sequence sequence model demonstrate impressive performance across wide variety task recent years common architecture class recurrent convolutional self attention network make different trade off amount computation need per layer length critical path train time generation still remain inherently sequential process overcome limitation propose novel blockwise parallel decode scheme make predictions multiple time step parallel back longest prefix validate score model allow substantial theoretical improvements generation speed apply architectures process output sequence parallel verify approach empirically series experiment use state art self attention model machine translation image super resolution achieve iteration reductions 2x baseline greedy decoder loss quality 7x exchange slight decrease performance term wall clock time fastest model exhibit real time speedups 4x standard greedy decode
recent study show frame level deep speaker feature derive deep neural network train target set discriminate speakers short speech segment pool frame level feature utterance level representations call vectors derive use automatic speaker verification asv task simple average pool however inherently sensitive phonetic content utterance interest idea borrow machine translation attention base mechanism contribution input word translation particular time weight attention score score reflect relevance input word present translation use idea align utterances different phonetic content paper propose phonetic attention score approach vector systems approach attention score compute frame pair score reflect similarity two frame phonetic content use weigh contribution frame pair utterance base score new score approach emphasize frame pair similar phonetic content essentially provide soft alignment utterances phonetic content experimental result show compare naive average pool phonetic attention score approach deliver consistent performance improvement asv task text dependent text independent
neural model particular vector x vector architectures produce state art performance many speaker verification task however two potential problems neural model deserve investigation firstly model suffer information leak mean parameters participate model train discard inference ie layer use classifier secondly model regulate distribution derive speaker vectors unconstrained distribution may degrade performance subsequent score component eg plda paper propose gaussian constrain train approach one discard parametric classifier two enforce distribution derive speaker vectors gaussian experiment voxceleb sitw databases demonstrate new train approach produce representative regular speaker embeddings lead consistent performance improvement
paper compare classical copy quantum entanglement natural language consider case verb phrase vp ellipsis vp ellipsis non linear linguistic phenomenon require reuse resources make ideal test case comparative study different copy behaviours compositional model natural language follow line research compositional distributional semantics set coecke et al two thousand and ten develop extension lambek calculus admit control form contraction deal copy linguistic resources develop two different compositional model distributional mean calculus first model follow categorical approach coecke et al two thousand and thirteen functorial passage send proof grammar linear map vector space use frobenius algebras allow copy second case follow traditional approach one find categorial grammars whereby intermediate step interpret proof non linear lambda term use multiple variable occurrences model classical copy case study apply model derive different read ambiguous elliptical phrase compare analyse model provide
categorical compositional distributional semantics provide method derive mean sentence mean individual word grammatical reduction sentence automatically induce linear map compose word vectors obtain distributional semantics paper extend passage word sentence sentence discourse composition achieve introduce notion basic anaphoric discourse mid level representation natural language discourse formalise term basic discourse representation structure drs knowledge base query semantic web describe basic graph pattern resource description framework rdf provide high level specification compositional algorithms question answer anaphora resolution allow us give picture natural language understand process involve statistical logical resources
dairector automate director collaborate humans storytellers live improvisational performances write assistance dairector use create short narrative arc contextual plot generation work present system architecture quantitative evaluation design choices case study usage system provide qualitative feedback professional improvisational performer present relevant metrics understudy domain human machine creative generation specifically long form narrative creation include alongside publication open source code others may test evaluate run dairector
paper investigate applications various multilingual approach develop conventional hide markov model hmm systems sequence sequence seq2seq automatic speech recognition asr set compose babel data first show effectiveness multi lingual train stack bottle neck sbn feature explore various architectures train strategies multi lingual seq2seq model base ctc attention network include combinations output layer ctc attention component train also investigate effectiveness language transfer learn low resource scenario target language include original multi lingual train data interestingly find multilingual feature superior multilingual model find suggest efficiently combine benefit hmm system seq2seq system multilingual feature techniques
consider technology assist mimicry attack context automatic speaker verification asv use asv select target speakers attack human base mimicry record six naive mimic select target celebrities voxceleb1 voxceleb2 corpora seven thousand, three hundred and sixty-five potential target use vector system attacker attempt mimic select target utterances subject asv test use independently develop x vector system main find negative even attacker score target speakers slightly increase mimic succeed spoof x vector system interestingly however relative order select target closest furthest median consistent systems suggest level transferability systems
imagine robot show new concepts visually together speak tag eg milk egg butter see one pair audio visual example per class show new set unseen instance object ask pick milk without receive hard label could learn match new continuous speech input correct visual instance although unimodal one shoot learn study one label example single modality give per class example motivate multimodal one shoot learn main contribution formally define task propose several baseline advance model use dataset pair speak visual digits specifically investigate recent advance siamese convolutional neural network best siamese model achieve twice accuracy nearest neighbour model use pixel distance image dynamic time warp speech eleven way cross modal match
propose decaprop densely connect attention propagation new densely connect neural architecture read comprehension rc two distinct characteristics model firstly model densely connect pairwise layer network model relationships passage query across hierarchical level secondly dense connectors network learn via attention instead standard residual skip connectors end propose novel bidirectional attention connectors bac efficiently forge connections throughout network conduct extensive experiment four challenge rc benchmarks propose approach achieve state art result four outperform exist baselines twenty-six one hundred and forty-two absolute f1 score
understand procedural text require track entities action effect narrative unfold focus challenge real world problem action graph extraction material science paper language highly specialize data annotation expensive scarce propose novel approach text2quest procedural text interpret instructions interactive game learn agent complete game execute procedure correctly text base simulate lab environment framework complement exist approach enable richer form learn compare static texts discuss potential limitations advantage approach release prototype proof concept hop encourage research direction
introduce end end neural network base model simulate users task orient dialogue systems user simulation dialogue systems crucial two different perspectives automatic evaluation different dialogue model ii train task orient dialogue systems design hierarchical sequence sequence model first encode initial user goal system turn fix length representations use recurrent neural network rnn encode dialogue history use another rnn layer turn user responses decode hide representations dialogue level rnn hierarchical user simulator hus approach allow model capture undiscovered part user goal without need explicit dialogue state track develop several variants utilize latent variable model inject random variations user responses promote diversity simulate user responses novel goal regularization mechanism penalize divergence user responses initial user goal evaluate propose model movie ticket book domain systematically interact user simulator various dialogue system policies train different objectives users
generate paraphrase different variations sentence convey mean important yet challenge task nlp automatically generate paraphrase utility many nlp task like question answer information retrieval conversational systems name paper introduce iterative refinement generate paraphrase within vae base generation framework current sequence generation model lack capability one make improvements sentence generate two rectify errors make decode propose technique iteratively refine output use multiple decoders one attend output sentence generate previous decoder improve current state art result significantly nine twenty-eight absolute increase meteor score quora question pair mscoco datasets respectively also show qualitatively examples decode approach generate better paraphrase compare single decoder rectify errors make improvements paraphrase structure induce variations introduce new semantically coherent information
attention base encoder decoder network use leave right beam search algorithm inference step current beam search expand hypotheses traverse expand hypotheses next time step traversal implement use loop program general lead speed recognition process paper propose parallelism technique beam search accelerate search process vectorizing multiple hypotheses eliminate loop program also propose technique batch multiple speech utterances line recognition use reduce loop program regard traverse multiple utterances extension trivial beam search unlike train due several prune thresholding techniques efficient decode addition method combine score external modules rnnlm ctc batch shallow fusion achieve thirty-seven x speedup compare original beam search algorithm vectoring hypotheses achieve one hundred and five x speedup change process unit gpu
ever expand volumes biomedical text require automate semantic annotation techniques curate put best use establish field research seek link mention text knowledge base include umls unify medical language system order enable sophisticate understand work yield good result task curating literature increasingly annotation systems broadly apply medical vocabularies expand size extent term ambiguity document collections increase size complexity create greater need speed robustness furthermore technologies turn new task requirements change example greater coverage expressions may require order annotate patient record greater accuracy may need applications affect patients place new demand approach currently use work present new system bio yodie compare two popular systems order give guidance suitable approach different scenarios systems might design accommodate future need
study across many discipline show lexical choice affect audience perception example users describe social media profile affect perceive socio economic status however lack general methods estimate causal effect lexical choice perception specific sentence randomize control trials may provide good estimate scale potentially millions comparisons necessary consider lexical choices instead paper first offer two class methods estimate effect perception change one word another give sentence first class algorithms build upon quasi experimental design estimate individual treatment effect observational data second class treat treatment effect estimation classification problem conduct experiment three data source yelp twitter airbnb find algorithmic estimate align well produce randomize control trials additionally find possible transfer treatment effect classifiers across domains still maintain high accuracy
automatic speech recognition asr use multiple microphone array achieve great success far field robustness take advantage information array share contribute crucial task motivate advance joint connectionist temporal classification ctc attention mechanism end end e2e asr stream attention base multi array framework propose work microphone array act information stream activate separate encoders decode instruction ctc attention network term attention hierarchical structure adopt top regular attention network stream attention introduce steer decoder toward informative encoders experiment conduct ami dirha multi array corpora use encoder decoder architecture compare best single array result propose framework achieve relative word error rat wers reduction thirty-seven ninety-seven two datasets respectively better conventional strategies well
word embed techniques heavily rely abundance train data individual word give zipfian distribution word natural language texts large number word usually appear frequently train data paper put forward technique exploit knowledge encode lexical resources wordnet induce embeddings unseen word approach adapt graph embed cross lingual vector space transformation techniques order merge lexical knowledge encode ontologies derive corpus statistics show approach provide consistent performance improvements across multiple evaluation benchmarks vitro multiple rare word similarity datasets vivo two downstream text classification task
explore blindfold question baselines embody question answer embodiedqa task require agent answer question intelligently navigate simulate environment gather necessary visual information first person vision finally answer consequently blindfold baseline ignore environment visual information degenerate solution yet show experiment eqav1 dataset simple question baseline achieve state art result embodiedqa task case except agent spawn extremely close object
end end approach draw much attention recently significantly simplify construction automatic speech recognition asr system rnn transducer rnn one popular end end methods previous study show rnn difficult train complex train process need reasonable performance paper explore rnn chinese large vocabulary continuous speech recognition lvcsr task aim simplify train process maintain performance first new strategy learn rate decay propose accelerate model convergence second find add convolutional layer begin network use order data discard pre train process encoder without loss performance besides design experiment find balance among usage gpu memory train circle model performance finally achieve one hundred and sixty-nine character error rate cer test set two absolute improvement strong blstm ce system language model train text corpus
attention base end end model listen attend spell las simplify whole pipeline traditional automatic speech recognition asr systems become popular field speech recognition previous work researchers show architectures acquire comparable result state art asr systems especially use bidirectional encoder global soft attention gsa mechanism however bidirectional encoder gsa two obstacles real time speech recognition work aim stream las baseline remove two obstacles encoder side use latency control lc bidirectional structure reduce delay forward computation meanwhile adaptive monotonic chunk wise attention amocha mechanism propose replace gsa calculation attention weight distribution furthermore propose two methods alleviate huge performance degradation combine lc amocha finally successfully acquire online las model lc amocha thirty-five relative performance reduction las baseline internal mandarin corpus
audio visual speech recognition avsr system think one promise solutions robust speech recognition especially noisy environment paper propose novel multimodal attention base method audio visual speech recognition could automatically learn fuse representation modalities base importance method realize use state art sequence sequence seq2seq architectures experimental result show relative improvements two thirty-six auditory modality alone obtain depend different signal noise ratio snr compare traditional feature concatenation methods propose approach achieve better recognition performance clean noisy condition believe modality attention base end end method easily generalize multimodal task correlate information
corporate distress model typically employ numerical financial variables firm annual report develop model employ unstructured textual data report well namely auditors report managements statements model consist convolutional recurrent neural network concatenate numerical financial variables learn descriptive representation text suit corporate distress prediction find unstructured data provide statistically significant enhancement distress prediction performance particular large firm accurate predictions utmost importance furthermore find auditors report informative managements statements joint model include managements statements auditors report display enhancement relative model include auditors report model demonstrate direct improvement exist state art model
user interaction voice power agents generate large amount unlabeled utterances paper explore techniques efficiently transfer knowledge unlabeled utterances improve model performance speak language understand slu task use embeddings language model elmo take advantage unlabeled data learn contextualized word representations additionally propose elmo light elmol faster simpler unsupervised pre train method slu find suggest unsupervised pre train large corpora unlabeled utterances lead significantly better slu performance compare train scratch even outperform conventional supervise transfer additionally show gain unsupervised transfer techniques improve supervise transfer improvements pronounce low resource settings use one thousand label domain sample techniques match performance train scratch ten 15x label domain data
give south african education crisis strategies improvement sustainability high quality date education must explore migration education online inclusion machine translation low resourced local languages become necessary paper aim spur use current neural machine translation nmt techniques low resourced local languages paper demonstrate state art performance english setswana translation use autshumato dataset use transformer architecture beat previous techniques five hundred and thirty-three bleu point demonstrate promise use current nmt techniques african languages
deep neural network model recently achieve state art performance gain variety natural language process nlp task young hazarika poria cambria two thousand and seventeen however gain rely availability large amount annotate examples without state art performance rarely achievable especially inconvenient many nlp field annotate examples scarce medical text improve nlp model situation evaluate five improvements name entity recognition ner task ten annotate examples available one layer wise initialization pre train weight two hyperparameter tune three combine pre train data four custom word embeddings five optimize vocabulary oov word experimental result show f1 score six hundred and ninety-three achievable state art model improve seven thousand, eight hundred and eighty-seven
multi label text classification textual document assign one label due nature multi label text classification task often consider challenge compare binary multi class text classification problems important task broad applications biomedicine assign diagnosis cod number different computational methods eg train combine binary classifiers label propose recent years however many suffer modest accuracy efficiency limit success practical use propose ml net novel deep learn framework multi label classification biomedical texts end end system ml net combine label prediction network automate label count prediction mechanism output optimal set label leverage predict confidence score label contextual information target document evaluate ml net three independent publicly available corpora two kinds text genres biomedical literature clinical note evaluation example base measure precision recall f measure use ml net compare several competitive machine learn baseline model benchmarking result show ml net compare favorably state art methods multi label classification biomedical texts ml net also show robust evaluate different text genres biomedicine unlike traditional machine learn methods ml net require human efforts feature engineer highly efficient scalable approach task large set label need build individual classifiers separate label finally ml net able dynamically estimate label count base document context systematic accurate manner
paper compare various methods compress text use neural model find extract tokens latent variables significantly outperform state art discrete latent variable model vq vae furthermore compare various extractive compression scheme two best perform methods perform equally one method simply choose tokens highest tf idf score another train bidirectional language model similar elmo choose tokens highest loss consider subsequence text text broader sense conclude language strong compression code find justify high quality generation achieve hierarchical method latent variables nothing natural language summary also conclude hierarchy language entire text predict much easily base sequence small number keywords easily find classical methods tf idf speculate extraction process may useful unsupervised hierarchical text generation
first derive human intuition later adapt machine translation automatic token alignment attention mechanism simple method use encode sequence data base importance score element assign widely apply attain significant improvement various task natural language process include sentiment classification text summarization question answer dependency parse etc paper survey recent work conduct introductory summary attention mechanism different nlp problems aim provide readers basic knowledge widely use method discuss different variants different task explore association techniques machine learn examine methods evaluate performance
web contain vast repositories unstructured text investigate opportunity build knowledge graph text source generate set triple use knowledge gather integration define architecture language compiler process subject predicate object triple use opennlp parser implement depth first search traversal pos tag syntactic tree append predicate object information parser enable higher precision higher recall extractions syntactic relationships across conjunction boundaries able extract two twenty-five time correct extractions reverb extractions use variety semantic web applications question answer verify extraction fifty thousand triple clueweb dataset
learn construct text representations end end systems difficult natural languages highly compositional task specific annotate datasets often limit size methods directly supervise language composition allow us guide model base exist knowledge regularize towards robust interpretable representations paper investigate objectives different granularities use learn better language representations propose architecture jointly learn label sentence tokens predictions level combine together use attention mechanism token level label also act explicit supervision compose sentence level representations experiment show learn perform task jointly multiple level model achieve substantial improvements sentence classification sequence label
paper propose novel pipeline automatic grammar augmentation provide significant improvement voice command recognition accuracy systems small footprint acoustic model improvement achieve augment user define voice command set also call grammar set alternate grammar expressions give grammar set set potential grammar expressions candidate set augmentation construct specific statistical pronunciation dictionary capture consistent pattern errors decode induce variations pronunciation pitch tempo accent ambiguous spell noise condition use candidate set greedy optimization base cross entropy method cem base algorithms consider search augment grammar set improve recognition accuracy utilize command specific dataset experiment show propose pipeline along algorithms consider paper significantly reduce mis detection mis classification rate without increase false alarm rate experiment also demonstrate consistent superior performance cem method greedy base algorithms
despite great success word embed sentence embed remain well solve problem paper present supervise learn framework exploit sentence embed medical question answer task learn framework consist two main part one sentence embed produce module two score module former develop contextual self attention multi scale techniques encode sentence embed tensor module shortly call contextual self attention multi scale sentence embed camse latter employ two score strategies semantic match score sms semantic association score sas sms measure similarity sas capture association sentence pair medical question concatenate candidate choice piece correspond supportive evidence propose framework examine two medical question answeringmedicalqa datasets collect real world applications medical exam clinical diagnosis base electronic medical record emr comparison result show propose framework achieve significant improvements compare competitive baseline approach additionally series control experiment also conduct illustrate multi scale strategy contextual self attention layer play important roles produce effective sentence embed two kinds score strategies highly complementary question answer problems
way perceive sound depend many aspects ecological frequency acoustic feature typicality notably identify source paper present hcu400 dataset four hundred and two sound range easily identifiable everyday sound intentionally obscure artificial ones aim lower barrier study aural phenomenology largest available audio dataset include analysis causal attribution sample annotate crowd source descriptions well familiarity imageability arousal valence rat extend exist calculations causal uncertainty automate generalize word embeddings upon analysis find individuals provide less polarize emotion rat sound source become increasingly ambiguous individual rat familiarity imageability hand diverge uncertainty increase despite clear negative trend average
language model heart many nlp problems always great interest researchers neural language model come advantage distribute representations long range contexts particular dynamics allow cycle information within network recurrent neural network rnn become ideal paradigm neural language model long short term memory lstm architecture solve inadequacies standard rnn model long range contexts spite plethora rnn variants possibility add multiple memory cells lstm nod seldom explore propose multi cell node architecture lstms study applicability neural language model propose multi cell lstm language model outperform state art result well know penn treebank ptb setup
due recent boom artificial intelligence ai research include computer vision cv become impossible researchers field keep exponentially increase number manuscripts response situation paper propose paper summary generation psg task use simple effective method automatically generate academic paper summary raw pdf data realize psg combination vision base supervise components detector language base unsupervised important sentence extractor applicable train format manuscripts show quantitative evaluation ability simple vision base components extraction qualitative evaluation system extract visual item sentence helpful understand process via psg nine hundred and seventy-nine manuscripts accept conference computer vision pattern recognition cvpr two thousand and eighteen available believe propose method provide better way researchers stay catch important academic paper
key initial step several natural language process nlp task involve embed phrase text vectors real number preserve semantic mean end several methods recently propose impressive result semantic similarity task however approach assume perfect transcripts available generate embeddings reasonable assumption analysis write text limit analysis transcribe text paper investigate effect word substitution errors come automatic speech recognition errors asr several state art sentence embed methods propose new simulator allow experimenter induce asr plausible word substitution errors corpus desire word error rate use simulator evaluate robustness several sentence embed methods result show pre train neural sentence encoders robust asr errors perform well textual similarity task errors introduce meanwhile unweighted average word vectors perform well perfect transcriptions performance degrade rapidly textual similarity task text word substitution errors
affect convey important implicit information human communication capability correctly express affect human machine conversations one major milestones artificial intelligence recent years extensive research open domain neural conversational model conduct however embed affect model still explore paper propose end end affect rich open domain neural conversational model produce responses appropriate syntax semantics also rich affect model extend seq2seq model adopt vad valence arousal dominance affective notations embed word affect addition model consider effect negators intensifiers via novel affective attention mechanism bias attention towards affect rich word input sentence lastly train model affect incorporate objective function encourage generation affect rich word output responses evaluations base perplexity human evaluations show model outperform state art baseline model comparable size produce natural affect rich responses
recently emergence signal conventions among language prime example draw considerable interdisciplinary interest range game theory robotics evolutionary linguistics wide spectrum research base much different assumptions methodologies complexity problem preclude formulation unify commonly accept explanation examine formation signal conventions framework multi agent reinforcement learn model network interactions agents complete graph sufficiently dense random graph global consensus typically reach emerge language nearly unique object word map contain synonyms homonyms finite dimensional lattices model get trap disorder configurations local consensus trap avoid introduce population renewal presence superlinear reinforcement restore ordinary surface tension drive coarsen considerably enhance formation efficient signal
code summarization provide high level natural language description function perform code benefit software maintenance code categorization retrieval best knowledge state art approach follow encoder decoder framework encode code hide space decode natural language space suffer two major drawbacks encoders consider sequential content code ignore tree structure also critical task code summarization b decoders typically train predict next word maximize likelihood next grind truth word previous grind truth word give however expect generate entire sequence scratch test time discrepancy textitexposure bias issue make learn decoder suboptimal paper incorporate abstract syntax tree structure well sequential content code snippets deep reinforcement learn framework ie actor critic network actor network provide confidence predict next word accord current state hand critic network evaluate reward value possible extensions current state provide global guidance explorations employ advantage reward compose bleu metric train network comprehensive experiment real world dataset show effectiveness propose model compare state art methods
reliable uncertainty quantification first step towards build explainable transparent accountable artificial intelligent systems recent progress bayesian deep learn make quantification realizable paper propose novel methods study benefit characterize model data uncertainties natural language process nlp task empirical experiment sentiment analysis name entity recognition language model use convolutional recurrent neural network model show explicitly model uncertainties necessary measure output confidence level also useful enhance model performances various nlp task
couple vocal fold source vocal tract filter one critical factor source filter articulation theory traditional linear source filter theory challenge current research clearly show impact acoustic load dynamic behavior vocal fold vibration well variations glottal flow pulse shape paper outline underlie mechanism source filter interactions demonstrate design work principles couple various exist vocal cord vocal tract biomechanical model study consider self oscillate lump element model acoustic source computational model vocal tract articulators understand limitations source filter interactions associate model compare concern mechanical design acoustic physiological characteristics aerodynamic simulation
availability open source software play remarkable role popularization speech recognition deep learn kaldi instance nowadays establish framework use develop state art speech recognizers pytorch use build neural network python language recently spawn tremendous interest within machine learn community thank simplicity flexibility pytorch kaldi project aim bridge gap popular toolkits try inherit efficiency kaldi flexibility pytorch pytorch kaldi simple interface software embed several useful feature develop modern speech recognizers instance code specifically design naturally plug user define acoustic model alternative users exploit several pre implement neural network customize use intuitive configuration file pytorch kaldi support multiple feature label stream well combinations neural network enable use complex neural architectures toolkit publicly release along rich documentation design properly work locally hpc cluster experiment conduct several datasets task show pytorch kaldi effectively use develop modern state art speech recognizers
user emotion analysis toward videos automatically recognize general emotional status viewers multimedia content embed online video stream exist work fall two categories one visual base methods focus visual content extract specific set feature videos however generally hard learn map function low level video pixels high level emotion space due great intra class variance two textual base methods focus investigation user generate comment associate videos learn word representations traditional linguistic approach typically lack emotion information global comment usually reflect viewers high level understand rather instantaneous emotions address limitations paper propose jointly utilize video content user generate texts simultaneously emotion analysis particular introduce exploit new type user generate texts ie danmu real time comment float video contain rich information convey viewers emotional opinions enhance emotion discriminativeness word textual feature extraction propose emotional word embed ewe learn text representations jointly consider semantics emotions afterwards propose novel visual textual emotion analysis model deep couple video danmu neural network dcvdn visual textual feature synchronously extract fuse form comprehensive representation deep canonically correlate autoencoder base multi view learn extensive experiment self crawl real world video danmu dataset prove dcvdn significantly outperform state art baselines
train task completion dialogue agents reinforcement learn usually require large number real user experience dyna q algorithm extend q learn integrate world model thus effectively boost train efficiency use simulate experience generate world model effectiveness dyna q however depend quality world model implicitly pre specify ratio real vs simulate experience use q learn end extend recently propose deep dyna q ddq framework integrate switcher automatically determine whether use real simulate experience q learn furthermore explore use active learn improve sample efficiency encourage world model generate simulate experience state action space agent fully explore result show combine switcher active learn new framework name switch base active deep dyna q switch ddq lead significant improvement ddq q learn baselines simulation human evaluations
often chat bots build solve purpose search engine human assistant primary goal provide information user help complete task however chat bots incapable respond unscripted query like hi favourite food human evaluation judgments show four humans come consensus intent give query chat domain seventy-seven time thus make evident non trivial task work show difficult break chitchat space clearly define intents propose system handle task chat bots keep mind scalability interpretability appropriateness trustworthiness relevance coverage work introduce pipeline query understand chitchat use hierarchical intents well way use seq seq auto generation model professional bots explore interpretable model chat domain detection also show various components adult offensive classification grammars regex pattern curated personality base responses generic guide evasive responses response generation model combine scalable way solve problem
behavioral skills policies autonomous agents conventionally learn reward function via reinforcement learn demonstrations via imitation learn however modes task specification disadvantage reward function require manual engineer demonstrations require human expert able actually perform task order generate demonstration instruction follow natural language instructions provide appeal alternative way specify goals humans simply speak write would like able specify task machine however single instruction may insufficient fully communicate intent even may insufficient autonomous agent actually understand perform desire task work propose interactive formulation task specification problem iterative language corrections provide autonomous agent guide acquire desire skill propose language guide policy learn algorithm integrate instruction sequence corrections acquire new skills quickly experiment show method enable policy follow instructions corrections simulate navigation manipulation task substantially outperform direct non interactive instruction follow
text base information retrieval ir systems index object word phrase discrete systems augment model use embeddings measure similarity continuous space continuous space model typically use rank top candidates consider problem end end continuous retrieval standard approximate nearest neighbor ann search replace usual discrete invert index rely entirely distance learn embeddings train simple model specifically retrieval appropriate model architecture improve discrete baseline eight twenty-six map two similar question retrieval task also discuss problem evaluation retrieval systems show modify exist pairwise similarity datasets purpose
audio sentiment analysis popular research area extend conventional text base sentiment analysis depend effectiveness acoustic feature extract speech however current progress audio sentiment analysis mainly focus extract homogeneous acoustic feature fuse heterogeneous feature effectively paper propose utterance base deep neural network model parallel combination convolutional neural network cnn long short term memory lstm base network obtain representative feature term audio sentiment vector asv maximally reflect sentiment information audio specifically model train utterance level label asv extract fuse creatively two branch cnn model branch spectrum graph produce signal feed input lstm model branch input include spectral feature cepstrum coefficient extract dependent utterances audio besides bidirectional long short term memory bilstm attention mechanism use feature fusion extensive experiment conduct show model recognize audio sentiment precisely quickly demonstrate asv better traditional acoustic feature vectors extract deep learn model furthermore experimental result indicate propose model outperform state art approach nine hundred and thirty-three multimodal opinion level sentiment intensity dataset mosi dataset
sequential data generate unprecedented pace various form include text genomic data create need efficient compression mechanisms enable better storage transmission process data solve problem many exist compressors attempt learn model data perform prediction base compression since neural network know universal function approximators capability learn arbitrarily complex mappings practice show excellent performance prediction task explore devise methods compress sequential data use neural network predictors combine recurrent neural network predictors arithmetic coder losslessly compress variety synthetic text genomic datasets propose compressor outperform gzip real datasets achieve near optimal compression synthetic datasets result also help understand neural network good alternatives traditional finite context model
parameters large vocabulary model use embed layer map categorical feature vectors softmax layer classification weight bottle neck memory constraint device train applications like federate learn device inference applications like automatic speech recognition asr one way compress embed softmax layer substitute larger units word smaller sub units character however often sub unit model perform poorly compare larger unit model propose west algorithm encode categorical feature output class sequence random domain dependent sub units demonstrate transduction lead significant compression without compromise performance west bridge gap larger unit sub unit model interpret maxent model sub unit feature independent interest
language change involve competition alternative linguistic form one spontaneous evolution form typically result monotonic growths decay two three like winner take attractor behaviors case spanish past subjunctive spontaneous evolution two compete form end ra se perturb appearance royal spanish academy one thousand, seven hundred and thirteen enforce spell form perfectly interchangeable variants four moment ra form dominant five time series extract massive corpus book six reveal regulation fact produce transient renew interest old form se fade leave ra dominant form present day show time series successfully explain two dimensional linear model integrate imitative novelty component model reveal temporal scale collective attention fade inverse proportion verb frequency integration two basic mechanisms imitation attention novelty allow understand diverse compete object lifetimes range hours memes news seven eight decades verbs suggest existence general mechanism underlie cultural evolution
domain generation algorithms dgas frequently employ malware generate domains use connect command control c2 servers recent work dga detection leverage deep learn architectures like convolutional neural network cnns character level long short term memory network lstms classify domains however classifiers perform poorly wordlist base dga families generate domains pseudorandomly concatenate dictionary word propose novel approach combine context sensitive word embeddings simple fully connect classifier perform classification domains base word level information word embeddings pre train large unrelated corpus leave freeze train domain data result small number trainable parameters enable extremely short train durations transfer language knowledge store representations allow high perform model small train datasets show architecture reliably outperform exist techniques wordlist base dga families thirty dga train examples achieve state art performance around one hundred dga train examples require order magnitude less time train compare current techniques special note technique performance matsnu dga classifier attain eight hundred and ninety-five detection rate eleven thousand false positive rate fpr train thirty examples dga domains nine hundred and twelve detection rate one hundred and ten thousand fpr ninety examples consider dgas wordlists several hundred word result demonstrate technique rely classifier learn dga wordlists instead classifier able learn semantic signatures wordlist base dga families
every speech signal carry implicit information emotions extract speech process methods paper propose algorithm extract feature independent speak language classification method comparatively good recognition performance different languages independent employ classification methods propose algorithm compose three stag first stage propose feature rank method analyze state art voice quality feature second stage propose method find subset common feature language classifier third stage compare approach recognition rate state art filter methods use three databases different languages namely polish serbian english also three different classifiers namely nearest neighbour support vector machine gradient descent neural network employ show method select significant language independent method independent feature many case outperform state art filter methods
study investigate phase reconstruction deep learn base monaural talker independent speaker separation short time fourier transform stft domain key observation mixture two source magnitudes accurately estimate geometric constraint absolute phase difference source mixture uniquely determine addition source phase time frequency f unit narrow two candidates pick right candidate propose three algorithms base iterative phase reconstruction group delay estimation phase difference sign prediction state art result obtain publicly available wsj0 2mix 3mix corpus
present two end end model audio byte a2b byte audio b2a multilingual speech recognition synthesis prior work predominantly use character sub word word unit choice model text units difficult scale languages large vocabularies particularly case multilingual process work model text via sequence unicode bytes specifically utf eight variable length byte sequence character bytes allow us avoid large softmaxes languages large vocabularies share representations multilingual model show bytes superior grapheme character wide variety languages monolingual end end speech recognition additionally multilingual byte model outperform respective single language baseline average forty-four relatively japanese english code switch speech multilingual byte model outperform monolingual baseline three hundred and eighty-six relatively finally present end end multilingual speech synthesis model use byte representations match performance monolingual baselines
although six thousand, five hundred languages world pronunciations many phonemes sound similar across languages people learn foreign language pronunciation often reflect native language characteristics motivate us investigate speech synthesis network learn pronunciation datasets different languages study interest analyze take advantage multilingual speech synthesis network first train speech synthesis network bilingually english korean analyze network learn relations phoneme pronunciation languages experimental result show learn phoneme embed vectors locate closer pronunciations similar across languages consequently train network synthesize english speakers korean speech vice versa use result propose train framework utilize information different language specific pre train speech synthesis network use datasets high resource language low resource language fine tune network use low resource language dataset finally conduct simulations ten different languages show generally extendable languages
competency question cqs natural language question outline constrain scope knowledge represent ontology despite cqs part several ontology engineer methodologies observe actual publication cqs available ontologies limit even scarcer publication respective formalisations term eg sparql query paper aim contribute address engineer shortcomings use cqs ontology development facilitate wider use cqs order understand relation cqs query ontology test cqs ontology gather analyse publicly release set two hundred and thirty-four cqs translations sparql owl several ontologies different domains develop different group analyse cqs two principal ways first stage focus linguistic analysis natural language text ie lexico syntactic analysis without presuppositions ontology elements subsequent step semantic analysis order find pattern increase diversity cq source result five fold increase hitherto publish pattern one hundred and six distinct cq pattern limit subset pattern share across cq set different ontologies next analyse relation find cq pattern forty-six sparql owl query signatures reveal one cq pattern may realise one sparql owl query signature vice versa hope work contribute establish common practice templates automation user tool support cq formulation formalisation execution general management
emotion recognition classification active area research paper present first approach emotion classification use persistent entropy support vector machine topology base model apply obtain single real number raw signal data use input support vector machine classify signal eight different emotions calm happy sad angry fearful disgust surprise
automatic voice control systems change way humans interact computer voice speech recognition systems allow user make hand free request computer turn process request serve user appropriate responses years research developments machine learn artificial intelligence today voice control technologies become efficient widely apply many domains enable improve human human human computer interactions state art e commerce applications help web technologies offer interactive user friendly interfaces however instance people especially visual disabilities able fully experience serviceability applications voice control system embed web application enhance user experience provide voice mean control functionality e commerce websites paper propose taxonomy speech recognition systems srs present voice control commodity purchase e commerce application use ibm watson speech text demonstrate usability prototype extend application scenarios government service kiosks enable analytics convert text data scenarios medical diagnosis clinics
deep learn currently play crucial role toward higher level artificial intelligence paradigm allow neural network learn complex abstract representations progressively obtain combine simpler ones nevertheless internal black box representations automatically discover current neural architectures often suffer lack interpretability make primary interest study explainable machine learn techniques paper summarize recent efforts develop interpretable neural model directly process speech raw waveform particular propose sincnet novel convolutional neural network cnn encourage first layer discover meaningful filter exploit parametrized sinc function contrast standard cnns learn elements filter low high cutoff frequencies band pass filter directly learn data inductive bias offer compact way derive customize filter bank front end depend parameters clear physical mean experiment conduct speaker speech recognition show propose architecture converge faster perform better interpretable standard cnns
sequence prediction model learn example sequence variety train algorithms maximum likelihood learn simple efficient yet suffer compound error test time reinforcement learn policy gradient address issue prohibitively poor exploration efficiency rich set algorithms raml spg data noise also develop different perspectives paper establish formal connection algorithms present generalize entropy regularize policy optimization formulation show apparently distinct algorithms reformulate special instance framework difference configurations reward function couple hyperparameters unify interpretation offer systematic view vary properties exploration learn efficiency besides inspire framework present new algorithm dynamically interpolate among family algorithms schedule sequence model learn experiment machine translation text summarization game imitation learn demonstrate superiority propose algorithm
recurrent neural network rnns long short term memory gate recurrent units pivotal build block across broad spectrum sequence model problems paper propose recurrently control recurrent network rcrn expressive powerful sequence encode concretely key idea behind approach learn recurrent gate function use recurrent network architecture split two components controller cell listener cell whereby recurrent controller actively influence compositionality listener cell conduct extensive experiment myriad task nlp domain sentiment analysis sst imdb amazon review etc question classification trec entailment classification snli scitail answer selection wikiqa trecqa read comprehension narrativeqa across twenty-six datasets result demonstrate rcrn consistently outperform bilstms also stack bilstms suggest controller architecture might suitable replacement widely adopt stack architecture
vision language navigation vln task navigate embody agent carry natural language instructions inside real 3d environments paper study address three critical challenge task cross modal ground ill pose feedback generalization problems first propose novel reinforce cross modal match rcm approach enforce cross modal ground locally globally via reinforcement learn rl particularly match critic use provide intrinsic reward encourage global match instructions trajectories reason navigator employ perform cross modal ground local visual scene evaluation vln benchmark dataset show rcm model significantly outperform previous methods ten spl achieve new state art performance improve generalizability learn policy introduce self supervise imitation learn sil method explore unseen environments imitate past good decisions demonstrate sil approximate better efficient policy tremendously minimize success rate performance gap see unseen environments three hundred and seven one hundred and seventeen
human robot communication situate environments involve complex interplay knowledge representations across wide variety modalities crucially linguistic information must associate representations object locations people goals may represent different ways previous work develop consultant framework facilitate modality agnostic access information distribute across set heterogeneously represent knowledge source work draw inspiration cognitive science augment distribute knowledge source short term memory buffer create stm augment algorithm refer expression generation discuss potential performance benefit approach insights cognitive science may inform future refinements design approach
bibliographic reference parsers extract machine readable metadata author name title journal year bibliographic reference string extract metadata parsers apply heuristics machine learn however reference parser algorithm consistently give best result every scenario instance one tool may best extract title acm citation style third best apa use another tool may best extract english author name another one best noisy data ie inconsistent citation style paper extend version recent recsys poster address problem reference parse recommender systems meta learn perspective propose parsrec meta learn base recommender system recommend potentially effective parser give reference string parsrec recommend one ten open source parsers anystyle parser biblio cermine citation citation parser grobid parscit pdfssa4met reference tagger science parse evaluate parsrec 105k reference chemistry propose two approach meta learn recommendations first approach learn best parser entire reference string second approach learn best parser metadata type reference string second approach achieve twenty-six increase f1 nine hundred and nine vs eight hundred and eighty-six best single parser grobid reduce false positive rate two hundred and two seventy-five vs ninety-four false negative rate one hundred and eighty-nine one hundred and seven vs one hundred and thirty-two
simile figure speech compare two things use connection word comparison intend take literally often use everyday communication also part linguistic cultural heritage paper present methodology semi automate collection similes world wide web use text mine machine learn techniques expand exist corpus collect four hundred and forty-two similes internet add exist corpus collect vuk stefanovic karadzic contain three hundred and thirty-three similes also introduce crowdsourcing collection figure speech help us build corpus contain seven hundred and eighty-seven unique similes
mean sentence function relations hold word instantiate relational view semantics series neural model base variants relation network rns represent set object us word form sentence term representations pair object propose two extensions basic rn model natural language first build intuition word pair equally informative mean sentence use constraints base supervise unsupervised dependency syntax control relations influence representation second since higher order relations poorly capture sum pairwise relations use recurrent extension rns propagate information form representations higher order relations experiment sentence classification sentence pair classification machine translation reveal basic rns modestly effective sentence representation recurrent rns latent syntax reliably powerful representational device
neural network base open end conversational agents automatically generate responses base predictive model learn large number pair utterances generate responses typically acceptable sentence often dull generic certainly devoid emotion paper present neural model learn express give emotion generate response propose four model evaluate three baselines encoder decoder framework base model multiple attention layer provide best overall performance term express require emotion outperform model emotions present promise result case
aspect level sentiment classification asc aim identify sentiment polarities towards aspects sentence aspect behave general aspect category ac specific aspect term however due especially expensive labor intensive label exist public corpora level relatively small meanwhile previous methods rely complicate structure give scarce data largely limit efficacy neural model paper exploit new direction name coarse fine task transfer aim leverage knowledge learn rich resource source domain coarse grain ac task easily accessible improve learn low resource target domain fine grain task resolve aspect granularity inconsistency feature mismatch domains propose multi granularity alignment network mgan mgan novel coarse2fine attention guide auxiliary task help ac task model fine grain level task alleviate feature false alignment contrastive feature alignment method adopt align aspect specific feature representations semantically addition large scale multi domain dataset ac task provide empirically extensive experiment demonstrate effectiveness mgan
word vectors core many natural language process task recently interest post process word vectors enrich semantic information paper introduce novel word vector post process technique base matrix conceptors jaeger2014 family regularize identity map concretely propose use conceptors suppress latent feature word vectors high variances propose method purely unsupervised rely corpus external linguistic database evaluate post process word vectors battery intrinsic lexical evaluation task show propose method consistently outperform exist state art alternatives also show post process word vectors use downstream natural language process task dialogue state track yield improve result different dialogue domains
distribute representations word better know word embeddings become important build block natural language process task numerous study devote transfer success unsupervised word embeddings sentence embeddings paper introduce simple representation sentence sentence embed represent weight average word vectors follow soft projection demonstrate effectiveness propose method clinical semantic textual similarity task biocreative ohnlp challenge two thousand and eighteen
electronic health record ehr increasingly use construct disease risk prediction model feature engineer ehr data however challenge due highly dimensional heterogeneous nature low dimensional representations ehr data potentially mitigate challenge paper use global vectors glove learn word embeddings diagnose procedures record use thirteen million ontology term across twenty-seven million hospitalisations national uk ehr demonstrate utility embeddings evaluate performance identify patients higher risk hospitalise congestive heart failure find indicate embeddings enable creation robust ehr derive disease risk prediction model address limitations associate manual clinical feature engineer
mine financial text document understand sentiments individual investors institutions market important challenge problem literature current approach mine sentiments financial texts largely rely domain specific dictionaries however dictionary base methods often fail accurately predict polarity financial texts paper aim improve state art introduce novel sentiment analysis approach employ concept financial non financial performance indicators present association rule mine base hierarchical sentiment classifier model predict polarity financial texts positive neutral negative performance propose model evaluate benchmark financial dataset model also compare state art dictionary machine learn base approach result find quite promise novel use performance indicators financial sentiment analysis offer interest useful insights
project demonstrate methodology estimate cooperate credibility natural language process approach cooperate transparency impact credibility possible future earn firm important factor consider bank investors risk assessments list firm approach estimate cooperate credibility bypass human bias inconsistency risk assessment use large quantitative data neural network model provide accurate estimation efficient manner compare manual assessment begin model employ latent dirichlet allocation thu open chinese lexicon tsinghua university classify topics article potentially relate corporate credibility keywords relate topics train residual convolutional neural network data label accord survey fund manager accountant opinion corporate credibility train run model preprocessed news report regard three thousand and sixty-five list company model suppose give back company rank base level transparency
categorical compositional distributional discocat model mean develop coecke et al two thousand and ten successful model various aspects mean however fail model fact language change give approach discocat allow us represent language model translations enable us describe translations one language another change within language unify product space representation give coecke et al two thousand and ten functorial description kartsaklis et al two thousand and thirteen way allow us view language catalogue mean formalize notion lexicon discocat define dictionary mean two lexicons do within framework monoidal categories give examples apply methods give concrete suggestion compositional translation corpora
paper present refinement framework wavenet vocoders variational autoencoder vae base voice conversion vc reduce quality distortion cause mismatch train data test data conventional wavenet vocoders train natural acoustic feature condition convert feature conversion stage vc mismatch often cause significant quality similarity degradation work take advantage particular structure vaes refine wavenet vocoders self reconstruct feature generate vae similar characteristics convert feature temporal structure target natural feature analyze feature show self reconstruct feature similar convert feature objective subjective experimental result demonstrate effectiveness propose framework
several natural language task label sequence available separate domains say languages goal label sequence mix domain code switch text may available model label whole passages say sentiments would like exploit toward better position specific label inference say target dependent sentiment annotation key characteristic share across task different position primary instance benefit different experts train auxiliary data label primary instance scarce label best expert position entail unacceptable cognitive burden propose gitnet unify position sensitive multi task recurrent neural network rnn architecture applications auxiliary primary task need share train instance auxiliary rnns train auxiliary instance primary instance also submit auxiliary rnn state sequence gate merge novel composite state sequence tailor primary inference task approach sharp contrast recent multi task network like cross stitch sluice network control state transfer fine granularity demonstrate superiority girnet use three applications sentiment classification code switch passages part speech tag code switch text target position sensitive annotation sentiment monolingual passages case establish new state art performance beyond recent competitive baselines
address problem phrase ground lear ing multi level common semantic space share textual visual modalities exploit multiple level feature map deep convolutional neural network well contextualized word sentence embeddings extract character base language model follow dedicate non linear mappings visual feature level word sentence embeddings obtain multiple instantiations common semantic space comparisons target text visual content perform cosine similarity guide model multi level multimodal attention mechanism output attend visual feature level best level choose compare text content maximize pertinence score image sentence pair grind truth experiment conduct three publicly available datasets show significant performance gain twenty sixty relative state art phrase localization set new performance record datasets provide detail ablation study show contribution element approach release code github
network representation aim represent nod network continuous compact vectors attract much attention recent years due ability capture complex structure relationships inside network however exist network representation methods commonly design homogeneous information network nod entities network type eg paper citation network paper propose universal network representation approach unra represent different type nod heterogeneous information network continuous common vector space unra build latest mutually update neural language module simultaneously capture inter relationship among homogeneous nod node content correlation relationships different type nod also assemble learn unify framework experiment validate unra achieve outstanding performance compare six state art algorithms node representation node classification network visualization node classification unra achieve three one hundred and thirty-two performance improvement term accuracy
semantic parse question answer often expensive collect gold parse even gold answer supervision signal propose convert model output set human understandable statements allow non expert users act proofreaders provide error mark learn signal parser model output suggest historic system operate counterfactual policy learn setup introduce new estimators effectively leverage give feedback avoid know degeneracies counterfactual learn still applicable stochastic gradient optimization neural semantic parse furthermore discuss feedback collection method seamlessly integrate deploy virtual personal assistants embed semantic parser work first show semantic parsers improve significantly counterfactual learn log human feedback data
clinical text provide essential information estimate acuity patient hospital stay addition structure clinical data study explore clinical text complement clinical predictive learn task leverage internal medical natural language process service perform name entity extraction negation detection clinical note compose select entities new text corpus train document representations propose multimodal neural network jointly train time series signal unstructured clinical text representations predict hospital mortality risk icu patients model outperform benchmark two auc
study problem jointly reason language vision navigation spatial reason task introduce touchdown task dataset agent must first follow navigation instructions real life visual urban environment identify location describe natural language find hide object goal position data contain nine thousand, three hundred and twenty-six examples english instructions spatial descriptions pair demonstrations empirical analysis show data present open challenge exist methods qualitative linguistic analysis show data display richer use spatial reason compare relate resources
internet rich rapidly increase source high quality educational content infer prerequisite relations educational concepts require modern large scale online educational technology applications personalize recommendations automatic curriculum creation present prereq new supervise learn method infer concept prerequisite relations prereq design use latent representations concepts obtain pairwise latent dirichlet allocation model neural network base siamese network architecture prereq learn unknown concept prerequisites course prerequisites label concept prerequisite data outperform state art approach benchmark datasets effectively learn less train data prereq also use unlabeled video playlists steadily grow source train data learn concept prerequisites thus obviate need manual annotation course prerequisites
separate mix distributions long stand challenge machine learn signal process current methods either rely make strong assumptions source distributions rely train sample source mixture work introduce new method neural egg separation tackle scenario extract signal unobserved distribution additively mix signal observe distribution method iteratively learn separate know distribution progressively finer estimate unknown distribution settings neural egg separation initialization sensitive therefore introduce latent mixture mask ensure good initialization extensive experiment audio image separation task show method outperform current methods use level supervision often achieve similar performance full supervision
indus script remain one last major undeciphered script ancient world focus indus inscriptions group miniature tablets discover meadow kenoyer harappa one thousand, nine hundred and ninety-seven draw parallel proto elamite proto cuneiform inscriptions explore miniature tablets may use record ration allocate porter laborers show similar inscriptions find stamp seal lead potentially provocative conclusion rather simply indicate ownership property indus seal may use generate tokens tablets seal repetitive economic transactions ration exchange canonical amount goods grain animals labor barter base economy
explosive growth fake news erosion democracy justice public trust increase demand fake news detection intervention survey review evaluate methods detect fake news four perspectives one false knowledge carry two write style three propagation pattern four credibility source survey also highlight potential research task base review particular identify detail relate fundamental theories across various discipline encourage interdisciplinary research fake news hope survey facilitate collaborative efforts among experts computer information sciences social sciences political science journalism research fake news efforts lead fake news detection efficient importantly explainable
cybercrime forums enable modern criminal entrepreneurs collaborate criminals increasingly efficient sophisticate criminal endeavor understand connections different products service often illuminate effective interventions however generate understand supply chain currently require time consume manual effort paper propose language agnostic method automatically extract supply chain cybercrime forum post reply supply chain detection algorithm identify thirty-six fifty-eight relevant chain within major english russian forums respectively show improvements baselines thirteen thirty-six respectively analysis automatically generate supply chain demonstrate underlie connections products service within forums example extract supply chain illuminate connection hack hire service sell rare valuable og account recently report understand connections products service expose potentially effective intervention point
solve text base game agent need formulate valid text command give context find ones lead success recent attempt solve text base game deep reinforcement learn focus latter ie learn act optimally valid action know advance work propose tackle first task train model generate set valid command give context try three generative model dataset generate textworld best model generate valid command unseen train achieve high f1 score test set
advance deep reinforcement learn lead agents perform well across variety sensory motor domains work study set agent must learn generate program diverse scenes condition give symbolic instruction final goals specify agent via image scenes symbolic instruction consistent goal image use condition input policies since single instruction correspond diverse set different still consistent end goal image agent need learn generate distribution program give instruction demonstrate simple change reinforce adversarial learn objective learn instruction condition policies achieve correspond diverse set goals importantly agent stochastic policy show accurately capture diversity goal distribution fix pixel base reward function baseline demonstrate efficacy approach two domains one draw mnist digits paint software condition instructions two construct scenes 3d editor satisfy certain instruction
natural language interface nli structure query intrigue due wide industrial applications high economical value work tackle problem domain adaptation nli limit data target domain two important approach consider effective general knowledge learn source domain semantic parse b data augmentation target domain present structure query inference network sqin enhance learn domain adaptation separate schema information nl decode sql structural aware manner also propose gin base augmentation technique augmentgan mitigate issue lack target domain data report solid result geoquery overnight wikisql demonstrate state art performances domain domain transfer task
neural network recently become good engage dialog however current approach base solely verbal text lack richness real face face conversation propose neural conversation model aim read generate facial gesture alongside text allow model adapt response base mood conversation particular introduce rnn encoder decoder exploit movement facial muscle well verbal conversation decoder consist two layer lower layer aim generate verbal response coarse facial expressions second layer fill subtle gesture make generate output smooth natural train neural network watch two hundred and fifty movies showcase joint face text model generate natural conversations automatic metrics human study demonstrate example application face face chat avatar
text base adventure game provide platform explore reinforcement learn context combinatorial action space natural language present deep reinforcement learn architecture represent game state knowledge graph learn exploration graph use prune action space enable efficient exploration question action take reduce question answer task form transfer learn pre train certain part architecture experiment use textworld framework show propose technique learn control policy faster baseline alternatives also open source code https githubcom rajammanabrolu kg dqn
work propose novel method supervise keyshots base video summarization apply conceptually simple computationally efficient soft self attention mechanism current state art methods leverage bi directional recurrent network bilstm combine attention network complex implement computationally demand compare fully connect network end propose simple self attention base network video summarization perform entire sequence sequence transformation single fee forward pass single backward pass train method set new state art result two benchmarks tvsum summe commonly use domain
past years neural abstractive text summarization sequence sequence seq2seq model gain lot popularity many interest techniques propose improve seq2seq model make capable handle different challenge saliency fluency human readability generate high quality summaries generally speak techniques differ one three categories network structure parameter inference decode generation also concern efficiency parallelism train model paper provide comprehensive literature survey different seq2seq model abstractive text summarization viewpoint network structure train strategies summary generation algorithms several model first propose language model generation task machine translation later apply abstractive text summarization hence also provide brief review model part survey also develop open source library namely neural abstractive text summarizer nats toolkit abstractive text summarization extensive set experiment conduct widely use cnn daily mail dataset examine effectiveness several different neural network components finally benchmark two model implement nats two recently release datasets namely newsroom bytecup
sentiment analysis sa task relate understand people feel write text start point would identify polarity level positive neutral negative give text move identify emotions whether text humorous task subject several research competitions number languages eg english spanish arabic among others contribution propose sa system namely evomsa unify participate systems various sa competitions make domain independent multilingual process text use language independent techniques evomsa classifier base genetic program work combine output different text classifiers text model produce final prediction analyze evomsa different sa competitions provide global overview performance result show evomsa competitive obtain top rank several sa competitions furthermore perform analysis evomsa components measure contribution performance idea facilitate practitioner newcomer implement competitive sa classifier finally worth mention evomsa available open source software
end end automatic speech recognition asr commonly transcribe audio signal sequence character performance evaluate measure word error rate wer suggest predict sequence word directly may helpful instead however train word level supervision difficult due sparsity examples per label class paper analyze end end asr model combine word character representation multi task learn mtl framework show improve wer study word level model benefit character level supervision analyze learn inductive preference bias model component empirically find add character level supervision mtl model interpolate recognize frequent word prefer word level model shorter word prefer character level model
paper describe ustc nel system speech translation task iwslt evaluation two thousand and eighteen system conventional pipeline system contain three modules speech recognition post process machine translation train group hybrid hmm model speech recognition machine translation train transformer base neural machine translation model speech recognition output style text input experiment conduct iwslt two thousand and eighteen task indicate compare baseline system kit system achieve one hundred and forty-nine bleu improvement
popularity critical success factor politician party win elections implement plan find reason behind popularity provide stable political movement research attempt measure popularity twitter use mix method recent years twitter data provide excellent opportunity explore public opinions analyze large number tweet study collect examine forty-five million tweet relate us politician senator bernie sanders study investigate eight economic reason behind senator popularity twitter research benefit politicians informatics experts policymakers explore public opinion collect data also available investigation
social media analytics allow us extract analyze establish semantic user generate content social media platforms study utilize mix method include three step process data collection topic model data annotation recognize exercise relate pattern base find eighty-six detect topics identify meaningful topics conduct data annotation process discuss exercise relate topics physical activity one hundred and eighty-seven lifestyle behaviors sixty-six diet four result experiment indicate exploratory data analysis practical approach summarize various characteristics text data different health medical applications
performance adversarial dialogue generation model rely quality reward signal produce discriminator reward signal poor discriminator sparse unstable may lead generator fall local optimum produce nonsense reply alleviate first problem first extend recently propose adversarial dialogue generation method adversarial imitation learn solution framework adversarial inverse reinforcement learn propose new reward model dialogue generation provide accurate precise reward signal generator train evaluate performance result model automatic metrics human evaluations two annotation settings experimental result demonstrate model generate high quality responses achieve higher overall performance state art
deep neural network dnns witness powerful approach year solve long stand artificial intelligence ai supervise unsupervised task exist natural language process speech process computer vision others paper attempt apply dnns three different cyber security use case android malware classification incident detection fraud detection data set use case contain real know benign malicious activities sample efficient network architecture dnn choose conduct various trail experiment network parameters network structure experiment choose efficient configurations dnns run one thousand epochs learn rate set range one five experiment dnn perform well comparison classical machine learn algorithms case experiment cyber security use case due fact dnns implicitly extract build better feature identify characteristics data lead better accuracy best accuracy obtain dnn xgboost android malware classification nine hundred and forty seven hundred and forty-one incident detection one hundred nine hundred and ninety-seven fraud detection nine hundred and seventy-two nine hundred and sixteen respectively
explore train attention base encoder decoder asr low resource settings model perform poorly train small amount transcribe speech part depend sufficient target side text train attention decoder network paper address shortcoming pretraining network parameters use text base data transcribe speech languages analyze relative contributions source data across three test languages text base approach result twenty average relative improvement text base augmentation technique without pretraining use transcribe speech nearby languages give twenty thirty relative reduction character error rate
present structure random walk model capture key aspects people communicate group model take form correlate l evy flight quantify balance focus discussion idea long distance leap semantic space apply model three case increase structural complexity philosophical texts aristotle hume kant four days parliamentary debate french revolution branch comment tree discussion website reddit philosophical parliamentary case model parameters describe balance converge coarse grain limit regions demonstrate emergence large scale structure result robust translation languages meanwhile find political forum consider reddit exhibit debate like pattern communities dedicate discussion science news show much less temporal order may make use emergent tree like topology comment reply structure epistemic explorations model allow us quantify ways social technologies parliamentary procedures online comment systems shape joint exploration ideas
paper provide theoretical understand word embed dimensionality motivate unitary invariance word embed propose pairwise inner product pip loss novel metric dissimilarity word embeddings use techniques matrix perturbation theory reveal fundamental bias variance trade dimensionality selection word embeddings bias variance trade shed light many empirical observations previously unexplained example existence optimal dimensionality moreover new insights discoveries like word embeddings robust fit reveal optimize bias variance trade pip loss explicitly answer open question dimensionality selection word embed
machine translation mt area study natural language process deal automatic translation human language one language another computer rich research history span nearly three decades machine translation one seek area research linguistics computational community paper investigate model base deep learn achieve substantial progress recent years become prominent method mt shall discuss two main deep learn base machine translation methods one component domain level leverage deep learn model enhance efficacy statistical machine translation smt end end deep learn model mt use neural network find correspondence source target languages use encoder decoder architecture conclude paper provide time line major research problems solve researchers also provide comprehensive overview present areas research neural machine translation
paper introduce variational autoencoder vae end end speech synthesis model learn latent representation speak style unsupervised manner style representation learn vae show good properties disentangle scale combination make easy style control style transfer achieve framework first infer style representation recognition network vae feed tts network guide style synthesize speech avoid kullback leibler kl divergence collapse train several techniques adopt finally propose model show good performance style control outperform global style token gst model abx preference test style transfer
important detect anomalous input deploy machine learn systems use larger complex input deep learn magnify difficulty distinguish anomalous distribution examples time diverse image text data available enormous quantities propose leverage data improve deep anomaly detection train anomaly detectors auxiliary dataset outliers approach call outlier exposure oe enable anomaly detectors generalize detect unseen anomalies extensive experiment natural language process small large scale vision task find outlier exposure significantly improve detection performance also observe cut edge generative model train cifar ten may assign higher likelihoods svhn image cifar ten image use oe mitigate issue also analyze flexibility robustness outlier exposure identify characteristics auxiliary dataset improve performance
softmax function use final layer nearly exist sequence sequence model language generation however usually slowest layer compute limit vocabulary size subset frequent type large memory footprint propose general technique replace softmax layer continuous embed layer primary innovations novel probabilistic loss train inference procedure generate probability distribution pre train word embeddings instead multinomial distribution vocabulary obtain via softmax evaluate new class sequence sequence model continuous output task neural machine translation show model obtain upto 25x speed train time perform par state art model term translation quality model capable handle large vocabularies without compromise translation quality also produce meaningful errors softmax base model errors typically lie subspace vector space reference translations
formulate coherence model regression task propose two novel methods combine techniques setup pairwise approach first methods model call first next operate similarly selection sort condition decision make information already sort sentence second consist technique add context regression base model concatenate sentence level representations encode correspond order paragraph latter model achieve kendall tau distance positional accuracy score match exceed current state art metrics result suggest many gain come complex machine translation inspire approach achieve simpler efficient model
describe new approach improve train generative adversarial net gans synthesize diverse image text input approach base conditional version gans expand previous work leverage auxiliary task discriminator generate image limit certain class suffer mode collapse semantically match text input key train methods form positive negative train examples respect class label give image instead select random train examples perform negative sample base semantic distance positive example class evaluate approach use oxford one hundred and two flower dataset adopt inception score multi scale structural similarity index ms ssim metrics assess discriminability diversity generate image empirical result indicate greater diversity generate image especially gradually select negative train examples closer positive example semantic space
neural tts show generate high quality synthesize speech paper investigate multi speaker latent space improve neural tts adapt system new speakers several minutes speech enhance premium voice utilize data speakers richer contextual coverage better generalization multi speaker neural tts model build embed speaker information spectral speaker latent space experimental result show less five minutes train data new speaker new model achieve mos score four hundred and sixteen naturalness four hundred and sixty-four speaker similarity close human record four hundred and seventy-four well train premium voice achieve mos score forty-five domain texts comparable mos four hundred and fifty-eight professional record significantly outperform single speaker result four hundred and twenty-eight
deep learn base text understand dltu backbone technique behind various applications include question answer machine translation text classification despite tremendous popularity security vulnerabilities dltu still largely unknown highly concern give increase use security sensitive applications sentiment analysis toxic content detection paper show dltu inherently vulnerable adversarial text attack maliciously craft texts trigger target dltu systems service misbehave specifically present textbugger general attack framework generate adversarial texts contrast prior work textbugger differ significant ways effective outperform state art attack term attack success rate ii evasive preserve utility benign text nine hundred and forty-nine adversarial text correctly recognize human readers iii efficient generate adversarial text computational complexity sub linear text length empirically evaluate textbugger set real world dltu systems service use sentiment analysis toxic content detection demonstrate effectiveness evasiveness efficiency instance textbugger achieve one hundred success rate imdb dataset base amazon aws comprehend within four hundred and sixty-one second preserve ninety-seven semantic similarity discuss possible defense mechanisms mitigate attack adversary potential countermeasures lead promise directions research
state art name entity recognition ner systems improve continuously use neural architectures past several years however many task include ner require large set annotate data achieve performance particular focus ner clinical note one fundamental critical problems medical text analysis work center effectively adapt neural architectures towards low resource settings use parameter transfer methods complement standard hierarchical ner model general transfer learn framework consist parameter share source target task showcase score significantly baseline architecture share scheme require exponential search tie parameter set generate optimal configuration mitigate problem exhaustively search model optimization propose dynamic transfer network dtn gate architecture learn appropriate parameter share scheme source target datasets dtn achieve improvements optimize transfer learn framework single train set effectively remove need exponential search
bayesian methods successfully apply sparsify weight neural network remove structure units network e g neurons apply develop approach gate recurrent architectures specifically addition sparsification individual weight neurons propose sparsify preactivations gate information flow lstm make gate information flow components constant speed forward pass improve compression moreover result structure gate sparsity interpretable depend task code available github https githubcom tipt0p sparsebayesianrnn
deep neural network learn complex abstract representations progressively obtain combine simpler ones recent trend speech speaker recognition consist discover representations start raw audio sample directly differently standard hand craft feature mfccs fbank raw waveform potentially help neural network discover better customize representations high dimensional raw input however make train significantly challenge paper summarize recent efforts develop neural architecture efficiently process speech audio waveforms particular propose sincnet novel convolutional neural network cnn encourage first layer discover meaningful filter exploit parametrized sinc function contrast standard cnns learn elements filter low high cutoff frequencies band pass filter directly learn data inductive bias offer compact way derive customize front end depend parameters clear physical mean experiment conduct speaker speech recognition show propose architecture converge faster perform better computationally efficient standard cnns
effect reason attribution effect cause one powerful unique skills humans possess multiple survey map causal attributions network unclear well efforts combine total size collective causal attribution network hold humans currently unknown make challenge assess progress survey study three causal attribution network determine well combine single network combine network require deal ambiguous nod nod represent write descriptions cause effect different descriptions may exist concept introduce netfuses method combine network ambiguous nod crucially treat different causal attributions network independent sample allow us use overlap estimate total size collective causal attribution network find exist survey capture five hundred and seventy-seven pm seven hundred and eighty-one approx293 zero cause effect estimate exist one hundred and ninety-eight pm one hundred and seventy-four approx10 two hundred zero attribute effect relationships
representation learn essential problem wide range applications important perform downstream task successfully paper propose new model learn couple representations domains intents slot take advantage hierarchical dependency speak language understand system propose model learn vector representation intents base slot tie intents aggregate representations slot similarly vector representation domain learn aggregate representations intents tie specific domain best knowledge first approach jointly learn representations domains intents slot use hierarchical relationships experimental result demonstrate effectiveness representations learn model evidence improve performance contextual cross domain reranking task
many natural language process nlp task amount annotate data limit urge need apply semi supervise learn techniques transfer learn meta learn work tackle name entity recognition ner task use prototypical network metric learn technique learn intermediate representations word cluster well name entity class property model allow classify word extremely limit number train examples potentially use zero shoot learn method couple technique transfer learn achieve well perform classifiers train twenty instance target class
present analysis problem identify biological context associate biochemical events biomedical texts constitute non trivial inter sentential relation extraction task focus biological context descriptions species tissue type cell type associate biochemical events describe properties annotate corpus context event relations present evaluate several classifiers context event association train syntactic distance frequency feature
one core challenge visual dialogue problems ask question provide useful information towards achieve require objective encourage agent ask right question difficult know priori information agent need achieve task explicit model know already propose solution problem base bayesian model uncertainty implicit model maintain visual dialogue agent function use select appropriate output select question minimise predict regret respect implicit model agent actively reduce ambiguity bayesian model uncertainty also enable principled method identify enough information acquire action select evaluate approach two goal orient dialogue datasets one visual base collaboration task negotiation base task uncertainty aware information seek model outperform counterparts two challenge problems
knowledge graph kg embed fundamental problem data mine research many real world applications aim encode entities relations graph low dimensional vector space use subsequent algorithms negative sample sample negative triplets non observe ones train data important step kg embed recently generative adversarial network gin introduce negative sample sample negative triplets large score methods avoid problem vanish gradient thus obtain better performance however use gin make original model complex hard train reinforcement learn must use paper motivate observation negative triplets large score important rare propose directly keep track cache however sample update cache two important question carefully design solutions efficient also achieve good balance exploration exploitation way method act distil version previous ga base methods waste train time additional parameters fit full distribution negative triplets extensive experiment show method gain significant improvement various kg embed model outperform state art negative sample methods base gin
characterise quirk shortcomings exploration visual dialogue sequential question answer task question correspond answer relate give visual stimuli develop embarrassingly simple method base canonical correlation analysis cca standard dataset achieve near state art performance mean rank mr direct contrast current complex parametrised architectures compute time intensive method ignore visual stimuli ignore sequence dialogue need gradients use shelf feature extractors least order magnitude fewer parameters learn practically time argue result indicative issue current approach visual dialogue conduct analyse highlight implicit dataset bias effect constrain evaluation metrics code publicly available
propose novel data augmentation method label sentence call conditional bert contextual augmentation data augmentation methods often apply prevent overfitting improve generalization deep neural network model recently propose contextual augmentation augment label sentence randomly replace word vary substitutions predict language model bert demonstrate deep bidirectional language model powerful either unidirectional language model shallow concatenation forward backward model retrofit bert conditional bert introduce new conditional mask language modelfootnotethe term conditional mask language model appear original bert paper indicate context conditional equivalent term mask language model paper conditional mask language model indicate apply extra label conditional constraint mask language model task well train conditional bert apply enhance contextual augmentation experiment six various different text classification task show method easily apply convolutional recurrent neural network classifier obtain obvious improvement
knowledge graph kind valuable knowledge base would benefit lot ai relate applications lot large scale knowledge graph build however non chinese design general purpose work introduce techkg large scale chinese knowledge graph technology orient build automatically massive technical paper publish chinese academic journals different research domains carefully design heuristic rule use extract high quality entities relations totally comprise two hundred and sixty million triplets build upon fifty-two million entities come thirty-eight research domains preliminary ex periments indicate techkg high adaptability use dataset many diverse ai relate applications release techkg http wwwtechkgcn
much recent excite work combine complementary strengths latent variable model deep learn latent variable model make easy explicitly specify model constraints conditional independence properties deep learn make possible parameterize conditional likelihoods powerful function approximators deep latent variable model provide rich flexible framework model many real world phenomena difficulties exist deep parameterizations conditional likelihoods usually make posterior inference intractable latent variable objectives often complicate backpropagation introduce point non differentiability tutorial explore issue depth lens variational inference
paper new method recognition consonant vowel phonemes combination new persian speech dataset title pcvc persian consonant vowel combination propose use recognize persian phonemes pcvc dataset twenty set audio sample ten speakers combinations twenty-three consonant six vowel phonemes persian language sample combination one vowel one consonant first consonant phoneme pronounce vowel phoneme pronounce sound sample frame two second audio every two second average five second speech rest silence paper propose method implementations mfcc mel frequency cepstrum coefficients every partition sound sample every train sample mfcc vector give multilayer perceptron fee forward ann artificial neural network train process end test sample examine ann model phoneme recognition train test process result present recognition vowels average percent recognition vowel phonemes compute
recent years rapid proliferation research publications field artificial intelligence become increasingly difficult researchers effectively keep latest research one domains however history show scientific breakthroughs often come collaborations researchers different domains traditional search algorithms like lexical search look literal match synonyms variants query word effective discover cross domain research paper meet need researchers age information overflow paper develop test innovative semantic search engine analogy search engine ase two thousand ai research paper abstract across domains like language technologies robotics machine learn computational biology human computer interactions etc ase combine recent theories methods computational analogy natural language process go beyond keyword base lexical search discover deeper analogical relationships among research paper abstract experimentally show ase capable find interest useful research paper baseline elasticsearch furthermore believe methods use ase go beyond academic paper benefit many document search task
traditional mode record fault heavy factory equipment via hand mark inspection sheet wherein machine engineer manually mark faulty machine regions paper outline machine years millions inspection sheet record data within sheet remain inaccessible however industries go digital wake potential value fault data machine health monitor increase impetus towards digitization hand mark inspection record target digitization propose novel visual pipeline combine state art deep learn model domain knowledge low level vision techniques follow inference visual relationships framework robust presence static non static background document variability machine template diagram unstructured shape graphical object identify variability stroke handwritten text propose pipeline incorporate capsule spatial transformer network base classifier accurate text read customize ctpn network text detection addition hybrid techniques arrow detection dialogue cloud removal test approach real world dataset fifty inspection sheet large containers boilers result visually appeal pipeline achieve accuracy eight hundred and seventy-one text detection nine hundred and forty-six text read
grow interest use neural network deep learn techniques create dialogue systems conversational recommendation interest set scientific exploration dialogue natural language associate discourse involve goal drive dialogue often transform naturally free form chat paper provide two contributions first publicly available large scale dataset consist real world dialogues center around recommendations address issue facilitate exploration collect redial dataset consist ten thousand conversations center around theme provide movie recommendations make data available community research second use dataset explore multiple facets conversational recommendations particular explore new neural architectures mechanisms methods suitable compose conversational recommendation systems dataset allow us systematically probe model sub components address different part overall problem domain range sentiment analysis cold start recommendation generation detail aspects natural language use set real world combine sub components full blow dialogue system examine behavior
cyberbullying disturb online misbehaviour trouble consequences appear different form social network textual format automatic detection incidents require intelligent systems exist study approach problem conventional machine learn model majority develop model study adaptable single social network time recent study deep learn base model find way detection cyberbullying incidents claim overcome limitations conventional model improve detection performance paper investigate find recent literature regard successfully reproduce find literature validate find use datasets namely wikipedia twitter formspring use author expand work apply develop methods new youtube dataset 54k post 4k users investigate performance model new social media platforms also transfer evaluate performance model train one platform another platform find show deep learn base model outperform machine learn model previously apply youtube dataset believe deep learn base model also benefit integrate source information look impact profile information users social network
use project gutenberg pg text corpus extremely popular statistical analysis language twenty-five years however contrast major linguistic datasets similar importance consensual full version pg exist date fact pg study far either consider small number manually select book lead potential bias subsets employ vastly different pre process strategies often specify insufficient detail raise concern regard reproducibility publish result order address shortcomings present standardize project gutenberg corpus spgc open science approach curated version complete pg data contain fifty thousand book three time one hundred and nine word tokens use different source annotate metadata provide broad characterization content pg also show different examples highlight potential spgc investigate language variability across time subject author publish methodology detail code download process data well obtain corpus three different level granularity raw text timeseries word tokens count word way provide reproducible pre process full size version project gutenberg new scientific resource corpus linguistics natural language process information retrieval
image caption task require model acquire multi modal understand world express understand natural language text state art task rapidly improve term n gram metrics model tend output generic caption similar image work address limitation train model generate diverse specific caption unsupervised train approach incorporate learn signal image retrieval model summarize previous result improve state art caption diversity novelty make source code publicly available online
present system generate song lyric line condition style specify artist system use variational autoencoder artist embeddings propose pre train artist embeddings representations learn cnn classifier train predict artists base mel spectrograms song clip work first step towards combine audio text modalities songs generate lyric condition artist style preliminary result suggest benefit initialize artists embeddings representations learn spectrogram classifier
humans social nature throughout history people form communities build relationships relationships coworkers friends family develop face face interactions relationships establish explicit mean communications word implicit intonation body language etc analyze human interactions derive information relationships influence among conversation participants however development internet people start communicate text online social network interestingly bring communicational habit internet many social network users form relationships establish communities leaders followers recognize hierarchical relationships important task help understand social network predict future trend improve recommendations better target advertisement improve national security identify leaders anonymous terror group work provide overview current research area present state art approach deal problem identify hierarchical relationships social network
image caption model achieve impressive result datasets contain limit visual concepts large amount pair image caption train data however model ever function wild much larger variety visual concepts must learn ideally less supervision encourage development image caption model learn visual concepts alternative data source object detection datasets present first large scale benchmark task dub nocaps novel object caption scale benchmark consist one hundred and sixty-six thousand, one hundred human generate caption describe fifteen thousand, one hundred image openimages validation test set associate train data consist coco image caption pair plus openimages image level label object bound box since openimages contain many class coco nearly four hundred object class see test image associate train caption hence nocaps extend exist novel object caption model establish strong baselines benchmark provide analysis guide future work task
wide spread use online recruitment service lead information explosion job market result recruiters seek intelligent ways person job fit bridge adapt right job seekers right position exist study person job fit focus measure match degree talent qualification job requirements mainly base manual inspection human resource experts despite subjective incomplete inefficient nature human judgement end paper propose novel end end ability aware person job fit neural network model goal reduce dependence manual labour provide better interpretation fit result key idea exploit rich information available abundant historical job application data specifically propose word level semantic representation job requirements job seekers experience base recurrent neural network along line four hierarchical ability aware attention strategies design measure different importance job requirements semantic representation well measure different contribution job experience specific ability requirement finally extensive experiment large scale real world data set clearly validate effectiveness interpretability apjfnn framework compare several baselines
field natural language process see impressive progress recent years neural network model replace many traditional systems plethora new model propose many think opaque compare feature rich counterparts lead researchers analyze interpret evaluate neural network novel fine grain ways survey paper review analysis methods neural language process categorize accord prominent research trend highlight exist limitations point potential directions future work
paper describe development microsoft xiaoice popular social chatbot world xiaoice uniquely design ai companion emotional connection satisfy human need communication affection social belong take account intelligent quotient iq emotional quotient eq system design cast human machine social chat decision make markov decision process mdps optimize xiaoice long term user engagement measure expect conversation turn per session cps detail system architecture key components include dialogue manager core chat skills empathetic compute module show xiaoice dynamically recognize human feel state understand user intent respond user need throughout long conversations since launch two thousand and fourteen xiaoice communicate six hundred and sixty million active users succeed establish long term relationships many analysis large scale online log show xiaoice achieve average cps twenty-three significantly higher chatbots even human conversations
learn environments large state action space sparse reward hinder reinforcement learn rl agent learn trial error instance follow natural language instructions web book flight ticket lead rl settings input vocabulary number actionable elements page grow large even though recent approach improve success rate relatively simple environments help human demonstrations guide exploration still fail environments set possible instructions reach millions approach aforementioned problems different perspective propose guide rl approach generate unbounded amount experience agent learn instead learn complicate instruction large vocabulary decompose multiple sub instructions schedule curriculum agent task gradually increase subset relatively easier sub instructions addition expert demonstrations available propose novel meta learn framework generate new instruction follow task train agent effectively train dqn deep reinforcement learn agent q value function approximate novel qweb neural network architecture smaller synthetic instructions evaluate ability agent generalize new instructions world bits benchmark form one hundred elements support fourteen million possible instructions qweb agent outperform baseline without use human demonstration achieve one hundred success rate several difficult environments
widespread approach process speak language first automatically transcribe text alternative use end end approach recent work propose learn semantic embeddings speak language image speak caption without intermediate transcription step propose use multitask learn exploit exist transcribe speech within end end set describe three task architecture combine objectives match speak caption correspond image speech text text image show addition speech text task lead substantial performance improvements image retrieval compare train speech image task isolation conjecture due strong inductive bias transcribe speech provide model offer support evidence
traditional semantic similarity model often fail encapsulate external context texts situate however textual datasets generate mobile platforms help us build truer representation semantic similarity introduce multimodal data especially important sparse datasets make solely text drive interpretation context difficult paper develop new algorithms build external feature sentence embeddings semantic similarity score test embed space data twitter use tweet time geolocation better understand context ultimately show apply pca eight components embed space append multimodal feature yield best outcomes yield considerable improvement pure text base approach discover similar tweet result suggest new algorithm help improve semantic understand various settings
give close source program proprietary software viruses binary code analysis indispensable many task code plagiarism detection malware analysis today source code often compile various architectures make cross architecture binary code analysis increasingly important binary disassemble express assembly languages thus recent work start explore natural language process nlp inspire binary code analysis nlp word usually represent high dimensional vectors ie embeddings facilitate process one common critical step many nlp task regard instructions word nlp inspire binary code analysis aim represent instructions embeddings well facilitate cross architecture binary code analysis goal similar instructions regardless architectures embeddings close end propose joint learn approach generate instruction embeddings capture semantics instructions within architecture also semantic relationships across architectures best knowledge first work build cross architecture instruction embed model showcase apply model resolve one fundamental problems binary code similarity comparison semantics base basic block comparison solution outperform code statistics base approach demonstrate promise apply model cross architecture binary code analysis task
answer set program asp purely declarative formalism develop field logic program nonmonotonic reason computational problems encode logic program whose answer set correspond solutions compute asp system different semantically equivalent program define problem however performance systems evaluate might significantly vary propose approach automatically transform input logic program equivalent one evaluate efficiently one make use exist tree decomposition techniques rewrite select rule set multiple ones idea guide adaptively apply basis proper new heuristics obtain smart rewrite algorithm integrate asp system method rather general adapt system implement different preference policies furthermore define set new heuristics tailor optimize ground one main phase asp computation use order implement approach asp system dlv particular ground subsystem dlv carry extensive experimental activity assess impact proposal consideration theory practice logic program tplp
paper introduce pansori program use create asr automatic speech recognition corpora online video content utilize cloud base speech api easily create corpus different languages use program semi automatically generate pansori tedxkr dataset korean ted conference talk community transcribe subtitle first high quality corpus korean language freely available independent research pansori release open source software generate corpus release permissive public license community use participation
use sequence sequence algorithms query expansion explore yet information retrieval literature question answer try fill gap literature custom query expansion engine train test open datasets start open datasets build query expansion train set use sentence embeddings base keyword extraction therefore assess ability sequence sequence neural network capture expand relations word embeddings space
paper new deep reinforcement learn base augment general sequence tag system propose new system contain two part deep neural network dnn base sequence tag model deep reinforcement learn drl base augment tagger augment tagger help improve system performance model data minority tag new system evaluate slu nlu sequence tag task use atis conll two thousand and three benchmark datasets demonstrate new system outstanding performance general tag task evaluate f1 score show new system outperform current state art model atis dataset nineteen conll two thousand and three dataset fourteen
intent detection slot fill two main task build speak language understandingslu system multiple deep learn base model demonstrate good result task effective algorithms base structure sequence sequence model encoder decoder model generate intents semantic tag either use separate model joint model previous study however either treat intent detection slot fill two separate parallel task use sequence sequence model generate semantic tag intent approach use one joint nn base model include encoder decoder structure model two task hence may fully take advantage cross impact paper new bi model base rnn semantic frame parse network structure design perform intent detection slot fill task jointly consider cross impact use two correlate bidirectional lstms blstm bi model structure decoder achieve state art result benchmark atis data five intent accuracy improvement nine slot fill improvement
language dynamic constantly evolve adapt respect time domain topic adaptability language active research area researchers discover social cultural domain specific change language use distributional tool word embeddings paper introduce global anchor method detect corpus level language shift show theoretically empirically global anchor method equivalent alignment method widely use method compare word embeddings term detect corpus level language shift despite equivalence term detection abilities demonstrate global anchor method superior term applicability compare embeddings different dimensionalities furthermore global anchor method implementation parallelization advantage show global anchor method reveal fine structure evolution language domain adaptation combine graph laplacian technique global anchor method recover evolution trajectory domain cluster disparate text corpora
entity link el task automatically identify entity mention piece text resolve correspond entity reference knowledge base like wikipedia large number el tool available different type document domains yet el remain challenge task lack precision particularly ambiguous mention often spoil usefulness automate disambiguation result real applications priori approximations difficulty link particular entity mention facilitate flag critical case part semi automate el systems detect latent factor affect el performance like corpus specific feature provide insights improve system base special characteristics underlie corpus paper first introduce consensus base method generate difficulty label entity mention arbitrary corpora difficulty label exploit train data supervise classification task able predict el difficulty entity mention use variety feature experiment corpus news article show el difficulty estimate high accuracy reveal also latent feature affect el performance finally evaluation result demonstrate effectiveness propose method inform semi automate el pipelines
islamophobic hate speech social media inflict considerable harm target individuals wider society also risk reputational damage host platforms accordingly press need robust tool detect classify islamophobic hate speech scale previous research largely approach detection islamophobic hate speech social media binary task however vary nature islamophobia mean often inappropriate theoretically inform social science effectively monitor social media draw depth conceptual work build multi class classifier distinguish non islamophobic weak islamophobic strong islamophobic content accuracy seven hundred and seventy-six balance accuracy eighty-three apply classifier dataset one hundred and nine thousand, four hundred and eighty-eight tweet produce far right twitter account two thousand and seventeen whilst tweet islamophobic weak islamophobia considerably prevalent thirty-six thousand, nine hundred and sixty-three tweet strong fourteen thousand, eight hundred and ninety-five tweet main input feature glove word embeddings model train newly collect corpus one hundred and forty million tweet outperform generic word embeddings model fifty-nine percentage point demonstrate importan4ce context unexpectedly also find one one multi class svm outperform deep learn algorithm
word embed encode word vectors important start point natural language process commonly use many text base machine learn task however current word embed approach similarity embed space optimize learn paper propose novel neighbor embed method directly learn embed simplex similarities map word optimal term minimal discrepancy input neighborhoods method build upon two step random walk word via topics thus able better reveal topics among word experiment result indicate method compare another exist word embed approach favorable various query
text corpora widely use resources measure societal bias stereotype common approach measure bias use corpus calculate similarities embed vector word like nurse vectors representative word concepts interest genders study show depend one aim quantify bias commonly use approach introduce non relevant concepts bias measurement propose alternative approach bias measurement utilize smooth first order co occurrence relations word representative concept word derive reconstruct co occurrence estimate inherent word embed model compare approach conduct several experiment scenario measure gender bias occupational word accord english wikipedia corpus experiment show higher correlations measure gender bias actual gender bias statistics yous job market two collections variety word embed model use first order approach comparison vector similarity base approach first order approach also suggest severe bias towards female specific occupations approach
introduce architecture learn joint multilingual sentence representations ninety-three languages belong thirty different families write twenty-eight different script system use single bilstm encoder share bpe vocabulary languages couple auxiliary decoder train publicly available parallel corpora enable us learn classifier top result embeddings use english annotate data transfer ninety-three languages without modification experiment cross lingual natural language inference xnli dataset cross lingual document classification mldoc dataset parallel corpus mine bucc dataset show effectiveness approach also introduce new test set align sentence one hundred and twelve languages show sentence embeddings obtain strong result multilingual similarity search even low resource languages implementation pre train encoder multilingual test set available https githubcom facebookresearch laser
automatic summarization natural language current topic computer science research industry study decades usefulness across multiple domains example summarization necessary create review one research applications achieve success extractive summarization key sentence curated however abstractive summarization synthesis state hard problem generally unsolved computer science literature review contrast historical progress current state art compare dimension extractive vs abstractive supervise vs unsupervised nlp natural language process vs knowledge base deep learn vs algorithms structure vs unstructured source measurement metrics rouge bleu multiple dimension contrast since current research use combinations approach see review matrix throughout summary synthesis critique provide review conclude insights improve abstractive summarization measurement surprise implications detect understand comprehension general
paper present meet bot reinforcement learn base conversational system interact multiple users schedule meet system able interpret user utterences map prefer time slot feed reinforcement learn rl system goal converge agreeable time slot rl system able adapt user preferences environmental change meet arrival rate still schedule effectively learn perform via policy gradient exploration utilize mlp approximator policy function result demonstrate system outperform standard schedule algorithms term overall schedule efficiency additionally system able adapt strategy situations users consistently reject accept meet certain slot friday afternoon versus thursday morning meet call members senior designation
hierarchical text classification aim classify text document give hierarchy important task many real world applications recently deep neural model gain increase popularity text classification due expressive power minimum requirement feature engineer however apply deep neural network hierarchical text classification remain challenge heavily rely large amount train data meanwhile easily determine appropriate level document hierarchical set paper propose weakly supervise neural method hierarchical text classification method require large amount train data require easy provide weak supervision signal class relate document keywords method effectively leverage weak supervision signal generate pseudo document model pre train perform self train real unlabeled data iteratively refine model train process model feature hierarchical neural structure mimic give hierarchy capable determine proper level document block mechanism experiment three datasets different domains demonstrate efficacy method compare comprehensive set baselines
deep text match approach widely study many applications include question answer information retrieval systems deal domain insufficient label data approach use transfer learn tl set leverage label data resource rich source domain achieve better performance source domain data selection essential process prevent negative transfer problem however emerge deep transfer model fit well exist data selection methods data selection policy transfer learn model jointly train lead sub optimal train efficiency paper propose novel reinforce data selector select high quality source domain data help tl model specifically data selector act source domain data find subset optimization tl model performance tl model provide reward turn update selector build reinforce data selector base actor critic framework integrate dnn base transfer learn model result reinforce transfer learn rtl method perform thorough experimental evaluation two major task text match namely paraphrase identification natural language inference experimental result show propose rtl significantly improve performance tl model investigate different settings state reward policy optimization methods examine robustness method last conduct case study select data find method able select source domain data whose wasserstein distance close target domain data reasonable intuitive source domain data provide transferability power model
correct interpretation quantifier statements context visual scene require non trivial inference mechanisms example discuss two strategies rely fundamentally different cognitive concepts aim identify strategy deep learn model visual question answer learn train question end carefully design data replicate experiment psycholinguistics question investigate humans focus film visual question answer model experiment indicate form approximate number system emerge whose performance decline difficult scenes predict weber law moreover identify confound factor like spatial arrangement scene impede effectiveness system
retrieve index document topical content write style open door number applications information retrieval ir one application retrieve textual content certain author x query ir system provide beforehand set reference texts x authorship verification av research subject field digital text forensics suitable purpose task av determine two document ie index reference document write author x even though av represent unary classification problem number exist approach consider binary classification task however underlie classification model av method number serious implications regard prerequisites evaluability applicability comprehensive literature review observe several misunderstand regard differentiation unary binary av approach require consideration objective paper therefore clarify propose clear criteria new properties aim improve characterization exist future av approach give investigate applicability eleven exist unary binary av methods well four generic unary classification algorithms two self compile corpora furthermore highlight important issue concern evaluation av methods base fix decision criterions pay attention previous av study
researchers financial professionals require robust computerize tool allow users rapidly operationalize assess semantic textual content financial news however exist methods commonly work document level deeper insights actual structure sentiment individual sentence remain blur result investors require apply utmost attention detail domain specific knowledge order assess information fine grain basis facilitate manual process paper propose use distribute text representations multi instance learn transfer information document level sentence level compare alternative approach method feature superior predictive performance preserve context interpretability analysis manually label dataset yield predictive accuracy six thousand, nine hundred and ninety exceed performance alternative approach least three hundred and eighty percentage point accordingly study benefit investors regard financial decision make also help company communicate message intend
twitter prominent social media platform mine population level health data accurate cluster health relate tweet topics important extract relevant health insights work propose deep convolutional autoencoders learn compact representations health relate tweet employ cluster compare method several conventional tweet representation methods include bag word term frequency inverse document frequency latent dirichlet allocation non negative matrix factorization three different cluster algorithms result show cluster performance use propose representation learn scheme significantly outperform conventional methods experiment different number cluster addition propose constraint learn representations neural network train order enhance cluster performance study introduce utilization deep neural network base architectures ie deep convolutional autoencoders learn informative representations health relate tweet
whether enjoy lucid prose favorite author slog writer cumbersome heavy set prattle full parentheses dash compound adjectives oxford commas readers notice stylistic signatures word choice grammar also punctuation indeed visual sequence punctuation different author produce marvelously different visually strike sequence punctuation largely overlook stylistic feature stylometry quantitative analysis write text paper examine punctuation sequence corpus literary document ask follow question properties sequence distinctive feature different author possible distinguish literary genres base punctuation sequence punctuation style author evolve time something interest try stylometry without word full sound fury signify nothing
conversational agents explode popularity however much work remain area social conversation well free form conversation broad range domains topics advance state art conversational ai amazon launch alexa prize twenty-five million dollar university competition sixteen select university team challenge build conversational agents know socialbots converse coherently engagingly humans popular topics sport politics entertainment fashion technology twenty minutes alexa prize offer academic community unique opportunity perform research live system use millions users competition provide university team real user conversational data scale along user provide rat feedback augment annotations alexa team enable team effectively iterate make improvements throughout competition evaluate real time live user interactions build socialbots university team combine state art techniques novel strategies areas natural language understand context model dialog management response generation knowledge acquisition support efforts participate team alexa prize team make significant scientific engineer investments build improve conversational speech recognition topic track dialog evaluation voice user experience tool traffic management scalability paper outline advance create university team well alexa prize team achieve common goal solve problem conversational ai
dialog evaluation challenge problem especially non task orient dialogs conversational success well define propose evaluate dialog quality use topic base metrics describe ability conversational bot sustain coherent engage conversations topic diversity topics bot handle detect conversation topics per utterance adopt deep average network dan train topic classifier variety question query data categorize multiple topics propose novel extension dan add topic word attention table allow system jointly capture topic keywords utterance perform topic classification compare propose topic base metrics rat provide users show metrics correlate complement human judgment analysis perform tens thousands real human bot dialogs alexa prize competition highlight user expectations conversational bots
conversational agents explode popularity however much work remain area non goal orient conversations despite significant growth research interest recent years advance state art conversational ai amazon launch alexa prize twenty-five million dollar university competition sixteen select university team build conversational agents deliver best social conversational experience alexa prize provide academic community unique opportunity perform research live system use millions users subjectivity associate evaluate conversations key element underlie challenge build non goal orient dialogue systems paper propose comprehensive evaluation strategy multiple metrics design reduce subjectivity select metrics correlate well human judgement propose metrics provide granular analysis conversational agents capture human rat show metrics use reasonable proxy human judgment provide mechanism unify metrics select top perform agents also apply throughout alexa prize competition knowledge date largest set evaluate agents millions conversations hundreds thousands rat users believe work step towards automatic evaluation process conversational ais
current study mechanism extract traffic relate information congestion incidents textual data internet propose current source data twitter data consider extremely large size automate model develop stream download mine data real time furthermore tweet traffic relate information model able infer extract data currently data collect unite state total one hundred and twenty thousand geo tag traffic relate tweet extract six million geo tag non traffic relate tweet retrieve classification model train furthermore data use various kinds spatial temporal analysis mechanism calculate level traffic congestion safety traffic perception cities yous propose traffic congestion safety rank various urban areas obtain statistically validate exist widely adopt rank traffic perception depict attitude perception people towards traffic also see traffic relate data visualize spatially temporally provide pattern actual traffic flow various urban areas visualize city level clearly visible flow tweet similar flow vehicles traffic relate tweet representative traffic within cities find current study show significant amount traffic relate information extract twitter source internet furthermore twitter data source freely available bind spatial temporal limitations wherever user potential data
present milabot deep reinforcement learn chatbot develop montreal institute learn algorithms mila amazon alexa prize competition milabot capable converse humans popular small talk topics speech text system consist ensemble natural language generation retrieval model include neural network template base model apply reinforcement learn crowdsourced data real world user interactions system train select appropriate response model ensemble system evaluate b test real world users perform significantly better systems result highlight potential couple ensemble systems deep reinforcement learn fruitful path develop real world open domain conversational agents
usage language solely reliant cognition arguably determine myriad external factor lead global variability linguistic pattern issue lie core sociolinguistics back many small scale study face face communication address construct dataset combine largest french twitter corpus date detail socioeconomic map obtain national census france show key linguistic variables measure individual twitter stream depend factor like socioeconomic status location time social network individuals find people higher socioeconomic status active greater degree daytime use standard language ii southern part country prone use standard language northern one locally use variety dialect determine spatial distribution socioeconomic status iii individuals connect social network closer linguistically disconnect ones even effect status homophily remove result inform sociolinguistic theory may inspire novel learn methods inference socioeconomic status people way tweet
speech emotion recognition ser important aspect effective human robot collaboration receive lot attention research community example many neural network base architectures propose recently push performance new level however applicability neural ser model train domain data noisy condition currently research work evaluate robustness state art neural acoustic emotion recognition model human robot interaction scenarios hypothesize robot ego noise room condition various acoustic events occur home environment significantly affect performance model conduct several experiment icub robot platform propose several novel ways reduce gap model performance train test real world condition furthermore observe large improvements model performance robot demonstrate necessity introduce several data augmentation techniques like overlay background noise loudness variations improve robustness neural approach
deep generative model achieve great success unsupervised learn ability capture complex nonlinear relationships latent generate factor observations among factorize hierarchical variational autoencoder fhvae variational inference base model formulate hierarchical generative process sequential data specifically fhvae model learn disentangle interpretable representations prove useful numerous speech applications speaker verification robust speech recognition voice conversion however elaborate paper train algorithm propose original paper scalable datasets thousands hours make model less applicable larger scale identify limitations term runtime memory hyperparameter optimization propose hierarchical sample train algorithm address three issue propose method evaluate comprehensively wide variety datasets range three one thousand hours involve different type generate factor record condition noise type addition also present new visualization method qualitatively evaluate performance respect interpretability disentanglement model train propose algorithm demonstrate desire characteristics datasets
online speech recognition crucial develop natural human machine interfaces modality however significantly challenge line asr since real time low latency constraints inevitably hinder use future information know helpful perform robust predictions popular solution mitigate issue consist feed neural acoustic model context windows gather future frame introduce latency depend number employ look ahead feature paper explore different approach base estimate future rather wait technique encourage hide representations unidirectional recurrent network embed useful information future inspire recently propose technique call twin network add regularization term force forward hide state close possible cotemporal backward ones compute twin neural network run backwards time experiment conduct number datasets recurrent architectures input feature acoustic condition show effectiveness approach one important advantage method introduce additional computation test time compare standard unidirectional recurrent network
measure distance concepts important field study natural language process use improve task relate interpretation concepts wordnet include wide variety concepts associate word ie synsets often use source compute distance paper explore distance wordnet synsets base visual feature instead lexical ones purpose extract graphic feature generate within deep convolutional neural network train imagenet use feature generate representative synset base representatives define distance measure synsets complement traditional lexical distance finally propose experiment evaluate performance compare current state art
paper propose end end approach single channel speaker independent multi speaker speech separation time frequency f mask short time fourier transform stft inverse represent layer within deep network previous approach rather compute loss reconstruct signal use surrogate loss base target stft magnitudes ignore reconstruction error introduce phase inconsistency approach loss function directly define reconstruct signal optimize best separation addition train unfold iterations phase reconstruction algorithm represent series stft inverse stft layer mask value typically limit lie zero one approach use mixture phase reconstruction limitation less relevant estimate magnitudes use together phase reconstruction thus propose several novel activation function output layer f mask allow mask value beyond one publicly available wsj0 2mix dataset approach achieve state art one hundred and twenty-six db scale invariant signal distortion ratio si sdr one hundred and thirty-one db sdr reveal new possibilities deep learn base phase reconstruction represent fundamental progress towards solve notoriously hard cocktail party problem
attention mechanisms biological perception think select subsets perceptual information sophisticate process would prohibitive perform sensory input computer vision however relatively little exploration hard attention information selectively ignore spite success soft attention information weight aggregate never filter introduce new approach hard attention find achieve competitive performance recently release visual question answer datasets equal case surpass similar soft attention architectures entirely ignore feature even though hard attention mechanism think non differentiable find feature magnitudes correlate semantic relevance provide useful signal mechanism attentional selection criterion hard attention select important feature input information also efficient analogous soft attention mechanisms especially important recent approach use non local pairwise operations whereby computational memory cost quadratic size set feature
supervise machine learn author name disambiguation negative train data often dominantly larger positive train data paper examine ratios negative positive train data affect performance machine learn algorithms disambiguate author name bibliographic record multiple label datasets three classifiers logistic regression nai bay random forest train representative feature coauthor name title word extract train data various positive negative train data ratios result show increase negative train data improve disambiguation performance percent performance gain sometimes degrade logistic regression nai bay learn optimal disambiguation model even base ratio eleven positive negative train data also performance improvement random forest tend quickly saturate roughly one hundred and ten one hundred and fifteen find imply contrary common practice use train data name disambiguation algorithms train use part negative train data without degrade much disambiguation performance increase computational efficiency study call attention author name disambiguation scholars methods machine learn imbalanced data
global style tokens gsts recently propose method learn latent disentangle representations high dimensional data gsts use within tacotron state art end end text speech synthesis system uncover expressive factor variation speak style work introduce text predict global style token tp gst architecture treat gst combination weight style embeddings virtual speak style label within tacotron tp gst learn predict stylistic render text alone require neither explicit label train auxiliary input inference show train dataset expressive speech system generate audio pitch energy variation two state art baseline model demonstrate tp gsts synthesize speech background noise remove corroborate analyse positive result human rat listener preference audiobook task finally demonstrate multi speaker tp gst model successfully factorize speaker identity speak style provide website audio sample find
ever grow datasets publish link open data mainly contain encyclopedic information however lack quality structure semantically annotate datasets extract unstructured real time source paper present principles develop knowledge graph interlink events use case study news headline publish twitter real time eventful source fresh information represent essential pipeline contain require task range choose background data model event annotation ie event recognition classification entity annotation eventually interlink events state art limit domain specific scenarios recognize classify events whereas paper play role domain agnostic road map develop knowledge graph interlink events
computational model human multimodal language emerge research area natural language process span language visual acoustic modalities comprehend multimodal language require model interactions within modality intra modal interactions importantly interactions modalities cross modal interactions paper propose recurrent multistage fusion network rmfn decompose fusion problem multiple stag focus subset multimodal signal specialize effective fusion cross modal interactions model use multistage fusion approach build upon intermediate representations previous stag temporal intra modal interactions model integrate propose fusion approach system recurrent neural network rmfn display state art performance model human multimodal language across three public datasets relate multimodal sentiment analysis emotion recognition speaker traits recognition provide visualizations show stage fusion focus different subset multimodal signal learn increasingly discriminative multimodal representations
grow need analyze large collections document lead great developments topic model since document frequently associate relate variables label rat much interest place supervise topic model however nature annotation task prone ambiguity noise often high volumes document deem learn single annotator assumption unrealistic unpractical real world applications article propose two supervise topic model one classification another regression problems account heterogeneity bias among different annotators encounter practice learn crowd develop efficient stochastic variational inference algorithm able scale large datasets empirically demonstrate advantage propose model state art approach
recent election surprise regime change leave impression politics become fast move unstable modern politics seem volatile little systematic evidence support claim paper seek address gap knowledge report data last seventy years use public opinion poll traditional media data uk germany countries good case study experience considerable change electoral behaviour new political party time period study measure volatility public opinion media coverage use approach information theory track change word use pattern across seven hundred thousand article preliminary analysis suggest increase number opinion issue time growth lack predictability media series 1970s
visual question answer vqa require ai model comprehend data two domains vision text current state art model use learn attention mechanisms extract relevant information input domains answer certain question thus robust attention mechanisms essential powerful vqa model paper propose recurrent attention mechanism show benefit compare traditional convolutional approach perform two ablation study evaluate recurrent attention first introduce baseline vqa model visual attention test performance difference convolutional recurrent attention vqa twenty dataset secondly design architecture vqa utilize dual textual visual recurrent attention units raus use model show effect possible combinations recurrent convolutional dual attention single model outperform first place winner vqa two thousand and sixteen challenge best knowledge second best perform single model vqa ten dataset furthermore model noticeably improve upon winner vqa two thousand and seventeen challenge moreover experiment replace attention mechanisms state art model raus show increase performance
opinion poll bridge public opinion politicians elections however develop survey disclose people feedback respect economic issue limit expensive time consume recent years social media twitter enable people share opinions regard elections social media provide platform collect large amount social media data paper propose computational public opinion mine approach explore discussion economic issue social media election current relate study use text mine methods independently election analysis election prediction research combine two text mine methods sentiment analysis topic model propose approach effectively deploy millions tweet analyze economic concern people two thousand and twelve us presidential election
propose efficient neural architecture search enas fast inexpensive approach automatic model design enas controller learn discover neural network architectures search optimal subgraph within large computational graph controller train policy gradient select subgraph maximize expect reward validation set meanwhile model correspond select subgraph train minimize canonical cross entropy loss thank parameter share child model enas fast deliver strong empirical performances use much fewer gpu hours exist automatic model design approach notably 1000x less expensive standard neural architecture search penn treebank dataset enas discover novel architecture achieve test perplexity five hundred and fifty-eight establish new state art among methods without post train process cifar ten dataset enas design novel architectures achieve test error two hundred and eighty-nine par nasnet zoph et al two thousand and eighteen whose test error two hundred and sixty-five
multi channel speech enhancement ad hoc sensors challenge task speech model guide beamforming algorithms able recover natural sound speech speech model tend oversimplify inference would otherwise complicate hand deep learn base enhancement approach able learn complicate speech distributions perform efficient inference unable deal variable number input channel also deep learn approach introduce lot errors particularly presence unseen noise type settings therefore propose enhancement framework call deepbeam combine two complementary class algorithms deepbeam introduce beamforming filter produce natural sound speech filter coefficients determine help monaural speech enhancement neural network experiment synthetic real world data show deepbeam able produce clean dry natural sound speech robust unseen noise
time delay neural network tdnns effective acoustic model large vocabulary speech recognition strength model attribute ability effectively model long temporal contexts however current tdnn model relatively shallow limit model capability paper propose method increase network depth deepen kernel use tdnn temporal convolutions best perform kernel consist three fully connect layer residual resnet connection output first output third addition spectro temporal process input tdnn form convolutional neural network cnn newly design grid rnn investigate grid rnn strongly outperform cnn different set parameters different frequency band use enhance use bi directional grid rnn experiment use multi genre broadcast mgb3 english data 275h show deep kernel tdnns reduce word error rate wer six relative combine frequency dependent grid rnn give relative wer reduction nine
introduce distance entropy measure homogeneity distribution path lengths give node neighbour complex network distance entropy define new centrality measure whose properties investigate variety synthetic network model couple distance entropy information closeness centrality introduce network cartography allow one reduce degeneracy rank base closeness alone apply methodology empirical multiplex lexical network encode linguistic relationships know english speak toddlers show distance entropy cartography better predict children learn word compare closeness centrality result highlight importance distance entropy gain insights distance pattern complex network
performance automatic speech recognition asr systems significantly compromise previously unseen condition typically due mismatch train test distributions paper address robustness study domain invariant feature domain information become transparent asr systems resolve mismatch problem specifically investigate recent model call factorize hierarchical variational autoencoder fhvae fhvaes learn factorize sequence level segment level attribute different latent variables without supervision argue set latent variables contain segment level information desire domain invariant feature asr experiment conduct aurora four chime four demonstrate forty-one twenty-seven absolute word error rate reductions respectively mismatch domains
deep learn still common tool speaker verification field study deep convolutional neural network performance text prompt speaker verification task prompt passphrase segment word state ie digits test digit utterance separately train single high level feature extractor state use cosine similarity metric score key feature network max feature map activation function act embed feature selector use multitask learn scheme train high level feature extractor able surpass classic baseline systems term quality achieve impressive result novice approach get two hundred and eighty-five ever rsr2015 evaluation set fusion propose baseline systems improve result
paper describe solution tackle common set challenge e commerce arise fact new products continually add catalogue challenge involve properly personalise customer experience forecast demand plan product range argue foundational piece solve problems consistent detail information product information rarely available consistent give multitude suppliers type products describe detail architecture methodology implement asos one world largest fashion e commerce retailers tackle problem show quantitative understand products leverage improve recommendations hybrid recommender system approach
present general method privacy preserve bayesian inference poisson factorization broad class model include widely use model social sciences method satisfy limit precision local privacy generalization local differential privacy introduce formulate privacy guarantee appropriate sparse count data develop mcmc algorithm approximate locally private posterior model parameters give data locally privatize geometric mechanism ghosh et al two thousand and twelve solution base two insights one novel reinterpretation geometric mechanism term skellam distribution skellam one thousand, nine hundred and forty-six two general theorem relate skellam bessel distribution yuan kalbfleisch two thousand demonstrate method two case study real world email data show method consistently outperform commonly use naive approach obtain higher quality topics text accurate link prediction network task privacy preserve method even outperform non private inference condition true data
major depressive disorder common mental disorder affect almost seven adult yous population two thousand and seventeen audio visual emotion challenge avec ask participants build model predict depression level base audio video text interview range seven thirty-three minutes since average feature entire interview lose temporal information discover capture preserve useful temporal detail long interview significant challenge therefore propose novel topic model base approach perform context aware analysis record experiment show propose approach outperform context unaware methods challenge baselines metrics
semi supervise bootstrapping techniques relationship extraction text iteratively expand set initial seed instance due lack label data key challenge bootstrapping semantic drift false positive instance add iteration follow iterations contaminate introduce brex new bootstrapping method protect contamination highly effective confidence assessment achieve use entity template seed jointly oppose one previous work expand entities templates parallel mutually constrain fashion iteration introduce higherquality similarity measure templates experimental result show brex achieve f1 thirteen eighty-seven vs seventy-four better state art four relationships
front end factor analysis fefa extension principal component analysis ppca tailor use gaussian mixture model gmms currently prevalent approach extract compact utterance level feature vectors automatic speaker verification asv systems little research conduct compare fefa conventional ppca apply maximum posteriori map adapt gmm supervectors study several alternative methods include ppca factor analysis fa two supervise approach supervise ppca sppca recently propose probabilistic partial least square ppls compress map adapt gmm supervectors result vectors use asv task probabilistic linear discriminant analysis plda back end experiment two different datasets telephone condition nist sre two thousand and ten recent voxceleb corpus collect youtube videos contain celebrity interview record various acoustical technical condition result suggest term asv accuracy supervector compression approach par fefa supervise approach result improve performance comparison fefa obtain hundred fold 100x speedups total variability model tvm train use ppca fa supervector compression approach
multimodal machine translation one applications integrate computer vision language process unique task give field machine translation many state arts algorithms still employ textual information work explore effectiveness reinforcement learn multimodal machine translation present novel algorithm base advantage actor critic a2c algorithm specifically cater multimodal machine translation task emnlp two thousand and eighteen third conference machine translation wmt18 experiment propose algorithm multi30k multilingual english german image description dataset flickr30k image entity dataset model take two channel input image text use translation evaluation metrics train reward achieve better result supervise learn mle baseline model furthermore discuss prospect limitations use reinforcement learn machine translation experiment result suggest promise reinforcement learn solution general task multimodal sequence sequence learn
debate phoneme viseme units effective lipread system study use phoneme units even though phonemes describe unique short sound study try improve lipread accuracy focus visemes vary result compare performance lipread system model visual speech use either thirteen viseme thirty-eight phoneme units report accuracy system word unit level evaluation task large vocabulary continuous speech use tcd timit corpus complete visual speech model via hybrid dnn hmms visual speech decoder weight finite state transducer wfst use dct eigenlips representation mouth roi image phoneme lipread system word accuracy outperform viseme base system word accuracy however phoneme system achieve lower accuracy unit level show importance dictionary decode classification output word
one main challenge online social systems face prevalence antisocial behavior harassment personal attack work introduce task predict start conversation whether get hand oppose detect undesirable behavior fact task aim enable early actionable prediction time conversation might still salvage end develop framework capture pragmatic devices politeness strategies rhetorical prompt use start conversation analyze relation future trajectory apply framework control set demonstrate feasibility detect early warn sign antisocial behavior online discussions
many research field codify find standard format often report correlations quantities interest space testable correlate far larger scientific resources currently address ability accurately predict correlations would useful plan research allocate resources use dataset approximately one hundred and seventy thousand correlational find extract lead social science journals show train neural network accurately predict report correlations use text descriptions correlate accurate predictive model guide scientists towards promise untested correlate better quantify information gain new find implications move artificial intelligence systems predict structure predict relationships real world
past decades amount scientific article technical literature increase exponentially size consequently great need systems ingest document scale make content discoverable unfortunately format document eg pdf format bitmap image well presentation data eg complex table make extraction qualitative quantitive data extremely challenge present platform ingest document scale power machine learn techniques allow user train custom model document collections show precision recall result greater ninety-seven regard conversion structure format well scale evidence microservices constitute platform
recently visual question answer vqa emerge one significant task multimodal learn require understand visual textual modalities exist methods mainly rely extract image question feature learn joint feature embed via multimodal fusion attention mechanism recent study utilize external vqa independent model detect candidate entities attribute image serve semantic knowledge complementary vqa task however candidate entities attribute might unrelated vqa task limit semantic capacities better utilize semantic knowledge image propose novel framework learn visual relation facts vqa specifically build relation vqa r vqa dataset base visual genome dataset via semantic similarity module data consist image correspond question correct answer support relation fact well define relation detector adopt predict visual question relate relation facts propose multi step attention model compose visual attention semantic attention sequentially extract relate visual knowledge semantic knowledge conduct comprehensive experiment two benchmark datasets demonstrate model achieve state art performance verify benefit consider visual relation facts
past years consumer review sit become main target deceptive opinion spam fictitious opinions review deliberately write sound authentic exist work detect deceptive review focus build supervise classifiers base syntactic lexical pattern opinion successful use neural network various classification applications paper propose fakegan system first time augment adopt generative adversarial network gans text classification task particular detect deceptive review unlike standard gin model single generator discriminator model fakegan use two discriminator model one generative model generator model stochastic policy agent reinforcement learn rl discriminators use monte carlo search algorithm estimate pass intermediate action value rl reward generator provide generator model two discriminator model avoid mod collapse issue learn distributions truthful deceptive review indeed experiment show use two discriminators provide fakegan high stability know issue gin architectures fakegan build upon semi supervise classifier know less accuracy evaluation result dataset tripadvisor hotel review show performance term accuracy state art approach apply supervise machine learn result indicate gans effective text classification task specifically fakegan effective detect deceptive review
multimodal sensory data resemble form information perceive humans learn easy obtain large quantities compare unimodal data synchronization concepts modalities data provide supervision disentangle underlie explanatory factor modality previous work leverage multimodal data mainly focus retain modality invariant factor discard rest paper present partition variational autoencoder pvae several train objectives learn disentangle representations encode share factor also modality dependent ones separate latent variables specifically pvae integrate variational inference framework multimodal generative model partition explanatory factor condition relevant subset generation evaluate model two parallel speech image datasets demonstrate ability learn disentangle representations qualitatively explore within modality cross modality conditional generation semantics style specify examples quantitative analysis evaluate classification accuracy automatically discover semantic units pvae achieve ninety-nine accuracy modalities
propose adversarial learn approach generate multi turn dialogue responses propose framework hredgan base conditional generative adversarial network gans gin generator modify hierarchical recurrent encoder decoder network hred discriminator word level bidirectional rnn share context word embeddings generator inference noise sample condition dialogue history use perturb generator latent space generate several possible responses final response one rank best discriminator hredgan show improve performance exist methods one generalize better network train use log likelihood criterion two generate longer informative diverse responses high utterance topic relevance even limit train data improvement demonstrate movie triple ubuntu dialogue datasets use automatic human evaluations
present probabilistic framework study adversarial attack discrete data base framework derive perturbation base method greedy attack scalable learn base method gumbel attack illustrate various tradeoffs design attack demonstrate effectiveness methods use quantitative metrics human evaluation various state art model text classification include word base cnn character base cnn lstm example result show accuracy character base convolutional network drop level random selection modify five character greedy attack
introduce method follow high level navigation instructions map directly image instructions pose estimate continuous low level velocity command real time control ground semantic map network gsmn fully differentiable neural network architecture build explicit semantic map world reference frame incorporate pinhole camera projection model within network information store map learn experience local world transformation compute explicitly train model use daggerfm modify variant dagger trade tabular convergence guarantee improve train speed memory use test gsmn virtual environments realistic quadcopter simulator show incorporate explicit map ground modules allow gsmn outperform strong neural baselines almost reach expert policy performance finally analyze learn map representations show use explicit map lead interpretable instruction follow model
future predictions sequence data eg videos audios require algorithms capture non markovian compositional properties high level semantics context free grammars natural choices capture properties traditional grammar parsers eg earley parser take symbolic sentence input paper generalize earley parser parse sequence data neither segment label generalize earley parser integrate grammar parser classifier find optimal segmentation label make top future predictions experiment show method significantly outperform approach future human activity prediction
paraphrase restatement mean text word paraphrase study enhance performance many natural language process task paper propose novel task iparaphrasing extract visually ground paraphrase vgps different phrasal expressions describe visual concept image extract vgps potential improve language image multimodal task visual question answer image caption model similarity vgps key iparaphrasing apply various exist methods well propose novel neural network base method image attention report result first attempt toward iparaphrasing
current trend automatic speech recognition leverage large amount label data train supervise neural network model unfortunately obtain data wide range domains train robust model costly however relatively inexpensive collect large amount unlabeled data domains want model generalize paper propose novel unsupervised adaptation method learn synthesize label data target domain unlabeled domain data label domain data first learn without supervision interpretable latent representation speech encode linguistic nuisance factor eg speaker channel use different latent variables transform label domain utterance without alter transcript transform latent nuisance variables maintain linguistic variables demonstrate approach focus channel mismatch set domain interest distant conversational speech label available close talk speech propose method evaluate ami dataset outperform baselines bridge gap unadapted domain model seventy-seven without use parallel data
deep neural network dnns achieve impressive predictive performance due ability learn complex non linear relationships variables however inability effectively visualize relationships lead dnns characterize black box consequently limit applications ameliorate problem introduce use hierarchical interpretations explain dnn predictions propose method agglomerative contextual decomposition acd give prediction train dnn acd produce hierarchical cluster input feature along contribution cluster final prediction hierarchy optimize identify cluster feature dnn learn predictive use examples stanford sentiment treebank imagenet show acd effective diagnose incorrect predictions identify dataset bias human experiment demonstrate acd enable users identify accurate two dnns better trust dnn output also find acd hierarchy largely robust adversarial perturbations imply capture fundamental aspects input ignore spurious noise
electronic healthcare record contain large volumes unstructured data include extensive free text yet source detail information often remain use lack methodologies extract interpretable content timely manner apply network theoretical tool analyse free text hospital patient incident report national health service find cluster document similar content unsupervised manner different level resolution combine deep neural network paragraph vector text embed multiscale markov stability community detection apply sparsified similarity graph document vectors showcase approach incident report imperial college healthcare nhs trust london multiscale community structure reveal different level mean topics dataset show descriptive term extract cluster record also compare posteriori hand cod categories assign healthcare personnel show approach outperform lda base model content cluster exhibit good correspondence two level hand cod categories yet also provide medical detail certain areas reveal complementary descriptors incidents beyond external classification taxonomy
consider problem neural semantic parse translate natural language question executable sql query introduce new mechanism execution guidance leverage semantics sql detect exclude faulty program decode procedure condition execution partially generate program mechanism use autoregressive generative model demonstrate four state art recurrent template base semantic parse model demonstrate execution guidance universally improve model performance various text sql datasets different scale query complexity wikisql atis geoquery result achieve new state art execution accuracy eight hundred and thirty-eight wikisql
deep reinforcement learn recently show many impressive successes however one major obstacle towards apply methods real world problems lack data efficiency end propose bottleneck simulator model base reinforcement learn method combine learn factorize transition model environment rollout simulations learn effective policy examples learn transition model employ abstract discrete bottleneck state increase sample efficiency reduce number model parameters exploit structural properties environment provide mathematical analysis bottleneck simulator term fix point learn policy reveal performance affect four distinct source error error relate abstract space structure error relate transition model estimation variance error relate transition model estimation bias error relate transition model class bias finally evaluate bottleneck simulator two natural language process task text adventure game real world complex dialogue response selection task task bottleneck simulator yield excellent performance beat compete approach
herein generate pseudo feature base multivariate probability distributions obtain feature map layer train deep neural network augment minor class data base generate pseudo feature overcome imbalanced data problems propose method ie cavity fill improve deep learn capabilities several problems real world data observe imbalanced
work propose new solution parallel wave generation wavenet contrast parallel wavenet van den oord et al two thousand and eighteen distill gaussian inverse autoregressive flow autoregressive wavenet minimize regularize kl divergence highly peak output distributions method compute kl divergence close form simplify train algorithm provide efficient distillation addition introduce first text wave neural architecture speech synthesis fully convolutional enable fast end end train scratch significantly outperform previous pipeline connect text spectrogram model separately train wavenet ping et al two thousand and eighteen also successfully distill parallel waveform synthesizer condition hide representation end end model
paper examine degree current deep learn architectures image caption generation capture spatial language basis evaluation examples generate caption literature argue systems capture object image data object locate caption generate systems output language model condition output object detector capture fine grain location information although language model provide useful knowledge image caption argue deep learn image caption architectures also model geometric relations object
natural language process nlp do use either top theory drive bottom data drive approach call mechanistic phenomenological respectively approach frequently consider stand opposition examine recent approach deep learn argue deep neural network incorporate perspectives furthermore leverage aspect deep learn may help solve complex problems within language technology model language perception domain spatial cognition
vocal tract configurations play vital role generate distinguishable speech sound modulate airflow create different resonant cavities speech production contain abundant information utilize better understand underlie speech production mechanism step towards automatic map vocal tract shape geometry acoustics paper employ effective video action recognition techniques like long term recurrent convolutional network lrcn model identify different vowel consonant vowel vcv sequence dynamic shape vocal tract model typically combine cnn base deep hierarchical visual feature extractor recurrent network ideally make network spatio temporally deep enough learn sequential dynamics short video clip video classification task use database consist 2d real time mri vocal tract shape vcv utterances seventeen speakers comparative performances class algorithms various parameter settings various classification task discuss interestingly result show mark difference model performance context speech classification respect generic sequence video classification task
songs well arrange professional music curators form rivet playlist create engage listen experience however time consume curators timely rearrange playlists fit trend future exploit techniques deep learn reinforcement learn paper consider music playlist generation language model problem solve propose attention language model policy gradient develop systematic interactive approach result playlists tune flexibly accord user preferences consider playlist sequence word first train attention rnn language model baseline recommend playlists optimize suitable impose reward function model thus refine correspond preferences experimental result demonstrate approach generate coherent playlists automatically also able flexibly recommend personalize playlists diversity novelty freshness
permutation invariant gaussian matrix model recently develop applications computational linguistics five parameter family model solve paper use representation theoretic approach solve general thirteen parameter gaussian model view zero dimensional quantum field theory express two linear eleven quadratic term action term representation theoretic parameters parameters coefficients simple quadratic expressions term appropriate linear combinations matrix variables transform specific irreducible representations symmetric group sd size matrices allow identification constraints ensure convergent gaussian measure well define expectation value polynomial function random matrix order graph theoretic interpretation know allow enumeration permutation invariants matrices linear quadratic higher order express expectation value quadratic graph basis invariants selection cubic quartic invariants term representation theoretic parameters model
deep learn base speech enhancement source separation systems recently reach unprecedented level quality point performance reach new ceiling systems rely estimate magnitude target source estimate real value mask apply time frequency representation mixture signal limit factor approach lack phase estimation phase mixture often use reconstruct estimate time domain signal propose magbook phasebook combook three new type layer base discrete representations use estimate complex time frequency mask magbook layer extend classical sigmoidal units recently introduce convex softmax activation mask base magnitude estimation phasebook layer use similar structure give estimate phase mask without suffer phase wrap issue combook layer alternative magbook phasebook combination directly estimate complex mask present various train inference scheme involve representations explain particular include end end learn framework also present oracle study assess upper bound performance various type mask use discrete phase representations evaluate propose methods wsj0 2mix dataset well study corpus single channel speaker independent speaker separation match performance state art mask base approach without require additional phase reconstruction step
propose practical approach base federate learn solve domain issue continuously run embed speech base model wake word detectors conduct extensive empirical study federate average algorithm hey snip wake word base crowdsourced dataset mimic federation wake word users empirically demonstrate use adaptive average strategy inspire adam place standard weight model average highly reduce number communication round require reach target performance associate upstream communication cost per user estimate eight mb reasonable context smart home voice assistants additionally dataset use experiment open source aim foster transparent research application federate learn speech data
automatic understand domain specific texts order extract useful relationships later use non trivial task one relationship would railroad accidents cause correspondent descriptions report two thousand and one two thousand and sixteen rail accidents yous cost 46b railroad involve accidents require submit accident report federal railroad administration fra report contain variety fix field entries include primary accidents cod variable three hundred and eighty-nine value well narrative field short text description accident although narratives provide information fix field entry terminologies use report easy understand non expert reader therefore provide assist method fill primary domain specific textsnarratives would help label accidents accuracy another important question transportation safety whether report accident consistent narrative description address question apply deep learn methods together powerful word embeddings word2vec glove classify accident value primary field use text narratives result show approach accurately classify accident cause base report narratives find important inconsistencies accident report
paper describe fbk submission end end english german speech translation task iwslt two thousand and eighteen system rely state art model base lstms cnns cnns use reduce temporal dimension audio input general much higher machine translation input model train audio text parallel data release task fine tune clean subsets original train corpus addition weight normalization label smooth improve baseline system ten bleu point validation set final submission also feature checkpoint average within train run ensemble decode model train multiple run test data best single model obtain bleu score ninety-seven ensemble obtain bleu score one thousand and twenty-four
meta data photo share websites flickr use obtain rich bag word descriptions geographic locations prove valuable among others model predict ecological feature one important insight previous work descriptions obtain flickr tend complementary structure information available traditional scientific resources better integrate two diverse source information paper consider method learn vector space embeddings geographic locations show experimentally method improve exist approach especially case structure information available
audiovisual speaker conversion method present simultaneously transform facial expressions voice source speaker target speaker transform facial acoustic feature together make possible convert voice facial expressions highly correlate generate target speaker appear sound natural use three neural network conversion network fuse transform facial acoustic feature waveform generation network produce waveform convert facial acoustic feature image reconstruction network output rgb facial image also base convert feature result experiment use emotional audiovisual database show propose method achieve significantly higher naturalness compare one separately transform acoustic facial feature
previous research acoustic word embeddings use query example speak term detection show remarkable performance improvements use triplet network however triplet network train use limit information acoustic similarity word paper propose novel architecture phonetically associate triplet network patn aim increase discriminative power acoustic word embeddings utilize phonetic information well word identity propose model learn minimize combine loss function make introduce cross entropy loss lower layer lstm base triplet network observe propose method perform significantly better baseline triplet network word discrimination task wsj dataset result twenty relative improvement recall rate ten false alarm per hour finally examine generalization ability conduct domain test rm dataset
work three lattice free lf discriminative train criteria purely sequence train neural network acoustic model compare lvcsr task namely maximum mutual information mmi boost maximum mutual information bmmi state level minimum bay risk smbr demonstrate analogous lf mmi neural network acoustic model also train scratch use lf bmmi lf smbr criteria respectively without need cross entropy pre train furthermore experimental result switchboard 300hrs switchboardfisher 2100hrs datasets show model train lf bmmi consistently outperform train plain lf mmi achieve relative word error rate wer reduction five competitive temporal convolution project lstm tdnn lstmp lf mmi baselines
propose approach map natural language instructions raw observations continuous control quadcopter drone model predict interpretable position visitation distributions indicate agent go execution stop use predict distributions select action execute two step model decomposition allow simple efficient train use combination supervise learn imitation learn evaluate approach realistic drone simulator demonstrate absolute task completion accuracy improvements one thousand, six hundred and eighty-five two state art instruction follow methods
demonstrate application futamura projections human computer interaction particularly stag human computer dialogs specifically provide stag analogs classical futamura projections demonstrate futamura projections apply stag human computer dialogs addition execution program
task determine speaker native language base speeches second language know native language identification nli due increase applications various domains speech signal process emerge important research area recent time paper propose vector base approach develop automatic nli system use mfcc gfcc feature evaluation approach test framework two thousand and sixteen compare native language sub challenge dataset english language speakers eleven different native language background propose method outperform baseline system improvement accuracy two thousand, one hundred and ninety-five mfcc feature base vector framework two thousand, two hundred and eighty-one gfcc feature base vector framework
electronic healthcare record contain large volumes unstructured data different form free text constitute large portion data yet source richly detail information often remain use practice lack suitable methodologies extract interpretable content timely manner apply network theoretical tool analysis free text hospital patient incident report english national health service find cluster report unsupervised manner different level resolution base directly free text descriptions contain within combine recently develop deep neural network text embed methodologies base paragraph vectors multi scale markov stability community detection apply similarity graph document obtain sparsified text vector similarities showcase approach analysis incident report submit imperial college healthcare nhs trust london multiscale community structure reveal level mean different resolution topics dataset show relevant descriptive term extract group record well compare posteriori hand cod categories assign healthcare personnel content communities exhibit good correspondence well define hand cod categories yet result also provide medical detail certain areas well reveal complementary descriptors incidents beyond external classification also discuss method use monitor report time across different healthcare providers detect emerge trend fall outside pre exist categories
propose moodnet deep convolutional neural network base architecture effectively predict emotion associate piece music give audio lyrical contentwe evaluate different architectures consist vary number two dimensional convolutional subsampling layersfollowed dense layerswe use mel spectrograms represent audio content word embeddings specifically one hundred dimensional word vectors represent textual content represent lyricswe fee input data modalities moodnet architecturethe output modalities fuse fully connect layer softmax classfier use predict category emotionusing f1 score metricour result show excellent performance moodnet two datasets experiment mirex multimodal dataset million song datasetour experiment reflect hypothesis complex model perform better train datawe also observe lyric outperform audio better express modality conclude combine use feature multiple modalities prediction task result superior performance comparison use single modality input
recent character phoneme base parametric tts systems use deep learn show strong performance natural speech generation however choice character phoneme input create serious limitations practical deployment direct control pronunciation crucial certain case demonstrate simple method combine multiple type linguistic information single encoder name representation mix enable flexible choice character phoneme mix representations inference experiment user study public audiobook corpus show efficacy approach
much human knowledge encode text available scientific publications book web give rapid growth resources need automate methods extract knowledge machine processable structure knowledge graph important task process entity normalization consist map noisy entity mention text canonical entities well know reference set however entity normalization challenge problem often many textual form canonical entity may capture reference set entities mention text may include many syntactic variations errors problem particularly acute scientific domains biology address problem develop general scalable solution base deep siamese neural network model embed semantic information entities well syntactic variations use embeddings fast map new entities large reference set empirically show effectiveness framework challenge bio entity normalization datasets
explore application end end stateless temporal model small footprint keyword spot oppose recurrent network model long term temporal dependencies use internal state propose model inspire recent success dilate convolutions sequence model applications allow train deeper architectures resource constrain configurations gate activations residual connections also add follow similar configuration wavenet addition apply custom target label back propagate loss specific frame interest therefore yield higher accuracy require detect end keyword experimental result show model outperform max pool loss train recurrent neural network use lstm cells significant decrease false rejection rate underlie dataset hey snip utterances record 22k different speakers make publicly available establish open reference wake word detection
humans final decision makers critical task involve ethical legal concern range recidivism prediction medical diagnosis fight fake news although machine learn model sometimes achieve impressive performance task task amenable full automation realize potential machine learn improve human decisions important understand assistance machine learn model affect human performance human agency paper use deception detection testbed investigate harness explanations predictions machine learn model improve human performance retain human agency propose spectrum full human agency full automation develop vary level machine assistance along spectrum gradually increase influence machine predictions find without show predict label explanations alone slightly improve human performance end task comparison human performance greatly improve show predict label twenty relative improvement improve explicitly suggest strong machine performance interestingly predict label show explanations machine predictions induce similar level accuracy explicit statement strong machine performance result demonstrate tradeoff human performance human agency show explanations machine predictions moderate tradeoff
discussions host discussion forums moocs reference online learn resources often central importance contextualize discussion anchor discussion participants presentation issue understand however usually mention free text without appropriate hyperlinking associate resource automate learn resource mention hyperlinking categorization facilitate discussion search within mooc forums also benefit contextualization resources across disparate view propose novel problem learn resource mention identification mooc forums novel task publicly available data first contribute large scale label dataset dub forum resource mention form dataset facilitate current research future research task formulate task sequence tag problem investigate solution architectures address problem importantly identify two major challenge hinder application sequence tag model task one diversity resource mention expression two long range contextual dependencies address challenge incorporate character level thread context information lstm crf model first incorporate character encoder address vocabulary problem cause diversity mention expressions second address context dependency challenge encode thread contexts use rnn base context encoder apply attention mechanism selectively leverage useful context information sequence tag experiment form show propose method improve baseline deep sequence tag model notably significantly better performance instance exemplify two challenge
introduce task acoustic question answer aqa area acoustic reason task agent learn answer question basis acoustic context order promote research area propose data generation paradigm adapt clevr johnson et al two thousand and seventeen generate acoustic scenes leverage bank elementary sound also provide number functional program use compose question answer exploit relationships attribute elementary sound scene provide aqa datasets various size well data generation code preliminary experiment validate data report accuracy current state art visual question answer model apply aqa task without modifications although plethora question answer task base text image video data knowledge first propose answer question directly audio stream hope contribution facilitate development research area
recent years witness rise popularity natural language process nlp relate field artificial intelligence ai machine learn ml many online course resources available even without strong background field often student curious specific topic quite know begin study answer question one learn first apply embed base method learn prerequisite relations course concepts domain nlp introduce lecturebank dataset contain one thousand, three hundred and fifty-two english lecture file collect university course classify accord exist taxonomy well two hundred and eight manually label prerequisite relation topics publicly available dataset useful educational purpose lecture preparation organization well applications read list generation additionally experiment neural graph base network non neural classifiers learn prerequisite relations dataset
speech datasets identify alzheimer disease ad generally restrict participants perform single task eg describe image show result model train linguistic feature derive datasets may generalizable across task build prior work demonstrate task data healthy participants help improve ad detection single task dataset pathological speech augment ad specific dataset consist subject describe picture multi task healthy data demonstrate normative data multiple speech base task help improve ad detection nine visualization decision boundaries reveal model train combination structure picture descriptions unstructured conversational speech least task error show potential generalize multiple task analyze impact age add sample affect fairness classification also provide explanations possible inductive bias effect across task use model agnostic feature anchor work highlight need heterogeneous datasets encode change multiple facets cognition develop task independent ad detection model
learn good representations crucial importance deep learn mutual information mi similar measure statistical dependence promise tool learn representations unsupervised way even though mutual information two random variables hard measure directly high dimensional space recent study show implicit optimization mi achieve encoder discriminator architecture similar generative adversarial network gans work learn representations capture speaker identities maximize mutual information encode representations chunk speech randomly sample sentence propose encoder rely sincnet architecture transform raw speech waveform compact feature vector discriminator feed either positive sample joint distribution encode chunk negative sample product marginals train separate report experiment show approach effectively learn useful speaker representations lead promise result speaker identification verification task experiment consider unsupervised semi supervise settings compare performance achieve different objective function
exploration new superconductors still rely experience intuition experts largely process experimental trial error one study three candidate materials show superconductivity report first deep learn model find new superconductors introduce method name read periodic table represent periodic table way allow deep learn learn read periodic table learn law elements purpose discover novel superconductors outside train data recognize difficult deep learn predict something outside train data although use chemical composition materials information obtain r2 value ninety-two predict ttextc materials database superconductors also introduce method name garbage create synthetic data non superconductors exist non superconductors report data must require deep learn distinguish superconductors non superconductors obtain three remarkable result deep learn predict superconductivity material precision sixty-two show usefulness model find recently discover superconductor cabi2 another one hf05nb02v2zr03 neither superconductor database find fe base high temperature superconductors discover two thousand and eight train data two thousand and eight result open way discovery new high temperature superconductor families candidate materials list data method openly available link https githubcom tomo835g deep learn find superconductors
elicit semantic similarity concepts biomedical domain remain challenge task recent approach found embed vectors gain popularity rise efficiently capture semantic relationships underlie idea two word close mean gather similar contexts study propose new neural network model name mesh gram rely straighforward approach extend skip gram neural network model consider mesh medical subject head descriptors instead word train publicly available corpus pubmed medline mesh gram evaluate reference standards manually annotate semantic similarity mesh gram first compare skip gram vectors size three hundred several windows contexts deeper comparison perform tewenty exist model obtain result spearman rank correlations human score compute similarities show mesh gram outperform skip gram model comparable best methods need computation external resources
transcribe datasets typically contain speaker identity instance data investigate two ways incorporate information train multi task learn adversarial learn multi task learn goal speaker prediction expect performance improvement joint train two task speech recognition speaker recognition share common set underlie feature contrast adversarial learn mean learn representations invariant speaker expect better performance learn invariance help generalize new speakers two approach seem natural context speech recognition incompatible correspond opposite gradients back propagate model order better understand effect approach term error rat compare strategies control settings moreover explore use additional untranscribed data semi supervise adversarial learn manner improve error rat result show deep model train big datasets already develop invariant representations speakers without auxiliary loss consider adversarial learn multi task learn impact acoustic model seem minor however model train semi supervise manner improve error rat
present vision base navigation language base assistance vnla ground vision language task agent visual perception guide via language find object photorealistic indoor environments task emulate real world scenario requester may know navigate target object thus make request specify high level end goals b agent capable sense lose query advisor qualify task obtain language subgoals make progress model language base assistance develop general framework term imitation learn indirect intervention i3l propose solution effective vnla task empirical result show approach significantly improve success rate learn agent baselines see unseen environments code data publicly available https githubcom debadeepta vnla
end end text speech tts system greatly improve quality synthesise speech usually suffer form high time latency due auto regressive structure synthesise speech may also suffer error modes eg repeat word mispronunciations skip word paper propose novel non autoregressive fully parallel end end tts system fpets utilize new alignment model recently propose shape convolutional structure ufans different rnn ufans capture long term information fully parallel manner trainable position encode two step train strategy use learn better alignments experimental result show fpets utilize power parallel computation reach significant speed inference compare state art end end tts systems specifically fpets 600x faster tacotron2 50x faster dctts 10x faster deep voice3 fpets generate audios equal better quality fewer errors compare system far know fpets first end end tts system fully parallel
paper propose new hdp base online review rat regression model name topic sentiment preference regression analysis tspra tspra combine topics ie product aspects word sentiment user preference regression factor able perform topic cluster review rat prediction sentiment analysis invent critical aspect analysis altogether one framework tspra extend sentiment approach integrate key concept user preference collaborative filter cf model consideration distinct current cf model decouple user preference sentiment independent factor experiment conduct twenty-two amazon datasets show overwhelm better performance rat predication state art model flame two thousand and fifteen term error pearson correlation number invert pair sentiment analysis compare derive word sentiments public sentiment resource senticnet3 sentiment estimations clearly make sense context online review last result de correlation user preference sentiment tspra able evaluate new concept critical aspects define product aspects seriously concern users negatively comment review improvement critical aspects could effective enhance user experience
multimodal sentiment analysis core research area study speaker sentiment express language visual acoustic modalities central challenge multimodal learn involve infer joint representations process relate information modalities however exist work learn joint representations require modalities input result learn representations may sensitive noisy miss modalities test time recent success sequence sequence seq2seq model machine translation opportunity explore new ways learn joint representations may require input modalities test time paper propose method learn robust joint representations translate modalities method base key insight translation source target modality provide method learn joint representations use source modality input augment modality translations cycle consistency loss ensure joint representations retain maximal information modalities translation model train pair multimodal data need data source modality test time final sentiment prediction ensure model remain robust perturbations miss information modalities train model couple translation prediction objective achieve new state art result multimodal sentiment analysis datasets cmu mosi ict mmmo youtube additional experiment show model learn increasingly discriminative joint representations input modalities maintain robustness miss perturb modalities
consider problem train speech recognition systems without use label data assumption learner access input utterances phoneme language model estimate non overlap corpus propose fully unsupervised learn algorithm alternate solve two sub problems learn phoneme classifier give set phoneme segmentation boundaries ii refine phoneme boundaries base give classifier solve first sub problem introduce novel unsupervised cost function name segmental empirical output distribution match generalize work liu et al two thousand and seventeen segmental structure second sub problem develop approximate map approach refine boundaries obtain wang et al two thousand and seventeen experimental result timit dataset demonstrate success fully unsupervised phoneme recognition system achieve phone error rate per four hundred and sixteen although still far away state art supervise systems show oracle boundaries match language model per could improve 325this performance approach supervise system model architecture demonstrate great potential propose method
neural model enjoy widespread use across variety task grow become crucial components many industrial systems despite effectiveness extensive popularity without exploitable flaw initially apply computer vision systems generation adversarial examples process seemingly imperceptible perturbations make image purpose induce deep learn base classifier misclassify image due recent trend speech process become noticeable issue speech recognition model late two thousand and seventeen attack show quite effective speech command classification model limit vocabulary speech classifiers speech command model use quite frequently variety applications particularly manage automate attendants telephony contexts adversarial examples produce attack could real world consequences previous work defend adversarial examples investigate use audio preprocessing reduce distort adversarial noise work explore idea flood particular frequency band audio signal random noise order detect adversarial examples technique flood require retrain modify model inspire work do computer vision build idea speech classifiers relatively robust natural noise combine defense incorporate five different frequency band flood signal noise outperform exist defenses audio space detect adversarial examples nine hundred and eighteen precision nine hundred and thirty-five recall
stock market volatility forecast task relevant assess market risk investigate interaction news price one day ahead volatility prediction use state art deep learn approach propose model train either end end use sentence encoders transfer task evaluate broad range stock market sectors namely consumer staple energy utilities heathcare financials experimental result show add news improve volatility forecast compare mainstream model rely price data particular model outperform widely recognize garch11 model sectors term coefficient determination r2 mse mae achieve best performance train news price data
propose novel dialogue model framework first ever nonparametric kernel function base approach dialogue model learn kernelized hashcodes compress text representations unlike traditional deep learn model handle well relatively small datasets also scale large ones also derive novel lower bind mutual information use model selection criterion favor representations better alignment utterances participants collaborative dialogue set well higher predictability generate responses demonstrate three real life datasets include prominently psychotherapy sessions propose approach significantly outperform several state art neural network base dialogue systems term computational efficiency reduce train time days weeks hours response quality achieve order magnitude improvement competitors frequency choose best model human evaluators
adversarial attack exploitative process minute alterations make natural input cause input misclassified neural model field speech recognition become issue increase significance although adversarial attack originally introduce computer vision since infiltrate realm speech recognition two thousand and seventeen genetic attack show quite potent speech command model limit vocabulary speech classifiers speech command model use variety applications particularly telephony adversarial examples produce attack pose major security threat paper explore various methods detect adversarial examples combinations audio preprocessing one particular combine defense incorporate compressions speech cod filter audio pan show quite effective attack speech command model detect audio adversarial examples nine hundred and thirty-five precision nine hundred and twelve recall
real world applications natural language generation often constraints target sentence addition fluency naturalness requirements exist language generation techniques usually base recurrent neural network rnns however non trivial impose constraints rnns maintain generation quality since rnns generate sentence sequentially beam search first word last paper propose cgmh novel approach use metropolis hastings sample constrain sentence generation cgmh allow complicate constraints occurrence multiple keywords target sentence handle traditional rnn base approach moreover cgmh work inference stage require parallel corpora train evaluate method variety task include keywords sentence generation unsupervised sentence paraphrase unsupervised sentence error correction cgmh achieve high performance compare previous supervise methods sentence generation code release https githubcom ningmiao cgmh