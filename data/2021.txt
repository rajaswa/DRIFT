retrieve relevant contexts large corpus crucial step task open domain question answer fact check although neural retrieval outperform traditional methods like tf idf bm25 performance degrade considerably apply domain data drive question whether neural retrieval model universal perform robustly wide variety problems propose multi task train model approach outperform previous methods shoot set also rival specialise neural retrievers even domain train data abundant help retriever improve exist model downstream task closely match improve state art multiple benchmarks
transfer learn pretrained language model recently become dominant approach solve many nlp task common approach transfer learn multiple task maximize parameter share train one task specific layer top language model paper present alternative approach base adversarial reprogramming extend earlier work automatic prompt generation adversarial reprogramming attempt learn task specific word embeddings concatenate input text instruct language model solve specify task use 25k trainable parameters per task approach outperform exist methods 25m trainable parameters public leaderboard glue benchmark method initialize task specific human readable prompt also work shoot set outperform gpt three two superglue task thirty-two train sample
understand privacy policies crucial users empower learn information matter sentence write privacy policy document explain privacy practice constituent text span convey specific information practice refer predict privacy practice explain sentence intent classification identify text span share specific information slot fill work propose policyie corpus consist five thousand, two hundred and fifty intent eleven thousand, seven hundred and eighty-eight slot annotations span thirty-one privacy policies websites mobile applications policyie corpus challenge benchmark limit label examples reflect cost collect large scale annotations present two alternative neural approach baselines one formulate intent classification slot fill joint sequence tag two model sequence sequence seq2seq learn task experiment result show approach perform comparably intent classification seq2seq method outperform sequence tag approach slot fill large margin error analysis reveal deficiency baseline approach suggest room improvement future work hope policyie corpus stimulate future research domain
sensor name typically alphanumeric string encode key context eg function location sensor need deploy smart build applications sensor name however curated build vendor specific manner use different structure vocabularies often esoteric thus require tremendous manual effort annotate per build basis even segment sensor name meaningful chunk paper propose fully automate self supervise framework sensei learn segment sensor name without human annotation specifically employ neural language model capture underlie sensor name structure induce self supervision base information language model build segmentation model extensive experiment five real world build comprise thousands sensors demonstrate superiority sensei baseline methods
bilingual lexicons map word one language translations another typically induce learn linear projections align monolingual word embed space paper show possible produce much higher quality lexicons methods combine one unsupervised bitext mine two unsupervised word alignment directly apply pipeline use recent algorithms subproblems significantly improve induce lexicon quality gain possible learn filter result lexical entries unsupervised semi supervise scheme final model outperform state art bucc two thousand and twenty share task fourteen f1 point average twelve language pair also provide interpretable approach allow rich reason word mean context
softmax operator one important function machine learn model apply neural network multi category classification correlations among different categories often ignore example text generation language model make choice new word base former selection context scenario link statistics information concurrent word base corpus analogy natural way expression also valuable choose next word help improve sentence fluency smoothness fully explore important information propose graph softmax function text generation expect final classification result would dominate language model graphical text relationships among word use graph total variation term regularize softmax incorporate concurrent relationship language model total variation generate word small locally apply propose graph softmax gpt2 text generation task experimental result demonstrate propose graph softmax achieve better bleu perplexity softmax human testers also easily distinguish text generate graph softmax softmax
number biomedical literature new biomedical concepts rapidly increase necessitate reliable biomedical name entity recognition bioner model identify new unseen entity mention however questionable whether exist bioner model effectively handle work systematically analyze three type recognition abilities bioner model memorization synonym generalization concept generalization find one bioner model overestimate term generalization ability two tend exploit dataset bias hinder model abilities generalize enhance generalizability present simple debiasing method base data statistics method consistently improve generalizability state art sota model five benchmark datasets allow better perform unseen entity mention
text level discourse analysis various discourse scheme relatively label data discourse research still immature labor intensive annotate inner logic text paper attempt unify multiple chinese discourse corpora different annotation scheme discourse dependency framework design semi automatic methods convert dependency structure also implement several benchmark dependency parsers research leverage unify data improve performance
recent rapid technological advancements online social network twitter lead great incline spread false information fake news misinformation especially prevalent ongoing coronavirus disease covid nineteen pandemic lead individuals accept bogus potentially deleterious claim article quick detection fake news reduce spread panic confusion among public analysis paper report methodology analyze reliability information share social media pertain covid nineteen pandemic best approach base ensemble three transformer model bert albert xlnet detect fake news model train evaluate context constraintai two thousand and twenty-one share task covid19 fake news detection english system obtain nine thousand, eight hundred and fifty-five f1 score testset rank 5th among one hundred and sixty team
fine tune de facto way leverage large pretrained language model perform downstream task however modify language model parameters therefore necessitate store full copy task paper propose prefix tune lightweight alternative fine tune natural language generation task keep language model parameters freeze optimize small continuous task specific vector call prefix prefix tune draw inspiration prompt allow subsequent tokens attend prefix virtual tokens apply prefix tune gpt two table text generation bart summarization find learn one parameters prefix tune obtain comparable performance full data set outperform fine tune low data settings extrapolate better examples topics unseen train
bert one pretrianed language model attract attention recent years create new benchmarks across glue task via fine tune one press issue open blackbox explain decision make bert number attribution techniques propose explain bert model often limit sequence sequence task paper adapt exist attribution methods explain decision make bert sequence classification task conduct extensive analyse four exist attribution methods apply four different datasets sentiment analysis compare reliability robustness method via various ablation study furthermore test whether attribution methods explain generalize semantics across semantically similar task work provide solid guidance use attribution methods explain decision make bert downstream classification task
pre train language model large volume data self supervise objectives become standard practice natural language process however state art model available english resource rich languages even multilingual model train hundreds languages low resource ones still remain underrepresented bangla seventh widely speak language world still low term resources downstream task datasets language understand bangla publicly available clear shortage good quality data pre train work build bangla natural language understand model pre train one hundred and eighty-six gb data crawl top bangla sit internet introduce new downstream task dataset benchmark four task sentence classification document classification natural language understand sequence tag model outperform multilingual baselines previous state art result one six process identify major shortcoming multilingual model hurt performance low resource languages share write script high resource one name embed barrier perform extensive experiment study barrier release datasets pre train model aid future nlp research bangla low resource languages code data available https githubcom csebuetnlp banglabert
counterfactual examples useful analysis train nlp model current generation methods either rely manual labor create counterfactuals instantiate limit type perturbations paraphrase word substitutions present polyjuice general purpose counterfactual generator allow control perturbation type locations train finetuning gpt two multiple datasets pair sentence show polyjuice produce diverse set realistic counterfactuals turn useful various distinct applications improve train evaluation three different task around seventy less annotation effort manual generation augment state art explanation techniques support systematic counterfactual error analysis reveal behaviors easily miss human experts
provide natural language process systems commonsense knowledge critical challenge achieve language understand recently commonsense knowledge model emerge suitable approach hypothesize situation relevant commonsense knowledge demand natural language applications however systems limit fix set relations capture schemas knowledge base train address limitation investigate train commonsense knowledge model shoot set limit tuples per commonsense relation graph perform five separate study different dimension shoot commonsense knowledge learn provide roadmap best practice train systems efficiently importantly find human quality rat knowledge produce shoot train system achieve performance within six knowledge produce fully supervise systems shoot performance enable coverage wide breadth relations future commonsense systems
despite considerable advancements deep neural language model lms neural text generation still suffer degeneration generate text repetitive generic self inconsistent lack commonsense empirical analyse sentence level attention pattern reveal neural text degeneration may associate insufficient learn inductive bias attention mechanism find motivate fly attention modularization simple effective method inject inductive bias attention computation inference result text produce language model attention modularization yield enhance diversity commonsense reason maintain fluency coherence
small class imbalanced datasets common many high level semantic task like discourse analysis present particular challenge current deep learn architectures work perform extensive analysis sentence level classification approach news discourse dataset one largest high level semantic discourse datasets recently publish show multitask approach improve seven micro f1 score upon current state art benchmarks due part label corrections across task improve performance underrepresented class also offer comparative review additional techniques propose address resource poor problems nlp show none approach improve classification accuracy set
many question answer qa datasets contain unanswerable question treatment qa systems remain primitive analysis natural question kwiatkowski et al two thousand and nineteen dataset reveal substantial portion unanswerable question sim21 explain base presence unverifiable presuppositions discuss shortcomings current model handle question describe improve system could handle user preference study demonstrate oracle behavior propose system provide responses base presupposition failure prefer oracle behavior exist qa systems discuss propose system could implement present novel framework break problem three step presupposition generation presupposition verification explanation generation report progress tackle subproblem present preliminary approach integrate step exist qa system find add presuppositions verifiability exist model yield modest gain downstream performance unanswerability detection biggest bottleneck verification component need substantially improve integrate system approach ideal behavior even transfer best entailment model currently fall short
end end semantic role label srl receive increase interest perform two subtasks srl predicate identification argument role label jointly recent work mostly focus graph base neural model transition base framework neural network widely use number closely relate task study joint task yet paper present first work transition base neural model end end srl transition model incrementally discover sentential predicate well arguments set transition action action two subtasks execute mutually full interactions besides suggest high order compositions extract non local feature enhance propose transition model experimental result conll09 universal proposition bank show final model produce state art performance meanwhile keep highly efficient decode also conduct detail experimental analysis deep understand propose model
work represent lex bert incorporate lexicon information chinese bert name entity recognition ner task natural manner instead use word embeddings newly design transformer layer flat identify boundary word sentence use special tokens modify sentence encode directly bert model introduce new parameters efficient flat addition require word embeddings accompany lexicon collection experiment ontonotes zhcrossner show model outperform flat baselines
input segmentation pretrained language model plms affect interpretations complex word present first study investigate question take bert example plm focus semantic representations english derivatives show plms interpret serial dual route model ie mean complex word either store else need compute subwords imply maximally meaningful input tokens allow best generalization new word hypothesis confirm series semantic probe task delbert derivation leverage bert model derivational input segmentation substantially outperform bert wordpiece segmentation result suggest generalization capabilities plms could improve morphologically inform vocabulary input tokens use
introduce new pretraining approach language model gear support multi document nlp task cross document language model cd lm improve mask language model task two key ideas first pretrain multiple relate document single input via cross document mask encourage model learn cross document long range relationships second extend recent longformer model pretrain long contexts several thousand tokens introduce new attention pattern use sequence level global attention predict mask tokens retain familiar local attention elsewhere show cd lm set new state art result several multi text task include cross document event entity coreference resolution paper citation recommendation document plagiarism detection use significantly reduce number train parameters relative prior work
study family data augmentation methods substructure substitution sub2 natural language process nlp task sub2 generate new examples substitute substructures eg subtrees subsequences ones label apply many structure nlp task part speech tag parse general task eg text classification explicitly annotate substructures present variations sub2 base constituency parse tree introduce structure aware data augmentation methods general nlp task case train augment dataset sub2 achieve better performance train original train set experiment show sub2 consistent performance investigate augmentation methods across different task size seed dataset
paper generalize text infilling eg mask language model propose sequence span rewrite ssr self supervise sequence sequence seq2seq pre train objective ssr provide fine grain learn signal text representations supervise model rewrite imperfect span grind truth consistent text infilling many downstream seq2seq task rewrite source sentence target sentence experiment t5 model various seq2seq task show ssr substantially improve seq2seq pre train moreover observe ssr especially helpful improve pre train small size seq2seq model powerful imperfect span generator indicate new perspective transfer knowledge large model smaller model seq2seq pre train
present knowledge enhance multimodal bart km bart transformer base sequence sequence model capable reason commonsense knowledge multimodal input image texts extend popular bart architecture multi modal model design new pretraining task improve model performance visual commonsense generation task pretraining task improve visual commonsense generation performance leverage knowledge large language model pretrained external knowledge graph best knowledge first propose dedicate task improve model performance visual commonsense generation experimental result show pretraining model reach state art performance visual commonsense generation task
machine translation systems vulnerable domain mismatch especially task low resource set domain translations often poor quality prone hallucinations due translation model prefer predict common word see train oppose uncommon ones different domain present two simple methods improve translation quality particular set first use lexical shortlist order restrict neural network predictions ibm model compute alignments second perform n best list reorder reranking translations base amount overlap methods computationally simpler faster alternative approach show moderate success low resource settings explicit domain test set however methods lose effectiveness domain mismatch great high resource set
introduction pretrained language model reduce many complex task specific nlp model simple lightweight layer exception trend coreference resolution sophisticate task specific model append pretrained transformer encoder highly effective model large memory footprint primarily due dynamically construct span span pair representations hinder process complete document ability train multiple instance single batch introduce lightweight end end coreference model remove dependency span representations handcraft feature heuristics model perform competitively current standard model simpler efficient
several question answer benchmarks pretrained model reach human parity fine tune order one hundred thousand annotate question answer explore realistic shoot set hundred train examples available observe standard model perform poorly highlight discrepancy current pretraining objectives question answer propose new pretraining scheme tailor question answer recur span selection give passage multiple set recur span mask set recur span one ask model select correct span passage mask span mask span replace special token view question representation later use fine tune select answer span result model obtain surprisingly good result multiple benchmarks eg seven hundred and twenty-seven f1 squad one hundred and twenty-eight train examples maintain competitive performance high resource set
large attention base encoder decoder network transformer become prevail recently due effectiveness high computation complexity decoder raise inefficiency issue examine mathematic formulation decoder show mild condition architecture could simplify compress sub layer basic build block transformer achieve higher parallelism thereby propose compress attention network whose decoder layer consist one sub layer instead three extensive experiment fourteen wmt machine translation task show model 142x faster performance par strong baseline strong baseline already 2x faster widely use standard baseline without loss performance
fundamental ability humans utilize commonsense knowledge language understand question answer recent years many knowledge enhance commonsense question answer cqa approach propose however remain unclear one far get exploit external knowledge cqa two much potential knowledge exploit current cqa model three promise directions future cqa answer question benchmark knowledge enhance cqa conduct extensive experiment multiple standard cqa datasets use simple effective knowledge text transformation framework experiment show one knowledge text framework effective achieve state art performance commonsenseqa dataset provide simple strong knowledge enhance baseline cqa two potential knowledge still far fully exploit cqa significant performance gap current model model golden knowledge three context sensitive knowledge selection heterogeneous knowledge exploitation commonsense rich language model promise cqa directions
semantic role label srl core natural language process task english recent methods base transformer model allow major improvements previous state art however low resource languages particular portuguese currently available srl model hinder scarce train data paper explore model architecture pre train bert base model linear layer softmax viterbi decode substantially improve state art performance portuguese 15f1 additionally improve srl result portuguese corpora exploit cross lingual transfer learn use multilingual pre train model xlm r transfer learn dependency parse portuguese evaluate various propose approach empirically result present heuristic support choice appropriate model consider available resources
introduce make publicly available entity link dataset reddit contain seventeen thousand, three hundred and sixteen link entities annotate three human annotators group gold silver bronze indicate inter annotator agreement analyze different errors disagreements make annotators suggest three type corrections raw data finally test exist entity link model train tune text non social media datasets find although exist entity link model perform well original datasets perform poorly social media dataset also show majority errors attribute poor performance mention detection subtask result indicate need better entity link model apply enormous amount social media text
transformer base model like bert roberta achieve state art result many natural language process task however memory footprint inference latency power consumption prohibitive efficient inference edge even data center quantization viable solution previous work quantize transformer base model use float point arithmetic inference efficiently utilize integer logical units recent turing tensor core traditional integer arm processors work propose bert novel quantization scheme transformer base model quantize entire inference integer arithmetic base lightweight integer approximation methods nonlinear operations eg gelu softmax layer normalization bert perform end end integer bert inference without float point calculation evaluate approach glue downstream task use roberta base large show case bert achieve similar slightly higher accuracy compare full precision baseline furthermore preliminary implementation bert show speedup twenty-four 40x int8 inference t4 gpu system compare fp32 inference framework develop pytorch open source
customer service set call empathy live human agent responses recent advance demonstrate open domain chatbots train demonstrate empathy respond live human utterances show blend skills chatbot model respond customer query likely resemble actual human agent response train recognize emotion exhibit appropriate empathy model without train analysis leverage twitter customer service dataset contain several million customer agent dialog examples customer service contexts twenty well know brand
present first multi task learn model name phonlp joint vietnamese part speech pos tag name entity recognition ner dependency parse experiment vietnamese benchmark datasets show phonlp produce state art result outperform single task learn approach fine tune pre train vietnamese language model phobert nguyen nguyen two thousand and twenty task independently publicly release phonlp open source toolkit apache license twenty although specify phonlp vietnamese phonlp train evaluation command script fact directly work languages pre train bert base language model gold annotate corpora available three task pos tag ner dependency parse hope phonlp serve strong baseline useful toolkit future nlp research applications vietnamese also languages phonlp available https githubcom vinairesearch phonlp
ongoing debate nlg community concern best way evaluate systems human evaluation often consider reliable method compare corpus base metrics however task involve subtle textual differences style transfer tend hard humans perform paper propose evaluation method task base purposely train classifiers show better reflect system differences traditional metrics bleu rouge
semantic parse long fundamental problem natural language process recently cross domain context dependent semantic parse become new focus research central problem challenge leverage contextual information natural language utterance database schemas interaction history paper present dynamic graph framework capable effectively model contextual utterances tokens database schemas complicate interaction conversation proceed framework employ dynamic memory decay mechanism incorporate inductive bias integrate enrich contextual relation representation enhance powerful reranking model time write demonstrate propose framework outperform exist model large margins achieve new state art performance two large scale benchmarks sparc cosql datasets specifically model attain five hundred and fifty-eight question match three hundred and eight interaction match accuracy sparc four hundred and sixty-eight question match one hundred and seventy interaction match accuracy cosql
food recommendation become important mean help guide users adopt healthy dietary habit previous work food recommendation either fail consider users explicit requirements ii ignore crucial health factor eg allergies nutrition need iii utilize rich food knowledge recommend healthy recipes address limitations propose novel problem formulation food recommendation model task constrain question answer large scale food knowledge base graph kbqa besides requirements user query personalize requirements user dietary preferences health guidelines handle unify way additional constraints qa system validate idea create qa style dataset personalize food recommendation base large scale food knowledge graph health guidelines furthermore propose kbqa base personalize food recommendation framework equip novel techniques handle negations numerical comparisons query experimental result benchmark show approach significantly outperform non personalize counterparts average five hundred and ninety-seven absolute improvement across various evaluation metrics able recommend relevant healthier recipes
state art extractive question answer model achieve superhuman performances squad benchmark yet unreasonably heavy need expensive gpu compute answer question reasonable time thus use real world query hundreds thousands document open domain question answer paradigm paper explore possibility transfer natural language understand language model dense vectors represent question answer candidates order make task question answer compatible simple nearest neighbor search task new model call efficientqa take advantage pair sequence kind input bert base model build meaningful dense representations candidate answer latter extract context question agnostic fashion model achieve state art result phrase index question answer piqa beat previous state art thirteen point exact match fourteen point f1 score result show dense vectors able embed rich semantic representations sequence although ones build language model originally train use case thus order build resource efficient nlp systems future train language model better adapt build dense representations phrase one possibilities
key limitation current datasets multi hop reason require step answer question mention explicitly work introduce strategyqa question answer qa benchmark require reason step implicit question infer use strategy fundamental challenge setup elicit creative question crowdsourcing workers cover broad range potential strategies propose data collection procedure combine term base prim inspire annotators careful control annotator population adversarial filter eliminate reason shortcuts moreover annotate question one decomposition reason step answer two wikipedia paragraph contain answer step overall strategyqa include two thousand, seven hundred and eighty examples consist strategy question decomposition evidence paragraph analysis show question strategyqa short topic diverse cover wide range strategies empirically show humans perform well eighty-seven task best baseline reach accuracy sim66
one fundamental principles contemporary linguistics state language process require ability extract recursively nest tree structure however remain unclear whether code could implement neural circuit recent advance recurrent neural network rnns achieve near human performance language task provide compel model address question present new framework study recursive process rnns use subject verb agreement probe representations neural network train six distinct type rnns simplify probabilistic context free grammar design independently manipulate length sentence depth syntactic tree rnns generalize subject verb dependencies longer see train however none systematically generalize deeper tree structure even structural bias towards learn nest tree ie stack rnns addition analyse reveal primacy recency effect generalization pattern lstm base model show model tend perform well outer innermost part center embed tree structure poorly middle level finally probe internal state model process sentence nest tree structure find complex encode grammatical agreement information eg grammatical number information multiple word nouns carry single unit take together result indicate neural network may extract bound nest tree structure without learn systematic recursive rule
recent years deep learn base automate personality trait detection receive lot attention especially due massive digital footprints individual moreover many researchers demonstrate strong link personality traits emotions paper build know correlation personality traits emotional behaviors propose novel multitask learn framework sogmtl simultaneously predict also empirically evaluate discuss different information share mechanisms two task ensure high quality learn process adopt maml like framework model optimization computationally efficient cnn base multitask model achieve state art performance across multiple famous personality emotion datasets even outperform language model base model
entity link el rapidly grow short text eg search query news title critical industrial applications exist approach rely adequate context long text el effective concise sparse short text paper propose novel framework call multi turn multiple choice machine read comprehension m3 solve short text el new perspective query generate ambiguous mention exploit surround context option selection module employ identify golden entity candidates use query way m3 framework sufficiently interact limit context candidate entities encode process well implicitly consider dissimilarities inside candidate bunch selection stage addition design two stage verifier incorporate m3 address commonly exist unlinkable problem short text consider topical coherence interdependence among refer entities m3 leverage multi turn fashion deal mention sequence manner retrospect historical cue evaluation show m3 framework achieve state art performance five chinese english datasets real world short text el
paper present system exploit different pre train language model assign domain label wordnet synsets without kind supervision furthermore system restrict use particular set domain label exploit knowledge encode within different shelf pre train language model task formulations infer domain label particular wordnet definition propose zero shoot system achieve new state art english dataset use evaluation
various applications computational linguistics artificial intelligence rely high perform word sense disambiguation techniques solve challenge task information retrieval machine translation question answer document cluster text comprehension intuitive humans machine face tremendous challenge process interpret human natural language paper present novel knowledge base word sense disambiguation algorithm namely sequential contextual similarity matrix multiplication scsmm scsmm algorithm combine semantic similarity heuristic knowledge document context respectively exploit merit local context consecutive term human knowledge term document main topic disambiguate term unlike algorithms scsmm algorithm guarantee capture maximum sentence context maintain term order within sentence propose algorithm outperform algorithms disambiguate nouns combine gold standard datasets demonstrate comparable result current state art word sense disambiguation systems deal dataset separately furthermore paper discuss impact granularity level ambiguity rate sentence size part speech distribution performance propose algorithm
spell irregularities know spell mistake find several centuries humans able understand misspell word base location sentence perceive pronunciation context unlike humans computer systems possess convenient auto complete functionality human brain capable many program provide spell correction functionality many systems take context account moreover artificial intelligence systems function way train many current natural language process nlp systems train grammatically correct text data many vulnerable adversarial examples yet correctly spell text process crucial learn paper investigate spell errors correct context pre train language model bert present two experiment base bert edit distance algorithm rank select candidate corrections result experiment demonstrate combine properly contextual word embeddings bert edit distance capable effectively correct spell errors
presentation slide become common addition teach material emphasize strong lead word presentation slide allow audience direct eye certain focal point instead read entire slide retain attention speaker presentation despite large volume study automatic slide generation study address automation design assistance creation process motivate demand study problem emphasis selection es presentation slide ie choose candidates emphasis introduce new dataset contain presentation slide wide variety topics annotate emphasis word crowdsourced set evaluate range state art model novel dataset organize share task invite multiple researchers model emphasis new domain present main find compare result model examine challenge dataset provide different analysis components
gamification approach use way create language resources nlp also use present teach algorithms nlp linguistic phenomena paper argue design gamification japanese syntactic dependendency parse latter objective user interface design base transition base shift reduce dependency parse need two action shift attach reduce attach japanese dependency structure assign two action two way directional control gamepad devices also design target sentence psycholinguistics research
introduce trankit light weight transformer base toolkit multilingual natural language process nlp provide trainable pipeline fundamental nlp task one hundred languages ninety pretrained pipelines fifty-six languages build state art pretrained language model trankit significantly outperform prior multilingual nlp pipelines sentence segmentation part speech tag morphological feature tag dependency parse maintain competitive performance tokenization multi word token expansion lemmatization ninety universal dependencies treebanks despite use large pretrained transformer toolkit still efficient memory usage speed achieve novel plug play mechanism adapters multilingual pretrained transformer share across pipelines different languages toolkit along pretrained model code publicly available https githubcom nlp uoregon trankit demo website toolkit also available http nlpuoregonedu trankit finally create demo video trankit https youtube q0kgp3zgjgc
paper illustrate detail description system result develop part participation constraint share task aaai two thousand and twenty-one share task comprise two task covid19 fake news detection english b hostile post detection hindi task binary classification problem fake real class task b multi label multi class classification task five hostile class ie defame fake hate offense non hostile various techniques use perform classification task include svm cnn bilstm cnnbilstm tf idf word2vec embed techniques result indicate svm tf idf feature achieve highest nine thousand, four hundred and thirty-nine weight f1 score test set task label powerset svm n gram feature obtain maximum coarse grain fine grain f1 score eight thousand, six hundred and three five thousand and ninety-eight task b test set respectively
sentence semantic understand key topic field natural language process recently contextualized word representations derive pre train language model elmo bert show significant improvements wide range semantic task eg question answer text classification sentiment analysis however add external knowledge improve semantic model capability model worth probe paper propose novel approach combine syntax information pre train language model order evaluate effect pre train model first introduce rnn base transformer base pre train language model secondly better integrate external knowledge syntactic information integrate pre train model propose dependency syntax expansion dse model evaluation select two subtasks sentence completion task biological relation extraction task experimental result show model achieve nine hundred and twelve accuracy outperform baseline model three hundred and seventy-eight sentence completion task also get competitive performance seven hundred and fifty-one f1 score relation extraction task
relation classification rc task one fundamental task information extraction aim detect relation information entity pair unstructured natural language text generate structure data form entity relation triple although distant supervision methods effectively alleviate problem lack train data supervise learn also introduce noise data still fundamentally solve long tail distribution problem train instance order enable neural network learn new knowledge instance like humans work focus shoot relation classification fsrc classifier generalize new class see train set give number sample class make full use exist information get better feature representation instance propose encode class prototype adaptive way two aspects first base prototypical network propose adaptive mixture mechanism add label word representation class prototype best knowledge first attempt integrate label information feature support sample class get interactive class prototypes second reasonably measure distance sample category introduce loss function joint representation learn encode support instance adaptive manner extensive experiment conduct fewrel different shoot fs settings result show propose adaptive prototypical network label word joint representation learn achieve significant improvements accuracy also increase generalization ability shoot rc model
context free grammars characterize simple proof theoretic grammatical formalism namely categorial grammar logic lambek calculus characterizations know tree adjoin grammars even mildly context sensitive languages class last forty years despite efforts settle problem paper basis exist fragment lambek grishin calculus capture tree adjoin languages present logic call hlg proof theoretic characterization tree adjoin languages base lambek grishin calculus restrict hyperedge replacement grammar rank two study moot hlg define display calculus cut admissibility several new techniques introduce proof purely structural connectives usefulness graph theoretic argument proof net hlg
acronym identification focus find acronyms phrase abbreviate crucial scientific document understand task however limit size manually annotate datasets hinder improvement problem recent breakthroughs language model pre train large corpora clearly show unsupervised pre train vastly improve performance downstream task paper present adversarial train bert method name bert win solution acronym identification task scientific document understand sdu challenge aaai two thousand and twenty-one specifically pre train bert adopt capture better semantic representation incorporate fgm adversarial train strategy fine tune bert make model robust generalize furthermore ensemble mechanism devise involve representations learn multiple bert variants assemble components together experimental result sciai dataset show propose approach outperform competitive state art methods
system paper present contribution constraint two thousand and twenty-one covid nineteen fake news detection share task pose challenge classify covid nineteen relate social media post either fake real system address challenge apply classical machine learn algorithms together several linguistic feature n grams readability emotional tone punctuation term pre process experiment various step like stop word removal stem lemmatization link removal find best perform system base linear svm obtain weight average f1 score nine thousand, five hundred and nineteen test data land place middle leaderboard place eighty one hundred and sixty-seven
abugida refer phonogram write system syllable represent use single consonant typographic ligature along default vowel optional diacritics denote vowels however texting languages unique challenge spite advent devices soft keyboard support custom key layouts number character languages large enough require character spread multiple view layout switch view many time type single word hinder natural think process prevent popular usage native keyboard layouts hand support romanize script native word transcribe use latin character language model base suggestions also set back lack uniform romanization rule end propose disambiguation algorithm showcase usefulness two novel mutually non exclusive input methods languages natively use abugida write system disambiguation ambiguous input abugida script b disambiguation word variants romanize script benchmark approach use public datasets show improvement type speed one thousand, nine hundred and forty-nine two thousand, five hundred and thirteen one thousand, four hundred and eighty-nine hindi bengali thai respectively use ambiguous input owe human ease locate key combine efficiency inference method word variant disambiguation wda map valid variants romanize word previously treat vocab vocabulary 100k word high accuracy lead increase error correction f1 score one thousand and three next word prediction nwp six thousand, two hundred and fifty average
two billion mobile users worldwide type multiple languages soft keyboard monolingual keyboard thirty-eight falsely auto correct word valid another language easily avoid detect language type word validate respective language language detection well know problem natural language process paper present fast light weight accurate language detection engine lde multilingual type dynamically adapt user intend language real time propose novel approach fusion character n gram model logistic regression base selector model use identify language additionally present unique method reduce inference time significantly parameter reduction technique also discuss various optimizations fabricate across lde resolve ambiguity input text among languages character pattern method demonstrate average accuracy nine hundred and forty-five indian languages latin script ninety-eight european languages code switch data model outperform fasttext six thousand and thirty-nine ml kit two thousand, three hundred and sixty-seven f1 score european languages lde faster mobile device average inference time two thousand, five hundred and ninety-one microseconds
increase number mobile devices continuous research generate optimize language model lms soft keyboard spite advance domain build single lm low end feature phone well high end smartphones still press need hence propose novel technique optimize n gram op ngram end end n gram pipeline utilise mobile resources efficiently faster word completion wc next word prediction nwp op ngram apply stupid backoff prune strategies generate light weight model lm load time mobile linear respect model size observe op ngram give thirty-seven improvement language model lm rom size seventy-six lm ram size eighty-eight load time eighty-nine average suggestion time compare sort array variant berkeleylm moreover method show significant performance improvement kenlm well
neural machine translation model sequence sequence converter base neural network exist model use recurrent neural network construct encoder decoder modules alternative research recurrent network substitute convolutional neural network capture syntactic structure input sentence decrease process time incorporate goodness approach propose convolutional recurrent encoder capture context information well sequential information source sentence word embed position embed source sentence perform prior convolutional encode layer basically n gram feature extractor capture phrase level context information rectify output convolutional encode layer add original embed vector sum normalize layer normalization normalize output give sequential input recurrent encode layer capture temporal information sequence decoder use attention base recurrent neural network translation task german english dataset verify efficacy propose approach higher bleu score achieve compare state art
biomedical relation statement commonly express multiple sentence consist many concepts include gene disease chemical mutation automatically extract information biomedical literature exist biomedical text mine approach typically formulate problem cross sentence n ary relation extraction task detect relations among n entities across multiple sentence use either graph neural network gnn long short term memory lstm attention mechanism recently transformer show outperform lstm many natural language process nlp task work propose novel architecture combine bidirectional encoder representations transformers graph transformer bert gt integrate neighbor attention mechanism bert architecture unlike original transformer architecture utilize whole sentence calculate attention current token neighbor attention mechanism method calculate attention utilize neighbor tokens thus token pay attention neighbor information little noise show critically important text long cross sentence abstract level relation extraction task benchmarking result show improvements five hundred and forty-four three hundred and eighty-nine accuracy f1 measure state art n ary chemical protein relation datasets suggest bert gt robust approach applicable biomedical relation extraction task datasets
romanian one understudy languages computational linguistics resources available development natural language process tool paper introduce laroseda large romanian sentiment data set compose fifteen thousand positive negative review collect one largest romanian e commerce platforms employ two sentiment classification methods baselines new data set one base low level feature character n grams one base high level feature bag word embeddings generate cluster word embeddings k mean additional contribution replace k mean cluster algorithm self organize map soms obtain better result generate cluster word embeddings closer zipf law distribution know govern natural language also demonstrate generalization capacity use soms cluster word embeddings another recently introduce romanian data set text categorization topic
likelihood train maximization base decode result dull repetitive generate texts even use powerful language model holtzman et al two thousand and nineteen add loss function regularization show improve text generation output help avoid unwanted properties contradiction repetition li al two thousand and twenty work propose fine tune language model use policy gradient reinforcement learn directly optimize better generation apply approach minimize repetition generate text show combine unlikelihood train welleck et al two thousand and twenty method reduce repetition without impact language model quality also evaluate methods improve generation train decode time compare use various metrics aim control better text generation output
investigate contract element extraction show lstm base encoders perform better dilate cnns transformers bert task also find domain specific word2vec embeddings outperform generic pre train glove embeddings morpho syntactic feature form pos tag token shape embeddings well context aware elmo embeddings improve performance several observations contradict choices find previous work contract element extraction generic sequence label task indicate contract element extraction require careful task specific choices analyze result plain transformer base ii bert base model find examine task entities highly context sensitive lack recurrency transformers greatly affect performance
intent classification important task natural language understand systems exist approach achieve perfect score benchmark datasets however suitable deployment low resource devices like mobiles tablets etc due massive model size therefore paper present novel light weight architecture intent classification run efficiently device use character feature enrich word representation experiment prove propose model outperform exist approach achieve state art result benchmark datasets also report model tiny memory footprint five mb low inference time two milliseconds prove efficiency resource constrain environment
determine plausibility causal relations clauses commonsense reason task require complex inference ability general approach task train large pretrained language model specific dataset however available train data task often scarce lead instability model train reliance shallow feature dataset paper present number techniques make model robust domain causal reason firstly perform adversarial train generate perturb input synonym substitution secondly base linguistic theory discourse connectives perform data augmentation use discourse parser detect causally link clauses large text generative language model generate distractors methods boost model performance choice plausible alternatives copa dataset well balance copa dataset modify version original data develop avoid superficial cue lead challenge benchmark show statistically significant improvement performance robustness datasets even small number additionally generate data point
due wide adoption social media platforms like facebook twitter etc emerge need detect online post go community acceptance standards hostility detection task well explore resource rich languages like english unexplored resource constrain languages like hindidue unavailability large suitable data view hostility detection multi label multi class classification problem propose effective neural network base technique hostility detection hindi post leverage pre train multilingual bidirectional encoder representations transformer mbert obtain contextual representations hindi post perform extensive experiment include different pre process techniques pre train model neural architectures hybrid strategies etc best perform neural classifier model include one vs rest approach obtain nine thousand, two hundred and sixty eighty-one million, one hundred and forty-six thousand, nine hundred and fifty-nine seven thousand, five hundred and twenty-nine seven thousand, three hundred and one f1 score hostile fake hate offensive defamation label respectively propose model outperform exist baseline model emerge state art model detect hostility hindi post
paper introduce data drive approach transliterate uzbek dictionary word cyrillic script latin script vice versa heuristically align character word source script sub string correspond word target script train decision tree classifier learn alignments test set cyrillic latin model achieve character level micro average f1 score nine thousand, nine hundred and ninety-two latin cyrillic model achieve score nine thousand, nine hundred and fifty-nine contribution novel method produce machine transliterate texts low resource uzbek language
paper introduce reproducible clean process text extract pdfs use n gram model approach compare originally extract text text generate expect model use earlier text stimulus guide process introduce notion consistency score refer proportion text expect model use monitor change clean process across different corpuses illustrate process text book jane eyre introduce shiny application r package make process easier others adopt
traditional data augmentation aim increase coverage input distribution generate augment examples strongly resemble original sample online fashion augment examples dominate train paper propose alternative perspective multi task view mtv data augmentation primary task train original examples auxiliary task train augment examples mtv data augmentation original augment sample weight substantively train relax constraint augment examples must resemble original data thereby allow us apply stronger level augmentation empirical experiment use four common data augmentation techniques three benchmark text classification datasets find mtv lead higher robust performance improvements traditional augmentation
hostile content social platforms ever increase lead need proper detection hostile post appropriate action take tackle though lot work do recently english language solve problem hostile content online similar work indian languages quite hard find paper present transfer learn base approach classify social media ie twitter facebook etc post hindi devanagari script hostile non hostile hostile post analyze determine hateful fake defamation offensive paper harness attention base pre train model fine tune hindi data hostile non hostile task auxiliary fuse feature sub task classification approach establish robust consistent model without ensembling complex pre process present result approach constraint two thousand and twenty-one share task hostile post detection model perform extremely well 3rd runner term weight fine grain f1 score
paper devote participation tudublin team constraintaaai2021 covid19 fake news detection challenge today problem fake news detection acute ever connection pandemic number fake news increase rapidly necessary create ai tool allow us identify prevent spread false information covid nineteen urgently main goal work create model would carry binary classification message social media real fake news context covid nineteen team construct ensemble consist bidirectional long short term memory support vector machine logistic regression naive bay combination logistic regression naive bay model allow us achieve ninety-four f1 score within five best result
present sick nl read signal dataset target natural language inference dutch sick nl obtain translate sick dataset marelli et al 2014from english dutch parallel inference dataset allow us compare monolingual multilingual nlp model english dutch two task paper motivate detail translation process perform baseline evaluation original sick dataset dutch incarnation sick nl take inspiration dutch skipgram embeddings contextualised embed model addition encapsulate two phenomena encounter translation formulate stress test verify well dutch model capture syntactic restructure affect semantics main find model perform worse sick nl sick indicate dutch dataset challenge english original result stress test show model fully capture word order freedom dutch warrant future systematic study
several machine learn base spoiler detection model propose recently protect users spoilers review websites although dependency relations context word important detect spoilers current attention base spoiler detection model insufficient utilize dependency relations address problem propose new spoiler detection model call sdgnn base syntax aware graph neural network experiment two real world benchmark datasets show sdgnn outperform exist spoiler detection model
recent advance regard question answer read comprehension result model surpass human performance answer contain single continuous passage text require single hop reason however actual scenarios lot complex query require multi hop reason key question answer task semantic feature interaction document question widely process bi directional attention flow bi daf bi daf generally capture surface semantics word complex question fail capture imply semantic feature intermediate answer result bi daf partially ignore part contexts relate question extract important part multiple document paper propose new model architecture multi hop question answer apply two completion strategies one coarse grain complex question decomposition cgde strategy introduce decompose complex question simple ones condition without additional annotations two fine grain interaction fgin strategy introduce better represent word document extract comprehensive accurate sentence relate inference path two strategies combine test squad hotpotqa datasets experimental result show method outperform state art baselines
reach internet increase pejorative term start flood social media platforms lead necessity identify hostile content social media platforms identification hostile content low resource languages like hindi pose different challenge due diverse syntactic structure compare english paper develop simple ensemble base model pre train mbert popular classification algorithms like artificial neural network ann xgboost hostility detection hindi post formulate problem binary classification hostile non hostile class multi label multi class classification problem fine grain hostile class receive third overall rank competition weight f1 score nine hundred and sixty-nine sixty-one binary multi label multi class classification task respectively
advent framenet propbank many semantic role label srl systems propose english although research japanese predicate argument structure analysis pasa conduct study focus surface case previous work japanese srl deep case model accuracies low therefore propose hierarchical multitask learn method dependency parse dp show model achieve state art result japanese srl also conduct experiment joint model perform argument identification argument classification simultaneously result suggest multitasking dp mainly effective argument identification
article present result study involve translation fictional story english catalan three modalities machine translate mt post edit mtpe translate without aid ht translation analyse evaluate creativity subsequently cohort eighty-eight catalan participants read story randomly assign modality complete survey result show ht present higher creativity score compare mtpe mt ht also rank higher narrative engagement translation reception mtpe rank marginally higher enjoyment ht mtpe show statistically significant differences category whereas mt variables test conclude creativity highest professional translators intervene process especially work without aid hypothesize creativity translation could factor enhance read engagement reception translate literary texts
speakers non english languages often adopt loanwords english express new unusual concepts loanwords may borrow unchanged speakers may also integrate word fit constraints native language eg create spanish tuitear english tweet linguists often consider process loanword integration dependent language internal constraints sociolinguistic constraints speaker background remain qualitatively understand investigate role social context speaker background spanish speakers use integrate loanwords social media find first newspaper author use integrate form loanwords native word often social media author show integration associate formal domains social media find speaker background expectations formality explain loanword native word integration author use spanish write wider audience tend use integrate verb form often study show loanword integration reflect language internal constraints also social expectations vary conversation speaker
paper propose unify explanation representation layer aware neural sequence encoders regard representation revisit multigraph call multi order graph mog model encode view process capture subgraphs mog relationship reflect multi order graph call n order dependency present exist simple direct graph explanation present propose mog explanation allow precisely observe every step generation representation put diverse relationship syntax unifiedly depict framework base propose mog explanation propose graph base self attention network empower graph transformer enhance ability capture subgraph information current model graph transformer accommodate different subgraphs different group allow model focus salient subgraphs result experiment neural machine translation task show mog inspire model yield effective performance improvement
development deep learn techniques large scale datasets question answer qa systems quickly improve provide accurate satisfy answer however current qa systems either focus sentence level answer ie answer selection phrase level answer ie machine read comprehension produce compositional answer throughout investigate compositional question answer systems assemble several support evidence document generate final answer difficult sentence level phrase level qa paper present large scale compositional question answer dataset contain 120k human label question answer dataset compose discontiguous sentence correspond document tackle comqa problem propose hierarchical graph neural network represent document low level word high level sentence also devise question selection node selection task pre train propose model achieve significant improvement previous machine read comprehension methods pre train methods cod dataset find urlhttps githubcom benywon comqa
slot fill identify contiguous span word utterance correspond certain parameters ie slot user request query slot fill one important challenge modern task orient dialog systems supervise learn approach prove effective tackle challenge need significant amount label train data give domain however new domains ie unseen train may emerge deployment thus imperative model seamlessly adapt fill slot see unseen domains unseen domains contain unseen slot type train data even see slot unseen domains typically present different contexts set commonly refer zero shoot slot fill little work focus set limit experimental evaluation exist model mainly rely context independent embed base similarity measure fail detect slot value unseen domains partially propose new zero shoot slot fill neural model leona work three step step one acquire domain oblivious context aware representations utterance word exploit linguistic feature b name entity recognition cue c contextual embeddings pre train language model step two fine tune rich representations produce slot independent tag word step three exploit generalizable context aware utterance slot similarity feature word level use slot independent tag contextualizes produce slot specific predictions word thorough evaluation four diverse public datasets demonstrate approach consistently outperform sota model one thousand, seven hundred and fifty-two two thousand, two hundred and fifteen one thousand, seven hundred and forty-two one thousand, seven hundred and ninety-five average unseen domains snip atis multiwoz sgd datasets respectively
dialogue state track dst form core component automate chatbot base systems design specific goals like hotel taxi reservation tourist information etc increase need deploy systems new domains solve problem zero shoot dst become necessary rise trend learn transfer knowledge resource rich domains unknown domains minimal need additional data work explore merit meta learn algorithms transfer hence propose meta learner reptile specific dst problem extensive experimentation provide clear evidence benefit conventional approach across different domains methods base model datasets significant five twenty-five improvement baseline low data set propose meta learner agnostic underlie model hence exist state art dst system improve performance unknown domains use train strategy
research text generation multimodal input largely focus static image less video data paper propose new task narration generation complement videos narration texts interject several place narrations part video contribute storyline unfold moreover context inform since include information appropriate timeframe video cover also need include every detail show input scenes caption would collect new dataset animate television series peppa pig furthermore formalize task narration generation include two separate task time content generation present set model new task
gpt three attract lot attention due superior performance across wide range nlp task especially powerful versatile context shoot learn ability despite success find empirical result gpt three depend heavily choice context examples work investigate whether effective strategies judiciously select context examples relative random sample better leverage gpt three shoot capabilities inspire recent success leverage retrieval module augment large scale neural network model propose retrieve examples semantically similar test sample formulate correspond prompt intuitively context examples select strategy may serve informative input unleash gpt three extensive knowledge evaluate propose approach several natural language understand generation benchmarks retrieval base prompt selection approach consistently outperform random baseline moreover observe sentence encoders fine tune task relate datasets yield even helpful retrieval result notably significant gain observe task table text generation four hundred and nineteen totto dataset open domain question answer four hundred and fifty-five nq dataset hope investigation could help understand behaviors gpt three large scale pre train lms general enhance shoot capabilities
e commerce opinion tag refer rank list tag provide e commerce platform reflect characteristics review item assist consumers quickly grasp large number review item opinion tag increasingly apply e commerce platforms current mechanisms generate opinion tag rely either manual label heuristic methods time consume ineffective paper propose abstractive opinion tag task systems automatically generate rank list opinion tag base need occur give set user generate review abstractive opinion tag task come three main challenge one noisy nature review two formal nature opinion tag vs colloquial language usage review three need distinguish different items similar aspects address challenge propose abstractive opinion tag framework name aot net generate rank list opinion tag give large number review first sentence level salience estimation component estimate review salience score next review cluster rank component rank review two step first review group cluster rank cluster size review within cluster rank distance cluster center finally give rank review rank aware opinion tag component incorporate alignment feature alignment loss generate rank list opinion tag facilitate study task create release large scale dataset call ecomtag crawl real world e commerce websites extensive experiment conduct ecomtag dataset verify effectiveness propose aot net term various evaluation metrics
past years knowledge base question answer kbqa aim answer natural language question use facts knowledge base well develop exist approach often assume static knowledge base however knowledge evolve time real world directly apply fine tune strategy evolve knowledge base suffer serious catastrophic forget problem paper propose new incremental kbqa learn framework progressively expand learn capacity humans specifically comprise margin distil loss collaborative exemplar selection method overcome catastrophic forget problem take advantage knowledge distillation reorganize simplequestion dataset evaluate propose incremental learn solution kbqa comprehensive experiment demonstrate effectiveness efficiency work evolve knowledge base
recent advancements language model base recurrent neural network transformers architecture achieve state art result wide range natural language process task pos tag name entity recognition text classification however language model pre train high resource languages like english german spanish multi lingual language model include indian languages like hindi telugu bengali train corpus often fail represent linguistic feature languages primary language study introduce hinflair language representation model contextual string embeddings pre train large monolingual hindi corpus experiment conduct six text classification datasets hindi dependency treebank analyze performance contextualized string embeddings hindi language result show hinflair outperform previous state art publicly available pre train embeddings downstream task like text classification pos tag also hinflair combine fasttext embeddings outperform many transformers base language model train particularly hindi language
conversational analyse humans manually weave multimodal information transcripts significantly time consume introduce system automatically expand verbatim transcripts video record conversations use multimodal data stream system use set preprocessing rule weave multimodal annotations verbatim transcripts promote interpretability feature engineer contributions two fold firstly identify range multimodal feature relevant detect rapport build secondly expand range multimodal annotations show expansion lead statistically significant improvements detect rapport build
present approach automatic punctuation restoration bert model english hungarian english conduct experiment ted talk commonly use benchmark punctuation restoration hungarian evaluate model szeged treebank dataset best model achieve macro average f1 score seven hundred and ninety-eight english eight hundred and twenty-two hungarian code publicly available
natural language inference nli recognize textual entailment rte task predict entailment relation pair sentence premise hypothesis task describe valuable test grind development semantic representations key component natural language understand evaluation benchmarks model understand entailment encode premise hypothesis however experiment poliak et al reveal strong preference model towards pattern observe hypothesis base ten dataset comparison result indicate existence statistical irregularities present hypothesis bias model perform competitively state art recast datasets provide large scale generation nli instance due minimal human intervention paper generate provide fine grain analysis potential statistical pattern bias nli model work analyze hypothesis model train one recast datasets provide poliak et al word level pattern result indicate existence potential lexical bias could contribute inflate model performance
focus paper address knowledge acquisition bottleneck name entity recognition ner mutations analyse different approach build manually annotate data address first impact use single annotator vs two annotators order measure whether multiple annotators require evaluate performance loss use single annotator apply different methods sample train data second annotation aim improve quality dataset without require full pass use hold double annotate data build two scenarios different type rank similarity base confidence base evaluate approach ability identify train instance erroneous case single annotator label differ double annotation discussion ii mutation ner performance state art classifiers integrate fix different thresholds
computational study lexical semantic change lsc take past years see increase interest field computational sciences linguistics research far focus methods model detect semantic change use large diachronic textual data majority approach employ neural embeddings methods offer easy model diachronic text one main reason spike interest lsc neural model leave many aspects problem unsolved field several open complex challenge chapter aim describe important challenge outline future directions
considerable progress academic benchmarks read comprehension rc task state art model close gap human performance extractive question answer datasets squad twenty nq also introduce auxiliary task require model predict question answer text however production settings also necessary provide confidence estimate performance underlie rc model answer extraction answerability detection propose novel post prediction confidence estimation model call mrc short mr confident train improve system ability refrain make incorrect predictions improvements four point measure area curve auc score mrc benefit novel white box feature leverage underlie rc model gradients performance prediction particularly important case domain shift measure train rc model squad twenty evaluate nq mrc improve auc also traditional answerability prediction measure five point improvement f1
recently nlp community start show interest towards challenge task hostile post detection paper present system share task constraint2021 hostile post detection hindi data share task provide hindi devanagari script collect twitter facebook multi label multi class classification problem data instance annotate one five class fake hate offensive defamation non hostile propose two level architecture make bert base classifiers statistical classifiers solve problem team albatross score nine thousand, seven hundred and nine coarse grain hostility f1 score measure hostile post detection hindi subtask secure 2nd rank forty-five team task submission rank 2nd 3rd total one hundred and fifty-six submissions coarse grain hostility f1 score nine thousand, seven hundred and nine nine thousand, seven hundred and three respectively fine grain score also encourage improve finetuning code publicly available
recent years lot research paper study publish development effective approach benefit large amount user generate content build intelligent predictive model top research apply machine learn base approach tackle hurdle come persian user generate textual content unfortunately still inadequate research exploit machine learn approach classify cluster persian text analyze persian text suffer lack resources specifically datasets text manipulation tool since syntax semantics persian language different english languages available resources languages instantly usable persian addition recognition nouns pronouns part speech tag find word boundary stem character manipulations persian language still unsolved issue require study therefore efforts make research address challenge present approach use machine translate datasets conduct sentiment analysis persian language finally dataset rehearse different classifiers feature engineer approach result experiment show promise state art performance contrast previous efforts best classifier support vector machine achieve precision nine thousand, one hundred and twenty-two recall nine thousand, one hundred and seventy-one f1 score nine thousand, one hundred and forty-six
intent classification slot fill two critical task natural language understand traditionally two task deem proceed independently however recently joint model intent classification slot fill achieve state art performance prove exist strong relationship two task article compilation past work natural language understand especially joint intent classification slot fill observe three milestones research far intent detection identify speaker intention slot fill label word token speech text finally joint intent classification slot fill task article describe trend approach issue data set evaluation metrics intent classification slot fill also discuss representative performance value describe share task provide pointers future work give prior work interpret state art trend provide multiple table describe summarise past research along different dimension include type feature base approach dataset domain use
despite pre train language model bert achieve appeal performance wide range natural language process task computationally expensive deploy real time applications typical method adopt knowledge distillation compress large pre train model teacher model small student model however target domain scarce train data teacher hardly pass useful knowledge student yield performance degradation student model tackle problem propose method learn augment data scarce domain bert knowledge distillation learn cross domain manipulation scheme automatically augment target help resource rich source domains specifically propose method generate sample acquire stationary distribution near target data adopt reinforce selector automatically refine augmentation strategy accord performance student extensive experiment demonstrate propose method significantly outperform state art baselines four different task data scarce domains compress student model even perform better original large teacher model much fewer parameters sim133 label examples available
annotate train data sequence tag texts usually time consume recent advance transfer learn natural language process conjunction active learn open possibility significantly reduce necessary annotation budget first thoroughly investigate powerful combination sequence tag task conduct extensive empirical study various bayesian uncertainty estimation methods monte carlo dropout options deep pre train model active learn framework find best combinations different type model besides also demonstrate acquire instance active learn full size transformer substitute distil version yield better computational performance reduce obstacles apply deep active learn practice
paper propose hybrid technique semantic question match use propose two layer taxonomy english question augment state art deep learn model question class obtain deep learn base question classifier experiment perform three open domain datasets demonstrate effectiveness propose approach achieve state art result partial order question rank poqr benchmark dataset empirical analysis show couple standard distributional feature provide question encoder knowledge taxonomy effective either deep learn dl taxonomy base knowledge alone
word alignment parallel corpora wide variety applications include learn translation lexicons cross lingual transfer language process tool automatic evaluation analysis translation output great majority past work word alignment work perform unsupervised learn parallel texts recently however work demonstrate pre train contextualized word embeddings derive multilingually train language model lms prove attractive alternative achieve competitive result word alignment task even absence explicit train parallel data paper examine methods marry two approach leverage pre train lms fine tune parallel text objectives design improve alignment quality propose methods effectively extract alignments fine tune model perform experiment five language pair demonstrate model consistently outperform previous state art model varieties addition demonstrate able train multilingual word aligners obtain robust performance different language pair aligner awesome align word embed space multilingual encoders pre train model available https githubcom neulab awesome align
dialog state track dst integral part modern dialog systems aim track user preferences constraints slot task orient dialogs real world settings constantly change service dst systems must generalize new domains unseen slot type exist methods dst generalize well new slot name many require know ontologies slot type value inference introduce novel ontology free framework support natural language query unseen constraints slot multi domain task orient dialogs approach base generative question answer use conditional language model pre train substantive english sentence model improve joint goal accuracy zero shoot domain adaptation settings nine absolute previous state art multiwoz twenty-one dataset
propose parasci first large scale paraphrase dataset scientific field include thirty-three thousand, nine hundred and eighty-one paraphrase pair acl parasci acl three hundred and sixteen thousand and sixty-three pair arxiv parasci arxiv dig characteristics common pattern scientific paper construct dataset though intra paper inter paper methods collect citations paper aggregate definitions scientific term take advantage sentence paraphrase partially put pdbert general paraphrase discover method major advantage paraphrase parasci lie prominent length textual diversity complementary exist paraphrase datasets parasci obtain satisfactory result human evaluation downstream task especially long paraphrase generation
ground human machine conversation document effective way improve performance retrieval base chatbots however part document content may relevant help select appropriate response round thus crucial select part document content relevant current conversation context paper propose document content selection network csn perform explicit selection relevant document content filter irrelevant part show experiment two public document ground conversation datasets csn effectively help select relevant document content conversation context produce better result state art approach code datasets available https githubcom daod csn
natural language understand see increase number publications last years especially robust word embeddings model become prominent prove able capture represent semantic relationships massive amount data nevertheless traditional model often fall short intrinsic issue linguistics polysemy homonymy expert system make use natural language core affect weak semantic representation text result inaccurate outcomes base poor decisions mitigate issue propose novel approach call suitable sense annotation mssa disambiguate annotate word specific sense consider semantic effect context approach bring three main contributions semantic representation scenario unsupervised technique disambiguate annotate word sense ii multi sense embeddings model extend traditional word embeddings algorithm iii recurrent methodology allow model use representations refine test approach six different benchmarks word similarity task show approach produce state art result outperform several complex state art systems
hateful toxic content become significant concern today world due exponential rise social media increase hate speech harmful content motivate researchers dedicate substantial efforts challenge direction hateful content identification task propose approach automatically classify hate speech offensive content use datasets obtain fire two thousand and nineteen two thousand and twenty share task perform experiment take advantage transfer learn model observe pre train bert model multilingual bert model give best result code make publically available https githubcom suman101112 hasoc fire two thousand and twenty
text classification fundamental problem field natural language process text classification mainly focus give importance relevant feature help classify textual data apart text redundant highly correlate feature feature increase complexity classification algorithm thus many dimensionality reduction methods propose traditional machine learn classifiers use dimensionality reduction methods machine learn classifiers achieve good result paper propose hybrid feature selection method obtain relevant feature combine various filter base feature selection methods fasttext classifier present three ways implement feature selection neural network pipeline observe reduction train time feature selection methods use along neural network also observe slight increase accuracy datasets
paper present transfer learn system perform technical domain identification multilingual text data submit two run one use transformer model bert use xlm roberta cnn model text classification model allow us identify domain give sentence icon two thousand and twenty share task techdofication technical domain identification system rank best subtasks 1d 1g give techdofication dataset
terminology extraction also know term extraction subtask information extraction goal terminology extraction extract relevant word phrase give corpus automatically paper focus unsupervised automate domain term extraction method consider chunk preprocessing rank domain specific term use relevance cohesion function icon two thousand and twenty share task two termtraction
reliable evaluation protocols utmost importance reproducible nlp research work show sometimes neither metric conventional human evaluation sufficient draw conclusions system performance use sentence compression example task demonstrate system game well establish dataset achieve state art result contrast result report previous work show correlation human judgements metric score manual analysis state art system output demonstrate high metric score may indicate better fit data better output perceive humans
prior work task orient dialogue systems restrict limit coverage domain apis users oftentimes domain relate request cover apis challenge track aim expand coverage task orient dialogue systems incorporate external unstructured knowledge source define three task knowledge seek turn detection knowledge selection knowledge ground response generation introduce data set neural baseline model three task challenge track receive total one hundred and five entries twenty-four participate team evaluation result ensemble methods different large scale pretrained language model achieve high performances improve knowledge selection capability better generalization unseen data
last two decades progressively turn internet social media find news entertain conversations share opinion recently openai develop chine learn system call gpt two generative pre train transformer two pro duce deepfake texts generate block text base brief write prompt look like write humans facilitate spread false auto generate text line progress order counteract potential dangers several methods pro pose detect text write language model paper propose transfer learn base model able detect arabic sentence write humans automatically generate bots dataset base tweet previous work crawl extend use twitter api use gpt2 small arabic generate fake arabic sentence evaluation compare different recurrent neural network rnn word embeddings base baseline model namely lstm bi lstm gru bi gru transformer base model new transfer learn model obtain accuracy ninety-eight best knowledge work first study arabert gpt2 combine detect classify arabic auto generate texts
lexical semantic change detection new innovative research field optimal fine tune model include pre post process largely unclear optimize exist model pre train large corpora refine diachronic target corpora tackle notorious small data problem ii apply post process transformations show improve performance synchronic task result provide guide application optimization lexical semantic change detection model across various learn scenarios
exponential growth online marketplaces user generate content therein aspect base sentiment analysis become important ever work critically review representative sample model publish past six years lens practitioner eye towards deployment production first rigorous empirical evaluation reveal poor reproducibility average four five drop test accuracy across sample second bolster confidence empirical evaluation report experiment two challenge data slice observe consistent twelve fifty-five drop accuracy third study possibility transfer across domains observe little ten twenty-five domain specific train dataset use conjunction datasets domains within locale largely close gap complete cross domain complete domain predictive performance lastly open source two large scale annotate review corpora large e commerce portal india order aid study replicability transfer hope fuel growth field
web search essential way human obtain information still great challenge machine understand content web page paper introduce task web base structural read comprehension give web page question task find answer web page task require system understand semantics texts also structure web page moreover propose websrc novel web base structural read comprehension dataset websrc consist 044m question answer pair collect 65k web page correspond html source code screenshots metadata question websrc require certain structural understand web page answer answer either text span web page yes evaluate various strong baselines dataset show difficulty task also investigate usefulness structural information visual feature dataset task publicly available https speechlab sjtugithubio websrc
pre train language model achieve great successes various natural language understand nlu task due capacity capture deep contextualized information text pre train large scale corpora one fundamental components pre train language model vocabulary especially train multilingual model many different languages technical report present practice train multilingual pre train language model bbpe byte level bpe ie byte pair encode experiment adopt architecture nezha underlie pre train language model result show nezha train byte level subwords consistently outperform google multilingual bert vanilla nezha notable margin several multilingual nlu task release source code byte level vocabulary build tool multilingual pre train language model
comparison numerous debiasing methods propose static non contextualised word embeddings discriminative bias contextualised embeddings receive relatively little attention propose fine tune method apply token sentence level debias pre train contextualised embeddings propose method apply pre train contextualised embed model without require retrain model use gender bias illustrative example conduct systematic study use several state art sota contextualised representations multiple benchmark datasets evaluate level bias encode different contextualised embeddings debiasing use propose method find apply token level debiasing tokens across layer contextualised embed model produce best performance interestingly observe trade create accurate vs unbiased contextualised embed model different contextualised embed model respond differently trade
word embeddings train large corpora show encode high level unfair discriminatory gender racial religious ethnic bias contrast human write dictionaries describe mean word concise objective unbiased manner propose method debiasing pre train word embeddings use dictionaries without require access original train resources knowledge regard word embed algorithms use unlike prior work propose method require type bias pre define form word list learn constraints must satisfy unbiased word embeddings automatically dictionary definitions word specifically learn encoder generate debiased version input word embed retain semantics pre train word embeddings b agree unbiased definition word accord dictionary c remain orthogonal vector space span bias basis vectors pre train word embed space experimental result standard benchmark datasets show propose method accurately remove unfair bias encode pre train word embeddings preserve useful semantics
natural languages predominant fix word order example english word order use often subject verb object work attempt explain phenomena well typological find regard word order functional perspective target question whether fix word order give functional advantage may explain languages common end consider evolutionary model language show theoretically use genetic algorithm base simulation optimal language one fix word order also show add information sentence case markers noun verb distinction reduce need fix word order accordance typological find
transformer base language model specifically bert base architectures achieve state art performance many downstream task however relatively low resource language thai choices model limit train bert base model base much smaller dataset finetuning multi lingual model yield suboptimal downstream performance moreover large scale multi lingual pretraining take account language specific feature thai overcome limitations pretrain language model base roberta base architecture large deduplicated clean train set 78gb total size curated diverse domains social media post news article publicly available datasets apply text process rule specific thai importantly preserve space important chunk sentence boundaries thai subword tokenization also experiment word level syllable level sentencepiece tokenization smaller dataset explore effect tokenization downstream performance model wangchanberta base att spm uncase train 785gb dataset outperform strong baselines nbsvm crf ulmfit multi lingual model xlmr mbert sequence classification token classification task human annotate mono lingual contexts
last years release bert multilingual transformer base model take nlp community storm bert base model achieve state art result various nlp task include dialog task one limitation bert lack ability handle long text sequence default bert maximum wordpiece token sequence length five hundred and twelve recently renew interest tackle bert limitation handle long text sequence addition new self attention base architectures however little research impact limitation respect dialog task dialog task inherently different nlp task due presence multiple utterances multiple speakers may interlink across different turn b longer length dialogs work empirically evaluate impact dialog length performance bert model next response selection dialog task four publicly available one internal multi turn dialog datasets observe little impact performance long dialogs even simplest approach truncate input work really well
paper present novel two stage framework extract opinionated sentence give news article first stage naive bay classifier utilize local feature assign score sentence score signify probability sentence opinionated second stage use prior within hit hyperlink induce topic search schema exploit global structure article relation sentence hit schema opinionated sentence treat hubs facts around opinions treat authorities algorithm implement evaluate set manually mark data show use hit significantly improve precision baseline naive bay classifier also argue propose method actually discover underlie structure article thus extract various opinions group support facts well support opinions article
bert achieve superior performances natural language understand nlu task however bert possess large number parameters demand certain resources deploy acceleration dynamic early exit bert deebert propose recently incorporate multiple exit adopt dynamic early exit mechanism ensure efficient inference obtain efficiency performance tradeoff performances early exit multi exit bert significantly worse late exit paper leverage gradient regularize self distillation robust train multi exit bert romebert effectively solve performance imbalance problem early late exit moreover propose romebert adopt one stage joint train strategy multi exit bert backbone deebert need two stag require train time extensive experiment glue datasets perform demonstrate superiority approach code available https githubcom romebert romebert
speak communication occur noisy channel characterize high level environmental noise variability within speakers lexical syntactic ambiguity give properties receive linguistic input robust speak word recognition language process generally rely heavily listeners prior knowledge evaluate whether candidate interpretations input less likely compare several broad coverage probabilistic generative language model ability capture human linguistic expectations serial reproduction experimental paradigm speak utterances reproduce successive participants similar children game telephone use elicit sample reflect linguistic expectations english speak adults evaluate suite probabilistic generative language model yield chain utterances find model make use abstract representations precede linguistic context ie phrase structure best predict change make people course serial reproduction logistic regression model predict word utterance likely lose change course speak transmission corroborate result interpret find light research highlight interaction memory base constraints representations language process
fake news article often stir readers attention mean emotional appeal arouse feel unlike short news texts author longer article exploit affective factor manipulate readers add exaggerations fabricate events order affect readers emotions capture propose paper model flow affective information fake news article use neural architecture propose model fakeflow learn flow combine topic affective information extract text evaluate model performance several experiment four real world datasets result show fakeflow achieve superior result compare state art methods thus confirm importance capture flow affective information news article
acronyms abbreviations short form longer phrase ubiquitously employ various type write despite usefulness save space write reader time read also provide challenge understand text especially acronym define text use far definition long texts alleviate issue considerable efforts research community software developers build systems identify acronyms find correct mean text however none exist work provide unify solution capable process acronyms various domains publicly available thus provide first web base acronym identification disambiguation system process acronyms various domains include scientific biomedical general domains web base system publicly available http iqcsuoregonedu5000 demo video available https youtube iksh7lqi42m system source code also available https githubcom amirveyseh maddog
new method text sql parse grammar pre train gp propose decode deep relations question database firstly better utilize information databases random value add behind question word recognize column new sentence serve model input secondly initialization vectors decoder part optimize reference former encode question information concern finally new approach call flood level adopt get non zero train loss generalize better result encode sentence grappa rat sql model achieve better performance spider cross db text sql dataset seven hundred and twenty-eight dev six hundred and ninety-eight test experiment show method easier converge train excellent robustness
rapid growth literature accumulate diverse yet comprehensive biomedical knowledge hide mine drug interactions however difficult extract heterogeneous knowledge retrieve even discover latest novel knowledge efficient manner address problem propose egfi extract consolidate drug interactions large scale medical literature text data specifically egfi consist two part classification generation classification part egfi encompass language model biobert comprehensively pre train biomedical corpus particular propose multi head attention mechanism pack bigru fuse multiple semantic information rigorous context model generation part egfi utilize another pre train language model biogpt two generation sentence select base filter rule evaluate classification part ddis two thousand and thirteen dataset dtis dataset achieve fi score eight hundred and forty-two seven hundred and twenty respectively moreover apply classification part distinguish high quality generate sentence verify exit growth truth confirm filter sentence generate sentence record drugbank ddis two thousand and thirteen dataset also demonstrate potential egfi identify novel drug relationships
paper propose cholan modular approach target end end entity link el knowledge base cholan consist pipeline two transformer base model integrate sequentially accomplish el task first transformer model identify surface form entity mention give text mention second transformer model employ classify target entity among predefined candidates list latter transformer feed enrich context capture sentence ie local context entity description gain wikipedia external contexts use state art el approach empirical study conduct two well know knowledge base ie wikidata wikipedia empirical result suggest cholan outperform state art approach standard datasets conll aida msnbc aquaint ace2004 rex
several methods explore automate part systematic map sm systematic review sr methodologies challenge typically evolve around gap semantic understand text well lack domain background knowledge necessary bridge gap paper investigate possible ways automate part sm sr process ie extract keywords key phrase scientific document use unsupervised methods use basis construct correspond classification scheme use semantic key phrase cluster techniques specifically explore effect ensemble score measure key phrase extraction explore semantic network base word embed embed representation phrase semantics finally also explore cluster use group relate key phrase evaluation conduct dataset publications pertain domain explainable ai construct use standard publicly available digital libraries set index term keywords result show ensemble rank score improve key phrase extraction performance semantic network base word embed base conceptnet semantic network similar performance contextualized word embed however former computationally efficient finally semantic key phrase cluster term level group similar term together suitable classification scheme
paper describe efforts establish simple knowledge base build semantic network compose concepts word relationships context disasters philippines primary source data collection news article scrap various philippine news websites use word embeddings extract semantically similar co occur word initial seed word list arrive expand ontology total four hundred and fifty word assertions let experts field linguistics disasters weather science evaluate knowledge base arrive agreeability rate sixty-four perform time base analysis assertions identify important semantic change capture knowledge base trend roles play human entities b memberships human entities c common association disaster relate word context specific knowledge base develop study adapt intelligent agents chat bots integrate platforms facebook messenger answer disaster relate query
recent work terminology integration machine translation assume terminology translations give already inflect form suitable target language sentence day day work professional translators however seldom case translators work bilingual glossaries term give dictionary form find right target language form part translation process argue requirement apriori specify target language form unrealistic impede practical applicability previous work work propose train machine translation systems use source side data augmentation method annotate randomly select source language word target language lemmas show systems train augment data readily usable terminology integration real life translation scenarios experiment terminology translation morphologically complex baltic uralic languages show improvement seven bleu point baseline systems mean terminology integration average improvement four bleu point previous work result human evaluation indicate four hundred and seventy-seven absolute improvement previous work term translation accuracy translate latvian
emotion recognition er important task natural language process nlp due high impact real world applications health well author profile consumer analysis security current approach er mainly classify emotions independently without consider emotions co exist approach overlook potential ambiguities multiple emotions overlap propose new model spanemo cast multi label emotion classification span prediction aid er model learn associations label word sentence furthermore introduce loss function focus model multiple co exist emotions input sentence experiment perform semeval2018 multi label emotion data three language set ie english arabic spanish demonstrate method effectiveness finally present different analyse illustrate benefit method term improve model performance learn meaningful associations emotion class word sentence
embed entities relations knowledge graph low dimensional space show impressive performance predict miss link entities although progress achieve exist methods heuristically motivate theoretical understand embeddings comparatively underdevelop paper extend random walk model arora et al 2016a word embeddings knowledge graph embeddings kges derive score function evaluate strength relation r two entities h head tail moreover show marginal loss minimisation popular objective use much prior work kge follow naturally log likelihood ratio maximisation probabilities estimate kges accord theoretical relationship propose learn objective motivate theoretical analysis learn kges give knowledge graph use derive objective accurate kges learn fb15k237 wn18rr benchmark datasets provide empirical evidence support theory
sentence compression task generate shorter yet grammatical version give sentence preserve essence original sentence paper propose black box optimizer compression b boc give black box compression algorithm assume sentence need compress find best candidates compression order maximize compression rate quality give require compression ratio consider two scenarios single sentence compression ii sentence sequence compression first scenario optimizer train predict well sentence could compress meet specify ratio requirement latter desire compression ratio apply sequence sentence eg paragraph whole rather individual sentence achieve use b boc assign optimal compression ratio sentence cast knapsack problem solve use bound dynamic program evaluate b boc scenarios three datasets demonstrate optimizer improve accuracy rouge f1 score compare direct application compression algorithms
linguistic accommodation process speakers adjust accent diction vocabulary aspects language accord communication style one another previous research show linguistic accommodation correlate gap power status speakers way promote approval discussion efficiency work provide novel perspective phenomena explore correlation open mindedness speaker rather social status process thousands unstructured argumentative discussions take place reddit change view cmv subreddit demonstrate open mindedness relate assume role speaker different contexts discussion level surprisingly find discussions reach agreement present lower level accommodation
objective work aim demonstrate effectiveness hybrid approach base sentence bert model retrofit algorithm compute relatedness two biomedical concepts materials methods generate concept vectors encode concept prefer term use elmo bert sentence bert model use bioelmo clinical elmo use ontology knowledge free okf model like pubmedbert biobert bioclinicalbert ontology knowledge inject oki model like sapbert coderbert kbbert umlsbert train bert model use siamese network snli stsb datasets allow model learn semantic information phrase sentence level represent multi word concepts better finally inject ontology relationship knowledge concept vectors use retrofit algorithm concepts various umls relationships evaluate hybrid approach four publicly available datasets also include recently release ehr relb dataset ehr relb largest publicly available relatedness dataset eighty-nine term multi word make challenge result sentence bert model mostly outperform correspond bert model concept vectors generate use sentence bert model base sapbert retrofit use umls relate concepts achieve best result four datasets conclusions sentence bert model effective compare bert model compute relatedness score case inject ontology knowledge concept vectors enhance quality contribute better relatedness score
joint entity relation extraction framework construct unify model perform entity recognition relation extraction simultaneously exploit dependency two task mitigate error propagation problem suffer pipeline model current efforts joint entity relation extraction focus enhance interaction entity recognition relation extraction parameter share joint decode ad hoc trick eg model semi markov decision process cast multi round read comprehension task however still two issue table first interaction utilize methods still weak uni directional unable model mutual dependency two task second relation trigger ignore methods help explain humans would extract relation sentence essential relation extraction overlook end present trigger sense memory flow framework trimf joint entity relation extraction build memory module remember category representations learn entity recognition relation extraction task base design multi level memory flow attention mechanism enhance bi directional interaction entity recognition relation extraction moreover without human annotations model enhance relation trigger information sentence trigger sensor module improve model performance make model predictions better interpretation experiment result show propose framework achieve state art result improve relation f1 five thousand, two hundred and forty-four thirty-two scierc six thousand, six hundred and forty-nine forty-nine ace05 seven thousand, two hundred and thirty-five six conll04 eight thousand and sixty-six twenty-three ade
develop process execution graph peg document level representation real world wet lab biochemistry protocols address challenge cross sentence relations long range coreference ground implicit arguments manually annotate peg corpus complex lab protocols novel interactive textual simulator keep track entity traits semantic constraints annotation use data develop graph prediction model find good entity identification local relation extraction corpus facilitate exploration challenge long range relations
assess quality arguments claim arguments compose become key task computational argumentation however even different claim share stance topic assessment depend prior perception weight different aspects topic discuss render difficult learn topic independent quality indicators paper study claim quality assessment irrespective discuss aspects compare different revisions claim compile large scale corpus 377k claim revision pair various type kialocom cover diverse topics politics ethics entertainment others propose two task assess claim revision pair better b rank versions claim quality first experiment embed base logistic regression transformer base neural network show promise result suggest learn indicators generalize well across topics detail error analysis give insights quality dimension claim assess reliably provide data script need reproduce result
task datasets evaluation metrics important concepts understand experimental scientific paper however previous work information extraction scientific literature mainly focus abstract treat datasets separate type entity zadeh schumann two thousand and sixteen luan et al two thousand and eighteen paper present new corpus contain domain expert annotations task dataset metric entities two thousand sentence extract nlp paper report experiment result tdm extraction use simple data augmentation strategy apply tagger around thirty thousand nlp paper acl anthology corpus make publicly available community foster research scientific publication summarization erera et al two thousand and nineteen knowledge discovery
adobe portable document format pdf popular way distribute view document rich visual markup present challenge nlp practitioners wish use information contain within pdf document train model data analysis annotate document difficult paper present pdf annotation label structure pawls new annotation tool design specifically pdf document format pawls particularly suit mix mode annotation scenarios annotators require extend context annotate accurately pawls support span base textual annotation n ary relations freeform non textual bound box export convenient format train multi modal machine learn model read pawls server available https pawlsappsallenaiorg source code available https githubcom allenai pawls
natural language process nlp task eg question answer english benefit knowledge task eg name entity recognition english knowledge languages eg question answer spanish share representations typically learn isolation either across task across languages work propose meta learn approach learn interactions task languages also investigate role different sample strategies use meta learn present experiment five different task six different languages xtreme multilingual benchmark dataset meta learn model clearly improve performance compare competitive baseline model also include multi task baselines also present zero shoot evaluations unseen target languages demonstrate utility propose model
understand attitudes towards climate emergency vary hold key drive policy change effective action mitigate climate relate risk oil gas industry account significant proportion global emissions could speculate relationship crude oil futures sentiment towards climate emergency use latent dirichlet allocation topic model bespeak twitter dataset study show possible split conversation surround climate emergency three distinct topics forecast crude oil futures use seasonal autoregressive integrate move average model give promise result root mean square error one hundred and ninety-six two hundred and nine train test data respectively understand variation attitudes towards climate emergency provide inconclusive result could improve use spatial temporal analysis methods density base cluster dbscan
paper survey fifty-four english machine read comprehension datasets view provide convenient resource researchers interest problem categorize datasets accord question answer form compare across various dimension include size vocabulary data source method creation human performance level first question word analysis reveal wikipedia far common data source relative lack question across datasets
avoid mean conflation deficiency word embeddings number model aim embed individual word sense methods one time perform well task word sense induction wsi since overtake task specific techniques exploit contextualized embeddings however sense embeddings contextualization need mutually exclusive introduce polylm method formulate task learn sense embeddings language model problem allow contextualization techniques apply polylm base two underlie assumptions word sense firstly probability word occur give context equal sum probabilities individual sense occur secondly give occurrence word one sense tend much plausible context others evaluate polylm wsi show perform considerably better previous sense embed techniques match current state art specialize wsi method despite six time fewer parameters code pre train model available https githubcom alanansell polylm
st lawrence island yupik iso six hundred and thirty-nine three ess endanger polysynthetic language inuit yupik language family indigenous alaska chukotka work present step step pipeline digitization write texts first publicly available digital corpus st lawrence island yupik create use pipeline corpus great potential future linguistic inquiry research nlp also develop use yupik language education revitalization primary goal enable easy access yupik texts educators members yupik community secondary goal support development language technology spell checker text completion systems interactive e book language learn apps use yupik community
aspect base sentiment analysis absa accomplish fine grain analysis define aspects give document sentence sentiments convey regard aspect level analysis detail version capable explore nuanced viewpoints review research available absa focus english language work available arabic previous work arabic base regular methods machine learn mainly depend group rare resources tool analyze process arabic content lexicons lack resources present another challenge overcome obstacles deep learn dl base methods propose use two model base gate recurrent units gru neural network absa first one dl model take advantage representations word character via combination bidirectional gru convolutional neural network cnn conditional random field crf make bgru cnn crf model extract main opinionated aspects ote second interactive attention network base bidirectional gru ian bgru identify sentiment polarity toward extract aspects evaluate model use benchmarked arabic hotel review dataset result indicate propose methods better baseline research task three hundred and eighty-five enhancement f1 score opinion target extraction t2 seventy-five accuracy aspect base sentiment polarity classification t3 obtain f1 score six thousand, nine hundred and forty-four t2 accuracy eight thousand, three hundred and ninety-eight t3
contrast word sentence level counterparts character embeddings still poorly understand aim close gap depth study english character embeddings use resources research grapheme color synesthesia neuropsychological phenomenon letter associate color give us insight character similar synesthetes character organize color space compare ten different character embeddings ask similar character embeddings synesthete perception character similar character embeddings extract different model find lstms agree humans transformers compare across task grapheme phoneme conversion result human like character embeddings finally elmo embeddings differ humans model
table web document pervasive directly use answer many query search web motivate integration question answer often information present table succinct hard interpret standard language representations hand table often appear within textual context article describe table use information article additional context potentially enrich table representations work aim improve question answer table refine table representations base information surround text also present effective method combine text table base predictions question answer full document obtain significant improvements natural question dataset
paraphrase generation play essential role natural language process nlp many downstream applications however train supervise paraphrase model require many annotate paraphrase pair usually costly obtain hand paraphrase generate exist unsupervised approach usually syntactically similar source sentence limit diversity paper demonstrate possible generate syntactically various paraphrase without need annotate paraphrase pair propose syntactically control paraphrase generator synpg encoder decoder base model learn disentangle semantics syntax sentence collection unannotated texts disentanglement enable synpg control syntax output paraphrase manipulate embed syntactic space extensive experiment use automatic metrics human evaluation show synpg perform better syntactic control unsupervised baselines quality generate paraphrase competitive also demonstrate performance synpg competitive even better supervise model unannotated data large finally show syntactically control paraphrase generate synpg utilize data augmentation improve robustness nlp model
tool explore scientific literature essential scientists especially biomedicine million new paper publish every year many tool provide users ability search specific entities eg proteins diseases track mention paper pubmed well know database biomedical paper rely human curators add annotations take several weeks new paper paper get tag machine learn model develop facilitate semantic index scientific paper however performance comprehensive ontologies biomedical concepts reach level typical entity recognition problems study nlp large part due low resources ontologies large lack descriptive text define entities label data cover small portion ontology paper develop new model overcome challenge one generalize entities unseen train time two incorporate link predictions mention segmentation decisions approach achieve new state art result umls ontology traditional recognition link eight f1 pts well semantic index base evaluation ten f1 pts
machine translation field academia industry grow interest increasingly powerful systems use corpora several hundred million several billion examples systems represent state art defend idea develop parallel bilingual translation systems train relatively small corpora base observation standard human professional translator estimate corpora compose maximum monolingual sub corpus seventy-five million examples source language second monolingual sub corpus six million examples target language align bilingual sub corpus six million bi examples less desirable alternative would align bilingual corpus four hundred and seventy-five million bi examples
despite recent success deep neural network natural language process extent demonstrate human like generalization capacities natural language understand remain unclear explore issue domain natural language inference nli focus transitivity inference relations fundamental property systematically draw inferences model capture transitivity compose basic inference pattern draw new inferences introduce analysis method use synthetic naturalistic nli datasets involve clause embed verbs evaluate whether model perform transitivity inferences compose veridical inferences arbitrary inference type find current nli model perform consistently well transitivity inference task suggest lack generalization capacity draw composite inferences provide train examples data code analysis publicly available https githubcom verypluming transitivity
semi supervise learn deep generative model multi lingual pretraining techniques orchestrate tremendous success across different areas nlp nonetheless development happen isolation combination could potentially effective tackle task specific label data shortage bridge gap combine semi supervise deep generative model multi lingual pretraining form pipeline document classification task compare strong supervise learn baselines semi supervise classification framework highly competitive outperform state art counterparts low resource settings across several languages
spark nlp natural language process nlp library build top apache spark ml provide simple performant accurate nlp annotations machine learn pipelines scale easily distribute environment spark nlp come one thousand, one hundred pre train pipelines model one hundred and ninety-two languages support nearly nlp task modules use seamlessly cluster download twenty-seven million time experience nine time growth since january two thousand and twenty spark nlp use fifty-four healthcare organizations worlds widely use nlp library enterprise
since popularization transformer general purpose feature encoder nlp many study attempt decode linguistic structure novel multi head attention mechanism however much work focus almost exclusively english language rigid word order lack inflectional morphology study present decode experiment multilingual bert across eighteen languages order test generalizability claim dependency syntax reflect attention pattern show full tree decode baseline accuracy single attention head individual relations often track head across languages furthermore attempt address recent debate status attention explanatory mechanism experiment fine tune mbert supervise parse objective freeze different series parameters interestingly steer objective learn explicit linguistic structure find much structure represent result attention pattern interest differences respect parameters freeze
detect arguments online interactions useful understand conflict arise get resolve users often use figurative language sarcasm either persuasive devices attack opponent ad hominem argument understand role sarcasm shape disagreement space present thorough experimental setup use corpus annotate argumentative move agree disagree sarcasm exploit joint model term apply discrete feature useful detect sarcasm task argumentative relation classification agree disagree none b multitask learn argumentative relation classification sarcasm detection use deep learn architectures eg dual long short term memory lstm hierarchical attention transformer base architectures demonstrate model sarcasm improve argumentative relation classification task agree disagree none setups
document level machine translation condition surround sentence produce coherent translations much recent work area introduction custom model architectures decode algorithms paper present systematic comparison select approach literature two benchmarks document level phenomena evaluation suit exist find simple method base purely back translate monolingual document level data perform well much elaborate alternatives term document level metrics well human evaluation
investigate multilingual bert mbert encode grammar examine high order grammatical feature morphosyntactic alignment different languages define count subject manifest across embed space different languages understand morphosyntactic alignment affect contextual embed space train classifiers recover subjecthood mbert embeddings transitive sentence contain overt information morphosyntactic alignment evaluate zero shoot intransitive sentence subjecthood classification depend alignment within across languages find result classifier distributions reflect morphosyntactic alignment train languages result demonstrate mbert representations influence high level grammatical feature manifest one input sentence robust across languages examine characteristics classifiers rely find feature passive voice animacy case strongly correlate classification decisions suggest mbert encode subjecthood purely syntactically subjecthood embed continuous dependent semantic discourse factor propose much functional linguistics literature together result provide insight grammatical feature manifest contextual embed space level abstraction cover previous work
multilingual pretrained language model demonstrate remarkable zero shoot cross lingual transfer capabilities transfer emerge fine tune task interest one language evaluate distinct language see fine tune despite promise result still lack proper understand source transfer use novel layer ablation technique analyse model internal representations show multilingual bert popular multilingual language model view stack two sub network multilingual encoder follow task specific language agnostic predictor encoder crucial cross lingual transfer remain mostly unchanged fine tune task predictor little importance transfer reinitialized fine tune present extensive experiment three distinct task seventeen typologically diverse languages multiple domains support hypothesis
propose novel approach cross lingual name entity recognition ner zero shoot transfer use parallel corpora build entity alignment model top xlm roberta project entities detect english part parallel data target language sentence whose accuracy surpass previous unsupervised model alignment model get pseudo label ner data set target language train task specific model unlike use translation methods approach benefit natural fluency nuances target language original corpus also propose modify loss function similar focal loss assign weight opposite direction improve model train noisy pseudo label data set evaluate propose approach four target languages benchmark data set get competitive f1 score compare recent sota model also give extra discussions impact parallel corpus size domain final transfer performance
work propose two stage method name entity recognition ner especially nest ner borrow idea two stage object detection computer vision way construct loss function first region proposal network generate region candidates second stage model discriminate classify entity make final prediction also design special loss function second stage train predict entityness entity type time model build top pretrained bert encoders try bert base bert large model experiment first apply flat ner task conll2003 ontonotes fifty get comparable result traditional ner model use sequence label methodology test model nest name entity recognition task ace2005 genia get f1 score eight hundred and fifty-six seven hundred and sixty-eight respectively term second stage train find add extra randomly select regions play important role improve precision also error profile better evaluate performance model different circumstances potential improvements future
linguistically inform analyse language model lms contribute understand improvement model introduce corpus chinese linguistic minimal pair climp use investigate knowledge chinese lms acquire climp consist set one thousand minimal pair mps sixteen syntactic contrast mandarin cover nine major mandarin linguistic phenomena mps semi automatically generate human agreement label climp nine hundred and fifty-eight evaluate eleven different lms climp cover n grams lstms chinese bert find classifier noun agreement verb complement selection phenomena model generally perform best however model struggle ba construction bind filler gap dependencies overall chinese bert achieve eight hundred and eighteen average accuracy performances lstms five grams moderately chance level
sentence order aim arrange list sentence correct order base observation sentence order different distance may rely different type information devise new approach base multi granular order sentence order form multiple constraint graph encode graph isomorphism network fuse sentence representations finally sentence order determine use order enhance sentence representations experiment five benchmark datasets show method outperform exist baselines significantly achieve new state art performance result demonstrate advantage consider multiple type order information use graph neural network integrate sentence content order information task code available https githubcom daod constraintgraph4nso
fine grain name entity type fg net aim classify entity mention wide range entity type usually hundreds depend upon context distant supervision common way acquire supervise train data bring label noise assign type label entity mention irrespective mention context attempt deal label noise lead research fg net assume fine grain entity type data possess euclidean nature restraints ability exist model combat label noise give fact fine grain type hierarchy exhibit hierarchal structure make hyperbolic space natural choice model fg net data research propose fgnet hr novel framework benefit hyperbolic geometry combination graph structure perform entity type performance enhance fashion fgnet hr initially use lstm network encode mention relation context later form graph distill refine mention encode hyperbolic space finally refine mention encode use entity type experimentation use different benchmark datasets show fgnet hr improve performance fg net thirty-five term strict accuracy
cross lingual transfer lead technique parse low resource languages absence explicit supervision simple direct transfer learn model base multilingual input encode provide strong benchmark paper present method unsupervised cross lingual transfer improve direct transfer systems use output implicit supervision part self train unlabelled text target language method assume minimal resources provide maximal flexibility accept pre train arc factor dependency parser b assume access source language data c support projective non projective parse support multi source transfer english source language show significant improvements state art transfer model distant nearby languages despite conceptually simpler approach provide analyse choice source languages multi source transfer advantage non projective parse code available online
manual evaluation essential judge progress automatic text summarization however conduct survey recent summarization system paper reveal little agreement perform evaluation study conduct two evaluation experiment two aspects summaries linguistic quality coherence repetitiveness compare likert type rank annotations show best choice evaluation method vary one aspect another survey also find study parameters overall number annotators distribution annotators annotation items often fully report subsequent statistical analysis ignore group factor arise one annotator judge multiple summaries use evaluation experiment show total number annotators strong impact study power current statistical analysis methods inflate type error rat eight fold addition highlight purpose system comparison current practice elicit multiple judgements per summary lead less powerful reliable annotations give fix study budget
great majority languages world consider resourced successful application deep learn methods work propose meta learn approach document classification limit resource set demonstrate effectiveness two different settings shoot cross lingual adaptation previously unseen languages multilingual joint train limit target language data available train conduct systematic comparison several meta learn methods investigate multiple settings term data availability show meta learn thrive settings heterogeneous task distribution propose simple yet effective adjustment exist meta learn methods allow better stable learn set new state art several languages perform par others use small amount label data
non native speakers show difficulties speak word process many study attribute difficulties imprecise phonological encode word lexical memory test alternative hypothesis difficulties arise non native speakers phonetic perception train computational model phonetic learn access phonology either one two languages first show model exhibit predictable behaviors phone level word level discrimination task test model speak word process task show phonology may necessary explain word process effect observe non native speakers run additional analysis model lexical representation space show two train languages fully separate space similarly languages bilingual human speaker
rapid development data drive task orient dialogue systems benefit large scale datasets however progress dialogue systems low resource languages lag far behind due lack high quality data advance cross lingual technology build dialog systems dstc9 introduce task cross lingual dialog state track test dst module low resource language give rich resource train dataset paper study transferability cross lingual generative dialogue state track system use multilingual pre train seq2seq model experiment different settings include joint train pre train cross lingual cross ontology datasets also find low cross lingual transferability approach provide investigation discussion
availability large amount computer readable textual data hardware process data shift focus knowledge project towards deep learn architecture natural language process particularly task name entity recognition exception bulk learn methods produce state art result change deep learn model train method use train data encode output ner system paper review significant learn methods employ ner recent past come linear learn methods past also cover progress relate task upstream downstream ner eg sequence tag entity link etc wherever process question also improve ner result
propose new name entity recognition ner method effectively make use result part speech pos tag chinese word segmentation cws parse avoid ner error cause pos tag error paper first use stanford natural language process nlp tool annotate large scale untagged data reduce dependence tag data new nlp model g bert model design compress bidirectional encoder representations transformers bert model order reduce calculation quantity finally model evaluate base chinese ner dataset experimental result show calculation quantity g bert model reduce sixty performance improve two test f1 nine hundred and sixty-five compare bert model
area geographic information process research geographic text classification however application task chinese relatively rare work intend implement method extract text contain geographical entities large number network text geographic information texts great practical significance transportation urban rural plan disaster relief field use method graph convolutional neural network attention mechanism achieve function graph attention network improvement graph convolutional neural network compare gcn advantage gat attention mechanism propose weight sum characteristics adjacent nod addition construct chinese dataset contain geographical classification multiple datasets chinese text classification macro f score geogat use reach ninety-five new chinese dataset
ease access information rapid dissemination internet velocity volume become challenge filter truthful information fake ones research community face task automatic detection fake news carry real world socio political impact one research contribution come form constraintaaa12021 share task covid19 fake news detection english paper would light novel method propose part share task team introduce approach combine topical distributions latent dirichlet allocation lda contextualized representations xlnet also compare method exist baselines show xlnet topic distributions outperform approach attain f1 score nine hundred and sixty-seven
adaptation pretrained language model solve supervise task become baseline nlp many recent work focus study linguistic information encode pretrained sentence representations among information show entire syntax tree implicitly embed geometry model model often fine tune become increasingly important understand encode knowledge evolve along fine tune paper analyze evolution embed syntax tree along fine tune process bert six different task cover level linguistic structure experimental result show encode syntactic information forget pos tag reinforce dependency constituency parse preserve semantics relate task different ways along fine tune process depend task
pronunciation model key task build speech technology new languages solid grapheme phoneme g2p map systems exist language coverage stand improve information need build g2p model many languages easily find wikipedia unfortunately store disparate format report system build mine pronunciation data set eight hundred and nineteen languages loosely structure table within wikipedia data include phoneme inventory sixty-three low resource languages also include grapheme phoneme g2p map fifty-four languages easily findable g2p mappings online otherwise turn information wikipedia structure machine readable tsv format make result data set publicly available improve use variety applications involve low resource languages
interactive speech recognition systems must generate word quickly also produce accurate result two pass model excel requirements employ first pass decoder quickly emit word second pass decoder require context accurate previous work establish deliberation network effective second pass model model attend two kinds input encode audio frame hypothesis text first pass model work explore use transformer layer instead long short term memory lstm layer deliberation rescoring transformer layer generalize encoder decoder attention attend encode audio first pass text hypotheses output context vectors combine merger layer compare lstm base deliberation best transformer deliberation achieve seven relative word error rate improvements along thirty-eight reduction computation also compare non deliberation transformer rescoring find nine relative improvement
propose architecture process use iterate learn model ilm artificial neural network show ilm lead clear compositionality observe use dcgs lead modest improvement compositionality measure holdout accuracy topologic similarity show ilm lead anti correlation holdout accuracy topologic rho demonstrate ilm increase compositionality use non symbolic high dimensional image input
neural module network nmns quite successful incorporate explicit reason learnable modules various question answer task include generic form numerical reason text machine read comprehension mrc however achieve contemporary nmns need strong supervision execute query specialize program reason modules fail generalize open end settings without supervision hence propose weakly supervise neuro symbolic module network wnsmn train answer sole supervision numerical reason base mrc learn execute noisy heuristic program obtain dependency parse query discrete action neural symbolic reason modules train end end reinforcement learn framework discrete reward answer match numerical answer subset drop wnsmn perform nmn thirty-two reason free language model genbert eight exact match accuracy train comparable weak supervise settings showcases effectiveness generalizability modular network handle explicit discrete reason noisy program end end manner
conceptualization claim lie core argument mine segregation claim complex owe divergence textual syntax context across different distributions another press issue unavailability label unstructured text experimentation paper propose lesa framework aim advance headfirst expunge former issue assemble source independent generalize model capture syntactic feature part speech dependency embeddings well contextual feature fine tune language model resolve latter issue annotate twitter dataset aim provide test grind large unstructured dataset experimental result show lesa improve upon state art performance across six benchmark claim datasets average three claim f1 point domain experiment two claim f1 point general domain experiment dataset lesa outperform exist baselines one claim f1 point domain experiment two claim f1 point general domain experiment also release comprehensive data annotation guidelines compile annotation phase miss current literature
exist approach dialogue state track dst rely turn level dialogue state annotations expensive acquire large scale call center task like manage book subscriptions user goal associate action egapi call issue customer service agents action log available large volumes utilize learn dialogue state however unlike turn level annotations log action available sparsely across dialogue provide form weak supervision dst model efficiently learn dst sparse label extend state art encoder decoder model model learn slot aware representation dialogue history focus relevant turn guide decoder present result two public multi domain dst datasets multiwoz schema guide dialogue settings ie train turn level sparse supervision propose approach improve baseline settings importantly model train sparse supervision competitive performance fully supervise baselines data cost efficient
standard model syntactic dependency parse take word elementary units enter dependency relations paper investigate whether benefit enrich model abstract notion nucleus propose tesniere show concept nucleus define framework universal dependencies use composition function make transition base dependency parser aware concept experiment twelve languages show nucleus composition give small significant improvements parse accuracy analysis reveal improvement mainly concern small number dependency relations include nominal modifiers relations coordination main predicate direct object
popular social media network provide perfect environment study opinions attitudes express users interactions social media twitter occur many natural languages research stance detection position attitude express respect specific topic within natural language process field largely do english although efforts recently make develop annotate data languages tell lack resources facilitate multilingual crosslingual research stance detection partially due fact manually annotate corpus social media texts difficult slow costly process furthermore stance highly domain topic specific phenomenon need annotate data specially demand result manually label resources hinder relatively small size skew class distribution paper present method obtain multilingual datasets stance detection twitter instead manually annotate per tweet basis leverage user base information semi automatically label large amount tweet empirical monolingual cross lingual experimentation qualitative analysis show method help overcome aforementioned difficulties build large balance multilingual label corpora believe method easily adapt easily generate label social media data natural language process task domains
rapid outbreak covid nineteen cause humanity come stand still bring plethora problems covid nineteen first pandemic history humanity technologically advance rely heavily social media platforms connectivity benefit unfortunately fake news misinformation regard virus also available people cause massive problems fight infodemic become significant challenge present solution constraintaaai2021 covid19 fake news detection english challenge work extensive experimentation numerous architectures techniques use eight different transformer base pre train model additional layer construct stack ensemble classifier fine tune purpose achieve nine hundred and seventy-nine million, nine hundred and six thousand, five hundred and forty-two accuracy nine hundred and seventy-nine million, nine hundred and thirteen thousand, one hundred and nineteen precision nine hundred and seventy-nine million, nine hundred and six thousand, five hundred and forty-two recall nine hundred and seventy-nine million, nine hundred and seven thousand, nine hundred and one f1 score test dataset competition
propose novel hybrid approach lemmatization enhance seq2seq neural model additional lemmas extract external lexicon rule base system train enhance lemmatizer learn generate lemmas via sequential decoder copy lemma character external candidates supply run time lemmatizer enhance candidates extract apertium morphological analyzer achieve statistically significant improvements compare baseline model utilize additional lemma information achieve average accuracy nine thousand, seven hundred and twenty-five set twenty-three ud languages fifty-five higher obtain stanford stanza model set languages also compare methods integrate external data lemmatization show enhance system perform considerably better simple lexicon extension method base stanza system achieve complementary improvements wrt data augmentation method
modern classification model tend struggle amount annotate data scarce overcome issue several neural shoot classification model emerge yield significant progress time computer vision natural language process latter model use rely fix word embeddings advent transformers additionally model use computer vision yet test nlp applications paper compare model first adapt make field image process nlp second provide access transformers test model equip transformer base encoder intent detection task know large number class result reveal methods perform almost equally arsc dataset case intent detection task recent supposedly best competitors perform worse older simpler ones give access transformers also show simple baseline surprisingly strong new develop model well evaluation framework make publicly available
answer sentence selection as2 efficient approach design open domain question answer qa systems order achieve low latency traditional as2 model score question answer pair individually ignore information document potential answer extract contrast computationally expensive model design machine read comprehension task typically receive one passages input often result better accuracy work present approach efficiently incorporate contextual information as2 model answer candidate first use unsupervised similarity techniques extract relevant sentence source document fee efficient transformer architecture fine tune as2 best approach leverage multi way attention architecture efficiently encode context improve six eleven noncontextual state art as2 minimal impact system latency experiment work conduct english
present lome system perform multilingual information extraction give text document input core system identify span textual entity event mention framenet baker et al one thousand, nine hundred and ninety-eight parser subsequently perform coreference resolution fine grain entity type temporal relation prediction events system construct event entity focus knowledge graph apply third party modules type annotation like relation extraction multilingual first party modules either outperform competitive monolingual state art achieve use multilingual encoders like xlm conneau et al two thousand and twenty leverage multilingual train data lome available docker container docker hub addition lightweight version system accessible web demo
recent years transformer base language model achieve state art performance various nlp benchmarks model able extract mostly distributional information semantics unstructured text however prove challenge integrate structure information knowledge graph model examine variety approach integrate structure knowledge current language model determine challenge possible opportunities leverage structure unstructured information source survey find still opportunities exploit adapter base injections may possible combine various explore approach one system
exist grammatical error correction gec methods base sequence sequence mainly focus generate pseudo data obtain better performance work address shoot gec domain adaptation paper treat different gec domains different gec task propose extend meta learn shoot gec domain adaptation without use pseudo data exploit set data rich source domains learn initialization model parameters facilitate fast adaptation new resource poor target domains adapt gec model first language l1 second language learner evaluate propose method use nine l1s source domains five l1s target domains experiment result l1 gec domain adaptation dataset demonstrate propose approach outperform multi task transfer learn baseline fifty f05 score average enable us effectively adapt new l1 domain two hundred parallel sentence
neural machine translation nmt monolingual data target language usually exploit method call back translation synthesize additional train parallel data synthetic data show helpful train better nmt especially low resource language pair domains nonetheless large monolingual data target domains languages always available generate large synthetic parallel data work propose new method generate large synthetic parallel data leverage small monolingual data specific domain fine tune pre train gpt two model small domain monolingual data use result model generate large amount synthetic domain monolingual data perform back translation forward translation generate synthetic domain parallel data preliminary experiment three language pair five domains show effectiveness method generate fully synthetic useful domain parallel data improve nmt configurations also show promise result extreme adaptation personalize nmt
cross document co reference resolution cdcr task identify link mention entities concepts across many text document current state art model task assume document type eg news article fall theme however also desirable perform cdcr across different domains type theme particular use case focus paper resolution entities mention across scientific work newspaper article discuss identify entities correspond concepts scientific article news help scientists understand work represent mainstream media propose new task english language dataset cross document cross domain co reference resolution cd2cr task aim identify link entities across heterogeneous document type show cross domain cross document set exist cdcr model perform well provide baseline model outperform current state art cdcr model cd2cr data set annotation tool guidelines well model cross document cross domain co reference supply open access open source resources
bias associations challenge development classifiers detect toxic language hinder fairness accuracy potential solutions investigate recently introduce debiasing methods text classification datasets model apply toxic language detection focus lexical eg swear word slur identity mention dialectal markers specifically african american english comprehensive experiment establish exist methods limit ability prevent bias behavior current toxicity detectors propose automatic dialect aware data correction method proof concept despite use synthetic label method reduce dialectal associations toxicity overall find show debiasing model train bias toxic language data effective simply relabeling data remove exist bias
rapid development science technology accompany exponential growth peer review scientific publications time review paper laborious process must carry subject matter experts thus provide high quality review grow number paper significant challenge work ask question automate scientific review discuss possibility use state art natural language process nlp model generate first pass peer review scientific paper arguably difficult part define good review first place first discuss possible evaluation measure review collect dataset paper machine learn domain annotate different aspects content cover review train target summarization model take paper generate review comprehensive experimental result show system generate review tend touch upon aspects paper human write review generate text suffer lower constructiveness aspects except explanation core ideas paper largely factually correct finally summarize eight challenge pursuit good review generation system together potential solutions hopefully inspire future research subject make code dataset publicly available https githubcom neulab reviewadvisor well reviewadvisor system http reviewnlpediaai
industry nlp application manually label data certain number noisy data present simple method find noisy data relabel manually meanwhile collect correction information present novel method incorporate human correction information deep learn model human know correct noisy data correction information inject deep learn model experiment text classification dataset manually label relabel noisy data dataset industry application experiment result show method improve classification accuracy nine hundred and seventeen nine hundred and twenty-five nine hundred and seventeen baseline base bert train correct dataset hard surpass
fine grain sentiment analysis attempt extract sentiment holders target polar expressions resolve relationship progress hamper difficulty annotation target sentiment analysis hand narrow task focus extract sentiment target classify polarityin paper explore whether incorporate holder expression information improve target extraction classification perform experiment eight english datasets conclude jointly predict target polarity bio label improve target extraction augment input text gold expressions generally improve target polarity classification highlight potential importance annotate expressions fine grain sentiment datasets time result show performance current model predict polar expressions poor hamper benefit information practice
extractive summarization suffer irrelevance redundancy incoherence exist work show abstractive rewrite extractive summaries improve conciseness readability rewrite systems consider extract summaries input relatively focus lose important background knowledge paper investigate contextualized rewrite ingest entire original document formalize contextualized rewrite seq2seq problem group alignments introduce group tag solution model alignments identify extract summaries content base address result show approach significantly outperform non contextualized rewrite systems without require reinforcement learn achieve strong improvements rouge score upon multiple extractive summarizers
bnlp open source language process toolkit bengali language consist tokenization word embed pos tag ner tag facilities bnlp provide pre train model high accuracy model base tokenization embed pos tag ner tag task bengali language bnlp pre train model achieve significant result bengali text tokenization word embed pos tag ner tag task bnlp use widely bengali research communities 16k download one hundred and nineteen star thirty-one fork bnlp available https githubcom sagorbrur bnlp
diversity patent language grow make find synonyms conduct patent search challenge addition approach deal diverse patent language base manual search human intuition paper word embed approach use statistical analysis human label data produce accurate language independent word vectors technical term introduce paper focus explanation idea behind statistical analysis show first qualitative result result algorithm development former eqmania ug eqmaniacom test eqalicecom april two thousand and twenty-one
keyword extraction task identify word multi word expressions best describe give document serve news portals link article similar topics work develop evaluate methods four novel data set cover less represent morphologically rich languages european news media industry croatian estonian latvian russian first perform evaluation two supervise neural transformer base methods tnt kid bertbilstm crf compare baseline tf idf base unsupervised approach next show combine keywords retrieve neural transformer base methods extend final set keywords unsupervised tf idf base technique drastically improve recall system make appropriate use recommendation system media house environment
recent techniques task short text cluster often rely word embeddings transfer learn component paper show sentence vector representations transformers conjunction different cluster methods successfully apply address task furthermore demonstrate algorithm enhancement cluster via iterative classification improve initial cluster performance different classifiers include base pre train transformer language model
optical character recognition ocr crucial deeper access historical collections ocr need account orthographic variations typefaces language evolution ie new letter word spell main source character word word segmentation transcription errors digital corpora historical print errors exacerbate due low scan quality lack language standardization task ocr post hoc correction propose neural approach base combination recurrent rnn deep convolutional network convnet correct ocr transcription errors character level flexibly capture errors decode correct output base novel attention mechanism account input output similarity propose new loss function reward model correct behavior evaluation historical book corpus german language show model robust capture diverse ocr transcription errors reduce word error rate three hundred and twenty-three eighty-nine
minority languages continue lack adequate resources development especially technological domain likewise jp harrington paper collection smithsonian institution difficult access practical term community members researchers due handwritten disorganize format current work seek make portion publicly available yet problematic material practically accessible natural language process use present harrington yowlumne narrative corpus corpus twenty narrative texts derive tejoneno yowlumne community tinliw rancheria kern county california one thousand, nine hundred and ten one thousand, nine hundred and twenty-five digitally transcribe texts provide gold standard align lexeme base normalize text texts altogether text contain sixty-seven thousand, eight hundred and thirty-five transcribe character align ten thousand, seven hundred and twenty-one gold standard text normalize word
answer selection task choose positive answer pool candidate answer give question paper propose novel strategy answer selection call hierarchical rank introduce three level rank point level rank pair level rank list level rank formulate optimization objectives employ supervisory information different perspectives achieve goal rank candidate answer therefore three level rank relate promote take well perform compare aggregate model backbone explore three scheme implement idea apply hierarchical rank jointly scheme multi task learn mtl strategy rank integration ri scheme progressive rank integration pri scheme experimental result two public datasets wikiqa trec qa demonstrate propose hierarchical rank effective method achieve state art non bert performance trec qa wikiqa
numerical table widely use present experimental result scientific paper table understand metric type essential discriminate number table introduce new information extraction task metric type identification multi level header numerical table provide dataset extract scientific paper consist header table caption metric type propose two joint learn neural classification generation scheme feature pointer generator base bert base model result show joint model handle header header metric type identification problems
era digitization different actors agriculture produce numerous data data contain already latent historical knowledge domain knowledge enable us precisely study natural hazard within global local aspects improve risk prevention task augment yield help tackle challenge grow population change alimentary habit particular french plant health bulletin bsv name french bulletin de sant e du v eg etal give information development stag phytosanitary risk agricultural production however write natural language thus machine human exploit efficiently could natural language process nlp technologies aim automatically process analyze large amount natural language data since 2010s increase computational power parallelization representation learn deep learn methods become widespread nlp recent advancements bidirectional encoder representations transformers bert inspire us rethink knowledge representation natural language understand plant health management domain goal work propose bert base approach automatically classify bsv make data easily indexable sample two hundred bsv finetune pretrained bert language model classify pest disease show preliminary result
paper introduce 2nd place solution riiid answer correctness prediction kaggle world largest data science competition website competition hold october sixteen two thousand and twenty january seven two thousand and twenty-one three thousand, three hundred and ninety-five team four thousand, three hundred and eighty-seven competitors main insights contributions paper follow point exist transformer base model suffer problem information query key value contain limit solve problem propose method use lstm obtain query key value verify effectiveness ii point inter container leakage problem happen datasets question sometimes serve together solve problem show special index mask techniques useful use rnn variants transformer iii find additional hand craft feature effective overcome limit transformer never consider sample older sequence length
learn idiomatic expressions see one challenge stag second language learn unpredictable mean similar situation hold identification within natural language process applications machine translation parse lack high quality usage sample exacerbate challenge humans also artificial intelligence systems article introduce gamified crowdsourcing approach collect language learn materials idiomatic expressions message bot design asynchronous multiplayer game native speakers compete provide idiomatic nonidiomatic usage examples rat players entries oppose classical crowdprocessing annotation efforts field first time literature crowdcreating crowdrating approach implement test idiom corpora construction approach language independent evaluate two languages comparison traditional data preparation techniques field reaction crowd monitor different motivational mean namely gamification affordances monetary reward result reveal propose approach powerful collect target materials although explicit crowdsourcing approach find entertain useful crowd approach show potential speed construction idiom corpora different natural languages use second language learn material train data supervise idiom identification systems sample lexicographic study
recently find monolingual english language model use knowledge base instead structural knowledge base query mask sentence paris capital mask use probe translate establish benchmarks trex googlere fifty-three languages work mbert investigate three question mbert use multilingual knowledge base prior work consider english extend research multiple languages important diversity accessibility ii mbert performance knowledge base language independent vary language language iii multilingual model train text eg mbert train one hundred and four wikipedias mbert leverage better performance find use mbert knowledge base yield vary performance across languages pool predictions across languages improve performance conversely mbert exhibit language bias eg query italian tend predict italy country origin
january two thousand and seventeen january two thousand and twenty-one thousands local news source unite state report forty-two thousand protest topics civil right immigration gun environment give vast number local journalists report protest daily extract events structure data understand temporal geographic trend empower civic decision make however task extract events news article present well know challenge nlp community field domain detection slot fill coreference resolution help improve resources available extract structure data news stories contribution three fold one release manually label dataset news article urls date locations crowd size estimate four hundred and ninety-four discrete descriptive tag correspond forty-two thousand, three hundred and forty-seven report protest events unite state january two thousand and seventeen january two thousand and twenty-one two describe semi automate data collection pipeline use discover sort review one hundred and forty-four thousand, five hundred and sixty-eight english article comprise dataset three benchmark long short term memory lstm low dimensional classifier demonstrate utility process news article base syntactic structure paragraph sentence count number report protest events
consistency model invariance behavior mean preserve alternations input highly desirable property natural language process paper study question pretrained language model plms consistent respect factual knowledge end create pararel high quality resource cloze style query english paraphrase contain total three hundred and twenty-eight paraphrase thirty-eight relations use pararel show consistency plms experiment poor though high variance relations analysis representational space plms suggest poor structure currently suitable represent knowledge robustly finally propose method improve model consistency experimentally demonstrate effectiveness
paper present submission eacl two thousand and twenty-one share task offensive language identification dravidian languages final system ensemble mbert xlm roberta model leverage task adaptive pre train multilingual bert model mask language model objective system rank 1st kannada 2nd malayalam 3rd tamil
datasets resources train accurate deployable systems also benchmarks develop new model approach large natural datasets necessary train accurate systems necessary drive model innovation example popular squad question answer benchmark drive development new model approach could synthetic smaller benchmarks lead similar innovations counterfactual question impossible answer study necessary condition ability benchmark recapitulate find make squad conduct retrospective study twenty squad model approach investigate well thirty-two exist synthesize benchmarks concur squad ie rank approach similarly carefully construct small target synthetic benchmarks resemble natural language yet high concurrence squad demonstrate naturalness size necessary reflect historical model improvements squad result raise intrigue possibility small carefully design synthetic benchmarks may useful drive development new model approach
generative speak language model involve learn jointly acoustic linguistic characteristics language raw audio without text label introduce metrics automatically evaluate generate output term acoustic linguistic quality two associate end end task respectively speech resynthesis repeat speech input use system voice speech generation produce novel speech output conditional speak prompt unconditionally validate metrics human judgment test baseline systems consist discrete speech encoder return discrete low bitrate pseudo text units generative language model train pseudo text units speech decoder generate waveform pseudo text compare three state art unsupervised speech encoders contrastive predictive cod cpc wav2vec twenty hubert vary number discrete units fifty one hundred two hundred investigate generative performance depend quality learn units measure unsupervised metrics zero shoot probe task open source evaluation stack baseline model
spite much recent research area still unclear whether subject area question answer data useful machine read comprehension mrc task paper investigate question collect large scale multi subject multiple choice question answer dataset examqa use incomplete noisy snippets return web search engine relevant context question answer instance convert weakly label mrc instance propose self teach paradigm better use generate weakly label mrc instance improve target mrc task experimental result show obtain fifty-one accuracy multiple choice mrc dataset c3 thirty-eight exact match extractive mrc dataset cmrc two thousand and eighteen state art mrc baselines demonstrate effectiveness framework usefulness large scale subject area question answer data different type machine read comprehension task
study conversational dialog many possible responses give history present multitalk dataset corpus three hundred and twenty thousand sentence write conversational dialog balance high branch factor ten several conversation turn six selective branch continuation make multiple contributions study dialog generation highly branch set order evaluate diverse set generations propose simple score algorithm base bipartite graph match optimally incorporate set diverse reference study multiple language generation task different level predictive conversation depth use textual attribute induce automatically pretrained classifiers culminate task challenge theory mind problem controllable generation task require reason expect reaction listener
sentence level relation extraction aim identify relationship two entities sentence many efforts devote problem best perform methods still far perfect paper revisit two problems affect performance exist model namely entity representation noisy ill define label improve baseline model incorporate entity representations type markers achieve f1 seven hundred and forty-six tacred significantly outperform previous sota methods furthermore present new baseline achieve f1 nine hundred and eleven refine tacred dataset demonstrate pre train language model achieve unexpectedly high performance task release code community future research
present demonstration two machine translation applications historical document first task consist generate new version historical document write modern version original language second application limit document orthography adapt document spell modern standards order achieve orthography consistency account lack spell conventions follow interactive adaptive framework allow user introduce corrections system hypothesis system react corrections generate new hypothesis take account user satisfy system hypothesis validate system adapt model follow online learn strategy system implement follow client server architecture develop website communicate neural model code open source publicly available demonstration host http demosmtprhltupves mthd
despite major advance open end text generation limit progress design evaluation metrics task propose mauve metric open end text generation directly compare distribution machine generate text human language mauve measure mean area divergence curve two distributions explore trade two type errors arise part human distribution model distribution approximate well present experiment across two open end generation task web text domain story domain variety decode algorithms model size result show evaluation mauve indeed reflect natural behavior respect model size compare prior metrics mauve order decode algorithms also agree generation perplexity widely use metric open end text generation however mauve present principled evaluation metric task consider model human text
click count relate amount money online advertisers pay news sit business model force news sit employ dirty trick click bait ie use hyperbolic interest word sometimes unfinished sentence headline purposefully tease readers indonesian online news sit also join party clickbait indirectly degrade establish news sit credibility neural network pre train language model bert act embed layer combine one hundred nod hide layer top sigmoid classifier train detect clickbait headline total six thousand, six hundred and thirty-two headline train dataset classifier perform remarkably well evaluate five fold cross validation accuracy score nine hundred and fourteen f1 score nine hundred and fourteen precision score nine hundred and sixteen roc auc ninety-two usage multilingual bert indonesian text classification task test possible enhance future possibilities societal impact limitations clickbait detection discuss
previous study demonstrate dynamic phone inform compression input audio beneficial speech translation st however require dedicate model phone recognition test solution direct st single model translate input audio target language without intermediate representations work propose first method able perform dynamic compression input indirect st model particular exploit connectionist temporal classification ctc compress input sequence accord phonetic characteristics experiment demonstrate solution bring thirteen fifteen bleu improvement strong baseline two language pair english italian english german contextually reduce memory footprint ten
present multilingual tedx corpus build support speech recognition asr speech translation st research across many non english source languages corpus collection audio record tedx talk eight source languages segment transcripts sentence align source language audio target language translations corpus release along open source code enable extension new talk languages become available corpus creation methodology apply languages previous work create multi way parallel evaluation set provide baselines multiple asr st settings include multilingual model improve translation performance low resource language pair
slang common type informal language flexible nature paucity data resources present challenge exist natural language systems take initial step toward machine generation slang develop framework model speaker word choice slang context framework encode novel slang mean relate conventional slang sense word incorporate syntactic contextual knowledge slang usage construct framework use combination probabilistic inference neural contrastive learn perform rigorous evaluations three slang dictionaries show approach outperform state art language model also better predict historical emergence slang word usages 1960s 2000s interpret propose model find contrastively learn semantic space sensitive similarities slang conventional sense word work create opportunities automate generation interpretation informal language
text sql crucial task toward develop methods understand natural language computers recent neural approach deliver excellent performance however model difficult interpret inhibit future developments hence study aim provide better approach toward interpretation neural model hypothesize internal behavior model hand become much easier analyze identify detail performance schema link simultaneously additional information text sql performance provide grind truth annotation schema link information onto spider dataset demonstrate usefulness annotate data analyze current state art neural model
paper introduce hebert hebemo hebert transformer base model modern hebrew text rely bert bidirectional encoder representations transformers architecture bert show outperform alternative architectures sentiment analysis suggest particularly appropriate mrls analyze multiple bert specifications find model complexity correlate high performance language task aim understand term sentence parsimonious model better capture sentiment entire sentence either way bert base language model outperform exist hebrew alternatives common language task hebemo tool use hebert detect polarity extract emotions hebrew ugc hebemo train unique covid nineteen relate ugc dataset collect annotate study data collection annotation follow active learn procedure aim maximize predictability show hebemo yield high f1 score ninety-six polarity classification emotion detection reach f1 score seventy-eight ninety-seven various target emotions exception surprise model fail capture f1 forty-one result better best report performance even among english language model emotion detection
introduce top approach discourse parse conceptually simpler predecessors kobayashi et al two thousand and twenty zhang et al two thousand and twenty frame task sequence label problem goal iteratively segment document individual discourse units able eliminate decoder reduce search space split point explore traditional recurrent model modern pre train transformer model task additionally introduce novel dynamic oracle top parse base full metric propose lstm model set new state art rst parse
track one dstc9 aim effectively answer user request question task orient dialogues scope apis db leverage external knowledge resources relevant information retrieve encode response generation api coverage query work explore several advance techniques enhance utilization external knowledge boost quality response generation include schema guide knowledge decision negative enhance knowledge selection knowledge ground response generation evaluate performance propose method comprehensive experiment carry publicly available dataset approach rank best human evaluation dstc9 track one
introduce novel task consist assign proof give mathematical statement task design improve process research level mathematical texts apply natural language process nlp tool research level mathematical article challenge since highly specialize domain mix natural language mathematical formulae also important requirement develop tool mathematical information retrieval computer assist theorem prove release dataset task consist 180k statement proof pair extract mathematical research article carry preliminary experiment assess difficulty task first experiment two bag word baselines show consider assignment problem globally use weight bipartite match algorithms help lot tackle task finally introduce self attention base model train either locally globally outperform baselines wide margin
classification essential fundamental task machine learn play cardinal role field natural language process nlp computer vision cv supervise learn set label always need classification task especially deep neural model large amount high quality label data require train however new domain come usually hard expensive acquire label transfer learn could option transfer knowledge source domain target domain challenge two domains different either feature distribution class distribution nature sample work evaluate exist transfer learn approach detect bias imbalanced class include traditional deep model besides propose approach bridge gap domain class imbalance issue
psycholinguistic study human word process lexical access provide ample evidence prefer nature word initial versus word final segment eg term attention pay listeners greater likelihood reduction speakers lower lead conjecture wedel et al 2019b common elsewhere languages evolve provide information earlier word later information theoretic methods establish tendencies lexicons suffer several methodological shortcomings leave open question whether high word initial informativeness actually property lexicon simply artefact incremental nature recognition paper point confound exist methods compare informativeness segment early word versus later word present several new measure avoid confound control confound still find evidence across hundreds languages indeed cross linguistic tendency front load information word
engage informative conversations users utmost goal open domain conversational systems recent advance transformer base language model applications dialogue systems succeed generate fluent human like responses however still lack control generation process towards produce contentful responses achieve engage conversations achieve goal present textbfdiscol textbfdialogue textbfsystems textbfcoversational textbfline guide response generation discol open domain dialogue system leverage conversational line briefly textbfconvlines controllable informative content plan elements guide generation model produce engage informative responses two primary modules discol pipeline conditional generators train one predict relevant informative convlines dialogue contexts two generate high quality responses condition predict convlines users also change return convlines textitcontrol direction conversations towards topics interest automatic human evaluations demonstrate efficiency convlines produce engage conversations
propose novel task multi document drive dialogue md3 agent guess target document user interest lead dialogue benchmark progress introduce new dataset guessmovie contain sixteen thousand, eight hundred and eighty-one document describe movie associate thirteen thousand, four hundred and thirty-four dialogues propose md3 model keep guess target document mind converse user condition document engagement user feedback order incorporate large scale external document dialogue pretrains document representation sensitive attribute talk object track dialogue state detect evolvement document belief attribute belief finally optimize dialogue policy principle entropy decrease reward increase expect successfully guess user target minimum number turn experiment show method significantly outperform several strong baseline methods close human performance
present language model combine large parametric neural network ie transformer non parametric episodic memory component integrate architecture model use extend short term context cache local hide state similar transformer xl global long term memory retrieve set nearest neighbor tokens timestep design gate function adaptively combine multiple information source make prediction mechanism allow model use either local context short term memory long term memory combination ad hoc basis depend context experiment word base character base language model datasets demonstrate efficacy propose method compare strong baselines
performance natural language generation systems improve substantially modern neural network test time typically employ beam search avoid locally optimal globally suboptimal predictions however due model errors larger beam size lead deteriorate performance accord evaluation metric reason common rerank output beam search rely beam search produce good set hypotheses limit potential gain alternatives beam search require change train model restrict applicability compare beam search paper propose incremental beam manipulation ie reranking hypotheses beam decode instead end way hypotheses unlikely lead good final output discard place hypotheses would ignore consider instead apply incremental beam manipulation lead improvement one hundred and ninety-three five hundred and eighty-two bleu point vanilla beam search test set e2e webnlg challenge respectively propose method also outperform strong reranker one hundred and four bleu point e2e challenge par webnlg dataset
unsupervised word representation learn large corpora badly need downstream task text classification information retrieval machine translation representation precision fasttext language model mostly due use subword information previous work optimization fasttext subword size largely neglect non english fasttext language model train use subword size optimize english german work train english german czech italian fasttext language model wikipedia optimize subword size english german czech italian word analogy task show optimization subword size result five improvement czech word analogy task also show computationally expensive hyperparameter optimization replace cheap n gram frequency analysis subword size closest cover three hundred and seventy-six unique subwords language show optimal fasttext hyperparameters english german czech italian word analogy task
recent approach data text generation adopt successful encoder decoder architecture variants thereof model generate text fluent often imprecise perform quite poorly select appropriate content order coherently overcome issue propose neural model macro plan stage follow generation stage reminiscent traditional methods embrace separate modules plan surface realization macro plan represent high level organization important content entities events interactions learn data give input generator extensive experiment two data text benchmarks rotowire mlb show approach outperform competitive baselines term automatic human evaluation
well establish data collection methods currently adopt nlp depend assumption speaker literacy consequently collect corpora largely fail represent swathe global population tend vulnerable marginalise people society often live rural develop areas underrepresented group thus ignore make model system design decisions also prevent benefit development outcomes achieve data drive nlp paper aim address representation illiterate communities nlp corpora identify potential bias ethical issue might arise collect data rural communities high illiteracy rat low income countries propose set practical mitigation strategies help future work
pre edit process modify source text st translate machine translation mt better quality despite unpredictability black box neural mt nmt pre edit deploy various practical mt use case although many study demonstrate effectiveness pre edit methods particular settings thus far deep understand pre edit work black box nmt lack elicit understand extensively investigate human pre edit practice first implement protocol incrementally record minimum edit st collect six thousand, six hundred and fifty-two instance pre edit across three translation directions two mt systems four text domains analyse instance three perspectives characteristics pre edit st diversity pre edit operations impact pre edit operations nmt output find include follow one enhance explicitness mean st syntactic structure important obtain better translations make st shorter simpler two although impact pre edit nmt generally unpredictable tendencies change nmt output depend edit operation type
story generation task aim automatically produce multiple sentence make meaningful story task challenge require high level understand semantic mean sentence causality story events naive sequence sequence model generally fail acquire knowledge logical correctness hardly guarantee text generation model without strategic plan paper focus plan sequence events assist event graph use events guide generator instead use sequence sequence model output storyline exist work propose generate event sequence walk event graph event graph build automatically base corpus evaluate propose approach conduct human evaluation event plan story generation base large scale human annotation result propose approach show produce logically correct event sequence stories
numerous methods propose defenses adversarial examples question answer qa techniques often model specific require retrain model give marginal improvements performance vanilla model work present simple model agnostic approach problem apply directly qa model without retrain method employ explicit answer candidate reranking mechanism score candidate answer basis content overlap question make final prediction combine strong base qamodel method outperform state art defense techniques call question well techniques actually strong adversarial testbeds
adults understand children speech children productions course language development often bear little resemblance typical adult pronunciations yet caregivers nonetheless reliably recover mean employ suite bayesian model speak word recognition understand adults overcome noisiness child language show communicative success children adults rely heavily adult inferential process evaluate compete model phonetically annotate corpora show adults recover mean best predict prior expectations fit specifically child language environment rather typical adult adult language quantify contribution child direct listen developmental time discuss consequences theories language acquisition well implications commonly use methods assess children linguistic proficiency
recent advancements data text generation largely take form neural end end systems efforts dedicate improve text generation systems change order train sample process know curriculum learn past research sequence sequence learn show curriculum learn help improve performance convergence speed work delve idea surround train sample consist structure data text pair update curriculum framework select train sample base model competence specifically experiment various difficulty metrics put forward soft edit distance metric rank train sample benchmarks show faster convergence speed train time reduce three hundred and eighty-seven performance boost four hundred and eighty-four bleu
many new application domains data text generation main obstacle train neural model consist lack train data usually large number instance available data side often text sample available address problem propose novel shoot approach set approach automatically augment data available train generate new text sample base replace specific value alternative ones category ii generate new text sample base gpt two iii propose automatic method pair new text sample data sample text augmentation introduce noise train data use cycle consistency objective order make sure give data sample correctly reconstruct formulate text text sample reconstruct data e2e webnlg benchmarks show weakly supervise train paradigm able outperform fully supervise seq2seq model less ten annotations utilize annotate data model boost performance standard seq2seq model five bleu point establish new state art datasets
recent progress pretraining language model large corpora result large performance gain many nlp task large model acquire linguistic knowledge pretraining help improve performance downstream task via fine tune assess kind knowledge acquire language model commonly probe query fill blank style cloze question exist probe datasets mainly focus knowledge relations word entities introduce wdlmpro word definition language model probe evaluate word understand directly use dictionary definitions word experiment three popular pretrained language model struggle match word definitions indicate understand many word poorly new probe task difficult challenge could help guide research lms future
ability quantify incivility online news congressional debate great interest political scientists computational tool detect online incivility english fairly accessible potentially could apply broadly test jigsaw perspective api ability detect degree incivility corpus develop consist manual annotations civility american news demonstrate toxicity model exemplify perspective inadequate analysis incivility news carry error analysis point need develop methods remove spurious correlations word often mention news especially identity descriptors incivility without improvements apply perspective similar model news likely lead wrong conclusions align human perception incivility
book aim review present recent advance distribute representation learn nlp include representation learn improve nlp representation learn take part various important topics nlp challenge still well address distribute representation
retrieve information correlative paragraph document answer open domain multi hop question challenge deal challenge exist work consider paragraph nod graph propose graph base methods retrieve however paper point intrinsic defect methods instead propose new architecture model paragraph sequential data consider multi hop information retrieval kind sequence label task specifically design rewritable external memory model dependency among paragraph moreover threshold gate mechanism propose eliminate distraction noise paragraph evaluate method full wiki distractor subtask hotpotqa public textual multi hop qa dataset require multi hop information retrieval experiment show method achieve significant improvement publish state art method retrieval downstream qa task performance
concept unsupervised universal sentence encoders gain traction recently wherein pre train model generate effective task agnostic fix dimensional representations phrase sentence paragraph methods vary complexity simple weight average word vectors complex language model base bidirectional transformers work propose novel technique generate sentence embeddings unsupervised fashion project sentence onto fix dimensional manifold objective preserve local neighbourhoods original space delineate neighbourhoods experiment several set distance metrics include recently propose word mover distance fix dimensional projection achieve employ scalable efficient manifold approximation method root topological data analysis test approach term emap embeddings manifold approximation projection six publicly available text classification datasets vary size complexity empirical result show method consistently perform similar better several alternative state art approach
paper present nlp natural language process approach detect spoilers book review use university california san diego ucsd goodreads spoiler dataset explore use lstm bert roberta language model perform spoiler detection sentence level contrast ucsd paper perform task use handcraft feature data preparation despite eschew use handcraft feature result lstm model able slightly exceed ucsd team performance spoiler detection
quality estimation aim measure quality translate content without access reference translation crucial machine translation systems real world scenarios high quality translation need many approach exist quality estimation base supervise machine learn require costly human label data alternative propose technique rely examples human annotators instead use synthetic train data train shelf architectures supervise quality estimation synthetic data show result model achieve comparable performance model train human annotate data sentence word level prediction
propose novel order chart base model constituent parse compare previous cky style top model model gain advantage order traversal tree rich feature lookahead information high efficiency make better use structural knowledge encode history decisions experiment penn treebank show model outperform previous chart base model achieve competitive performance compare discriminative single model
information security cyber world major concern significant increase number attack surface exist information vulnerabilities attack control advisories available web provide opportunity represent knowledge perform security analytics mitigate concern represent security knowledge form ontology facilitate anomaly detection threat intelligence reason relevance attribution attack many necessitate dynamic automate enrichment information security ontologies however exist ontology enrichment algorithms base natural language process ml model issue contextual extraction concepts word phrase sentence motivate need sequential deep learn architectures traverse dependency paths text extract embed vulnerabilities threats control products security relate concepts instance learn path representations propose approach bidirectional lstms train large dbpedia dataset wikipedia corpus twenty-eight gb along universal sentence encoder deploy enrich iso twenty-seven thousand and one base information security ontology approach yield test accuracy eighty test knock concepts ontology web page instance validate robustness
paper train mozilla deepspeech architecture german swiss german speech datasets compare result different train methods first train model scratch languages improve upon result use english pretrained version deepspeech weight initialization experiment effect freeze different layer train see even freeze one layer already improve result dramatically
outcome prediction clinical text prevent doctor overlook possible risk help hospitals plan capacities simulate patients admission time decision support especially valuable contribute novel admission discharge task four common outcome prediction target diagnose discharge procedures perform hospital mortality length stay prediction ideal system infer outcomes base symptoms pre condition risk factor patient evaluate effectiveness language model handle scenario propose clinical outcome pre train integrate knowledge patient outcomes multiple public source present simple method incorporate icd code hierarchy model show approach improve performance outcome task several baselines detail analysis reveal strengths model include transferability also weaknesses handle vital value inconsistencies underlie data
paper discuss current critique neural network base natural language understand nlu solutions know language model argue much current debate rest argumentation error refer singleton fallacy assumption language mean understand single uniform phenomena unobtainable current language model contrast argue many different type language use mean understand current language model build explicit purpose acquire represent one type structural understand language argue structural understand may cover several different modalities handle several different type mean position currently see theoretical reason structural knowledge would insufficient count real understand
abstractive summarization systems generally rely large collections document summary pair however performance abstractive systems remain challenge due unavailability parallel data low resource languages like bengali overcome problem propose graph base unsupervised abstractive summarization system single document set bengali text document require part speech pos tagger pre train language model train bengali texts also provide human annotate dataset document summary pair evaluate abstractive model support comparison future abstractive summarization systems bengali language conduct experiment dataset compare system several well establish unsupervised extractive summarization systems unsupervised abstractive summarization model outperform baselines without expose human annotate reference summaries
pervasiveness internet social media enable rapid anonymous spread hate speech content microblogging platforms twitter current eu us legislation hateful language conjunction large amount data produce platforms lead automatic tool necessary component hate speech detection task pipeline study examine performance several diverse text representation techniques pair multiple classification algorithms automatic hate speech detection abusive language discrimination task perform experimental evaluation binary multiclass datasets pair significance test result show simple hate keyword frequency feature bow work best follow pre train word embeddings glove well n gram graph nggs graph base representation prove produce efficient low dimensional rich feature task combination representations pair logistic regression three layer neural network classifiers achieve best detection performance term micro macro f measure
paper summarize work first track ninth dialog system technology challenge dstc nine beyond domain apis task orient conversational model unstructured knowledge access goal task generate responses user turn task orient dialog require knowledge unstructured document task divide three subtasks detection selection generation order compute efficient formulate selection problem term hierarchical classification step achieve best result model alternatively employ siamese sequence embed model refer dense knowledge retrieval retrieve relevant document method reduce computation time factor 100x cost degradation r1 five six compare first model either approach use retrieval augment generation generate responses base multiple select snippets show method use fine tune train embeddings
context model play critical role build multi turn dialogue systems conversational query rewrite cqr aim simplify multi turn dialogue model single turn problem explicitly rewrite conversational query self contain utterance however exist approach rely massive supervise train data labor intensive annotate detection omit important information context improve besides intent consistency constraint contextual query rewrite query also ignore tackle issue first propose construct large scale cqr dataset automatically via self supervise learn need human annotation introduce novel cqr model teresa base transformer enhance self attentive keywords detection intent consistency constraint finally conduct extensive experiment two public datasets experimental result demonstrate propose model outperform exist cqr baselines significantly also prove effectiveness self supervise learn improve cqr performance
state art neural language model lms represent transformers highly complex use fix deterministic parameter estimate fail account model uncertainty lead fit poor generalization give limit train data order address issue paper propose full bayesian learn framework transformer lm estimation efficient variational inference base approach use estimate latent parameter posterior distributions associate different part transformer model architecture include multi head self attention fee forward embed layer statistically significant word error rate wer reductions five absolute three hundred and eighteen relative consistent perplexity gain obtain baseline transformer lms state art switchboard corpus train lf mmi factor tdnn systems vector speaker adaptation performance improvements also obtain cross domain lm adaptation task require port transformer lm train switchboard fisher data low resource dementiabank elderly speech corpus
accreditation body call curriculum development process open stakeholders reflect viewpoints students industry university faculty society however communication difficulties faculty non faculty group leave unexplored immense collaboration potential use classification learn objectives natural language process data visualization paper present method deliver program plan representations universal self explanatory empower simple example show method contribute representative program plan experience case study use confirm method accuracy utility
representation learn significant challenge task multimodal learn effective modality representations contain two part characteristics consistency difference due unify multimodal annotation exist methods restrict capture differentiate information however additional uni modal annotations high time labor cost paper design label generation module base self supervise learn strategy acquire independent unimodal supervisions joint train multi modal uni modal task learn consistency difference respectively moreover train stage design weight adjustment strategy balance learn progress among different subtasks guide subtasks focus sample larger difference modality supervisions last conduct extensive experiment three public multimodal baseline datasets experimental result validate reliability stability auto generate unimodal supervisions mosi mosei datasets method surpass current state art methods sims dataset method achieve comparable performance human annotate unimodal label full cod available https githubcom thuiar self mm
pre train language model plms like bert make great progress nlp news article usually contain rich textual information plms potentials enhance news text model various intelligent news applications like news recommendation retrieval however exist plms huge size hundreds millions parameters many online news applications need serve millions users low latency tolerance pose huge challenge incorporate plms scenarios knowledge distillation techniques compress large plm much smaller one meanwhile keep good performance however exist language model pre train distil general corpus like wikipedia gap news domain may suboptimal news intelligence paper propose newsbert distill plms efficient effective news intelligence approach design teacher student joint learn distillation framework collaboratively learn teacher student model student model learn learn experience teacher model addition propose momentum distillation method incorporate gradients teacher model update student model better transfer useful knowledge learn teacher model extensive experiment two real world datasets three task show newsbert effectively improve model performance various intelligent news applications much smaller model
present preprocessed ready use automatic speech recognition corpus bembaspeech consist twenty-four hours read speech bemba language write low resourced language speak thirty population zambia assess usefulness train test asr systems bemba train end end bemba asr system fine tune pre train deepspeech english model train portion bembaspeech corpus best model achieve word error rate wer five thousand, four hundred and seventy-eight result show corpus use build asr systems bemba corpus model publicly release https githubcom csikasote bembaspeech
hate speech increasingly prevalent online negative outcomes include increase prejudice extremism even offline hate crime automatic detection online hate speech help us better understand impact however field recently progress advance natural language process challenge still remain particular exist approach hate speech detection focus single social media platform isolation limit use model validity nature language vary platform platform propose new cross platform approach detect hate speech leverage multiple datasets classification model different platforms train superlearner combine exist novel train data improve detection increase model applicability demonstrate approach outperform exist model achieve good performance test message novel social media platforms include original train data
advent neural network nlp bring substantial improvements supervise relation extraction however obtain sufficient quantity train data remain key challenge work propose process bootstrapping train datasets perform quickly non nlp experts take advantage search engines syntactic graph shlain et al two thousand and twenty expose friendly example syntax use obtain positive examples search sentence syntactically similar user input examples apply technique relations tacred docred show result model competitive model train manually annotate data data obtain distant supervision model also outperform model train use nlg data augmentation techniques extend search base approach nlg method improve result
question answer qa benchmark natural language process nlp task model predict answer give question use relate document image knowledge base question answer pair automatic qa successfully apply various domains like search engines chatbots however specific domains like biomedicine qa systems still rarely use real life settings biomedical qa bqa emerge qa task enable innovative applications effectively perceive access understand complex biomedical knowledge work provide critical review recent efforts bqa comprehensively investigate prior bqa approach classify six major methodologies open domain knowledge base information retrieval machine read comprehension question entailment visual qa four topics content scientific clinical consumer health examination five type format yes extraction generation multi choice retrieval end highlight several key challenge bqa explore potential directions future work
anomia word find difficulties hallmark aphasia acquire language disorder commonly cause stroke assessment speech performance use picture name task key method diagnosis monitor responses treatment interventions people aphasia pwa currently assessment conduct manually speech language therapists slt surprisingly despite advancements automatic speech recognition asr artificial intelligence technologies like deep learn research develop automate systems task scarce present nuva utterance verification system incorporate deep learn element classify correct versus incorrect name attempt aphasic stroke patients test eight native british english speak pwa system performance accuracy range eight hundred and thirty-six nine hundred and thirty-six ten fold cross validation mean eight hundred and ninety-five performance significantly better baseline create study use one lead commercially available asrs google speech text service also comparable instance two independent slt rat dataset
draw causal conclusions observational data require make assumptions true data generate process causal inference research typically consider low dimensional data categorical numerical field structure medical record high dimensional unstructured data natural language complicate evaluation causal inference methods evaluations rely synthetic datasets know causal effect model natural language generation widely study perform well empirically however exist methods immediately applicable produce synthetic datasets causal evaluations allow quantify causal effect text work develop framework adapt exist generation model produce synthetic text datasets know causal effect use framework perform empirical comparison four recently propose methods estimate causal effect text data release code synthetic datasets
develop system detect online offensive language important health security online users study show cyberhate online harassment misuse technology rise particularly global coronavirus pandemic two thousand and twenty accord latest report anti defamation league adl thirty-five online users report online harassment relate identity base characteristics three increase two thousand and nineteen apply advance techniques natural language process nlp field support development online hate free community critical task social justice transfer learn enhance performance classifier allow transfer knowledge one domain one dataset others see thus support classifier generalizable study apply principles transfer learn cross multiple arabic offensive language datasets compare effect system performance study aim investigate effect fine tune train bidirectional encoder representations transformers bert model multiple arabic offensive language datasets individually test use datasets individually experiment start comparison among multiple bert model guide selection main model use study study also investigate effect concatenate datasets use fine tune train bert model result demonstrate limit effect transfer learn performance classifiers particularly highly dialectic comment
goal generative phonology formulate chomsky halle one thousand, nine hundred and sixty-eight specify formal system explain set attest phonological string language traditionally collection rule constraints case optimality theory underlie form uf posit work tandem generate phonological string however degree abstraction ufs respect concrete realizations contentious main contribution work implement phonological generative system neural model differentiable end end rather set rule constraints contrary traditional phonology model ufs continuous vectors mathbbrd rather discrete string consequence ufs discover automatically rather posit linguists model scale size realistic vocabulary moreover compare several modes generative process contemplate presence absence underlie representation morphemes surface form sfs ii conditional dependence independence ufs respect sfs evaluate ability mode predict attest phonological string two datasets cover five twenty-eight languages respectively result corroborate two tenets generative phonology viz necessity ufs independence sfs general neural model generative phonology learn ufs sfs automatically large scale
recently text speech representation learn successfully improve many language relate task however exist methods learn one input modality unify acoustic text representation desire many speech relate task speech translation propose fuse acoustic text mask language model fat mlm jointly learn unify representation acoustic text put within cross modal representation learn framework present end end model fuse acoustic text speech translation fat st experiment three translation directions show propose speech translation model fine tune fat mlm substantially improve translation quality five hundred and ninety bleu
previous work slogan generation focus generate novel slogans utilise templates mine real slogans slogans catchy often coherent company focus style across market communications templates mine company slogans propose sequence sequence transformer model generate slogans brief company description naive sequence sequence model fine tune slogan generation prone introduce false information especially unrelated company name appear train data use delexicalisation address problem improve generate slogans quality large margin furthermore apply two simple effective approach generate diverse slogans firstly train slogan generator condition industry inference time change industry obtain different flavour slogans secondly instead use company description input sequence sample random paragraph company website surprisingly model generate meaningful slogans even input sequence resemble company description validate effectiveness propose method quantitative evaluation qualitative evaluation best model achieve rouge one two l f1 score five thousand, three hundred and thirteen three thousand, three hundred and thirty four thousand, six hundred and forty-nine besides human evaluators assign generate slogans average score three hundred and thirty-nine scale one five indicate system generate plausible slogans quality close human write ones average score three hundred and fifty-five
text encode one important step natural language process nlp do well self attention mechanism current state art transformer encoder bring significant improvements performance many nlp task though transformer encoder may effectively capture general information result representations backbone information mean gist input text specifically focus paper propose explicit implicit text compression approach enhance transformer encode evaluate model use approach several typical downstream task rely encode heavily explicit text compression approach use dedicate model compress text implicit text compression approach simply add additional module main model handle text compression propose three ways integration namely backbone source side fusion target side fusion side fusion integrate backbone information transformer base model various downstream task evaluation benchmark datasets show propose explicit implicit text compression approach improve result comparison strong baselines therefore conclude compare encode baseline model text compression help encoders learn better language representations
present joint model entity level relation extraction document contrast approach focus local intra sentence mention pair thus require annotations mention level model operate entity level multi task approach follow build upon coreference resolution gather relevant signal via multi instance learn multi level representations combine global entity local mention information achieve state art relation extraction result docred dataset report first entity level end end relation extraction result future reference finally experimental result suggest joint approach par task specific learn though efficient due share parameters train step
sentiment task hate speech detection sentiment analysis especially perform languages english often low resource study exploit emotional information encode emojis enhance performance variety sentiment task do use transfer learn approach parameters learn emoji base source task transfer sentiment target task analyse efficacy transfer three condition ie emoji content ii label distribution target task well iii difference monolingually multilingually learn source task find ia transfer beneficial target task balance high emoji content monolingually learn source task benefit take account culturally specific use emojis gain f1 two hundred and eighty baseline
neural dependency parse achieve remarkable performance many domains languages bottleneck massive label data limit effectiveness approach low resource languages work focus dependency parse morphological rich languages mrls low resource set although morphological information essential dependency parse task morphological disambiguation lack powerful analyzers pose challenge get information mrls address challenge propose simple auxiliary task pretraining perform experiment ten mrls low resource settings measure efficacy propose pretraining method observe average absolute gain two point uas thirty-six point las code data available https githubcom jivnesh lcm
recent advance deep learn lead significant improvements machine translation neural machine translation often still able continuously adapt environment humans well machine translation bilingual dictionaries promise knowledge source continuously integrate new knowledge however exploitation pose several challenge system need able perform one shoot learn well model morphology source target language work propose evaluation framework assess ability neural machine translation continuously learn new phrase integrate one shoot learn methods neural machine translation different word representations show important address order successfully make use bilingual dictionaries address challenge able improve ability translate new rare word phrase thirty seventy correct lemma even generate ninety
recently universal neural machine translation nmt share encoder decoder gain good performance zero shoot translation unlike universal nmt jointly train language specific encoders decoders aim achieve universal representation across non share modules language language family non share architecture advantage mitigate internal language competition especially share vocabulary model parameters restrict size however performance use multiple encoders decoders zero shoot translation still lag behind universal nmt work study zero shoot translation use language specific encoders decoders propose generalize non share architecture universal nmt differentiate transformer layer language specific interlingua selectively share parameters apply cross attentions explore maximize representation universality realize best alignment language agnostic information also introduce denoising auto encode dae objective jointly train model translation task multi task manner experiment two public multilingual parallel datasets show propose model achieve competitive better result universal nmt strong pivot baseline moreover experiment incrementally add new language train model update new model parameters little effort zero shoot translation newly add language exist languages achieve comparable result model train jointly scratch languages
task graph text generation aim produce sentence preserve mean input graph crucial defect current state art model may mess even drop core structural information input graph generate output propose tackle problem leverage richer train signal guide model preserve input information particular introduce two type autoencoding losses individually focus different aspects aka view input graph losses back propagate better calibrate model via multi task train experiment two benchmarks graph text generation show effectiveness approach state art baseline code available urlhttp githubcom soistesimmer amr multiview
responsible development technology involve applications inclusive diverse set users hope support important part understand many ways refer person able fluently change different form need perform case study singular common way promote gender inclusion english define write task create evaluation benchmark show model train produce gender neutral english one word error rate human label data discuss practical applications ethical considerations task provide direction future work inclusive natural language systems
automatic comment generation special challenge task verify model ability news content comprehension language generation comment convey salient interest information news article also imply various different reader characteristics treat essential clue diversity however comment generation approach focus saliency information extraction reader aware factor imply comment neglect address issue propose unify reader aware topic model saliency information detection framework enhance quality generate comment reader aware topic model design variational generative cluster algorithm latent semantic learn topic mine reader comment saliency information detection introduce bernoulli distribution estimate news content select saliency information obtain topic representations well select saliency information incorporate decoder generate diversify informative comment experimental result three datasets show framework outperform exist baseline methods term automatic metrics human evaluation potential ethical issue also discuss detail
study estimate inherent human disagreement annotation label distribution natural language inference task post hoc smooth predict label distribution match expect label entropy effective simple manipulation reduce kl divergence almost half yet improve majority label prediction accuracy learn label distributions end introduce small amount examples multiple reference train depart standard practice collect single reference per train example find collect multiple reference achieve better accuracy fix annotation budget lastly provide rich analyse compare two methods improve label distribution estimation
hausa language belong afroasiatic phylum first language speakers sub saharan african language majority speakers reside northern southern areas nigeria republic niger respectively estimate one hundred million people speak language hence make one speak chadic language hausa consider well study document language among sub saharan african languages view low resource language perspective natural language process nlp due limit resources utilise nlp relate task common languages africa thus crucial enrich languages resources support speed pace conduct various downstream task meet demand modern society exist useful datasets notably news sit religious texts diversity need corpus provide expansive collection curated datasets consist formal informal form language refutable websites online social media network respectively collection large diverse exist corpora provide first largest set hausa social media data post capture peculiarities language collection also consist parallel dataset use task machine translation applications areas detection spurious inciteful online content describe curation process collection preprocessing obtain data proffer research problems could address use data
paper present submission team indicnlpkgp eacl two thousand and twenty-one share task offensive language identification dravidian languages task aim classify different offensive content type three code mix dravidian language datasets work leverage exist state art approach text classification incorporate additional data transfer learn pre train model final submission ensemble awd lstm base model along two different transformer model architectures base bert roberta achieve weight average f1 score ninety-seven seventy-seven seventy-two malayalam english tamil english kannada english datasets rank 1st 2nd 3rd respective task
language model lms virtual assistants vas typically train large amount data result prohibitively large model require excessive memory use serve user request real time entropy prune result smaller model significant degradation effectiveness tail user request distribution customize entropy prune allow keep list infrequent n grams require relax prune threshold propose three methods construct keep list method advantage disadvantage respect lm size asr accuracy cost construct keep list best lm give eight average word error rate wer reduction target test set three time larger baseline also propose discriminative methods reduce size lm retain majority wer gain achieve largest lm
multi label text classification refer problem assign give document relevant label label set commonly metadata give document hierarchy label available real world applications however exist study focus model text information attempt utilize either metadata hierarchy signal paper bridge gap formalize problem metadata aware text classification large label hierarchy eg tens thousands label address problem present match solution end end framework leverage metadata hierarchy information incorporate metadata pre train embeddings text metadata space also leverage fully connect attentions capture interrelations leverage label hierarchy propose different ways regularize parameters output probability child label parent extensive experiment two massive text datasets large scale label hierarchies demonstrate effectiveness match state art deep learn baselines
intent classification task speak language understand intent classification system usually implement pipeline process speech recognition module follow text process classify intents also study end end system take acoustic feature input classify intents directly systems take advantage relevant linguistic information suffer limit train data work propose novel intent classification framework employ acoustic feature extract pretrained speech recognition system linguistic feature learn pretrained language model use knowledge distillation technique map acoustic embeddings towards linguistic embeddings perform fusion acoustic linguistic embeddings cross attention approach classify intents propose method achieve nine thousand and eighty-six nine thousand, nine hundred and seven accuracy atis fluent speech corpus respectively
paper present self supervise learn method pointer generator network improve speak text normalization speak text normalization convert speak style text style normalize text become important technology improve subsequent process machine translation summarization successful speak text normalization method date sequence sequence seq2seq map use pointer generator network possess copy mechanism input sequence however model require large amount pair data speak style text style normalize text difficult prepare volume data order construct speak text normalization model limit pair data focus self supervise learn utilize unpaired text data improve seq2seq model unfortunately conventional self supervise learn methods assume pointer generator network utilize therefore propose novel self supervise learn method mask pointer generator network mapgn propose method effectively pre train pointer generator network learn fill mask tokens use copy mechanism experiment demonstrate mapgn effective pointer generator network conventional self supervise learn methods two speak text normalization task
explore cross lingual transfer register classification web document register text varieties blog news one primary predictors linguistic variation thus affect automatic process language introduce two new register annotate corpora frecore swecore french swedish demonstrate deep pre train language model perform strongly languages outperform previous state art english finnish specifically show one zero shoot cross lingual transfer large english core corpus match surpass previously publish monolingual model two lightweight monolingual classification require little train data reach surpass zero shoot performance analyse classification result find certain register continue pose challenge particular cross lingual transfer
recent advance self supervise learn dramatically improve state art wide variety task however research language model pre train mostly focus natural languages unclear whether model like bert variants provide best pre train apply modalities source code paper introduce new pre train objective dobf leverage structural aspect program languages pre train model recover original version obfuscate source code show model pre train dobf significantly outperform exist approach multiple downstream task provide relative improvements thirteen unsupervised code translation twenty-four natural language code search incidentally find pre train model able de obfuscate fully obfuscate source file suggest descriptive variable name
recognition personalize content contact name remain challenge problem end end speech recognition systems work demonstrate first second pass rescoring strategies leverage together improve recognition word follow previous work use shallow fusion approach bias towards recognition personalize content first pass decode show approach improve personalize content recognition sixteen minimum degradation general use case describe fast scalable algorithm enable bias model remain word level apply bias subword level advantage require bias model dependent subword symbol table also describe novel second pass de bias approach use conjunction first pass shallow fusion optimize oracle wer achieve additional fourteen improvement personalize content recognition even improve accuracy general use case twenty-five
word malleable object influence events reflect write texts situate global outbreak covid nineteen research aim detect semantic shift social media language trigger health crisis covid nineteen relate big data extract twitter train separate word embed model different time periods outbreak employ alignment base approach compare embeddings general purpose twitter embed unrelated covid nineteen also compare train embeddings among observe diachronic evolution carry case study set word choose topic detection verify alignment approach valid finally quantify size global semantic shift stability measure base back forth rotational alignment
success pre train language model recent years researchers focus open black box model follow interest carry qualitative quantitative analysis constituency grammar attention head bert roberta employ syntactic distance method extract implicit constituency grammar attention weight head result show exist head induce grammar type much better baselines suggest head act proxy constituency grammar also analyze attention head constituency grammar induce cgi ability change fine tune two kinds task include sentence mean similarity sms task natural language inference nli task result suggest sms task decrease average cgi ability upper layer nli task increase lastly investigate connections cgi ability natural language understand ability qqp mnli task
current model word sense disambiguation wsd struggle disambiguate rare sense despite reach human performance global wsd metrics stem lack data model evaluate rare sense exist wsd datasets paper introduce fews shoot examples word sense new low shoot wsd dataset automatically extract example sentence wiktionary fews high sense coverage across different natural language domains provide one large train set cover many sense previous datasets two comprehensive evaluation set contain zero shoot examples wide variety sense establish baselines fews knowledge base neural wsd approach present transfer learn experiment demonstrate model additionally train fews better capture rare sense exist wsd datasets finally find humans outperform best baseline model fews indicate fews support significant future work low shoot wsd
non autoregressive generation nag recently attract great attention due fast inference speed however generation quality exist nag model still lag behind autoregressive counterparts work show bert employ backbone nag model greatly improve performance additionally devise mechanisms alleviate two common problems vanilla nag model inflexibility prefix output length conditional independence individual token predictions lastly increase speed advantage propose model propose new decode strategy ratio first applications output lengths approximately estimate beforehand comprehensive evaluation test propose model three text generation task include text summarization sentence compression machine translation experimental result show model significantly outperform exist non autoregressive baselines achieve competitive performance many strong autoregressive model addition also conduct extensive analysis experiment reveal effect propose component
question answer qa systems deploy real world users query variety interfaces speak voice assistants type question search engine even translate question languages support qa system significant community attention devote identify correct answer passages assume perfectly form question show components pipeline precede answer engine introduce vary considerable source error performance degrade substantially base upstream noise source even powerful pre train qa model conclude substantial room progress qa systems effectively deploy highlight need qa evaluation expand consider real world use hope find spur greater community interest issue arise systems actually need utility humans
neural sequence sequence model currently predominant choice language generation task yet word level task exact inference model reveal empty string often global optimum prior work speculate phenomenon result inadequacy neural model language generation however case morphological inflection find empty string almost never probable solution model greedy search often find global optimum observations suggest poor calibration many neural model may stem characteristics specific subset task rather general ill suitedness model language generation
many modern entity recognition systems include current state art de identification systems base bidirectional long short term memory bilstm units augment conditional random field crf sequence optimizer systems process input sentence sentence approach prevent systems capture dependencies sentence boundaries make accurate sentence boundary detection prerequisite since sentence boundary detection problematic especially clinical report dependencies co reference across sentence boundaries abundant systems clear limitations study build new system framework one current state art de identification systems neuroner overcome limitations new system incorporate context embeddings forward backward n grams without use sentence boundaries context enhance de identification cedi system capture dependencies sentence boundaries bypass sentence boundary detection problem altogether enhance system deep affix feature attention mechanism capture pertinent part input cedi system outperform neuroner two thousand and six i2b2 de identification challenge dataset two thousand and fourteen i2b2 share task de identification dataset two thousand and sixteen cegs n grid de identification dataset p001 datasets comprise narrative clinical report english contain different note type vary discharge summaries psychiatric note enhance cedi deep affix feature attention mechanism increase performance
aspect sentiment triplet extraction aste aim extract triplets sentence include target entities associate sentiment polarities opinion span rationalize polarities exist methods short build correlation target opinion pair neglect mutual interference among different sentiment triplets address issue propose novel two stage method enhance correlation target opinions stage one extract target opinions sequence tag insert group artificial tag name perceivable pair indicate span target opinion sequence establish correlation candidate target opinion pair meanwhile reduce mutual interference triplets restrict tokens attention field finally polarity identify accord representation perceivable pair conduct experiment four datasets experimental result show model outperform state art methods
recently human behavioral data read mainly interest researchers understand human cognition however human language process signal also beneficial machine learn base natural language process task use eeg brain activity purpose largely unexplored yet paper present first large scale study systematically analyze potential eeg brain activity data improve natural language process task special focus feature signal beneficial present multi modal machine learn architecture learn jointly textual input well eeg feature find filter eeg signal frequency band beneficial use broadband signal moreover range word embed type eeg data improve binary ternary sentiment classification outperform multiple baselines complex task relation detection research need finally eeg data show particularly promise limit train data available
first step text simplification predict word consider complex give target population carry lexical substitution task commonly refer complex word identification cwi often model supervise classification problem train systems annotate datasets word sometimes multi word expressions label regard complexity require paper analyze previous work carry task investigate properties complex word identification datasets english
present systems submit share task acronym identification ai acronym disambiguation ad hold workshop sdu mainly experiment bert scibert addition assess effectiveness bioless tag blend along prowess ensembling ai ad formulate problem span prediction task experiment different train techniques also leverage use external data systems rank 11th 3rd ai ad task respectively
prerequisite computational study literature availability properly digitize texts ideally reliable meta data grind truth annotation poetry corpora exist number languages larger collections lack consistency encode various standards annotate corpora typically constrain particular genre design analysis certain linguistic feature like rhyme work provide large poetry corpora english german annotate prosodic feature smaller corpora train corpus drive neural model enable robust large scale analysis show bilstm crf model syllable embeddings outperform crf baseline different bert base approach multi task setup particular beneficial task relations illustrate inter dependence poetic feature model learn foot boundaries better jointly predict syllable stress aesthetic emotions verse measure benefit find caesuras quite dependent syntax also integral shape overall measure line
hate speech one type harmful online content directly attack promote hate towards group individual member base actual perceive aspects identity ethnicity religion sexual orientation online hate speech rise automatic detection natural language process task gain increase interest however recently show exist model generalise poorly unseen data survey paper attempt summarise generalisable exist hate speech detection model reason hate speech model struggle generalise sum exist attempt address main obstacles propose directions future research improve generalisation hate speech detection
propose automatic speech recognition asr model inspire echo state network esn subset recurrent neural network rnn layer model randomly initialize untrained study focus rnn conformer model show model quality drop even decoder fully randomize furthermore model train efficiently decoders require update contrast randomize encoders hurt model quality indicate optimize encoders learn proper representations acoustic input vital speech recognition overall challenge common practice train asr model components demonstrate esn base model perform equally well enable efficient train storage fully trainable counterparts
open domain multi turn conversations mainly three feature hierarchical semantic structure redundant information long term dependency ground select relevant context become challenge step multi turn dialogue generation however exist methods differentiate useful word utterances long distance response besides previous work perform context selection base state decoder lack global guidance could lead focus irrelevant unnecessary information paper propose novel model hierarchical self attention mechanism distant supervision detect relevant word utterances short long distance also discern relate information globally decode experimental result two public datasets automatic human evaluations show model significantly outperform baselines term fluency coherence informativeness
work describe approach address social media variety geolocation task feature two thousand and twenty-one vardial evaluation campaign focus second subtask base data set form approximately thirty thousand swiss german jodels dialect identification task accurately predict latitude longitude test sample frame task double regression problem employ xgboost meta learner combine power variety machine learn approach predict latitude longitude model include ensemble range simple regression techniques support vector regression deep neural model hybrid neural network neural transformer minimize prediction error approach problem different perspectives consider various type feature low level character n grams high level bert embeddings xgboost ensemble result combine power aforementioned methods achieve median distance two hundred and thirty-six km test data place us third place rank difference six hundred and five km twenty-nine km submissions first second place respectively
nlp significant role advance healthcare find key extract structure information radiology report understand recent developments nlp application radiology significance recent review limit study systematically assess recent literature nlp apply radiology report automate literature search yield four thousand, seven hundred and ninety-nine result use automate filter metadata enrich step citation search combine manual review analysis base twenty-one variables include radiology characteristics nlp methodology performance study clinical application characteristics present comprehensive analysis one hundred and sixty-four publications retrieve categorise one six clinical application categories deep learn use increase conventional machine learn approach still prevalent deep learn remain challenge data scarce little evidence adoption clinical practice despite seventeen study report greater eighty-five f1 score hard comparatively evaluate approach give use different datasets fourteen study make data fifteen code available ten externally validate result automate understand clinical narratives radiology report potential enhance healthcare process reproducibility explainability model important domain move applications clinical use could do share code enable validation methods different institutional data reduce heterogeneity report study properties allow inter study comparisons result significance researchers provide systematic synthesis exist work build identify gap opportunities collaboration avoid duplication
natural language process nlp rely heavily train data transformers get bigger require massive amount train data satisfy requirement text augmentation look way expand current dataset generalize model one text augmentation look translation augmentation take english sentence translate another language translate back english paper look effect one hundred and eight different language back translations various metrics text embeddings
paper present approach address eacl wanlp two thousand and twenty-one share task one nuanced arabic dialect identification nadi task aim develop system identify geographical locationcountry province arabic tweet form modern standard arabic dialect come solve task two part first part involve pre process provide dataset clean add segment various part text follow carry experiment different versions two transformer base model arabert araelectra final approach achieve macro f1 score two hundred and sixteen two hundred and thirty-five fifty-four forty-three four subtasks rank second msa identification subtasks fourth da identification subtasks
inspire curriculum learn propose consecutive ie image text text generation framework divide problem radiology report generation two step contrary generate full radiology report image model generate global concepts image first step reform finer coherent texts use transformer base architecture follow transformer base sequence sequence paradigm step improve upon state art two benchmark datasets
measure similarity two different sentential arguments important task argument mine however one challenge field dataset must annotate use expertise variety topics make supervise learn label data expensive paper investigate whether problem could alleviate transfer learn first adapt pretrained language model domain interest use self supervise learn fine tune model task measure similarity sentence take different domains approach improve correlation human annotate similarity score compare competitive baseline model argument facet similarity dataset unsupervised set moreover achieve comparable performance fully supervise baseline model use sixty label data sample believe work suggest possibility generalize argument cluster model various argumentative topics
last years emotion detection social media text become popular problem due wide range application better understand consumers psychology aid human interaction computers design smart systems etc availability huge amount data social media regularly use express sentiments opinions problem garner great attention paper present hinglish dataset label emotion detection highlight deep learn base approach detect emotions hindi english code mix tweet use bilingual word embeddings derive fasttext word2vec approach well transformer base model experiment various deep learn model include cnns lstms bi directional lstms without attention along transformers like bert roberta albert transformer base bert model outperform model give best performance accuracy seven thousand, one hundred and forty-three
curriculum learn cl recently gain traction natural language process task still adequately analyze previous work show effectiveness fail short explain interpret internal work fully paper analyze curriculum learn sentiment analysis along multiple ax ax propose earlier work need depth study analysis require understand curriculum learn work ax analysis include task difficulty cl compare cl pace techniques qualitative analysis visualize movement attention score model curriculum phase progress find curriculum learn work best difficult task may even lead decrement performance task higher performance without curriculum learn see one pass curriculum strategies suffer catastrophic forget attention movement visualization within curriculum pace show curriculum learn break challenge main task easier sub task solve sequentially
many time reviewers fail appreciate novel ideas researcher provide generic feedback thus proper assignment reviewers base area expertise necessary moreover read every paper end end assign reviewer tedious task paper describe system team fidelipi submit share task sdpra two thousand and twenty-one fourteen comprise four independent sub systems capable classify abstract scientific literature one give seven class first one roberta ten base model build abstract add topic model latent dirichlet allocation lda two base feature first model result second sub system third one sentence level roberta ten model fourth one logistic regression model build use term frequency inverse document frequency tf idf feature ensemble predictions four sub systems use majority vote develop final system give f1 score ninety-three test validation set outperform exist state art sota model scibert one term f1 score validation setour codebase available https githubcom sdpra two thousand and twenty-one share task tree main fidelipi
nlp deeply intertwine formal study language conceptually historically arguably connection go way back chomsky syntactic structure one thousand, nine hundred and fifty-seven also still hold true today strand recent work build formal analysis modern neural network methods term formal languages document aim explain background formal languages relate recent work necessity ignore large part rich history field instead focus concepts connect modern deep learn base nlp
many modern neural machine translation nmt systems train nonhomogeneous datasets several distinct dimension variation eg domain source generation method style etc describe empirically evaluate multidimensional tag mdt simple yet effective method pass sentence level information model human bleu evaluation result show mdt apply problem multi domain adaptation significantly reduce train cost without sacrifice translation quality constituent domains
machine translation mt systems especially design industrial set train general parallel data derive web thus style typically drive word structure distribution come average many domains contrast mt customers want translations specialize domain typically able provide text sample describe approach customize mt systems specific domains select data similar target customer data train neural translation model build document classifiers use monolingual target data eg provide customers select parallel train data web crawl data finally train mt model automatically select data obtain system specialize target domain test approach benchmark wmt eighteen translation task news domains enable comparisons state art mt systems result show model outperform top systems use less data smaller model
present study design multilingual answer sentence selection as2 model core component modern question answer qa systems main idea transfer data create one resource rich language eg english languages less rich term resources main find paper train data as2 translate target language use effectively fine tune transformer base model language ii one multilingual transformer model enough rank answer multiple languages iii mix language question answer pair use fine tune model select answer language input question one language highly reduce complexity technical requirement multilingual qa system experiment validate find show modest drop three respect state art english model
automatic table detection pdf document achieve great success tabular data extraction still challenge due integrity noise issue detect table areas accurate data extraction extremely crucial finance area inspire aim research propose automate table detection tabular data extraction financial pdf document propose method consist three main process detect table areas faster r cnn region base convolutional neural network model feature pyramid network fpn page image extract content structure compound layout segmentation technique base optical character recognition ocr formulate regular expression rule table header separation tabular data extraction feature embed rule base filter restructure function highly scalable annotate new financial document dataset table regions experiment excellent table detection performance detection model obtain customize dataset main contributions paper propose financial document dataset table area annotations superior detection model rule base layout segmentation technique tabular data extraction pdf file
argument mine systems often consider contextual information ie information outside argumentative discourse unit train accomplish task argument component identification classification relation extraction however prior work carefully analyze utility different contextual properties context aware model work show two different type contextual information local discourse context speaker context incorporate computational model classify argument components multi party classroom discussions find context type improve performance although improvements dependent context size position
teach collaborative argumentation advance skill many k twelve teachers struggle develop address develop discussion tracker classroom discussion analytics system base novel algorithms classify argument move specificity collaboration result classroom deployment indicate teachers find analytics useful underlie classifiers perform moderate substantial agreement humans
understand infer mean language neural model learn complicate nuances discover distinctive linguistic phenomena data easy task instance lexical ambiguity fundamental feature language challenge learn even prominently infer mean rare unseen lexical units difficult neural network mean often determine context context languages allow mean convey even specific word use know reader model learn process system learn instance context able generalize well unseen case learn process hinder train data scarce task even sufficient data learn pattern long tail lexical distribution challenge thesis focus understand certain potentials contexts neural model design augmentation model benefit focus machine translation important instance general language understand problem translate source language target language neural model understand mean constituents provide context generate constituents mean target language task accentuate value capture nuances language necessity generalization observations main problem study thesis neural machine translation model learn data devise focus contexts enhance learn look depth role context impact data learn model essential advance nlp field moreover help highlight vulnerabilities current neural network provide insights design robust model
paper present system propose reliable intelligence indentification vietnamese social network sit reintel task vietnamese language speech process two thousand and twenty vlsp two thousand and twenty share task task vlsp two thousand and twenty provide dataset approximately six thousand trainning news post annotate reliable unreliable label test set consist two thousand examples without label paper conduct experiment different transfer learn model bert4news phobert fine tune predict whether news reliable experiment achieve auc score nine thousand, four hundred and fifty-two private test set reintel organizers
present extend comparison contextualized language model hungarian compare hubert hungarian model four multilingual model include multilingual bert model evaluate model three task morphological probe pos tag ner find hubert work better model often large margin particularly near global optimum typically middle layer also find hubert tend generate fewer subwords one word use last subword token level task generally better choice use first one
contextual word representations become standard modern natural language process systems model use subword tokenization handle large vocabularies unknown word word level usage systems require way pool multiple subwords correspond single word paper investigate choice subword pool affect downstream performance three task morphological probe pos tag ner nine typologically diverse languages compare two massively multilingual model mbert xlm roberta morphological task widely use choose first subword worst strategy best result obtain use attention subwords pos tag strategies perform poorly best choice use small lstm subwords strategy work best ner show mbert better xlm roberta nine languages publicly release code data full result table urlhttps githubcom juditacs subword choice
information verification quite challenge task many time verify claim require pick piece information multiple piece evidence hierarchy complex semantic relations previously lot researchers mainly focus simply concatenate multiple evidence sentence accept reject claim approach limit evidence contain hierarchical information dependencies research aim verify facts base evidence select list article take wikipedia pretrained language model xlnet use generate meaningful representations graph base attention convolutions use way system require little additional train learn verify facts
urdu widely speak language south asia though immoderate literature exist urdu language still data enough naturally process language nlp techniques efficient language model exist english language high resource language urdu resourced languages neglect long time create efficient language model languages must good word embed model urdu find word embeddings train develop use skip gram model paper build corpus urdu scrap integrate data various source compile vocabulary urdu language also modify fasttext embeddings n grams model enable train build corpus use train embeddings word similarity task compare result exist techniques
pretrained language model widespread use natural language process despite success apply low resource languages still huge challenge although multilingual model hold great promise apply specific low resource languages eg roman urdu excessive paper show code switch property languages may use perform cross lingual transfer learn correspond high resource language also show transfer learn technique term bilingual language model use produce better perform model roman urdu enable train experimentation also present collection novel corpora roman urdu extract various source social network sit eg twitter train monolingual multilingual bilingual model roman urdu propose bilingual model achieve twenty-three accuracy compare two eleven monolingual multilingual model respectively mask language model mlm task
state art sota neural machine translation nmt systems translate texts sentence level ignore context intra textual information like previous sentence extra textual information like gender speaker sentence translate incorrectly personalise nmt persnmt document level nmt docnmt incorporate information translation process field relatively new previous work within limit moreover readily available robust evaluation metrics make difficult develop better systems well track global progress compare different methods thesis proposal focus persnmt docnmt domain dialogue extract tv subtitle five languages english brazilian portuguese german french polish three main challenge address one incorporate extra textual information directly nmt systems two improve machine translation cohesion devices three reliable evaluation persnmt docnmt
analyze frequency rank relationship sub vocabularies correspond three different grammatical class nouns verbs others collection literary work english whose word automatically tag accord grammatical role compare null hypothesis assume word belong class uniformly distribute across frequency rank vocabulary whole work disclose statistically significant differences three class result point fact frequency rank relationships may reflect linguistic feature associate grammatical function
auto regressive language model leave right generation order predominant paradigm language generation recently order text generation beyond traditional leave right paradigm attract extensive attention notable variation insertion base generation model use gradually extend context complete sentence purely insertion operations however since insertion operations disturb position information token often believe step insertion base likelihood estimation require bi directional textitre encode whole generate sequence computational overhead prohibit model scale generate long diverse texts stories news article report address issue propose insnet insertion base sequence model train efficiently traditional transformer decoders maintain performance bi directional context encoder evaluate insnet story generation clevr cogent caption show advantage insnet several dimension include computational cost generation quality ability perfectly incorporate lexical control better compositional generalization
free text clinical note detail aspects patient care great potential facilitate quality improvement assurance initiatives well advance clinical research however concern patient privacy confidentiality limit use clinical note research result information document note remain unavailable researchers de identification de id ie locate remove personally identify protect health information phi one way improve access clinical narratives however limit shelf de identification systems able consistently detect phi across different data source medical specialties abstract present performance state art de id system call neuroner1 diverse set note university washington uw model train data external institution partner healthcare vs institution uw present result level phi note type
internet contain wealth public opinion food safety include view food adulteration food bear diseases agricultural pollution irregular food distribution food production issue order systematically collect analyse public opinion food safety develop ifoodcloud platform real time sentiment analysis public opinion food safety china collect data three thousand, one hundred public source use explore public opinion trend public sentiment regional attention differences food safety incidents time construct sentiment classification model use multiple lexicon base deep learn base algorithms integrate ifoodcloud provide unprecedented rapid mean understand public sentiment toward specific food safety incidents best model f1 score achieve nine thousand, seven hundred and thirty-seven three real world case present demonstrate application robustness ifoodcloud could consider valuable tool promote scientisation food safety supervision risk communication
current generative base dialogue systems data hungry fail adapt new unseen domains small amount target data available additionally real world applications domains underrepresented need create system capable generalize domains use minimal data paper propose method adapt unseen domains combine transfer meta learn datml datml improve previous state art dialogue model diktnet introduce different learn technique meta learn use reptile first order optimization base meta learn algorithm improve train method evaluate model multiwoz dataset outperform diktnet bleu entity f1 score amount data available
paper explore difficulties annotate transcribe speak dutch frisian code switch utterances universal dependencies make use data fame corpus consist transcriptions audio data besides usual annotation difficulties dataset extra challenge frisian low resource informal nature data code switch non standard sentence segmentation start point two annotators annotate one hundred and fifty random utterances three stag fifty utterances stage disagreements discuss resolve increase seventy-eight uas one hundred and five las point achieve first third round paper focus issue arise annotate transcribe speech corpus resolve issue several solutions propose
recent study show multilingual language model underperform monolingual counterparts also well know fact train maintain monolingual model language costly time consume process roman urdu resource starve language use popularly social media platforms chat apps research propose novel dataset scrap tweet contain 54m tokens 3m sentence additionally also propose rubert bilingual roman urdu model create additional pretraining english bert compare performance monolingual roman urdu bert train scratch multilingual roman urdu bert create additional pretraining multilingual bert show experiment additional pretraining english bert produce notable performance improvement
paper address problem simultaneous machine translation simt explore two main concepts adaptive policies learn good trade high translation quality low latency b visual information support process provide additional visual contextual information may available textual input produce propose multimodal approach simultaneous machine translation use reinforcement learn strategies integrate visual textual information agent environment provide exploration different type visual information integration strategies affect quality latency simultaneous translation model demonstrate visual cue lead higher quality keep latency low
reinforcement learn rl powerful framework address discrepancy loss function use train final evaluation metrics use test time apply neural machine translation mt minimise mismatch cross entropy loss non differentiable evaluation metrics like bleu however suitability metrics reward function train time questionable tend sparse bias towards specific word use reference texts propose address problem make model less reliant metrics two ways entropy regularise rl method maximise reward function also explore action space avoid peaky distributions b novel rl method explore dynamic unsupervised reward function balance exploration exploitation base proposals soft actor critic sac framework adapt policy maximum entropy model language generation applications mt demonstrate sac bleu reward tend overfit less train data perform better domain data also show dynamic unsupervised reward lead better translation ambiguous word
text categorization essential task web content analysis consider ever evolve web data new emerge categories instead laborious supervise set paper focus minimally supervise set aim categorize document effectively couple seed document annotate per category recognize texts collect web often structure rich ie accompany various metadata one easily organize corpus text rich network join raw text document document attribute high quality phrase label surface name nod associations edge network provide holistic view corpus heterogeneous data source enable joint optimization network base analysis deep textual model train therefore propose novel framework minimally supervise categorization learn text rich network specifically jointly train two modules different inductive bias text analysis module text understand network learn module class discriminative scalable network learn module generate pseudo train label unlabeled document set modules mutually enhance co train use pool pseudo label test model two real world datasets challenge e commerce product categorization dataset six hundred and eighty-three categories experiment show give three seed document per category framework achieve accuracy ninety-two significantly outperform compare methods accuracy less two away supervise bert model train 50k label document
psychotherapy session counselor typically adopt techniques codify along specific dimension eg display warmth confidence attempt set collaboration facilitate evaluation session construct traditionally score train human raters reflect complex nature psychotherapy highly depend context interaction recent advance deep contextualized language model offer avenue accurate domain linguistic representations lead robust recognition score psychotherapy relevant behavioral construct support quality assurance supervision work bert base model propose automatic behavioral score specific type psychotherapy call cognitive behavioral therapy cbt prior work limit frequency base language feature short text excerpt capture unique elements involve spontaneous long conversational interaction model train multi task manner order achieve higher interpretability bert base representations augment available therapy metadata provide relevant non linguistic context lead consistent performance improvements
despite deep neural network dnns achieve enormous success many domains like natural language process nlp also prove vulnerable maliciously generate adversarial examples inherent vulnerability threaten various real world deploy dnns base applications strength model robustness several countermeasures propose english nlp domain obtain satisfactory performance however due unique language properties chinese trivial extend exist defenses chinese domain therefore propose advgraph novel defense enhance robustness chinese base nlp model incorporate adversarial knowledge semantic representation input extensive experiment two real world task show advgraph exhibit better performance compare previous work effective significantly strengthen model robustness even adaptive attack set without negative impact model performance legitimate input ii generic key component ie representation connotative adversarial knowledge task agnostic reuse chinese base nlp model without retrain iii efficient light weight defense sub linear computational complexity guarantee efficiency require practical scenarios
paper aim describe approach use detect hope speech hopeedi dataset experiment two approach first approach use contextual embeddings train classifiers use logistic regression random forest svm lstm base modelsthe second approach involve use majority vote ensemble eleven model obtain fine tune pre train transformer model bert albert roberta indicbert add output layer find second approach superior english tamil malayalam solution get weight f1 score ninety-three seventy-five forty-nine englishmalayalam tamil respectively solution rank first english eighth malayalam eleventh tamil
classic texts kurdish literature poems know meter poems helpful correct read better understand mean avoidance ambiguity paper present rule base method automatic classification poem meter central kurdish language metrical system kurdish poetry divide three class quantitative syllabic free verse vowel length phonemic language uncertainties syllable weight meter identification propose method generate possible situations consider line input poem common meter pattern kurdish poetry identify probable meter type pattern input poem evaluation method dataset vejinbooks kurdish corpus result nine hundred and seventy-three precision meter type nine hundred and sixty-two precision pattern identification
large scale question answer qa pair critical advance research areas like machine read comprehension question answer construct qa pair document require determine ask question correspond answer exist methods qa pair generation usually follow pipeline approach namely first choose likely candidate answer span generate answer specific question pipeline approach however undesired mine appropriate qa pair document since ignore connection question generation answer extraction may lead incompatible qa pair generation ie select answer span inappropriate question generation however human annotators take whole qa pair account consider compatibility question answer inspire motivation instead conventional pipeline approach propose model name onestop generate qa pair document one stop approach specifically question correspond answer span extract simultaneously process question generation answer extraction mutually affect additionally onestop much efficient train deploy industrial scenarios since involve one model solve complex qa generation task conduct comprehensive experiment three large scale machine read comprehension datasets squad newsqa dureader experimental result demonstrate onestop model outperform baselines significantly regard quality generate question quality generate question answer pair model efficiency
word segmentation part speech tag two critical preliminary step downstream task vietnamese natural language process reality people tend consider also phrase boundary perform word segmentation part speech tag rather solely process word word leave right paper implement idea improve word segmentation part speech tag vietnamese language employ simplify constituency parser neural model joint word segmentation part speech tag architecture syllable base crf constituency parser reduce complexity parse replace constituent label single label indicate phrase model augment predict word boundary part speech tag tool vietnamese chinese similar linguistic phenomena evaluate propose model augment versions three vietnamese benchmark datasets six chinese benchmark datasets experimental result show propose model achieve higher performances previous work languages
paper discuss result obtain different techniques apply perform sentiment analysis social media twitter code mix text write hinglish various stag involve perform sentiment analysis data consolidation data clean data transformation model various data clean techniques apply data clean five iterations result experiment conduct note iteration data transform use count vectorizer one hot vectorizer tf idf vectorizer doc2vec word2vec fasttext embeddings model create use various machine learn algorithms svm knn decision tree random forest naive bay logistic regression ensemble vote classifiers data obtain task codalab competition website list task9 semeval two thousand and twenty competition website model create evaluate use f1 score macro best f1 score six thousand, nine hundred and seven achieve use ensemble vote classifier
instantaneous growth text information retrieve domain orient information text data broad range applications information retrieval natural language process thematic keywords give compress representation text usually domain identification play significant role machine translation text summarization question answer information extraction sentiment analysis paper propose multichannel lstm cnn methodology technical domain identification telugu architecture use evaluate context icon share task techdofication two thousand and twenty task h system get six hundred and ninety-nine f1 score test dataset nine thousand and one validation set
word embed learn methods require large number occurrences word accurately learn embed however vocabulary oov word appear train corpus emerge frequently smaller downstream data recent work formulate oov embed learn shoot regression problem demonstrate meta learn improve result obtain however algorithm use model agnostic meta learn maml know unstable perform worse large number gradient step use parameter update work propose use leap meta learn algorithm leverage entire trajectory learn process instead begin end point thus ameliorate two issue experiment benchmark oov embed learn dataset extrinsic evaluation leap perform comparably better maml go examine contexts beneficial learn oov embed propose choice contexts may matter meta learn employ
evolution language hotly debate subject contradict hypotheses unreliable claim draw signal game dynamic population mechanics machine learn algebraic topology present method detect evolutionary pattern sociological model language evolution develop minimalistic model provide rigorous base generalize evolutionary model language base communication individuals also discuss theoretical guarantee model range stability language representations fast convergence language temporal communication language drift interactive set present empirical result interpretations real world dataset rdt identify communities echo chamber opinions thus place obstructions reliable communication among communities
use task specific pre train leverage cross lingual transfer two popular ways handle code switch data paper aim compare effect task sentiment analysis work two dravidian code switch languages tamil engish malayalam english four different bert base model compare effect task specific pre train cross lingual transfer find task specific pre train result superior zero shoot supervise performance compare performance achieve leverage cross lingual transfer multilingual bert model
probe classifiers emerge one prominent methodologies interpret analyze deep neural network model natural language process basic idea simple classifier train predict linguistic property model representations use examine wide variety model properties however recent study demonstrate various methodological weaknesses approach article critically review probe classifiers framework highlight shortcomings improvements alternative approach
work present large scale analysis artificial intelligence ai machine learn ml reference within news article scientific publications two thousand and eleven two thousand and nineteen implement word association measurements automatically identify shift language co occur ai ml quantify strength word associations result highlight evolution perceptions definitions around ai ml detect emerge application areas model systems eg blockchain cybersecurity recent small scale manual study explore ai ml discourse within general public policymaker community researcher community limit scalability longevity methods provide new view public perceptions subject area expert discussions ai ml greatly exceed explanative power prior work
scope survey paper explore challenge automatic story generation hope contribute follow ways one explore previous research story generation address challenge two discuss future research directions new technologies may aid advancements three would light emerge often overlook challenge creativity discourse
rapid production data internet need understand users feel business research perspective prompt creation numerous automatic monolingual sentiment detection systems recently however due unstructured nature data social media observe instance multilingual code mix texts development content type create new demand code mix sentiment analysis systems study collect label thus create dataset persian english code mix tweet proceed introduce model use bert pretrained embeddings well translation model automatically learn polarity score tweet model outperform baseline model use nai bay random forest methods
paper introduce systems three subtasks semeval two thousand and twenty-one task four read comprehension abstract mean help model better represent understand abstract concepts natural language well design many simple effective approach adapt backbone model roberta specifically formalize subtasks multiple choice question answer format add special tokens abstract concepts final prediction question answer consider result subtasks additionally employ many finetuning trick improve performance experimental result show approach achieve significant performance compare baseline systems approach achieve eighth rank subtask one tenth rank subtask two
appraisal theories explain cognitive evaluation event lead particular emotion contrast theories basic emotions affect valence arousal theory receive lot attention natural language process yet psychology prove powerful smith ellsworth one thousand, nine hundred and eighty-five show appraisal dimension attention certainty anticipate effort pleasantness responsibility control situational control discriminate least fifteen emotion class study different annotation strategies dimension base event focus enisear corpus troiano et al two thousand and nineteen analyze two manual annotation settings one show text annotate mask experience emotion label two reveal emotion associate text set two enable annotators develop realistic intuition describe event set one standard annotation procedure purely rely text evaluate strategies two ways measure inter annotator agreement fine tune roberta predict appraisal variables result show knowledge emotion increase annotators reliability evaluate purely automatic rule base label strategy infer appraisal annotate emotion class train automatically assign label lead competitive performance classifier even test manual annotations indicator might possible automatically create appraisal corpora every domain emotion corpora already exist
development language proficiency model non native learners active area interest nlp research past years although language proficiency multidimensional nature exist research typically consider single overall proficiency build model exist approach also consider one language time paper describe experiment observations role pre train fine tune multilingual embeddings perform multi dimensional multilingual language proficiency classification report experiment three languages german italian czech model seven dimension proficiency range vocabulary control sociolinguistic appropriateness result indicate fine tune embeddings useful multilingual proficiency model none feature achieve consistently best performance dimension language proficiency code data relate supplementary material find https githubcom nishkalavallabhi multidimcefrscoring
distant supervision allow obtain label train corpora low resource settings limit hand annotate data exist however use effectively distant supervision must easy gather work present anea tool automatically annotate name entities texts base entity list span whole pipeline obtain list analyze errors distant supervision tune step allow user improve automatic annotation linguistic insights without label check tokens manually six low resource scenarios show f1 score increase average eighteen point distantly supervise data obtain anea
paper explore learn rich self supervise entity representations large amount associate text pre train model become applicable multiple entity centric task rank retrieval knowledge base completion question answer unlike methods harvest self supervision signal base merely local context within sentence radically expand notion context include available text relate entity enable new class powerful high capacity representations ultimately distill much useful information entity multiple text source without human supervision present several train strategies unlike prior approach learn jointly predict word entities strategies compare experimentally downstream task tv movies domain movielens tag prediction user review natural language movie search evidence result model match outperform competitive baselines sometimes little fine tune scale large corpora finally make datasets pre train model publicly available include reviews2movielens see https google research docent map 1b word corpus amazon movie review mcauley two thousand and sixteen movielens tag harper konstan two thousand and sixteen well reddit movie suggestions see https urikzgithubio docent natural language query correspond community recommendations
paper examine gender age salience stereotypicality british english talk aim predict gender age categories base lexical phrasal turn take feature examine spokenbnc corpus around one hundred and fourteen million word british english conversations identify behavioural differences speakers label gender age categories explore differences language use turn take dynamics identify range characteristics set categories apart find female speakers tend produce slightly longer turn turn male speakers feature higher type token ratio distinct range minimal particles eh uh across age group observe instance swear word laughter characterize young speakers talk old speakers tend produce truncate word use observe characteristics predict gender age label speakers per conversation per turn classification task show non lexical utterances minimal particles usually leave dialog data contribute set categories apart
incident stream track research challenge aim find important information social media crises emergency response purpose specifically give stream crisis relate tweet challenge ask participate system one classify type users concern need express tweet know information type classification task two estimate critical tweet regard emergency response know priority level prediction task paper describe multi task transfer learn approach challenge approach leverage state art transformer model include encoder base model bert sequence sequence base t5 joint transfer learn two task base approach submit several run track return evaluation result show run substantially outperform participate run classification priority level prediction
mitigate negative effect low quality train data performance neural machine translation model exist strategies focus filter harmful data train start paper explore strategies dynamically optimize data usage train process use model gradients small set clean data train step algorithm calculate gradient alignment train data clean data mask data negative alignment method natural intuition good train data update model parameters similar direction clean data experiment three wmt language pair show method bring significant improvement strong baselines improvements generalizable across test data different domains
job learn consist continuously learn use production open environment mean system deal situations elements never see kind systems seem especially adapt job learn dialogue systems since take advantage interactions users collect feedback adapt improve components time dialogue systems perform job learn build evaluate general methodology yet define thus paper propose first general methodology evaluate job learn dialogue systems also describe task orient dialogue system improve job natural language component user interactions finally evaluate system describe methodology
creation task orient conversational data costly data augmentation techniques propose create synthetic data improve model performance new domains learn base techniques eg paraphrase still require moderate amount data make application low resource settings infeasible tackle problem introduce augmentation framework create synthetic task orient dialogues operate five shots framework utilize belief state annotations define dialogue function turn pair create templates pair de lexicalization dialogue function codify allowable incoming outgo link template generate new dialogues framework compose allowable adjacent templates bottom manner evaluate framework use trade base dst model observe significant improvements fine tune scenarios within low resource set conclude end end dialogue augmentation framework practical tool natural language understand performance emerge task orient dialogue domains
train large unfiltered crawl internet language model pick reproduce kinds undesirable bias find data often generate racist sexist violent otherwise toxic language large model often require millions train examples achieve good performance difficult completely prevent expose content paper investigate whether pretrained language model least know exhibit undesirable bias produce toxic content base find propose decode algorithm reduce probability model produce problematic text give textual description undesired behavior algorithm rely manually curated word list require train data change model parameters approach mean eliminate issue language model generate bias text believe important step direction
increase accessibility internet facilitate social media usage encourage individuals express opinions liberally nevertheless also create place content polluters disseminate offensive post content offensive post write cross lingual manner easily evade online surveillance systems paper present automate system identify offensive text multilingual code mix data task datasets provide three languages include tamil malayalam kannada code mix english participants ask implement separate model language accomplish task employ two machine learn techniques lr svm three deep learn lstm lstmattention techniques three transformers bert indic bert xlm r base methods result show xlm r outperform techniques tamil malayalam languages bert achieve highest score kannada language propose model gain weight f1 score seventy-six tamil ninety-three malayalam seventy-one kannada rank 3rd 5th 4th respectively
recent years several systems develop regulate spread negativity eliminate aggressive offensive abusive content online platforms nevertheless limit number research carry identify positive encourage supportive content work goal identify whether social media post comment contain hope speech propose three distinct model identify hope speech english tamil malayalam language serve purpose attain goal employ various machine learn support vector machine logistic regression ensemble deep learn convolutional neural network long short term memory transformer bert indic bert xlnet xlm roberta base methods result indicate xlm roberta outdo techniques gain weight f1 score ninety-three sixty eighty-five respectively english tamil malayalam language team achieve 1st 2nd 1st rank three task respectively
acronym disambiguation ad task aim find correct expansions ambiguous ancronym give sentence although convenient use acronyms sometimes could difficult understand identify appropriate expansions acronym practical task natural language process since work do ad scientific field propose binary classification model incorporate bert several train strategies include dynamic negative sample selection task adaptive pretraining adversarial train pseudo label paper experiment sciad show effectiveness propose model score rank 1st sduaaai twenty-one share task two acronym disambiguation
bidirectional encoder representations transformers bert show promise way dramatically improve performance across various natural language process task devlin et al two thousand and nineteen meanwhile progress make past years various neural net work also prove effectiveness neural network field natural language process project roberta wwm ext cui et al two thousand and nineteen pre train language model adopt fine tune chinese text classification model able classify chinese texts two categories contain descriptions legal behavior descriptions illegal behavior four different model also propose paper model use roberta wwm extas embed layer fee embed different neural network motivation hind propose model straightforward introduce complex output layer architecture overall performance model could improve model train data set derive chinese public court record performance different model comparedthe experiment show performance pro pose model fail beat original roberta wwm ext model term accuracy train efficiency
success pre train transformer language model bring great deal interest model work learn language however prior research field mainly devote english little know regard languages end introduce rusenteval enhance set fourteen probe task russian include ones explore yet apply combination complementary probe methods explore distribution various linguistic properties five multilingual transformers two typologically contrast languages russian english result provide intrigue find contradict common understand linguistic knowledge represent demonstrate properties learn similar manner despite language differences
misinformation covid nineteen prevalent social media pandemic unfold associate risk extremely high thus critical detect combat misinformation recently deep learn model use natural language process techniques bert bidirectional encoder representations transformers achieve great successes detect misinformation paper propose explainable natural language process model base distilbert shap shapley additive explanations combat misinformation covid nineteen due efficiency effectiveness first collect dataset nine hundred and eighty-four claim covid nineteen fact check augment data use back translation double sample size dataset distilbert model able obtain good performance accuracy nine hundred and seventy-two areas curve nine hundred and ninety-three detect misinformation covid nineteen model also test larger dataset aaai2021 covid nineteen fake news detection share task obtain good performance accuracy nine hundred and thirty-eight areas curve nine hundred and eighty-five performance datasets better traditional machine learn model second order boost public trust model prediction employ shap improve model explainability evaluate use subject experiment three condition ie text textshap explanation tse textshap explanationsource evidence tsese participants significantly likely trust share information relate covid nineteen tse tsese condition condition result provide good implications detect misinformation covid nineteen improve public trust
abstractive summarization task compress long document coherent short document retain salient information modern abstractive summarization methods base deep neural network often require large train datasets since collect summarization datasets expensive time consume task practical industrial settings usually low resource paper study challenge low resource set summarize long legal brief average source document length four thousand, two hundred and sixty-eight word one hundred and twenty available document summary pair account data scarcity use modern pretrained abstractive summarizer bart lewis et al two thousand and twenty achieve one hundred and seventy-nine rouge l struggle long document thus attempt compress long document identify salient sentence source best grind summary use novel algorithm base gpt two radford et al two thousand and nineteen language model perplexity score operate within low resource regime feed compress document bart observe sixty rouge l improvement method also beat several competitive salience detection baselines furthermore identify salient sentence tend agree independent human label domain experts
work construct largest dataset multimodal pretraining chinese consist 19tb image 292gb texts cover wide range domains propose cross modal pretraining method call m6 refer multi modality multi modality multitask mega transformer unify pretraining data single modality multiple modalities scale model size ten billion one hundred billion parameters build largest pretrained model chinese apply model series downstream applications demonstrate outstanding performance comparison strong baselines furthermore specifically design downstream task text guide image generation show finetuned m6 create high quality image high resolution abundant detail
significant progress towards develop nlu resources indic languages syntactic evaluation relatively less explore unlike english indic languages rich morphosyntax grammatical genders free linear word order highly inflectional morphology paper introduce vyakarana benchmark gender balance colorless green sentence indic languages syntactic evaluation multilingual language model benchmark comprise four syntax relate task pos tag syntax tree depth prediction grammatical case mark subject verb agreement use datasets evaluation task probe five multilingual language model vary architectures syntax indic languages due prevalence also include code switch set experiment result show token level sentence level representations indic language model indicbert muril capture syntax indic languages efficiently highly multilingual language model layer wise probe experiment reveal mbert distilmbert xlm r localize syntax middle layer indic language model show syntactic localization
paper tackle nuanced arabic dialect identification nadi share task abdul mageed et al two thousand and twenty-one demonstrate state art result four subtasks task identify geographic origin short dialectal da modern standard arabic msa utterances level country province final model ensemble variants build top marbert achieve f1 score three thousand, four hundred and three da country level development set improvement seven hundred and sixty-three previous work
contact trace globally adopt fight control infection rate covid nineteen thank digital technologies smartphones wearable devices contact covid nineteen patients easily trace inform potential exposure virus aim several interest mobile applications develop however ever grow concern work mechanism performance applications literature already provide interest exploratory study community response applications analyze information different source news users review applications however best knowledge exist solution automatically analyze users review extract evoke sentiments work propose pipeline start manual annotation via crowd source study conclude development train ai model automatic sentiment analysis users review total employ eight different methods achieve average f1 score nine hundred and forty-eight indicate feasibility automatic sentiment analysis users review covid nineteen contact trace applications also highlight key advantage drawbacks users concern applications moreover also collect annotate large scale dataset compose thirty-four thousand, five hundred and thirty-four review manually annotate contract trace applications forty-six distinct countries present analysis dataset expect provide baseline benchmark future research domain
recent complementary strand research show leverage information data source encode properties embeddings lead performance increase train single model heterogeneous data source however remain unclear situations dataset embeddings effective use large variety settings languages task furthermore usually assume gold information data source available test data distribution see train work compare effect dataset embeddings mono lingual settings multi lingual settings predict data source label zero shoot set evaluate three morphosyntactic task morphological tag lemmatization dependency parse use one hundred and four datasets sixty-six languages two different dataset group strategies performance increase highest datasets language know distribution test instance draw contrast setups data unseen distribution performance increase vanish
evaluation multi turn dialogues remain challenge common approach label user satisfaction experience dialogue level reflect task difficulty therefore assign experience score two task different complexity level mislead another approach suggest evaluate dialogue turn independently ignore turn long term influence final user experience dialogue instead develop new method estimate turn level satisfaction dialogue context sensitive long term view approach data drive make easily personalize interactions users dialogue systems formulate use budget consumption setup assume user initial interaction budget conversation base task complexity dialogue turn cost task complete budget run user quit interaction demonstrate effectiveness method extensive experimentation simulate dialogue platform realistic dialogue dataset
despite recent successes transformer base model term effectiveness variety task decisions often remain opaque humans explanations particularly important task like offensive language toxicity detection social media manual appeal process often place dispute automatically flag content work propose technique improve interpretability model base simple powerful assumption post least toxic toxic span incorporate assumption transformer model score post base maximum toxicity span augment train process identify correct span find approach effective produce explanations exceed quality provide logistic regression analysis often regard highly interpretable model accord human study
emotion dynamics framework measure individual emotions change time powerful tool understand behave interact world paper introduce framework track emotion dynamics one utterances specifically introduce number utterance emotion dynamics ued metrics inspire work psychology use approach trace emotional arc movie character analyze thousands character arc test hypotheses inform broader understand stories notably show tendency character use increasingly negative word become increasingly emotionally discordant ninety percent narrative length ued also applications behavior study social sciences public health
guidelines application snacs semantic network adposition case supersenses schneider et al two thousand and eighteen modern standard hindi delhi snacs inventory fifty supersenses semantic label label use adpositions case markers respect lexical semantic function relation underlie context english guidelines schneider et al two thousand and twenty use model document besides case system hindi extremely rich adpositional system build oblique genitive productive incorporation loanwords even present day hinglish document align version twenty-five english guidelines
present unsupervised word segmentation model learn objective maximize generation probability sentence give possible segmentation generation probability factorize likelihood possible segment give context recursive way order better capture long short term dependencies propose use bi directional neural language model better capture feature segment context two decode algorithms also describe combine context feature directions generate final segmentation help reconcile word boundary ambiguities experimental result show context sensitive unsupervised segmentation model achieve state art different evaluation settings various data set chinese comparable result thai
product description generation pdg user care aspect critical recommendation system improve user experience also obtain click high quality customer review consider ideal source mine user care aspects however reality large number new products know long tail commodities gather sufficient amount customer review bring big challenge product description generation task exist work tend generate product description solely base item information ie product attribute title word lead tedious content attract customers effectively tackle problem propose adaptive posterior network base transformer architecture utilize user care information customer review specifically first extend self attentive transformer encoder encode product title attribute apply adaptive posterior distillation module utilize useful review information integrate user care aspects generation process finally apply transformer base decode phase copy mechanism automatically generate product description besides also collect large scare chinese product description dataset support work research field experimental result show model superior traditional generative model automatic indicators human evaluation
two thousand and twenty us elections ever characterize social media campaign mutual accusations investigate paper manifest also online communication supporters candidates biden trump utter hateful offensive communication formulate annotation task join task hateful offensive speech detection stance detection annotate three thousand tweet campaign period express particular stance towards candidate next establish class favorable add mix neutral stances also annotate candidate mention without opinion expression annotate tweet write offensive style enable us analyze supporters joe biden democratic party communicate differently supporters donald trump republican party bert baseline classifier show detection somebody supporter candidate perform high quality eighty-nine f1 trump ninety-one f1 biden detection somebody express candidate challenge seventy-nine f1 sixty-four f1 respectively automatic detection hate offensive speech remain challenge fifty-three f1 corpus publicly available constitute novel resource computational model offensive language consideration stances
humans judge affective content texts also implicitly assess correctness judgment confidence hypothesize people inconfidence perform well annotation task lead disagreements among true confidence may serve diagnostic tool systematic differences annotations probe assumption conduct study subset corpus contemporary american english ask raters distinguish neutral sentence emotion bear ones score confidence answer confidence turn approximate inter annotator disagreements find confidence correlate emotion intensity perceive stronger affect text prompt annotators certain classification performances insight relevant model study intensity open question wether automatic regressors classifiers actually predict intensity rather human self perceive confidence
paper present strategy tackle eacl wanlp two thousand and twenty-one share task two sarcasm sentiment detection one subtasks aim develop system identify whether give arabic tweet sarcastic nature aim identify sentiment arabic tweet approach task two step first step involve pre process provide arsarcasm v2 dataset perform insertions deletions segmentation operations various part text second step involve experiment multiple variants two transformer base model araelectra arabert final approach rank seventh fourth sarcasm sentiment detection subtasks respectively
paper describe recently initiate research project aim support development computerise dialogue systems handle breach conversational norms gricean maxims describe dialogue participants ideally form utterances order informative relevant brief etc approach model dialogue norms co operate distribute grammar systems cdgss develop methods detect breach handle dialogue systems verbal human robot interaction
ongoing debate nlp community whether modern language model contain linguistic knowledge recover call textitprobes paper study whether linguistic knowledge necessary condition good performance modern language model call textitrediscovery hypothesis first place show language model significantly compress perform well pretraining objectives retain good score probe linguistic structure result support rediscovery hypothesis lead second contribution paper information theoretic framework relate language model objective linguistic information framework also provide metric measure impact linguistic information word prediction task reinforce analytical result various experiment synthetic real task
paper introduce large scale multimodal multilingual dataset aim facilitate research ground word image contextual usage language dataset consist image select unambiguously illustrate concepts express sentence movie subtitle dataset valuable resource image align text fragment rather whole sentence ii multiple image possible text fragment sentence iii sentence free form real world like iv parallel texts multilingual set fill blank game humans evaluate quality automatic image selection process dataset show utility dataset two automatic task fill blank ii lexical translation result human evaluation automatic model demonstrate image useful complement textual context dataset benefit research visual ground word especially context free form sentence
transformers state art model variety sequence model task core attention function model pairwise interactions input every timestep attention powerful scale efficiently long sequence due quadratic time space complexity sequence length propose rfa linear time space attention use random feature methods approximate softmax function explore application transformers rfa use drop replacement conventional softmax attention offer straightforward way learn recency bias optional gate mechanism experiment language model machine translation demonstrate rfa achieve similar better performance compare strong transformer baselines machine translation experiment rfa decode twice fast vanilla transformer compare exist efficient transformer variants rfa competitive term accuracy efficiency three long text classification datasets analysis show rfa efficiency gain especially notable long sequence suggest rfa particularly useful task require work large input fast decode speed low memory footprints
fine tune know improve nlp model adapt initial model train plentiful less domain salient examples data target domain domain adaptation typically do use one stage fine tune demonstrate gradually fine tune multi stage process yield substantial gain apply without modify model learn objective
linear embed transformation show effective zero shoot cross lingual transfer task achieve surprisingly promise result however cross lingual embed space map usually study static word level embeddings space transformation derive align representations translation pair refer dictionaries move line investigate contextual embed alignment approach sense level dictionary free enhance quality map also provide deep view properties contextual embeddings ie anisotropy problem solution experiment zero shoot dependency parse concept share space build embed transformation substantially outperform state art methods use multilingual embeddings
data augmentation attract lot research attention deep learn era ability alleviate data sparseness lack data unseen evaluation databases exactly major challenge cross domain text sql parse previous work either require human intervention guarantee quality generate data fail handle complex sql query paper present simple yet effective data augmentation framework first give database automatically produce large amount sql query base abstract syntax tree grammar require generate query cover least eighty sql pattern train data better distribution match second propose hierarchical sql question generation model obtain high quality natural language question major contribution work experiment three cross domain datasets ie wikisql spider english dusql chinese show propose data augmentation framework consistently improve performance strong baselines particular hierarchical generation model key improvement
technical report propose algorithm call lex2vec exploit lexical resources inject information word embeddings name embed dimension mean distant supervision evaluate optimal parameters extract number informative label readable good coverage embed dimension
compound probabilistic context free grammars c pcfgs recently establish new state art phrase structure grammar induction however due high time complexity chart base representation inference difficult investigate comprehensively work rely fast implementation c pcfgs conduct evaluation complementary ofcitetkim etal two thousand and nineteen compound highlight three key find one c pcfgs data efficient two c pcfgs make best use global sentence level information preterminal rule probabilities three best configurations c pcfgs english always generalize morphology rich languages
metric base learn well know family methods shoot learn especially computer vision recently use many natural language process applications slot tag paper explore metric base learn methods slot tag task propose novel metric base learn architecture attentive relational network propose method extend relation network make suitable natural language process applications general leverage pretrained contextual embeddings elmo bert use attention mechanism result snip data show propose method outperform state art metric base learn methods
enrich language model domain knowledge crucial difficult base world largest public academic graph open academic graph oag pre train academic language model namely oag bert integrate massive heterogeneous entities include paper author concept venue affiliation better endow oag bert ability capture entity information develop novel pre train strategies include heterogeneous entity type embed entity aware 2d positional encode span aware entity mask zero shoot inference design special decode strategy allow oag bert generate entity name scratch evaluate oag bert various downstream academic task include nlp benchmarks zero shoot entity inference heterogeneous graph link prediction author name disambiguation result demonstrate effectiveness propose pre train approach comprehend academic texts model knowledge heterogeneous entities oag bert deploy multiple real world applications reviewer recommendations paper tag aminer system also available public cogdl package
nlc2cmd competition host neurips two thousand and twenty aim bring power natural language process command line participants task build model transform descriptions command line task english bash syntax report competition detail task metrics data attempt solutions lessons learn
paper propose chinese multi turn topic drive conversation dataset naturalconv allow participants chat anything want long element topic mention topic shift smooth corpus contain 199k conversations six domains 400k utterances average turn number two hundred and one conversations contain depth discussions relate topics widely natural transition multiple topics believe either way normal human conversation facilitate research corpus provide result several benchmark model comparative result show dataset current model able provide significant improvement introduce background knowledge topic therefore propose dataset good benchmark research evaluate validity naturalness multi turn conversation systems dataset available https aitencentcom ailab nlp dialogue datasets
podcast episodes often contain material extraneous main content advertisements interleave within audio write descriptions present classifiers leverage textual listen pattern order detect content podcast descriptions audio transcripts demonstrate model effective evaluate downstream task podcast summarization show substantively improve rouge score reduce extraneous content generate summaries
speak language understand slu aim extract semantics frame user query core component task orient dialog system burst deep neural network evolution pre train language model research slu obtain significant breakthroughs however remain lack comprehensive survey summarize exist approach recent trend motivate work present article paper survey recent advance new frontiers slu specifically give thorough review research field cover different aspects include one new taxonomy provide new perspective slu file include single model vs joint model implicit joint model vs explicit joint model joint model non pre train paradigm vs pre train paradigm2 new frontiers emerge areas complex slu well correspond challenge three abundant open source resources help community collect organize relate paper baseline project leaderboard public website slu researchers could directly access recent progress hope survey would light future research slu field
paper propose decode strategy end end simultaneous speech translation leverage end end model train offline mode conduct empirical study two language pair english german english portuguese also investigate different output token granularities include character byte pair encode bpe units result show propose decode approach allow control bleu average lag trade along different latency regimes best decode settings achieve comparable result strong cascade model evaluate simultaneous translation track iwslt two thousand and twenty share task
aspect base sentiment analysis absa many neural model equip attention mechanism quantify contribution context word sentiment prediction however mechanism suffer one drawback frequent word sentiment polarities tend take consideration final sentiment decision abundant infrequent sentiment word ignore model deal issue propose progressive self supervise attention learn approach attentional absa model approach iteratively perform sentiment prediction train instance continually learn useful attention supervision information meantime train iteration context word highest impact sentiment prediction identify base attention weight gradients extract word active mislead influence correct incorrect prediction instance word extract way mask subsequent iterations exploit extract word refine absa model augment conventional train objective regularization term encourage absa model take full advantage extract active context word also decrease weight mislead word integrate propose approach three state art neural absa model experiment result depth analyse show approach yield better attention result significantly enhance performance three model release source code train model https githubcom deeplearnxmu pssattention
one biggest bottleneck build accurate high coverage neural open ie systems need large label corpora diversity open domain corpora variety natural language expressions exacerbate problem paper propose syntactic semantic drive learn approach learn neural open ie model without human label data leverage syntactic semantic knowledge noisier higher level supervisions specifically first employ syntactic pattern data label function pretrain base model use generate label propose syntactic semantic drive reinforcement learn algorithm effectively generalize base model open situations high accuracy experimental result show approach significantly outperform supervise counterparts even achieve competitive performance supervise state art soa model
relation extraction type information extraction task recognize semantic relationships entities sentence many previous study focus extract one semantic relation two entities single sentence however multiple entities sentence associate various relations address issue propose relation extraction model base dual pointer network multi head attention mechanism propose model find n one subject object relations use forward object decoder find one n subject object relations use backward subject decoder experiment confirm propose model outperform previous model f1 score eight hundred and eight ace two thousand and five corpus f1 score seven hundred and eighty-three nyt corpus
community base question answer cqa platforms automatic answer rank give question critical find potentially popular answer early time mainstream approach learn generate answer rank score base match degree question answer representations well influence respondents however encounter two main limitations one correlations answer question often overlook two question respondent representations build independently specific answer affect answer representations address limitations devise novel graph base tri attention network namely gtan two innovations first gtan propose construct graph question learn answer correlations graph graph neural network gnns second base representations learn gnns alternate tri attention method develop alternatively build target aware respondent representations answer specific question representations context aware answer representations attention computation gtan finally integrate representations generate answer rank score experiment three real world cqa datasets demonstrate gtan significantly outperform state art answer rank methods validate rationality network architecture
although previous research aspect base sentiment analysis absa indonesian review hotel domain conduct use cnn xgboost model generalize well test data high number oov word contribute misclassification case nowadays state art result wide array nlp task achieve utilize pretrained language representation paper intend incorporate one foremost language representation model bert perform absa indonesian review dataset combine multilingual bert bert task transformation method manage achieve significant improvement eight f1 score compare result previous study
paper propose multi document summarization system use semantic role label srl semantic graph indonesian news article order improve exist summarizer system modify summarizer employ subject predicate object adverbial svoa extraction predicate argument structure pas extraction svoa extraction replace srl model indonesian also replace genetic algorithm identify important pas decision tree classifier since summarizer without genetic algorithm give better performance decision tree model employ identify important pas decision tree model ten feature achieve better performance decision tree four sentence feature experiment evaluations conduct generate one hundred word summary two hundred word summary evaluation show propose model get three hundred and thirteen average rouge two recall one hundred word summary three hundred and ninety-four average rouge two recall two hundred word summary
explain outcome deep learn decisions base affect challenge necessary expect social companion robots interact users emotional level paper present commonsense approach utilize interpretable hybrid neural symbolic system associate extract target noun chunk determine associate express emotion affective label natural language expression leverage pre train neural network well adapt tree sub tree process dependency tree lstm learn affect label dynamic target determine symbolic rule natural language find make use unique properties recursive network provide higher accuracy interpretability compare unstructured sequential methods determine target affect associations aspect base sentiment analysis task
limerick generation exemplify difficult challenge face poetry generation poems must tell story five line constraints rhyme stress meter address challenge introduce limgen novel fully automate system limerick generation outperform state art neural network base poetry model well prior rule base poetry model limgen consist three important piece adaptive multi templated constraint algorithm constrain search space realistic poems multi templated beam search algorithm search efficiently space probabilistic storyline algorithm provide coherent storylines relate user provide prompt word result limericks satisfy poetic constraints thematically coherent storylines sometimes even funny lucky
one strategy facilitate read comprehension present information question answer format demo system integrate task question answer qa question generation qg order produce qanda items convey content multi paragraph document report experiment qa qg yield improvements task assess interact produce list qanda items text demo accessible qnasdlcom
modern natural language understand model depend pretrained subword embeddings applications may need reason word never rarely see pretraining show examples depend critically rarer word challenge natural language inference model explore model could learn use definitions provide natural text overcome handicap model understand definition usually weaker well model word embed recover performance gap use completely untrained word
present framework measure compositional inductive bias model context emergent communications devise corrupt compositional grammars probe limitations compositional inductive bias frequently use model use corrupt compositional grammars compare contrast wide range model compare choice soft gumbel discrete representations propose hierarchical model might show inductive bias towards relocatable atomic group tokens thus potentially encourage emergence word experiment probe compositional inductive bias sender receiver network isolation also place end end auto encoder
essay submissions standardize test like act occasionally include reference bully self harm violence form disturb content graders must take great care identify case like decide whether alert authorities behalf students may danger grow need robust computer systems support human decision makers automatically flag potential instance disturb content paper describe mtlhealth disturb content detection pipeline build around recent advance computational linguistics particularly pre train language model transformer network
pre train language model like bert achieve superior performances various nlp task without explicit consideration syntactic information meanwhile syntactic information prove crucial success nlp applications however incorporate syntax tree effectively efficiently pre train transformers still unsettle paper address problem propose novel framework name syntax bert framework work plug play mode applicable arbitrary pre train checkpoint base transformer architecture experiment various datasets natural language understand verify effectiveness syntax tree achieve consistent improvement multiple pre train model include bert roberta t5
enable empathetic behavior arabic dialogue agents important aspect build human like conversational model arabic natural language process see significant advance natural language understand nlu language model arabert natural language generation nlg remain challenge shortcomings nlg encoder decoder model primarily due lack arabic datasets suitable train nlg model conversational agents overcome issue propose transformer base encoder decoder initialize arabert parameters initialize weight encoder decoder arabert pre train weight model able leverage knowledge transfer boost performance response generation enable empathy conversational model train use arabicempatheticdialogues dataset achieve high performance empathetic response generation specifically model achieve low perplexity value one hundred and seventy increase five bleu point compare previous state art model also propose model rat highly eighty-five human evaluators validate high capability exhibit empathy generate relevant fluent responses open domain settings
paper present modern standard arabic msa sentence difficulty classifier predict difficulty sentence language learners use either cefr proficiency level binary classification simple complex compare use sentence embeddings different kinds fasttext mbert xlm r arabic bert well traditional language feature pos tag dependency tree readability score frequency list language learners best result achieve use fin tune arabic bert accuracy three way cefr classification f one eighty seventy-five arabic bert xlm r classification respectively seventy-one spearman correlation regression binary difficulty classifier reach f one ninety-four f one ninety-eight sentence pair semantic similarity classifier
text sql aim map natural language question sql query sketch base method combine execution guide eg decode strategy show strong performance wikisql benchmark however execution guide decode rely database execution significantly slow inference process hence unsatisfactory many real world applications paper present schema dependency guide multi task text sql model sdsql guide network effectively capture interactions question schemas propose model outperform exist methods settings without eg show schema dependency learn partially cover benefit eg alleviate need sdsql without eg significantly reduce time consumption inference sacrifice small amount performance provide flexibility downstream applications
argument mine often address pipeline method segmentation text argumentative units conduct first proceed argument component identification task research apply token level classification identify claim premise tokens new corpus argumentative essay write middle school students end compare variety state art model discrete feature deep learn architectures eg bilstm network bert base architectures identify argument components demonstrate bert base multi task learn architecture ie token sentence level classification adaptively pretrained relevant unlabeled dataset obtain best result
question answer systems usually use keyword search retrieve potential passages relate question extract answer passages machine read comprehension methods however many question tend unanswerable real world case significant challenge model determine answer support passage abstain answer exist systems design simple classifier determine answerability implicitly without explicitly model mutual interaction relation question passage lead poor performance determine unanswerable question tackle problem propose multi step co interactive relation network mcr net explicitly model mutual interaction locate key clue coarse fine introduce co interactive relation module co interactive relation module contain stack interaction fusion block continuously integrate fuse history guide current query guide clue explicit way experiment squad twenty dureader datasets show model achieve remarkable improvement outperform bert style baselines literature visualization analysis also verify importance mutual interaction question passage
propose structure extension bidirectional context conditional language generation infilling inspire frame semantic theory fillmore one thousand, nine hundred and seventy-six guidance provide two approach one model fine tune condition directly observe symbolic frame two novel extension disjunctive lexically constrain decode leverage frame semantic lexical units automatic human evaluations confirm frame guide generation allow explicit manipulation intend infill semantics minimal loss indistinguishability human generate text methods flexibly apply variety use scenarios provide interactive web demo available https nlpjhuedu demo
biomedical entity link task identify mention biomedical concepts text document map canonical entities target thesaurus recent advancements entity link use bert base model follow retrieve rerank paradigm candidate entities first select use retriever model retrieve candidates rank reranker model paradigm produce state art result slow train test time process one mention time mitigate issue propose bert base dual encoder model resolve multiple mention document one shoot show propose model multiple time faster exist bert base model competitive accuracy biomedical entity link additionally modify dual encoder model end end biomedical entity link perform mention span detection entity disambiguation perform two recently propose model
study automatic title generation present method generate domain control title scientific article good title allow get attention research deserve title interpret high compression description document contain information implement process domain control title use pre train text text transformer model additional token technique title tokens sample local distribution subset global vocabulary domain specific vocabulary global vocabulary thereby generate catchy title closely link correspond abstract generate title look realistic convince close grind truth perform automate evaluation use rouge metric human evaluation use five parameters make comparison human machine generate title title produce consider acceptable higher metric rat contrast original title thus conclude research propose promise method domain control title generation
edit base approach recently show promise result multiple monolingual sequence transduction task contrast conventional sequence sequence seq2seq model learn generate text scratch train parallel corpora methods prove much effective since able learn make fast accurate transformations leverage powerful pre train language model inspire ideas present tst simple efficient text simplification system base sequence tag leverage pre train transformer base encoders system make simplistic data augmentations tweak train inference pre exist system make less reliant large amount parallel train data provide control output enable faster inference speed best model achieve near state art performance benchmark test datasets task since fully non autoregressive achieve faster inference speed eleven time current state art text simplification system
interleave texts post belong different thread occur sequence commonly occur online chat post time consume quickly obtain overview discussions exist systems first disentangle post thread extract summaries thread major issue systems error propagation disentanglement component end end trainable summarization system could obviate explicit disentanglement systems require large amount label data address propose pretrain end end trainable hierarchical encoder decoder system use synthetic interleave texts show fine tune real world meet dataset ami system perform traditional two step system twenty-two also compare transformer model observe pretraining synthetic data encoder decoder outperform bertsumextabs transformer model pretrains encoder large dataset
paper deliver new perspective think utilize syntactic n grams sn grams sn grams type non linear n grams play critical role many nlp task introduce sn grams compare document semantics thus appeal application study report progress however proceed application find three major issue sn grams lack significance sensitive word order fail capture indirect syntactic relations address issue propose new variant sn grams name generalize phrase gps base gps propose topological approach name dscoh compute document semantic similarities dscoh extensively test document semantics comparison document cluster task experimental result show dscoh outperform state art embed base methods
pseudo label adopt method pre train automatic speech recognition asr model however performance suffer supervise teacher model degrade quality low resource setups domain transfer inspire successes contrastive representation learn computer vision speech applications recently supervise learn visual object propose contrastive semi supervise learn csl csl eschew directly predict teacher generate pseudo label favor utilize select positive negative examples challenge task transcribe public social media videos use csl reduce wer eight compare standard cross entropy pseudo label ce pl 10hr supervise data use annotate 75000hr videos wer reduction jump nineteen ultra low resource condition use 1hr label teacher supervision csl generalize much better domain condition show seventeen wer reduction compare best ce pl pre train model
previous researchers consider sentiment analysis document classification task input document classify predefined sentiment class although sentence document support important evidence sentiment analysis sentence treat document bag sentence word consider importance sentence document effectively determine polarity document sentence document deal different degrees importance address problem propose document level sentence classification model base deep neural network importance degrees sentence document automatically determine gate mechanisms verify new sentiment analysis model conduct experiment use sentiment datasets four different domains movie review hotel review restaurant review music review experiment propose model outperform previous state art model consider importance differences sentence document experimental result show importance sentence consider document level sentiment classification task
topics equally flammable term toxicity calm discussion turtle fish less often fuel inappropriate toxic dialogues discussion politics sexual minorities define set sensitive topics yield inappropriate toxic message describe methodology collect label dataset appropriateness toxicity user generate data well study aim define fine grain notion inappropriateness core inappropriateness harm reputation speaker different toxicity two respect inappropriateness topic relate ii inappropriate message toxic still unacceptable collect release two datasets russian topic label dataset appropriateness label dataset also release pre train classification model train data
sentiment analysis transition classify sentiment entire sentence provide contextual information target exist sentence sentiment individual target causal word responsible sentiment however lead elaborate requirements place datasets need train neural network joint triplet task determine entity sentiment causal word sentiment require kind data train systems problematic suffer stack subjective annotations domain fit lead poor model generalisation apply new contexts problems also likely compound attempt jointly determine additional contextual elements future mitigate problems present hybrid neural symbolic method utilise dependency tree lstm compositional sentiment parse structure complementary symbolic rule correctly extract target sentiment triplets sentence without need triplet train data show method potential perform line state art approach also simplify data require provide degree interpretability tree lstm
automatically identify fake news internet challenge problem deception detection task online news modify constantly propagation eg malicious users distort original truth make fake news however continuous evolution process would generate unprecedented fake news cheat original model present fake news evolution fne dataset new dataset track fake news evolution process dataset compose nine hundred and fifty pair data consist article represent three significant phase evolution process truth fake news evolve fake news observe feature evolution disinformation techniques text similarity top ten keywords classification accuracy part speech sentiment properties
neural machine translation nmt approach employ monolingual data show steady improvements resource rich condition however evaluations use real world low resource languages still result unsatisfactory performance work propose novel zero shoot nmt model approach learn without standard assumption pivot language share parallel data zero shoot source target languages approach base three stag initialization pre train nmt model observe least target language augmentation source side leverage target monolingual data learn optimize initial model zero shoot pair latter two constitute self learn cycle empirical find involve four diverse term language family script relatedness zero shoot pair show effectiveness approach five hundred and ninety-three bleu improvement supervise bilingual baseline compare unsupervised nmt consistent improvements observe even domain mismatch set attest usability method
speak language understand slu core component task orient dialogue system make substantial progress research single turn dialogue however performance multi turn dialogue still satisfactory sense exist multi turn slu methods low portability compatibility single turn slu model exist multi turn slu methods exploit historical predict result predict current utterance waste helpful information gap shortcomings paper propose novel result base portable framework slu rpfslu rpfslu allow exist single turn slu model obtain contextual information multi turn dialogues take full advantage predict result dialogue history current prediction experimental result public dataset kvret show slu model baselines acquire enhancement rpfslu multi turn slu task
emotion fundamental humanity ability perceive understand respond social interactions human like manner one desire capabilities artificial agents particularly social media bots past years computational understand detection emotional aspects language vital advance human computer interaction wassa share task two thousand and twenty-one release dataset news stories across two track track one empathy distress prediction track two multi dimension emotion prediction essay level describe system entry wassa two thousand and twenty-one share task track one track two leverage information pre train language model track specific task propose model achieve average pearson score four hundred and seventeen macro f1 score five hundred and two track one track two respectively share task leaderboard secure 4th rank track one 2nd rank track two
propose knowledge base approach extraction effect ce relations biomedical text approach combination unsupervised machine learn technique discover causal trigger set high precision linguistic rule identify effect arguments causal trigger evaluate approach use corpus fifty-eight thousand, seven hundred and sixty-one leukaemia relate pubmed abstract consist five hundred and sixty-eight thousand, five hundred and twenty-eight sentence could extract one hundred and fifty-two thousand, six hundred and fifty-five ce triplets corpus triplet consist phrase effect phrase causal trigger compare exist knowledge base semmeddb kilicoglu et al two thousand and twelve number extractions almost twice moreover propose approach outperform exist technique semrep rindflesch fiszman two thousand and three dataset five hundred sentence
relation extraction important task information extraction deal identify semantic relations entity mention traditionally relation extraction carry entity extraction pipeline fashion relation extraction focus determine whether semantic relation exist pair extract entity mention lead propagation errors entity extraction stage relation extraction stage also entity extraction carry without knowledge relations hence observe jointly perform entity relation extraction beneficial task paper survey various techniques jointly extract entities relations categorize techniques base approach adopt joint extraction ie whether employ joint inference joint model describe representative techniques joint inference joint model also describe two standard datasets evaluation techniques performance joint extraction approach datasets present brief analysis application general domain joint extraction approach biomedical dataset survey useful researchers well practitioners field information extraction cover broad landscape joint extraction techniques
acute respiratory distress syndrome ards life threaten condition often undiagnosed diagnose late ards especially prominent infect covid nineteen explore automatic identification ards indicators confound factor free text chest radiograph report present new annotate corpus chest radiograph report introduce hierarchical attention network sentence objectives hanso text classification framework hanso utilize fine grain annotations improve document classification performance hanso extract ards relate information high performance leverage relation annotations even annotate span noisy use annotate chest radiograph image gold standard hanso identify bilateral infiltrate indicator ards chest radiograph report performance eighty-seven f1 comparable human annotations eighty-four f1 algorithm could facilitate efficient expeditious identification ards clinicians researchers contribute development new therapies improve patient care
advance utility social media data research applications require methods automatically detect demographic information social media study populations include users age objective study develop evaluate method automatically identify exact age users base self report tweet end end automatic natural language process nlp pipeline reportage include query pattern retrieve tweet potentially mention age classifier distinguish retrieve tweet self report user exact age age tweet age tweet rule base extraction identify age develop evaluate reportage manually annotate eleven thousand tweet match query pattern base one thousand tweet annotate five annotators inter annotator agreement fleiss kappa eighty distinguish age age tweet ninety-five identify exact age among age tweet annotators agree deep neural network classifier base roberta large pretrained model achieve highest f1 score nine hundred and fourteen precision nine hundred and five recall nine hundred and forty-two age class age extraction evaluate use classifier predictions achieve f1 score eight hundred and fifty-five precision eight hundred and five recall nine hundred and fourteen age class evaluate directly hold test set achieve f1 score nine hundred and thirty-one precision eight hundred and seventy-three recall nine hundred and ninety-eight age class deploy reportage twelve billion tweet post two hundred and forty-five thousand, nine hundred and twenty-seven users predict age one hundred and thirty-two thousand, six hundred and thirty-seven fifty-four scale detection exact age large number users advance utility social media data research applications align predefined age group extant binary multi class classification approach
since leverage large amount unlabeled data without human supervision train model transfer knowledge target task self supervise learn de facto component recent success deep learn various field however many case discrepancy self supervise learn objective task specific objective order tackle discrepancy text sql task propose novel self supervise learn framework utilize task specific properties text sql task underlie structure table content train model learn useful knowledge textitheader column alignment task unlabeled table data able transfer knowledge supervise text sql train annotate sample model leverage knowledge better perform textitheader span alignment task predict sql statements experimental result show self supervise learn framework significantly improve performance exist strong bert base model without use large external corpora particular method effective train model scarce label data source code work available github
mediasum large scale media interview dataset consist 4636k transcripts abstractive summaries create dataset collect interview transcripts npr cnn employ overview topic descriptions summaries compare exist public corpora dialogue summarization dataset order magnitude larger contain complex multi party conversations multiple domains conduct statistical analysis demonstrate unique positional bias exhibit transcripts televise radio interview also show mediasum use transfer learn improve model performance dialogue summarization task
multilingual pre train language model eg mbert xlm xlm r show impressive performance cross lingual natural language understand task however model computationally intensive difficult deploy resource restrict devices paper propose simple yet effective distillation method lightmbert transfer cross lingual generalization ability multilingual bert small student model experiment result empirically demonstrate efficiency effectiveness lightmbert significantly better baselines perform comparable teacher mbert
question answer qa important use case voice assistants popular approach qa extractive read comprehension rc find answer span text passage however extractive answer often unnatural conversational context result suboptimal user experience work investigate conversational answer generation qa propose answerbart end end generative rc model combine answer generation multiple passages passage rank answerability moreover hurdle apply generative rc hallucinations answer factually inconsistent passage text leverage recent work summarization evaluate factuality experiment show answerbart significantly improve previous best publish result ms marco twenty-one nlgen twenty-five rouge l narrativeqa ninety-four rouge l
unsupervised pretraining integral part many natural language process systems transfer learn language model achieve remarkable result many downstream task clinical application medical code assignment diagnosis procedure cod infer lengthy clinical note hospital discharge summaries however clear pretrained model useful medical code prediction without architecture engineer paper conduct comprehensive quantitative analysis various contextualized language model performance pretrained different domains medical code assignment clinical note propose hierarchical fine tune architecture capture interactions distant word adopt label wise attention exploit label information contrary current trend demonstrate carefully train classical cnn outperform attention base model mimic iii subset frequent cod empirical find suggest directions improve medical code assignment application
recent research efforts nlp demonstrate distributional word vector space often encode stereotypical human bias racism sexism word representations ubiquitously use nlp model pipelines raise ethical issue jeopardize fairness language technologies exist large body work bias measure debiasing methods date platform would unify research efforts make bias measure debiasing representation space widely accessible work present debie first integrate platform one measure two mitigate bias word embeddings give embed space users choose predefined space upload ii bias specification users choose exist bias specifications create debie one compute several measure implicit explicit bias modify embed space execute two mutually composable debiasing model debie functionality access four different interfaces web application b desktop application c rest ful api command line application debie available debieinformatikuni mannheimde
sentiment analysis attract increase attention e commerce sentiment polarities underlie user review great value business intelligence aspect category sentiment analysis acsa review rat prediction rp two essential task detect fine coarse sentiment polarities consider sentiment aspectsacsa overall review ratingrp simultaneously potential improve overall performance acsa rp highly correlate usually employ jointly real world e commerce scenarios public datasets construct acsa rp separately may limit exploitation task address problem advance relate research present large scale chinese restaurant review dataset textbfas soon possible include forty-six thousand, seven hundred and thirty genuine review lead online offline o2o e commerce platform china besides five star scale rat review manually annotate accord sentiment polarities towards eighteen pre define aspect categories hope release dataset could would light field sentiment analysis moreover propose intuitive yet effective joint model acsa rp experimental result demonstrate joint model outperform state art baselines task
task orient dialogue systems aim help users achieve goals specific domains recent neural dialogue systems use entire dialogue history abundant contextual information accumulate multiple conversational turn however dialogue history become increasingly longer number turn increase thereby increase memory usage computational cost paper present dot domain state track simplify dialogue system task orient dialogue system use simplify input context instead entire dialogue history however neglect dialogue history result loss contextual information previous conversational turn address issue dot track domain state addition belief state use input context use simplify input dot improve inform rate success rate one hundred and nine point one hundred and twenty-four point respectively compare previous state art model multiwoz well know benchmark
paper explore effect language variants data size fine tune task type arabic pre train language model build three pre train language model across three variants arabic modern standard arabic msa dialectal arabic classical arabic addition fourth language model pre train mix three also examine importance pre train data size build additional model pre train scale set msa variant compare different model well eight publicly available model fine tune five nlp task span twelve datasets result suggest variant proximity pre train data fine tune data important pre train data size exploit insight define optimize system selection model study task
work look add new language multilingual nmt system unsupervised fashion utilization pre train cross lingual word embeddings seek exploit language independent multilingual sentence representation easily generalize new language use cross lingual embeddings word lookup decode yet entirely unseen source language process call blind decode blindly decode portuguese use basesystem contain several romance languages achieve score three hundred and sixty-four bleu portuguese english one hundred and twenty-eight bleu russian english attempt train map encoder sentence representation new target language use model autoencoder merely train translate portuguese portuguese freeze encoder achieve twenty-six bleu english portuguese twenty-eight bleu add artificial noise input lastly explore practical adaptation approach non iterative backtranslation exploit model ability produce high quality translations blind decode yield us three hundred and forty-six bleu english portuguese attain near parity model adapt real bilingual data
generate metaphors challenge task require proper understand abstract concepts make connections unrelated concepts deviate literal mean paper aim generate metaphoric sentence give literal expression replace relevant verbs base theoretically ground connection metaphors symbols propose method automatically construct parallel corpus transform large number metaphorical sentence gutenberg poetry corpus jacobs two thousand and eighteen literal counterpart use recent advance mask language model couple commonsense inference generation task incorporate metaphor discriminator guide decode sequence sequence model fine tune parallel data generate high quality metaphors human evaluation independent test set literal statements show best model generate metaphors better three well craft baselines sixty-six time average task base evaluation show human write poems enhance metaphors propose model prefer sixty-eight time compare poems without metaphors
propose straightforward vocabulary adaptation scheme extend language capacity multilingual machine translation model pave way towards efficient continual learn multilingual machine translation approach suitable large scale datasets apply distant languages unseen script incur minor degradation translation performance original language pair provide competitive performance even case possess monolingual data new languages
paper evaluate morphology base embeddings english russian languages despite interest introduction several morphology base word embed model past acclaim performance improvements word similarity language model task experiment observe stable preference two baseline model skipgram fasttext performance exhibit morphological embeddings average two baselines mention
interpretation anaphors depend antecedents semantic value anaphor eventually convey co specify value antecedent interestingly occur give syntactic position different anaphors may different set admissible antecedents differences basis categorization anaphoric expressions accord anaphoric capacity important determine set admissible antecedents represent process anaphoric capacity type anaphor empirical perspective constraints stem appear quite cogent generalisations exhibit universal character give cross linguistic validity conceptual point view turn relations among bind constraints involve non trivial cross symmetry lend modular nature provide strength plausibility universal character kind anaphoric bind constraints appear thus significant subset natural language knowledge usually refer bind theory paper provide integrate overview constraints hold pair nominal anaphors admissible antecedents base grammatical relations structure along increase interest neuro symbolic approach natural language paper seek contribute revive interest intrigue research topic
preregistration refer practice specify go expect find study carry study practice increasingly common medicine psychology rarely discuss nlp paper discuss preregistration detail explore nlp researchers could preregister work present several preregistration question different kinds study finally argue favour register report could provide firmer ground slow science nlp research goal paper elicit discussion nlp community hope synthesise general nlp preregistration form future research
low resource multilingual neural machine translation mnmt typically task improve translation performance one language pair aid high resource language pair paper propose two simple search base curricula order multilingual train data help improve translation performance conjunction exist techniques fine tune additionally attempt learn curriculum mnmt scratch jointly train translation system aid contextual multi arm bandits show flores low resource translation dataset learn curricula provide better start point fine tune improve overall performance translation system
large web crawl corpora represent excellent resource improve performance neural machine translation nmt systems across several language pair however since corpora typically extremely noisy use fairly limit current approach deal problem mainly focus filter use heuristics single feature language model score bi lingual similarity work present alternative approach learn weight multiple sentence level feature feature weight optimize directly task improve translation performance use score filter sentence noisy corpora effectively provide result apply technique build nmt systems use paracrawl corpus estonian english show beat strong single feature baselines hand design combinations additionally analyze sensitivity method different type noise explore learn weight generalize language pair use maltese english paracrawl corpus
relation prediction knowledge graph dominate embed base methods mainly focus transductive set unfortunately able handle inductive learn unseen entities relations present take advantage prior knowledge furthermore inference process easily explainable work propose one solution call bertrl bert base relational learn leverage pre train language model fine tune take relation instance possible reason paths train sample bertrl outperform sotas fifteen eighteen case inductive transductive settings meanwhile demonstrate strong generalization capability shoot learn explainable
paper study constrain text generation generate sentence certain pre condition focus commongen task generate text base set concepts representative task constrain text generation traditional methods mainly rely supervise train maximize likelihood target sentenceshowever global constraints common sense coverage incorporate likelihood objective autoregressive decode process paper consider use reinforcement learn address limitation measure global constraints include fluency common sense concept coverage comprehensive score serve reward reinforcement learn besides design guide decode method word fragment sentence level experiment demonstrate method significantly increase concept coverage outperform exist model various automatic evaluations
problem design nlp solvers math word problems mwp see sustain research activity steady gain test accuracy since exist solvers achieve high performance benchmark datasets elementary level mwps contain one unknown arithmetic word problems problems often consider solve bulk research attention move complex mwps paper restrict attention english mwps teach grade four lower provide strong evidence exist mwp solvers rely shallow heuristics achieve high performance benchmark datasets end show mwp solvers access question ask mwp still solve large fraction mwps similarly model treat mwps bag word also achieve surprisingly high accuracy introduce challenge dataset svamp create apply carefully choose variations examples sample exist datasets best accuracy achieve state art model substantially lower svamp thus show much remain do even simplest mwps
international library standards require cataloguers tediously input romanization catalogue record benefit library users without specific language expertise paper present first report result task automatic romanization undiacritized arabic bibliographic entries complex task require model arabic phonology morphology even semantics collect 25m word corpus parallel arabic romanize bibliographic entries benchmarked number model vary term complexity resource dependence best system reach eight hundred and ninety-three exact word romanization blind test set make data code publicly available
type token base embed architectures still compete lexical semantic change detection recent success type base model semeval two thousand and twenty task one raise question success token base model variety nlp task translate field investigate influence range variables cluster bert vectors show low performance largely due orthographic information target word encode even higher layer bert representations reduce influence orthography considerably improve bert performance
assess proper difficulty level read materials texts general first step towards effective comprehension learn study improve conventional methodology automatic readability assessment incorporate word mover distance wmd rank texts additional post process technique grind difficulty level give model result experiment three multilingual datasets filipino german english show post process technique outperform previous vanilla rank base model use svm
neural machine translation model brittle input noise current robustness techniques mostly adapt model exist noisy texts model generally fail face unseen noise performance degrade clean texts paper introduce idea visual context improve translation robustness noisy texts addition propose novel error correction train regime treat error correction auxiliary task improve robustness experiment english french english german translation show multimodality error correction train beneficial model robustness know new type errors keep quality clean texts
shoot text classification fundamental nlp task model aim classify text large number categories give train examples per category paper explore data augmentation technique particularly suitable train limit data shoot highly multiclass text classification set four diverse text classification task find common data augmentation techniques improve performance triplet network thirty average boost performance present simple train strategy call curriculum data augmentation leverage curriculum learn first train original examples introduce augment data train progress explore two stage gradual schedule find compare standard single stage train curriculum data augmentation train faster improve performance remain robust high amount noise augmentation
neural text generation suffer text degeneration issue repetition traditional methods focus truncate unreliable tail distribution address head part show might contain tedious even repetitive candidates high probability lead repetition loop also address issue human text always favor high probability word inspire work propose heuristic sample method propose use interquartile range predict distribution determine head part permutate rescale head inverse probability aim decrease probability tedious possibly repetitive candidates higher probability increase probability rational surprise candidates lower probability propose algorithm provide controllable variation predict distribution enhance diversity without compromise rationality distribution use pre train language model compare algorithm nucleus sample result show algorithm effectively increase diversity generate sample achieve close resemblance human text
aspect sentiment triplet extraction aste aim identify aspects review sentence along correspond opinion expressions sentiments emerge task fine grain opinion mine since aste consist multiple subtasks include opinion entity extraction relation detection sentiment classification critical challenge appropriately capture utilize associations among paper transform aste task multi turn machine read comprehension mtmrc task propose bidirectional mrc bmrc framework address challenge specifically devise three type query include non restrictive extraction query restrictive extraction query sentiment classification query build associations among different subtasks furthermore consider aspect sentiment triplet derive either aspect opinion expression design bidirectional mrc structure one direction sequentially recognize aspects opinion expressions sentiments obtain triplets direction identify opinion expressions first aspects last sentiments make two directions complement framework identify triplets comprehensively verify effectiveness approach conduct extensive experiment four benchmark datasets experimental result demonstrate bmrc achieve state art performances
neural semantic parse approach widely use question answer qa systems knowledge graph methods provide flexibility handle qa datasets complex query large number entities work propose novel framework name carton perform multi task semantic parse handle problem conversational question answer large scale knowledge graph framework consist stack pointer network extension context transformer model parse input question dialog history framework generate sequence action execute knowledge graph evaluate carton standard dataset complex sequential question answer carton outperform baselines specifically observe performance improvements f1 score eight ten question type compare previous state art logical reason question improvement eleven absolute point reach
paper present paraqa question answer qa dataset multiple paraphrase responses single turn conversation knowledge graph kg dataset create use semi automate framework generate diverse paraphrase answer use techniques back translation exist datasets conversational question answer kgs single turn multi turn focus question paraphrase provide one answer verbalization however paraqa contain five thousand question answer pair minimum two maximum eight unique paraphrase responses question complement dataset baseline model illustrate advantage multiple paraphrase answer commonly use metrics bleu meteor paraqa dataset publicly available persistent uri broader usage adaptation research community
explore create automate personalize feedback intelligent tutor system goal pinpoint correct incorrect concepts student answer order achieve better student learn gain although automatic methods provide personalize feedback exist explicitly inform students concepts answer correct incorrect approach involve decompose students answer use neural discourse segmentation classification techniques decomposition yield relational graph discourse units cover reference solutions student answer use infer relational graph structure neural classifier match student answer reference solutions generate personalize feedback although process completely automate data drive personalize feedback generate highly contextual domain aware effectively target student misconceptions knowledge gap test method dialogue base demonstrate approach result high quality feedback significantly improve student learn gain
vision language pre train vlp large scale image text pair recently witness rapid progress learn cross modal representations exist pre train methods either directly concatenate image representation text representation feature level input single stream transformer use two stream cross modal transformer align image text representation high level semantic space real world image text data observe easy image text pair align simple semantics modalities others may relate higher level abstraction therefore paper propose new pre train method semvlp jointly align low level high level semantics image text representations model pre train iteratively two prevalent fashion single stream pre train align fine grain feature level two stream pre train align high level semantics employ share transformer network pluggable cross modal attention module extensive set experiment conduct four well establish vision language understand task demonstrate effectiveness propose semvlp align cross modal representations towards different semantic granularities
widely use emojis social network heighten mitigate negate sentiment text emoji suggestions already exist many cross platform applications emoji predict solely base prominent word instead understand subject substance text paper showcase importance use twitter feature help model understand sentiment involve hence predict suitable emoji text hashtags application source like android etc two feature find important yet underused emoji prediction twitter sentiment analysis whole approach shortcoming understand emoji behavioral pattern propose balance dataset crawl additional twitter data include timestamp hashtags application source act additional attribute tweet data analysis neural network model performance evaluations depict use hashtags application source feature allow encode different information effective emoji prediction
infer probability distribution sentence word sequence key process natural language process word level language model lms widely adopt compute joint probabilities word sequence difficulty capture context long enough sentence probability estimation spe overcome recent study introduce train methods use sentence level noise contrastive estimation nce recurrent neural network rnns work attempt extend contextual spe aim estimate conditional sentence probability give previous text propose nce sample negative sentence independently previous text train model give higher probabilities sentence consistent textcolorbluethe context apply method simple word level rnn lm focus effect sentence level nce train rather network architecture quality estimation evaluate multiple choice cloze style question include human automatically generate question experimental result show propose method improve spe quality word level rnn lm
background term reproducibility crisis science nlp field become increasingly interest conscientious reproducibility result past years see impressive range new initiatives events active research area however field far reach consensus reproducibility define measure address diversity view currently increase rather converge focus contribution aim provide wide angle near possible complete snapshot current work reproducibility nlp delineate differences similarities provide pointers common denominators
previous ccg supertaggers usually predict categories use multi class classification despite simplicity internal structure categories usually ignore rich semantics inside structure may help us better handle relations among categories bring robustness exist supertaggers work propose generate categories rather classify category decompose sequence smaller atomic tag tagger aim generate correct sequence show finer view categories annotations different categories could share interactions sentence contexts could enhance propose category generator able achieve state art tag nine hundred and fifty-five accuracy parse eight hundred and ninety-eight label f1 performances standard ccgbank furthermore performances infrequent even unseen categories domain texts low resource language give promise result introduce generation model general ccg analyse
recent years machine speech speech speech text translation gain momentum thank advance artificial intelligence especially domains speech recognition machine translation quality applications commonly test automatic metrics bleu primarily goal assess improvements release context evaluation campaign however little know systems compare human performances similar communicative task performance systems perceive final users paper present result experiment aim evaluate quality simultaneous speech translation engine compare performance professional interpreters select framework develop assessment human interpreters use perform manual evaluation human machine performances sample find better performance human interpreters term intelligibility machine perform slightly better term informativeness limitations study possible enhancements choose framework discuss despite intrinsic limitations use framework represent first step towards user centric communication orient methodology evaluate simultaneous speech translation
present find result second nuanced arabic dialect identification share task nadi two thousand and twenty-one share task include four subtasks country level modern standard arabic msa identification subtask eleven country level dialect identification subtask twelve province level msa identification subtask twenty-one province level sub dialect identification subtask twenty-two share task dataset cover total one hundred provinces twenty-one arab countries collect twitter domain total fifty-three team twenty-three countries register participate task thus reflect interest community area receive sixteen submissions subtask eleven five team twenty-seven submissions subtask twelve eight team twelve submissions subtask twenty-one four team thirteen submissions subtask twenty-two four team
multilingual pretrained representations generally rely subword segmentation algorithms create share multilingual vocabulary however standard heuristic algorithms often lead sub optimal segmentation especially languages limit amount data paper take two major step towards alleviate problem first demonstrate empirically apply exist subword regularization methodskudo two thousand and eighteen provilkov et al two thousand and twenty fine tune pre train multilingual representations improve effectiveness cross lingual transfer second take full advantage different possible input segmentations propose multi view subword regularization mvr method enforce consistency predictions use input tokenized standard probabilistic segmentations result xtreme multilingual benchmarkhu et al two thousand and twenty show mvr bring consistent improvements twenty-five point use standard segmentation algorithms
massively multilingual machine translation mt show impressive capabilities include zero shoot translation low resource language pair however model often evaluate high resource languages assumption generalize low resource ones difficulty evaluate mt model low resource pair often due lack standardize evaluation datasets paper present menyo 20k first multi domain parallel corpus low resource yorub english yo en language pair standardize train test split benchmarking provide several neural mt nmt benchmarks dataset compare performance popular pre train massively multilingual mt model show almost case simple benchmarks outperform pre train mt model major gain bleu ninety-nine eighty-six en2yo achieve comparison facebook m2m one hundred google multilingual nmt respectively use menyo 20k fine tune generic model
episodic logicunscoped logical form el ulf semantic representation capture predicate argument structure well challenge aspects language within episodic logic formalism present first learn approach parse sentence ulfs use grow set annotate examples result provide strong baseline future improvement method learn sequence sequence model predict transition action sequence within modify cache transition system evaluate efficacy type grammar base constraints word symbol lexicon transition system state feature task system available https githubcom genelkim ulf transition parser also present first official annotate ulf dataset https wwwcsrochesteredu gkim21 ulf resources
paper explore multi task learn mtl second pretraining step learn enhance universal language representation transformer language model use mtl enhance representation across several natural language understand task improve performance generalization moreover incorporate knowledge distillation kd mtl boost performance devise kd variant learn effectively multiple teachers combine mtl kd propose robustly optimize distil road model framework use road together electra model obtain state art result machine read comprehension natural language inference
multi modal machine translation mmt improve translation quality introduce visual information however exist mmt model ignore problem image bring information irrelevant text cause much noise model affect translation quality paper propose novel gumbel attention multi modal machine translation select text relate part image feature specifically different previous attention base method first use differentiable method select image information automatically remove useless part image feature score matrix gumbel attention image feature image aware text representation generate independently encode text representation image aware text representation multi modal encoder finally final output encoder obtain multi modal gate fusion experiment case analysis prove method retain image feature relate text remain part help mmt model generate better translations
paper address representation coordinate constructions enhance universal dependencies ud relevant dependency link propagate conjunction head conjuncts english treebanks enhance ud create gold basic dependencies use heuristic rule base converter propagate core arguments aim determine set link propagate semantic perspective create large scale dataset manually edit syntax graph identify several systematic errors original data propose also propagate adjuncts observe high inter annotator agreement semantic annotation task use new manually verify dataset perform first principled comparison rule base partially novel machine learn base methods conjunction propagation english show learn propagation rule effective hand design heuristic rule use automatic parse neural graph parser base edge predictor outperform currently predominant pipelinesusing basic layer tree parser plus converters
previous work text generation graph structure data rely pretrained language model plms utilize graph linearization heuristics rather explicitly consider graph structure efficiently encode graph structure plms challenge pretrained natural language model structure data may lead catastrophic forget distributional knowledge paper propose structadapt adapter method encode graph structure plms contrary prior work structadapt effectively model interactions among nod base graph connectivity train graph structure aware adapter parameters way avoid catastrophic forget maintain topological structure graph empirically show benefit explicitly encode graph structure plms use adapters achieve state art result two amr text datasets train fifty-one plm parameters
sensitive text data share among nlp researchers practitioners share document need comply data protection privacy laws hence grow interest automate approach text anonymization however measure methods performance challenge miss single identify attribute reveal individual identity paper draw attention problem argue researchers practitioners develop automate text anonymization systems carefully assess whether evaluation methods truly reflect system ability protect individuals identify propose tild set evaluation criteria comprise anonymization method technical performance information loss result anonymization human ability de anonymize redact document criteria may facilitate progress towards standardize way measure anonymization performance
exist work information extraction ie mainly solve four main task separately entity mention recognition relation extraction event trigger detection argument extraction thus fail benefit inter dependencies task paper present novel deep learn model simultaneously solve four task ie single model call fourie compare prior work jointly perform four ie task fourie feature two novel contributions capture inter dependencies task first representation level introduce interaction graph instance four task use enrich prediction representation one instance relate instance task second label level propose dependency graph information type four ie task capture connections type express input sentence new regularization mechanism introduce enforce consistency golden predict type dependency graph improve representation learn show propose model achieve state art performance joint ie monolingual multilingual learn settings three different languages
exist multi turn context response match methods mainly concentrate obtain multi level multi dimension representations better interactions context utterances response however real place conversation scenarios whether response candidate suitable count give dialogue context also background eg word habit user specific dialogue history content fill gap date methods real world applications incorporate user specific dialogue history response selection propose personalize hybrid match network phmn contributions two fold one model extract personalize word behaviors user specific dialogue history extra match information two perform hybrid representation learn context response utterances explicitly incorporate customize attention mechanism extract vital information context response interactions improve accuracy match evaluate model two large datasets user identification ie personalize ubuntu dialogue corpus p ubuntu personalize weibo dataset p weibo experimental result confirm method significantly outperform several strong model combine personalize attention word behaviors hybrid representation learn
term low resourced toss around field natural language process degree almost language english call low resourced sometimes even sake make mundane mediocre paper appear interest insightful field english synonym language low resourced synonym anything english call endanger languages low resourced bite overstatement paper inspect relation endanger low resourced experience
ever increase availability digital information toxic content also rise therefore detection type language paramount importance tackle problem utilize combination state art pre train language model characterbert traditional bag word technique since content full toxic word write accord dictionary spell attendance individual character crucial therefore use characterbert extract feature base word character consist charactercnn module learn character embeddings context feed well know bert architecture bag word method hand improve upon make sure frequently use toxic word get label accordingly four percent difference first team system rank 36th competition code available search reproduction result
exist work multimodal affective compute task emotion recognition generally adopt two phase pipeline first extract feature representations single modality hand craft algorithms perform end end learn extract feature however extract feature fix fine tune different target task manually find feature extraction algorithms generalize scale well different task lead sub optimal performance paper develop fully end end model connect two phase optimize jointly addition restructure current datasets enable fully end end train furthermore reduce computational overhead bring end end model introduce sparse cross modal attention mechanism feature extraction experimental result show fully end end model significantly surpass current state art model base two phase pipeline moreover add sparse cross modal attention model maintain performance around half computation feature extraction part
darija open dataset doda open source project moroccan dialect ten thousand entries doda arguably largest open source collaborative project darija english translation build natural language process purpose fact besides semantic categorization doda also adopt syntactic one present word different spell offer verb noun masculine feminine correspondences contain conjugation hundreds verbs different tense many subsets help researchers better understand study moroccan dialect data paper present description doda feature collect well first application image classification use imagenet label translate darija collaborative project host github platform mit open source license aim standard resource researchers students anyone interest moroccan dialect
paper introduce human evaluation datasheet template record detail individual human evaluation experiment natural language process nlp originally take inspiration seminal paper bender friedman two thousand and eighteen mitchell et al two thousand and nineteen gebru et al two thousand and twenty human evaluation datasheet intend facilitate record properties human evaluations sufficient detail sufficient standardisation support comparability meta evaluation reproducibility test
study neologism use two sample early english correspondence one thousand, six hundred and forty one thousand, six hundred and sixty one thousand, seven hundred and sixty one thousand, seven hundred and eighty especial interest early adopters new vocabulary social group represent type function neologisms describe computer assist approach note difficulties associate massive variation corpus find include male letter writers tend use neologisms frequently women eighteenth century seem provide opportunities women lower rank participate neologism use well sample neologisms frequently occur letter write close friends could due less stable relationship trigger creative language use seventeenth century sample observe influence english civil war eighteenth century sample appear reflect change function letter write correspondence increasingly use tool build maintain social relationships addition exchange information
natural language process nlp task range text classification text generation revolutionise pre train language model bert allow corporations easily build powerful apis encapsulate fine tune bert model downstream task however fine tune bert model deploy service may suffer different attack launch malicious users work first present adversary steal bert base api service victim target model multiple benchmark datasets limit prior knowledge query show extract model lead highly transferable adversarial attack victim model study indicate potential vulnerabilities bert base api service still hold even architectural mismatch victim model attack model finally investigate two defence strategies protect victim model find unless performance victim model sacrifice model ex traction adversarial transferability effectively compromise target model
propose multilingual data drive method generate read comprehension question use dependency tree method provide strong mostly deterministic inexpensive train baseline less resourced languages language specific corpus still require size nowhere near require modern neural question generation qg architectures method surpass qg baselines previously report literature show good performance term human evaluation
online misogyny become increase worry arab women experience gender base online abuse daily basis misogyny automatic detection systems assist prohibition anti women arabic toxic content develop systems hinder lack arabic misogyny benchmark datasets paper introduce arabic levantine twitter dataset misogynistic language let mi first benchmark dataset arabic misogyny provide detail review dataset creation annotation phase consistency annotations propose dataset emphasize inter rater agreement evaluation measure moreover let mi use evaluation dataset binary multi target classification task conduct several state art machine learn systems along multi task learn mtl configuration obtain result indicate performances achieve use systems consistent state art result languages arabic employ mtl improve performance misogyny target classification task
current sequence sequence model train minimize cross entropy use softmax compute locally normalize probabilities target sequence setup lead strong result variety task one unsatisfying aspect length bias model give high score short inadequate hypotheses often make empty string argmax call cat get tongue problem recently propose entmax base sparse sequence sequence model present possible solution since shrink search space assign zero probability bad hypotheses ability handle word level task transformers never test work show entmax base model effectively solve cat get tongue problem remove major source model error neural machine translation addition generalize label smooth critical regularization technique broader family fenchel young losses include cross entropy entmax losses result label smooth entmax loss model set new state art multilingual grapheme phoneme conversion deliver improvements better calibration properties cross lingual morphological inflection machine translation six language pair
goal orient conversational interfaces design accomplish specific task typically interactions tend span multiple turn adhere pre define structure goal however conventional neural language model nlm automatic speech recognition asr systems mostly train sentence wise limit context paper explore different ways incorporate context lstm base nlm order model long range dependencies improve speech recognition specifically use context carry across multiple turn use lexical contextual cue system dialog act natural language understand nlu model user provide structure chatbot also propose new architecture utilize context embeddings derive bert sample utterances provide inference time experiment show word error rate wer relative reduction seven non contextual utterance level nlm rescorers goal orient audio datasets
present event structure ontology empirically derive inferential properties annotate sentence document level semantic graph induce ontology jointly semantic role entity type event event relation ontologies use document level generative model identify set type align closely previous theoretically motivate taxonomies
successful methods unsupervised neural machine translation unmt employ crosslingual pretraining via self supervision often form mask language model sequence generation task require model align lexical high level representations two languages cross lingual pretraining work similar languages abundant corpora perform poorly low resource distant languages previous research show representations sufficiently align paper enhance bilingual mask language model pretraining lexical level information use type level cross lingual subword embeddings empirical result demonstrate improve performance unmt forty-five bleu bilingual lexicon induction use method compare unmt baseline
multiple study demonstrate behavior internet base social media platforms indicative individual mental health status widespread availability data spur interest mental health research computational lens previous research raise concern possible bias model produce data study quantify bias actually manifest respect different demographic group gender racial ethnic group analyze fairness depression classifiers train twitter data respect gender racial demographic group find model performance systematically differ underrepresented group discrepancies fully explain trivial data representation issue study conclude recommendations avoid bias future research
text summarization process extract important information text present concisely fewer sentence call transcript text involve textual description phone conversation customer caller agents customer representatives paper present indigenously develop method combine topic model sentence selection punctuation restoration condense ill punctuate un punctuate call transcripts produce summaries readable extensive test evaluation comparisons demonstrate efficacy summarizer call transcript summarization
india multilingual society one thousand, three hundred and sixty-nine rationalize languages dialects speak across country india two thousand and eleven twenty-two schedule languages stagger total one hundred and seventeen billion speakers one hundred and twenty-one languages ten thousand speakers india two thousand and eleven india also second largest ever grow digital footprint statista two thousand and twenty despite today state art multilingual systems perform suboptimally indian languages explain fact multilingual language model lms often train one hundred languages together lead small representation languages vocabulary train data multilingual lms substantially less effective resource lean scenarios wu dredze two thousand and twenty lauscher et al two thousand and twenty limit data help capture various nuances language one also commonly observe language text transliterate latin code mix english especially informal settings example social media platforms rijhwani et al two thousand and seventeen phenomenon adequately handle current state art multilingual lms address aforementioned gap propose muril multilingual lm specifically build languages muril train significantly large amount text corpora explicitly augment monolingual text corpora translate transliterate document pair serve supervise cross lingual signal train muril significantly outperform multilingual bert mbert task challenge cross lingual xtreme benchmark hu et al two thousand and twenty also present result transliterate native latin script test set choose datasets demonstrate efficacy muril handle transliterate data
paper describe efforts make bidirectional congolese swahili swc french fra neural machine translation system motivation improve humanitarian translation workflows train create twenty-five thousand, three hundred and two sentence general domain parallel corpus combine publicly available data experiment low resource methodologies like cross dialect transfer semi supervise learn record improvements twenty-four thirty-five bleu point swc fra fra swc directions respectively perform human evaluations assess usability model covid domain chatbot operate democratic republic congo drc direct assessment swc fra direction demonstrate average quality rank sixty-three ten seventy-five target string convey main message source text fra swc direction preliminary test post edit assessment show potential usefulness machine assist translation make model datasets contain one million sentence development pipeline translator web app available public use
colexification refer phenomenon multiple mean share one word language cross linguistic lexification pattern show largely predictable similar concepts often colexified test recent claim beyond general tendency communicative need play important role shape colexification pattern approach question mean series human experiment use artificial language communication game paradigm result across four experiment match previous cross linguistic find things equal speakers prefer colexify similar concepts however also find evidence support communicative need hypothesis face frequent need distinguish similar pair mean speakers adjust colexification preferences maintain communicative efficiency avoid colexifying similar mean need distinguish communication research provide evidence support argument languages shape need preferences speakers
large language model benefit train large amount unlabeled text give increasingly fluent diverse generation capabilities however use model text generation take account target attribute sentiment polarity specific topics remain challenge propose simple flexible method control text generation align disentangle attribute representations contrast recent efforts train discriminator perturb token level distribution attribute use data learn alignment function guide pre train non control language model generate texts target attribute without change original language model parameters evaluate method sentiment topic control generation show large performance gain previous methods retain fluency diversity
train model referential dialogue guess game best model usually choose base task success show popular end end approach choice prevent model learn generate linguistically richer dialogues since acquisition language proficiency take longer learn guess task compare model play different game guesswhat guesswhich mutual friends show discrepancy model task agnostic investigate whether better language quality could lead higher task success show guesswhat model could increase accuracy learn grind encode decode also word occur frequently train set
warn paper contain content may offensive upset numerous natural language process model try inject commonsense use conceptnet knowledge base improve performance different task conceptnet however mostly crowdsourced humans may reflect human bias lawyers dishonest important bias conflate notion commonsense study miss yet important problem first define quantify bias conceptnet two type representational harm overgeneralization polarize perceptions representation disparity find conceptnet contain severe bias disparities across four demographic categories addition analyze two downstream model use conceptnet source commonsense knowledge find existence bias model well propose filter base bias mitigation approach examine effectiveness show mitigation approach reduce issue resource model lead performance drop leave room future work build fairer stronger commonsense model
state art abstractive summarization model generally rely extensive label data lower generalization ability domains data available paper present study domain adaptation abstractive summarization task across six diverse target domains low resource set specifically investigate second phase pre train large scale generative model three different settings one source domain pre train two domain adaptive pre train three task adaptive pre train experiment show effectiveness pre train correlate similarity pre train data target domain task moreover find continue pre train could lead pre train model catastrophic forget learn method less forget alleviate issue furthermore result illustrate huge gap still exist low resource high resource settings highlight need advance domain adaptation methods abstractive summarization task
paper introduce namerec task aim highly accurate fine grain person name recognition traditional name entity recognition model good performance recognise well form person name text consistent complete syntax news article however rapidly grow scenarios sentence incomplete syntax name various form user generate content academic homepages address person name recognition context propose fine grain annotation scheme base anthroponymy take full advantage fine grain annotations propose co guide neural network cognn person name recognition cognn fully explore intra sentence context rich train signal name form better utilize inter sentence context implicit relations extremely essential recognize person name long document propose inter sentence bert model isbert isbert overlap input processor inter sentence encoder bidirectional overlap contextual embed learn multi hop inference mechanisms derive benefit different document diverse abundance context propose advance adaptive inter sentence bert model ada isbert dynamically adjust inter sentence overlap ratio different document conduct extensive experiment demonstrate superiority propose methods academic homepages news article
pre train language model bert family define state arts wide range nlp task however performance bert base model mainly drive enormous amount parameters hinder application resource limit scenarios face problem recent study attempt compress bert small scale model however previous work primarily focus single kind compression technique attention pay combination different methods bert compress integrate techniques critical question design entire compression framework obtain optimal performance response question integrate three kinds compression methods weight prune low rank factorization knowledge distillation kd explore range design concern model architecture kd strategy prune frequency learn rate schedule find careful choice design crucial performance compress model base empirical find best compress model dub refine bert compression integrate techniques rosita seventy-five time smaller bert maintain nine hundred and eighty-five performance five task glue benchmark outperform previous bert compression methods similar parameter budget code available https githubcom llyx97 rosita
swiss german dialect continuum whose natively acquire dialects significantly differ formal variety language dialects mostly use verbal communication standard orthography lead lack annotate datasets render use many nlp methods infeasible paper introduce first annotate parallel corpus speak swiss german across eight major dialects plus standard german reference goal create make available basic dataset employ data drive nlp applications swiss german present data collection procedure detail validate quality corpus conduct experiment recent neural model speech synthesis
code summaries brief natural language descriptions source code piece main purpose code summarization assist developers understand code reduce documentation workload paper design novel multi task learn mtl approach code summarization mine relationship method code summaries method name specifically since method name consider shorter version code summary first introduce task generation informativeness prediction method name two auxiliary train objectives code summarization novel two pass deliberation mechanism incorporate mtl architecture generate consistent intermediate state feed summary decoder especially informative method name exist evaluate deliberation mtl approach carry large scale experiment two exist datasets java python experiment result show technique easily apply many state art neural model code summarization improve performance meanwhile approach show significant superiority generate summaries methods non informative name
recent years vietnam witness mass development social network users different social platforms facebook youtube instagram tiktok social medias hate speech become critical problem social network users solve problem introduce vihsd human annotate dataset automatically detect hate speech social network dataset contain thirty thousand comment comment dataset one three label clean offensive hate besides introduce data creation process annotate evaluate quality dataset finally evaluate dataset deep learn model transformer model
still challenge task learn neural text generation model framework generative adversarial network gans since entire train process differentiable exist train strategies either suffer unreliable gradient estimations imprecise sentence representations inspire principle sparse cod propose sparsegan generate semantic interpretable sparse sentence representations input discriminator key idea treat embed matrix complete dictionary use linear combination select word embeddings approximate output feature representation generator time step semantic rich representations reduce unnecessary noise efficient adversarial train also make entire train process fully differentiable experiment multiple text generation datasets yield performance improvements especially sequence level metrics bleu
paper present dataset himachali low resource endanger language kangri iso six hundred and thirty-nine 3xnr list unite nations educational scientific cultural organization unesco compilation kangri corpus challenge task due non availability digitalize resources corpus contain one hundred and eighty-one thousand, five hundred and fifty-two monolingual twenty-seven thousand, three hundred and sixty-two hindi kangri parallel corpora share pre train kangri word embeddings also report bilingual evaluation understudy bleu score metric evaluation translation explicit order meteor score statistical machine translation smt neural machine translation nmt result corpus corpus freely available non commercial usages research best knowledge first himachali low resource endanger language corpus resources available https githubcom chauhanshweta kangricorpus
popular strategy train recurrent neural network rnns know teacher force take grind truth input time step make later predictions partly condition input train strategy impair ability learn rich distributions entire sequence choose input hinder gradients back propagate previous state end end manner propose fully differentiable train algorithm rnns better capture long term dependencies recover probability whole sequence key idea time step network take input bundle similar word predict previous step instead single grind truth representations similar word form convex hull take kind regularization input smooth input way make whole process trainable differentiable design make possible model explore feasible combinations possibly unseen sequence interpret computationally efficient approximation beam search experiment multiple sequence generation task yield performance improvements especially sequence level metrics blue rouge two
paper propose new problem complementary evidence identification open domain question answer qa problem aim efficiently find small set passages cover full evidence multiple aspects answer complex question end propose method learn vector representations passages model sufficiency diversity within select set addition relevance question passages experiment demonstrate method consider dependence within support evidence significantly improve accuracy complementary evidence selection qa domain
conduct label work speak japanese dataset jas text classification contain fifty interview dialogues two way japanese conversation discuss participants past present future dialogue thirty minutes long dataset select interview dialogues native japanese speakers sample give dataset annotate sentence thirteen label label work conduct native japanese speakers experience data annotation total amount annotate sample twenty thousand, one hundred and thirty
social media twitter provide valuable information crisis managers affect people natural disasters machine learn help structure extract information large volume message share crisis however constantly evolve nature crises make effective domain adaptation essential supervise classification limit unchangeable class label may relevant new events unsupervised topic model insufficient prior knowledge paper bridge gap two show bert embeddings finetuned crisis relate tweet classification effectively use adapt new crisis discover novel topics preserve relevant class supervise train leverage bidirectional self attention extract topic keywords create dataset tweet snowstorm evaluate method transferability new crises find outperform traditional topic model automatic human evaluations ground need crisis managers broadly method use textual domain adaptation latent class unknown overlap know class domains
objective study aim develop end end natural language process pipeline triage diagnosis covid nineteen patient author social media post materials methods text process pipeline first extract covid nineteen symptoms relate concepts severity duration negations body part patients post use conditional random field unsupervised rule base algorithm apply establish relations concepts next step pipeline extract concepts relations subsequently use construct two different vector representations post vectors apply separately build support vector machine learn model triage patients three categories diagnose covid nineteen result report macro micro average f1 score range seventy-one ninety-six sixty-one eighty-seven respectively triage diagnosis covid nineteen model train grind truth label data experimental result indicate similar performance achieve model train use predict label concept extraction rule base classifiers thus yield end end machine learn discussion highlight important feature uncover diagnostic machine learn model compare frequent symptoms reveal another covid nineteen dataset particular find important feature always frequent ones conclusions preliminary result show possible automatically triage diagnose patients covid nineteen natural language narratives use machine learn pipeline
introduce emphnutri bullets multi document summarization task health nutrition first present two datasets food health summaries multiple scientific study furthermore propose novel emphextract compose model solve problem regime limit parallel data explicitly select key span several abstract use policy network follow compose select span present summary via task specific language model compare state art methods approach lead faithful relevant diverse summarization properties imperative application instance breastcancer dataset approach get fifty improvement relevance faithfulnessfootnoteour code data available urlhttps githubcom darsh10 nutribullets
recent advance open domain qa lead strong model base dense retrieval focus retrieve textual passages work tackle open domain qa table first time show retrieval improve retriever design handle tabular context present effective pre train procedure retriever improve retrieval quality mine hard negative relevant datasets miss extract subset natural question kwiatkowski et al two thousand and nineteen table qa dataset find retriever improve retrieval result seven hundred and twenty eight hundred and eleven recall10 end end qa result three hundred and thirty-eight three hundred and seventy-seven exact match bert base retriever
question answer qa task require information multiple document often rely retrieval model identify relevant information reason model derive answer retrieval model typically train maximize likelihood label support evidence however retrieve large text corpora wikipedia correct answer often obtain multiple evidence candidates label positive thus render train signal weak noisy problem exacerbate question unanswerable answer boolean since model rely lexical overlap map answer support evidence develop new parameterization set value retrieval properly handle unanswerable query show marginalize set train allow model mitigate false negative annotate support evidence test method two multi document qa datasets iirc hotpotqa iirc show joint model marginalization alternative contexts improve model performance fifty-five f1 point achieve new state art performance five hundred and six f1 also show marginalization result nine sixteen qa f1 improvement hotpotqa various settings
introduce selfexplain novel self explain framework explain text classifier predictions use phrase base concepts selfexplain augment exist neural classifiers add one globally interpretable layer identify influential concepts train set give sample two locally interpretable layer quantify contribution local input concept compute relevance score relative predict label experiment across five text classification datasets show selfexplain facilitate interpretability without sacrifice performance importantly explanations selfexplain perceive understandable adequately justify trustworthy human judge compare exist widely use baselines
predicate head verbal expression play role structural center sentence identify predicate head critical understand sentence play lead role organize relevant syntactic elements sentence include subject elements adverbial elements etc languages english word morphologies valuable identify predicate head however chinese offer morphological information indicate word grammatical roles chinese sentence often contain several verbal expressions identify expression play role predicate head easy task furthermore chinese sentence inattentive structure provide delimitation word therefore identify chinese predicate head involve significant challenge chinese information extraction little work perform predicate head recognition generally accept evaluation dataset support work important area paper present first attempt develop annotation guideline chinese predicate head relevant syntactic elements annotation guideline emphasize role predicate structural center sentence design relevant syntactic element annotation also follow principle many considerations propose achieve goal eg pattern predicate head flatten annotation structure simpler syntactic unit type base propose annotation guideline one thousand, five hundred document manually annotate corpus available online public access guideline annotate corpus goal broadly impact advance research area chinese information extraction provide research community critical resource lack long time
efficient discovery emotion state speakers multi party conversation highly important design human like conversational agents conversation cognitive state speaker often alter due certain past utterances may lead flip emotion state therefore discover reason trigger behind one emotion flip conversation important explain emotion label individual utterances paper along address task emotion recognition conversations erc introduce novel task emotion flip reason efr aim identify past utterances trigger one emotion state flip certain time propose mask memory network address former transformer base network latter task end consider meld benchmark emotion recognition dataset multi party conversations task erc augment new grind truth label efr extensive comparison four state art model suggest improve performances model task present anecdotal evidence qualitative quantitative error analyse support superiority model compare baselines
today internet awash memes humorous satirical ironic make people laugh accord survey thirty-three social media users age bracket thirteen thirty-five send memes every day whereas fifty send every week memes spread rapidly within short time frame virality depend novelty textual visual content convey positive message funny motivational quote others mean mock hurt someone feel sarcastic offensive message despite appeal nature memes rapid emergence social media effective analysis memes adequately attempt extent deserve paper attempt solve set task suggest semeval twenty memotion analysis competition propose multi hop attention base deep neural network framework call mha meme whose prime objective leverage spatial domain correspondence visual modality image various textual segment extract fine grain feature representations classification evaluate mha meme memotion analysis dataset three sub task sentiment classification affect classification affect class quantification comparative study show sota performances mha meme three task compare top systems participate competition unlike baselines perform inconsistently across three task mha meme outperform baselines task average moreover validate generalization mha meme another set manually annotate test sample observe consistent finally establish interpretability mha meme
sophisticate language model openai gpt three generate hateful text target marginalize group give capacity interest whether large language model use identify hate speech classify text sexist racist use gpt three identify sexist racist text passages zero one shoot learn find zero one shoot learn gpt three able identify sexist racist text accuracy forty-eight per cent sixty-nine per cent shoot learn instruction include prompt model accuracy high seventy-eight per cent conclude large language model role play hate speech detection development language model could use counter hate speech even self police
exponential rise user generate web content social media proliferation abusive languages towards individual group across different section internet also rapidly increase challenge human moderators identify offensive content filter deep neural network show promise reasonable accuracy hate speech detection ally applications however classifiers heavily dependent size quality train data high quality large data set easy obtain moreover exist data set emerge recent time create follow annotation guidelines often concern different type sub type relate hate solve data sparsity problem obtain global representative feature propose convolution neural network cnn base multi task learn model mtlsfootnotecode available https githubcom imprasshant stl mtl leverage information multiple source empirical analysis perform three benchmark datasets show efficacy propose approach significant improvement accuracy f score obtain state art performance respect exist systems
speech act complain use humans communicate negative mismatch reality expectations reaction unfavorable situation linguistic theory pragmatics categorize complaints various severity level base face threat complainer will undertake particularly useful understand intent complainers humans develop suitable apology strategies paper study severity level complaints first time computational linguistics facilitate enrich publicly available data set complaints four severity categories train different transformer base network combine linguistic information achieve five hundred and fifty-seven macro f1 also jointly model binary complaint classification complaint severity multi task set achieve new state art result binary complaint detection reach eight hundred and eighty-two macro f1 finally present qualitative analysis behavior model predict complaint severity level
learn analyze rap lyric significant basis many web applications music recommendation automatic music categorization music information retrieval due abundant source digital music world wide web although numerous study explore topic knowledge field far satisfactory critical issue prosodic information effective representation well appropriate integration various feature usually ignore paper propose hierarchical attention variational autoencoder framework havae simultaneously consider semantic prosodic feature rap lyric representation learn specifically representation prosodic feature encode phonetic transcriptions novel effective strategyie rhyme2vec moreover feature aggregation strategy propose appropriately integrate various feature generate prosodic enhance representation comprehensive empirical evaluation demonstrate propose framework outperform state art approach various metrics different rap lyric learn task
summarization evaluation remain open research problem current metrics rouge know limit correlate poorly human judgments alleviate issue recent work propose evaluation metrics rely question answer model assess whether summary contain relevant information source document though promise propose approach far fail correlate better rouge human judgments paper extend previous approach propose unify framework name questeval contrast establish metrics rouge bertscore questeval require grind truth reference nonetheless questeval substantially improve correlation human judgments four evaluation dimension consistency coherence fluency relevance show extensive experiment report
pronouns important determinants text mean difficult translate pronoun choice depend entities describe previous sentence languages pronouns may drop referent inferrable context issue lead neural machine translation nmt systems make critical errors pronouns impair intelligibility even reinforce gender bias investigate severity pronoun issue show one domains pronoun choice account half nmt systems errors two pronouns disproportionately large impact perceive translation quality investigate possible solution fine tune bert pronoun prediction task use chunk source side sentence use result classifier repair translations exist nmt model offer initial case study approach japanese english language pair observe small number translations significantly improve accord human evaluators
note short description tecominer interactive tool explore topic content text collections unlike topic model tool tecominer base generative probabilistic model topological considerations co occurrence network term outline methods use identify topics describe feature tool sketch application use corpus policy relate scientific news environmental issue publish european commission last decade
commonsense ai long see near impossible goal recently research interest sharply increase influx new benchmarks model propose two new ways evaluate commonsense model emphasize generality new task build diverse recently introduce benchmarks first propose new multitask benchmark rainbow promote research commonsense model generalize well multiple task datasets second propose novel evaluation cost equivalent curve shed new insight choice source datasets pretrained language model transfer learn methods impact performance data efficiency perform extensive experiment two hundred experiment encompass four thousand, eight hundred model report multiple valuable sometimes surprise find eg transfer almost always lead better equivalent performance follow particular recipe qa base commonsense datasets transfer well commonsense knowledge graph perhaps counter intuitively larger model benefit transfer smaller ones last least introduce new universal commonsense reason model unicorn establish new state art performance across eight popular commonsense benchmarks anli eight hundred and seventy-three cosmosqa nine hundred and eighteen hellaswag nine hundred and thirty-nine piqa nine hundred and one socialiqa eight hundred and thirty-two winogrande eight hundred and sixty-six cycic nine hundred and forty commonsenseqa seven hundred and ninety-three
concept literary genre highly complex one different genres frequently define several necessarily level description consideration genres cognitive social scholarly construct rich history complicate matter contribution focus thematic aspects genre quantitative approach namely topic model topic model prove useful discover thematic pattern trend large collections texts view class browse basis dominant theme rarely ever however apply collections dramatic texts contribution topic model use analyze collection french drama classical age enlightenment general aim contribution discover semantic type topics find collection whether different dramatic subgenres distinctive dominant topics plot relate topic pattern inversely extent cluster methods base topic score per play produce group texts agree conventional genre distinctions contribution show interest topic pattern detect provide new insights thematic subgenre relate structure french drama well history french drama classical age enlightenment
paper describe train process first czech monolingual language representation model base bert albert architectures pre train model 340k sentence fifty time multilingual model include czech data outperform multilingual model nine eleven datasets addition establish new state art result nine datasets end discuss properties monolingual multilingual model base upon result publish pre train fine tune model freely research community
think aloud effective meta cognitive strategy human reasoners apply solve difficult problems suggest improve reason ability pre train neural language model similar way namely expand task context problem elaborations dynamically generate language model main result dynamic problem elaboration significantly improve zero shoot performance gpt two deductive reason natural language inference task model use syntactic heuristic predict answer capable degree generate reason additional context facilitate successful application heuristic explore different ways generate elaborations include fewshot learn find relative performance vary specific problem characteristics problem difficulty moreover effectiveness elaboration explain term degree elaboration semantically cohere correspond problem particular elaborations faithful original problem description may boost accuracy twenty-four
transformers outperform recurrent neural network rnns natural language generation come significant computational overhead attention mechanism scale quadratic complexity sequence length efficient transformer variants receive increase interest recent work among linear complexity recurrent variant prove well suit autoregressive generation approximate softmax attention randomize heuristic feature map difficult train yield suboptimal accuracy work aim convert pretrained transformer efficient recurrent counterpart improve efficiency retain accuracy specifically propose swap finetune procedure shelf pretrained transformer replace softmax attention linear complexity recurrent alternative finetune learn feature map approach provide improve tradeoff efficiency accuracy standard transformer recurrent variants also show finetuning process need lower train cost train recurrent variants scratch many recent model natural language task increasingly dependent large scale pretrained transformers work present viable approach improve inference efficiency without repeat expensive pretraining process
interpretability explainability emerge research field nlp user centric point view goal build model provide proper justification decisions similar humans require model satisfy additional constraints end introduce new application legal text contrary mainstream literature target word level rationales conceive rationales select paragraph multi paragraph structure court case also release new dataset comprise european court human right case include annotations paragraph level rationales use dataset study effect already propose rationale constraints ie sparsity continuity comprehensiveness formulate regularizers find indicate constraints beneficial paragraph level rationale extraction others need formulation better handle multi label nature task consider also introduce new constraint singularity improve quality rationales even compare noisy rationale supervision experimental result indicate newly introduce task challenge large scope research
paper introduce first fully manually annotate paraphrase corpus finnish contain fifty-three thousand, five hundred and seventy-two paraphrase pair harvest alternative subtitle news head paraphrase pair corpus ninety-eight manually classify paraphrase least give context contexts additionally establish manual candidate selection method demonstrate feasibility high quality paraphrase selection term cost quality
conduct empirical study unsupervised neural machine translation nmt truly low resource languages explore case parallel train data compute resource lack reflect reality world languages researchers work languages propose simple scalable method improve unsupervised nmt show add comparable data mine use bilingual dictionary along modest additional compute resource train model significantly improve performance also demonstrate use dictionary code switch monolingual data create comparable data improve performance weak supervision best method achieve bleu score improve supervise result englishrightarrowgujarati one thousand, eight hundred and eighty-eight englishrightarrowkazakh five hundred and eighty-four englishrightarrowsomali one hundred and sixteen show promise weakly supervise nmt many low resource languages modest compute resource world best knowledge work first quantitatively showcase impact different modest compute resource low resource nmt
big languages english finnish many natural language process nlp resources model case low resourced endanger languages resources scarce despite great advantage would provide language communities common type resources available low resourced endanger languages translation dictionaries universal dependencies paper present method construct word embeddings endanger languages use exist word embeddings different resource rich languages translation dictionaries resource poor languages thereafter embeddings fine tune use sentence universal dependencies align match semantic space big languages result cross lingual embeddings endanger languages work erzya moksha komi zyrian skolt sami furthermore build universal sentiment analysis model languages part study whether endanger utilize cross lingual word embeddings evaluation conduct show word embeddings endanger languages well align resource rich languages suitable train task specific model demonstrate sentiment analysis model achieve high accuracy cross lingual word embeddings sentiment analysis model release openly via easy use python library
paraphrase often perform less concern control style conversion especially question command style variant paraphrase crucial tone manner also matter industrial applications dialog system paper attack issue corpus construction scheme simultaneously consider core content style directives namely intent formality korean language utilize manually generate natural language query six daily topics expand corpus formal informal sentence human rewrite transfer verify validity industrial applicability approach check adequate classification inference performance fit fine tune approach time propose supervise formality transfer task
electronic health record ehrs become primary form medical data keep across unite state federal law restrict share ehr data contain protect health information phi de identification process identify remove phi crucial make ehr data publicly available scientific research project explore several deep learn base name entity recognition ner methods determine methods perform better de identification task train test model i2b2 train dataset qualitatively assess performance use ehr data collect local hospital find one bilstm crf represent best perform encoder decoder combination two character embeddings crfs tend improve precision price recall three transformers alone perform context encoders future work focus structure medical text may improve extraction semantic syntactic information purpose ehr de identification
network base procedures topic detection huge text collections offer intuitive alternative probabilistic topic model present detail method especially design requirements domain experts mind like similar methods employ community detection term co occurrence graph enhance include resolution parameter use change target topic granularity also establish term rank use semantic word embed present term communities way facilitate interpretation demonstrate application method widely use corpus general news article show result detail social sciences expert evaluations detect topics various resolutions comparison topics detect latent dirichlet allocation also include finally discuss factor influence topic interpretation
sentence order aim arrange sentence give text correct order recent work frame rank problem apply deep neural network work propose new method name bert4so fine tune bert sentence order concatenate sentence compute representations use multiple special tokens carefully design segment interval embeddings tokens across multiple sentence attend greatly enhance interactions also propose margin base listwise rank loss base listmle facilitate optimization process experimental result five benchmark datasets demonstrate effectiveness propose method
transformer attention base neural network consist two sublayers namely self attention network san fee forward network ffn exist research explore enhance two sublayers separately improve capability transformer text representation paper present novel understand san ffn mask attention network man show two special case man static mask matrices however static mask matrices limit capability localness model text representation learn therefore introduce new layer name dynamic mask attention network dman learnable mask matrix able model localness adaptively incorporate advantage dman san ffn propose sequential layer structure combine three type layer extensive experiment various task include neural machine translation text summarization demonstrate model outperform original transformer
speech enable systems typically first convert audio text automatic speech recognition asr model fee text downstream natural language process nlp modules errors asr system seriously downgrade performance nlp modules therefore essential make robust asr errors previous work show effective employ data augmentation methods solve problem inject asr noise train process paper utilize prevalent pre train language model generate train sample asr plausible noise compare previous methods approach generate asr noise better fit real world error distribution experimental result speak language translationslt speak language understand slu show approach effectively improve system robustness asr errors achieve state art result task
domain adaptation widely use practical applications neural machine translation aim achieve good performance general domain domain however exist methods domain adaptation usually suffer catastrophic forget domain divergence model explosion address three problems propose method divide conquer base importance neurons parameters translation model method first prune model keep important neurons parameters make responsible general domain domain translation train prune model supervise original unpruned model knowledge distillation method last expand model original size fine tune add parameters domain translation conduct experiment different languages domains result show method achieve significant improvements compare several strong baselines
paper present monolingual bert model galician follow recent trend show feasible build robust monolingual bert model even relatively low resource languages perform better well know official multilingual bert mbert particularly release two monolingual galician bert model build use six twelve transformer layer respectively train limit resources forty-five million tokens single gpu 24gb provide exhaustive evaluation number task pos tag dependency parse name entity recognition purpose task cast pure sequence label setup order run bert without need include additional layer top use output classification layer map contextualized representations predict label experiment show model especially twelve layer one outperform result mbert task
visual ground promise path toward robust accurate natural language process nlp model many multimodal extensions bert eg videobert lxmert vl bert allow joint model texts image lead state art result multimodal task visual question answer leverage multimodal model purely textual task language model classification expectation multimodal pretraining provide ground improve text process accuracy propose possible strategies respect first type strategy refer transfer ground consist apply multimodal model text task use placeholder replace image input second one call associative ground harness image retrieval match texts relate image pretraining text downstream task draw distinctions strategies compare accord impact language model commonsense relate downstream task show improvement text baselines
recent qa logical reason question require passage level relations among sentence however current approach still focus sentence level relations interact among tokens work explore aggregate passage level clue solve logical reason qa use discourse base information propose discourse aware graph network dagn reason rely discourse structure texts model encode discourse information graph elementary discourse units edus discourse relations learn discourse aware feature via graph network downstream qa task experiment conduct two logical reason qa datasets reclor logiqa propose dagn achieve competitive result source code available https githubcom eleanor h dagn
study semantic parse interactive set users correct errors natural language feedback present nl edit model interpret natural language feedback interaction context generate sequence edit apply initial parse correct errors show nl edit boost accuracy exist text sql parsers twenty one turn correction analyze limitations model discuss directions improvement evaluation code datasets use paper publicly available http akams nledit
present contrast learn approach data augmentation techniques learn document representations unsupervised manner inspire recent contrastive self supervise learn algorithms use image nlp pretraining hypothesize high quality document embed invariant diverse paraphrase preserve semantics original document different backbones contrastive learn frameworks study reveal enormous benefit contrastive augmentation document representation learn two additional insights one include data augmentation contrastive way substantially improve embed quality unsupervised document representation learn two general stochastic augmentations generate simple word level manipulation work much better sentence level document level ones plug method classifier compare broad range baseline methods six benchmark datasets method decrease classification error rate sixty-four sota approach document classification task match even surpass fully supervise methods
mask language model revolutionize natural language process systems past years recently introduce generalization mask language model call warp language model train robust type errors appear automatic manual transcriptions speak language expose language model type errors train work propose novel approach take advantage robustness warp language model transcription noise correct transcriptions speak language show propose approach able achieve ten reduction word error rat automatic manual transcriptions speak language
multi label text classification mltc attractive challenge task natural language process nlp compare single label text classification mltc wider range applications practice paper propose heterogeneous graph convolutional network model solve mltc problem model tokens label nod heterogeneous graph way able take account multiple relationships include token level relationships besides model allow good explainability token label edge expose evaluate method three real world datasets experimental result show achieve significant improvements outperform state art comparison methods
datasets widely use abusive language detection contain list message usually tweet manually judge abusive one annotators annotation perform message level paper investigate happen hateful content message judge also base context give message often ambiguous need interpret context occurrence first annotate part widely use dataset abusive language detection english two condition ie without context compare performance three classification algorithms obtain two type dataset argue context aware classification challenge also similar real application scenario
prepositional supersense annotation time consume require expert train present two sensible methods obtain prepositional supersense annotations elicit surface substitution similarity judgments four pilot study suggest methods potential produce prepositional supersense annotations comparable quality expert annotations
understand offense subjective people may different opinions offensiveness comment moreover offenses hate speech may occur sarcasm hide real intention comment make decision annotators confuse therefore provide well structure annotation process crucial better understand hate speech offensive language phenomena well supply better performance machine learn classifiers paper describe corpus annotation process propose linguist hate speech specialist machine learn engineer order support identification hate speech offensive language social media addition provide first robust dataset kind brazilian portuguese language corpus collect instagram post political personalities manually annotate compose seven thousand annotate document accord three different layer binary classification offensive versus non offensive language level offense highly offensive moderately offensive slightly offensive message identification regard target discriminatory content xenophobia racism homophobia sexism religious intolerance partyism apology dictatorship antisemitism fatphobia comment annotate three different annotators achieve high inter annotator agreement propose annotation approach also language domain independent nevertheless currently customize brazilian portuguese
gqa hudson man two thousand and nineteen dataset real world visual reason compositional question answer find many answer predict best visionlanguage model gqa dataset match grind truth answer still semantically meaningful correct give context fact case exist visual question answer vqa datasets assume one grind truth answer question propose alternative answer set aas grind truth answer address limitation create automatically use shelf nlp tool introduce semantic metric base aas modify top vqa solvers support multiple plausible answer question implement approach gqa dataset show performance improvements
despite improvements performances different natural language generation task deep neural model prone hallucinate facts incorrect nonexistent different hypotheses propose examine separately different task systematic explanations available across task study draw connections hallucinations predictive uncertainty conditional language generation investigate relationship image caption data text generation propose simple extension beam search reduce hallucination analysis show higher predictive uncertainty correspond higher chance hallucination epistemic uncertainty indicative hallucination aleatoric total uncertainties help achieve better result trade performance standard metric less hallucination propose beam search variant
sentence insertion delicate fundamental nlp problem current approach sentence order text coherence question answer qa neither suitable good solve paper propose insertgnn simple yet effective model represent problem graph adopt graph neural network gnn learn connection sentence also supervise local global information local interactions neighbor sentence consider best knowledge first record attempt apply supervise graph structure model sentence insertion evaluate method newly collect toefl dataset verify effectiveness larger arxivdataset use cross domain learn experiment show insertgnn outperform unsupervised text coherence method topological sentence order approach qa architecture specifically achieve accuracy seventy rival average human test score
paper present submission eacl two thousand and twenty-one srw methodology aim bridge gap high low resource languages context open information extraction showcasing greek language goals paper twofold first build neural machine translation nmt model english greek greek english base transformer architecture second leverage nmt model produce english translations greek text input nlp pipeline apply series pre process triple extraction task finally back translate extract triple greek conduct evaluation nmt oie methods benchmark datasets demonstrate approach outperform current state art greek natural language
aspect sentiment triplet extraction aste aim extract aspect term sentiment opinion term triplets sentence try provide complete solution aspect base sentiment analysis absa however triplets extract aste confuse since sentiment triplet extract aste sentiment sentence express toward aspect term rather sentiment aspect term opinion term pair paper introduce fine grain aspect sentiment opinion triplet extraction asote task asote also extract aspect term sentiment opinion term triplets however sentiment triplet extract asote sentiment aspect term opinion term pair build four datasets asote base several popular absa benchmarks propose two methods asote first method extract opinion term aspect term predict sentiments aspect term opinion term pair jointly unify tag schema second method base multiple instance learn train aste datasets also perform asote task experimental result four datasets demonstrate effectiveness methods
unsupervised document summarization acquire lot attention recent years thank simplicity data independence paper propose graph base unsupervised approach extractive document summarization instead rank sentence salience extract sentence one one approach work summary level utilize graph centrality centroid first extract summary candidates subgraphs base centrality sentence graph select summary candidates match centroid perform extensive experiment two bench mark summarization datasets result demonstrate effectiveness model compare state art baselines
large transformer base language model aid human author suggest plausible continuations text write far however current interactive write assistants allow author guide text generation desire topical directions address limitation design framework display multiple candidate upcoming topics user select subset guide generation framework consist two components one method produce set candidate topics predict center word cluster possible continuations two text generation model whose output adhere choose topics train components self supervise use unlabeled text experiment demonstrate topic options better standard cluster approach framework often generate fluent sentence relate choose topics judge automate metrics crowdsourced workers
universal schema uschema assume two sentence pattern share entity pair similar assumption widely adopt solve various type relation extraction task nevertheless sentence pattern could contain multiple facets every facet similar facets another sentence pattern co occur entity pair address violation uschema assumption propose multi facet universal schema use neural model represent sentence pattern multiple facet embeddings encourage one facet embeddings close another sentence pattern co occur entity pair experiment demonstrate multi facet embeddings significantly outperform single facet embed counterpart compositional universal schema cuschema verga et al two thousand and sixteen distantly supervise relation extraction task moreover also use multiple embeddings detect entailment relation two sentence pattern manual label available
electroencephalography eeg signal record people read natural languages commonly use cognitive method interpret human language understand neuroscience psycholinguistics previous study demonstrate human fixation activation word read associate brain regions clear measure brain dynamics across time frequency domains study propose first analysis event relate brain potentials erps event relate spectral perturbations ersps benchmark datasets consist sentence level simultaneous eeg relate eye track record human natural read experiment task result show peak evoke around one hundred and sixty-two ms stimulus start read sentence occipital area indicate brain retriving lexical semantic visual information process approach two hundred ms sentence onset furthermore occipital erp around 200ms present negative power positive power short long reaction time addition occipital ersp around 200ms demonstrate increase high gamma decrease low beta low gamma power relative baseline result imply semantic perception responses occur around 200ms alpha beta gamma band eeg signal find also provide potential impact promote cognitive natural language process model evaluation eeg dynamics
paper propose adaptation method end end speech recognition method multiple automatic speech recognition asr one best hypotheses integrate computation connectionist temporal classification ctc loss function integration multiple asr hypotheses help alleviate impact errors asr hypotheses computation ctc loss asr hypotheses use apply semi supervise adaptation scenarios part adaptation data label ctc loss propose method compute different asr one best hypotheses obtain decode unlabeled adaptation data experiment perform clean multi condition train scenarios ctc base end end asr systems train wall street journal wsj clean train data chime four multi condition train data respectively test aurora four test data propose adaptation method yield sixty-six fifty-eight relative word error rate wer reductions clean multi condition train scenarios respectively compare baseline system adapt part adaptation data manual transcriptions use back propagation fine tune
recent study reveal security threat natural language process nlp model call backdoor attack victim model maintain competitive performance clean sample behave abnormally sample specific trigger word insert previous backdoor attack methods usually assume attackers certain degree data knowledge either dataset users would use proxy datasets similar task implement data poison procedure however paper find possible hack model data free way modify one single word embed vector almost accuracy sacrifice clean sample experimental result sentiment analysis sentence pair classification task show method efficient stealthier hope work raise awareness critical security risk hide embed layer nlp model code available https githubcom lancopku embed poison
wav2vec twenty state art speech recognition model map speech audio waveforms latent representations largest version wav2vec twenty contain three hundred and seventeen million parameters hence inference latency wav2vec twenty bottleneck production lead high cost significant environmental footprint improve wav2vec applicability production set explore multiple model compression methods borrow domain large language model use teacher student approach distil knowledge original wav2vec twenty model student model two time faster forty-eight time smaller original model increase performance accomplish seven degradation word error rate wer quantize model thirty-six time smaller original model one degradation wer best knowledge first work compress wav2vec twenty
train data machine learn model come many different source dubious quality resource rich languages like english lot data available afford throw dubious data low resource languages much less data available necessarily afford throw dubious data case end train set small train model study examine effect text normalization data set quality set low resource languages africa afrikaans amharic hausa igbo malagasy somali swahili zulu describe text normalizer build pynini framework python library finite state transducers experiment train language model african languages use natural language toolkit nltk open source python library nlp
data augmentation way increase diversity available data apply constrain transformations original data strategy widely use image classification best knowledge yet use aspect base sentiment analysis absa absa text analysis technique determine aspects associate sentiment opinionated text paper investigate effect data augmentation state art hybrid approach aspect base sentiment analysis haabsa apply modify versions easy data augmentation eda backtranslation word mixup evaluate propose techniques semeval two thousand and fifteen semeval two thousand and sixteen datasets best result obtain adjust version eda yield five percentage point improvement semeval two thousand and sixteen dataset one percentage point increase semeval two thousand and fifteen dataset compare original haabsa model
many high performance machine learn model aspect base sentiment classification absc produce black box model therefore barely explain classify certain sentiment value towards aspect paper propose explanation model inspect internal dynamics state art neural attention model lcr rot hop use technique call diagnostic classification diagnostic classifier simple neural network evaluate whether internal layer lcr rot hop model encode useful word information classification ie part speech sentiment value presence aspect relation aspect relate sentiment value word conclude lower layer lcr rot hop model encode part speech sentiment value whereas higher layer represent presence relation aspect aspect relate sentiment value word
automatic translation dialogue texts much need demand many real life scenarios however currently exist neural machine translation deliver unsatisfying result paper conduct deep analysis dialogue corpus summarize three major issue dialogue translation include pronoun drop droppro punctuation drop droppun typos typo response challenge propose joint learn method identify omission typo utilize context translate dialogue utterances properly evaluate performance propose manually annotate dataset one thousand, nine hundred and thirty-one chinese english parallel utterances three hundred dialogues benchmark testbed dialogue translation experiment show propose method improve translation quality thirty-two bleu baselines also elevate recovery rate omit pronouns two thousand, six hundred and nine four thousand, seven hundred and sixteen publish code dataset publicly https githubcom rgwt123 dialoguemt
generate knowledge ground responses goal non goal orient dialogue systems important research challenge knowledge graph kg view abstraction real world potentially facilitate dialogue system produce knowledge ground responses however integrate kgs dialogue generation process end end manner non trivial task paper propose novel architecture integrate kgs response generation process train bert model learn answer use elements kg entities relations multi task end end set k hop subgraph kg incorporate model train inference use graph laplacian empirical evaluation suggest model achieve better knowledge groundedness measure via entity f1 score compare state art model goal non goal orient dialogues
describe new addition webvectors toolkit use serve word embed model web new elmoviz module add support contextualized embed architectures particular elmo model provide visualizations follow metaphor two dimensional text show lexical substitute word semantically similar context word input sentence system allow user change elmo layer token embeddings infer also convey corpus information query word lexical substitute namely frequency tiers part speech module well integrate rest webvectors toolkit provide lexical hyperlinks word representations static embed model two web service already implement new functionality pre train elmo model russian norwegian english
text generation systems ubiquitous natural language process applications however evaluation systems remain challenge especially multilingual settings paper propose l ambre metric evaluate morphosyntactic well formedness text use dependency parse morphosyntactic rule language present way automatically extract various rule govern morphosyntax directly dependency treebanks tackle noisy output text generation systems propose simple methodology train robust parsers show effectiveness metric task machine translation diachronic study systems translate morphologically rich languages
face considerable lack resources african languages carry work natural language process nlp natural language understand nlu artificial intelligence research team ntealan association set objective build open source platforms collaborative construction lexicographic data african languages article present first report two years collaborative construction lexicographic resources useful african nlp tool
introduce new balance assignment experts base layer large language model greatly simplify exist high capacity sparse layer sparse layer dramatically improve efficiency train inference rout token specialize expert modules contain small fraction model parameters however difficult learn balance rout function make full use available experts exist approach typically use rout heuristics auxiliary expert balance loss function contrast formulate token expert allocation linear assignment problem allow optimal assignment expert receive equal number tokens optimal assignment scheme improve efficiency guarantee balance compute load also simplify train require new hyperparameters auxiliary losses code publicly release https githubcom pytorch fairseq
show performance neural machine translation nmt drop starkly low resource condition often require large amount auxiliary data achieve competitive result effective method generate auxiliary data back translation target language sentence work present case study tigrinya investigate several back translation methods generate synthetic source sentence find low resource condition back translation pivot higher resource language relate target language prove effective result substantial improvements baselines
fringe group organizations long history use euphemisms ordinary sound word secret mean conceal discuss nowadays one common use euphemisms evade content moderation policies enforce social media platforms exist tool enforce policy automatically rely keyword search word ban list notoriously imprecise even limit swearwords still embarrass false positives commonly use ordinary word acquire euphemistic mean add keyword base ban list hopeless consider pot storage container marijuana heater household appliance firearm current generation social media company instead hire staff check post manually expensive inhumane much effective usually apparent human moderator word use euphemistically may know secret mean therefore whether message violate policy also euphemism ban group use need invent another one leave moderators one step behind paper demonstrate unsupervised algorithms analyze word sentence level context detect word use euphemistically identify secret mean word compare exist state art use context free word embeddings algorithm detect euphemisms achieve thirty four hundred higher detection accuracies unlabeled euphemisms text corpus algorithm reveal euphemistic mean word first kind far aware arm race content moderators policy evaders algorithms may help shift balance direction moderators
emotional voice conversion evc aim change emotional state utterance preserve linguistic content speaker identity paper propose novel two stage train strategy sequence sequence emotional voice conversion limit amount emotional speech data note propose evc framework leverage text speech tts share common goal generate high quality expressive voice stage one perform style initialization multi speaker tts corpus disentangle speak style linguistic content stage two perform emotion train limit amount emotional speech data learn disentangle emotional style linguistic information speech propose framework perform spectrum prosody conversion achieve significant improvement state art baselines objective subjective evaluation
machine translation mt model use industries constantly change topics translation news agencies need adapt new data maintain performance time aim teach pre train mt model translate previously unseen word accurately base examples propose experimental setup allow us simulate novel vocabulary appear human submit translations ii correspond evaluation metrics compare approach extend data augmentation approach use pre train language model create train examples similar contexts novel word compare different fine tune data augmentation approach show adaptation scale one five examples possible combine data augmentation randomly select train sentence lead highest bleu score accuracy improvements impressively one five examples model report better accuracy score reference system train average three hundred and thirteen parallel examples
recently advance make continuous representation word word embeddings deep neural architectures many research work publish area relation extraction difficult keep track many paper help future research present comprehensive review recently publish research work relation extraction mostly focus relation extraction use deep neural network achieve state art performance publicly available datasets survey cover sentence level relation extraction document level relation extraction pipeline base approach joint extraction approach annotate datasets distantly supervise datasets along recent research directions zero shoot shoot relation extraction noise mitigation distantly supervise datasets regard neural architectures cover convolutional model recurrent network model attention network model graph convolutional model survey
present corpus professionally annotate grammatical error correction gec fluency edit ukrainian language best knowledge first gec corpus ukrainian language collect texts errors twenty thousand, seven hundred and fifteen sentence diverse pool contributors include native non native speakers data cover wide variety write domains text chat essay formal write professional proofreaders correct annotate corpus errors relate fluency grammar punctuation spell corpus use develop evaluate gec systems ukrainian generally use research multilingual low resource nlp morphologically rich languages document level gec fluency correction corpus publicly available https githubcom grammarly ua gec
definition extraction systems valuable knowledge source humans algorithms paper describe submissions defteval share task semeval two thousand and twenty task six evaluate english textbook corpus provide detail explanation system joint extraction definition concepts relations among furthermore provide ablation study model variations describe result error analysis
paper describe corpus assist discourse analysis base keyword kw identification interpretation benefit employ market basket analysis mba kw extraction mba data mine technique use originally market reveal consistent associations items shop cart also keywords corpus many texts identify recur associations kws compensate lack wider context major issue impede interpretation isolate kws esp analyze large data showcase advantage mba contextualizing keywords within discourse pilot study topic migration conduct contrast anti system center right czech internet media conduct result show mba useful identify dominant strategy anti system news portals weave confound ideological undercurrent connect concept migrants multitude topics ie flood discourse
multi encoder model broad family context aware neural machine translation nmt systems aim improve translation quality encode document level contextual information alongside current sentence context encode undertake contextual parameters train document level data work show train parameters take large amount data since contextual train signal sparse propose efficient alternative base split sentence pair allow enrich train signal set parallel sentence break intra sentential syntactic link thus frequently push model search context disambiguate clue evaluate approach bleu contrastive test set show allow multi encoder model achieve comparable performances set train times10 document level data also show approach viable option context aware nmt language pair zero document level parallel data
describe verse verse experiment augment creative process write poetry ai create group ai poets style various american classic poets able offer suggestions generate line verse user compose poem paper describe underlie system offer suggestions include generative model task generate large corpus line verse offline store index dual encoder model task recommend next possible set verse index give previous line verse
translate low resource languages challenge machine translation mt systems due lack parallel data paper address issue domain specific mt bambara resourced mande language speak mali present first domain specific parallel dataset mt bambara french discuss challenge work small quantities domain specific data low resource language present result machine learn experiment data
quality summarization evaluation metric quantify calculate correlation score human annotations across large number summaries currently clear precise correlation estimate whether differences two metrics correlations reflect true difference due random chance work address two problems propose methods calculate confidence intervals run hypothesis test correlations use two resampling methods bootstrapping permutation evaluate propose methods appropriate summarization two simulation experiment analyze result apply methods several different automatic evaluation metrics across three set human annotations find confidence intervals rather wide demonstrate high uncertainty reliable automatic metrics truly although many metrics fail show statistical improvements rouge two recent work qaeval bertscore evaluation settings
question answer qa model knowledge base kbs capable provide precise answer utilize relation information among entities although effective model solely rely fix relation representations obtain answer different question relate kb subgraphs hence rich structure information subgraphs may overlook relation representation vectors meanwhile direction information reason prove effective answer prediction graph fully explore exist work address challenge propose novel neural model relation update direction guide answer selector rdas convert relations subgraph additional nod learn structure information additionally utilize direction information enhance reason ability experimental result show model yield substantial improvements two widely use datasets
recently supervise learn paradigm surprisingly remarkable performance garner considerable attention sanskrit computational linguists result sanskrit community put laudable efforts build task specific label data various downstream natural language process nlp task primary component approach come representations word embeddings word embed help transfer knowledge learn readily available unlabelled data improve task specific performance low resource set last decade much excitement field digitization sanskrit effectively use readily available resources much essential perform systematic study word embed approach sanskrit language work investigate effectiveness word embeddings classify word embeddings broad categories facilitate systematic experimentation evaluate four intrinsic task investigate efficacy embeddings approach originally propose languages sanskrit sanskrit along various challenge pose language
exist table question answer datasets contain abundant factual question primarily evaluate query schema comprehension capability system fail include question require complex reason integration information due constraint associate short form answer address issue demonstrate full challenge table question answer introduce fetaqa new dataset 10k wikipedia base table question free form answer support table cells pair fetaqa yield challenge table question answer set require generate free form text answer retrieval inference integration multiple discontinuous facts structure knowledge source unlike datasets generative qa text answer prevalent copy short text span source answer dataset human generate explanations involve entities high level relations provide two benchmark methods propose task pipeline method base semantic parse base qa systems end end method base large pretrained text generation model show fetaqa pose challenge methods
high dimensional distribute semantic space prove useful effective aggregate process visual auditory lexical information many task relate human generate data human language make use large vary number feature lexical constructional items well contextual discourse specific data various type interact represent various aspects communicative information feature mostly local useful organisation eg argument structure predication others persistent course discourse necessary achieve reasonable level understand content paper describe model high dimensional representation utterance text level data include feature constructions contextual data base mathematically principled behaviourally plausible approach represent linguistic information implementation representation straightforward extension random index model previously use lexical linguistic items paper show implement model able represent broad range linguistic feature common integral framework fix dimensionality computationally habitable suitable bridge symbolic representations dependency analysis continuous representations use eg classifiers machine learn approach achieve operations vectors constitute powerful computational algebra accompany associative memory vectors paper provide technical overview framework work implement example apply various type linguistic feature
business process management bpm discipline responsible management discover analyze redesign monitor control business process one crucial task bpm discover model business process text document paper present system resolve end end problem consist one recognize conditional sentence technical document two find boundaries extract conditional resultant clauses conditional sentence three categorize resultant clause action consequence later help generate new step business process model automatically create new dataset three model solve problem best model achieve promise result eight thousand, three hundred and eighty-two eight thousand, seven hundred and eighty-four eight thousand, five hundred and seventy-five precision recall f1 respectively extract condition action consequence clauses use exact match metric
darknet market forums frequently use exchange illegal goods service party use encryption conceal identities tor network use host market guarantee additional anonymization ip location track make challenge link across malicious users use multiple account sybils additionally users migrate new forums one close make difficult link users across multiple forums develop novel stylometry base multitask learn approach natural language interaction model use graph embeddings construct low dimensional representations short episodes user activity authorship attribution provide comprehensive evaluation methods across four different darknet forums demonstrate efficacy state art lift 25x mean retrieval rank 2x recall10
voice assistive technologies give rise far reach privacy security concern paper investigate whether modular automatic speech recognition asr improve privacy voice assistive systems combine independently train separation recognition discretization modules design configurable privacy preserve asr systems evaluate privacy concern effect apply various state art techniques stage system report result use task specific metrics ie wer abx accuracy show overlap speech input asr systems present privacy concern may mitigate use speech separation optimization techniques discretization module show minimize paralinguistics privacy leakage asr acoustic model level commensurate random guess show voice privacy configurable argue present new opportunities privacy preserve applications incorporate asr
morphological segmentation involve decompose word morphemes smallest mean bear units language important nlp task morphologically rich agglutinative languages southern african nguni language group paper investigate supervise unsupervised model two variants morphological segmentation canonical surface segmentation train sequence sequence model canonical segmentation underlie morphemes may equal surface form word conditional random field crf surface segmentation transformers outperform lstms attention canonical segmentation obtain average f1 score seven hundred and twenty-five across four languages feature base crfs outperform bidirectional lstm crfs obtain average nine hundred and seventy-one f1 surface segmentation unsupervised set entropy base approach use character level lstm language model fail outperform morfessor baseline languages neither approach perform much better random baseline hope high performance supervise segmentation model help facilitate development better nlp tool nguni languages
language model foundation current neural network base model natural language understand generation however research intrinsic performance language model african languages extremely limit make challenge lack large standardise train evaluation set exist english high resource languages paper evaluate performance open vocabulary language model low resource south african languages use byte pair encode handle rich morphology languages evaluate different variants n gram model feedforward neural network recurrent neural network rnns transformers small scale datasets overall well regularize rnns give best performance across two isizulu one sepedi datasets multilingual train improve performance datasets hope research open new avenues research multilingual low resource language model african languages
multiwoz twenty dataset release two thousand and eighteen consist ten thousand task orient dialogues span seven domains greatly stimulate research task orient dialogue systems however substantial noise state annotations hinder proper evaluation dialogue state track model tackle issue massive efforts devote correct annotations result three improve versions dataset ie multiwoz twenty-one twenty-three even still lot incorrect inconsistent annotations work introduce multiwoz twenty-four refine annotations validation set test set top multiwoz twenty-one annotations train set remain unchanged encourage robust noise resilient model train benchmark eight state art dialogue state track model model achieve much higher performance multiwoz twenty-four multiwoz twenty-one
exist goal orient dialogue datasets focus mainly identify slot value however customer support interactions reality often involve agents follow multi step procedures derive explicitly define company policies well study customer service dialogue systems realistic settings introduce action base conversations dataset abcd fully label dataset 10k human human dialogues contain fifty-five distinct user intents require unique sequence action constrain policies achieve task success propose two additional dialog task action state track cascade dialogue success establish series baselines involve large scale pre train language model dataset empirical result demonstrate sophisticate network outperform simpler model considerable gap five hundred and eight absolute accuracy still exist reach human level performance abcd
sequence sequence model deliver impressive result word formation task morphological inflection often learn model subtle morphophonological detail limit train data despite performance opacity neural model make difficult determine whether complex generalizations learn whether kind separate rote memorization morphophonological process take place investigate whether complex alternations simply memorize whether level generalization across relate sound change sequence sequence model perform several experiment finnish consonant gradation complex set sound change trigger word certain suffix find model often though always encode seventeen different consonant gradation process handful dimension rnn also show scale activations dimension control whether consonant gradation occur direction gradation
recently model show predict effect unexpected situations eg would cloudy sky help hinder plant growth give context goal situational reason elicit consequences new situation st arise context propose method iteratively build graph relevant consequences explicitly structure situational graph st graph use natural language query finetuned language model across multiple domains curie generate st graph humans find relevant meaningful elicit consequences new situation show st graph generate curie improve situational reason end task wiqa qa three point accuracy simply augment input generate situational graph especially hard subset require background knowledge multi hop reason
humor offense highly subjective due multiple word sense cultural knowledge pragmatic competence hence accurately detect humorous offensive texts several compel use case recommendation systems personalize content moderation however due lack extensive label dataset prior work domain explore large neural model subjective humor understand paper explore whether large neural model ensembles capture intricacies associate humor offense detection rat experiment semeval two thousand and twenty-one task seven hahackathon show develop reasonable humor offense detection systems model model rank third subtask 1b consistently rank around top thirty-three leaderboard remain subtasks
medical cod translate professionally write medical report standardize cod essential part medical information systems health insurance reimbursement manual cod train human coders time consume error prone thus automate cod algorithms develop build especially recent advance machine learn deep neural network solve challenge encode lengthy noisy clinical document capture code associations propose multitask recalibrated aggregation network particular multitask learn share information across different cod scheme capture dependencies different medical cod feature recalibration aggregation share modules enhance representation learn lengthy note experiment real world mimic iii dataset show significantly improve predictive performance
paper describe contribution semeval two thousand and twenty-one task one lexical complexity prediction approach leverage electra model attempt mirror data annotation scheme although task regression task show treat aggregation several classification regression model somewhat counter intuitive approach achieve mae score six hundred and fifty-four sub task one mae eight hundred and eleven sub task two additionally use concept weak supervision signal gloss bert work significantly improve mae score sub task one
present tapas contribution share task statement verification evidence find table semeval two thousand and twenty-one task nine wang et al two thousand and twenty-one sem tab fact task classification task recognize statement entail neutral refute content give table adopt binary tapas model eisenschlos et al two thousand and twenty task learn two binary classification model first model predict statement neutral non neutral second one predict entail refute share task train set contain entail refute examples generate artificial neutral examples train first model model pre train use masklm objective intermediate counter factual synthetic data eisenschlos et al two thousand and twenty tabfact chen et al two thousand and twenty large table entailment dataset find artificial neutral examples somewhat effective train first model achieve six thousand, eight hundred and three test f1 versus six thousand and forty-seven majority baseline second stage find pre train intermediate data tabfact improve result masklm pre train six thousand, eight hundred and three vs five thousand, seven hundred and one
intent recognition slot identification crucial components speak language understand slu systems paper present novel approach towards task context low resourced unwritten languages present acoustic base slu system convert speech phonetic transcription use universal phone recognition system build word free natural language understand module intent recognition slot identification phonetic transcription propose slu system perform competitively resource rich scenarios significantly outperform exist approach amount available data reduce observe ten improvement intent classification tamil five improvement intent classification sinhala also present novel approach towards unsupervised slot identification use normalize attention score approach use unsupervised slot label data augmentation generate data new slot one shoot way one speech record
computational measure linguistic diversity help us understand linguistic landscape use digital language data contribution paper calibrate measure linguistic diversity use restrictions international travel result covid nineteen pandemic previous work map distribution languages use geo reference social media web data goal however describe corpora rather make inferences underlie populations paper show difference differences method base herfindahl hirschman index identify bias digital corpora introduce non local populations methods tell us significant change take place whether lead increase decrease diversity important step align digital corpora like social media real world populations produce
paper measure similarity within eighty-four language varieties across nine languages corpora draw digital source web tweet allow us evaluate whether geo reference corpora reliable model linguistic variation basic idea source adequately represent single underlie language variety similarity source stable across languages countries paper show consistent agreement source use frequency base corpus similarity measure provide evidence digital geo reference corpora consistently represent local language varieties
paper formulate evaluate series multi unit measure directional association build pairwise deltap measure able quantify association sequence vary length type representation multi unit measure face additional segmentation problem implicit length constraint pairwise measure abandon association measure must also identify border meaningful sequence paper take vector base approach segmentation problem use eighteen unique measure describe different aspects multi unit association examination measure across eight languages show stable across languages provide unique rank associate sequence take together measure expand corpus base approach association generalize across vary lengths type representation
paper develop construction base dialectometry capable identify previously unknown constructions measure degree give construction subject regional variation central idea learn grammar constructions cxg use construction grammar induction use constructions feature dialectometry offer method measure aggregate similarity regional cxgs without limit advance set constructions subject variation learn cxg evaluate well describe hold test corpora dialectometry evaluate well model regional varieties english themethod test use two distinct datasets first international corpus english represent eight outer circle varieties second web crawl corpus represent five inner circle varieties result show themethod one produce grammar stable quality across sub set single corpus two capable distinguish regional varieties englishwith high degree accuracy thus three support dialectometricmethods formeasuring similarity varieties english four measure degree construction subject regional variation important cognitive sociolinguistics operationalizes idea competition constructions organize functional level dialectometry need represent much available functional space possible
goal paper provide complete representation regional linguistic variation global scale end paper focus remove three constraints previously limit work within dialectology dialectometry first rather assume fix incomplete set variants use computational construction grammar provide replicable falsifiable set syntactic feature second rather assume specific area interest use global language map base web crawl social media datasets determine selection national varieties third rather look single language isolation model seven major languages together use methods arabic english french german portuguese russian spanish result show model language able robustly predict region origin hold sample better use construction grammars use simpler syntactic feature global scale experiment use argue new methods computational sociolinguistics able provide generalize model regional variation essential understand language variation change scale
paper present system semeval two thousand and twenty-one task eight measeval measeval novel span extraction classification relation extraction task focus find quantities attribute quantities additional information include relate measure entities properties measurement contexts submit system place fifth team rank leaderboard consist scibert cls token embed crf layer top also place first quantity tie unit subtasks second measuredentity modifier qualify subtasks third qualifier subtask
recent approach unsupervised opinion summarization predominantly use review reconstruction train paradigm encoder decoder model train reconstruct single review learn latent review encode space summarization time unweighted average latent review vectors decode summary paper challenge convention simply average latent vector set claim simplistic approach fail consider variations quality input review idiosyncrasies decoder propose coop convex vector aggregation framework opinion summarization search better combinations input review coop require supervision use simple word overlap objective help model generate summaries consistent input review experimental result show extend opinion summarizers coop result state art performance rouge one improvements thirty-seven twenty-nine yelp amazon benchmark datasets respectively
emotional text speech synthesis etts see much progress recent years however generate voice often perceptually identifiable intend emotion category address problem propose new interactive train paradigm etts denote etts seek directly improve emotion discriminability interact speech emotion recognition ser model moreover formulate iterative train strategy reinforcement learn ensure quality etts optimization experimental result demonstrate propose etts outperform state art baselines render speech accurate emotion style best knowledge first study reinforcement learn emotional text speech synthesis
paper approach hate speech detection women arabic community social media eg youtube propose literature similar work present languages english however best knowledge much work conduct arabic language new hate speech corpus arabicfren develop use three different annotators corpus validation three different machine learn algorithms use include deep convolutional neural network cnn long short term memory lstm network bi directional lstm bi lstm network simulation result demonstrate best performance cnn model achieve f1 score eighty-six unbalance corpus compare lstm bi lstm
several study carry reveal linguistic feature capture bert usually achieve train diagnostic classifier representations obtain different layer bert subsequent classification accuracy interpret ability model encode correspond linguistic property despite provide insights study leave potential role token representations paper provide analysis representation space bert search distinct meaningful subspaces explain probe result base set probe task help attribution methods show bert tend encode meaningful knowledge specific token representations often ignore standard classification setups allow model detect syntactic semantic abnormalities distinctively separate grammatical number tense subspaces
semantic relationships hyponym hypernym effect meronym holonym etc pair entities sentence usually reflect syntactic pattern automatic extraction pattern benefit several downstream task include entity extraction ontology build question answer unfortunately automatic extraction pattern yet receive much attention nlp information retrieval researchers work propose attention base supervise deep learn model asper extract syntactic pattern entities exhibit give semantic relation sentential context validate performance asper three distinct semantic relations hyponym hypernym effect meronym holonym six datasets experimental result show semantic relations asper automatically identify collection syntactic pattern reflect existence relation pair entities sentence comparison exist methodologies syntactic pattern extraction asper performance substantially superior
work present approach find semeval two thousand and twenty-one task five toxic span detection task main aim identify span give text toxicity could attribute task challenge mainly due two constraints small train dataset imbalanced class distribution paper investigate two techniques semi supervise learn learn self adjust dice loss tackle challenge submit system rank ninth leader board consist ensemble various pre train transformer language model train use either propose techniques
work present approach solve semeval two thousand and twenty-one task two multilingual cross lingual word context disambiguation mcl wic task sentence pair classification problem goal detect whether give word common sentence evoke mean submit systems settings multilingual pair sentence belong language cross lingual pair sentence belong different languages train data provide english consequently employ cross lingual transfer techniques approach employ fine tune pre train transformer base language model like electra albert english task xlm r task improve systems performance propose add signal word disambiguate augment data sentence pair reversal augment dataset provide us wic xl wic semcor thirty use ensembles achieve strong performance multilingual task place first en en fr fr sub task cross lingual set employ translate test methods zero shoot method use multilingual model latter perform slightly better
paper address task complex conversational question answer knowledge graph task propose lasagne multi task semantic parse transformer graph attention network first approach employ transformer architecture extend graph attention network multi task neural semantic parse lasagne use transformer model generate base logical form graph attention model use exploit correlations entity type predicate produce node representations lasagne also include novel entity recognition module detect link rank relevant entities question context evaluate lasagne standard dataset complex sequential question answer outperform exist baseline average question type specifically show lasagne improve f1 score eight ten question type case increase f1 score twenty compare state art
paper describe use recurrent neural network capture sequential information self attention representations improve transformers although self attention mechanism provide mean exploit long context sequential information ie arrangement tokens explicitly capture propose cascade recurrent neural network transformers refer transfornn model capture sequential information find transfornn model consist shallow transformers stack suffice give comparable better performance deeper transformer model evaluate penn treebank wikitext two corpora propose transfornn model show lower model perplexities fewer number model parameters penn treebank corpus model perplexities reduce fifty-five model size reduce one hundred and five wikitext two corpus model perplexity reduce twenty-two two hundred and seventy-seven smaller model also transfornn model apply librispeech speech recognition task show comparable result transformer model
research natural language process make rapid advance result publication large number research paper find relevant research paper contribution domain challenge problem paper address challenge via semeval two thousand and twenty-one task eleven nlpcontributiongraph develop system research paper contributions focus knowledge graph natural language process literature task divide three sub task extract contribution sentence show important contributions research article extract phrase contribution sentence predict information units research article together triplet formation phrase propose system agnostic subject domain apply build knowledge graph area find transformer base language model significantly improve exist techniques utilize scibert base model first sub task use bidirectional lstm bilstm stack top scibert model layer second sub task use conditional random field crf top scibert bilstm third sub task use combine scibert base neural approach heuristics information unit prediction triplet formation phrase system achieve f1 score thirty-eight sixty-three seventy-six end end pipeline test phrase extraction test triplet extraction test respectively
model pre train multiple languages show significant promise improve speech recognition particularly low resource languages work focus phoneme recognition use allosaurus method multilingual recognition base phonetic annotation incorporate phonological knowledge language dependent allophone layer associate universal narrow phone set phonemes appear language evaluate challenge real world scenario curate phone recognition datasets bukusu saamia two varieties luhya language cluster western kenya eastern uganda knowledge datasets first kind carry similar experiment dataset endanger tangkhulic language east tusom tibeto burman language variety speak mostly india explore zero shoot shoot recognition fine tune use datasets vary size ten one thousand utterances find fine tune allosaurus even one hundred utterances lead significant improvements phone error rat
exploit label hierarchies become promise approach tackle zero shoot multi label text classification zs mtc problem conventional methods aim learn match model text label use graph encoder incorporate label hierarchies obtain effective label representations citerios2018few recently pretrained model like bert citedevlin2018bert use convert classification task textual entailment task citeyin etal two thousand and nineteen benchmarking approach naturally suitable zs mtc task however pretrained model underexplored exist work generate individual vector representations text label make unintuitive combine conventional graph encode methods paper explore improve pretrained model label hierarchies zs mtc task propose reinforce label hierarchy reason rlhr approach encourage interdependence among label hierarchies train meanwhile overcome weakness flat predictions design rollback algorithm remove logical errors predictions inference experimental result three real life datasets show approach achieve better performance outperform previous non pretrained methods zs mtc task
event coreference resolution important research problem many applications despite recent remarkable success pretrained language model argue still highly beneficial utilize symbolic feature task however input coreference resolution typically come upstream components information extraction pipeline automatically extract symbolic feature noisy contain errors also depend specific context feature informative others motivate observations propose novel context dependent gate module adaptively control information flow input symbolic feature combine simple noisy train method best model achieve state art result two datasets ace two thousand and five kbp two thousand and sixteen
generate summaries different style without require corpora target style train separate model present two novel methods deploy summary decode pre train transformer base summarization model one decoder state adjustment instantly modify decoder final state externally train style scorers iteratively refine output target style two word unit prediction constrain word usage impose strong lexical control generation experiment summarize simplicity control automatic evaluation human judge find model produce output simpler languages still informative also generate news headline various ideological lean distinguish humans reasonable probability
propose new approach generate multiple variants target summary diverse content vary lengths score select admissible ones accord users need abstractive summarizers train single reference summaries may struggle produce output achieve multiple desirable properties ie capture important information faithful original grammatical fluent paper propose two stag strategy generate diverse set candidate summaries source text stage one score select admissible ones stage two importantly generator give precise control length summary especially well suit space limit selectors design predict optimal summary length put special emphasis faithfulness original text stag effectively train optimize evaluate experiment benchmark summarization datasets suggest paradigm achieve state art performance
compositional structure model appeal explicitly decompose problems provide interpretable intermediate output give confidence model simply latch onto data artifacts learn model challenge however end task supervision provide weak indirect signal value latent decisions take often result model fail learn perform intermediate task correctly work introduce way leverage pair examples provide stronger cue learn latent decisions two relate train examples share internal substructure add additional train objective encourage consistency latent decisions objective require external supervision value latent output even end task yet provide additional train signal provide individual train examples apply method improve compositional question answer use neural module network drop dataset explore three ways acquire pair question drop discover naturally occur pair examples within dataset b construct pair examples use templates c generate pair examples use question generation model empirically demonstrate propose approach improve distribution generalization lead correct latent decision predictions
produce embed sentence unsupervised way valuable natural language match retrieval problems practice work conduct thorough examination pretrained model base unsupervised sentence embeddings study four pretrained model conduct massive experiment seven datasets regard sentence semantics main find first average tokens better use cls vector second combine top andbottom layer better use top layer lastly easy whiten base vector normalization strategy less ten line code consistently boost performance
healthcare predictive analytics aid medical decision make diagnosis prediction drug review analysis therefore prediction accuracy important criteria also necessitate robust predictive language model however model use deep learn prove vulnerable towards insignificantly perturb input instance less likely misclassified humans recent efforts generate adversaries use rule base synonyms bert mlms witness general domain ever increase biomedical literature pose unique challenge propose bbaeg biomedical bert base adversarial example generation black box attack algorithm biomedical text classification leverage strengths domain specific synonym replacement biomedical name entities bertmlm predictions spell variation number replacement automatic human evaluation two datasets demonstrate bbaeg perform stronger attack better language fluency semantic coherence compare prior work
relation extraction text important task automatic knowledge base population thesis first propose syntax focus multi factor attention network model find relation two entities next propose two joint entity relation extraction frameworks base encoder decoder architecture finally propose hierarchical entity graph convolutional network relation extraction across document
customer review represent rich data source extract valuable information different online shop experience amount collect data may large especially trendy items products movies tv show hotels service number available customers opinions could easily surpass thousands fact good number review could indeed give hint quality item potential customer may time effort read review purpose make inform decision buy rent book thus need right tool technologies help task become necessity buyer seller research goal thesis develop reputation systems automatically provide e commerce customers valuable information support online decision make process mine online review express natural language
although large neural language model lms like bert finetuned yield state art result many nlp task often unclear model actually learn study use lms fill entities human author comparative question like country older india ie study ability neural lms ask answer reasonable question show accuracy fill blank task well correlate human judgements whether question reasonable model train achieve nearly human level performance complete comparative question three different subdomains however analysis show learn fail model sort broad notion entities semantically comparable similar instead train model domain specific performance highly correlate co occurrences specific entities observe train set true model pretrained general text corpora well model train large corpus comparison question study thus reinforce recent result difficulty make claim deep model world knowledge linguistic competence base performance specific benchmark problems make evaluation datasets publicly available foster future research complex understand reason model standards human interaction
intent detection slot fill important task speak natural language understand however vietnamese low resource language research topics paper present first public intent detection slot fill dataset vietnamese addition also propose joint model intent detection slot fill extend recent state art jointbertcrf model intent slot attention layer order explicitly incorporate intent context information slot fill via soft intent label embed experimental result vietnamese dataset show propose model significantly outperform jointbertcrf publicly release dataset implementation model https githubcom vinairesearch jointidsf
paper investigate natural language understand nlu could apply emotion recognition specific task affective compute finetuned different transformers language model bert distilbert roberta xlnet electra use fine grain emotion dataset evaluate term performance f1 score time complete
quadratic computational memory complexities large transformers limit scalability long document summarization paper propose hepos novel efficient encoder decoder attention head wise positional stride effectively pinpoint salient information source conduct systematic study exist efficient self attentions combine hepos able process ten time tokens exist model use full attentions evaluation present new dataset govreport significantly longer document summaries result show model produce significantly higher rouge score competitive comparisons include new state art result pubmed human evaluation also show model generate informative summaries fewer unfaithful errors
word error rate wer predominant metric use evaluate performance automatic speech recognition asr systems however wer sometimes good indicator downstream natural language understand nlu task intent recognition slot fill semantic parse task orient dialog systems wer take consideration literal correctness instead semantic correctness latter typically important downstream task study propose novel semantic distance semdist measure alternative evaluation metric asr systems address issue define semdist distance reference hypothesis pair sentence level embed space represent reference hypothesis sentence embed exploit roberta state art pre train deep contextualized language model base transformer architecture demonstrate effectiveness propose metric various downstream task include intent recognition semantic parse name entity recognition
evaluation many natural language understand nlu task break unreliable bias systems score highly standard benchmarks little room researchers develop better systems demonstrate improvements recent trend abandon iid benchmarks favor adversarially construct distribution test set ensure current model perform poorly ultimately obscure abilities want benchmarks measure position paper lay four criteria argue nlu benchmarks meet argue current benchmarks fail criteria adversarial data collection meaningfully address cause failures instead restore healthy evaluation ecosystem require significant progress design benchmark datasets reliability annotate size ways handle social bias
propose dynamic encoder transducer det device speech recognition one det model scale multiple devices different computation capacities without retrain finetuning trade accuracy latency det assign different encoders decode different part utterance apply compare layer dropout collaborative learn det train layer dropout method randomly drop encoder layer train phase demand layer dropout decode collaborative learn jointly train multiple encoders different depths one single model experiment result librispeech house data show det provide flexible accuracy latency trade result librispeech show full size encoder det relatively reduce word error rate size baseline eight lightweight encoder det train collaborative learn reduce model size twenty-five still get similar wer full size baseline det get similar accuracy baseline model better latency large house data set assign lightweight encoder begin part one utterance full size encoder rest
effectively inform content selection transformer base abstractive summarization model work present simple yet effective attention head mask technique apply encoder decoder attentions pinpoint salient content inference time use attention head mask able reveal relation encoder decoder attentions content selection behaviors summarization model demonstrate effectiveness three document summarization datasets base domain cross domain settings importantly model outperform prior state art model cnn daily mail new york time datasets moreover inference time mask technique also data efficient require twenty train sample outperform bart fine tune full cnn dailymail dataset
subtle overt racism still present physical online communities today impact many live different segment society short piece work present tackle societal issue natural language process release biascorp dataset contain one hundred and thirty-nine thousand and ninety comment news segment three specific source fox news breitbartnews youtube first batch forty-five thousand manually annotate ready publication currently final phase manually label remain dataset use amazon mechanical turk bert use widely several downstream task work present hbert modify certain layer pretrained bert model new hopfield layer hbert generalize well across different distributions add advantage reduce model complexity also release javascript library chrome extension application help developers make use train model web applications say chat application users identify report racially bias content web respectively
mandarin english code switch cs frequently use among east southeast asian people however intra sentence language switch two different languages make recognize cs speech challenge meanwhile recent successful non autoregressive nar asr model remove need leave right beam decode autoregressive ar model achieve outstanding performance fast inference speed therefore paper take advantage mask ctc nar asr framework tackle cs speech recognition issue propose change mandarin output target encoder pinyin faster encoder train introduce pinyin mandarin decoder learn contextualized information moreover propose word embed label smooth regularize decoder contextualized information projection matrix regularization bridge gap encoder decoder evaluate propose methods seame corpus achieve excite result
find residual network euler discretization solutions ordinary differential equations odes paper explore deeper relationship transformer numerical methods odes show residual block layer transformer describe higher order solution odes lead us design new architecture call ode transformer analogous runge kutta method well motivate odes natural extension transformer ode transformer easy implement parameter efficient experiment three wmt task demonstrate genericity model large improvements performance several strong baselines achieve three thousand and seventy-six four thousand, four hundred and eleven bleu score wmt fourteen en de en fr test data set new state art wmt fourteen en fr task
serrant system code automatic classification english grammatical errors combine sercl errant serrant use errant annotations informative provide sercl otherwise
advance speech language technologies enable tool voice search text speech speech recognition machine translation however available high resource languages like english french chinese without foundational digital resources african languages consider low resource digital context advance tool remain reach work detail ai4d african language program three part project one incentivised crowd source collection curation language datasets online quantitative qualitative challenge two support research fellows period three four months create datasets annotate nlp task three host competitive machine learn challenge basis datasets key outcomes work far include one creation nine open source african language datasets annotate variety ml task two creation baseline model datasets host competitive ml challenge
paper introduce new dysarthric speech command dataset italian call easycall corpus dataset consist twenty-one thousand, three hundred and eighty-six audio record twenty-four healthy thirty-one dysarthric speakers whose individual degree speech impairment assess neurologists therapy outcome measure corpus aim provide resource development asr base assistive technologies patients dysarthria particular may exploit develop voice control contact application commercial smartphones aim improve dysarthric patients ability communicate family caregivers record dataset participants administer survey evaluate command likely employ dysarthric individuals voice control contact application addition dataset include list non command ie word near inside command phonetically close command leverage build robust command recognition system present commercial asr systems perform poorly easycall corpus report paper result corroborate need dysarthric speech corpora develop effective assistive technologies best knowledge database represent richest corpus dysarthric speech date
grow popularity virtual assistants pose new challenge entity resolution task link mention text referent entities knowledge base specifically shop domain customers tend use implicit utterances eg organic milk rather explicit name lead large number candidate products meanwhile query different customers may expect different result example add milk cart customer may refer certain organic product customers may want order products regularly purchase address issue propose new framework leverage personalize feature improve accuracy product rank first build cross source heterogeneous knowledge graph customer purchase history product knowledge graph jointly learn customer product embeddings incorporate product customer history representations neural reranking model predict candidate likely purchase specific customer experiment show model substantially improve accuracy top rank candidates two hundred and forty-six compare state art product search model
add linguistic information syntax semantics neural machine translation nmt mostly focus use point estimate pre train model directly use capacity massive pre train contextual word embed model bert devlin et al two thousand and nineteen marginally useful nmt effective fine tune difficult obtain nmt without make train brittle unreliable augment nmt extract dense fine tune vector base linguistic information bert instead use point estimate experimental result show method incorporate linguistic information help nmt generalize better variety train contexts difficult train conventional transformer base nmt
deep transformer model good evaluation score mean subnetwork aka transformer block learn reasonable representation diagnose abnormal representation avoid contribute achieve better evaluation score propose innovative perspective analyze attention pattern summarize block level pattern assume abnormal pattern contribute negative influence leverage wav2vec twenty research target analyze pre train model pattern experiment leverage librispeech one hundred clean train data avoid diagnose abnormal ones custom wav2vec twenty outperform original version forty-eight absolute word error rate wer test clean viterbi decode version still nine better decode four gram language model moreover identify avoid abnormal pattern main contributor performance boost
workshop gender bias nlp gebnlp would like encourage author give explicit consideration wider aspects bias social implications two thousand and twenty edition workshop therefore request author include explicit bias statement work clarify work relate social context nlp systems use programme committee workshops include number reviewers background humanities social sciences addition nlp experts bulk review paper assign one reviewers ask pay specific attention provide bias statements review initiative well receive author submit paper workshop several say receive useful suggestions literature hint bias reviewers therefore plan keep feature review process future editions workshop
previous work text summarization scientific domain mainly focus content input document seldom consider citation network however scientific paper full uncommon domain specific term make almost impossible model understand true mean without help relevant research community paper redefine task scientific paper summarization utilize citation graph propose citation graph base summarization model cgsum incorporate information source paper reference addition construct novel scientific paper summarization dataset semantic scholar network ssn contain 141k research paper different domains 661k citation relationships entire dataset constitute large connect citation graph extensive experiment show model achieve competitive performance compare pretrained model even simple architecture result also indicate citation graph crucial better understand content paper generate high quality summaries
recently interest factual verification prediction structure data like table graph circumvent false news incident necessary model predict structure data efficiently also explain predictions paper part semeval two thousand and twenty-one task nine tackle problem fact verification evidence find tabular data two subtasks give table statement fact subtask determine whether statement infer tabular data subtask b determine cells table provide evidence former subtask make comparison baselines state art approach give semtabfact dataset also propose novel approach cellbert solve evidence find form natural language inference task obtain three way f1 score sixty-nine subtask f1 score sixty-five subtask b
present grammartagger open source grammar profiler give input text identify grammatical feature useful language education model architecture enable learn small amount texts annotate span label one enable easier intuitive annotation two support overlap span three less prone error propagation compare complex hand craft rule define constituency dependency parse show bootstrap grammar profiler model f1 approx six couple hundred sentence english chinese boost via learn multilingual model grammartagger also build octanove learn search engine language learn materials index read difficulty grammatical feature code pretrained model publicly available urlhttps githubcom octanove grammartagger
tackle problem identify metaphors text treat sequence tag task pre train word embeddings glove elmo bert individually show good performance sequential metaphor identification embeddings generate different model train target corpora thus encode different semantic syntactic information show leverage glove elmo feature base bert base multi channel cnn bidirectional lstm model significantly outperform single word embed method combination two embeddings incorporate linguistic feature model improve model performance yield state art performance three public metaphor datasets also provide depth analysis effectiveness leverage multiple word embeddings include analyse spatial distribution different embed methods metaphors literals show well embeddings complement different genres part speech
paper contain description submissions summarization task podcast track trec text retrieval conference two thousand and twenty goal challenge generate short informative summaries contain key information present podcast episode use automatically generate transcripts podcast audio since podcast vary respect genre topic granularity information propose two summarization model explicitly take genre name entities consideration order generate summaries appropriate style podcast model abstractive supervise use creator provide descriptions grind truth summaries result submit summaries show best model achieve aggregate quality score one hundred and fifty-eight comparison creator descriptions baseline abstractive system score one hundred and forty-nine improvement nine assess human evaluators
automate text score ats task automate essay score readability assessment important educational applications natural language process due interpretability model predictions traditional machine learn ml algorithms base handcraft feature still wide use ats task practitioners often need experiment variety model include deep traditional ml ones feature train objectives regression classification although modern deep learn frameworks pytorch require deep ml expertise fully utilize paper present expats open source framework allow users develop experiment different ats model quickly offer flexible components easy use configuration system command line interface toolkit also provide seamless integration language interpretability tool light one interpret visualize model predictions also describe two case study build ats model quickly minimal engineer efforts toolkit available urlhttps githubcom octanove expats
metaphorical expressions difficult linguistic phenomena challenge diverse natural language process task previous work show paraphrase metaphor literal counterpart help machine better process metaphors downstream task paper interpret metaphors bert wordnet hypernyms synonyms unsupervised manner show method significantly outperform state art baseline also demonstrate method help machine translation system improve accuracy translate english metaphors eight target languages
present method generate comparative summaries highlight similarities contradictions input document key challenge create summaries lack large parallel train data require train typical summarization systems end introduce hybrid generation approach inspire traditional concept text systems enable accurate comparison different source model first learn extract pertinent relations input document content plan component use deterministic operators aggregate relations identify subset inclusion summary surface realization component lexicalize information use text infilling language model separately model content selection realization effectively train limit annotations implement test model domain nutrition health rife inconsistencies compare conventional methods framework lead faithful relevant aggregation sensitive summarization equally fluent
recent progress language model drive advance neural architectures also hardware optimization improvements paper revisit neural probabilistic language model nplm ofcitetbengio2003anp simply concatenate word embeddings within fix window pass result fee forward network predict next word scale modern hardware model despite many limitations perform much better expect word level language model benchmarks analysis reveal nplm achieve lower perplexity baseline transformer short input contexts struggle handle long term dependencies inspire result modify transformer replace first self attention layer nplm local concatenation layer result small consistent perplexity decrease across three word level language model datasets
dominant approach probe neural network linguistic properties train new shallow multi layer perceptron mlp top model internal representations approach detect properties encode model cost add new parameters may learn task directly instead propose subtractive prune base probe find exist subnetwork perform linguistic task interest compare mlp subnetwork probe achieve higher accuracy pre train model lower accuracy random model better find properties interest worse learn next vary complexity probe show subnetwork probe pareto dominate mlp probe achieve higher accuracy give budget probe complexity finally analyze result subnetworks across various task locate task encode find lower level task capture lower layer reproduce similar find past work
morphological analysis lexical normalization ln important task japanese user generate text ugt evaluate compare different ln systems construct publicly available japanese ugt corpus corpus comprise nine hundred and twenty-nine sentence annotate morphological normalization information along category information classify frequent ugt specific phenomena experiment corpus demonstrate low performance exist ln methods non general word non standard form indicate corpus would challenge benchmark research ugt
paper present bstc baidu speech translation corpus large scale chinese english speech translation dataset dataset construct base collection license videos talk lecture include sixty-eight hours mandarin data manual transcripts translations english well automate transcripts automatic speech recognition asr model ask three experience interpreters simultaneously interpret test talk mock conference set corpus expect promote research automatic simultaneous translation well development practical systems organize simultaneous translation task use corpus evaluate automatic simultaneous translation systems
taxonomies widely use various machine learn text mine systems organize knowledge facilitate downstream task one critical challenge data business scope grow real applications exist taxonomies need expand incorporate new concepts previous work taxonomy expansion process new concepts independently simultaneously ignore potential relationships among appropriate order insert operations however reality new concepts tend mutually correlate form local hypernym hyponym structure scenario ignore dependencies new concepts order insertion may trigger error propagation example exist taxonomy expansion systems may insert hyponyms exist taxonomies hypernym lead sub optimal expand taxonomies complement exist taxonomy expansion systems propose taxoorder novel self supervise framework simultaneously discover local hypernym hyponym structure among new concepts decide order insertion taxoorder directly plug taxonomy expansion system improve quality expand taxonomies experiment real world dataset validate effectiveness taxoorder enhance taxonomy expansion systems lead better result taxonomies comparison baselines various evaluation metrics
smart contract use computer technology automate performance aspects commercial agreements yet confidence computer code faithful intentions party understand depth subtlety question require exploration natural computer languages semantics expressions languages gap exist discipline law computer science provide perspective key issue explore current research directions explain importance language design development reliable smart contract include specific methodology computable contract
describe uppsala nlp submission semeval two thousand and twenty-one task two multilingual cross lingual word context disambiguation explore usefulness three pre train multilingual language model xlm roberta xlmr multilingual bert mbert multilingual distil bert mdistilbert compare three model two setups fine tune feature extractors second case also experiment use dependency base information find fine tune better feature extraction xlmr perform better mbert cross lingual set fine tune feature extraction whereas two model give similar performance multilingual set mdistilbert perform poorly fine tune give similar result model use feature extractor submit two best systems fine tune xlmr mbert
detect lexical semantic shift smaller data set eg historical linguistics digital humanities challenge due lack statistical power issue exacerbate non contextual word embeddings produce one embed per token therefore mask variability present data article propose approach estimate semantic shift combine contextual word embeddings permutation base statistical test multiple comparisons address use false discovery rate procedure demonstrate performance approach simulation achieve consistently high precision suppress false positives additionally analyze real world data semeval two thousand and twenty task one liverpool fc subreddit corpus show take sample variation account improve robustness individual semantic shift estimate without degrade overall performance
text classification significant branch natural language process many applications include document classification sentiment analysis unsurprisingly text classification concern run time algorithms many depend size corpus vocabulary due bag word representation although many study examine effect preprocessing techniques vocabulary size accuracy none examine methods affect model run time fill gap provide comprehensive study examine preprocessing techniques affect vocabulary size model performance model run time evaluate ten techniques four model two datasets show individual methods reduce run time loss accuracy combinations methods trade two five accuracy sixty-five reduction run time furthermore combinations preprocessing techniques even provide fifteen reduction run time simultaneously improve model accuracy
recently variety probe task propose discover linguistic properties learn contextualized word embeddings many work implicitly assume embeddings lay certain metric space typically euclidean space work consider family geometrically special space hyperbolic space exhibit better inductive bias hierarchical structure may better reveal linguistic hierarchies encode contextualized representations introduce poincare probe structural probe project embeddings poincare subspace explicitly define hierarchies focus two probe objectives dependency tree hierarchy define head dependent structure b lexical sentiments hierarchy define polarity word positivity negativity argue key desideratum probe sensitivity existence linguistic structure apply probe bert typical contextualized embed model syntactic subspace probe better recover tree structure euclidean probe reveal possibility geometry bert syntax may necessarily euclidean sentiment subspace reveal two possible meta embeddings positive negative sentiments show lexically control contextualization would change geometric localization embeddings demonstrate find poincare probe via extensive experiment visualization result reproduce https githubcom franxyao poincareprobe
current covid nineteen pandemic lead creation many corpora facilitate nlp research downstream applications help fight pandemic however corpora exclusively english pandemic global problem worth create covid nineteen relate datasets languages english paper present first manually annotate covid nineteen domain specific dataset vietnamese particularly dataset annotate name entity recognition ner task newly define entity type use future epidemics dataset also contain largest number entities compare exist vietnamese ner datasets empirically conduct experiment use strong baselines dataset find automatic vietnamese word segmentation help improve ner result highest performances obtain fine tune pre train language model monolingual model phobert vietnamese nguyen nguyen two thousand and twenty produce higher result multilingual model xlm conneau et al two thousand and twenty publicly release dataset https githubcom vinairesearch phonercovid19
many sequence sequence task natural language process roughly monotonic alignment source target sequence previous work facilitate enforce learn monotonic attention behavior via specialize attention function pretraining work introduce monotonicity loss function compatible standard attention mechanisms test several sequence sequence task grapheme phoneme conversion morphological inflection transliteration dialect normalization experiment show achieve largely monotonic behavior performance mix larger gain top rnn baselines general monotonicity benefit transformer multihead attention however see isolate improvements subset head bias towards monotonic behavior
large pre train language model plms become ubiquitous development language understand technology lie heart many artificial intelligence advance advance report english use plms unprecedented report advance use plms hebrew far problem twofold first hebrew resources available train nlp model order magnitude english counterparts second accept task benchmarks evaluate progress hebrew plms work aim remedy aspects first present alephbert large pre train language model modern hebrew train larger vocabulary larger dataset hebrew plm second use alephbert present new state art result multiple hebrew task benchmarks include segmentation part speech tag full morphological tag name entity recognition sentiment analysis make alephbert model publicly available provide single point entry development hebrew nlp applications
aim develop english yoruba machine translation system translate english verb phrase text yoruba equivalentwords languages source language target language collect verb phrase group home domainthe lexical translation do assign value match word dictionarythe syntax two languages realize use context free grammarwe validate rewrite rule finite state automatathe human evaluation method use expert fluency scoredthe evaluation show system perform better sample google translation seventy percent response match system output
recent pre train abstractive summarization systems start achieve credible performance major barrier use practice propensity output summaries faithful input contain factual errors number annotate datasets statistical model assess factuality explore clear picture errors important target current techniques succeed fail explore synthetic human label data source train model identify factual errors summarization study factuality word dependency sentence level observations threefold first exhibit factual errors differ significantly across datasets commonly use train set simple synthetic errors reflect errors make abstractive datasets like xsum second human label data fine grain annotations provide effective train signal sentence level annotations synthetic data finally show best factuality detection model enable train factual xsum summarization model allow us identify non factual tokens train data
recent study deep learn show significant progress name entity recognition ner exist work assume clean data annotation yet fundamental challenge real world scenarios large amount noise variety source eg pseudo weak distant annotations work study ner noisy label set calibrate confidence estimation base empirical observations different train dynamics noisy clean label propose strategies estimate confidence score base local global independence assumptions partially marginalize label low confidence crf model propose calibration method confidence score base structure entity label integrate approach self train framework boost performance experiment general noisy settings four languages distantly label settings demonstrate effectiveness method code find https githubcom liukun95 noisy ner confidence estimation
automatic summarisation potential aid physicians streamline clerical task note take notoriously difficult evaluate systems demonstrate safe use clinical set circumvent issue propose semi automatic approach whereby physicians post edit generate note submit conduct preliminary study time save automatically generate consultation note post edit evaluators ask listen mock consultations post edit three generate note time find faster write note scratch present insights lessons learn experiment
propose method evaluate quality generate text ask evaluators count facts compute precision recall f score accuracy raw count believe approach lead objective easier reproduce evaluation apply task medical report summarisation measure objective quality accuracy paramount importance
development neural network pretraining techniques spawn many sentence level tag systems achieve superior performance typical benchmarks however relatively less discuss topic context information introduce current top score tag systems although several exist work attempt shift tag systems sentence level document level still consensus conclusion work limit applicability larger context approach tag task paper instead pursue state art tag system architectural exploration focus investigate larger context train general strategy work end conduct thorough comparative study four propose aggregators context information collect present attribute aid evaluation method interpret improvement bring larger context train experimentally set testbed base four tag task thirteen datasets hopefully preliminary observations deepen understand larger context train enlighten follow work use contextual information
discourse signal often implicit leave interpreter draw require inferences time discourse embed social context mean interpreters apply assumptions beliefs resolve inferences lead multiple valid interpretations however current discourse data frameworks ignore social aspect expect single grind truth present first discourse dataset multiple subjective interpretations english conversation form perceive conversation act intents carefully analyze dataset create computational model one confirm hypothesis take account bias interpreters lead better predictions interpretations two show disagreements nuanced require deeper understand different contextual factor share dataset code http githubcom elisaf subjectivediscourse
explain neural network model important increase trustworthiness real world applications exist methods generate post hoc explanations neural network model identify individual feature attributions detect interactions adjacent feature however model text pair input eg paraphrase identification exist methods sufficient capture feature interactions two texts simple extension compute word pair interactions two texts computationally inefficient work propose group mask gmask method implicitly detect word correlations group correlate word input text pair together measure contribution correspond nlp task whole propose method evaluate two different model architectures decomposable attention model bert across four datasets include natural language inference paraphrase identification task experiment show effectiveness gmask provide faithful explanations model
token level attributions extensively study explain model predictions wide range classification task nlp eg sentiment analysis explanation techniques less explore machine read comprehension rc task although transformer base model use identical use classification underlie reason model perform different different type explanations require propose methodology evaluate explanations explanation allow us understand rc model high level behavior respect set realistic counterfactual input scenarios define counterfactuals several rc settings connect explanation techniques output high level model behavior evaluate useful different explanations really analysis suggest pairwise explanation techniques better suit rc token level attributions often unfaithful scenarios consider additionally propose improvement attention base attribution technique result explanations better reveal model behavior
extract semantic information measurements count important topic term analyze scientific discourse 8th task semeval two thousand and twenty-one count measurements measeval aim boost research direction provide new dataset participants train model extract meaningful information measurements scientific texts competition compose five subtasks build top one quantity span identification two unit extraction identify quantities value modifier classification three span identification measure entities measure properties four qualifier span identification five relation extraction identify quantities measure entities measure properties qualifiers approach challenge first identify quantities extract units measurement classify correspond modifiers afterwards use jointly solve last three subtasks multi turn question answer manner best perform model obtain overlap f1 score three thousand, six hundred and ninety-one test set
beyond success story pre train language model prlms recent natural language process susceptible fit due unusual large model size end dropout serve therapy however exist methods like random base knowledge base search base dropout general less effective onto self attention base model broadly choose fundamental architecture prlms paper propose novel dropout method name attendout let self attention empower prlms capable robust task specific tune demonstrate state art model elaborate train design may achieve much stronger result verify universality approach extensive natural language process task
release foolmetwice fm2 short large dataset challenge entailment pair collect fun multi player game gamification encourage adversarial examples drastically lower number examples solve use shortcuts compare popular entailment datasets players present two task first task ask player write plausible claim base evidence wikipedia page second one show two plausible claim write players one false goal identify time run players pay see clue retrieve evidence pool evidence player need harder claim game play motivate players lead diverse strategies craft claim temporal inference divert unrelated evidence result higher quality data entailment evidence retrieval task open source dataset game code
pre train neural language model give high performance natural language inference nli task whether actually understand mean process sequence remain unclear propose new diagnostics test suite allow assess whether dataset constitute good testbed evaluate model mean understand capabilities specifically apply control corruption transformations widely use benchmarks mnli anli involve remove entire word class often lead non sensical sentence pair model accuracy corrupt data remain high dataset likely contain statistical bias artefacts guide prediction inversely large decrease model accuracy indicate original dataset provide proper challenge model reason capabilities hence propose control serve crash test develop high quality data nli task
detect part sentence contribute sentence toxicity rather provide sentence level verdict hatefulness would increase interpretability model allow human moderators better understand output system paper present team utnlp methodology result semeval two thousand and twenty-one share task five toxic span detection test multiple model contextual embeddings report best set experiment start keyword base model follow attention base name entity base transformers base ensemble model best approach ensemble model achieve f1 six hundred and eighty-four competition evaluation phase
transformer architecture achieve great success abundant natural language process task parameterization transformer model motivate plenty work alleviate overfitting superior performances explorations find simple techniques dropout greatly boost model performance careful design therefore paper integrate different dropout techniques train transformer model specifically propose approach name unidrop unite three different dropout techniques fine grain coarse grain ie feature dropout structure dropout data dropout theoretically demonstrate three dropouts play different roles regularization perspectives empirically conduct experiment neural machine translation text classification benchmark datasets extensive result indicate transformer unidrop achieve around fifteen bleu improvement iwslt14 translation task better accuracy classification even use strong pre train roberta backbone
aspect base sentiment analysis absa aim predict polarities aspects fine grain task field sentiment analysis previous work show syntactic information eg dependency tree effectively improve absa performance recently pre train model ptms also show effectiveness absa therefore question naturally arise whether ptms contain sufficient syntactic information absa obtain good absa model base ptms paper firstly compare induce tree ptms dependency parse tree several popular model absa task show induce tree fine tune roberta ft roberta outperform parser provide tree analysis experiment reveal ft roberta induce tree sentiment word orient could benefit absa task experiment also show pure roberta base model outperform approximate previous sota performances six datasets across four languages since implicitly incorporate task orient syntactic information
norway large amount dialectal variation well general tolerance use public sphere however available resources study variation change time informal areas eg social media paper propose first step create corpus dialectal variation write norwegian collect small corpus tweet manually annotate bokmaal nynorsk dialect mix perform preliminary experiment state art model well analysis data expand corpus future finally make annotations model available future work
cross document event coreference resolution foundational task nlp applications involve multi text process however exist corpora task scarce relatively small annotate modest size cluster document belong topic complement resources enhance future research present wikipedia event coreference wec efficient methodology gather large scale dataset cross document event coreference wikipedia coreference link restrict within predefined topics apply methodology english wikipedia extract large scale wec eng dataset notably dataset creation method generic apply relatively little effort wikipedia languages set baseline result develop algorithm adapt components state art model within document coreference resolution cross document set model suitably efficient outperform previously publish state art result task
neural topic model augment replace bag word input learn representations deep pre train transformer base word prediction model one add benefit use representations multilingual model facilitate zero shoot polylingual topic model however widely observe pre train embeddings fine tune give task immediately clear supervision look like unsupervised task topic model thus propose several methods fine tune encoders improve monolingual zero shoot polylingual neural topic model consider fine tune auxiliary task construct new topic classification task integrate topic classification objective directly topic model train continue pre train find fine tune encoder representations topic classification integrate topic classification task directly topic model improve topic quality fine tune encoder representations task important factor facilitate cross lingual transfer
pre train language model achieve huge success wide range nlp task however contextual representations pre train model contain entangle semantic syntactic information therefore directly use derive useful semantic sentence embeddings task paraphrase pair offer effective way learn distinction semantics syntax naturally share semantics often vary syntax work present parabart semantic sentence embed model learn disentangle semantics syntax sentence embeddings obtain pre train language model parabart train perform syntax guide paraphrase base source sentence share semantics target paraphrase parse tree specify target syntax way parabart learn disentangle semantic syntactic representations respective input separate encoders experiment english show parabart outperform state art sentence embed model unsupervised semantic similarity task additionally show approach effectively remove syntactic information semantic sentence embeddings lead better robustness syntactic variation downstream semantic task
reference free evaluation potential make machine translation evaluation substantially scalable allow us pivot easily new languages domains recently show probabilities give large multilingual model achieve state art result use reference free metric experiment various modifications model demonstrate scale match performance bleu analyze various potential weaknesses approach find surprisingly robust likely offer reasonable performance across broad spectrum domains different system qualities
propose new reference free summary quality evaluation measure emphasis faithfulness measure design find count possible minute inconsistencies summary respect source document propose estime estimator summary text inconsistency mismatch embeddings correlate expert score summary level summeval dataset stronger common evaluation measure consistency also fluency also introduce method generate subtle factual errors human summaries show estime sensitive subtle errors common evaluation measure
human rat one prevalent methods evaluate performance natural language process algorithms similarly common measure quality sentence generate natural language generation model use human raters paper argue explore use subjective evaluations within process train language generation model multi task learn set case study use crowd author dialogue corpus fine tune six different language generation model two model incorporate multi task learn use subjective rat line part explicit learn goal human evaluation generate dialogue line reveal utterances generate multi task model subjectively rat typical move conversation forward least offensive base promise first result discuss future research directions incorporate subjective human evaluations language model train hence keep human user loop development process
language model notoriously difficult evaluate release supersim large scale similarity relatedness test set swedish build expert human judgments test set compose one thousand, three hundred and sixty word pair independently judge relatedness similarity five annotators evaluate three different model word2vec fasttext glove train two separate swedish datasets namely swedish gigaword corpus swedish wikipedia dump provide baseline future comparison release fully annotate test set code baseline model data
petroni et al two thousand and nineteen demonstrate possible retrieve world facts pre train language model express cloze style prompt interpret model prediction accuracy lower bind amount factual information encode subsequent work attempt tighten estimate search better prompt use disjoint set facts train data work make two complementary contributions better understand factual probe techniques first propose optiprompt novel efficient method directly optimize continuous embed space find simple method able predict additional sixty-four facts lama benchmark second raise important question really interpret probe result lower bind possible prompt search methods learn train data find somewhat surprisingly train data use methods contain certain regularities underlie fact distribution exist prompt methods include able exploit better fact prediction conduct set control experiment disentangle learn learn recall provide detail picture different prompt reveal pre train language model
pre train language model bert become common choice natural language process nlp task research word representation show isotropic embeddings significantly improve performance downstream task however measure analyze geometry pre train bert embed find far isotropic find word vectors center around origin average cosine similarity two random word much higher zero indicate word vectors distribute narrow cone deteriorate representation capacity word embed propose simple yet effective method fix problem remove several dominant directions bert embed set learnable weight train weight word similarity task show process embed isotropic method evaluate three standardize task word similarity word analogy semantic textual similarity task word embed process method consistently outperform original embed average improvement thirteen word analogy sixteen semantic textual similarity two baseline methods method also prove robust change hyperparameter
present go work evaluate knowledge first large generative language model train converse swedish use data online discussion forum flashback conduct human evaluation pilot study indicate model often able respond conversations human like informative manner diverse set topics data online forums useful build conversational systems reflect negative consequences incautious application might need take active measure safeguard
state art basic single antecedent anaphora greatly improve recent years researchers therefore start pay attention complex case anaphora split antecedent anaphora time warner consider legal challenge telecommunications inc plan buy half showtime network inc move could lead war two powerful company split antecedent anaphora rarer complex resolve single antecedent anaphora result annotate many datasets design test coreference previous work resolve type anaphora carry unrealistic condition assume gold mention gold split antecedent anaphors available systems also focus split antecedent anaphors work introduce system resolve single split antecedent anaphors evaluate realistic set use predict mention also start address question evaluate single split antecedent anaphors together use standard coreference evaluation metrics
outline great misalignment problem natural language process research mean simply problem definition line method propose human evaluation line definition method study misalignment problem survey ten randomly sample paper publish acl two thousand and twenty report result human evaluation result show one paper fully line term problem definition method evaluation two paper present human evaluation line model method result highlight great misalignment problem major one affect validity reproducibility result obtain human evaluation
analyze large language model able predict pattern human read behavior compare performance language specific multilingual pretrained transformer model predict read time measure reflect natural human sentence process dutch english german russian texts result accurate model human read behavior indicate transformer model implicitly encode relative importance language way comparable human process mechanisms find bert xlm model successfully predict range eye track feature series experiment analyze cross domain cross language abilities model show reflect human sentence process
paper describe detail design development novel annotation framework annotate resources internal displacement outcome collaboration internal displacement monitor centre aim improve accuracy monitor platform idetect schema include multi faceted description events include quantity people displace location date higher order facets aim improve information extraction document relevance type propose also report case study machine learn application document classification task finally discuss importance standardize schema dataset benchmark development impact development reliable disaster monitor infrastructure
leverage deep learn model anomaly detection ad see widespread use recent years due superior performances traditional methods recent deep methods anomalies image learn better feature normality end end self supervise set methods train model discriminate different transformations apply visual data use output compute anomaly score use approach ad text introduce novel pretext task text sequence learn date model end end enforce two independent complementary self supervision signal one token level one sequence level new task formulation show strong quantitative qualitative result 20newsgroups ag news datasets semi supervise set outperform state art result one hundred and thirty-five sixty-nine respectively auroc unsupervised configuration date surpass methods even ten train data contaminate outliers compare zero others
present samanantar largest publicly available parallel corpora collection indic languages collection contain total four hundred and sixty-nine million sentence pair english eleven indic languages two language families particular compile one hundred and twenty-four million sentence pair exist publicly available parallel corpora additionally mine three hundred and forty-six million sentence pair web result 28x increase publicly available sentence pair mine parallel sentence web combine many corpora tool methods particular use web crawl monolingual corpora b document ocr extract sentence scan document c multilingual representation model align sentence approximate nearest neighbor search search large collection sentence human evaluation sample newly mine corpora validate high quality parallel sentence across eleven language pair extract eight hundred and twenty-seven million sentence pair fifty-five indic language pair english centric parallel corpus use english pivot language train multilingual nmt model span languages samanantar compare baselines previously report result publicly available benchmarks model outperform exist model benchmarks establish utility samanantar data https indicnlpai4bharatorg samanantar model https githubcom ai4bharat indictrans available publicly hope help advance research indic nmt multilingual nlp indic languages
paper introduce semantic frame forecast task predict semantic frame occur next ten one hundred even one thousand sentence run story prior work focus predict immediate future story one sentence ahead however novelists write long stories generate sentence enough help gain high level insight develop follow story paper formulate long story sequence story block block contain fix number sentence eg ten one hundred two hundred formulation allow us predict follow story arc beyond scope sentence represent story block use term frequencies tf semantic frame normalize frame inverse document frequency idf conduct semantic frame forecast experiment four thousand, seven hundred and ninety-four book bookcorpus seven thousand, nine hundred and sixty-two scientific abstract coda nineteen block size range five one thousand sentence result show automate model forecast follow story block better random prior replay baselines indicate task feasibility also learn model use frame representation feature outperform exist approach block size one hundred and fifty sentence human evaluation also show propose frame representation visualize word cloud comprehensible representative specific humans code available https githubcom appleternity frameforecasting
numerous attempt make jointly parse syntax semantics high performance one domain typically come price performance trade contradict large body research focus rich interactions syntax semantics interface explore multiple model architectures allow us exploit rich syntactic semantic annotations contain universal decompositional semantics uds dataset jointly parse universal dependencies uds obtain state art result formalisms analyze behaviour joint model syntax semantics find pattern support linguistic theory syntax semantics interface investigate degree joint model generalize multilingual set find similar trend across eight languages
paper describe tokofou system ensemble model misinformation detection task base six different transformer base pre train encoders implement context covid nineteen infodemic share task english fine tune model task question aggregate prediction score use majority vote approach tokofou obtain overall f1 score eight hundred and ninety-seven rank first
shoot learn arise important practical scenarios natural language understand system need learn new semantic label emerge resource scarce domain paper explore retrieval base methods intent classification slot fill task shoot settings retrieval base methods make predictions base label examples retrieval index similar input thus adapt new domains simply change index without retrain model however non trivial apply methods task complex label space like slot fill end propose span level retrieval method learn similar contextualized representations span label via novel batch softmax objective inference time use label retrieve span construct final structure highest aggregate score method outperform previous systems various shoot settings clinc snip benchmarks
consider problem learn simplify medical texts important reliable date information biomedicine dense jargon thus practically inaccessible lay audience furthermore manual simplification scale rapidly grow body biomedical literature motivate need automate approach unfortunately large scale resources available task work introduce new corpus parallel texts english comprise technical lay summaries publish evidence pertain different clinical topics propose new metric base likelihood score mask language model pretrained scientific texts show automate measure better differentiate technical lay summaries exist heuristics introduce evaluate baseline encoder decoder transformer model simplification propose novel augmentation explicitly penalize decoder produce jargon term find yield improvements baselines term readability
semantic parse aim translate natural language nl utterances onto machine interpretable program execute real world environment expensive annotation utterance program pair long acknowledge major bottleneck deployment contemporary neural model real life applications work focus task semi supervise learn limit amount annotate data available together many unlabeled nl utterances base observation program correspond nl utterances must always executable propose encourage parser generate executable program unlabeled utterances due large search space executable program conventional methods use approximations base beam search self train top k marginal likelihood train perform well instead view problem learn executions perspective posterior regularization propose set new train objectives experimental result overnight geoquery show new objectives outperform conventional methods bridge gap semi supervise supervise learn
saliency methods widely use interpret neural network predictions different variants saliency methods often disagree even interpretations prediction make model case identify interpretations trustworthy enough use analyse address question conduct comprehensive quantitative evaluation saliency methods fundamental category nlp model neural language model evaluate quality prediction interpretations two perspectives represent desirable property interpretations plausibility faithfulness evaluation conduct four different datasets construct exist human annotation syntactic semantic agreements sentence level document level evaluation identify various ways saliency methods could yield interpretations low quality recommend future work deploy methods neural language model carefully validate interpretations draw insights
synthesize data semantic parse gain increase attention recently however methods require handcraft high precision rule generative process hinder exploration diverse unseen data work propose generative model feature non neural pcfg model composition program eg sql bart base translation model map program utterance due simplicity pcfg pre train bart generative model efficiently learn exist data hand moreover explicitly model compositions use pcfg lead better exploration unseen program thus generate diverse data evaluate method domain domain settings text sql parse standard benchmarks geoquery spider respectively empirical result show synthesize data generate model substantially help semantic parser achieve better compositional domain generalization
present simple yet effective target adversarial train tat algorithm improve adversarial train natural language understand key idea introspect current mistake prioritize adversarial train step model err experiment show tat significantly improve accuracy standard adversarial train glue attain new state art zero shoot result xnli code release https githubcom namisan mt dnn
exist work probe pretrained language model lms predominantly focus sentence level syntactic task paper introduce document level discourse probe evaluate ability pretrained lms capture document level relations experiment seven pretrained lms four languages seven discourse probe task find bart overall best model capture discourse encoder bert perform surprisingly well baseline model across different model substantial differences layer best capture discourse information large disparities model
complex question answer often require find reason chain consist multiple evidence piece current approach incorporate strengths structure knowledge unstructured text assume text corpora semi structure build dense retrieval methods propose new multi step retrieval approach beamdr iteratively form evidence chain beam search dense representations evaluate multi hop question answer beamdr competitive state art systems without use semi structure information query composition dense space beamdr capture implicit relationships evidence reason chain code available https githubcom henryzhao5852 beamdr
understand linguistic structure encode contextualized embed could help explain impressive performance across nlp exist approach probe usually call train classifiers use accuracy mutual information complexity proxy representation goodness work argue unreliable different representations may need different classifiers develop heuristic directprobe directly study geometry representation build upon notion version space task experiment several linguistic task contextualized embeddings show even without train classifiers directprobe shine light embed space represent label also anticipate classifier performance representation
event extraction long treat sentence level task ie community argue set match human information seek behavior lead incomplete uninformative extraction result propose document level neural event argument extraction model formulate task conditional generation follow event templates also compile new document level event extraction benchmark dataset wikievents include complete event coreference annotation task argument extraction achieve absolute gain seventy-six f1 fifty-seven f1 next best model ram wikievents datasets respectively challenge task informative argument extraction require implicit coreference reason achieve ninety-three f1 gain best baseline demonstrate portability model also create first end end zero shoot event extraction framework achieve ninety-seven fully supervise model trigger extraction performance eighty-two argument extraction performance give access ten thirty-three type ace
meet key component human collaboration increase number meet record transcribe meet summaries become essential remind may may attend meet key decisions make task complete however hard create single short summary cover content long meet involve multiple people topics order satisfy need different type users define new query base multi domain meet summarization task model select summarize relevant span meet response query introduce qmsum new benchmark task qmsum consist one thousand, eight hundred and eight query summary pair two hundred and thirty-two meet multiple domains besides investigate locate summarize method evaluate set strong summarization baselines task experimental result manual analysis reveal qmsum present significant challenge long meet summarization future research dataset available urlhttps githubcom yale lily qmsum
machine translation mt technology facilitate daily task provide accessible shortcuts gather elaborate communicate information however suffer bias harm users society large relatively new field inquiry gender bias mt still lack internal cohesion advocate unify framework ease future research end critically review current conceptualizations bias light theoretical insights relate discipline ii summarize previous analyse aim assess gender bias mt iii discuss mitigate strategies propose far iv point toward potential directions future work
primary paradigm multi task train natural language process represent input share pre train language model add small thin network head per task give input target head head select output final prediction work examine behaviour non target head output head give input belong different task one train find non target head exhibit emergent behaviour may either explain target task generalize beyond original task example numerical reason task span extraction head extract input arguments computation result number generate target generative head addition summarization head train target question answer head output query base summaries give question context answer extract emergent behaviour suggest multi task train lead non trivial extrapolation skills harness interpretability generalization
essence embed algorithms work optimize distance word usual context order generate embed space encode distributional representation word addition single word word piece feature result linguistic analysis text include lexical grammatical semantic information use improve quality embed space however precise understand impact individual annotations possible combinations may quality embeddings paper conduct comprehensive study use explicit linguistic annotations generate embeddings scientific corpus quantify impact result representations result show effect annotations embeddings vary depend evaluation task general observe learn embeddings use linguistic annotations contribute achieve better evaluation result
instead use expensive manual annotations researchers propose train name entity recognition ner systems use heuristic label rule however devise label rule challenge often require considerable amount manual effort domain expertise alleviate problem propose textscglara graph base label rule augmentation framework learn new label rule unlabeled data first create graph nod represent candidate rule extract unlabeled data design new graph neural network augment label rule explore semantic relations rule finally apply augment rule unlabeled data generate weak label train ner model use weakly label data evaluate method three ner datasets find achieve average improvement twenty f1 score best baseline give small set seed rule
study discuss model diachronic process logistic regression approach suggest raimund piotrowski hence label piotrowski law even actual linguistic evidence usually speak use notion law context study apply logistic regression model nine change occur 15th 18th century polish language attest course majority change closely follow expect value prove language change might indeed resemble nonlinear phase change scenario also extend original piotrowski approach propose polynomial logistic regression case hardly describe standard version also propose consider individual language change case jointly order inspect possible collinearity likely different dynamics function time last least evaluate result test influence subcorpus size model goodness fit
work present information theoretic operationalisation cross linguistic non arbitrariness new idea small cross linguistic associations form mean word instance claim blasi et al two thousand and sixteen word tongue likely chance contain phone l control influence language family geographic proximity within large concept align cross lingual lexicon extend methods previously use detect within language non arbitrariness pimentel et al two thousand and nineteen measure cross linguistic associations find significant effect non arbitrariness unsurprisingly small less five average accord information theoretic estimate also provide concept level analysis show quarter concepts consider work exhibit significant level cross linguistic non arbitrariness sum paper provide new methods detect cross linguistic associations scale confirm effect minor
recent work show fine tune large network surprisingly sensitive change random seed explore implications phenomenon model fairness across demographic group clinical prediction task electronic health record ehr mimic iii standard dataset clinical nlp research apparent subgroup performance vary substantially seed yield similar overall performance although evidence trade overall subgroup performance however also find small sample size inherent look intersections minority group somewhat rare condition limit ability accurately estimate disparities find jointly optimize high overall performance low disparities yield statistically significant improvements result suggest fairness work use mimic iii carefully account variations apparent differences may arise stochasticity small sample size
probe neural model ability perform downstream task use activation pattern often use localize part network specialize perform task however little work address potential mediate factor comparisons test case mediate factor consider prediction context length namely length span whose process minimally require perform prediction show control context length may lead contradictory conclusions localization pattern network depend distribution probe dataset indeed probe bert seven task find possible get one hundred and ninety-six different rank manipulate distribution context lengths probe dataset conclude present best practice conduct comparisons future
paper present contribution semeval two thousand and twenty-one task two multilingual cross lingual word context disambiguation mcl wic experiment cover english en en sub track multilingual set task experiment several pre train language model investigate impact different top layer fine tune find combination cosine similarity relu activation lead effective fine tune procedure best model result accuracy nine hundred and twenty-seven fourth best score en en sub track
recent years use word embeddings become popular measure presence bias texts despite fact measure show effective detect wide variety bias metrics base word embeddings lack transparency explainability interpretability study propose pmi base metric quantify bias texts show metric approximate odds ratio allow estimate confidence interval statistical significance textual bias also show pmi base measure express function conditional probabilities provide simple interpretation term word co occurrences approach produce performance comparable glove base skip gram base metrics experiment gender occupation gender name associations discuss advantage disadvantage use methods base first order vs second order co occurrences point view interpretability metric sparseness data
deep learn sequence model successfully apply task morphological inflection result sigmorphon share task past several years indicate model perform well train data cover good amount different lemmata lemmata inflect test time also see train indeed largely case task surprisingly standard model transformer almost completely fail generalize inflection pattern ask inflect previously unseen lemmata ie wug test like circumstances establish data augmentation techniques employ alleviate shortcoming introduce copy bias hallucinate synthetic new word form use alphabet language hand show effective hallucination process need pay attention substrings syllable like length rather individual character stem report significant performance improvement substring base hallucination model previous data hallucination methods train test data overlap lemmata
natural language inference require reason contradictions negations commonsense implications give simple premise eg i mad humans reason vary shade contradictory statements range straightforward negations i mad commonsense contradictions i happy moreover negate contradictory statements shift commonsense implications original premise nontrivial ways example i mad imply i unhappy something negate premise ie i mad necessarily negate correspond commonsense implications paper present first comprehensive study focus commonsense implications negate statements contradictions introduce anion1 new commonsense knowledge graph 624k rule focus negate contradictory events present joint generative discriminative inference model new resource provide novel empirical insights logical negations commonsense contradictions reshape commonsense implications original premise
study new application text generation idiomatic sentence generation aim transfer literal phrase sentence idiomatic counterparts inspire psycholinguistic theories idiom use one native language propose novel approach task retrieve appropriate idiom give literal sentence extract span sentence replace idiom generate idiomatic sentence use neural model combine retrieve idiom remainder sentence experiment novel dataset create task show model able effectively transfer literal sentence idiomatic ones furthermore automatic human evaluations show task propose model outperform series competitive baseline model text generation
present ongoing norlm initiative support creation use large contextualised language model norwegian principle nordic languages include ready use software environment well experience report data preparation train paper introduce first large scale monolingual language model norwegian base elmo bert frameworks addition detail train process present contrastive benchmark result suite nlp task norwegian additional background access data model software please see http norlmnlpleu
reasonable amount annotate data require fine tune pre train language model plm downstream task however obtain label examples different language varieties costly paper investigate zero shoot performance dialectal arabic da fine tune plm modern standard arabic msa data identify significant performance drop evaluate model da remedy performance drop propose self train unlabeled da data apply context name entity recognition ner part speech pos tag sarcasm detection srd several da varieties result demonstrate effectiveness self train unlabeled da data improve zero shoot msa da transfer large texttildelow ten f1 ner two accuracy pos tag forty-five f1 srd conduct ablation experiment show performance boost observe directly result unlabeled da examples use self train work open opportunities leverage relatively abundant label msa datasets develop da model zero low resource dialects also report new state art performance three task open source fine tune model research community
analytical reason essential challenge task require system analyze scenario involve set particular circumstances perform reason make conclusions paper study challenge analytical reason text introduce new dataset consist question law school admission test one thousand, nine hundred and ninety-one two thousand and sixteen analyze knowledge understand reason abilities require well task furthermore address reason challenge design two different baselines one transformer base method leverage state art pre train language model two analytical reason machine arm logical level reason framework extract symbolic knowledge eg participants facts logical function deduce legitimate solutions experiment find transformer base model struggle solve task performance close random guess arm achieve better performance leverage symbolic knowledge interpretable reason step result show methods still lag far behind human performance leave space future research
present model jointly learn denotations word together ground use truth conditional semantics model build neurosymbolic approach mao et al two thousand and nineteen learn grind object clevr dataset johnson et al two thousand and seventeen use novel parallel attention mechanism model achieve state art performance visual question answer learn detect grind object question performance train signal also show model able learn flexible non canonical ground adjust answer question train set
paper improve speech translation st effectively leverage large quantities unlabeled speech text data different complementary ways explore pretraining self train use large libri light speech audio corpus language model commoncrawl experiment improve previous state art twenty-six bleu average four consider covost two language pair via simple recipe combine wav2vec twenty pretraining single iteration self train decode language model different exist work approach leverage supervision st data code model publicly release
ability generate clarification question ie question identify useful miss information give context important reduce ambiguity humans use previous experience similar contexts form global view compare give context ascertain miss useful context inspire propose model clarification question generation first identify miss take difference global local view train model identify useful generate question model outperform several baselines judge automatic metrics humans
salience estimation aim predict term importance document due exist human annotate datasets subjective notion salience previous study typically generate pseudo grind truth evaluation however investigation reveal evaluation protocol propose prior work difficult replicate thus lead follow study exist moreover evaluation process problematic entity link tool use entity match noisy ignorance event argument event evaluation lead boost performance work propose light yet practical entity event salience estimation evaluation protocol incorporate reliable syntactic dependency parser furthermore conduct comprehensive analysis among popular entity event definition standards present definition salience estimation task reduce noise pseudo grind truth generation process furthermore construct dependency base heterogeneous graph capture interactions entities events empirical result show baseline methods novel gnn method utilize heterogeneous graph consistently outperform previous sota model propose metrics
development deep learn techniques allow neural machine translation nmt model become extremely powerful give sufficient train data train time however systems struggle translate text new domain distinct style vocabulary tune representative train corpus allow good domain translation data centric approach fit new data catastrophic forget previously learn behaviour concentrate robust approach domain adaptation nmt particularly case system may need translate sentence multiple domains divide techniques relate data selection model architecture parameter adaptation procedure inference procedure finally highlight benefit domain adaptation multi domain adaptation techniques line nmt research
exist pre train language model plms demonstrate effectiveness self supervise learn broad range natural language process nlp task however explicitly aware domain specific knowledge essential downstream task many domains task e commerce scenarios paper propose k plug knowledge inject pre train language model base encoder decoder transformer transfer natural language understand generation task verify method diverse range e commerce scenarios require domain specific knowledge specifically propose five knowledge aware self supervise pre train objectives formulate learn domain specific knowledge include e commerce domain specific knowledge base aspects product entities categories product entities unique sell proposition product entities k plug achieve new state art result suite domain specific nlp task include product knowledge base completion abstractive product summarization multi turn dialogue significantly outperform baselines across board demonstrate propose method effectively learn diverse set domain specific knowledge language understand generation task
learn sentence embeddings often require large amount label data however task domains label data seldom available create expensive work present new state art unsupervised method base pre train transformers sequential denoising auto encoder tsdae outperform previous approach sixty-four point achieve nine hundred and thirty-one performance domain supervise approach show tsdae strong pre train method learn sentence embeddings significantly outperform approach like mask language model crucial shortcoming previous study narrow evaluation work mainly evaluate single task semantic textual similarity sts require domain knowledge unclear propose methods generalize domains task fill gap evaluate tsdae recent approach four different datasets heterogeneous domains
read complex process require proper understand texts order create coherent mental representations however comprehension problems may arise due hard understand section prove troublesome readers account specific language skills step towards simplify section perform accurately identify evaluate difficult structure paper describe approach semeval two thousand and twenty-one task one lexical complexity prediction competition consist mixture advance nlp techniques namely transformer base language model pre train word embeddings graph convolutional network capsule network well series hand craft textual complexity feature model applicable subtasks achieve good performance result mae seven person correlation seventy-three single word identification well mae eight person correlation seventy-nine multiple word target result five hundred and forty-six sixty-five lower top score obtain competition first second subtasks respectively
minho quotation resource originally release two thousand and twelve provide approximately five hundred thousand quote business leaders analysts politicians span period two thousand and eight two thousand and twelve original resource several fail include large number miss job title affiliations well unnormalised job title produce large variation spell format employment position also numerous duplicate post update standardise job title text well imputation miss job title affiliations duplicate quote delete update also provide metaphor simile extraction well emotion distribution quote update also replace antiquate version lucene index jsonl format well rudimentary interface query data supply resource hop update encourage study business communication time financial crisis
online social media platforms increasingly rely natural language process nlp techniques detect abusive content scale order mitigate harm cause users however techniques suffer various sample association bias present train data often result sub par performance content relevant marginalize group potentially further disproportionate harm towards study bias far focus handful ax disparities subgroups annotations lexicons available consequently bias concern non western contexts largely ignore literature paper introduce weakly supervise method robustly detect lexical bias broader geocultural contexts case study cross geographic toxicity detection demonstrate method identify salient group errors follow demonstrate group reflect human judgments offensive inoffensive language geographic contexts
large scale pretrained language model significantly improve write assistance functionalities autocomplete complex controllable write assistants yet explore leverage advance language model build interactive write assistant generate rephrase text accord fine grain author specifications users provide input intent guide assistant iga form text intersperse tag correspond specific rhetorical directives eg add description contrast rephrase particular sentence fine tune language model dataset heuristically label author intent allow iga fill tag generate text users subsequently edit like series automatic crowdsourced evaluations confirm quality iga generate output small scale user study demonstrate author preference iga baseline methods creative write task release dataset code demo spur research ai assist write
previous work indicate discourse information benefit summarization paper explore whether synergy discourse summarization bidirectional infer document level discourse tree pre train neural summarizers particular generate unlabeled rst style discourse tree self attention matrices transformer model experiment across model datasets reveal summarizer learn dependency constituency style discourse information typically encode single head cover long short distance discourse dependencies overall experimental result suggest learn discourse information general transferable inter domain
task organize shuffle set sentence coherent text important nlp use evaluate machine understand causal temporal relations present reorder bart bart sentence order framework leverage pre train transformer base model identify coherent order give set shuffle sentence reformulate task conditional text marker generation setup input set shuffle sentence sentence specific markers output sequence position markers order text framework achieve state art performance across six datasets perfect match ratio pmr kendall tau tau metric perform evaluations zero shoot set showcasing model able generalize well across datasets additionally perform series experiment understand function explore limitations framework
work explore unsupervised domain adaptation uda pretrained language model downstream task introduce udalm fine tune procedure use mix classification mask language model loss adapt target domain distribution robust sample efficient manner experiment show performance model train mix loss scale amount available target data mix loss effectively use stop criterion uda train furthermore discuss relationship distance target error explore limitations domain adversarial train approach method evaluate twelve domain pair amazon review sentiment dataset yield nine thousand, one hundred and seventy-four accuracy one hundred and eleven absolute improvement state art
question answer systems help users access knowledge broad range topics answer wide array different question systems fall short expectation specialize one particular set eg answer factual question wikipedia data overcome limitation propose compose multiple qa agents within meta qa system argue exist wide range specialize qa agents literature thus address central research question effectively efficiently identify suitable qa agents give question study supervise unsupervised approach address challenge show tweac transformer extendable agent classifiers achieve best performance overall ninety-four accuracy provide extensive insights scalability tweac demonstrate scale robustly one hundred qa agents provide one thousand examples question answer
introduce summscreen summarization dataset comprise pair tv series transcripts human write recap dataset provide challenge testbed abstractive summarization several reason plot detail often express indirectly character dialogues may scatter across entirety transcript detail must find integrate form succinct plot descriptions recap also tv script contain content directly pertain central plot rather serve develop character provide comic relief information rarely contain recap since character fundamental tv series also propose two entity centric evaluation metrics empirically characterize dataset evaluate several methods include neural model base nearest neighbor oracle extractive approach outperform benchmarked model accord automatic metrics show neural model unable fully exploit input transcripts human evaluation qualitative analysis reveal non oracle model competitive oracle counterparts term generate faithful plot events benefit better content selectors oracle non oracle model generate unfaithful facts suggest future research directions
recent research investigate factual knowledge store large pretrained language model plms instead structural knowledge base kb query mask sentence paris capital mask use probe good performance analysis task interpret plms become potential repositories factual knowledge experiment across ten linguistically diverse languages study knowledge contain static embeddings show restrict output space candidate set simple nearest neighbor match use static embeddings perform better plms eg static embeddings perform sixteen point better bert use three energy train one important factor good comparative performance static embeddings standardly learn large vocabulary contrast bert exploit sophisticate expensive ability compose meaningful representations much smaller subword vocabulary
despite peer review essential component academia since 1600s repeatedly receive criticisms lack transparency consistency posit recent work machine learn explainable ai provide tool enable insights decisions give peer review process start extract global explanations form linguistic feature affect acceptance scientific paper publication open peer review dataset second since global explanations justify causal interpretations provide methodology detect confound effect natural language order generate causal explanations assumptions form lexicons propose linguistic explanation methodology indicate follow case dataset iclr submissions organise committee follow part recommendations reviewers b paper main characteristics lead reviewers recommend acceptance publication originality clarity substance
many crowdsourced nlp datasets contain systematic gap bias identify data collection complete identify issue early data sample crowdsourcing make mitigation efficient especially do iteratively take natural language inference test case ask whether beneficial put linguist loop data collection dynamically identify address gap data introduce novel constraints task directly compare three data collection protocols baseline protocol ii linguist loop intervention iteratively update constraints task iii extension linguist loop provide direct interaction linguists crowdworkers via chatroom datasets collect linguist involvement reliably challenge baseline without loss quality see evidence use data train lead better domain model performance addition chat platform measurable effect result dataset suggest integrate expert analysis textitduring data collection expert dynamically address gap bias dataset
chinese pre train language model usually process text sequence character ignore coarse granularity eg word work propose novel pre train paradigm chinese lattice bert explicitly incorporate word representations along character thus model sentence multi granularity manner specifically construct lattice graph character word sentence fee text units transformers design lattice position attention mechanism exploit lattice structure self attention layer propose mask segment prediction task push model learn rich redundant information inherent lattices avoid learn unexpected trick experiment eleven chinese natural language understand task show model bring average increase fifteen twelve layer set achieve new state art among base size model clue benchmarks analysis show lattice bert harness lattice structure improvement come exploration redundant information multi granularity representations code available https githubcom alibaba pretrained language model latticebert
although recent work show potential complementarity among different state art systems work try investigate problem text summarization researchers areas commonly refer techniques reranking stack approach problem work highlight several limitations previous methods motivate us present new framework refactor provide unify view text summarization summaries combination experimentally perform comprehensive evaluation involve twenty two base systems four datasets three different application scenarios besides new state art result cnn dailymail dataset four thousand, six hundred and eighteen rouge one also elaborate propose method address limitations traditional methods effectiveness refactor model shed light insight performance improvement system directly use researchers shelf tool achieve performance improvements open source code provide convenient interface use https githubcom yixinl7 refactoring summarization also make demo work available http explainaboardnlpediaai leaderboard task summ indexphp
prior methods text segmentation mostly token level despite adequacy nature limit full potential capture long term dependencies among segment work propose novel framework incrementally segment natural language sentence segment level every step segmentation recognize leftmost segment remain sequence implementations involve lstm minus technique construct phrase representations recurrent neural network rnn model iterations determine leftmost segment conduct extensive experiment syntactic chunk chinese part speech pos tag across three datasets demonstrate methods significantly outperform previous baselines achieve new state art result moreover qualitative analysis study segment long length sentence verify effectiveness model long term dependencies
emotion pair extraction ecpe emerge task sentiment analysis aim extract pair emotions correspond cause document challenge problem emotion extraction ece since require emotion signal demonstrate important role ece task exist work follow two stage pipeline identify emotions cause first step pair second step however error propagation across step pair combine without contextual information limit effectiveness therefore propose dual question attention network alleviate limitations specifically question candidate emotions cause context independently attention network contextual semantical answer also explore weight loss function control error propagation step empirical result show method perform better baselines term multiple evaluation metrics source code obtain https githubcom qixuansun dqan
task orient semantic parse model typically high resource requirements support new ontologies ie intents slot practitioners crowdsource thousands sample supervise fine tune partly due structure de facto copy generate parsers model treat ontology label discrete entities rely parallel data extrinsically derive mean work instead exploit intrinsically know ontology label example fact sltimezone categorical type slot language base span time zone use motivation build approach offline online stag preprocessing ontology label extract intrinsic properties component insert component inventory cache sort train fine tune seq2seq pre train transformer map utterances inventory frame parse tree comprise utterance ontology tokens formulation encourage model consider ontology label union intrinsic properties therefore substantially bootstrapping learn low resource settings experiment show model highly sample efficient use low resource benchmark derive topv2 inventory parser outperform copy generate parser fifteen absolute forty-four relative fine tune ten sample unseen domain
open domain question answer qa retrieve read mechanism inherent benefit interpretability easiness add remove edit knowledge compare parametric approach close book qa model however also know suffer large storage footprint due document corpus index discuss several orthogonal strategies drastically reduce footprint retrieve read open domain qa system 160x result indicate retrieve read viable option even highly constrain serve environment edge devices show achieve better accuracy purely parametric model comparable docker level system size
eye movement data read useful source information understand language comprehension process paper describe submission cmcl two thousand and twenty-one share task predict human read pattern model use roberta regression layer predict five eye track feature train model two stag first fine tune provo corpus another eye track dataset fine tune task data compare different transformer model apply ensembling methods improve performance final submission achieve mae score three thousand, nine hundred and twenty-nine rank 3rd place thirteen team participate share task
effective recipe build seq2seq non autoregressive task orient parsers map utterances semantic frame proceed three step encode utterance x predict frame length decode size frame utterance ontology tokens though empirically strong model typically bottleneck length prediction even small inaccuracies change syntactic semantic characteristics result frame work propose span pointer network non autoregressive parsers shift decode task text generation span prediction impute utterance span frame slot model produce endpoints eg j oppose text eg 6pm natural quantization output space reduce variability gold frame therefore improve length prediction ultimately exact match furthermore length prediction responsible frame syntax decoder responsible frame semantics result coarse fine model evaluate approach several task orient semantic parse datasets notably bridge quality gap non autogressive autoregressive parsers achieve eighty-seven topv2 chen et al two thousand and twenty furthermore due consistent gold frame show strong improvements model generalization cross domain cross lingual transfer low resource settings finally due diminish output vocabulary observe seventy reduction latency eighty-three reduction memory beam size five compare prior non autoregressive parsers
multi hop question answer qa challenge task require precise reason entity relations every step towards answer relations represent term label knowledge graph eg textitspouse text text corpus eg textitthey marry twenty-six years exist model usually infer answer predict sequential relation path aggregate hide graph feature former hard optimize latter lack interpretability paper propose transfernet effective transparent model multi hop qa support label text relations unify framework transfernet jump across entities multiple step step attend different part question compute activate score relations transfer previous entity score along activate relations differentiable way carry extensive experiment three datasets demonstrate transfernet surpass state art model large margin particular metaqa achieve one hundred accuracy two hop three hop question qualitative analysis show transfernet transparent interpretable intermediate result
numerical reason text nrot present unique challenge well address exist pre train objectives explore five sequential train schedule adapt pre train t5 model nrot final model adapt t5 pre train three datasets design strengthen skills necessary nrot general read comprehension fine tune discrete reason text drop dataset train improve drop adjust f1 performance numeracy focus score four thousand, five hundred and ninety seven thousand and eighty-three model close genbert seven hundred and twenty-four custom bert base model use datasets significantly parameters show train t5 multitasking framework multiple numerical reason datasets increase difficulty good performance drop achieve without manually engineer partition functionality distribute symbol modules
multilingual machine translation attract much attention recently due support knowledge transfer among languages low cost train deployment compare numerous bilingual model know challenge multilingual model negative language interference order enhance translation quality deeper wider architectures apply multilingual model larger model capacity suffer increase inference cost time point recent study parameters share among languages interference may also enable positive transfer base insights propose adaptive sparse architecture multilingual model train model learn share language specific parameters improve positive transfer mitigate interference sparse architecture activate subnetwork preserve inference efficiency adaptive design select different subnetworks base input languages evaluate multilingual translation across multiple public datasets model outperform strong baselines term translation quality without increase inference cost
increment toxic comment online space cause tremendous effect vulnerable users reason considerable efforts make deal semeval two thousand and twenty-one task five toxic span detection one task ask competitors extract span toxicity give texts do several analyse understand structure experiment solve task two approach name entity recognition spacy library question answer roberta combine toxicbert former gain highest f1 score six thousand, six hundred and ninety-nine
bilingual terminologies important resources natural language process nlp applications acquisition bilingual terminology pair either human translation automatic extraction parallel data notice comparable corpora could also good resource extract bilingual terminology pair especially e commerce domain parallel corpora particularly scarce e commerce settings non parallel corpora different languages domain easily available paper propose novel framework extract bilingual terminologies non parallel comparable corpus e commerce benefit cross lingual pre train e commerce framework extract correspond target terminology fully utilize deep semantic relationship source side terminology target side sentence experimental result various language pair show approach achieve significantly better performance various strong baselines
parallel corpora indispensable train neural machine translation nmt model parallel corpora language pair exist scarce case pivot language nmt helpful pivot language use exist parallel corpora source pivot pivot target languages naturally quality pivot language translation inferior could achieve direct parallel corpus reasonable size pair real time simultaneous translation set quality pivot language translation deteriorate even give model output translations moment source word become available solve issue propose multi pivot translation apply simultaneous translation set involve pivot languages approach involve simultaneously translate source language multiple pivot simultaneously translate together target language leverage multi source nmt experiment low resource set use n way parallel un corpus arabic english nmt via french spanish pivot reveal simultaneous pivot nmt set use two pivot languages lead improvement fifty-eight bleu
use pretrained mask language model mlms drastically improve performance zero anaphora resolution zar expand approach novel pretraining task finetuning method japanese zar pretraining task aim acquire anaphoric relational knowledge necessary zar large scale raw corpus zar model finetuned manner pretraining experiment show combine propose methods surpass previous state art performance large margins provide insight remain challenge
neural machine translation inference procedures like beam search generate likely output model exacerbate demographic bias exhibit model focus gender bias result systematic errors grammatical gender translation lead human referents misrepresent misgendered approach problem adjust train data model contrast experiment simply adjust inference procedure experiment reranking nbest list use gender feature obtain automatically source sentence apply gender constraints decode improve nbest list gender diversity find combination techniques allow large gain winomt accuracy without require additional bilingual data additional nmt model
automate story generation remain difficult area research lack strong objective measure generate stories may linguistically sound many case suffer poor narrative coherence require compel logically sound story address present fabula entropy index fei evaluation method assess story coherence measure degree human participants agree answer true false question stories devise two theoretically ground measure reader question answer entropy entropy world coherence ewc entropy transitional coherence etc focus global local coherence respectively evaluate metrics test human write stories compare stories corrupt introduce incoherencies show control study entropy indices provide reliable objective measure story coherence
sequence sequence seq2seq model prevalent semantic parse find struggle distribution compositional generalization specialize model architectures pre train seq2seq model propose address issue former often come cost generality latter show limit success paper study impact intermediate representations compositional generalization pre train seq2seq model without change model architecture identify key aspects design effective representations instead train directly map natural language executable form map reversible lossy intermediate representation stronger structural correspondence natural language combination propose intermediate representations pre train model surprisingly effective best combinations obtain new state art cfq one hundred and forty-eight accuracy point template split three text sql datasets one hundred and fifty one hundred and ninety-four accuracy point work highlight intermediate representations provide important potentially overlook degree freedom improve compositional generalization abilities pre train seq2seq model
transformer language model become fundamental components natural language process base pipelines although several transformer model introduce serve many languages shortage model pre train low resource indigenous languages work introduce indt5 first transformer language model indigenous languages train indt5 build indcorpus new dataset ten indigenous languages spanish also present application indt5 machine translation investigate different approach translate spanish indigenous languages part contribution americasnlp two thousand and twenty-one share task open machine translation indt5 indcorpus publicly available research
mask language model mlms show superior performances numerous downstream nlp task use text encoders unfortunately mlms also demonstrate significantly worry level social bias show previously propose evaluation metrics quantify social bias mlms problematic due follow reason one prediction accuracy mask tokens tend low mlms raise question regard reliability evaluation metrics use pseudo likelihood predict tokens two correlation prediction accuracy mask performance downstream nlp task take consideration three high frequency word train data mask often introduce noise due selection bias test case overcome mention disfluencies propose unmask likelihood aul bias evaluation measure predict tokens test case give mlm embed unmask input find aul accurately detect different type bias mlms also propose aul attention weight aula evaluate tokens base importance sentence however unlike aul aula previously propose bias evaluation measure mlms systematically overestimate measure bias heavily influence unmask tokens context
language ground aim link symbolic representation language eg word rich perceptual knowledge outside world general approach embed textual visual information common space ground space confine explicit relationship modalities argue approach sacrifice abstract knowledge obtain linguistic co occurrence statistics process acquire perceptual information focus paper solve issue implicitly ground word embeddings rather learn two mappings joint space approach integrate modalities determine reversible ground map textual ground space mean multi task learn evaluations intrinsic extrinsic task show embeddings highly beneficial abstract concrete word strongly correlate human judgments outperform previous work wide range benchmarks ground embeddings publicly available
try apply recent advance natural language understand nlu technologies real world applications privacy preservation impose crucial challenge unfortunately well resolve address issue study improve effectiveness nlu model local privacy set use bert widely use pretrained language model lm example systematically study strengths weaknesses impose dx privacy relax variant local differential privacy different stag language model input text token embeddings sequence representations focus former two privacy constrain fine tune experiment reveal utility bert local privacy constraints importantly best knowledge first propose privacy adaptive lm pretraining methods demonstrate significantly improve model performance privatize text input also interpret level privacy preservation provide guidance privacy parameter selections
current study extractive question answer eqa model single span extraction set single answer span label predict give question passage pair set natural general domain eqa majority question general domain answer single span follow general domain eqa model current biomedical eqa bioeqa model utilize single span extraction set post process step paper investigate difference question distribution across general biomedical domains discover biomedical question likely require list type answer multiple answer factoid type answer single answer real world use case emphasize need biomedical eqa model able handle multiple question type base preliminary study propose multi span extraction set namely sequence tag approach bioeqa directly tackle question variable number phrase answer approach learn decide number answer question train data experimental result bioasq 7b 8b list type question outperform best perform exist model without require post process step
one challenge current sequence sequence seq2seq model process long sequence summarization document level machine translation task task require model reason token level well sentence paragraph level design study new hierarchical attention transformer base architecture hat outperform standard transformers several sequence sequence task particular model achieve stateof art result four summarization task include arxiv cnn dm samsum ami push pubmed r1 r2 sota model significantly outperform document level machine translation baseline twenty-eight bleu wmt19 en de document translation task also investigate hierarchical layer learn visualize hierarchical encoder decoder attention finally study hierarchical learn encoder pre train analyze performance classification downstream task
recent work crosslingual semantic parse successfully apply machine translation localize accurate parse new languages however advance assume access high quality machine translation systems tool word aligners test languages remove assumptions study cross lingual semantic parse zero shoot problem without parallel data seven test languages de zh fr es pt hi tr propose multi task encoder decoder model transfer parse knowledge additional languages use english logical form pair data unlabeled monolingual utterances test language train encoder generate language agnostic representations jointly optimize generate logical form utterance reconstruction language discriminability system frame zero shoot parse latent space alignment problem find pre train model improve generate logical form minimal cross lingual transfer penalty experimental result overnight new executable version multiatis find zero shoot approach perform back translation baselines case approach supervise upper bind
paper explore questeval text vs text metric adapt evaluation data text generation systems questeval reference less metric compare predictions directly structure input data automatically ask answer question adaptation data text straightforward require multi modal question generation answer qg qa systems purpose propose build synthetic multi modal corpora enable train multi modal qg qa result metric reference less multi modal obtain state art correlations human judgement e2e webnlg benchmark
automatic evaluation remain open research question natural language generation context sentence simplification particularly challenge task require nature replace complex word simpler ones share mean limit effectiveness n gram base metrics like bleu go hand hand recent advance nlg new metrics propose bertscore machine translation summarization questeval metric propose automatically compare two texts question paper first propose simple modification questeval allow tackle sentence simplification extensively evaluate correlations wrt human judgement several metrics include recent bertscore questeval show latter obtain state art correlations outperform standard metrics like bleu sari importantly also show large part correlations actually spurious metrics investigate phenomenon release new corpus evaluate simplifications time generate systems instead write humans allow us remove spurious correlations draw different conclusions original ones result better understand metrics particular raise concern low correlations traditional metrics result show significant measure mean preservation adaptation questeval
question answer qa task use benchmarks general machine intelligence therefore robust qa evaluation critical metrics indicate model answer question however major qa datasets skew distributions gender profession nationality despite skew model generalize find little evidence accuracy lower people base gender nationality instead variation question topic question ambiguity adequately access generalization qa systems require representative datasets
vector base language representations pretrained language model set new standard many nlp task yet complete account inner work particular entirely clear aspects sentence level syntax capture representations build along stack layer network paper aim address question general class interventional input perturbation base analyse representations pretrained language model import computational cognitive neuroscience notion representational invariance perform series probe design test sensitivity representations several kinds structure sentence probe involve swap word sentence compare representations perturb sentence original experiment three different perturbations one random permutations n grams vary width test scale representation sensitive word position two swap two span form syntactic phrase test sensitivity global phrase structure three swap two adjacent word break apart syntactic phrase test sensitivity local phrase structure result probe collectively suggest transformers build sensitivity larger part sentence along layer hierarchical phrase structure play role process broadly result also indicate structure input perturbations widen scope analyse perform often opaque deep learn systems serve complement exist tool supervise linear probe interpret complex black box model
novel neural architectures train strategies availability large scale corpora drive force behind recent progress abstractive text summarization however due black box nature neural model uninformative evaluation metrics scarce tool model data analysis true performance failure modes summarization model remain largely unknown address limitation introduce summvis open source tool visualize abstractive summaries enable fine grain analysis model data evaluation metrics associate text summarization lexical semantic visualizations tool offer easy entry point depth model prediction exploration across important dimension factual consistency abstractiveness tool together several pre compute model output available https githubcom robustness gym summvis
pre train transformer base sequence sequence model become go solution many text generation task include summarization however result produce model tend contain significant issue hallucinations irrelevant passages one solution mitigate problems incorporate better content plan neural summarization propose use entity chain ie chain entities mention summary better plan grind generation abstractive summaries particular augment target prepending entity chain experiment pre train finetuning content plan objective evaluate cnn dailymail samsum xsum model train objective improve entity correctness summary conciseness achieve state art performance rouge samsum xsum
train coreference resolution model require comprehensively label data model train one dataset may successfully transfer new domains paper investigate approach active learn coreference resolution feed discrete annotations incremental cluster model recent developments incremental coreference resolution allow novel approach active learn set new framework analyze important factor data acquisition like source model uncertainty balance read label cost explore different settings simulate label gold data lower data barrier coreference coreference resolvers rapidly adapt series previously unconsidered domains
release sina bert language model pre train bert devlin et al two thousand and eighteen address lack high quality persian language model medical domain sina bert utilize pre train large scale corpus medical content include formal informal texts collect variety online resources order improve performance health care relate task employ sina bert complete follow representative task categorization medical question medical sentiment analysis medical question retrieval task develop persian annotate data set train evaluation learn representation data task especially complex long medical question architecture use across task sina bert outperform bert base model previously make available persian language
rapid progress neural machine translation nmt systems last years drive primarily towards improve translation quality secondary focus improve robustness input perturbations eg spell grammatical mistake performance robustness important objectives focus risk overlook important properties paper draw attention fact applications faithfulness original input text important preserve even mean introduce unusual language pattern output translation propose simple novel way quantify whether nmt system exhibit robustness faithfulness focus case word order perturbations explore suite function perturb word order source sentence without delete inject tokens measure effect target side term robustness faithfulness across several experimental condition observe strong tendency towards robustness rather faithfulness result allow us better understand trade faithfulness robustness nmt open possibility develop systems users autonomy control select property best suit use case
work present methods learn cross lingual sentence representations use pair unpaired bilingual texts hypothesize cross lingual alignment strategy transferable therefore model train align two languages encode multilingually align representations transfer bilingual alignment multilingual alignment dual pivot transfer two pivot languages language pair study theory train unsupervised model unpaired sentence another single pair supervise model bitexts base unsupervised language model xlm r experiment evaluate model universal sentence encoders task unsupervised bitext mine two datasets unsupervised model reach state art unsupervised retrieval alternative single pair supervise model approach performance multilingually supervise model result suggest bilingual train techniques propose apply get sentence representations higher multilingual alignment
recent approach exploit weaknesses monolingual question answer qa model add adversarial statements passage attack cause reduction state art performance almost fifty paper first explore successfully attack multilingual qa mlqa system pre train multilingual bert use several attack strategies adversarial statement reduce performance much eighty-five show model give priority english language question regardless languages qa pair also show add attack strategies train help alleviate attack
target evaluations find machine translation systems often output incorrect gender even gender clear context furthermore incorrectly gendered translations potential reflect amplify social bias propose gender filter self train technique improve gender translation accuracy unambiguously gendered input approach use source monolingual corpus initial model generate gender specific pseudo parallel corpora add train data filter gender specific corpora source target side ensure sentence pair contain correctly translate specify gender evaluate approach translation english five languages find model improve gender translation accuracy without cost generic translation quality addition show viability approach several settings include train scratch fine tune control balance train data forward translation back translation
goal semantic role label srl recognise predicate argument structure sentence recent model show syntactic information enhance srl performance syntax agnostic approach achieve reasonable performance best way encode syntactic information srl task still open question paper propose syntax aware graph graph transformer syng2g tr architecture encode syntactic structure novel way input graph relations embeddings directly self attention mechanism transformer approach add soft bias towards attention pattern follow syntactic structure also allow model use information learn alternative pattern evaluate model dependency base span base srl datasets outperform previous syntax aware syntax agnostic model domain domain settings conll two thousand and five conll two thousand and nine datasets architecture general apply encode graph information desire downstream task
develop text normalization tn systems text speech tts new languages hard propose novel architecture facilitate multiple languages use data less three size data use state art result english treat tn sequence classification problem propose granular tokenization mechanism enable system learn majority class normalizations train data combine minimal precoded linguistic knowledge class publish first result tn tts spanish tamil also demonstrate performance approach comparable previous work do english annotate datasets use experimentation release https githubcom amazon research proteno
segmentation remain important preprocessing step languages word important syntactic semantic units like morphemes clearly delineate white space well deal continuous speech data often meaningful pause word near perfect supervise methods develop use resource rich languages chinese many world languages morphologically complex large dataset gold segmentations meaningful units solve problem propose new type segmental language model sun deng two thousand and eighteen kawakami et al two thousand and nineteen wang et al two thousand and twenty-one use unsupervised lightly supervise segmentation task introduce mask segmental language model mslm build span mask transformer architecture harness power bi directional mask model context attention series experiment model consistently outperform recurrent slms chinese pku corpus segmentation quality perform similarly recurrent model english ptb conclude discuss different challenge pose segment phonemic type write systems
work aim build dialogue agent weave new factual content conversations naturally humans draw insights linguistic principles conversational analysis annotate human human conversations switchboard dialog act corpus examine humans strategies acknowledgement transition detail selection presentation current chatbots explicitly provide new factual content introduce facts conversation generate responses acknowledge prior turn model train two contexts new factual content conversational history generate responses non specific wrt one contexts typically conversational history show specificity wrt conversational history better capture pointwise conditional mutual information textpcmih establish use pointwise mutual information textpmi propose method fuse pcmi trade textpmi textpcmih prefer humans overall quality max pmi baseline sixty time human evaluators also judge responses higher textpcmih better acknowledgement seventy-four time result demonstrate systems mimic human conversational traits case acknowledgement improve overall quality broadly illustrate utility linguistic principles improve dialogue agents
twitter serve data source many natural language process nlp task challenge identify topics twitter due continuous update data stream paper present unsupervised graph base framework identify evolution sub topics within two weeks real world twitter data first employ markov cluster algorithm mcl node removal method identify optimal graph cluster temporal graph word gow subsequently model cluster transition temporal graph identify topic evolution finally transition flow generate computational approach human annotations compare ensure validity framework
cross lingual entity alignment aim precisely connect entities different monolingual knowledge base kbs together often suffer challenge feature inconsistency sequence context unawareness paper present dual adversarial learn framework cross lingual entity alignment daea two original contributions first order address structural attribute feature inconsistency entities two knowledge graph kgs adversarial kernel embed technique propose extract graph invariant information unsupervised manner project two kgs common embed space second order improve successful rate entity alignment propose produce multiple random walk entity align mask entities random walk guidance know align entities context multiple random walk adversarial knowledge translation model develop fill translate mask entities pairwise random walk two kgs extensive experiment perform real world datasets show daea well solve feature inconsistency sequence context unawareness issue significantly outperform thirteen state art entity alignment methods
transformer base model modern work horse neural machine translation nmt reach state art across several benchmarks despite impressive accuracy observe systemic rudimentary class errors make transformer base model regard translate language mark gender nouns others find even surround context provide unambiguous evidence appropriate grammatical gender mark transformer base model test able accurately gender occupation nouns systematically release evaluation scheme dataset measure ability transformer base nmt model translate gender morphology correctly unambiguous contexts across syntactically diverse sentence dataset translate english source twenty languages several different language families availability dataset hope nmt community iterate solutions class especially egregious errors
sentence embed model aim provide general purpose embeddings sentence model study paper claim perform well sts task report suitability cluster paper look four recent sentence embed model universal sentence encoder cer et al two thousand and eighteen sentence bert reimers gurevych two thousand and nineteen laser artetxe schwenk two thousand and nineteen declutr giorgi et al two thousand and twenty give brief overview ideas behind implementations investigate well topic class two text classification datasets amazon review ni et al two thousand and nineteen news category dataset misra two thousand and eighteen map cluster correspond sentence embed space performance result classification model far perfect better random interest classification model construct unsupervised way topic class real life topic classification datasets partly reconstruct cluster correspond sentence embeddings
draw inferences open domain natural language predicate necessity true language understand much progress unsupervised learn entailment graph purpose make three contributions one reinterpret distributional inclusion hypothesis model entailment predicate different valencies like defeatbiden trump entail winbiden two actualize theory learn unsupervised multivalent entailment graph open domain predicate three demonstrate capabilities graph novel question answer task show directional entailment helpful inference bidirectional similarity question fine grain semantics also show draw evidence across valencies answer question use valency evidence
grammatical error correction gec suffer lack sufficient parallel data therefore gec study develop various methods generate pseudo data comprise pair grammatical artificially produce ungrammatical sentence currently mainstream approach generate pseudo data back translation bt previous gec study use bt employ architecture gec bt model however gec model different correction tendencies depend architectures thus study compare correction tendencies gec model train pseudo data generate different bt model namely transformer cnn lstm result confirm correction tendencies error type different every bt model additionally examine correction tendencies use combination pseudo data generate different bt model result find combination different bt model improve interpolate f05 score error type compare single bt model different seed
typical asr systems segment input audio utterances use purely acoustic information may resemble sentence like units expect conventional machine translation mt systems speak language translation work propose model correct acoustic segmentation asr model low resource languages improve performance downstream task propose use subtitle proxy dataset correct asr acoustic segmentation create synthetic acoustic utterances model common error modes train neural tag model correct asr acoustic segmentation show improve downstream performance mt audio document cross language information retrieval clir
model language train large corpora demonstrate useful nlp fix artifacts become object intense study many researchers probe extent linguistic abstractions factual commonsense knowledge reason abilities acquire readily demonstrate build line work consider new question type knowledge language model learn pretraining acquire plot probe performance across iterations use roberta case study among find linguistic knowledge acquire fast stably robustly across domains facts commonsense slower domain sensitive reason abilities general stably acquire new datasets pretraining protocols probe emerge believe probe across time analyse help researchers understand complex intermingle learn model undergo guide us toward efficient approach accomplish necessary learn faster
explicate implicit reason ie warrant arguments long stand challenge natural language understand systems recent approach focus explicate warrant via crowdsourcing expert annotations quality warrant questionable due extreme complexity subjectivity task paper tackle complex task warrant explication devise various methodologies collect warrant conduct extensive study train experts evaluate result warrant methodology find methodologies allow high quality warrant collect construct preliminary dataset six thousand warrant annotate six hundred arguments three debatable topics facilitate research relate downstream task release guidelines preliminary dataset
improvement machine learn base nlp performance often present bigger model complex code present trade better score come cost larger tool bigger model tend require train inference time present multiple methods measure size model compare model performance case study part speech tag apply techniques taggers eight languages present novel analysis identify taggers size performance optimal result indicate classical taggers place size performance skyline across languages although deep model highest performance multiple score often complex reach peak performance
pre train technique ubiquitous natural language process field prophetnet pre train base natural language generation method show powerful performance english text summarization question generation task paper extend prophetnet domains languages present prophetnet family pre train model name prophetnet x x english chinese multi lingual pre train cross lingual generation model prophetnet multi chinese generation model prophetnet zh two open domain dialog generation model prophetnet dialog en prophetnet dialog zh also provide plg program language generation model prophetnet code show generation performance besides nlg natural language generation task experiment prophetnet x model achieve new state art performance ten benchmarks model prophetnet x share model structure allow users easily switch different model make code model publicly available keep update pre train model finetuning script video introduce prophetnet x usage also release
real world information extraction ie system semi structure document image often involve long pipeline multiple modules whose complexity dramatically increase development maintenance cost one instead consider end end model directly map input target output simplify entire process however generation approach know lead unstable performance design carefully present recent effort transition exist pipeline base ie system end end system focus practical challenge associate replace deploy system real large scale production carefully formulate document ie sequence generation task show single end end ie system build still achieve competent performance
entity link task identify reference free text relevant knowledge base representations often focus single languages consider multilingual entity link single model train link reference language knowledge base several languages propose neural ranker architecture leverage multilingual transformer representations text easily apply multilingual set explore neural ranker train one language eg english transfer unseen language eg chinese find consistent large drop performance drop performance alleviate explore add adversarial objective force model learn language invariant representations find use approach improve recall several datasets often match language performance thus alleviate performance loss occur zero shoot transfer
paper present set experiment evaluate compare performance use cbow word2vec lemma2vec model arabic word context wic disambiguation without use sense inventory sense embeddings part semeval two thousand and twenty-one share task two wic disambiguation use devar ar dataset 2k sentence pair decide whether two word give sentence pair carry mean use two word2vec model wiki cbow pre train model arabic wikipedia another model train large arabic corpora three billion tokens two lemma2vec model also construct base two word2vec model four model use wic disambiguation task evaluate semeval two thousand and twenty-one testar ar dataset end report performance different model compare use lemma base word base model
language use differ domains even within domain language use change time previous work show adapt pretrained language model like bert domain continue pretraining improve performance domain downstream task article investigate whether adapt bert time addition domain increase performance even purpose introduce benchmark corpus social media comment sample three years corpus consist 3636m unlabelled comment adaptation evaluation upstream mask language model task well 09m label comment finetuning evaluation downstream document classification task find temporality matter task temporal adaptation improve upstream task performance temporal finetuning improve downstream task performance however find clear evidence adapt bert time domain improve downstream task performance adapt domain temporal adaptation capture change language use downstream task change actually relevant performance
adversarial attack show vulnerability machine learn model however non trivial conduct textual adversarial attack natural language process task due discreteness data previous approach conduct attack atomic textitreplacement operation usually lead fix length adversarial examples therefore limit exploration decision space paper propose variable length textual adversarial attacksvl attack integrate three atomic operations namely textitinsertion textitdeletion textitreplacement unify framework introduce manipulate special textitblank token attack way approach able comprehensively find adversarial examples around decision boundary effectively conduct adversarial attack specifically method drop accuracy imdb classification ninety-six edit thirteen tokens attack pre train bert model addition fine tune victim model generate adversarial sample improve robustness model without hurt performance especially length sensitive model task non autoregressive machine translation method achieve three thousand, three hundred and eighteen bleu score iwslt14 german english translation achieve improvement one hundred and forty-seven baseline model
winograd schema ws propose test measure commonsense capabilities model recently pre train language model base approach boost performance ws benchmarks source improvement still clear begin show current evaluation method ws sub optimal propose modification make use twin sentence evaluation also propose two new baselines indicate existence bias ws benchmarks finally propose method evaluate ws like sentence zero shoot set observe popular language model perform randomly set conclude much apparent progress ws may necessarily reflect progress commonsense reason much come supervise data likely account require commonsense reason skills knowledge
use pretrained word embeddings show effective way improve performance natural language process task fact almost natural language task think improve pretrained embeddings task range sentiment analysis translation sequence prediction amongst many others one successful word embeddings word2vec cbow model propose mikolov train negative sample technique mai et al modify objective train cmow embeddings sensitive word order use modify version negative sample objective context word model context embeddings taylor series rate matrices show different modes taylor series produce different type embeddings compare embeddings similar counterparts like cbow cmow show achieve comparable performance also introduce novel leave right context split objective improve performance task sensitive word order word2rate model ground statistical foundation use rate matrices competitive variety language task
benchmark provide ecosystem measure advancement model standard datasets automatic human evaluation metrics introduce indonlg first benchmark indonesian language natural language generation nlg cover six task summarization question answer open chitchat well three different language pair machine translation task provide vast clean pre train corpus indonesian sundanese javanese datasets call indo4b plus use train pre train nlg model indobart evaluate effectiveness efficiency indobart conduct extensive evaluation indonlg task find show indobart achieve competitive performance indonesian task five time fewer parameters compare largest multilingual model benchmark mbart large liu et al two thousand and twenty almost 4x 25x faster inference time cpu gpu respectively additionally demonstrate ability indobart learn javanese sundanese achieve decent performance machine translation task
neural knowledge ground generative model dialogue often produce content factually inconsistent source text rely consequence model unreliable limit real world applicability inspire recent work evaluate factual consistency abstractive summarization durmus et al two thousand and twenty wang et al two thousand and twenty propose automatic evaluation metric factual consistency knowledge ground dialogue model use automatic question generation question answer unlike previous work use naive token base comparison answer span metric make use co reference resolution natural language inference capabilities greatly improve performance foster proper evaluation curate novel dataset state art dialogue system output wizard wikipedia dataset dinan et al two thousand and nineteen manually annotate factual consistency perform thorough meta evaluation metric metrics use new dataset two others greatly outperform baselines
machine translation model discrete vocabularies commonly use subword segmentation techniques achieve open vocabulary approach rely consistent correct underlie unicode sequence make model susceptible degradation common type noise variation motivate robustness human language process propose use visual text representations dispense finite set text embeddings favor continuous vocabularies create process visually render text show model use visual text representations approach match performance text baselines clean ted datasets importantly model visual embeddings demonstrate significant robustness vary type noise achieve eg two hundred and fifty-nine bleu character permute german english task subword model degrade nineteen
propose multi task probabilistic approach facilitate distantly supervise relation extraction bring closer representations sentence contain knowledge base pair achieve bias latent space sentence via variational autoencoder vae train jointly relation classifier latent code guide pair representations influence sentence reconstruction experimental result two datasets create via distant supervision indicate multi task learn result performance benefit additional exploration employ knowledge base priors vae reveal sentence space shift towards knowledge base offer interpretability improve result
design better automate dialogue evaluation metrics offer potential accelerate evaluation research conversational ai however exist trainable dialogue evaluation model generally restrict classifiers train purely supervise manner suffer significant risk adversarial attack eg nonsensical response enjoy high classification score alleviate risk propose adversarial train approach learn robust model att adversarial turing test discriminate machine generate responses human write reply contrast previous perturbation base methods discriminator train iteratively generate unrestricted diverse adversarial examples use reinforcement learn key benefit unrestricted adversarial train approach allow discriminator improve robustness iterative attack defense game discriminator show high accuracy strong attackers include dialogpt gpt three
intermediate task fine tune show culminate large transfer gain across many nlp task abundance candidate datasets well pre train language model become infeasible run cross product combinations find best transfer set work first establish similar sequential fine tune gain achieve adapter settings subsequently consolidate previously propose methods efficiently identify beneficial task intermediate transfer learn experiment diverse set forty-two intermediate eleven target english classification multiple choice question answer sequence tag task result show efficient embed base methods rely solely respective datasets outperform computational expensive shoot fine tune approach best methods achieve average regret3 less one across target task demonstrate able efficiently identify best datasets intermediate train
script standardize event sequence describe typical everyday activities show help understand narratives provide expectations resolve ambiguity fill unstated information however date prove hard author extract text work demonstrate first time pre train neural language model lms finetuned generate high quality script vary level granularity wide range everyday scenarios eg bake cake collect large 64k crowdsourced partially order script name proscript substantially larger prior datasets develop model generate script combine language generation structure prediction define two complementary task edge prediction give scenario unordered events organize events valid possibly partial order script ii script generation give scenario generate events organize possibly partial order script experiment show model perform well eg f1757 task illustrate new approach overcome previous barriers script collection also show still significant room improvement toward human level performance together task dataset model offer new research direction learn script knowledge
exist document level neural machine translation nmt model leverage fix number previous global source sentence handle context independent problem standard nmt however translate source sentence benefit various size context inappropriate context may harm translation performance work introduce data adaptive method enable model adopt necessary useful context specifically introduce light predictor two document level translation model select explicit context experiment demonstrate propose approach significantly improve performance previous methods gain one hundred and ninety-nine bleu point
paper introduce scigen new challenge dataset task reason aware data text generation consist table scientific article correspond descriptions describe scientific table go beyond surface realization table content require reason table value unique properties scigen one table mostly contain numerical value two correspond descriptions require arithmetic reason scigen therefore first dataset assess arithmetic reason capabilities generation model complex input structure ie table scientific article study effectiveness state art data text generation model scigen evaluate result use common metrics well human evaluation result analyse show humans like reason describe scientific table ability state art model severely limit task b add train data improve result solution reason aware text generation c one main bottleneck task lack proper automatic evaluation metrics data code annotations human evaluation available https githubcom ukplab scigen scigen open new avenues future research reason aware text generation evaluation
deep neural network dnn model show high empirical privacy leakages clinical language model clms train clinical data use improve performance biomedical natural language process task work investigate risk train data leakage white box black box access clms design employ membership inference attack estimate empirical privacy leak model architectures like bert gpt2 show membership inference attack clms lead non trivial privacy leakages seven result show smaller model lower empirical privacy leakages larger ones mask lms lower leakages auto regressive lms show differentially private clms improve model utility clinical domain ensure low empirical privacy leakage lastly also study effect group level membership inference disease rarity clm privacy leakages
large language model show promise result zero shoot settings brown et al2020 radford et al two thousand and nineteen example perform multiple choice task simply condition question select answer highest probability however rank string probability problematic due surface form competition wherein different surface form compete probability mass even represent underlie concept eg computer pc since probability mass finite lower probability correct answer due competition string valid answer one multiple choice options introduce domain conditional pointwise mutual information alternative score function directly compensate surface form competition simply reweighing option accord term proportional priori likelihood within context specific zero shoot task achieve consistent gain zero shoot performance calibrate zhao et al two thousand and twenty-one uncalibrated score function gpt two gpt three model variety multiple choice datasets
active learn al method iteratively select data annotation pool unlabeled data aim achieve better model performance random selection previous al approach natural language process nlp limit either task specific model train scratch iteration use label data hand use shelf pretrained language model lms adapt effectively downstream task paper address limitations introduce balm bayesian active learn pretrained language model first propose adapt pretrained lm downstream task continue train available unlabeled data use al also suggest simple yet effective fine tune method ensure adapt lm properly train low high resource scenarios al finally apply monte carlo dropout downstream model obtain well calibrate confidence score data selection uncertainty sample experiment five standard natural language understand task demonstrate balm provide substantial data efficiency improvements compare various combinations acquisition function model fine tune methods propose recent al literature
stories narratives compose base variety events understand events semantically relate essence read comprehension recent event centric read comprehension datasets focus either event arguments event temporal commonsense although task evaluate machine ability narrative understand human like read comprehension require capability process event base semantics beyond arguments temporal commonsense example understand causality events need infer motivations purpose understand event hierarchy need parse composition events facilitate task introduce ester comprehensive machine read comprehension mrc dataset event semantic relation reason study five commonly use event semantic relations formulate question answer task experimental result show current sota systems achieve six hundred and five five hundred and seventy-eight seven hundred and sixty-three event base f1 token base f1 hit1 score respectively significantly human performances
image become integral part online media enhance self expression dissemination knowledge pose serious accessibility challenge adequate textual descriptions rare caption abundant consistently provide need descriptive detail systems train texts inherit shortcomings address introduce publicly available wikipedia base corpus concadia consist ninety-six thousand, nine hundred and eighteen image correspond english language descriptions caption surround context use concadia characterize commonalities differences descriptions caption lead us hypothesis caption substitute descriptions provide useful signal create effective descriptions substantiate hypothesis show image caption systems train concadia benefit caption embeddings part input experiment also begin show concadia powerful tool address underlie accessibility issue pose image data
summarization systems face core challenge identify select important information paper tackle problem content selection unsupervised extractive summarization long structure document introduce wide range heuristics leverage cognitive representations content units retain forget human memory find properties representations human memory exploit capture relevance content units scientific article experiment show propose heuristics effective leverage cognitive structure organization document ie section article automatic human evaluations provide strong evidence heuristics extract summary worthy content units
tacred one largest widely use sentence level relation extraction datasets propose model evaluate use dataset consistently set new state art performance however still exhibit large error rat despite leverage external knowledge unsupervised pretraining large text corpora recent study suggest may due poor dataset quality study observe fifty challenge sentence development test set incorrectly label account average drop eight f1 score model performance however study limit small bias sample 5k total 106k sentence substantially restrict generalizability broader implications find paper address shortcomings perform comprehensive study whole tacred dataset ii propose improve crowdsourcing strategy deploy annotate whole dataset iii perform thorough analysis understand correct tacred annotations affect previously publish result verification observe two hundred and thirty-nine tacred label incorrect moreover evaluate several model revise dataset yield average f1 score improvement one hundred and forty-three help uncover significant relationships different model rather simply offset scale score constant factor finally aside analysis also release tacred new completely annotate version tacred dataset use perform reliable evaluation relation extraction model
abstractive conversation summarization receive much attention recently however generate summaries often suffer insufficient redundant incorrect content largely due unstructured complex characteristics human human interactions end propose explicitly model rich structure conversations precise accurate conversation summarization first incorporate discourse relations utterances action triple utterances structure graph better encode conversations design multi granularity decoder generate summaries combine level information experiment show propose model outperform state art methods generalize well domains term automatic evaluations human judgments publicly release code https githubcom gt salt structure aware bart
grow evidence pretrained language model improve task specific fine tune languages see pretraining also new languages even non linguistic data nature surprise cross domain transfer offer partial answer via systematic exploration much transfer occur model deny information word identity via random scramble four classification task two sequence label task evaluate baseline model lstms use glove embeddings bert find bert show high rat transfer scramble domains classification sequence label task analyse seek explain transfer succeed task others isolate separate contributions pretraining versus fine tune quantify role word frequency find help explain cross domain transfer occur guide future study practical fine tune efforts
relate entities events text key component natural language understand cross document coreference resolution particular important grow interest multi document analysis task work propose new model extend efficient sequential prediction paradigm coreference resolution cross document settings achieve competitive result entity event coreference provide strong evidence efficacy sequential model higher order inference cross document settings model incrementally compose mention cluster representations predict link mention already construct cluster approximate higher order model addition conduct extensive ablation study provide new insights importance various input representation type coreference
recently end end mispronunciation detection diagnosis mdandd systems become popular alternative greatly simplify model build process conventional hybrid dnn hmm systems represent complicate modules single deep network architecture paper order utilize prior text end end structure present novel text dependent model difference sed mdd model achieve fully end end system align audio phoneme sequence prior text inside model attention mechanism moreover prior text input problem imbalance positive negative sample phoneme sequence alleviate problem propose three simple data augmentation methods effectively improve ability model capture mispronounce phonemes conduct experiment l2 arctic best performance improve four thousand, nine hundred and twenty-nine five thousand, six hundred and eight f measure metric compare cnn rnn ctc model
present hierarchical transformer network model long term dependencies across clinical note purpose patient level prediction network equip three level transformer base encoders learn progressively word sentence sentence note finally note patients first level word sentence directly apply pre train bert model second third level implement stack two layer encoders final patient representation feed classification layer clinical predictions compare traditional bert model model increase maximum input length five hundred and twelve word much longer sequence appropriate long sequence clinical note empirically examine experiment different parameters identify optimal trade give computational resource limit experimental result mimic iii dataset different prediction task demonstrate propose hierarchical model outperform previous state art hierarchical neural network
deep learn techniques achieve great success many field time deep learn model get complex expensive compute severely hinder wide applications model order alleviate problem model distillation emerge effective mean compress large model smaller one without significant drop accuracy paper study relate orthogonal issue data distillation aim distill knowledge large train dataset smaller synthetic one potential address large grow neural network train problem base small dataset develop novel data distillation method text classification evaluate method eight benchmark datasets result distil data size one original text data achieve approximately ninety performance original rather impressive
adversarial attack methods text classification change classifier prediction synonym substitution propose adversarial sentence rewrite sampler asrs rewrite whole sentence generate similar higher quality adversarial examples method achieve better attack success rate four seven datasets well significantly better sentence quality seven datasets asrs indispensable supplement exist attack methods classifiers resist attack asrs unless train adversarial examples find asrs
dialogue systems power large pre train language model lm exhibit innate ability deliver fluent natural look responses despite impressive generation performance model often generate factually incorrect statements impede widespread adoption paper focus task improve faithfulness thus reduce hallucination neural dialogue systems know facts supply knowledge graph kg propose neural path hunter follow generate refine strategy whereby generate response amend use k hop subgraph kg neural path hunter leverage separate token level fact critic identify plausible source hallucination follow refinement stage consist chain two neural lm retrieve correct entities craft query signal propagate k hop subgraph propose model easily apply dialogue generate responses without retrain model empirically validate propose approach opendialkg dataset suite metrics report relative improvement faithfulness gpt2 dialogue responses eighty-four
academic neural model coreference resolution typically train single dataset ontonotes model improvements benchmarked dataset however real world usages coreference resolution model depend annotation guidelines domain target dataset often differ ontonotes aim quantify transferability coreference resolution model base number annotate document available target dataset examine five target datasets find continue train consistently effective especially beneficial target document establish new benchmarks across several datasets include state art result litbank preco
theme study syntactic structure data longobardi 2017b collins two thousand and ten ceolin et al two thousand and twenty koopman two thousand and eleven use general markov model initiate shu et al two thousand and seventeen explore question consistent data idea general markov model ideas explore present paper generally applicable set syntactic structure use analyze consistency data general markov model additionally give interpretation methods ceolin et al two thousand and twenty infinite sit evolutionary model compare markov model explore context evolutionary process act human language syntax
introduce multilabel probe task assess morphosyntactic representations word embeddings multilingual language model demonstrate task multilingual bert devlin et al two thousand and eighteen train probe seven typologically diverse languages vary morphological complexity afrikaans croatian finnish hebrew korean spanish turkish simple robust paradigm show multilingual bert render many morphosyntactic feature easily simultaneously extractable eg gender grammatical case pronominal type evaluate probe six hold languages zero shoot transfer set arabic chinese marathi slovenian tagalog yoruba style probe add benefit reveal linguistic properties language model recognize share across languages instance probe perform well recognize nouns hold languages suggest multilingual bert conception noun hood transcend individual languages yet true adjectives
word frequency pre train data affect behavior similarity metrics contextualized bert embeddings systematic ways word relationships exaggerate understate work explore geometric characteristics contextualized word embeddings two novel tool one identity probe predict identity word use embed two minimal bound sphere word contextualized representations result reveal word high low frequency differ significantly respect representational geometry differences introduce distortions compare human judgments point estimate embed similarity eg cosine similarity estimate semantic similarity two word depend frequency word train data downstream societal implications bert base trouble differentiate south american african countries north american european ones find distortions persist use bert multilingual suggest easily fix additional data turn introduce new distortions
neural machine translation nmt recently gain widespread attention high translation accuracy however show poor performance translation long sentence major issue low resource languages assume issue cause insufficient number long sentence train data therefore study propose simple data augmentation method handle long sentence method use give parallel corpora train data generate long sentence concatenate two sentence base experimental result confirm improvements long sentence translation propose data augmentation method despite simplicity moreover translation quality improve propose method combine back translation
multi domain sentiment classification deal scenario label data exist multiple domains insufficient train effective sentiment classifiers work across domains thus fully exploit sentiment knowledge share across domains crucial real world applications many exist work try extract domain invariant feature high dimensional space model fail explicitly distinguish share private feature text level extent lack interpretablity base assumption remove domain relate tokens texts would help improve domain invariance instead first transform original sentence domain agnostic end propose bertmasker network explicitly mask domain relate word texts learn domain invariant sentiment feature domain agnostic texts use mask word form domain aware sentence representations empirical experiment well adopt multiple domain sentiment classification dataset demonstrate effectiveness propose model multi domain sentiment classification cross domain settings increase accuracy ninety-four eighteen respectively analysis mask prove remove domain relate sentiment irrelevant tokens decrease texts domain distinction result performance degradation bert base domain classifier twelve
explore shoot learn fsl relation classification rc focus realistic scenario fsl test instance might belong target categories none aka nota first revisit recent popular dataset structure fsl point unrealistic data distribution remedy propose novel methodology derive realistic shoot test data available datasets supervise rc apply tacred dataset yield new challenge benchmark fsl rc state art model show poor performance next analyze classification scheme within popular embed base nearest neighbor approach fsl respect constraints impose embed space trigger analysis propose novel classification scheme nota category represent learn vectors show empirically appeal option fsl
neural model various flavour morphological inflection task prove extremely accurate give ample label data data may slow costly obtain work aim overcome annotation bottleneck bootstrapping label data seed little five label paradigms accompany large bulk unlabeled text approach exploit different kinds regularities morphological systems two phase setup word tag base analogies follow word pair base distance experiment paradigm cell fill problem eight typologically different languages find languages relatively simple morphology orthographic regularities allow inflection model achieve respectable accuracy combine orthographic semantic regularities alleviate difficulties particularly complex morpho phonological systems result suggest hand craft many tag examples might unnecessary effort however work need order address rarely use form
present systematic study multilingual cross lingual intent detection speak data study leverage new resource put forth work term mind fourteen first train evaluation resource intent detection task speak data cover fourteen intents extract commercial system e bank domain associate speak examples fourteen diverse language varieties key result indicate combine machine translation model state art multilingual sentence encoders eg labse yield strong intent detectors majority target languages cover mind fourteen offer comparative analyse across different ax eg zero shoot versus shoot learn translation direction impact speech recognition see work important step towards inclusive development evaluation multilingual intent detectors speak data much wider spectrum languages compare prior work
recent years automate approach assess linguistic complexity second language l2 write make significant progress gauge learner performance predict human rat quality learner productions benchmarking l2 development contrast comparatively little work area speak particularly respect fully automate approach assess l2 spontaneous speech importance well perform asr system widely recognize little research conduct investigate impact performance subsequent automatic text analysis paper focus issue examine impact use state art asr system subsequent automatic analysis linguistic complexity spontaneously produce l2 speech set thirty-four select measure consider fall four categories syntactic lexical n gram frequency information theoretic measure agreement score measure obtain basis asr generate vs manual transcriptions determine correlation analysis differential effect asr performance specific type complexity measure control task type effect also present
authorship attribution problem identify plausible author anonymous text set candidate author researchers investigate topic cross topic scenarios authorship attribution differ accord whether unseen topics use test phase however neither scenario allow us explain whether errors cause failure capture authorship style topic shift factor motivate propose emphtopic confusion task switch author topic configuration train test set setup allow us probe errors attribution process investigate accuracy two error measure one cause model confusion switch feature capture topics one cause feature inability capture write style lead weaker model evaluate different feature show stylometric feature part speech tag less susceptible topic variations increase accuracy attribution process show combine word level n grams outperform state art technique cross topic scenario finally show pretrained language model bert roberta perform poorly task outperform simple n gram feature
language usage change time impact effectiveness nlp systems work investigate methods adapt change discourse crisis events explore social media data crisis effective time sensitive methods necessary experiment two separate methods accommodate change data temporal pretraining use unlabeled data target time periods train better language model model embed shift base tool analyze semantic change shift allow us counteract temporal drift normalize incoming data base observe pattern language change simulate scenarios lack access incoming label data demonstrate effectiveness methods wide variety crises show improve performance eighty f1 score relevance classification across datasets
community question answer cqa forums stack overflow yahoo answer contain rich resource answer wide range question question thread receive large number answer different perspectives goal multi perspective answer summarization produce summary include perspectives answer major obstacle multi perspective abstractive answer summarization absence dataset provide supervision produce summaries work introduce novel dataset creation method automatically create multi perspective bullet point abstractive summaries exist cqa forum supervision provide dataset train model inherently produce multi perspective summaries additionally train model output diverse faithful answer summaries retain multiple perspectives propose multi reward optimization technique couple sentence relevance prediction multi task loss methods demonstrate improve coverage perspectives faithfulness measure automatic human evaluations compare strong baseline
word mean notoriously difficult capture synchronically diachronically paper describe creation largest resource grade contextualized diachronic word mean annotation four different languages base one hundred thousand human semantic proximity judgments thoroughly describe multi round incremental annotation process choice cluster algorithm group usages sense possible diachronic synchronic use dataset
task orient dialogue tod user hold conversation artificial agent complete concrete task although technology represent one central objectives ai focus ever intense research development efforts currently limit narrow domains eg food order ticket book handful languages eg english chinese work provide extensive overview exist methods resources multilingual tod entry point excite emerge field find critical factor prevent creation truly multilingual tod systems lack datasets languages train evaluation fact acquire annotations human feedback component modular systems data hungry end end systems expensive tedious hence state art approach multilingual tod mostly rely zero shoot cross lingual transfer resource rich languages almost exclusively english either mean machine translation multilingual representations approach currently viable typologically similar languages languages parallel monolingual corpora available hand effectiveness beyond boundaries doubtful hard assess due lack linguistically diverse benchmarks especially natural language generation end end evaluation overcome limitation draw parallel components tod pipeline nlp task inspire solutions learn low resource scenarios finally list additional challenge multilinguality pose relate areas speech human centre evaluation indicate future directions hold promise expand language coverage dialogue capabilities current tod systems
exist neural machine translation system achieve near human level performance general domain languages lack parallel corpora pose key problem specific domains biomedical domain parallel corpus less accessible work present new unsupervised sentence alignment method explore feature train biomedical neural machine translation nmt systems use simple effective way build bilingual word embeddings bwes evaluate bilingual word similarity transfer sentence alignment problem extend earth mover distance emd problem propose method achieve high accuracy one one many many case pre train general domain larger domain dataset n sentence pair benefit nmt model fine tune domain corpus help translation model learn terminology fit domain style text
cross lingual name entity lexicon important resource multilingual nlp task machine translation cross lingual wikification knowledge base contain large number entities high resource languages english french correspond entities lower resource languages often miss address propose lexical semantic phonetic align lsp align technique automatically mine cross lingual entity lexicon web demonstrate lsp align outperform baselines extract cross lingual entity pair mine one hundred and sixty-four million entity pair one hundred and twenty different languages align english release cross lingual entity pair along massively multilingual tag name entity corpus resource nlp community
language powerful tool use correct manner major mode communication use correct choice word style serve long last impact stylistics study use various language style communication pass message bigger impact communicate indirectly stylistic analysis therefore study use linguistic style texts determine style use communicate communicate honest deception use choice word imply something different literal mean person listen read text honest deception use literal understand may completely miss point issue honesty falsehood arise however would better understand honest deception use intention last impact rather deceive readers viewers listeners major style use honest deception hyperboles litotes irony sarcasm seinfeld sitcom tv series situational tv comedy show air one thousand, nine hundred and ninety one thousand, nine hundred and ninety-eight show attempt bring understand daily life comedian comedian view life experience convert hilarious joke also show jerry struggle get right partner many women come life reflect honest deception seinfeld sitcom tv series paper go investigate honest deception use series use communicate study go use recapitulative form give better analysis group different style use honest deception throughout series
numerous online conversations produce daily basis result press need conversation understand basis structure discussion identify respond relations conversation discourse link response utterances initiations figure respond explore consistency topic content dependency discourse roles indicate interactions whereas prior work ignore effect latent factor underlie word occurrences propose model learn latent topics discourse word distributions predict pairwise initiation response link via exploit topic consistency discourse dependency experimental result english chinese conversations show model significantly outperform previous state arts seventy-nine vs seventy-three mrr chinese customer service dialogues probe output would light topics discourse indicate conversational user interactions
although research emotion classification significantly progress high resource languages still infancy resource constrain languages like bengali however unavailability necessary language process tool deficiency benchmark corpora make emotion classification task bengali challenge complicate work propose transformer base technique classify bengali text one six basic emotions anger fear disgust sadness joy surprise bengali emotion corpus consist six thousand, two hundred and forty-three texts develop classification task experimentation carry use various machine learn lr rf mnb svm deep neural network cnn bilstm cnnbilstm transformer bangla bert bert xlm r base approach experimental outcomes indicate xlm r outdo techniques achieve highest weight f1 score six thousand, nine hundred and seventy-three test data dataset publicly available https githubcom omar sharif03 naacl srw two thousand and twenty-one
real world impact polarization toxicity online sphere mark end two thousand and twenty begin year negative way semeval two thousand and twenty-one task five toxic span detection base novel annotation subset jigsaw unintended bias dataset first language toxicity detection task dedicate identify toxicity level span task participants automatically detect character span short comment render message toxic model consider apply virtual adversarial train semi supervise set fine tune process several transformer base model ie bert roberta combination conditional random field approach lead performance improvements robust model enable us achieve f1 score six thousand, five hundred and seventy-three official submission f1 score six thousand, six hundred and thirteen tune post evaluation
capture word mean context distinguish correspondences variations across languages key build successful multilingual cross lingual text representation model however exist multilingual evaluation datasets evaluate lexical semantics context various limitations particular one language coverage restrict high resource languages skew favor language families areas two design make task solvable via superficial cue result artificially inflate sometimes super human performances pretrained encoders many target languages limit usefulness model probe diagnostics three support cross lingual evaluation order address gap present am2ico adversarial multilingual mean context wide coverage cross lingual multilingual evaluation set aim faithfully assess ability state art sota representation model understand identity word mean cross lingual contexts fourteen language pair conduct series experiment wide range setups demonstrate challenge nature am2ico result reveal current sota pretrained encoders substantially lag behind human performance largest gap observe low resource languages languages dissimilar english
distance word calculate word units study compare distributions random matrix theory rmt find distribution distance word well describe single parameter brody distribution use brody distribution fit find distance give word set texts show mix dynamics coexist regular chaotic regimes find distributions correctly fit brody distribution certain goodness fit threshold identifid stop word usually consider uninformative part text apply various threshold value goodness fit extract uninformative word texts analysis desire extent basis formulate fully agnostic recipe use creation customize set stop word texts language base word
recent years pre train multilingual language model multilingual bert xlm r exhibit good performance zero shoot cross lingual transfer learn however since multilingual contextual embed space different languages perfectly align difference representations different languages might zero shoot cross lingual transfer fail case work draw connections fail case adversarial examples propose use robust train methods train robust model tolerate noise input embeddings study two widely use robust train methods adversarial train randomize smooth experimental result demonstrate robust train improve zero shoot cross lingual transfer text classification performance improvements become significant distance source language target language increase
much recent work nlp document dataset artifacts bias spurious correlations input feature output label however tell feature spurious instead legitimate correlations typically leave unspecified work argue complex language understand task simple feature correlations spurious formalize notion class problems call competency problems example word amaze give information sentiment label independent context appear could include negation metaphor sarcasm etc theoretically analyze difficulty create data competency problems human bias take account show realistic datasets increasingly deviate competency problems dataset size increase analysis give us simple statistical test dataset artifacts use show subtle bias describe prior work include demonstrate model inappropriately affect less extreme bias theoretical treatment problem also allow us analyze propose solutions make local edit dataset instance give recommendations future data collection model design efforts target competency problems
recent efforts information extraction rely many deep neural model however model easily overfit noisy label suffer performance degradation costly filter noisy label large learn resources recent study show label take train step memorize frequently forget clean label therefore identifiable train motivate properties propose simple co regularization framework entity centric information extraction consist several neural model different parameter initialization model jointly optimize task specific loss regularize generate similar predictions base agreement loss prevent overfitting noisy label end take train model inference extensive experiment two widely use noisy benchmarks information extraction tacred conll03 demonstrate effectiveness framework
dependency parse tool widely use field natural language process computational linguistics however hardly work connect dependency parse monotonicity essential part logic linguistic semantics paper present system automatically annotate monotonicity information base universal dependency parse tree system utilize surface level monotonicity facts quantifiers lexical items token level polarity information compare system performance exist systems literature include natlog ccg2mono small evaluation dataset result show system outperform natlog ccg2mono
idioms unlike phrase two important ways first word idiom unconventional mean second unconventional mean word idiom contingent presence word idiom linguistic theories disagree whether two properties depend one another well whether special theoretical machinery need accommodate idioms define two measure correspond two properties show idioms fall expect intersection two dimension dimension correlate result suggest idioms anomalous type phrase introduce special machinery handle idioms may warrant
numerous work analyze bias vision pre train language model individually however less attention pay bias interact multimodal settings work extend text base bias analysis methods investigate multimodal language model analyze intra inter modality associations bias learn model specifically demonstrate vl bert su et al two thousand and twenty exhibit gender bias often prefer reinforce stereotype faithfully describe visual scene demonstrate find control case study extend larger set stereotypically gendered entities
communicate new research ideas involve highlight similarities differences past work author write fluent often long section survey distinction new paper relate work work model generate relate work section cognisant motivation behind cite paper content plan model generate tree cite paper surface realization model lexicalize skeleton model outperform several strong state art summarization multi document summarization model generate relate work acl anthology aa base dataset contribute
self supervise learn make rapid advance natural language process remain unclear researchers engage resource intensive domain specific pretraining domain pretraining law puzzlingly yield document instance substantial gain domain pretraining spite fact legal language widely see unique hypothesize exist result stem fact exist legal nlp task easy fail meet condition domain pretraining help address first present casehold case hold legal decisions new dataset comprise fifty-three thousand multiple choice question identify relevant hold cite case dataset present fundamental task lawyers legally meaningful difficult nlp perspective f1 four bilstm baseline second assess performance gain casehold exist legal nlp datasets transformer architecture bert pretrained general corpus google book wikipedia improve performance domain pretraining use corpus approximately 35m decisions across court yous larger bert custom legal vocabulary exhibit substantial performance gain casehold gain seventy-two f1 represent twelve improvement bert consistent performance gain across two legal task third show domain pretraining may warrant task exhibit sufficient similarity pretraining corpus level performance increase three legal task directly tie domain specificity task find inform researchers engage resource intensive pretraining show transformer base architectures learn embeddings suggestive distinct legal language
word vector representations reveal emotions associate word study consider task estimate word level emotion intensity score specific emotions explore unsupervised supervise finally self supervise method extract emotional associations word vector representations overall find word vectors carry substantial potential induce fine grain emotion intensity score show far higher correlation human grind truth rat achieve state art emotion lexicons
work explore prompt tune simple yet effective mechanism learn soft prompt condition freeze language model perform specific downstream task unlike discrete text prompt use gpt three soft prompt learn backpropagation tune incorporate signal number label examples end end learn approach outperform gpt three shoot learn large margin remarkably ablations model size use t5 show prompt tune become competitive scale model exceed billions parameters method close gap match strong performance model tune model weight tune find especially relevant large model costly share serve ability reuse one freeze model multiple downstream task ease burden method see simplification recently propose prefix tune li liang two thousand and twenty-one provide comparison similar approach finally show condition freeze model soft prompt confer benefit robustness domain transfer compare full model tune
multilingual t5 mt5 pretrains sequence sequence model massive monolingual texts show promise result many cross lingual task paper improve multilingual text text transfer transformer translation pair mt6 specifically explore three cross lingual text text pre train task namely machine translation translation pair span corruption translation span corruption addition propose partially non autoregressive objective text text pre train evaluate methods seven multilingual benchmark datasets include sentence classification name entity recognition question answer abstractive summarization experimental result show propose mt6 improve cross lingual transferability mt5
large scale pretrained language model surprisingly good recall factual knowledge present train corpus paper explore implicit knowledge store pretrained transformers introduce concept knowledge neurons give relational fact propose knowledge attribution method identify neurons express fact present activation knowledge neurons highly correlate expression correspond facts addition even without fine tune leverage knowledge neurons explicitly edit update erase specific factual knowledge pretrained transformers
complex natural language understand modules dialog systems richer understand user utterances thus critical provide better user experience however model often create scratch specific clients use case require annotation large datasets encourage share annotate data across multiple clients facilitate introduce idea intent feature domain topic agnostic properties intents learn syntactic cue hence share introduce new neural network architecture global local model show significant improvement strong baselines identify feature deploy multi intent natural language understand module generally classification set part utterance classify utilize whole context
dense retrieval show effective retrieve relevant document open domain qa surpass popular sparse retrieval methods like bm25 realm guu et al two thousand and twenty end end dense retrieval system rely mlm base pretraining improve downstream qa efficiency across multiple datasets study finetuning realm various qa task explore limit various hyperparameter supervision choices find realm significantly undertrained finetuning simple improvements train supervision inference setups significantly benefit qa result exceed performance model publish post best model realm incorporate best work find achieve significant qa accuracy improvements baselines fifty-five absolute accuracy without model design change additionally realm match performance large open domain qa model 3x parameters demonstrate efficiency setup
task identify reason circumstantial precondition associate everyday facts natural humans unclear whether state art language model lms understand implicit precondition enable invalidate commonsense facts glass use drink water despite impressive accuracy exist commonsense task paper propose new problem reason circumstantial precondition present dataset call corequisite annotate commonsense facts precondition express natural language base resource create three canonical evaluation task use examine capability exist lms understand situational pre condition result show ten 30gap machine human performance task make resources software publicly available
popular natural language process task decades ago word alignment dominate recently giza statistical method base thirty year old ibm model though recent years finally see giza performance best new methods primarily rely large machine translation model massively multilingual language model supervision giza alignments introduce embed enhance giza outperform giza without aforementioned factor take advantage monolingual embed space geometry source target language exceed giza performance every test scenario three languages lowest resource scenario five hundred line bitext improve performance giza one hundred and nine aer method scale monotonically outperform giza test scenarios five hundred nineteen million line bitext code make publicly available
hashtag annotation microblog post recently formulate sequence generation problem handle emerge hashtags unseen train set state art method leverage conversations initiate post enrich contextual information short post however unrealistic assume existence conversations hashtag annotation therefore propose leverage news article publish microblog post generate hashtags follow retriever generator framework extensive experiment english twitter datasets demonstrate superior performance significant advantage leverage news article generate hashtags
recently pre train language model plms dominate conditional text generation task give impressive performance prevalence plms seemingly natural assume could figure attend input include output via seq2seq learn without guidance train input output pair however rigorous study regard assumption still lack paper present systematic analysis conditional generation study whether current plms good enough preserve important concepts input extent explicitly guide generation lexical constraints beneficial conduct extensive analytical experiment range conditional generation task try answer scenarios guide generation lexical constraints work well propose framework automatic constraint extraction denoising enforcement show perform comparably better unconstrained generation hope find could serve reference determine whether appropriate worthwhile use explicit constraints specific task datasetfootnoteour code available urlhttps githubcom morningmoni lcgen eval
pretrained multilingual model able perform cross lingual transfer zero shoot set even languages unseen pretraining however prior work evaluate performance unseen languages largely limit low level syntactic task remain unclear zero shoot learn high level semantic task possible unseen languages explore question present americasnli extension xnli conneau et al two thousand and eighteen ten indigenous languages americas conduct experiment xlm r test multiple zero shoot translation base approach additionally explore model adaptation via continue pretraining provide analysis dataset consider hypothesis model find xlm r zero shoot performance poor ten languages average performance three thousand, eight hundred and sixty-two continue pretraining offer improvements average accuracy four thousand, four hundred and five surprisingly train poorly translate data far outperform methods accuracy four thousand, eight hundred and seventy-two
dialogue systems form chatbots personal assistants increasingly integrate people live dialogue systems often ability adopt anthropomorphic persona mimic societal demographic appear approachable trustworthy users however adoption persona result adoption bias define persona bias harmful differences text eg vary level offensiveness affirmations bias statements generate adopt different demographic personas paper present first large scale study persona bias dialogue systems conduct analyse personas different social class sexual orientations race genders furthermore introduce open source framework unitpersonabias tool explore aggregate subtle persona bias dialogue systems study blender dialogpt dialogue systems show choice personas affect degree harm generate responses additionally adopt personas diverse historically marginalize demographics appear decrease harmful responses
train modern read comprehension model question associate context treat independent however closely relate question correspond answer independent leverage relationships could provide strong supervision signal model draw ideas contrastive estimation introduce several new supervision techniques compare question answer score across multiple relate instance specifically normalize score across various neighborhoods closely contrast question answer add another cross entropy loss term use addition traditional maximum likelihood estimation techniques require bundle relate question answer pair either mine within exist data create use various automate heuristics empirically demonstrate effectiveness train instance bundle two datasets hotpotqa rope show eleven absolute gain accuracy
pre train language model lms like bert show store factual knowledge world knowledge use augment information present knowledge base tend incomplete however prior attempt use bert task knowledge base completion kbc result performance worse embed base techniques rely graph structure work develop novel model cross entity aware reranker cear use bert rank output exist kbc model cross entity attention unlike prior work score entity independently cear use bert score entities together effective exploit factual knowledge cear establish new state art performance four hundred and twenty-six hits1 fb15k two hundred and thirty-seven three hundred and twenty-seven relative improvement fifty-three pt improvement hits1 open link prediction
introduce technique improve document level language model lm leverage ancient history text outside lm current context window learn auxiliary function select span ancient history help lm predict future text select text span copy directly lm context window replace less predictive span method improve perplexity pretrained lms update lm parameters observe auxiliary function train specific textual domain like wikipedia also work substantially different domain scientific publications technique see seven percent perplexity reduction wikipedia article twelve percent perplexity reduction scientific texts
compositional reason task like multi hop question answer require make latent decisions get final answer give question however crowdsourced datasets often capture slice underlie task distribution induce unanticipated bias model perform compositional reason furthermore discriminatively train model exploit bias get better hold performance without learn right way reason necessitate pay attention question representation condition variable entirety estimate answer likelihood work propose generative context selection model multi hop question answer reason give question could generate give context pair comparable state art answer performance propose generative passage selection model better performance forty-nine higher baseline adversarial hold set test robustness model multi hop reason capabilities
previous work mainly focus improve cross lingual transfer nlu task multilingual pretrained encoder mpe improve translation performance nmt task bert however improve cross lingual transfer nmt model multilingual pretrained encoder explore paper focus zero shoot cross lingual transfer task nmt task nmt model train one parallel dataset shelf mpe directly test zero shoot language pair propose sixt simple yet effective model task sixt model leverage mpe two stage train schedule get improvement position disentangle encoder capacity enhance decoder extensive experiment prove sixt significantly improve translation quality unseen languages much less computation cost train data model achieve better performance many english testsets criss m2m one hundred two strong multilingual nmt baselines
class explainable nlp model reason task support decisions generate free form structure explanations happen support structure contain errors goal allow users interactively correct explanation structure natural language feedback introduce mercurie interactive system refine explanations give reason task get human feedback natural language approach generate graph forty fewer inconsistencies compare shelf system simply append correct explanation structure output lead gain twelve point accuracy defeasible reason across three domains release dataset 450k graph defeasible reason generate system https tinyurlcom mercurie
explore use large pretrained language model shoot semantic parsers goal semantic parse generate structure mean representation give natural language input however language model train generate natural language bridge gap use language model paraphrase input control sublanguage resemble english automatically map target mean representation small amount data little code convert english like representations provide blueprint rapidly bootstrapping semantic parsers demonstrate good performance multiple task
study power cross attention transformer architecture within context machine translation transfer learn experiment fine tune translation model dataset one new language find apart new language embeddings cross attention parameters need fine tune obtain competitive bleu performance provide insights case find limit fine tune manner yield cross lingually align type embeddings implications find include mitigation catastrophic forget network potential zero shoot translation
recent years witness emergence variety post hoc interpretations aim uncover natural language process nlp model make predictions despite surge new interpretations remain open problem define quantitatively measure faithfulness interpretations ie extent conform reason process behind model tackle issue start three criteria removal base criterion sensitivity interpretations stability interpretations quantify different notions faithfulness propose novel paradigms systematically evaluate interpretations nlp result show performance interpretations different criteria faithfulness could vary substantially motivate desideratum faithfulness notions introduce new class interpretation methods adopt techniques adversarial robustness domain empirical result show propose methods achieve top performance three criteria along experiment analysis text classification dependency parse task come comprehensive understand diverse set interpretations
measure sentence similarity key research area nowadays allow machine better understand human languages paper propose cross attention siamese network catsnet carry task learn semantic mean chinese sentence compare similarity two sentence novel model capable catch non local feature additionally also try apply long short term memory lstm network model improve performance experiment conduct lcqmc dataset result show model could achieve higher accuracy previous work
prior beliefs readers impact way project mean onto news headline beliefs influence perception news reliability well reaction news likelihood spread misinformation social network however prior work focus fact check veracity news stylometry rather measure impact misinformation propose misinfo belief frame formalism understand readers perceive reliability news impact misinformation also introduce misinfo belief frame mbf corpus dataset 66k inferences 235k headline misinformation frame use commonsense reason uncover implications real fake news headline focus global crises covid nineteen pandemic climate change result use large scale language model predict misinformation frame show machine generate inferences influence readers trust news headline readers trust news headline affect two hundred and ninety-three case demonstrate potential effectiveness use generate frame counter misinformation
augment pre train language model knowledge graph kgs achieve success various commonsense reason task although work attempt explain behavior kg augment model indicate kg input salient ie important model prediction always clear explanations use make model better paper explore whether kg explanations use supervision teach kg augment model filter unhelpful kg information end propose salkg simple framework learn kg explanations coarse kg salient fine part kg salient granularity give explanations generate task train set salkg train kg augment model solve task focus kg information highlight explanations salient across two popular commonsense qa benchmarks three kg augment model find salkg train process consistently improve model performance
aim generate set keyphrases keyphrase generation kg classical task capture central idea give document typically traditional kg evaluation metrics aware exact correctness predictions phrase level ignore semantic similarities similar predictions target inhibit model learn deep linguistic pattern paper propose new fine grain evaluation metric consider different granularity token level f1 score edit distance duplication prediction quantities learn recessive linguistic pattern use pre train model eg bert compute continuous similarity score predict keyphrases target keyphrases whole propose two stage reinforcement learn rl train framework two reward function propose fine grain evaluation score vanilla f1 score framework help model identify partial match phrase optimize exact match ones experiment four kg benchmarks show propose train framework outperform traditional rl train frameworks among evaluation score addition method effectively ease synonym problem generate higher quality prediction
recent advance large pre train language model greatly improve performance broad set nlp task however adapt exist model new task often require repeat train enormous label data prohibitively expensive obtain moreover model learn new task may gradually forget knowledge learn earlier task ie catastrophic forget paper study challenge lifelong learn shoot learn sequence diverse nlp task continuously fine tune language model investigate model ability shoot generalization new task retain performance previously learn task explore exist continual learn methods solve problem propose continual meta learn approach learn generate adapter weight examples regularize change weight mitigate catastrophic forget demonstrate approach preserve model performance train task lead positive knowledge transfer future task learn
pretrained transformers achieve remarkable performance test data follow distribution train data however real world nlu task model often face distribution ood instance instance severe semantic shift problem inference hence suppose identify reject model paper study ood detection problem pretrained transformers use distribution data train observe instance find use mahalanobis distance penultimate layer propose contrastive loss improve compactness representations ood instance better differentiate distribution ones experiment glue benchmark demonstrate effectiveness propose methods
simultaneous machine translation recently gain traction thank significant quality improvements advent stream applications simultaneous translation systems need find trade translation quality response time purpose multiple latency measure propose however latency evaluations simultaneous translation estimate sentence level take account sequential nature stream scenario indeed sentence level latency measure well suit continuous stream translation result figure coherent simultaneous translation policy system assess work propose stream level adaptation current latency measure base segmentation approach apply output translation successfully evaluate stream condition reference iwslt task
multimodal pre train text layout image achieve sota performance visually rich document understand task recently demonstrate great potential joint learn across different modalities paper present layoutxlm multimodal pre train model multilingual document understand aim bridge language barriers visually rich document understand accurately evaluate layoutxlm also introduce multilingual form understand benchmark dataset name xfun include form understand sample seven languages chinese japanese spanish french italian german portuguese key value pair manually label language experiment result show layoutxlm model significantly outperform exist sota cross lingual pre train model xfun dataset pre train layoutxlm model xfun dataset publicly available https akams layoutxlm
paper present emotion regularize conditional variational autoencoder emo cvae model generate emotional conversation responses conventional cvae base emotional response generation emotion label simply use additional condition prior posterior decoder network consider emotion style naturally entangle semantic content language space emo cvae model utilize emotion label regularize cvae latent space introduce extra emotion prediction network train stage estimate latent variables require predict emotion label token sequence input responses simultaneously experimental result show emo cvae model learn informative structure latent space conventional cvae model output responses better content emotion performance baseline cvae sequence sequence seq2seq model
investigate ground language learn real world data model teacher learner dynamics natural interactions occur users search engines particular explore emergence semantic generalization unsupervised dense representations outside synthetic environments ground domain denotation function composition function learn user data show result semantics noun phrase exhibit compositional properties fully learnable without explicit label benchmark ground semantics compositionality zero shoot inference task show provide better result better generalizations sota non ground model word2vec bert
prepositions important vehicle indicate semantic roles mean difficult analyze often discard process text preposition project design provide comprehensive database preposition sense suitable use natural language process applications project prepositions framenet corpus disambiguate use sense inventory current dictionary guide comprehensive treatment preposition mean methodology provide framework identify characterize semantic roles gold standard corpus instance analysis account semantic role alternation pattern adhere methodology hop comprehensive improve characterization preposition behavior semantic role identification syntactic semantic properties preposition complement attachment point develop databases generate project publicly available use researchers application developers
evolution internet increase amount information express people different platforms information product review discussions forums social media platforms accessibility opinions people feel open door opinion mine sentiment analysis language speech technologies become advance many languages use best model obtain however due linguistic diversity lack datasets african languages leave behind study use current state art model multilingual bert perform sentiment classification swahili datasets data create extract annotate 82k review comment different social media platforms isear emotion dataset data classify either positive negative model fine tune achieve best accuracy eight thousand, seven hundred and fifty-nine
paper ask whether distinction production base perception base grammar induction influence either growth curve grammars lexicons ii similarity representations learn independent sub set corpus production base model train usage single individual thus simulate grammatical knowledge single speaker perception base model train aggregation many individuals thus simulate grammatical generalizations learn exposure many different speakers ensure robustness experiment replicate across two register write english four additional register reserve control set three computational experiment show production base grammars significantly different perception base grammars across condition steeper growth curve explain substantial inter individual grammatical differences
task semantic role label srl dedicate find predicate argument structure previous work srl mostly supervise consider difficulty label example expensive time consume paper present first neural unsupervised model srl decompose task two argument relate subtasks identification cluster propose pipeline correspondingly consist two neural modules first train neural model two syntax aware statistically develop rule neural model get relevance signal token sentence fee bilstm adversarial layer noise add classify simultaneously thus enable model learn semantic structure sentence propose another neural model argument role cluster do cluster learn argument embeddings bias towards dependency relations experiment conll two thousand and nine english dataset demonstrate model outperform previous state art baseline term non neural model argument identification classification
despite significant progress neural abstractive summarization recent study show current model prone generate summaries unfaithful original context address issue study contrast candidate generation selection model agnostic post process technique correct extrinsic hallucinations ie information present source text unfaithful summaries learn discriminative correction model generate alternative candidate summaries name entities quantities generate summary replace ones compatible semantic type source document model use select best candidate final output summary experiment analysis across number neural summarization systems show propose method effective identify correct extrinsic hallucinations analyze typical hallucination phenomenon different type neural summarization systems hope provide insights future work direction
world fill serious challenge like climate change religious political conflict global pandemics terrorism racial discrimination internet full hate speech abusive offensive content last thing desire paper work identify promote positive supportive content platforms work several transformer base model classify social media comment hope speech hope speech english malayalam tamil languages paper portray work share task hope speech detection equality diversity inclusion lt edi two thousand and twenty-one eacl two thousand and twenty-one
tamil dravidian language commonly use speak southern part asia era social media memes fun moment day day life people try analyze true mean tamil memes categorize troll non troll propose ingenious model comprise transformer transformer architecture try attain state art use attention main component dataset consist troll non troll image caption text task binary classification task objective model pay attention extract feature ignore noise image text
subword units commonly use end end automatic speech recognition asr fully acoustic orient subword model approach somewhat miss propose acoustic data drive subword model adsm approach adapt advantage several text base acoustic base subword methods one pipeline fully acoustic orient label design learn process adsm produce acoustic structure subword units acoustic match target sequence asr train obtain adsm label evaluate different end end asr approach include ctc rnn transducer attention model experiment librispeech corpus show adsm clearly outperform byte pair encode bpe pronunciation assist subword model pasm case detail analysis show adsm achieve acoustically logical word segmentation balance sequence length thus suitable time synchronous label synchronous model also briefly describe apply acoustic base subword regularization unseen text segmentation use adsm
remove extract commentary section series websites tedious task standard way code widely adopt operation thus rarely perform paper show commentary section induce significant bias analyse especially case controversial highlight bullet commentary section induce bias analysis websites content bullet analyze section interest per se bullet illustrate point use corpus anti vaccine websites bullet provide guidelines remove extract section
paper describe transformer model pre train eight billion tokens crawl text croatian bosnian serbian montenegrin web domains evaluate transformer model task part speech tag name entity recognition geo location prediction commonsense causal reason show improvements task state art model commonsense reason evaluation introduce copa hr translation choice plausible alternatives copa dataset croatian berti c model make available free usage task specific fine tune huggingface
source code summarization aim generate concise descriptions give program functionalities transformer base approach achieve promise performance explicitly incorporate code structure information important capture code semantics besides without explicit constraints multi head attentions transformer may suffer attention collapse lead poor code representations summarization effectively integrate code structure information transformer explore task domain paper propose novel approach name sg trans incorporate code structural properties transformer specifically capture hierarchical characteristics code inject local symbolic information eg code tokens global syntactic structure eg data flow self attention module inductive bias extensive evaluation show superior performance sg trans state art approach
probe pre train transformer language model bridge inference first investigate individual attention head bert observe attention head higher layer prominently focus bridge relations comparison lower middle layer also specific attention head concentrate consistently bridge importantly consider language model whole second approach bridge anaphora resolution formulate mask token prediction task cloze test formulation produce optimistic result without fine tune indicate pre train language model substantially capture bridge inference investigation show distance anaphor antecedent context provide language model play important role inference
paper address end end automatic speech recognition asr long audio record lecture conversational speeches end end asr model design recognize independent utterances contextual information eg speaker topic multiple utterances know useful asr prior work propose context expand transformer accept multiple consecutive utterances time predict output sequence last utterance achieve five fifteen relative error reduction utterance base baselines lecture conversational asr benchmarks although result show remarkable performance gain still potential improve model architecture decode process paper extend prior work one introduce conformer architecture improve accuracy two accelerate decode process novel activation recycle technique three enable stream decode trigger attention demonstrate extend transformer provide state art end end asr performance obtain one hundred and seventy-three character error rate hkust dataset one hundred and twenty sixty-three word error rat switchboard three hundred eval2000 callhome switchboard test set new decode method reduce decode time fifty enable stream asr limit accuracy degradation
overwhelm amount biomedical scientific texts call development effective language model able tackle wide range biomedical natural language process nlp task recent dominant approach domain specific model initialize general domain textual data train variety scientific corpora however observe specialize domains large corpora exist train model scratch domain knowledge may yield better result moreover increase focus compute cost pre train recently lead design efficient architectures electra paper propose pre train domain specific language model call electramed suit biomedical field novel approach inherit learn framework general domain electra architecture well computational advantage experiment perform benchmark datasets several biomedical nlp task support usefulness electramed set novel state art result bc5cdr corpus name entity recognition provide best outcome two five run 7th bioasq factoid challange question answer task
target syntactic evaluation subject verb number agreement english tse evaluate language model syntactic knowledge use hand craft minimal pair sentence differ main verb conjugation method evaluate whether language model rate grammatical sentence likely ungrammatical counterpart identify two distinct goals tse first evaluate systematicity language model syntactic knowledge give sentence conjugate arbitrary verbs correctly second evaluate model likely behavior give sentence model concentrate probability mass correctly conjugate verbs even subset possible verbs argue current implementations tse directly capture either goals propose new metrics capture goal separately metrics find tse overestimate systematicity language model model score forty better verbs predict likely context
journalists publish statements provide people textitsources contextualize current events help voters make inform decisions hold powerful individuals accountable work construct ontological label system source base source textitaffiliation textitrole build probabilistic model infer attribute name source describe news article mixtures source model outperform exist mixture model co cluster approach correctly infer source type eighty expert evaluate trials work facilitate research downstream task like opinion argumentation mine represent first step towards machine loop textitcomputational journalism systems
since seminal work mikolov et al 2013a bojanowski et al two thousand and seventeen word representations shallow log bilinear language model find way many nlp applications mikolov et al two thousand and eighteen introduce positional log bilinear language model characteristics attention base language model reach state art performance intrinsic word analogy task however positional model never evaluate qualitative criteria extrinsic task speed impractical outline similarities attention mechanism positional model propose constrain positional model adapt sparse attention mechanism dai et al two thousand and eighteen evaluate positional constrain positional model three novel qualitative criteria extrinsic language model task botha blunsom two thousand and fourteen show positional constrain positional model contain interpretable information word order outperform subword model bojanowski et al two thousand and seventeen language model also show constrain positional model outperform positional model language model twice fast
multilingual model bert xlm r gain increase popularity due zero shoot cross lingual transfer learn capabilities however generalization ability still inconsistent typologically diverse languages across different benchmarks recently meta learn garner attention promise technique enhance transfer learn low resource scenarios particularly cross lingual transfer natural language understand nlu work propose x metra ada cross lingual meta transfer learn adaptation approach nlu approach adapt maml optimization base meta learn approach learn adapt new languages extensively evaluate framework two challenge cross lingual nlu task multilingual task orient dialog typologically diverse question answer show approach outperform naive fine tune reach competitive performance task languages analysis reveal x metra ada leverage limit data faster adaptation
evaluation natural language process guide promote research model methods recent years new evalua tion data set evaluation task continuously propose time series problems expose ex isting evaluation also restrict progress natural language process technology start concept com position development mean natural language evaluation article classify summarize task char acteristics mainstream natural language evaluation summarize problems cause natural language pro cessing evaluation finally article refer human language ability evaluation standard put forward concept human like machine language ability evaluation propose series basic principles implementation ideas hu man like machine language ability evaluation three aspects reliability difficulty validity
performance neural model name entity recognition degrade time become stale degradation due temporal drift change target variables statistical properties time issue especially problematic social media data topics change rapidly order mitigate problem data annotation retrain model common despite usefulness process expensive time consume motivate new research efficient model update paper propose intuitive approach measure potential trendiness tweet use metric select informative instance use train conduct experiment three state art model temporal twitter dataset approach show larger increase prediction accuracy less train data alternatives make attractive practical solution
weakly supervise text classification aim induce text classifiers user provide seed word vast majority previous work assume high quality seed word give however expert annotate seed word sometimes non trivial come furthermore weakly supervise learn set label document measure seed word efficacy make seed word selection process walk dark work remove need expert curated seed word first mine noisy candidate seed word associate category name train interim model individual candidate seed word lastly estimate interim model error rate unsupervised manner seed word yield lowest estimate error rat add final seed word set comprehensive evaluation six binary classification task four popular datasets demonstrate propose method outperform baseline use category name seed word obtain comparable performance counterpart use expert annotate seed word
neural machine translation nmt achieve significant breakthrough performance know suffer vulnerability input perturbations real input noise difficult predict train robustness big issue system deployment paper improve robustness nmt model reduce effect noisy word context enhance reconstruction cer approach cer train model resist noise two step one perturbation step break naturalness input sequence make word two reconstruction step defend noise propagation generate better robust contextual representation experimental result chinese english zh en french english fr en translation task demonstrate robustness improvement news social media text fine tune experiment social media text show approach converge higher position provide better adaptation
paper describe contribution wassa two thousand and twenty-one share task empathy prediction emotion classification broad goal task model empathy score distress score overall level emotion essay write response newspaper article associate harm someone use electra model abundantly also advance deep learn approach like multi task learn additionally also leverage standard machine learn techniques like ensembling system achieve pearson correlation coefficient five hundred and thirty-three sub task macro f1 score five thousand, five hundred and twenty-eight sub task ii rank 1st emotion classification sub task 3rd empathy prediction sub task
advance neural language model focus linguistic steganography shift edit base approach generation base ones latter payload capacity impressive generate genuine look texts remain challenge paper revisit edit base linguistic steganography idea mask language model offer shelf solution propose method eliminate painstaking rule construction high payload capacity edit base model also show secure automatic detection generation base method offer better control security payload capacity trade
link prediction address issue complete kgs miss facts broadly study however less light would ubiquitous hyper relational kgs exist hyper relational kg embed model still tear n ary fact smaller tuples neglect indecomposability n ary facts frameworks work certain arity facts ignore significance primary triple paper represent n ary fact whole simultaneously keep integrity n ary fact maintain vital role primary triple play addition generalize hyperbolic poincar e embed binary arbitrary arity data study yet tackle weak expressiveness high complexity issue propose hyper2 qualify capture interaction entities within beyond triple information aggregation tangent space extensive experiment demonstrate hyper2 achieve superior performance translational deep analogues improve sota three hundred and forty-five relatively dimension moreover study side effect literals theoretically experimentally compare computational complexity hyper2 several best perform baselines hyper2 forty-nine sixty-one time quicker counterparts
perform neural machine translation sentence fragment order create large amount train data english grammatical error correction method aim simulate mistake make second language learners produce wider range non native style language comparison state art synthetic data creation methods addition purely grammatical errors approach generate type errors lexical errors perform grammatical error correction experiment use neural sequence sequence model carry quantitative qualitative evaluation model train data create use propose method show outperform baseline model test data high proportion errors
present work semeval two thousand and twenty-one task five toxic span detection task aim build model identify toxic word whole post use bilstm crf model combine toxic bert classification train detection model identify toxic word post model achieve six thousand, two hundred and twenty-three f1 score toxic span detection task
present novel deep learn base framework generate embed representations fine grain emotions use computationally describe psychological model emotions framework integrate contextualized embed encoder multi head probe model enable interpret dynamically learn representations optimize emotion classification task model evaluate empathetic dialogue dataset show state art result classify thirty-two emotions layer analysis derive emotion graph depict hierarchical relations among emotions emotion representations use generate emotion wheel directly comparable one plutchik sln model also augment value miss emotions pad emotional state model
paper target problem procedural multimodal machine comprehension m3c task require ai comprehend give step multimodal instructions answer question compare vanilla machine comprehension task ai require understand textual input procedural m3c challenge ai need comprehend temporal causal factor along multimodal input recently yagcioglu et al thirty-five introduce recipeqa dataset evaluate m3c first contribution introduction two new m3c datasets woodworkqa decorationqa 16k 10k instructional procedures respectively evaluate m3c use textual cloze style question answer task highlight inherent bias question answer generation method thirty-five enable naive baseline cheat learn answer choices naive baseline perform similar popular method use question answer impatient reader six use attention context query hypothesize naturally occur bias present dataset affect even best perform model verify propose hypothesis propose algorithm capable modify give dataset remove bias elements finally report performance debiased dataset several strong baselines observe performance methods fall margin eight sixteen correct bias hope datasets analysis provide valuable benchmarks encourage research area
sign languages visual languages produce movement hand face body paper evaluate representations base skeleton pose explainable person independent privacy preserve low dimensional representations basically skeletal representations generalize individual appearance background allow us focus recognition motion much information lose skeletal representation perform two independent study use two state art pose estimation systems analyze applicability pose estimation systems sign language recognition evaluate failure case recognition model importantly allow us characterize current limitations skeletal pose estimation approach sign language recognition
understand natural language require common sense one aspect ability discern plausibility events distributional model recently pre train transformer language model demonstrate improvements model event plausibility performance still fall short humans work show transformer base plausibility model markedly inconsistent across conceptual class lexical hierarchy infer person breathe plausible dentist breathe example find inconsistency persist even model softly inject lexical knowledge present simple post hoc method force model consistency improve correlation human plausibility judgements
punctuation prediction automatic speech recognition asr output transcripts play crucial role improve readability asr transcripts improve performance downstream natural language process applications however achieve good performance punctuation prediction often require large amount label speech transcripts expensive laborious paper propose discriminative self train approach weight loss discriminative label smooth exploit unlabeled speech transcripts experimental result english iwslt2011 benchmark test set internal chinese speak language dataset demonstrate propose approach achieve significant improvement punctuation prediction accuracy strong baselines include bert roberta electra model propose discriminative self train approach outperform vanilla self train approach establish new state art sota iwslt2011 test set outperform current sota model thirteen absolute gain f1
pretrained language model show success many natural language process task many work explore incorporate knowledge language model biomedical domain experts take decades effort build large scale knowledge base example unify medical language system umls contain millions entities synonyms define hundreds relations among entities leverage knowledge benefit variety downstream task name entity recognition relation extraction end propose kebiolm biomedical pretrained language model explicitly leverage knowledge umls knowledge base specifically extract entities pubmed abstract link umls train knowledge aware language model firstly apply text encode layer learn entity representation apply text entity fusion encode aggregate entity representation besides add two train objectives entity detection entity link experiment name entity recognition relation extraction blurb benchmark demonstrate effectiveness approach analysis collect probe dataset show model better ability model medical knowledge
traditional cascade architecture speak language understand slu observe automatic speech recognition errors could detrimental performance natural language understand end end e2e slu model propose directly map speech input desire semantic frame single model hence mitigate asr error propagation recently pre train technologies explore e2e model paper propose novel joint textual phonetic pre train approach learn speak language representations aim explore full potentials phonetic information improve slu robustness asr errors explore phoneme label high level speech feature design compare pre train task base conditional mask language model objectives inter sentence relation objectives also investigate efficacy combine textual phonetic information fine tune experimental result speak language understand benchmarks fluent speech command snip show propose approach significantly outperform strong baseline model improve robustness speak language understand asr errors
end end speech translation model become new trend research due potential reduce error propagation however model still suffer challenge data scarcity effectively make use unlabeled parallel corpora machine translation promise still open problem paper propose cross speech text network xstnet end end model speech text translation xstnet take speech text input output transcription translation text model benefit three key design aspects self supervise pre train sub network audio encoder multi task train objective exploit additional parallel bilingual text progressive train procedure evaluate performance xstnet baselines must c en de fr ru datasets xstnet achieve state art result three language directions average bleu two hundred and seventy-eight outperform previous best method thirty-seven bleu code model release public
recent study emphasize need document context human evaluation machine translations little research do impact user interfaces annotator productivity reliability assessments work compare human assessment data last two wmt evaluation campaign collect via two different methods document level evaluation analysis show document centric approach evaluation annotator present entire document context screen lead higher quality segment document level assessments improve correlation segment document score increase inter annotator agreement document score considerably time consume annotators
disease name recognition normalization generally call biomedical entity link fundamental process biomedical text mine recently neural joint learn task propose utilize mutual benefit approach achieve high performance disease concepts appear train dataset accurately predict study introduce novel end end approach combine span representations dictionary match feature address problem model handle unseen concepts refer dictionary maintain performance neural network base model end end fashion experiment use two major datasets demonstrate model achieve competitive result strong baselines especially unseen concepts train
wordnet lexical database group english word set synonyms call synsets synsets utilize several applications field text mine however also open criticism although reality members synset represent mean synset degree practice consider members synset identically thus fuzzy version synsets call fuzzy synsets fuzzy word sense class propose study study discuss type one fuzzy synsets t1 f synsets properly model membership uncertainty propose upgrade version fuzzy synsets membership degrees word sense represent intervals similar interval type two fuzzy set it2 fs discuss it2 fs theoretical framework insufficient analysis design synsets propose new concept call interval probabilistic fuzzy ipf set present algorithm construct ipf synsets language give corpus word sense disambiguation system utilize algorithm open american online corpus oanc ukb word sense disambiguation construct publish ipf synsets wordnet english language
experiment model devise train evaluate automate psychotherapist client text conversations use state art seq2seq transformer base natural language generation nlg systems train model upon mix cornell movie dialogue corpus language understand open source anonymized public license psychotherapeutic dataset model achieve statistically significant performance publish standardize qualitative benchmarks human write validation data meet exceed human write responses performance five hundred and ninety-seven six hundred and seventy-one test set two independent test methods respectively although model replace work psychotherapists entirely ability synthesize human appear utterances majority test set serve promise step towards communize ease stigma psychotherapeutic point care
disfluency detection model approach high accuracy english text however little exploration do improve size inference time model time automatic speech recognition asr model move server side inference local device inference support model transcription pipeline like disfluency detection must follow suit work concentrate disfluency detection task focus small fast device model base bert architecture demonstrate possible train disfluency detection model small thirteen mib retain high performance build previous work show benefit data augmentation approach self train evaluate effect domain mismatch conversational write text model performance find domain adaptation data augmentation strategies pronounce effect smaller model compare conventional bert model
adverse drug events ades unexpected incidents cause administration drug medication identify extract events require information drug attribute describe drug eg strength dosage reason drug initially prescribe adverse reaction drug paper explore relationship drug associate attribute use relation extraction techniques explore three approach rule base approach deep learn base approach contextualized language model base approach evaluate system n2c2 two thousand and eighteen ade extraction dataset experimental result demonstrate contextualized language model base approach outperform model overall obtain state art performance ade extraction precision ninety-three recall ninety-six f1 score ninety-four however certain relation type rule base approach obtain higher precision recall either learn approach
language model train billions tokens recently lead unprecedented result many nlp task success raise question whether principle system ever understand raw text without access form ground formally investigate abilities ungrounded systems acquire mean analysis focus role assertions contexts within raw text provide indirect clue underlie semantics study whether assertions enable system emulate representations preserve semantic relations like equivalence find assertions enable semantic emulation expressions language referentially transparent however language use non transparent pattern like variable bind show emulation become uncomputable problem finally discuss differences formal model natural language explore result generalize modal set semantic relations together result suggest assertions code language provide sufficient signal fully emulate semantic representations formalize ways ungrounded language model appear fundamentally limit ability understand
humans often communicate use imprecise language suggest fuzzy concepts unclear boundaries prevalent language use paper test extent model train capture distributional statistics language show correspondence fuzzy membership pattern use task natural language inference test recent state art model classical case temperature examine map temperature data fuzzy perceptions cool hot etc find model show pattern similar classical fuzzy set theoretic formulations linguistic hedge albeit substantial amount noise suggest model train solely language show promise encode fuzziness
current intent classification approach assign binary intent class memberships natural language utterances disregard inherent vagueness language correspond vagueness intent class boundaries work propose scheme address ambiguity single intent well multi intent natural language utterances create degree memberships fuzzified intent class knowledge first work address quantify impact fuzzy nature natural language utterances intent category memberships additionally approach overcome sparsity multi intent utterance data train classification model use small database single intent utterances generate class memberships multi intent utterances evaluate approach two task orient dialog datasets across different fuzzy membership generation techniques approximate string similarity measure result reveal impact lexical overlap utterances different intents underlie data distributions fuzzification intent memberships moreover evaluate accuracy approach compare defuzzified memberships binary counterparts across different combinations membership function string similarity measure
contextual word representation model show massive improvements multitude nlp task yet word sense disambiguation capabilities remain poorly explain address gap assess whether contextual word representations extract deep pretrained language model create distinguishable representations different sense give word analyze representation geometry find layer deep pretrained language model create highly anisotropic representations point towards existence representation degeneration problem contextual word representations account anisotropy study reveal variability sense learn capabilities across different language model finally propose laser low anisotropy sense retrofit approach render shelf representations isotropic semantically meaningful resolve representation degeneration problem post process step conduct sense enrichment contextualized representations extract deep neural language model
performance relation extraction model increase considerably rise neural network however key issue neural relation extraction robustness model scale well long sentence multiple entities relations work address problem enrich attention mechanism attention allow model focus part input sentence relevant relation extraction propose enrich attention function feature model knowledge relation arguments shortest dependency path thus different relation arguments model pay attention different part sentence model outperform prior work use comparable setups two popular benchmarks analysis confirm indeed scale long sentence many entities
understand news media frame political issue important due impact public attitudes yet hard automate computational approach largely focus classify frame full news article frame signal often subtle local furthermore automatic news analysis sensitive domain exist classifiers lack transparency predictions paper address issue novel semi supervise model jointly learn embed local information events relate actors news article auto encode framework leverage signal document level frame classification experiment show model outperform previous model frame prediction improve performance unlabeled train data leverage semi supervise nature model learn event actor embeddings intuitively corroborate document level predictions provide nuanced interpretable article frame representation
large language model lead state art accuracies across range task howevertraining large language model need massive compute resource open source pre train model available worthy study take full advantage available model find method save train time resource cost change small well train model large model initialize larger target model smaller source model copy weight value source model pad zero small initialization value make source target model approximate output valid due block matrix multiplication residual connection transformer structure test target model several data set find still comparable source model continue train target model train loss start smaller value
one promise way inquire particular information dialog bot question answer dialog systems gain increase research interest recently design interactive qa systems always challenge task natural language process use benchmark evaluate machine ability natural language understand however systems often struggle question answer carry multiple turn users seek information base already learn thus give rise another complicate form call conversational question answer cqa cqa systems often criticize understand utilize previous context conversation answer question address research gap paper explore integrate conversational history neural machine comprehension system one hand introduce framework base publically available pre train language model call bert incorporate history turn system hand propose history selection mechanism select turn relevant contribute answer current question experimentation result reveal framework comparable performance state art model quac leader board also conduct number experiment show side effect use entire context information bring unnecessary information noise signal result decline model performance
fake news detection important task increase credibility information media since fake news constantly spread social media every day serious concern society fake news usually create manipulate image texts videos paper present novel method detect fake news fuse multimodal feature derive textual visual data specifically use pre train bert model learn text feature vgg nineteen model pre train imagenet dataset extract image feature propose scale dot product attention mechanism capture relationship text feature visual feature experimental result show approach perform better current state art method public twitter dataset thirty-one accuracy
improve model generalization hold data one core objectives commonsense reason recent work show model train dataset superficial cue tend perform well easy test set superficial cue perform poorly hard test set without superficial cue previous approach resort manual methods encourage model overfit superficial cue methods improve performance hard instance also lead degrade performance easy instance propose explicitly learn model well easy test set superficial cue hard test set without superficial cue use meta learn objective learn model improve performance easy test set hard test set evaluate model choice plausible alternatives copa commonsense explanation show propose method lead improve performance easy test set hard test set upon observe one hundred and sixty-five percentage point improvement baseline
essay form assessment test student knowledge deeper level short answer multiple choice question however manual evaluation essay time labor consume automatic cluster essay fragment prior manual evaluation present possible solution reduce effort require evaluation process cluster present numerous challenge due variability ambiguity natural language paper introduce two datasets undergraduate student essay finnish manually annotate salient arguments sentence level use datasets evaluate several deep learn embed methods suitability sentence cluster support essay grade find choice suitable method depend nature exam question answer deep learn methods capable guarantee better performance simpler methods base lexical overlap
scientific claim verification unique challenge attract increase interest sciver share task offer benchmark scenario test compare claim verification approach participate team consist three step relevant abstract selection rationale selection label prediction paper present team qmul sds participation share task propose approach perform scientific claim verification binary classifications step step train biobert large classifier select abstract base pairwise relevance assessments continue train select rationales retrieve abstract base propose two step set label prediction ie first predict notenoughinfo enoughinfo label mark enoughinfo either support contradict compare baseline system achieve substantial improvements dev set result team four team leaderboard
increase use machine learn drive algorithmic judgements critical develop model robust evolve manipulate input propose extensive analysis model robustness linguistic variation set deceptive news detection important task context misinformation spread online consider two prediction task compare three state art embeddings highlight consistent trend model performance high confidence misclassifications high impact failures measure effectiveness adversarial defense strategies evaluate model susceptibility adversarial attack use character word perturb text find character mix ensemble model effective defenses character perturbation base attack tactics successful
evaluate model robustness critical develop trustworthy model gain deeper understand model behavior strengths weaknesses also develop future model generalizable robust across expect environments model may encounter deployment paper present framework measure model robustness important difficult text classification task deceptive news detection evaluate model robustness domain data modality specific feature languages english investigation focus three type model lstm model train multiple datasetscross domain several fusion lstm model train image text evaluate three state art embeddings bert elmo glove cross modality character level cnn model train multiple languages cross language analyse reveal significant drop performance test neural model domain data non english languages may mitigate use diverse train data find additional image content input elmo embeddings yield significantly fewer errors compare bert orglove importantly work carefully analyze deception model robustness also provide framework analyse apply new model extend datasets future
differentially private mechanisms text generation typically add carefully calibrate noise input word use nearest neighbor noise input output word noise small magnitude mechanisms susceptible reconstruction original sensitive text nearest neighbor noise input likely original input mitigate empirical privacy risk propose novel class differentially private mechanisms parameterizes nearest neighbor selection criterion traditional mechanisms motivate vickrey auction second highest price reveal highest price keep private balance choice first second nearest neighbor propose class mechanisms use tune parameter parameter select empirically solve constrain optimization problem maximize utility maintain desire privacy guarantee argue empirical measurement framework use align different mechanisms along common benchmark privacy utility tradeoff particularly different distance metrics use calibrate amount noise add experiment real text classification datasets show fifty improvement utility compare exist state art empirical privacy guarantee
text classification usually study label natural language texts relevant categories predefined set real world new class might keep challenge exist system limit label data system intelligent enough recognize upcoming new class examples work define new task nlp domain incremental shoot text classification system incrementally handle multiple round new class round batch new class label examples per class two major challenge exist new task learn process system incrementally learn new class round round without train examples precede class ii performance system perform well new class without much loss precede class addition formulate new task also release two benchmark datasets incremental shoot set intent classification relation classification moreover propose two entailment approach entailment hybrid show promise solve novel problem
non autoregressive neural machine translation nat achieve significant inference speedup generate tokens simultaneously despite high efficiency nat usually suffer two kinds translation errors translation eg repeat tokens translation eg miss translations eventually limit translation quality paper argue issue nat address coverage model prove useful autoregressive decode propose novel coverage nat model coverage information directly token level coverage iterative refinement mechanism sentence level coverage agreement remind model source token translate improve semantics consistency translation source respectively experimental result wmt14 en de wmt16 en ro translation task show method alleviate errors achieve strong improvements baseline system
task agnostic knowledge distillation teacher student framework prove effective bert compression although achieve promise result nlp task require enormous computational resources paper propose extract distill etd generic flexible strategy reuse teacher parameters efficient effective task agnostic distillation apply students size specifically introduce two variants etd etd rand etd impt extract teacher parameters random manner follow importance metric respectively way student already acquire knowledge begin distillation process make distillation process converge faster demonstrate effectiveness etd glue benchmark squad experimental result show one compare baseline without etd strategy etd save seventy computation cost moreover achieve better result baseline use compute resource two etd generic prove effective different distillation methods eg tinybert minilm students different size source code publicly available upon publication
customer product review play role improve quality products service business organizations brand complain attitude express dissatisfaction event product meet customer expectations paper build open domain complaint detection dataset uit viocd include five thousand, four hundred and eighty-five human annotate review four categories product review e commerce sit data collection phase proceed annotation task achieve inter annotator agreement eighty-seven present extensive methodology research purpose achieve nine thousand, two hundred and sixteen f1 score identify complaints result future aim build system open domain complaint detection e commerce websites
intent understand play important role dialog systems typically formulate supervise classification problem however challenge time consume design intent label manually support new domain paper propose unsupervised two stage approach discover intents generate meaningful intent label automatically collection unlabeled utterances first stage aim generate set semantically coherent cluster utterances within cluster convey intent obtain utterance representation various pre train sentence embeddings present metric balance score determine optimal number cluster k mean cluster second stage objective generate intent label automatically cluster extract action object pair utterance use dependency parser take frequent pair within cluster eg book restaurant generate cluster label empirically show propose unsupervised approach generate meaningful intent label automatically achieve high precision recall utterance cluster intent discovery
automatic post edit ape important remedy reduce errors raw translate texts produce machine translation mt systems software aid translation paper present first attempt tackle ape task vietnamese specifically construct first large scale dataset 5m vietnamese translate correct sentence pair apply strong neural mt model handle ape task use construct dataset experimental result automatic human evaluations show effectiveness neural mt model handle vietnamese ape task
classic information extraction techniques consist build question answer facts indeed still challenge subjective information extraction systems identify opinions feel context sentiment base nlp task resources information extraction offensive hateful opinions context fill important gap short paper provide new cross lingual contextual offensive lexicon consist explicit implicit offensive swear expressions opinion annotate two different class context dependent context independent offensive addition provide markers identify hate speech annotation approach evaluate expression level achieve high human inter annotator agreement provide offensive lexicon available portuguese english languages
language model ubiquitous current nlp multilingual capacity recently attract considerable attention however current analyse almost exclusively focus multilingual variants standard benchmarks rely clean pre train task specific corpora multilingual signal paper introduce xlm framework use evaluate multilingual language model twitter framework feature two main assets one strong multilingual baseline consist xlm r conneau et al two thousand and twenty model pre train millions tweet thirty languages alongside starter code subsequently fine tune target task two set unify sentiment analysis twitter datasets eight different languages modular framework easily extend additional task well integrate recent efforts also aim homogenization twitter specific datasets barbieri et al two thousand and twenty
paper provide new approach offensive language hate speech detection social media approach incorporate offensive lexicon compose implicit explicit offensive swear expressions annotate binary class context dependent context independent offensive due severity hate speech offensive comment brazil lack research portuguese brazilian portuguese language use validate propose method nevertheless proposal may apply language domain base obtain result propose approach show high performance overcome current baselines european brazilian portuguese
paper investigate use linguistically motivate computationally efficient structure language model reranking n best hypotheses statistical machine translation system language model develop constraint dependency grammar parse tightly integrate knowledge word morphological lexical feature syntactic dependency constraints two structure language model apply n best rescoring one almost parse language model utilize syntactic feature explicitly model syntactic dependencies word also investigate effective efficient language model methods use n grams extract one teraword web document apply language model n best rank nist darpa gale program two thousand and six two thousand and seven machine translation evaluation task find combination language model increase bleu score sixteen absolutely blind test set
meet minutes record subject matter discuss decisions reach action take meet importance minuting overemphasize time significant number meet take place virtual space paper present slide window approach automatic generation meet minutes aim tackle issue associate nature speak text include lengthy transcripts lack document structure make difficult identify salient content include meet minutes approach combine slide window neural abstractive summarizer navigate transcripts find salient content approach evaluate transcripts natural meet conversations compare result obtain human transcripts two versions automatic transcripts discuss extent summarizer succeed capture salient content
large scale pretrained language model plms become new paradigm natural language process nlp plms hundreds billions parameters gpt three demonstrate strong performances natural language understand generation textitfew shoot context learn work present practice train large scale autoregressive language model name pangu alpha two hundred billion parameters pangu alpha develop mindspore train cluster two thousand and forty-eight ascend nine hundred and ten ai processors train parallelism strategy implement base mindspore auto parallel compose five parallelism dimension scale train task two thousand and forty-eight processors efficiently include data parallelism op level model parallelism pipeline model parallelism optimizer model parallelism rematerialization enhance generalization ability pangu alpha collect 11tb high quality chinese data wide range domains pretrain model empirically test generation ability pangu alpha various scenarios include text summarization question answer dialogue generation etc moreover investigate effect model scale shoot performances across broad range chinese nlp task experimental result demonstrate superior capabilities pangu alpha perform various task shoot zero shoot settings
multiparty dialogue machine read comprehension mrc differ traditional mrc model must handle complex dialogue discourse structure previously unconsidered traditional mrc fully exploit discourse structure multiparty dialogue present discourse aware dialogue graph neural network dadgraph explicitly construct dialogue graph use discourse dependency link discourse relations validate model perform experiment molweni corpus large scale mrc dataset build multiparty dialogue annotate discourse structure experiment molweni show discourse aware model achieve statistically significant improvements compare strong neural network mrc baselines
describe make available game base material develop laboratory run several italian science festivals popularize nlp among young students
although natural language process nlp core many tool young people use everyday life high school curricula italy include computational linguistics education lack exposure make use tool less responsible could make choose computational linguistics university degree unlikely raise awareness curiosity longer term interest young people develop interactive workshop design illustrate basic principles nlp computational linguistics high school italian students age thirteen eighteen years workshop take form game participants play role machine need solve common problems computer face understand language voice recognition markov chain syntactic parse participants guide workshop help instructors present activities explain core concepts computational linguistics workshop present numerous outlets italy two thousand and nineteen two thousand and twenty-one face face online
field explainable ai recently see explosion number explanation methods highly non linear deep neural network extent methods often propose test domain computer vision appropriate address explainability challenge nlp yet relatively unexplored work consider contextual decomposition cd shapley base input feature attribution method show work well recurrent nlp model test extent useful model contain attention operations end extend cd cover operations necessary attention base model compare long distance subject verb relationships process model without attention consider number different syntactic structure two different languages english dutch experiment confirm cd successfully apply attention base model well provide alternative shapley base attribution method modern neural network particular use cd show english dutch model demonstrate similar process behaviour hood consistent differences attention non attention model
ability persuade others critical professional personal success however craft persuasive message demand pose various challenge conduct nine exploratory case study identify adaptations professional non professional writers make write scenarios increase subjective persuasiveness furthermore identify challenge writers face identify strategies resolve persuasive natural language generation ie artificial intelligence find show humans achieve high degrees persuasiveness professional level writers artificial intelligence complement achieve increase celerity alignment process
ultra large scale pre train model effectively improve effect variety task also bring heavy computational burden inference paper introduce series ultra large scale pre train model optimization methods combine algorithm characteristics gpu processor hardware characteristics basis propose inference engine easy efficient transformer eet significant performance improvement exist scheme firstly introduce pre pad decode mechanism improve token parallelism generation task design high optimize kernels remove sequence mask achieve cost free calculation pad tokens well support long sequence long embed size thirdly user friendly inference system easy service pipeline introduce greatly reduce difficulty engineer deployment high throughput compare faster transformer implementation gpt two a100 eet achieve fifteen 15x state art speedup vary context lengtheet available https githubcom netease fuxi eet
transfer learn adapt model train data rich source low resource target widely apply natural language process nlp however train transfer model multiple source every source equally useful target better transfer model essential understand value source paper develop seal shap efficient source valuation framework quantify usefulness source eg domains languages transfer learn base shapley value method experiment comprehensive analyse cross domain cross lingual transfer demonstrate framework effective choose useful transfer source also source value match intuitive source target similarity
word sense disambiguation wsd long stand problem natural language process one significant challenge supervise word wsd classify among sense majority word lie long tail distribution instance eighty-four annotate word less ten examples semcor train data issue pronounce imbalance occur word sense distributions work propose metricwsd non parametric shoot learn approach mitigate data imbalance issue learn compute distance among sense give word episodic train metricwsd transfer knowledge learn metric space high frequency word infrequent ones metricwsd construct train episodes tailor word frequencies explicitly address problem skew distribution oppose mix word train parametric model previous work without resort lexical resources metricwsd obtain strong performance parametric alternatives achieve seven hundred and fifty-one f1 score unify wsd evaluation benchmark raganato et al 2017b analysis validate infrequent word sense enjoy significant improvement
document ground generation task use information provide document improve text generation work focus two different document ground generation task wikipedia update generation task dialogue response generation work introduce two novel adaptations large scale pre train encoder decoder model focus build context drive representation document enable specific attention information document additionally provide stronger bart baseline task propose techniques outperform exist methods automate least forty-eight increase bleu four point human evaluation closeness reference relevance document furthermore perform comprehensive manual inspection generate output categorize errors provide insights future directions model task
paper present new massive open online course natural language process target non english speak students course last twelve weeks every week consist lecture practical sessions quiz assignments three weeks twelve follow kaggle style cod assignments course intend serve multiple purpose familiarize students core concepts methods nlp language model word sentence representations ii show recent advance include pre train transformer base model build upon concepts iii introduce architectures demand real life applications iv develop practical skills process texts multiple languages course prepare record two thousand and twenty launch end year early two thousand and twenty-one receive positive feedback
outstanding performance transformer base language model great variety nlp nlu task stimulate interest explore inner work recent research focus primarily higher level complex linguistic phenomena syntax semantics world knowledge common sense majority study anglocentric little remain know regard languages precisely morphosyntactic properties end work present morph call suite forty-six probe task four indo european languages different morphology english french german russian propose new type probe task base detection guide sentence perturbations use combination neuron layer representation level introspection techniques analyze morphosyntactic content four multilingual transformers include less explore distil versions besides examine fine tune pos tag affect model knowledge result show fine tune improve decrease probe performance change morphosyntactic knowledge distribute across model code data publicly available hope fill gap less study aspect transformers
advance novel explanation similarity base interference effect subject verb reflexive pronoun agreement process ground surprisal value compute pretrained large scale transformer model gpt two specifically show surprisal verb reflexive pronoun predict facilitatory interference effect ungrammatical sentence distractor noun match number verb pronoun lead faster read time despite distractor participate agreement relation review human empirical evidence effect include recent meta analyse large scale study also show attention pattern index entropy measure transformer show pattern diffuse attention presence similar distractors consistent cue base retrieval model parse contrast model attentional cue memory representations learn entirely simple self supervise task predict next word
paper explore construction natural language explanations news claim goal assist fact check news evaluation applications experiment two methods one extractive method base bias textrank resource effective unsupervised graph base algorithm content extraction two abstractive method base gpt two language model perform comparative evaluations two misinformation datasets political health news domains find extractive method show promise
text style transfer aim change style sentence preserve semantic mean due lack parallel data denoising auto encoder dae widely use task model distributions different sentence style however conflict target conventional denoising procedure target style transfer task vanilla dae produce satisfy enough result improve transferability model exist work combine dae various complicate unsupervised network make whole system become complex work design novel dae model name style enhance dae se dae specifically design text style transfer task compare previous complicate style transfer model model consist complicate unsupervised network rely high quality pseudo parallel data generate novel data refinement mechanism moreover alleviate conflict target conventional denoising procedure style transfer task propose another novel style denoising mechanism compatible target style transfer task validate effectiveness model two style benchmark datasets automatic evaluation human evaluation show propose model highly competitive compare previous strong state art sota approach greatly outperform vanilla dae
lightgbm model feed target word lexical characteristics feature obtain word frequency list psychometric data bigram association measure optimize two thousand and twenty-one cmcl share task eye track data prediction obtain best performance team two five eye track measure predict allow rank first official challenge criterion outperform deep learn base systems participate challenge
readability difficulty estimation word document investigate independently literature often assume existence extensive annotate resources motivate analysis show recursive relationship word document difficulty propose jointly estimate word document difficulty graph convolutional network gcn semi supervise fashion experimental result reveal gcn base method achieve higher accuracy strong baselines stay robust even smaller amount label data
toxic span detectiontsd task define highlight span make text toxic many work do classify give comment document toxic non toxic however none propose model work token level paper propose self attention base bidirectional gate recurrent unitbigru multi embed representation tokens propose model enrich representation combination gpt two glove roberta embeddings lead promise result experimental result show propose approach effective detect span tokens
knowledge graph question answer important technology intelligent human robot interaction aim automatically give answer human natural language question give knowledge graph multi relation question higher variety complexity tokens question different priority triple selection reason step exist model take question whole ignore priority information solve problem propose question aware memory network multi hop question answer name qa2mn update attention question timely reason process addition incorporate graph context information knowledge graph embed model increase ability represent entities relations use initialize qa2mn model fine tune train process evaluate qa2mn pathquestion worldcup2014 two representative datasets complex multi hop question answer result demonstrate qa2mn achieve state art hits1 accuracy two datasets validate effectiveness model
modern summarization model generate highly fluent often factually unreliable output motivate surge metrics attempt measure factuality automatically generate summaries due lack common benchmarks metrics compare moreover methods treat factuality binary concept fail provide deeper insights kinds inconsistencies make different systems address limitations devise typology factual errors use collect human annotations generate summaries state art summarization systems cnn dm xsum datasets annotations identify proportion different categories factual errors various summarization model benchmark factuality metrics show correlation human judgment well specific strengths weaknesses
paper describe submissions 2nd 3rd slavner share task hold bsnlp two thousand and nineteen bsnlp two thousand and twenty-one respectively task focus analysis name entities multilingual web document slavic languages rich inflection solution take advantage large collections unstructured structure document former serve data unsupervised train language model embeddings lexical units latter refer wikipedia structure counterpart wikidata source lemmatization rule real world entities aid resources system could recognize normalize link entities train small amount label data
continue spread misinformation disinformation online increase importance develop combat mechanisms scale form automate systems support multiple languages one task interest claim veracity prediction address use stance detection respect relevant document retrieve online end present new arabic stance detection dataset arastance four thousand and sixty-three claim article pair diverse set source comprise three fact check websites one news website arastance cover false true claim multiple domains eg politics sport health several arab countries well balance relate unrelated document respect claim benchmark arastance along two stance detection datasets use number bert base model best model achieve accuracy eighty-five macro f1 score seventy-eight leave room improvement reflect challenge nature arastance task stance detection general
knowledge graph kgs widely use facilitate relation extraction task previous methods focus leverage deterministic kgs uncertain kgs assign confidence score relation instance provide prior probability distributions relational facts valuable external knowledge model paper propose exploit uncertain knowledge improve relation extraction specifically introduce probase uncertain kg indicate extent target entity belong concept architecture design novel multi view inference framework systematically integrate local context global knowledge across three view mention entity concept view experimental result show model achieve competitive performances sentence document level relation extraction verify effectiveness introduce uncertain knowledge multi view inference framework design
paper introduce sentiment emotion lexicon finnish self finnish emotion intensity lexicon feil describe lexicon creation process evaluate lexicon use commonly available tool lexicon use annotations project nrc emotion lexicon carefully edit translations knowledge first comprehensive sentiment emotion lexicon finnish
probabilistic context free grammars pcfgs neural parameterization show effective unsupervised phrase structure grammar induction however due cubic computational complexity pcfg representation parse previous approach scale relatively large number nonterminal preterminal symbols work present new parameterization form pcfgs base tensor decomposition quadratic computational complexity symbol number therefore allow us use much larger number symbols use neural parameterization new form improve unsupervised parse performance evaluate model across ten languages empirically demonstrate effectiveness use symbols code https githubcom sustcsonglin tn pcfg
contrastive learn use learn high quality representation image computer vision however contrastive learn widely utilize natural language process due lack general method data augmentation text data work explore method employ contrastive learn improve text representation bert model relation extraction key knob framework unique contrastive pre train step tailor relation extraction task seamlessly integrate linguistic knowledge data augmentation furthermore investigate large scale data construct external knowledge base enhance generality contrastive pre train bert experimental result three relation extraction benchmark datasets demonstrate method improve bert model representation achieve state art performance addition explore interpretability model show bert contrastive pre train rely rationales prediction code data publicly available https githubcom udel biotm lab bert clre
determine whether often use lexical association measure assign high score n grams chance could produce frequently observe use extension fisher exact test sequence longer two word analyse corpus four million word result base precision recall curve new index call chance correct average precision show expect simple extremely effective also show however mi3 efficient hypothesis test base measure even reach performance level almost equal simple three grams additionally observe measure efficient three grams two grams others stagnate
map lexical mean wordforms major feature natural languages usage pressure might assign short word frequent mean zipf law abbreviation need productive open end vocabulary local constraints sequence symbols various factor shape lexicons world languages despite importance shape lexical structure relative contributions factor fully quantify take cod theoretic view lexicon make use novel generative statistical model define upper bound compressibility lexicon various constraints examine corpora seven typologically diverse languages use upper bound quantify lexicon optimality explore relative cost major constraints natural cod find compositional morphology graphotactics sufficiently account complexity natural cod measure code length
new generation pre train nlp model push sota new limit cost computational resources point use real production environments often prohibitively expensive tackle problem evaluate standard quality metrics downstream task also memory footprint inference time present morocco framework compare language model compatible textttjiant environment support fifty nlu task include superglue benchmark multiple probe suit demonstrate applicability two glue like suit different languages
abstract mean representation parse sentence graph prediction task target nod explicitly align sentence tokens however since graph nod semantically base one sentence tokens implicit alignments derive transition base parsers operate sentence leave right capture inductive bias via alignments cost limit expressiveness work propose transition base system combine hard attention sentence target side action pointer mechanism decouple source tokens node representations address alignments model transition well pointer mechanism straightforward modifications within single transformer architecture parser state graph structure information efficiently encode use attention head show action pointer approach lead increase expressiveness attain large gain sixteen point best transition base amr parser similar condition use graph categorization single model yield second best smatch score amr twenty eight hundred and eighteen improve eight hundred and thirty-four silver data ensemble decode
pre train language model lms encode rich information linguistic structure knowledge lexical polysemy remain unclear propose novel experimental setup analyse knowledge lms specifically train different languages english french spanish greek multilingual bert perform analysis datasets carefully design reflect different sense distributions control parameters highly correlate polysemy frequency grammatical category demonstrate bert derive representations reflect word polysemy level partitionability sense polysemy relate information clearly present english bert embeddings model languages also manage establish relevant distinctions word different polysemy level result contribute better understand knowledge encode contextualised representations open new avenues multilingual lexical semantics research
coreference resolution important component analyze narrative text administrative data eg clinical police source however exist coreference model train general language corpora suffer poor transferability due domain gap especially apply gender inclusive data lesbian gay bisexual transgender lgbt individuals paper analyze challenge coreference resolution exemplary form administrative text write english violent death narratives usa center disease control cdc national violent death report system develop set data augmentation rule improve model performance use probabilistic data program framework experiment narratives administrative database well exist gender inclusive coreference datasets demonstrate effectiveness data augmentation train coreference model better handle text data lgbt individuals
social media remarkably grow past years nowadays post message social media websites become one popular internet activities vast amount user generate content make social media extensive data source public opinion sentiment analysis one techniques use analyze user generate data persian language specific feature thereby require unique methods model adopt sentiment analysis different english language sentiment analysis language specify prerequisites hence direct use methods tool resources develop english language persian limitations main target paper provide comprehensive literature survey state art advance persian sentiment analysis regard present study aim investigate compare previous sentiment analysis study persian texts describe contributions present article publish last decade first level approach task sentiment analysis describe detail survey sentiment analysis methods use persian texts present previous relevant work persian language discuss moreover present survey authentic publish standard sentiment analysis resources advance do persian sentiment analysis finally accord state art development english sentiment analysis issue challenge address persian texts list guidelines trend provide future research persian texts paper provide information help new establish researchers field well industry developers aim deploy operational complete sentiment analysis system
user query real world dialog system may sometimes fall outside scope system capabilities appropriate system responses enable smooth process throughout human computer interaction paper concern user intent focus scope intent classification dialog systems although user intents highly correlate application domain study exploit correlations intent classification rather develop two stage approach first classify domain intent propose hierarchical multi task learn approach base joint model classify domain intent simultaneously novelties propose approach include one share supervise scope signal joint model domain intent classification replace two stage pipeline two introduce hierarchical model learn intent domain representations higher lower layer respectively experiment show model outperform exist methods term accuracy scope recall f1 additionally threshold base post process improve performance balance precision recall intent classification
recently various neural encoder decoder model pioneer seq2seq framework propose achieve goal generate abstractive summaries learn map input text output text high level neural model freely generate summaries without constraint word phrase use moreover format closer human edit summaries output readable fluent however neural model abstraction ability double edge sword commonly observe problem generate summaries distortion fabrication factual information article inconsistency original text summary cause various concern applicability previous evaluation methods text summarization suitable issue response problems current research direction predominantly divide two categories one design fact aware evaluation metrics select output without factual inconsistency errors develop new summarization systems towards factual consistency survey focus present comprehensive review fact specific evaluation methods text summarization model
survey natural language process nlp approach summarize simplify generate patent text solve task important practical applications give patent centrality randd process patent idiosyncrasies open peculiar challenge current nlp state art survey aim describe patent characteristics question raise current nlp systems b critically present previous work evolution c draw attention directions research work need best knowledge first survey generative approach patent domain
describe simple procedure automatic creation word level alignments print document respective full text versions procedure unsupervised use standard shelf components reach f score eight thousand, five hundred and one basic setup eight thousand, six hundred and sixty-three use pre post process potential areas application manual database curation incl document triage biomedical expression ocr
present system allow users train state art paraphrastic sentence representations variety languages also release train model english arabic german french spanish russian turkish chinese train model large amount data achieve significantly improve performance original paper propose methods suite monolingual semantic similarity cross lingual semantic similarity bitext mine task moreover result model surpass prior work unsupervised semantic textual similarity significantly outperform even bert base model like sentence bert reimers gurevych two thousand and nineteen additionally model order magnitude faster prior work use cpu little difference inference speed even improve speed gpu use cpu core make model attractive choice users without access gpus use embed devices finally add significantly increase functionality code base train paraphrastic sentence model ease use inference train desire language parallel data also include code automatically download preprocess train data
knowledge ground dialogue agents systems design conduct conversation base externally provide background information wikipedia page dialogue agents especially base neural network language model often produce responses sound fluent justify background information progress towards address problem require develop automatic evaluation metrics quantify extent responses ground background information facilitate evaluation metrics introduce benchmark evaluation ground interaction begin begin consist eight thousand, one hundred and thirteen dialogue turn generate language model base dialogue systems accompany humans annotations specify relationship system response background information annotations base extension natural language inference paradigm use benchmark demonstrate effectiveness adversarially generate data improve evaluation metric base exist natural language inference datasets
able generate informative coherent dialogue responses crucial design human like open domain dialogue systems encoder decoder base dialogue model tend produce generic dull responses decode step predictable response likely non informative response instead suitable one alleviate problem propose train generation model bidirectional manner add backward reason step vanilla encoder decoder train propose backward reason step push model produce informative coherent content forward generation step output use infer dialogue context backward direction advantage method forward generation backward reason step train simultaneously use latent variable facilitate bidirectional optimization method improve response quality without introduce side information eg pre train topic model propose bidirectional response generation method achieve state art performance response quality
end end architecture make promise progress speech translation st however st task still challenge low resource condition st model show unsatisfactory result especially absence word information source speech utterance study survey methods improve st performance without use source transcription propose learn framework utilize language independent universal phone recognizer framework base attention base sequence sequence model encoder generate phonetic embeddings phone aware acoustic representations decoder control fusion two embed stream produce target token sequence addition investigate different fusion strategies explore specific usage byte pair encode bpe compress phone sequence syllable like segment sequence semantic information experiment conduct fisher spanish english taigi mandarin drama corpora show method outperform conformer base baseline performance close exist best method use source transcription
people convey information extremely effectively speak interaction use multiple channel information transmission lexical channel say non lexical channel say propose study human perception speak communication mean better understand information encode across channel focus question characteristics communicative context affect listener expectations speech investigate present novel behavioural task test whether listeners discriminate true utterance dialogue utterances sample contexts lexical content characterize perception subsequent discriminative capability affect different degrees additional contextual information across lexical non lexical channel speech result demonstrate people effectively discriminate different prosodic realisations non lexical context informative channel provide salient information lexical channel highlight importance non lexical channel speak interaction
present three different question answer model train squad20 dataset bidaf documentqa albert retro reader demonstrate improvement language model past three years research fine tune pre train model question answer develop novel approach capable achieve two point improvement squad20 f1 reduce train time method initialize select layer parameter share language model simple yet empirically powerful
event argument extraction refer task extract structure information unstructured text particular event interest exist work exhibit poor capabilities extract causal event arguments like reason effect furthermore exist work model task sentence level restrict context local scope may effective short span text longer body text news article often observe arguments event necessarily occur sentence contain event trigger tackle issue argument scatter across sentence use global context become imperative task work propose external knowledge aid approach infuse document level event information aid extraction complex event arguments develop causal network event annotate dataset extract relevant event causal structure conceptnet phrase wikipedia use extract event causal feature bi directional transformer encoder effectively capture long range inter sentence dependencies report effectiveness propose approach qualitative quantitative analysis task establish find event annotate dataset five indian languages dataset add complexity task label arguments entity type like time place well complex argument type like reason effect approach achieve state art performance across five languages since work rely language specific feature easily extend languages
recent work demonstrate effectiveness cross lingual language model pretraining cross lingual understand study present result two larger multilingual mask language model 35b 107b parameters two new model dub xlm r xl xlm r xxl outperform xlm r eighteen twenty-four average accuracy xnli model also outperform roberta large model several english task glue benchmark three average handle ninety-nine languages suggest pretrained model larger capacity may obtain strong performance high resource languages greatly improve low resource languages make code model publicly available
common applications name entity recognition ner english highly available languages work present find name entity recognition five nigerian languages nigerian english nigerian pidgin english igbo yoruba hausa languages consider low resourced little openly available natural language process work do work individual ner model train metrics record languages also work combine model handle name entity recognition ner five languages combine model work well name entity recognitionner languages better performance compare individual ner model train specifically annotate data specific language aim work share learn information extraction use name entity recognition optimize list nigerian languages inclusion ease deployment production reusability model model develop project available github https gitio jy0kk interactive web app https nignerherokuappcom
recently increase number knowledge graph query experts however describe question use structure query straightforward non expert users need sufficient knowledge vocabulary structure query knowledge graph well syntax structure query language use describe user information need popular approach introduce overcome aforementioned challenge use natural language query knowledge graph although several question answer benchmarks use evaluate question answer systems number popular knowledge graph choose benchmark accurately assess quality question answer system challenge task paper introduce cbench extensible informative benchmarking suite analyze benchmarks evaluate question answer systems cbench use analyze exist benchmarks respect several fine grain linguistic syntactic structural properties question query benchmark show exist benchmarks vary significantly respect properties deem choose small subset unreliable evaluate qa systems research improve quality comprehensiveness benchmarks cbench use facilitate evaluation use set popular benchmarks augment user provide benchmarks cbench evaluate question answer system base popular single number metrics also give detail analysis linguistic syntactic structural properties answer unanswered question better help developers question answer systems better understand system excel struggle
recent years see rapid development information extraction well subtask relation extraction relation extraction able detect semantic relations entities sentence currently many efficient approach apply relation extraction task supervise learn approach especially good performance however still many difficult challenge one serious problems manually label data difficult acquire case limit data supervise approach equal lousy performance thus situation limit train data focus improve performance supervise baseline system unsupervised pre train feature one key components improve supervise approach traditional approach usually apply hand craft feature require expert knowledge expensive human labor however type feature might suffer data sparsity train set size small model parameters might poorly estimate thesis present several novel unsupervised pre train model learn distribute text representation feature encode rich syntactic semantic pattern relation expressions experiment demonstrate type feature combine traditional hand craft feature could improve performance logistic classification model relation extraction especially classification relations minor train instance
summarization clinical narratives long stand research problem introduce task hospital course summarization give documentation author throughout patient hospitalization generate paragraph tell story patient admission construct english text text dataset one hundred and nine thousand hospitalizations 2m source note correspond summary proxy clinician author brief hospital course paragraph write part discharge note exploratory analyse reveal bhc paragraph highly abstractive long extract fragment concise yet comprehensive differ style content organization source note exhibit minimal lexical cohesion represent silver standard reference analysis identify multiple implications model complex multi document summarization task
semantics emoji date consider static perspective offer first longitudinal study emoji semantics change time apply techniques computational linguistics six years twitter data identify five pattern emoji semantic development find evidence less abstract emoji likely undergo semantic change addition analyse select emoji detail examine effect seasonality world events emoji semantics aid future work emoji semantics make data publicly available along web base interface anyone use explore semantic change emoji
nlp sphere influence go much beyond computer science research development software applications past decade see people use nlp methods range academic discipline asian study clinical oncology also notice presence nlp module data science curricula within outside regular university setups course take students diverse background paper take closer look issue relate teach nlp diverse audiences base classroom experience identify challenge instructors face particularly ecosystem relate course students process also identify challenge areas nlp researchers tool developers
shoot intent detection challenge task due scare annotation problem paper propose pseudo siamese network psn generate label data shoot intents alleviate problem psn consist two identical subnetworks structure different weight action network object network subnetwork transformer base variational autoencoder try model latent distribution different components sentence action network learn understand action tokens object network focus object relate expressions provide interpretable framework generate utterance action object exist give intent experiment two real world datasets show psn achieve state art performance generalize shoot intent detection task
gender race social bias recently detect evident examples unfairness applications natural language process key path towards fairness understand analyse interpret data algorithms recent study show human generate data use train apparent factor get bias addition current algorithms also prove amplify bias data address concern paper study state art recurrent neural language model behave train data represent females use pre train standard debiased word embeddings result show language model inherit higher bias train unbalance data use pre train embeddings comparison use embeddings train within task moreover result show data language model inherit lower bias use debiased pre train emdeddings compare use standard pre train embeddings
readability assessment task evaluate read difficulty give piece text although research computational approach readability assessment two decades old much work synthesize research article brief survey contemporary research develop computational model readability assessment identify common approach discuss shortcomings identify challenge future possible also connect computational research insights relate work discipline education psychology
paper present result russian news cluster headline selection share task part propose task russian news event detection headline selection headline generation task accompany datasets baselines present datasets event detection headline selection first public russian datasets task headline generation dataset base cluster provide multiple reference headline every cluster unlike previous datasets finally approach propose share task participants report analyze
utilize clinical texts survival analysis difficult largely unstructured current automatic extraction model fail capture textual information comprehensively since label limit scope furthermore typically require large amount data high quality expert annotations train work present novel method use bert base hide layer representations clinical texts covariates proportional hazard model predict patient survival outcomes show hide layer yield notably accurate predictions predefined feature outperform previous baseline model fifty-seven average across c index time dependent auc make work publicly available https githubcom bionlplab heartfailuremortality
contribution describe two course module seek provide humanities major basic understand language technology applications use python learn materials consist interactive jupyter notebooks accompany youtube videos openly available creative commons licence
intensity relationship hold scalar adjectives eg nice great wonderful highly relevant natural language inference common sense reason previous research scalar adjective rank focus english mainly due availability datasets evaluation introduce new multilingual dataset order promote research scalar adjectives new languages perform series experiment set performance baselines dataset use monolingual multilingual contextual language model additionally introduce new binary classification task english scalar adjective identification examine model ability distinguish scalar relational adjectives probe contextualised representations report baseline result future comparison task
leader board like superglue see important incentives active development nlp since provide standard benchmarks fair comparison modern language model drive world best engineer team well resources collaborate solve set task general language understand performance score often claim close even higher human performance result encourage thorough analysis whether benchmark datasets feature statistical cue machine learn base language model exploit english datasets show often contain annotation artifacts allow solve certain task simple rule achieve competitive rank paper similar analysis do russian superglue rsg recently publish benchmark set leader board russian natural language understand show test datasets vulnerable shallow heuristics often approach base simple rule outperform come close result notorious pre train language model like gpt three bert likely simplest explanation significant part sota model performance rsg leader board due exploit shallow heuristics nothing common real language understand provide set recommendations improve datasets make rsg leader board even representative real progress russian nlu
discourse relations typically model discrete class characterize relation segment text eg causal explanations expansions however predefined discrete class limit universe potential relationships nuanced differences analogous contextual word embeddings propose represent discourse relations point high dimensional continuous space however unlike word discourse relations often surface form relations two segment often word phrase gap present challenge exist embed techniques present novel method automatically create discourse relation embeddings discre address embed challenge weakly supervise multitask approach learn diverse nuanced relations discourse segment social media result show discre one obtain best performance twitter discourse relation classification task macro f1076 two improve state art social media causality prediction f179 eighty-one three perform beyond modern sentence contextual word embeddings traditional discourse relation classification four capture novel nuanced relations eg relations semantically intersection causal explanations counterfactuals
transformer base language model approach automate story generation currently provide state art result however still suffer plot incoherence generate narratives time critically lack basic commonsense reason furthermore exist methods generally focus single character stories fail track character improve coherence generate narratives expand scope character centric narrative generation introduce commonsense inference augment neural storytelling cast framework introduce commonsense reason generation process model interaction multiple character find cast method produce significantly coherent topic two character stories outperform baselines dimension include plot plausibility stay topic also show cast method use train language model generate coherent stories reduce computation cost
machine read comprehension mrc sub field natural language process aim help computers understand unstructured texts answer question relate practice conversation essential way communicate transfer information help machine understand conversation texts present uit vicoqa new corpus conversational machine read comprehension vietnamese language corpus consist ten thousand question answer two thousand conversations health news article evaluate several baseline approach conversational machine comprehension uit vicoqa corpus best model obtain f1 score four thousand, five hundred and twenty-seven three thousand and ninety-one point behind human performance seven thousand, six hundred and eighteen indicate ample room improvement
paper investigate drive factor behind concatenation simple effective data augmentation method low resource neural machine translation experiment suggest discourse context unlikely improvement one bleu across four language pair instead demonstrate improvement come three factor unrelated discourse context diversity length diversity lesser extent position shift
timely responses policy makers mitigate impact covid nineteen pandemic rely comprehensive grasp events cause impact events report speed scale overwhelm paper present excavatorcovid machine read system ingest open source text document eg news scientific publications extract covid19 relate events relations build temporal causal analysis graph tcag excavator help government agencies alleviate information overload understand likely downstream effect political economic decisions events relate pandemic respond timely manner mitigate impact covid nineteen expect utility excavator outlive covid nineteen pandemic analysts decision makers empower excavator better understand solve complex problems future interactive tcag visualization available http afrl402bbncom5050 indexhtml also release demonstration video https vimeocom five hundred and twenty-eight million, six hundred and nineteen thousand and seven
stefan kopp nicole kramer say recent paperfrontiers psychology twelve two thousand and twenty-one five hundred and ninety-seven despite impressive demonstrations last decade still know make computer half decent conversation human argue capabilities require include incremental joint co construction mentalizing although agree whole heartedly statement problem paper argue different approach solution base new ai situate action
distribute word representations popularly use many task natural language process add pre train word vectors huge text corpus achieve high performance many different nlp task paper introduce multiple high quality word vectors french language two train huge crawl french data others train already exist french corpus also evaluate quality propose word vectors exist french word vectors french word analogy task addition evaluation multiple real nlp task show important performance enhancement pre train word vectors compare exist random ones finally create demo web application test visualize obtain word embeddings produce french word embeddings available public along fine tune code nlu task demo code
identification rare diseases clinical note natural language process nlp challenge due case available machine learn need data annotation clinical experts propose method use ontologies weak supervision approach include two step text umls link text mention concepts unify medical language system umls name entity link tool eg semehr weak supervision base customise rule bidirectional encoder representations transformers bert base contextual representations ii umls ordo match umls concepts rare diseases orphanet rare disease ontology ordo use mimic iii discharge summaries case study show text umls process greatly improve weak supervision without annotate data domain experts analysis show overall pipeline process discharge summaries surface rare disease case mostly uncaptured manual icd cod hospital admissions
present adam software system design run child language learn experiment python system use virtual world simulate ground language acquisition process language learner utilize cognitively plausible learn algorithms form perceptual linguistic representations observe world modular nature adam make easy design test different language learn curricula well learn algorithms report describe architecture adam system detail illustrate components examples provide code
introduction pretrained cross lingual language model bring decisive improvements multilingual nlp task however lack label task data necessitate variety methods aim close gap high resource languages zero shoot methods particular often use translate task data train signal bridge performance gap source target languages introduce xeroalign simple method task specific alignment cross lingual pretrained transformers xlm r xeroalign use translate task data encourage model generate similar sentence embeddings different languages xeroaligned xlm r call xlm ra show strong improvements baseline model achieve state art zero shoot result three multilingual natural language understand task xlm ra text classification accuracy exceed xlm r train label data perform par state art model cross lingual adversarial paraphrase task
paper present quantitative evaluation differences alternative translations large recently release finnish paraphrase corpus focus particular non trivial variation translation combine series automatic step detect systematic variation manual analysis reveal regularities identify categories translation differences find paraphrase corpus contain highly non trivial translation variants difficult recognize automatic approach
work explore application plato two various dialogue systems include open domain conversation knowledge ground dialogue task orient conversation plato two initially design open domain chatbot train via two stage curriculum learn first stage coarse grain response generation model learn fit simplify one one map relationship model apply task orient conversation give semantic mappings tend deterministic task completion second stage another fine grain generation model evaluation model learn diverse response generation coherence estimation respectively superior capability capture one many map model suitable open domain conversation knowledge ground dialogue comprehensive evaluation plato two participate multiple task dstc9 include interactive evaluation open domain conversation track3 task2 static evaluation knowledge ground dialogue track3 task1 end end task orient conversation track2 task1 plato two obtain 1st place three task verify effectiveness unify framework various dialogue systems
present new fully symbolic bayesian model semantic parse reason hope first step research program toward domain task general nlu ai humans create internal mental model observations greatly aid ability understand reason large variety problems aim capture model fully interpretable bayesian design specifically generality mind therefore provide clearer path future research expand capabilities derive implement inference algorithm evaluate domain proofwriter question answer reason task achieve zero shoot accuracies one hundred nine thousand, three hundred and forty-three depend experimental set thereby demonstrate value proof concept
paper propose enhance pair wise aspect opinion term extraction paote task incorporate rich syntactic knowledge first build syntax fusion encoder encode syntactic feature include label aware graph convolutional network lagcn model dependency edge label well pos tag unifiedly local attention module encode pos tag better term boundary detection pair adopt biaffine triaffine score high order aspect opinion term pair meantime harness syntax enrich representations lagcn syntactic aware score experimental result four benchmark datasets demonstrate model outperform current state art baselines meanwhile yield explainable predictions syntactic knowledge
important aspect develop dialogue systems evaluate compare performance different systems exist automatic evaluation metrics base turn level quality evaluation use average score system level comparison paper propose measure performance dialogue system compute distribution wise distance generate conversations real world conversations specifically two distribution wise metrics fbd prd develop evaluate experiment several dialogue corpora show propose metrics correlate better human judgments exist metrics
exist work tabular representation learn jointly model table associate text use self supervise objective function derive pretrained language model bert joint pretraining improve task involve pair table text eg answer question table show underperform task operate table without associate text eg populate miss cells devise simple pretraining objective corrupt cell detection learn exclusively tabular data reach state art suite table base prediction task unlike compete approach model tabbie provide embeddings table substructures cells row columns also require far less compute train qualitative analysis model learn cell column row representations show understand complex table semantics numerical trend
nlp rich history represent prior understand language form graph recent work analyze contextualized text representations focus hand design probe model understand extent representations encode particular linguistic phenomenon however due inter dependence various phenomena randomness train probe model detect representations encode rich information linguistic graph remain challenge problem paper propose new information theoretic probe bird eye fairly simple probe method detect representations encode information linguistic graph instead use classifier performance probe take information theoretic view probe estimate mutual information linguistic graph embed continuous space contextualized word representations furthermore also propose approach use probe investigate localize linguistic information linguistic graph use perturbation analysis call probe setup worm eye use probe analyze bert model ability encode syntactic semantic graph structure find model encode degree syntactic well semantic information albeit syntactic information greater extent
neural network architectures natural language process often use attention mechanisms produce probability distributions input token representations attention empirically demonstrate improve performance various task weight extensively use explanations model predictions recent study jain wallace two thousand and nineteen serrano smith two thousand and nineteen wiegreffe pinter two thousand and nineteen show generally consider faithful explanation jacovi goldberg two thousand and twenty across encoders task paper seek improve faithfulness attention base explanations text classification achieve propose new family task scale tasc mechanisms learn task specific non contextualised information scale original attention weight evaluation test explanation faithfulness show three propose variants tasc improve attention base explanations across two attention mechanisms five encoders five text classification datasets without sacrifice predictive performance finally demonstrate tasc consistently provide faithful attention base explanations compare three widely use interpretability techniques
qa model base pretrained language mod els achieve remarkable performance onv arious benchmark datasetshowever qa model generalize well unseen data fall outside train distribution due distributional shiftsdata augmentationda techniques drop replace word show effective regularize model overfitting train datayet may adversely affect qa task since incur semantic change may lead wrong answer qa task tackle problem propose simple yet effective da method base stochastic noise generator learn perturb word embed input question context without change semantics validate performance qa model train word embed perturbation single source dataset five different target domainsthe result show method significantly outperform baselineda methods notably model train outperform model train 240k artificially generate qa pair
whereas much success current generation neural language model drive increasingly large train corpora relatively little research dedicate analyze massive source textual data exploratory analysis delve deeper common crawl colossal web corpus extensively use train language model find contain significant amount undesirable content include hate speech sexually explicit content even filter procedures discuss potential impact content language model conclude future research directions mindful approach corpus collection analysis
evident deep text classification model train human data could bias particular produce bias outcomes texts explicitly include identity term certain demographic group refer type bias explicit bias extensively study however deep text classification model also produce bias outcomes texts write author certain demographic group refer bias implicit bias still rather limit understand paper first demonstrate implicit bias exist different text classification task different demographic group build learn base interpretation method deepen knowledge implicit bias specifically verify classifiers learn make predictions base language feature relate demographic attribute author next propose framework debiased tc train deep text classifiers make predictions right feature consequently mitigate implicit bias conduct extensive experiment three real world datasets result show text classification model train propose framework outperform traditional model significantly term fairness also slightly term classification performance
presidential speeches indicate government intentions justifications support dedicate style rhetoric oscillate explanation controversy period sixty years observe stylistic variations different french presidents fifth republic one thousand, nine hundred and fifty-eight two thousand and eighteen base official transcripts allocution paper illustrate stylistic evolution present underlie main trend study show de gaulle rhetoric mainly dedicate person two term j chirac fully similar accord several overall stylistic indicators macron style appear complex compare predecessors f hollande n sarkozy careful analysis clearly demonstrate noticeable new style compare recent us presidents french ones present similarities eg similar mean sentence length dissimilarities word less word comparative analysis macron style also clearly distinctive us former french presidents opt abstract discourse less anchor space use less number e macron tend use long sentence various stylistic rhetorical feature could explain misunderstand french people recurrent low approval rat
many minority languages resources need train large model available investigate performance zero shoot transfer learn little data possible influence language similarity process retrain lexical layer four bert base model use data two low resource target language varieties transformer layer independently fine tune pos tag task model source language combine new lexical layer fine tune transformer layer achieve high task performance target languages high language similarity 10mb data appear sufficient achieve substantial monolingual transfer performance monolingual bert base model generally achieve higher downstream task performance retrain lexical layer multilingual bert even target language include multilingual model
nowadays many e commerce platforms conduct global business e commerce search systems require handle product retrieval multilingual scenarios moreover compare maintain per country specific e commerce search systems universal system across countries reduce operational computational cost facilitate business expansion new countries paper introduce universal end end multilingual retrieval system discuss learn technical detail train deploy system serve billion scale product retrieval e commerce search particular propose multilingual graph attention base retrieval network leverage recent advance transformer base multilingual language model graph neural network architectures capture interactions search query items e commerce search offline experiment five countries data show algorithm outperform state art baselines thirty-five recall twenty-five map average moreover propose model show significant increase conversion revenue online b experiment deploy production multiple countries
build research argue possibility conceptual categorical knowledge acquisition statistics contain language evaluate predictive language model lms inform solely textual input prevalent phenomenon cognitive science typicality inspire experiment involve language process show robust typicality effect humans propose two test lms first test target whether typicality modulate lm probabilities assign taxonomic category memberships items second test investigate sensitivities typicality lms probabilities extend new information items categories test show modest completely absent correspondence lms humans suggest text base exposure alone insufficient acquire typicality knowledge
readers academic research paper often read goal answer specific question question answer systems answer question make consumption content much efficient however build tool require data reflect difficulty task arise complex reason claim make multiple part paper contrast exist information seek question answer datasets usually contain question generic factoid type information therefore present qasper dataset five thousand and forty-nine question one thousand, five hundred and eighty-five natural language process paper question write nlp practitioner read title abstract correspond paper question seek information present full text question answer separate set nlp practitioners also provide support evidence answer find exist model well qa task perform well answer question underperform humans least twenty-seven f1 point answer entire paper motivate research document ground information seek qa dataset design facilitate
despite recent advance natural language generation remain challenge control attribute generate text propose dexperts decode time experts decode time method control text generation combine pretrained language model expert lms anti expert lms product experts intuitively ensemble tokens get high probability consider likely experts unlikely anti experts apply dexperts language detoxification sentiment control generation outperform exist controllable generation methods automatic human evaluations moreover dexperts operate output pretrained lm effective anti experts smaller size include operate gpt three work highlight promise tune small lms text undesirable attribute efficient decode time steer
behavior deep neural network inconsistent different versions regressions model update common concern often weigh benefit accuracy efficiency gain work focus quantify reduce analyze regression errors nlp model update use negative flip rate regression measure show regression prevalent presence across task glue benchmark formulate regression free model update constrain optimization problem reduce relax form approximately optimize knowledge distillation train method empirically analyze model ensemble reduce regression finally conduct checklist behavioral test understand distribution regressions across linguistic phenomena efficacy ensemble distillation methods
numerous potential applications great impact end end speech translation st long treat independent task fail fully draw strength rapid advance sibling text machine translation mt text audio input represent differently modality gap render mt data end end model incompatible st counterparts observation obstacle propose bridge representation gap chimera project audio text feature common semantic representation chimera unify mt st task boost performance st benchmarks must c augment librispeech new state art specifically chimera obtain two hundred and seventy-one bleu must c en de improve sota nineteen bleu margin experimental analyse demonstrate share semantic space indeed convey common knowledge two task thus pave new way augment train resources across modalities code data resources available https githubcom glaciohound chimera st
work leverage commonsense knowledge form knowledge paths establish connections sentence form explicitation implicit knowledge connections direct singlehop paths require intermediate concepts multihop paths construct paths combine two model type joint framework call co nnect relation classifier predict direct connections concepts target prediction model generate target intermediate concepts give source concept relation use construct multihop paths unlike prior work rely exclusively static knowledge source leverage language model finetuned knowledge store conceptnet dynamically generate knowledge paths explanations implicit knowledge connect sentence texts central contribution design manual automatic evaluation settings assess quality generate paths conduct evaluations two argumentative datasets show combination two model type generate meaningful high quality knowledge paths sentence reveal implicit knowledge convey text
prior work show twitter users use skin tone emoji act self representation express racial ethnic identity test whether signal identity influence readers perceptions content post contain signal large scale n944 pre register control experiment manipulate presence skin tone emoji profile photos task readers rate obscure trivia facts present tweet true false use bayesian statistical analysis find neither emoji profile photo effect readers rate facts result comfort anyone concern manipulation online users craft fake profile
introduce implement cognitively plausible model learn generic language statements express generalizations members category important aspect concept development language acquisition carlson pelletier one thousand, nine hundred and ninety-five gelman two thousand and nine extend computational framework design model ground language acquisition introduce concept network new layer abstraction enable system encode knowledge learn generic statements represent associations concepts learn system three task utilize concept network demonstrate extensions adam acquire generic information provide example adam use model language acquisition
exist model machine read comprehension mrc require complex model architecture effectively model long texts paragraph representation classification thereby make inference computationally inefficient production use work propose vault light weight parallel efficient paragraph representation mrc base contextualized representation long document input train use new gaussian distribution base objective pay close attention partially correct instance close grind truth validate vault architecture show experimental result two benchmark mrc datasets require long context model one wikipedia base natural question nq technotes techqa vault achieve comparable performance nq state art sota complex document model approach sixteen time faster demonstrate efficiency propose model also demonstrate model also effectively adapt completely different domain techqa large improvement model fine tune previously publish large plm
facilitate effective translation model translation study one crucial question address assess translation quality perspectives accuracy reliability repeatability cost translation quality assessment tqa rich challenge task work present high level concise survey tqa methods include manual judgement criteria automate evaluation metrics classify detail sub categories hope work asset translation model researchers quality assessment researchers addition hope enable practitioners quickly develop better understand conventional tqa field find correspond closely relevant evaluation solutions need work may also serve inspire development quality assessment evaluation methodologies natural language process nlp task addition machine translation mt automatic text summarization ats natural language understand nlu natural language generation nlg
paper show process notam notice airmen data field civil aviation main research content follow 1data preprocessing original data notam mixture chinese english structure poor original data clean chinese data english data process separately word segmentation complete stop word remove use glove word vector methods represent data use custom map vocabulary 2decoupling feature classifiers order improve ability text classification model recognize minority sample overall model train process decouple perspective algorithm whole divide two stag feature learn classifier learn weight feature learn stage classifier learn stage adopt different strategies overcome influence head data tail data imbalanced data set classification model experiment prove use decouple feature classifier methods base neural network classification model complete text multi classification task field civil aviation time improve recognition accuracy minority sample data set
propose deep learn architecture test three machine learn model automatically detect individuals attempt suicide within one thirty days two six months use social media post data provide clpsych two thousand and twenty-one share task additionally create extract three set handcraft feature suicide risk detection base three stage theory suicide prior work emotions use pronouns among persons exhibit suicidal ideations extensive experimentations show traditional machine learn methods outperform baseline f1 score seven hundred and forty-one f2 score eight hundred and thirty-three subtask one prediction suicide attempt thirty days prior however propose deep learn method outperform baseline f1 score seven hundred and thirty-seven f2 score eight hundred and forty-three subtask two prediction suicide six months prior
sequence sequence seq2seq problems machine translation bidirectional naturally derive pair directional task two directional learn signal however typical seq2seq neural network simplex model one unidirectional task fully exploit potential bidirectional learn signal parallel data address issue propose duplex seq2seq neural network reder reversible duplex transformer apply machine translation architecture reder two end specialize language read yield sequence language result reder simultaneously learn bidirectional signal enable reversible machine translation simply flip input output end experiment widely use machine translation benchmarks verify reder achieve first success reversible machine translation help obtain considerable gain several strong baselines
recent work neural machine translation demonstrate necessity feasibility use inter sentential context context sentence currently translate however many current methods present model architectures theoretically use extra context often clear much actually utilize translation time paper introduce new metric conditional cross mutual information quantify usage context model use metric measure much document level machine translation systems use particular varieties context find target context reference source context condition longer context diminish effect result introduce new simple train method context aware word dropout increase usage context context aware model experiment show method increase context usage reflect translation quality accord metrics bleu comet well performance anaphoric pronoun resolution lexical cohesion contrastive datasets
human level nlp task predict mental health personality demographics number observations often smaller standard seven hundred and sixty-eight hide state size layer within modern transformer base language model limit ability effectively leverage transformers provide systematic study role dimension reduction methods principal components analysis factorization techniques multi layer auto encoders well dimensionality embed vectors sample size function predictive performance first find fine tune large model limit amount data pose significant difficulty overcome pre train dimension reduction regime roberta consistently achieve top performance human level task pca give benefit reduction methods better handle users write longer texts finally observe majority task achieve result comparable best performance frac112 embed dimension
coherent discourse distinguish mere collection utterances satisfaction diverse set constraints example choice expression logical relation denote events implicit compatibility world knowledge neural language model encode constraints design extendable set test suit address different aspects discourse dialogue coherence unlike previous coherence evaluation study address specific linguistic devices beyond sentence order perturbations allow fine grain analysis constitute coherence neural model train language model objective encode extend target evaluation paradigm neural language model marvin linzen two thousand and eighteen phenomena beyond syntax show paradigm equally suit evaluate linguistic qualities contribute notion coherence
learn prerequisite chain essential task efficiently acquire knowledge know unknown domains example one may expert natural language process nlp domain want determine best order learn new concepts unfamiliar computer vision domain cv domains share common concepts machine learn basics deep learn model paper propose unsupervised cross domain concept prerequisite chain learn use optimize variational graph autoencoder model learn transfer concept prerequisite relations information rich domain source domain information poor domain target domain substantially surpass baseline model also expand exist dataset introduce two new domains cv bioinformatics bio annotate data resources well code make publicly available
negation core construction natural language despite successful many task state art pre train language model often handle negation incorrectly improve language model regard propose augment language model objective unlikelihood objective base negate generic sentence raw text corpus train bert result combine objective reduce mean top1 error rate four negate lama dataset also see improvements negate nli benchmarks
dialogue state track dst play key role task orient dialogue systems monitor user goal general two strategies track dialogue state predict scratch update previous state scratch base strategy obtain slot value inquire dialogue history previous base strategy rely current turn dialogue update previous dialogue state however hard scratch base strategy correctly track short dependency dialogue state noise meanwhile previous base strategy useful long dependency dialogue state track obviously play different roles context information different granularity track different kinds dialogue state thus paper study discuss context information different granularity affect dialogue state track first explore greatly different granularities affect dialogue state track discuss combine multiple granularities dialogue state track finally apply find context granularity shoot learn scenario besides publicly release cod
substantial improvements make machine read comprehension machine answer question base give context current state art model even surpass human performance several benchmarks however abilities cross lingual scenario still explore previous work reveal abilities pre train multilingual model zero shoot cross lingual read comprehension paper utilize unlabeled data improve performance model first supervise train source language corpus self train unlabeled target language data experiment result show improvements languages also analyze self train benefit cross lingual read comprehension qualitative aspects
logical reason text require understand critical logical information text perform inference large scale pre train model logical reason mainly focus word level semantics text struggle capture symbolic logic paper propose understand logical symbols expressions text arrive answer base logical information put forward context extension framework also propose data augmentation algorithm former extend context cover implicit logical expressions follow logical equivalence laws latter augment literally similar logically different instance better capture logical information especially logical negative conditional relationships conduct experiment reclor dataset result show method achieve state art performance logic drive context extension framework data augmentation algorithm help improve accuracy multi model ensemble system first surpass human performance easy set hard set reclor
presentations critical communication areas live yet creation slide deck often tedious time consume limit research aim automate document slide generation process face critical challenge publicly available dataset train benchmarking work first contribute new dataset sciduet consist pair paper correspond slide deck recent years nlp ml conferences eg acl secondly present d2s novel system tackle document slide task two step approach one use slide title retrieve relevant engage text figure table two summarize retrieve context bullet point long form question answer evaluation suggest long form qa outperform state art summarization baselines automate rouge metrics qualitative human evaluation
recent advance neural architectures revive problem morphological rule learn evaluate transformer model morphological rule learn compare recurrent neural network rnn english german russian bring fore hitherto overlook problem morphological gap expect inflection word miss example sixty-three russian verbs lack first person singular present form one comfortably say ofvsvcyouvsvcyou feel even english gap past participle stride function morphological inflection partial neural architectures produce inflections ought miss analyse reveal transformers recapitulate statistical distribution inflections train data similar rnns model success english german drive fact rule languages identify majority form universal
recently certify defense methods develop provably guarantee robustness text classifier adversarial synonym substitutions however exist certify defense methods assume defenders inform adversaries generate synonyms realistic scenario paper propose certifiably robust defense method randomly mask certain proportion word input text unrealistic assumption longer necessary propose method defend word substitution base attack also character level perturbations certify classifications fifty texts robust perturbation five word agnews two word sst2 dataset experimental result show randomize smooth method significantly outperform recently propose defense methods across multiple datasets
transfer learn become dominant paradigm many natural language process task addition model pretrained large datasets train intermediate supervise task similar target task small natural language inference nli datasets language model typically follow pretraining large label nli dataset fine tune nli subtask work explore gradient boost decision tree gbdts alternative commonly use multi layer perceptron mlp classification head gbdts desirable properties good performance dense numerical feature effective ratio number sample wrt number feature low introduce freegbdt method fit gbdt head feature compute fine tune increase performance without additional computation neural network demonstrate effectiveness method several nli datasets use strong baseline model roberta large mnli pretraining freegbdt show consistent improvement mlp classification head
transformer base model achieve state art result wide range natural language process nlp task include document summarization typically systems train fine tune large pre train model target task one issue transformer base model scale well term memory compute requirements input length grow thus long document summarization challenge train fine tune model work exploit large pre train transformer base model address long span dependencies abstractive summarization use two methods local self attention explicit content selection approach compare range network configurations experiment carry standard long span summarization task include spotify podcast arxiv pubmed datasets demonstrate combine methods achieve state art result three task rouge score moreover without large scale gpu card approach achieve comparable better result exist approach
natural language generation task challenge generate informative coherent review text order enhance informativeness generate text exist solutions typically learn copy entities triple knowledge graph kgs however lack overall consideration select arrange incorporate knowledge tend text incoherence address issue focus improve entity centric coherence generate review leverage semantic structure kgs paper propose novel coherence enhance text plan model cetp base knowledge graph kgs improve global local coherence review generation propose model learn two level text plan generate document one document plan model sequence sentence plan order two sentence plan model entity base subgraph kg local coherence naturally enforce kg subgraphs intra sentence correlations entities global coherence design hierarchical self attentive architecture subgraph node level attention enhance correlations subgraphs knowledge first utilize kg base text plan model enhance text coherence review generation extensive experiment three datasets confirm effectiveness model improve content coherence generate texts
legal artificial intelligence legalai aim benefit legal systems technology artificial intelligence especially natural language process nlp recently inspire success pre train language model plms generic domain many legalai researchers devote effort apply plms legal task however utilize plms address legal task still challenge legal document usually consist thousands tokens far longer length mainstream plms process paper release longformer base pre train language model name lawformer chinese legal long document understand evaluate lawformer variety legalai task include judgment prediction similar case retrieval legal read comprehension legal question answer experimental result demonstrate model achieve promise improvement task long document input
patent analysis mine time consume costly process company nevertheless essential will remain competitive face overload induce numerous patent idea automatically filter bring read experts paper report successful application fine tune retrain pre train deep natural language process model patent classification solution propose combine several state art treatments achieve goal decrease workload preserve recall precision metrics
paper propose message pass mechanism address language model new layer type introduce aim substitute self attention system show competitive exist methods give n tokens computational complexity ofn log n memory complexity ofn reasonable assumptions end dispatcher layer see achieve comparable perplexity prior result efficient
introduce docscan completely unsupervised text classification approach use semantic cluster adopt nearest neighbor scan document obtain semantically informative vectors large pre train language model similar document proximate vectors neighbor representation space tend share topic label learnable cluster approach use pair neighbor datapoints weak learn signal propose approach learn assign class whole dataset without provide grind truth label five topic classification benchmarks improve various unsupervised baselines large margin datasets relatively balance outcome class docscan approach performance supervise classification method fail type classification sentiment analysis point important conceptual practical differences classify image texts
online political advertise central aspect modern election campaign influence public opinion computational analysis political ads utmost importance political science understand characteristics digital campaign also important computational linguistics study feature political discourse communication large scale work present first computational study online political ads aim one infer political ideology ad sponsor two identify whether sponsor official political party third party organization develop two new large datasets two task consist ads yous evaluation result show approach combine textual visual information pre train neural model outperform state art method generic commercial ad classification finally provide depth analysis limitations best perform model linguistic analysis study characteristics political ads discourse
technology language generation advance rapidly spur advancements pre train large model massive amount data need intelligent agents communicate natural manner techniques effectively generate fluent text also produce undesirable societal bias disproportionately negative impact marginalize populations language generation present unique challenge bias term direct user interaction structure decode techniques better understand challenge present survey societal bias language generation focus data techniques contribute bias progress towards reduce bias motivate lack study bias decode techniques also conduct experiment quantify effect techniques discuss general trend open challenge call attention promise directions research importance fairness inclusivity considerations language generation applications
pre train language model plms achieve great success machine read comprehension mrc past years although general language representation learn large scale corpora benefit mrc poor support evidence extraction require reason across multiple sentence hinder plms advance mrc bridge gap general plms mrc present rept retrieval base pre train approach particular introduce two self supervise task strengthen evidence extraction pre train inherit downstream mrc task consistent retrieval operation model architecture evaluate propose method conduct extensive experiment five mrc datasets require collect evidence reason across multiple sentence experimental result demonstrate effectiveness pre train approach moreover analysis show approach able enhance capacity evidence extraction without explicit supervision
automatic classification arabic dialects ongoing research challenge explore recent work define dialects base increasingly limit geographic areas like cities provinces paper focus relate yet relatively unexplored topic effect geographical proximity cities locate arab countries dialectical similarity work twofold reliant one compare textual similarities dialects use cosine similarity two measure geographical distance locations study madar nadi two establish datasets arabic dialects many cities provinces result indicate cities locate different countries may fact dialectical similarity cities within country depend geographical proximity correlation dialectical similarity city proximity suggest cities closer together likely share dialectical attribute regardless country border nuance provide potential important advancements arabic dialect research indicate granular approach dialect classification essential understand frame problem arabic dialects identification
zero shoot cross domain dialogue state track dst enable us handle task orient dialogue unseen domains without expense collect domain data paper propose slot description enhance generative approach zero shoot cross domain dst specifically model first encode dialogue context slot pre train self attentive encoder generate slot value auto regressive manner addition incorporate slot type inform descriptions capture share information across slot facilitate cross domain knowledge transfer experimental result multiwoz dataset show propose method significantly improve exist state art result zero shoot cross domain set
open information extraction openie aim extract structure relational tuples subject relation object sentence play critical roles many downstream nlp applications exist solutions perform extraction sentence level without refer additional contextual information reality however sentence typically exist part document rather standalone often need access relevant contextual information around sentence accurately interpret document level context aware openie dataset available manually annotate eight hundred sentence eighty document two domains healthcare transportation form docoie dataset evaluation addition propose docie novel document level context aware openie model experimental result base docie demonstrate incorporate document level context helpful improve openie performance docoie dataset docie model release public
sentence embed methods use natural language inference nli datasets successfully apply various task however methods available limit languages due rely heavily large nli datasets paper propose defsent sentence embed method use definition sentence word dictionary since dictionaries available many languages defsent broadly applicable methods use nli datasets without construct additional datasets demonstrate defsent perform comparably unsupervised semantics textual similarity sts task slightly better senteval task methods use large nli datasets
paper introduce two level attention schema poolingformer long document model first level use smaller slide window pattern aggregate information neighbor second level employ larger window increase receptive field pool attention reduce computational cost memory consumption first evaluate poolingformer two long sequence qa task monolingual nq multilingual tydi qa experimental result show poolingformer sit atop three official leaderboards measure f1 outperform previous state art model nineteen point seven hundred and ninety-eight vs seven hundred and seventy-nine nq long answer nineteen point seven hundred and ninety-five vs seven hundred and seventy-six tydi qa passage answer sixteen point six hundred and seventy-six vs six hundred and sixty tydi qa minimal answer evaluate poolingformer long sequence summarization task experimental result arxiv benchmark continue demonstrate superior performance
grammatical error correction gec aim correct write errors help language learners improve write skills however exist gec model tend produce spurious corrections fail detect lot errors quality estimation model necessary ensure learners get accurate gec result avoid mislead poorly correct sentence well train gec model generate several high quality hypotheses decode beam search provide valuable gec evidence use evaluate gec quality however exist model neglect possible gec evidence different hypotheses paper present neural verification network vernet gec quality estimation multiple hypotheses vernet establish interactions among hypotheses reason graph conduct two kinds attention mechanisms propagate gec evidence verify quality generate hypotheses experiment four gec datasets show vernet achieve state art grammatical error detection performance achieve best quality estimation result significantly improve gec performance reranking hypotheses data source cod available https githubcom thunlp vernet
paper describe submission iwslt two thousand and twenty-one offline speech translation task upc machine translation group task consist build system capable translate english audio record extract ted talk german text submit systems either cascade end end use custom give segmentation submission end end speech translation system combine pre train model wav2vec twenty mbart couple modules encoder decoder use efficient fine tune technique train twenty total parameters show add adapter system pre train increase convergence speed final result achieve bleu score two hundred and seventy-three must c test set final model ensemble obtain two thousand, eight hundred and twenty-two bleu score set submission also use custom segmentation algorithm employ pre train wav2vec twenty identify periods untranscribable text bring improvements twenty-five three bleu score iwslt two thousand and nineteen test set compare result give segmentation
use trigram model fine tune pretrained bert model sequence classification show machine translation human translation classify accuracy chance level suggest machine translation human translation different systematic way classification accuracy machine translation much higher human translation show may explain difference lexical diversity machine translation human translation machine translation independent pattern human translation automatic metrics measure deviation machine translation human translation may conflate difference quality experiment two different type automatic metrics show correlation result classification task therefore suggest difference lexical diversity machine translation human translation give attention machine translation evaluation
economic policy uncertainty epu critical indicator economic study use forecast recession higher level uncertainty firm owners cut investment lead longer post recession recovery epu index compute count news article contain pre define keywords relate policy make economy convey uncertainty unfortunately method sensitive original keyword set richness news coverage thus reproduce result different countries challenge paper propose unsupervised text mine method use word embed representation space select relevant keywords method strictly sensitive semantic similarity threshold apply word embed vectors require pre define dictionary experiment use massive repository persian news show epu series compute propose method precisely follow major events affect iran economy compatible world uncertainty index wui iran
humans experience world profoundly multimodal begin exist state art language model use text modality learn represent semantic mean paper review literature role embodiment emotion interactive set speak dialogue necessary prerequisites language learn human children include word child vocabularies largely concrete shift become abstract children get older sketch model semantics leverage current transformer base model word level ground model explain robot dialogue system make use semantic model set system learn language exist benchmarks evaluation
propose novel framework model interaction graphical structure natural language text associate nod edge exist approach typically fall two categories group ignore relational structure convert linear sequence utilize highly successful seq2seq model side ignore sequential nature text represent fix dimensional vectors apply graph neural network simplifications lead information loss propose method utilize graphical structure well sequential nature texts input model set text segment associate nod edge graph process transformer encoder decoder model equip self attention mechanism aware graphical relations nod contain segment also allow us use bert like model already train large amount text propose model wide applications demonstrate capabilities data text generation task approach compare favorably state art methods four task without tailor model architecture also provide early demonstration novel practical application generate clinical note medical entities mention clinical visit
multilingual transformer base language model usually pretrained one hundred languages show achieve outstanding result wide range cross lingual transfer task however remain unknown whether optimization different languages condition capacity model generalize syntactic structure languages syntactic phenomena different complexity affect work explore syntactic generalization capabilities monolingual multilingual versions bert roberta specifically evaluate syntactic generalization potential model english spanish test compare syntactic abilities monolingual multilingual model language english multilingual model two different languages english spanish english use available syntaxgym test suite spanish introduce syntaxgymes novel ensemble target syntactic test spanish design evaluate syntactic generalization capabilities language model syntaxgym online platform
study possibilities build non autoregressive speech text translation model use connectionist temporal classification ctc use ctc base automatic speech recognition auxiliary task improve performance ctc success translation counter intuitive due monotonicity assumption analyze reorder capability kendall tau distance introduce quantitative metric gradient base visualization provide intuitive way take closer look model analysis show transformer encoders ability change word order point future research direction worth explore non autoregressive speech translation
code switch csw common phenomenon occur multilingual geographic social contexts raise challenge problems natural language process tool focus machine translation mt csw texts aim simultaneously disentangle translate two mix languages due lack actual translate csw data generate artificial train data regular parallel texts experiment show train strategy yield mt systems surpass multilingual systems code switch texts result confirm alternative task aim provide contextual translations l2 write assistant
social network platforms provide conduit disseminate ideas view thoughts proliferate information lead amalgamation english natively speak languages prevalence hindi english code mix data hinglish rise urban population world hate speech detection algorithms deploy social network platforms unable filter offensive abusive content post code mix languages thus worldwide hate speech detection rate around forty-four drop even consider content indian colloquial languages slang paper propose methodology efficient detection unstructured code mix hinglish language fine tune base approach hindi english code mix language employ utilize contextual base embeddings elmo embeddings language model flair transformer base bert bidirectional encoder representations transformers propose approach compare pre exist methods result compare various datasets model outperform methods frameworks
persuasion game fundamental economics ai research serve basis important applications however work setup assume communication stylize message consist rich human language paper consider repeat sender expert receiver decision maker game sender fully inform state world aim persuade receiver accept deal send one several possible natural language review design automatic expert play repeat game aim achieve maximal payoff expert implement within monte carlo tree search mcts algorithm deep learn model exploit behavioral linguistic signal order predict next action decision maker future payoff expert give state game candidate review demonstrate superiority expert strong baselines adaptability different decision makers select review nicely adapt propose deal
article report survey carry across natural language process nlp community survey aim capture opinions research community issue surround share task respect participation organisation amongst one hundred and seventy-five responses receive positive negative observations make carry report extensive analysis responses lead us propose share task organisation checklist could support future participants organisers propose checklist flexible enough accommodate wide diversity share task field goal prescriptive rather serve tool encourage share task organisers foreground ethical behaviour begin common issue one hundred and seventy-five respondents deem important usage would serve instrument reflect important aspects share task would also promote increase transparency around
recent work show distribute word representations encode abstract semantic syntactic information child direct speech paper use diachronic distribute word representations perform temporal model analysis lexical development children unlike previous work use temporally slice speech corpus learn distribute word representations child child direct speech model experiment demonstrate dynamics grow lexical knowledge children time compare saturate level lexical knowledge child direct adult speech also fit linear mix effect model rate semantic change diachronic representations word frequencies allow us inspect role word frequencies towards lexical development children perform qualitative analysis diachronic representations model reveal categorization word associations mental lexicon children
demonstrate feasible diacritize hebrew script without human curated resources plain diacritized text present nakdimon two layer character level lstm perform par much complicate curation dependent systems across diverse array modern hebrew source
recent progress natural language understand nlu see latest model outperform human performance many standard task impressive result lead community introspect dataset limitations iterate nuanced challenge paper introduce task headline group hlg correspond dataset hlgd consist twenty thousand and fifty-six pair news headline label binary judgement whether pair belong within group hlgd human annotators achieve high performance around nine f one current state art transformer model reach seventy-five f one open path improvements propose novel unsupervised headline generator swap model task headline group achieve within three f one best supervise model finally analyze high perform model consistency test find model consistent predictions reveal model limit current architectures
pretrained language model excel many nlp task recently however social intelligence still unsatisfactory enable machine need general understand complicate world develop ability perform commonsense reason besides fit specific downstream task external commonsense knowledge graph kgs conceptnet provide rich information word relationships thus towards general commonsense learn propose two approach emphimplicitly emphexplicitly infuse kgs pretrained language model demonstrate propose methods perform well socialiqa social commonsense reason task limit full train data regimes
large body work scrutinize mean conditional sentence considerably less attention pay formal model pragmatic use interpretation take probabilistic approach pragmatic reason conditionals flexibly integrate gradient beliefs richly structure world state model listeners update prior beliefs causal structure world joint probabilities consequent antecedent base assumptions speaker utterance production protocol show supply natural contextual assumptions model uniformly explain number inferences attest literature include epistemic inferences conditional perfection dependency antecedent consequent conditional argue approach also help explain three puzzle introduce douven two thousand and twelve update conditionals depend utterance context listener belief antecedent may increase decrease remain unchanged
propose ensemble model predict lexical complexity word multiword expressions mwes model receive input sentence target word mweand output complexity score give key challenge task limit size annotate data model rely pretrained contextual representations different state art transformer base language model ie bert roberta variety train methods enhance model generalization robustnessmulti step fine tune multi task learn adversarial train additionally propose enrich contextual representations add hand craft feature train model achieve competitive result rank among top ten systems sub task
construct first ever multimodal sarcasm dataset spanish audiovisual dataset consist sarcasm annotate text align video audio dataset represent two varieties spanish latin american variety peninsular spanish variety ensure wider dialectal coverage global language present several model sarcasm detection serve baselines future research result show result text eighty-nine worse combine text audio nine hundred and nineteen finally best result obtain combine modalities text audio video nine hundred and thirty-one
open pit mine leave many regions worldwide inhospitable uninhabitable put regions back use entire stretch land must renaturalized sustainable subsequent use transfer new primary use many contaminate sit soil information permanently manage case information available form expert report unstructured data collections file folders best case digitize due size complexity data difficult single person overview data order able make reliable statements one important obstacles rapid transfer areas use information base approach issue support fulfil several sustainable development goals regard environment issue health climate action use stack optical character recognition text classification active learn geographic information system visualization effectively mine visualize information subsequently link extract information geographic coordinate visualize use geographic information system active learn play vital role dataset provide train data total process nine categories actively learn representation dataset evaluate ocr active learn text classification separately report performance system active learn text classification result twofold whereas categories restrictions work sufficient eighty-five f1 seven topic orient categories complicate human coders hence result achieve mediocre evaluation score seventy f1
work propose bertgcn model combine large scale pretraining transductive learn text classification bertgcn construct heterogeneous graph dataset represent document nod use bert representations jointly train bert gcn modules within bertgcn propose model able leverage advantage worlds large scale pretraining take advantage massive amount raw data transductive learn jointly learn representations train data unlabeled test data propagate label influence graph convolution experiment show bertgcn achieve sota performances wide range text classification datasets code available https githubcom zerorin bertgcn
relevance key information extraction kie task increasingly important natural language process problems still well define problems serve benchmarks solutions area bridge gap introduce two new datasets kleister nda kleister charity involve mix scan bear digital long formal english language document datasets nlp system expect find infer various type entities employ textual structural layout feature kleister charity dataset consist two thousand, seven hundred and eighty-eight annual financial report charity organizations sixty-one thousand, six hundred and forty-three unique page twenty-one thousand, six hundred and twelve entities extract kleister nda dataset five hundred and forty non disclosure agreements three thousand, two hundred and twenty-nine unique page two thousand, one hundred and sixty entities extract provide several state art baseline systems kie domain flair bert roberta layoutlm lambert show datasets pose strong challenge exist model best model achieve eight thousand, one hundred and seventy-seven eight thousand, three hundred and fifty-seven f1 score respectively kleister nda kleister charity datasets share datasets encourage progress depth complex information extraction task
research sociology linguistics show people use language express identity understand identity others recent work establish connection expression identity emoji usage social media use emoji skin tone modifiers motivate find work ask language readers sensitive act self expression use understand identity author behavioral experiment n488 text emoji content social media post carefully control present participants find affirmative emoji salient signal author identity signal distinct complementary one encode language participant group base self identify ethnicity show differences perceive signal except case default yellow emoji group associate white identity effect stronger white participants find emoji index social variables experimental applications researchers also implications designers supposedly neutral default may representative users others
pretrained language model demonstrate outstanding performance many nlp task recently however social intelligence require commonsense reason current situation mental state others still develop towards improve language model social intelligence focus social iqa dataset task require social emotional commonsense reason build top pretrained roberta gpt2 model propose several architecture variations extensions well leverage external commonsense corpora optimize model social iqa propose system achieve competitive result top rank model leaderboard work demonstrate strengths pretrained language model provide viable ways improve performance particular task
paper argue design development multimodal datasets natural language process nlp challenge enhance two significant respect broadly represent commonsense semantic inferences better reflect dynamics action events substantive alignment textual visual information identify challenge task reflective linguistic cognitive competencies humans speak reason rather merely performance systems isolate task introduce distinction challenge base task competence base performance describe diagnostic dataset recipe video question r2vq design test competence base comprehension multimodal recipe collection http r2vqorg corpus contain detail annotation support inferencing task facilitate rich set question families use evaluate nlp systems
evaluate large summarization corpora use humans prove expensive organizational financial perspective therefore many automatic evaluation metrics develop measure summarization quality fast reproducible way however metrics still rely humans need gold standard summaries generate linguistic experts since blanc require golden summaries supposedly use underlie language model consider application evaluation summarization german work demonstrate adjust blanc metric language english compare blanc score crowd expert rat well commonly use automatic metrics german summarization data set result show blanc german especially good evaluate informativeness
task orient dialog tod systems typically manage structure knowledge eg ontologies databases guide goal orient conversations however fall short handle dialog turn ground unstructured knowledge eg review document paper formulate task model tod ground structure unstructured knowledge address task propose tod system hybrid knowledge management hyknow extend belief state manage structure unstructured knowledge first end end model jointly optimize dialog model ground two kinds knowledge conduct experiment modify version multiwoz twenty-one dataset dialogs ground hybrid knowledge experimental result show hyknow strong end end performance compare exist tod systems also outperform pipeline knowledge management scheme higher unstructured knowledge retrieval accuracy
model thematic fit verb argument compositional semantics task currently require large burden data take high perform neural approach model verb argument fit previously train linguistically machine annotate large corpus replace corpus layer output higher quality taggers contrary popular beliefs deep learn era data effective higher quality annotation discover higher annotation quality dramatically reduce data requirement demonstrate better supervise predicate argument classification apply model psycholinguistic task outside train objective saw small gain one two thematic fit estimation task none replicate previous study modify certain role representation detail set new state art event model use fraction data
isomorphisms allow human cognition transcribe potentially unsolvable problem one domain different domain problem might easily address current approach focus transcribe structural information source target structure ignore semantic pragmatic information functional language theory present five subconstructs classification understand languages derive map metamodels linguistics graph theory show currently construct exist canonical graph representation semantic pragmatic information find work need do understand graph enrich allow isomorphisms capture semantic pragmatic information capture additional information could lead understand source structure enhance manipulations interrogations contain relationships current mathematical graph structure general definition allow expression higher information level source
substantial variability expectations communication partner bring interactions create potential misunderstand directly probe gap ability overcome propose communication task base color concept associations experiment one establish several key properties mental representations expectations emphlexical priors base recent probabilistic theories associations variable abstract concepts variability represent uncertainty within individual uncertainty enable accurate predictions whether others likely share association experiment two examine downstream consequences representations communication accuracy initially low communicate concepts variable associations rapidly increase participants form ad hoc conventions together find suggest people cope variability maintain well calibrate uncertainty partner appropriately adaptable representations
stance detection social media help identify understand slant news commentary everyday life work propose new model zero shoot stance detection twitter use adversarial learn generalize across topics model achieve state art performance number unseen test topics minimal computational cost addition extend zero shoot stance detection new topics highlight future directions zero shoot transfer
rapid development artificial intelligence ai trend move ai applications neural machine translation nmt cloud mobile devices smartphones constrain limit hardware resources battery performance device nmt systems far satisfactory inspire conditional computation propose improve performance device nmt systems dynamic multi branch layer specifically design layer wise dynamic multi branch network one branch activate train inference branch activate train propose share private reparameterization ensure sufficient train branch almost computational cost method achieve improvements seventeen bleu point wmt14 english german translation task eighteen bleu point wmt20 chinese english translation task transformer model respectively compare strong baseline also use multiple branch propose method sixteen time faster number parameters
present dalaj ten dataset linguistic acceptability judgments swedish comprise nine five hundred and ninety-six sentence first version initial experiment use binary classification task dalaj base swell second language learner data consist essay different level proficiency make sure dataset freely available despite gdpr regulations sentence scramble learner essay remove part metadata learners keep sentence information mother tongue level course essay write use normalize version learner language basis dalaj sentence keep one error per sentence repeat sentence individual correction tag use sentence dalaj ten use four error categories thirty-five available swell connect lexical word build choices baseline result binary classification show accuracy fifty-eight dalaj ten use bert embeddings dataset include swedishglue swe superlim benchmark describe format dataset first experiment insights motivation choose approach data share
automatic phenotyping task identify cohorts patients match predefined set criteria phenotyping typically involve classify long clinical document contain thousands tokens time recent state art transformer base pre train language model limit input hundred tokens eg five hundred and twelve tokens bert evaluate several strategies incorporate pre train sentence encoders document level representations clinical text find hierarchical transformers without pre train competitive task pre train model
name entity recognition ner well study task natural language process traditional ner research deal flat entities ignore nest entities span base methods treat entity recognition span classification task although methods innate ability handle nest ner suffer high computational cost ignorance boundary information utilization span partially match entities difficulties long entity recognition tackle issue propose two stage entity identifier first generate span proposals filter boundary regression seed span locate entities label boundary adjust span proposals correspond categories method effectively utilize boundary information entities partially match span train boundary regression entities length cover theoretically improve ability recognize long entities addition many low quality seed span filter first stage reduce time complexity inference experiment nest ner datasets demonstrate propose method outperform previous state art model
task empathetic response generation aim generate syntactically correct importantly emotionally appropriate responses follow previous dialog turn exist model either directly incorporate pre define emotion information guide response generation use deterministic rule decide response emotion ignore subtle emotion interactions capture human conversations advent advance language model possible learn nuanced emotional exchange capture natural language dialogs fully explore range emotions dialog intents important curate dataset large enough would light general understand human emotional interactions conversations paper describe detail curation process large scale dialog dataset utterance label one thirty-two emotions nine intent categories show build multi turn empathetic dialog model perform well compare baselines six thousand human evaluate instance
deal navigation problem agent follow natural language instructions observe environment focus language understand show importance spatial semantics ground navigation instructions visual perceptions propose neural agent use elements spatial configurations investigate influence navigation agent reason ability moreover model sequential execution order align visual object spatial configurations instruction neural agent improve strong baselines see environments show competitive performance unseen environments additionally experimental result demonstrate explicit model spatial semantic elements instructions improve ground spatial reason model
scarcity parallel data cause formality style transfer model scarce success preserve content show fine tune pre train language gpt two sequence sequence bart model boost content preservation possible even limit amount parallel data augment model reward target style content two core aspects task achieve new state art
language model process syntactically complex sentence use abstract syntactic information present sentence manner consistent grammar english rely solely set heuristics propose method tackle question alterrep linguistic feature sentence alterrep allow us generate counterfactual representations alter feature encode leave aspects original representation intact measure change model word prediction counterfactual representations different sentence draw causal conclusions contexts model use linguistic feature apply method study bert use relative clause rc span information find bert use information rc span agreement prediction use linguistically correct strategy also find counterfactual representations generate specific rc subtype influence number prediction sentence rc subtypes suggest information rc boundaries encode abstractly bert representation
current abstractive summarization systems outperform extractive counterparts widespread adoption inhibit inherent lack interpretability achieve best worlds propose ease extractive abstractive framework evidence base text generation apply document summarization present explainable summarization system base information bottleneck principle jointly train extraction abstraction end end fashion inspire previous research humans use two stage framework summarize long document jing mckeown two thousand framework first extract pre define amount evidence span explanations generate summary use evidence use automatic human evaluations show explanations framework relevant simple baselines without substantially sacrifice quality generate summary
multiple study show transformers remarkably robust prune contrary receive wisdom demonstrate pre train transformer encoders surprisingly fragile removal small number feature layer output one model weight case bert pre train encoder transformers affect component scale factor bias layernorm outliers high magnitude normalization parameters emerge early pre train show consistently dimensional position throughout model show disable significantly degrade mlm loss downstream task performance effect observe across several bert family model popular pre train transformer architectures include bart xlnet electra also show similar effect gpt two
black box probe model reliably extract linguistic feature like tense number syntactic role pretrained word representations however manner feature encode representations remain poorly understand present systematic study linear geometry contextualized word representations elmo bert show variety linguistic feature include structure dependency relationships encode low dimensional subspaces refine geometric picture show hierarchical relations subspaces encode general linguistic categories specific ones low dimensional feature encode distribute rather align individual neurons finally demonstrate linear subspaces causally relate model behavior use perform fine grain manipulation bert output distribution
reason one major challenge human like ai recently attract intensive attention natural language process nlp researchers however cross modal reason need research cross modal reason observe methods fall shallow feature match without depth human like reasoningthe reason lie exist cross modal task directly ask question image however human reason real scenes often make specific background information process study abc theory social psychology propose share task name premise base multimodal reason pmr require participate model reason establish profound understand background information believe propose pmr would contribute help would light human like depth reason
uniform information density uid hypothesis posit speakers behave optimally tend distribute information uniformly across linguistic signal gain traction psycholinguistics explanation certain syntactic morphological prosodic choices work explore whether uid hypothesis operationalized inductive bias statistical language model specifically augment canonical mle objective train language model regularizer encode uid experiment ten languages span five language families find use uid regularization consistently improve perplexity language model larger effect train data limit moreover via analysis generate sequence find uid regularize language model desirable properties eg generate text lexically diverse result suggest uid reasonable inductive bias language model also provide alternative validation uid hypothesis use modern day nlp tool
lexicon information pre train model bert combine explore chinese sequence label task due respective strengths however exist methods solely fuse lexicon feature via shallow random initialize sequence layer integrate bottom layer bert paper propose lexicon enhance bert lebert chinese sequence label integrate external lexicon knowledge bert layer directly lexicon adapter layer compare exist methods model facilitate deep lexicon knowledge fusion lower layer bert experiment ten chinese datasets three task include name entity recognition word segmentation part speech tag show lebert achieve state art result
machine translation quality estimation qe task predict quality machine translations without rely reference recently predictor estimator framework train predictor feature extractor leverage extra parallel corpora without qe label achieve promise qe performance however argue gap predictor estimator data quality train objectives preclude qe model benefit large number parallel corpora directly propose novel framework call directqe provide direct pretraining qe task directqe generator train produce pseudo data closer real qe data detector pretrained data novel objectives akin qe task experiment widely use benchmarks show directqe outperform exist methods without use pretraining model bert also give extensive analyse show fix two gap contribute improvements
widespread use string solvers formal analysis string heavy program lead grow demand efficient reliable techniques apply context especially real world case design algorithm generally undecidable satisfiability problem systems string constraints require thorough understand structure constraints present target case paper investigate benchmarks present literature contain regular expression membership predicate extract different first order logic theories prove decidability resp undecidability notably common theories real world benchmarks pspace complete directly lead implementation efficient algorithm solve string constraints
despite achieve state art accuracy temporal order events neural model showcase significant gap performance work seek fill one gap leverage explore dimension textual semantics rich semantic information provide explicit textual time cue develop stage system consist novel temporal framework parser automatically extract time cue convert representations suitable integration neural model demonstrate utility extract cue integrate event order model use joint bilstm ilp constraint architecture outline functionality three part stage process approach show two methods integrate representations bilstm ilp model incorporate semantic cue additional feature ii generate new constraints semantic cue enforce ilp demonstrate promise result two event order datasets highlight important issue semantic cue representation integration future research
lack publicly available evaluation data low resource languages limit progress speak language understand slu key task like intent classification slot fill require abundant train data desirable reuse exist data high resource languages develop model low resource scenarios introduce xsid new benchmark cross lingual slot intent detection thirteen languages six language families include low resource dialect tackle challenge propose joint learn approach english slu train data non english auxiliary task raw text syntax translation transfer study two setups differ type language coverage pre train embeddings result show jointly learn main task mask language model effective slot machine translation transfer work best intent classification
recent years see rise interest cross lingual transfer languages similar typology languages various script however interplay language similarity difference script cross lingual transfer less study problem explore interplay cross lingual transfer two supervise task namely part speech tag sentiment analysis introduce newly annotate corpus algerian user generate comment comprise parallel annotations algerian write latin arabic code switch script well annotations sentiment topic categories perform baseline experiment fine tune multi lingual language model explore effect script vs language similarity cross lingual transfer fine tune multi lingual model languages typologically distinct use script b typologically similar use distinct script c typologically similar use script find delicate relationship script typology part speech sentiment analysis less sensitive
transformer language model show remarkable ability detect word anomalous context likelihood score offer information anomaly work use gaussian model density estimation intermediate layer three language model bert roberta xlnet evaluate method blimp grammaticality judgement benchmark lower layer surprisal highly correlate low token frequency correlation diminish upper layer next gather datasets morphosyntactic semantic commonsense anomalies psycholinguistic study find best perform model roberta exhibit surprisal earlier layer anomaly morphosyntactic semantic commonsense anomalies exhibit surprisal intermediate layer result suggest language model employ separate mechanisms detect different type linguistic anomalies
sign language translation slt often decompose video gloss recognition gloss text translation gloss sequence transcribe speak language word order sign focus gloss text translation treat low resource neural machine translation nmt problem however unlike traditional low resource nmt gloss text translation differ gloss text pair often higher lexical overlap lower syntactic overlap pair speak languages exploit lexical overlap handle syntactic divergence propose two rule base heuristics generate pseudo parallel gloss text pair monolingual speak language text pre train thus obtain synthetic data improve translation american sign language asl english german sign language dgs german three hundred and fourteen two hundred and twenty bleu respectively
argument mine achieve significant success classify argumentative relations statements support attack neutral limit computational understand logical mechanisms constitute relations recent study rely black box model linguistically insightful desire hand earlier study use rather simple lexical feature miss logical relations statements overcome limitations work classify argumentative relations base four logical theory inform mechanisms two statements namely factual consistency ii sentiment coherence iii causal relation iv normative relation demonstrate operationalization logical mechanisms classify argumentative relations without directly train data label relations significantly better several unsupervised baselines demonstrate mechanisms also improve supervise classifiers representation learn
quality estimation qe machine translation mt task estimate quality score give translation output unknown mt system however qe score low resource languages usually intractable hard collect paper focus sentence level qe share task fifth conference machine translation wmt20 challenge set aim predict qe score give translation output barely none qe score pair languages give train propose ensemble base predictor estimator qe model transfer learn overcome qe data scarcity challenge leverage qe score miscellaneous languages translation result target languages base evaluation result provide detail analysis extension affect qe model reliability generalization ability perform transfer learn multilingual task finally achieve best performance ensemble model combine model pretrained individual languages well different level parallel train corpus pearson correlation two hundred and ninety-eight two hundred and fifty-four time higher baselines
exist methods measure sentence similarity face two challenge one label datasets usually limit size make insufficient train supervise neural model two train test gap unsupervised language model lm base model compute semantic score sentence since sentence level semantics explicitly model train result inferior performances task work propose new framework address two issue propose framework base core idea mean sentence define contexts sentence similarity measure compare probabilities generate two sentence give context propose framework able generate high quality large scale dataset semantic similarity score two sentence unsupervised manner train test gap largely bridge extensive experiment show propose framework achieve significant performance boost exist baselines supervise unsupervised settings across different datasets
statutory reason task determine whether legal statute state natural language apply text description case prior work introduce resource approach statutory reason monolithic textual entailment problem neural baselines perform nearly chance address challenge decompose statutory reason four type language understand challenge problems introduction concepts structure find prolog program augment exist benchmark provide annotations four task baselines three model statutory reason show benefit additional structure improve prior baselines decomposition subtasks facilitate finer grain model diagnostics clearer incremental progress
many people aim change everyone succeed number social psychology theories propose motivation relate characteristics persist change computational study explore motivational stage personal change paper investigate new dataset consist write people manifest intention change persist others use variety linguistic analysis techniques first examine write pattern distinguish two group people persistent people tend reference topics relate long term self improvement use complicate write style draw consistent differences build classifier reliably identify people likely persist base language experiment provide new insights motivation relate behavior people persist intention change
paper write tutorial fine grain interpretation causation analysis deep nlp model present naacl two thousand and twenty-one present discuss research work interpret fine grain components model two perspectives fine grain interpretation ii causation analysis former introduce methods analyze individual neurons group neurons respect language property task latter study role neurons input feature explain decisions make model also discuss application neuron analysis network manipulation domain adaptation moreover present two toolkits namely neurox captum support functionalities discuss tutorial
one ways blind people understand surround click image rely descriptions generate image caption systems current work caption image visually impair use textual data present image generate caption problem critical many visual scenes contain text moreover twenty-one question ask blind people image click pertain text present work propose alter aoanet state art image caption model leverage text detect image input feature addition use pointer generator mechanism copy detect text caption tokens need reproduce accurately model outperform aoanet benchmark dataset vizwiz give thirty-five one hundred and sixty-two performance improvement cider spice score respectively
introduce share system hierarchical assistive recipe edit assist home cook dietary restrictions population serve exist cook resources hierarchical recipe editor make necessary substitutions recipe ingredients list write directions make use new ingredients introduce novel recipepairs dataset 84k pair similar recipes one recipe satisfy one seven dietary constraints allow supervise train recipe edit model experiment dataset demonstrate system produce convince coherent recipes appropriate target dietary constraint contain prohibit ingredients show challenge task adequately solve human write ingredient substitution rule straightforward adaptation state art model recipe generation demonstrate human evaluations real world cook trials recipes edit system easily follow home cook create delicious satisfactory dish
many type text style transfer achieve small precise edit eg sentiment transfer terrible time great time propose coarse fine editor style transfer transform text use levenshtein edit operations eg insert replace delete unlike prior single span edit methods method concurrently edit multiple span source text train without parallel style text pair eg pair sentiment statements propose unsupervised data synthesis procedure first convert text style agnostic templates use style classifier attention eg slot time fill slot templates use fine tune pretrained language model method outperform exist generation edit style transfer methods sentiment yelp amazon politeness polite transfer particular multi span edit achieve higher performance diverse output single span edit moreover compare previous methods unsupervised data synthesis method result higher quality parallel style pair improve model performance
majority available text summarization datasets include short form source document lack long range causal temporal dependencies often contain strong layout stylistic bias relevant datasets offer limit challenge future generations text summarization systems address issue introduce booksum collection datasets long form narrative summarization dataset cover source document literature domain novels play stories include highly abstractive human write summaries three level granularity increase difficulty paragraph chapter book level domain structure dataset pose unique set challenge summarization systems include process long document non trivial causal temporal dependencies rich discourse structure facilitate future work train evaluate multiple extractive abstractive summarization model baselines dataset
distantly supervise relation extraction draw significant attention recently however almost prior work ignore fact sentence appearance order two entities contribute understand semantics furthermore leverage relation hierarchies fully exploit heuristic effect relation level ie higher level relations give useful information lower ones paper design novel recursive hierarchy interactive attention network rhia use hierarchical structure relation model interactive information relation level handle long tail relations generate relation augment sentence representations along hierarchical relation chain recursive structure besides introduce newfangled train objective call entity order perception eop make sentence encoder retain entity appearance information substantial experiment popular new york time nyt dataset conduct compare prior baselines approach achieve state art performance term precision recall p r curve auc top n precision evaluation metrics
commodity news contain wealth information sum mary recent commodity price movement notable events lead tothe movement event extraction useful information extract fromcommodity news extremely useful mine causal relation betweenevents commodity price movement use commodity priceprediction facilitate future research introduce new dataset withthe follow information identify annotate entities nomi nal name ii events trigger word argument roles iii eventmetadata modality polarity intensity iv event event relations
recent years witness great progress build emotional chatbots tremendous methods propose chatbots generate responses give emotions however emotion change user conversation fully explore work study problem positive emotion elicitation aim generate responses elicit positive emotion user human machine conversation propose weakly supervise emotion elicit machine eem address problem specifically first collect weak label user emotion status change conversion base pre train emotion classifier propose dual encoder decoder structure model generation responses positive negative side base change user emotion status conversation emotion elicit factor introduce top dual structure balance positive negative emotional impact generate response emotion elicitation factor also provide fine grain control manner emotion elicitation experimental result large real world dataset show eem outperform exist model generate responses positive emotion elicitation
chit chat base conversational recommendation systems crs provide item recommendations users natural language interactions better understand user intentions external knowledge graph kg introduce chit chat base crs however exist chit chat base crs usually generate repetitive item recommendations properly infuse knowledge kg crs generate informative responses remedy issue first reformulate conversational recommendation task highlight recommend items new possibly interest users propose knowledge enrich conversational recommendation system kecrs specifically develop bag entity boe loss infusion loss better integrate kg crs generate diverse informative responses boe loss provide additional supervision signal guide crs learn human write utterances kg infusion loss bridge gap word embeddings entity embeddings minimize distance word two embeddings moreover facilitate study construct high quality kg ie movie domain knowledge graph tmdkg experimental result large scale dataset demonstrate kecrs outperform state art chit chat base crs term recommendation accuracy response generation quality
capacity empathy crucial success open domain dialog systems due nature multi dimensionality various factor relate empathy expression communication mechanism dialog act emotion however exist methods empathetic response generation usually either consider one empathy factor ignore hierarchical relationships different factor lead weak ability empathy model paper propose multi factor hierarchical framework comae empathetic response generation model three key factor empathy expression hierarchical way show experimentally comae base model generate empathetic responses previous methods also highlight importance hierarchical model different factor empirical analysis real life corpus extensive experiment cod use data available https githubcom chujiezheng comae
relation classification aim predict relation two entities sentence exist methods regard relations candidate relations two entities sentence methods neglect restrictions candidate relations entity type lead inappropriate relations candidate relations paper propose novel paradigm relation classification entity type restriction recent exploit entity type restrict candidate relations specially mutual restrictions relations entity type formalize introduce relation classification besides propose paradigm recent model agnostic base two representative model gcn spanbert respectively recentgcn recentspanbert train recent experimental result standard dataset indicate recent improve performance gcn spanbert sixty-nine forty-four f1 point respectively especially recentspanbert achieve new state art tacred
continual lifelong learn long stand challenge machine learn date especially natural language process nlp although state art language model bert usher new era field due outstanding performance multitask learn scenarios suffer forget expose continuous stream data shift data distributions paper introduce drill novel continual learn architecture open domain text classification drill leverage biologically inspire self organize neural architecture selectively gate latent language representations bert task incremental manner demonstrate experiment drill outperform current methods realistic scenario imbalanced non stationary data without prior knowledge task boundaries best knowledge drill first kind use self organize neural architecture open domain lifelong learn nlp
well know typical word embed methods word2vec glove property mean compose add embeddings additive compositionality several theories propose explain additive compositionality follow question remain unanswered q1 assumptions theories hold practical word embed q2 ordinary additive compositionality see operation word mean well understand operations compute embeddings address issue idea frequency weight center core paper propose post process method bridge gap practical word embed assumption theory additive compositionality answer q1 also give method take mean linear operation word embed answer q2 moreover confirm experimentally accuracy operation ie ordinary additive compositionality improve post process method 35x improvement top one hundred accuracy operations perform correctly
deep learn model natural language process nlp inherently complex often view black box nature paper develop approach interpret convolutional neural network text classification problems exploit local linear model inherent relu dnns cnn model combine word embed convolutional layer filter use max pool optimize use relu dnn classification get overall self interpretable model system local linear model relu dnn map back max pool filter appropriate n grams result experimental datasets demonstrate propose technique produce parsimonious model self interpretable comparable performance respect complex cnn model also study impact complexity convolutional layer classification layer model performance
paper describe team lcp rit submission semeval two thousand and twenty-one task one lexical complexity prediction lcp task organizers provide participants augment version complex shardlow et al two thousand and twenty english multi domain dataset word context annotate respect complexity use five point likert scale system use logistic regression wide range linguistic feature eg psycholinguistic feature n grams word frequency pos tag predict complexity single word dataset analyze impact different linguistic feature classification performance evaluate result term mean absolute error mean square error pearson correlation spearman correlation
describe model focus understudy problem translate monolingual code mix language pair specifically offer wide range model convert monolingual english text hinglish code mix hindi english give recent success pretrained language model also test utility two recent transformer base encoder decoder model ie mt5 mbart task find work well give paucity train data code mix also propose dependency free method generate code mix texts bilingual distribute representations exploit improve language model performance particular arm additional data adopt curriculum learn approach first finetune language model synthetic data gold code mix data find although simple synthetic code mix method competitive case even superior several standard methods backtranslation method base equivalence constraint theory diverse set condition work show mt5 model finetuned follow curriculum learn procedure achieve best translation performance one thousand, two hundred and sixty-seven bleu model place first overall rank english hinglish official share task
attention matrix transformer self attention sublayer provably decompose two components one effective attention contribute model output lead us ask whether visualize effective attention give different conclusions interpretation standard attention use subset glue task bert carry analysis compare two attention matrices show interpretations differ effective attention less associate feature relate language model pretraining separator token potential illustrate linguistic feature capture model solve end task give find differences recommend use effective attention study transformer behavior since pertinent model output design
name entity recognition ner widely study task natural language process recently grow number study focus nest ner span base methods consider entity recognition span classification task deal nest entities naturally suffer huge search space lack interactions entities address issue propose novel sequence set neural network nest ner instead specify candidate span advance provide fix set learnable vectors learn pattern valuable span utilize non autoregressive decoder predict final set entities one pass able capture dependencies entities compare sequence sequence method model suitable unordered recognition task insensitive label order addition utilize loss function base bipartite match compute overall train loss experimental result show propose model achieve state art three nest ner corpora ace two thousand and four ace two thousand and five kbp two thousand and seventeen
automatic metrics essential develop natural language generation nlg model particularly open end language generation task story generation however exist automatic metrics observe correlate poorly human evaluation lack standardize benchmark datasets make difficult fully evaluate capabilities metric fairly compare different metrics therefore propose openmeva benchmark evaluate open end story generation metrics openmeva provide comprehensive test suite assess capabilities metrics include correlation human judgments b generalization different model output datasets c ability judge story coherence robustness perturbations end openmeva include manually annotate stories auto construct test examples evaluate exist metrics openmeva observe poor correlation human judgments fail recognize discourse level incoherence lack inferential knowledge eg causal order events generalization ability robustness study present insights develop nlg model metrics research
predict answer product relate question emerge field research recently attract lot attention answer subjective opinion base question challenge due dependency customer generate content previous work mostly focus review aware answer prediction however approach fail new unpopular products review hand work propose novel complementary approach predict answer question base answer similar question ask similar products measure contextual similarity products base answer provide question mixture expert framework use predict answer aggregate answer contextually similar products empirical result demonstrate model outperform strong baselines segment question namely roughly ten similar resolve question corpus additionally publish two large scale datasets use work one similar product question pair second product question answer pair
generate long coherent text important challenge task particularly open end language generation task story generation despite success model intra sentence coherence exist generation model eg bart still struggle maintain coherent event sequence throughout generate text conjecture difficulty decoder capture high level semantics discourse structure context beyond token level co occurrence paper propose long text generation model represent prefix sentence sentence level discourse level decode process end propose two pretraining objectives learn representations predict inter sentence semantic similarity distinguish normal shuffle sentence order extensive experiment show model generate coherent texts state art baselines
recent years knowledge graph completion methods extensively study graph embed approach learn low dimensional representations entities relations predict miss facts model usually view relation vector translation transe rotation rotate quate entity pair enjoy advantage simplicity efficiency however quate two main problems one model capture ability representation feature interaction entities relations relatively weak rely rigorous calculation three embed vectors two although model handle various relation pattern include symmetry anti symmetry inversion composition map properties relations consider one many many one many many paper propose novel model quatde dynamic map strategy explicitly capture variety relational pattern enhance feature interaction capability elements triplet model rely three extra vectors donate subject transfer vector object transfer vector relation transfer vector map strategy dynamically select transition vectors associate triplet use adjust point position entity embed vectors quaternion space via hamilton product experiment result show quatde achieve state art performance three well establish knowledge graph completion benchmarks particular mr evaluation relatively increase twenty-six wn18 fifteen wn18rr prove generalization quatde
development vietnamese language process general machine read comprehension particular attract great attention research community recent years datasets machine read comprehension task vietnamese large size uit viquad uit vinewsqa however datasets diverse answer serve research paper introduce uit viwikiqa first dataset evaluate sentence extraction base machine read comprehension vietnamese language uit viwikiqa dataset convert uit viquad dataset consist comprise twenty-three thousand and seventy-four question answer base five thousand, one hundred and nine passages one hundred and seventy-four vietnamese article wikipedia propose conversion algorithm create dataset sentence extraction base machine read comprehension three type approach sentence extraction base machine read comprehension vietnamese experiment show best machine model xlm rlarge achieve exact match score eight thousand, five hundred and ninety-seven f1 score eight thousand, eight hundred and seventy-seven dataset besides analyze experimental result term question type vietnamese effect context performance mrc model thereby show challenge uit viwikiqa dataset propose natural language process community
deep neural network bert make great progress relation classification although achieve good performance still question concern whether model recognize directionality relations especially may lack interpretability explore question novel evaluation task call relation direction recognition rdr propose explore whether model learn directionality relations three metrics rdr introduce measure degree model recognize directionality relations several state art model evaluate rdr experimental result real world dataset indicate clear gap among recognize directionality relations even though model obtain similar performance traditional metric eg macro f1 finally suggestions discuss enhance model recognize directionality relations perspective model design train
persona function prior knowledge maintain consistency dialogue systems previous study adopt self persona dialogue whose response select set candidates directly generate notice role partner dialogue paper make attempt thoroughly explore impact utilize personas describe either self partner speakers task response selection retrieval base chatbots four persona fusion strategies design assume personas interact contexts responses different ways strategies implement three representative model response selection base hierarchical recurrent encoder hre interactive match network imn bidirectional encoder representations transformers bert respectively empirical study persona chat dataset show partner personas neglect previous study improve accuracy response selection imn bert base model besides bert base model implement context response aware persona fusion strategy outperform previous methods margins larger twenty-seven original personas forty-six revise personas term hits1 top one accuracy achieve new state art performance persona chat dataset
automatic essay score aes define computer technology evaluate score write essay aim provide computational model grade essay either automatically minimal human involvement several aes study variety languages focus portuguese language main reason lack corpus manually grade essay order bridge gap create large corpus several essay write brazilian high school students online platform essay argumentative score across five competencies experts moreover conduct experiment create corpus show challenge pose portuguese language corpus publicly available https githubcom rafaelanchieta essay
paper describe system nlptea two thousand and twenty task chinese grammatical error diagnosis cged goal cged diagnose four type grammatical errors word selection redundant word r miss word disorder word w automatic cged system contain two part include error detection error correction system design solve error detection problem system build three model one bert base model leverage syntactic information two bert base model leverage contextual embeddings three lexicon base graph neural network leverage lexical information also design ensemble mechanism improve single model performance finally system achieve highest f1 score detection level identification level among team participate cged two thousand and twenty task
automatic detection humor pose grand challenge natural language process transformer base systems recently achieve remarkable result task usually 1were evaluate setups serious vs humorous texts come entirely different source 2focused benchmarking performance without provide insights model work make progress respect train analyze transformer base humor recognition model recently introduce dataset consist minimal pair align sentence one serious humorous find although align dataset much harder previous datasets transformer base model recognize humorous sentence align pair high accuracy seventy-eight careful error analysis characterize easy vs hard instance finally analyze attention weight obtain important insights mechanisms transformers recognize humor remarkably find clear evidence one single attention head learn recognize word make test sentence humorous even without access information train time
curated wikipii automatically label dataset compose wikipedia biography page annotate personal information extraction although automatic annotation lead high degree label noise inexpensive process generate large volumes annotate document train bert base ner model wikipii show adequately large train dataset model significantly decrease cost manual information extraction despite high level label noise similar approach organizations leverage text mine techniques create customize annotate datasets historical data without share raw data human annotation also explore collaborative train ner model federate learn annotation noisy result suggest depend level trust ml operator volume available data distribute train effective way train personal information identifier privacy preserve manner research material available https githubcom ratmcu wikipiifed
recent time see increase use text chat communication social network smartphones particularly involve use hindi english code mix text contain word recognize english vocabulary work detect emotions mix data classify sentence human emotions angry fear happy sad use state art natural language process model compare performance dataset comprise sentence mix data dataset collect annotate source use train model
multilingual neural machine translation aim learn single translation model multiple languages jointly train model often suffer performance degradation rich resource language pair attribute degeneration parameter interference paper propose lass jointly train single unify multilingual mt model lass learn language specific sub network lass language pair counter parameter interference comprehensive experiment iwslt wmt datasets various transformer architectures show lass obtain gain thirty-six language pair twelve bleu besides lass show strong generalization performance easy extension new language pair zero shoot translationlass boost zero shoot translation average eighty-three bleu thirty language pair cod train model available https githubcom nlp playground lass
neural network approach apply computational morphology great success improve performance task large margin provide new perspectives model paper start brief introduction computational morphology follow review recent work computational morphology neural network approach provide overview area end analyze advantage problems neural network approach computational morphology point directions explore future research study
dependency parse crucial step towards deep language understand therefore widely demand numerous natural language process applications particular leave right top transition base algorithms rely pointer network among accurate approach perform dependency parse additionally observe top algorithm pointer network sequential decode improve implement hierarchical variant adequate model dependency structure consider develop bottom orient hierarchical pointer network leave right parser propose two novel transition base alternatives approach parse sentence right leave order variant outside empirically test propose neural architecture different algorithms wide variety languages outperform original approach practically set new state art result english chinese penn treebanks non contextualized bert base embeddings
empathetic response therapist key success clinical psychotherapy especially motivational interview previous work computational model empathy motivational interview focus offline session level assessment therapist empathy empathy capture efforts therapist make understand client perspective convey understand client position paper propose novel task turn level detection client need empathy concretely propose leverage pre train language model empathy relate general conversation corpora unique labeller detector framework labeller automatically annotate motivational interview conversation corpus empathy label train detector determine need therapist empathy also lay strategies extend detector additional input multi task setups improve detection explainability
paper describe system develop laboratoire analyse statistique des textes last lexical complexity prediction share task semeval two thousand and twenty-one propose system make lightgbm model feed feature obtain many word frequency list publish lexical norms psychometric data tackle specificity multi word task use bigram association measure despite contextual feature use sentence length system achieve honorable performance multi word task poorer single word task bigram association measure find useful limit extent
introduce korean language understand evaluation klue benchmark klue collection eight korean natural language understand nlu task include topic classification semantictextual similarity natural language inference name entity recognition relation extraction dependency parse machine read comprehension dialogue state track build task scratch diverse source corpora respect copyright ensure accessibility anyone without restrictions ethical considerations mind carefully design annotation protocols along benchmark task data provide suitable evaluation metrics fine tune recipes pretrained language model task furthermore release pretrained language model plm klue bert klue roberta help reproduce baseline model klue thereby facilitate future research make interest observations preliminary experiment use propose klue benchmark suite already demonstrate usefulness new benchmark suite first find klue roberta large outperform baselines include multilingual plms exist open source korean plms second see minimal degradation performance even replace personally identifiable information pretraining corpus suggest privacy nlu capability odds lastly find use bpe tokenization combination morpheme level pre tokenization effective task involve morpheme level tag detection generation addition accelerate korean nlp research comprehensive documentation create klue facilitate create similar resources languages future klue available url
describe work information extraction medical document write german especially detect negations use architecture base uima pipeline base previous work software modules cover medical concepts like diagnose examinations etc employ version negex regular expression algorithm large set trigger baseline show significantly smaller trigger set sufficient achieve similar result order reduce adaptation time new text type elaborate question whether dependency parse base stanford corenlp model good alternative describe potentials shortcomings approach
constituent dependency parse two classic form syntactic parse find benefit joint train decode uniform formalism head drive phrase structure grammar hpsg however decode unify grammar higher time complexity ofn5 decode either form individually ofn3 since factor consider decode thus propose improve head scorer help achieve novel performance preserve parser ofn3 time complexity furthermore basis propose practical hpsg parser investigate strengths hpsg base parse explore general method train hpsg base parser constituent dependency annotations multilingual scenario thus present effective depth general work hpsg parse
recent advance computational cognitive science ie simulation base probabilistic program pave way significant progress formal implementable model pragmatics rather describe pragmatic reason process prose model formalize implement one derive qualitative quantitative predictions human behavior predictions consistently prove correct demonstrate viability value framework current paper provide practical introduction critical assessment bayesian rational speech act model framework unpack theoretical foundations explore technological innovations draw connections issue beyond current applications
sarcasm detection humor classification inherently subtle problems primarily due dependence contextual non verbal information furthermore exist study two topics usually constrain non english languages hindi due unavailability qualitative annotate datasets work make two major contributions consider limitations one develop hindi english code mix dataset masac multi modal sarcasm detection humor classification conversational dialog knowledge first dataset kind two propose msh comics novel attention rich neural architecture utterance classification learn efficient utterance representation utilize hierarchical attention mechanism attend small portion input sentence time incorporate dialog level contextual attention mechanism leverage dialog history multi modal classification perform extensive experiment task vary multi modal input various submodules msh comics also conduct comparative analysis exist approach observe msh comics attain superior performance exist model one f1 score point sarcasm detection ten f1 score point humor classification diagnose model perform thorough analysis result understand superiority pitfalls
work introduce asq tool automatically mine question answer sentence use abstract mean representation amr previous work make case use question answer pair specify predicate argument structure sentence use natural language require linguistic expertise train result creation datasets qa srl qamr question answer pair annotations crowdsourced approach end goal automatic make faster cost effective without compromise quality validity question answer pair thus obtain qualitative evaluation output generate asq amr twenty data show question answer pair natural valid demonstrate good coverage content run asq sentence qamr dataset observe semantic roles qamr also capture asqwe intend make tool result publicly available others use build upon
span base joint extraction simultaneously conduct name entity recognition ner relation extraction text span form recent study show token label convey crucial task specific information enrich token semantics however far know due completely abstain sequence tag mechanism prior span base work fail use token label formation solve problem pro pose sequence tag enhance span base network stsn span base joint extrac tion network enhance token bio label information derive sequence tag ging base ner stack multiple atten tion layer depth design deep neu ral architecture build stsn atten tion layer consist three basic attention units deep neural architecture first learn seman tic representations token label span base joint extraction construct formation interactions also realize bidirectional information interac tions span base ner fur thermore extend bio tag scheme make stsn extract overlap en tity experiment three benchmark datasets show model consistently outperform previous optimal model large margin create new state art result
modern transformer base neural architectures yield impressive result nearly every nlp task word sense disambiguation problem discern correct sense word give context exception state art approach wsd today leverage lexical information along pre train embeddings model achieve result comparable human inter annotator agreement standard evaluation benchmarks vein experiment several strategies optimize bi encoders specific task propose alternative methods present lexical information model multi stage pre train fine tune pipeline state art word sense disambiguation
propose novel approach summarization base bayesian deep learn approximate bayesian summary generation first extend state art summarization model monte carlo dropout use perform multiple stochastic forward pass method allow us improve summarization performance simply use median multiple stochastic summaries show variational equivalents bart pegasus outperform deterministic counterparts multiple benchmark datasets addition rely bayesian inference measure uncertainty model generate summaries reliable uncertainty measure improve experience end user filter generate summaries high uncertainty furthermore propose metric could use criterion select sample annotation pair nicely active learn human loop approach
although neural model achieve competitive result dialogue systems show limit ability represent core semantics ignore important entities end exploit abstract mean representation amr help dialogue model compare textual input amr explicitly provide core semantic knowledge reduce data sparsity develop algorithm construct dialogue level amr graph sentence level amrs explore two ways incorporate amrs dialogue systems experimental result dialogue understand response generation task show superiority model knowledge first leverage formal semantic representation neural dialogue model
personalize conversation model pcms generate responses accord speaker preferences exist personalize conversation task typically require model extract speaker preferences user descriptions conversation histories scarce newcomers inactive users paper propose shoot personalize conversation task auxiliary social network task require model generate personalize responses speaker give conversations speaker social network exist methods mainly design incorporate descriptions conversation histories methods hardly model speakers conversations connections speakers better cater newcomers resources propose personalize conversation model pcm learn adapt new speakers well enable new speakers learn resource rich speakers particularly base meta learn base pcm propose task aggregator ta collect speakers information social network ta provide prior knowledge new speaker meta learn experimental result show methods outperform baselines appropriateness diversity consistency speakers
exist model multilingual sentence embeddings require large parallel data resources available low resource languages propose novel unsupervised method derive multilingual sentence embeddings rely monolingual data first produce synthetic parallel corpus use unsupervised machine translation use fine tune pretrained cross lingual mask language model xlm derive multilingual sentence representations quality representations evaluate two parallel corpus mine task improvements twenty-two f1 point vanilla xlm addition observe single synthetic bilingual corpus able improve result language pair
ontologies prove beneficial different settings make use textual review however manually construct ontologies laborious time consume process need automation propose novel methodology automatically extract ontologies form meronomies product review use limit amount hand annotate train data show ontologies generate method outperform hand craft ontologies wordnet ontologies extract exist methods text2onto comet several diverse settings specifically generate ontologies outperform others evaluate human annotators well exist qanda dataset amazon moreover method better able generalise capture knowledge unseen products finally consider real world set show method better able determine recommend products base review alternative use amazon standard score aggregations
pre train language model achieve human level performance many machine read comprehension mrc task remain unclear whether model truly understand language answer question exploit statistical bias datasets demonstrate simple yet effective method attack mrc model reveal statistical bias model apply method race dataset answer mrc question select four options find several pre train language model include bert albert roberta show consistent preference options even options irrelevant question interfere irrelevant options performance mrc model reduce human level performance chance level performance human readers however clearly affect irrelevant options finally propose augment train method greatly reduce model statistical bias
present context preserve text simplification ts approach recursively split rephrase complex english sentence semantic hierarchy simplify sentence use set linguistically principled transformation pattern input sentence convert hierarchical representation form core sentence accompany contexts link via rhetorical relations hence oppose previously propose sentence split approach commonly take account discourse level aspects ts approach preserve semantic relationship decompose constituents output comparative analysis annotations contain rst dt show able capture contextual hierarchy split sentence precision eighty-nine reach average precision sixty-nine classification rhetorical relations hold
human evaluation become necessity test performance chatbots however shelf settings suffer severe reliability replication issue partly extremely high diversity criteria high time come standard criteria exact definitions end conduct investigation one hundred and five paper involve human evaluation chatbots derive propose five standard criteria along precise definitions
large pre train language model achieve state art result fine tune downstream nlp task however almost exclusively focus text representation neglect cell level layout information important form image understand paper propose new pre train approach structurallm jointly leverage cell layout information scan document specifically pre train structurallm two new design make interactions cell layout information one cell semantic unit two classification cell position pre train structurallm achieve new state art result different type downstream task include form understand seven thousand, eight hundred and ninety-five eight thousand, five hundred and fourteen document visual question answer seven thousand, two hundred and fifty-nine eight thousand, three hundred and ninety-four document image classification nine thousand, four hundred and forty-three nine thousand, six hundred and eight
de identification task detect privacy relate entities text person name email contact data well study within medical domain need de identification technology increase privacy preserve data handle high demand many domains paper focus job post present jobstack new corpus de identification personal data job vacancies stackoverflow introduce baselines compare long short term memory lstm transformer model improve upon baselines experiment contextualized embeddings distantly relate auxiliary data via multi task learn result show auxiliary data improve de identification performance surprisingly vanilla bert turn effective bert model train portion stackoverflow
label noise long tail distributions two major challenge distantly supervise relation extraction recent study show great progress denoising pay little attention problem long tail relations paper introduce constraint graph model dependencies relation label top propose novel constraint graph base relation extraction frameworkcgre handle two challenge simultaneously cgre employ graph convolution network gcns propagate information data rich relation nod data poor relation nod thus boost representation learn long tail relations improve noise immunity constraint aware attention module design cgre integrate constraint information experimental result widely use benchmark dataset indicate approach achieve significant improvements previous methods denoising long tail relation extraction
cross lingual text classification aim train classifier source language transfer knowledge target languages useful low resource languages recent multilingual pretrained language model mplm achieve impressive result cross lingual classification task rarely consider factor beyond semantic similarity cause performance degradation language pair paper propose simple yet effective method incorporate heterogeneous information within across languages cross lingual text classification use graph convolutional network gcn particular construct heterogeneous graph treat document word nod link nod different relations include part speech roles semantic similarity document translations extensive experiment show graph base method significantly outperform state art model task also achieve consistent performance gain baselines low resource settings external tool like translators unavailable
fine tune pre train language model plms achieve awesome performance almost nlp task use additional prompt fine tune plms stimulate rich knowledge distribute plms better serve downstream task prompt tune achieve promise result class classification task sentiment classification natural language inference however manually design lot language prompt cumbersome fallible auto generate prompt also expensive time consume verify effectiveness non shoot scenarios hence challenge prompt tune address many class classification task end propose prompt tune rule ptr many class text classification apply logic rule construct prompt several sub prompt way ptr able encode prior knowledge class prompt tune conduct experiment relation classification typical many class classification task result benchmarks show ptr significantly consistently outperform exist state art baselines indicate ptr promise approach take advantage plms complicate classification task
propose text classification tool base support vector machine assessment organizational leadership style appear twitter users collect twitter data fifty-one days relate first thirty italian organizations two thousand and fifteen rank forbes global two thousand select five relevant volumes tweet analyze communication company leaders together dialogue among stakeholders company understand association perceive leadership style dimension assess leadership profile refer ten factor model develop barchiesi la bella two thousand and seven maintain distinctiveness approach propose allow rapid assessment perceive leadership capabilities enterprise emerge social media interactions also use show company respond manage communication specific events take place assess stakeholder reactions
virtual personal assistants like siri great potential developments hit fundamental problem make computational devices understand human speech natural language understand one disappoint failures ai research seem something computer scientists get nature language course philosophers linguists think quite differently language paper describe take ideas discipline implement background work take seriously notion language action look people actually language use techniques conversation analysis observation human communication behind scenes management social relations well foreground pass information claim one thing implement require mechanism mechanism describe base notion language intentional think intentionally talk recognise others cooperative compel help way compel point solution ever present problem keep human topic approach lead recent success significantly improve user satisfaction independent task completion talk markup language talkml draft alternative voicexml propose greatly simplify script interaction provide default behaviours input recognise speech events
paper introduce dan new multi domain corpus annotation guidelines danish nest name entities nes lexical normalization support research cross lingual cross domain learn less resourced language empirically assess three strategies model two layer name entity recognition ner task compare transfer capabilities german versus language annotation scratch examine language specific versus multilingual bert study effect lexical normalization ner result show one robust strategy multi task learn rival multi label decode two bert base ner model sensitive domain shift three language bert lexical normalization beneficial least canonical data result also show domain setup remain challenge performance news plateaus quickly highlight importance cross domain evaluation cross lingual transfer
present robeczech monolingual roberta language representation model train czech data roberta robustly optimize transformer base pretraining approach show robeczech considerably outperform equally size multilingual czech train contextualized language representation model surpass current state art five evaluate nlp task reach state theart result four robeczech model release publicly https hdlhandlenet eleven thousand, two hundred and thirty-four one three thousand, six hundred and ninety-one https huggingfaceco ufal robeczech base
last years significant developments area question answer knowledge graph kgqa despite notable advancements current kgqa datasets provide answer direct output result formal query rather full sentence incorporate question context achieve coherent answer sentence question vocabulary template base verbalization usually employ better representation answer turn require extensive expert intervention thus make way machine learn approach however scarcity datasets empower machine learn model area hence provide vanilla dataset aim reduce gap offer answer natural language sentence answer sentence dataset syntactically semantically closer question triple fact dataset consist 100k simple question adapt csqa simplequestionswikidata datasets generate use semi automatic framework also present result train dataset multiple baseline model adapt current state art natural language generation nlg architectures believe dataset allow researchers focus find suitable methodologies architectures answer verbalization
propose new architecture diacritics restoration base contextualized embeddings namely bert evaluate twelve languages diacritics furthermore conduct detail error analysis czech morphologically rich language high level diacritization notably manually annotate mispredictions show roughly forty-four actually errors either plausible variants nineteen system corrections erroneous data twenty-five finally categorize real errors detail release code https githubcom ufal bert diacritics restoration
exist pre train language model plms often computationally expensive inference make impractical various resource limit real world applications address issue propose dynamic token reduction approach accelerate plms inference name tr bert could flexibly adapt layer number token inference avoid redundant calculation specially tr bert formulate token reduction process multi step token selection problem automatically learn selection strategy via reinforcement learn experimental result several downstream nlp task show tr bert able speed bert two five time satisfy various performance demand moreover tr bert also achieve better performance less computation suite long text task since token level layer number adaption greatly accelerate self attention operation plms source code experiment detail paper obtain https githubcom thunlp tr bert
knowledge base question answer kbqa aim answer question knowledge base kb recently large number study focus semantically syntactically complicate question paper elaborately summarize typical challenge solutions complex kbqa begin introduce background kbqa task next present two mainstream categories methods complex kbqa namely semantic parse base sp base methods information retrieval base ir base methods review advance methods comprehensively perspective two categories specifically explicate solutions typical challenge finally conclude discuss promise directions future research
computer naturally interact human need human like paper propose neural response generation model multi task learn generation classification focus emotion model base bart lewis et al two thousand and twenty pre train transformer encoder decoder model train generate responses recognize emotions simultaneously furthermore weight losses task control update parameters automatic evaluations crowdsourced manual evaluations show propose model make generate responses emotionally aware
text generation receive lot attention computational argumentation research recent particularly challenge task generation counter arguments far approach primarily focus rebut give conclusion yet ways counter argument exist work go beyond previous research explore argument undermine counter argument attack one premise hypothesize identify argument weak premise key effective counter accordingly propose pipeline approach first assess premise strength generate counter argument target weak ones one hand manual automatic evaluation prove importance identify weak premise counter argument generation hand consider correctness content richness human annotators favor approach state art counter argument generation
knowledge retrieval reason two key stag multi hop question answer qa web scale exist approach suffer low confidence retrieve evidence facts fill knowledge gap lack transparent reason process paper propose new framework exploit valid facts obtain explainability multi hop qa dynamically construct semantic graph reason employ abstract mean representation amr semantic graph representation framework contain three new ideas tt amr sg amr base semantic graph construct candidate fact amrs uncover hop relations among question answer multiple facts b novel path base fact analytics approach exploit tt amr sg extract active facts large fact pool answer question c fact level relation model leverage graph convolution network gcn guide reason process result two scientific multi hop qa datasets show surpass recent approach include use additional knowledge graph maintain high explainability openbookqa achieve new state art result arc challenge computationally practicable set
current mode use electronic health record ehr elicit text redundancy clinicians often populate new document duplicate exist note update accordingly data duplication lead propagation errors inconsistencies misreporting care therefore quantify information redundancy play essential role evaluate innovations operate clinical narratives work quantitative examination information redundancy ehr note present evaluate two strategies measure redundancy information theoretic approach lexicosyntactic semantic model evaluate measure train large transformer base language model use clinical text large openly available us base icu dataset large multi site uk base trust compare information theoretic content train model open domain language model language model train use clinical text show 15x 3x less efficient open domain corpora manual evaluation show high correlation lexicosyntactic semantic redundancy average forty-three sixty-five
despite recent advance standard sequence label systems often fail process noisy user generate text consume output optical character recognition ocr process paper improve noise aware train method propose empirical error generation approach employ sequence sequence model train perform translation error free erroneous text use ocr engine generate large parallel text corpus train produce several real world noisy sequence label benchmarks evaluation moreover overcome data sparsity problem exacerbate case imperfect textual input learn noisy language model base embeddings approach outperform baseline noise generation error correction techniques erroneous sequence label data set facilitate future research robustness make code embeddings data conversion script publicly available
sentiment analysis sa important research area cognitive computation thus depth study pattern sentiment analysis necessary present rich resource data base sa well develop challenge practical multi source unsupervised sa ie target domain sa transfer multiple source domains seldom study challenge behind problem mainly locate lack supervision information semantic gap among domains ie domain shift loss knowledge however exist methods either lack distinguishable capacity semantic gap among domains lose private knowledge alleviate problems propose two stage domain adaptation framework first stage multi task methodology base share private architecture employ explicitly model domain common feature domain specific feature label source domains second stage two elaborate mechanisms embed share private architecture transfer knowledge multiple source domains first mechanism selective domain adaptation sda method transfer knowledge closest source domain second mechanism target orient ensemble toe method knowledge transfer well design ensemble method extensive experiment evaluations verify performance propose framework outperform unsupervised state art competitors conclude experiment transfer different distribute source domains may degrade target domain performance crucial choose proper source domains transfer
many people consider news article reliable source information current events however due range factor influence news agencies coverage may always impartial media bias slant news coverage substantial impact public perception events accordingly potentially alter beliefs view public main data gap current research media bias detection robust representative diverse dataset contain annotations bias word sentence particular exist datasets control individual background annotators may affect assessment thus represent critical information contextualizing annotations poster present matrix base methodology crowdsource data use self develop annotation platform also present mbic media bias include characteristics first sample one thousand, seven hundred statements represent various media bias instance statements review ten annotators contain label media bias identification word sentence level mbic first available dataset media bias report detail information annotator characteristics individual background current dataset already significantly extend exist data domain provide unique reliable insights perception bias future extend respect number article annotators per article
professional summaries write document level information theme document mind contrast seq2seq decoders simultaneously learn focus salient content decide generate decode step motivation narrow gap introduce focus attention mechanism simple yet effective method encourage decoders proactively generate tokens similar topical input document propose focus sample method enable generation diverse summaries area currently understudy summarization evaluate bbc extreme summarization task two state art model augment focus attention generate summaries closer target faithful input document outperform vanilla counterparts rouge multiple faithfulness measure also empirically demonstrate focus sample effective generate diverse faithful summaries top k nucleus sample base decode methods
speakers communicate influence partner beliefs shape action belief action base objectives explore independently recent computational model challenge explicitly compare integrate indeed find conflate standard referential communication task distinguish account introduce new paradigm call signal bandits generalize classic lewis signal game multi arm bandit set target context relative value develop three speaker model belief orient speaker purely informative objective action orient speaker instrumental objective combine speaker integrate two induce listener beliefs generally lead desirable action present series simulations demonstrate ground production choices future listener action result relevance effect flexible use nonliteral language broadly find suggest language game base richer decision problems promise avenue insight rational communication
abstractive summarization long document multi document remain challenge seq2seq architecture seq2seq good analyze long distance relations text paper present bass novel framework boost abstractive summarization base unify semantic graph aggregate co referent phrase distribute across long range context convey rich relations phrase graph base encoder decoder model propose improve document representation summary generation process leverage graph structure specifically several graph augmentation methods design encode explicit implicit relations text graph propagation attention mechanism develop decoder select salient content summary empirical result show propose architecture bring substantial improvements long document multi document summarization task
present intellicat interactive translation interface neural model streamline post edit process machine translation output leverage two quality estimation qe model different granularities sentence level qe predict quality machine translate sentence word level qe locate part machine translate sentence need correction additionally introduce novel translation suggestion model condition leave right contexts provide alternatives specific word phrase correction finally word alignments intellicat automatically preserve original document style translate document experimental result show post edit base propose qe translation suggestions significantly improve translation quality furthermore user study reveal three feature provide intellicat significantly accelerate post edit task achieve five hundred and twenty-nine speedup translation time compare translate scratch interface publicly available https intellicatberinglabcom
natural language process nlp task text classification name entity recognition etc see revolutionary improvements last years due language model bert achieve deep knowledge transfer use large pre train model fine tune model specific task bert architecture show even better performance domain specific task model pre train use domain relevant texts inspire recent advancements develop nukelm nuclear domain language model pre train fifteen million abstract yous department energy office scientific technical information osti database nukelm model fine tune classification research article either binary class relate nuclear fuel cycle nfc multiple categories relate subject article show continue pre train bert style architecture prior fine tune yield greater performance article classification task information critical properly triaging manuscripts necessary task better understand citation network publish nuclear space uncover new areas research nuclear nuclear relevant domains
great progress make unsupervised bilingual lexicon induction ubli align source target word embeddings independently train monolingual corpora common assumption ubli model embed space two languages approximately isomorphic therefore performance bind degree isomorphism especially etymologically typologically distant languages address problem propose transformation base method increase isomorphism embeddings two languages make match rotate scale method require form supervision apply language pair benchmark data set bilingual lexicon induction approach achieve competitive superior performance compare state art methods particularly strong result find distant languages
previous study show effective pre train language model sentiment analysis however study ignore importance sentimental information pre train modelstherefore fully investigate sentimental information pre train model enhance pre train language model semantic graph sentiment analysisin particular introduce semantic graph base pre trainingsgpt use semantic graph obtain synonym knowledge aspect sentiment pair similar aspect sentiment termswe optimize pre train language model semantic graphsempirical study several downstream task show propose model outperform strong pre train baselines result also show effectiveness propose semantic graph pre train model
chinese spell check csc aim detect correct erroneous character user generate text chinese language chinese spell errors misuse semantically phonetically graphically similar character previous attempt notice phenomenon try use similarity task however methods use either heuristics handcraft confusion set predict correct character paper propose chinese spell checker call realise directly leverage multimodal information chinese character realise model tackle csc task one capture semantic phonetic graphic information input character two selectively mix information modalities predict correct output experiment sighan benchmarks show propose model outperform strong baselines large margin
work propose mask noun phrase prediction mnpp pre train strategy tackle pronoun resolution fully unsupervised set firstly evaluate pre train model various pronoun resolution datasets without finetuning method outperform previous unsupervised methods datasets large margins secondly proceed shoot set finetune pre train model winogrande xs separately method outperform roberta large baseline large margins meanwhile achieve higher auc score finetuning remain three official split winogrande
increase use dialogue agents make extremely desirable understand acknowledge imply emotions respond like humans empathy chatbots use traditional techniques analyze emotions base context mean text lack understand emotions express face emojis represent facial expressions present promise way express emotions however none ai systems utilize emojis empathetic conversation generation propose sentemojibot base sentemoji dataset generate empathetic conversations combination emojis text evaluation metrics show bert base model outperform vanilla transformer model user study indicate dialogues generate model understandable add emojis improve empathetic traits conversations ninety-eight
since traditional tokenizers isolate downstream task model output appropriate tokenization depend task model although recent study imply appropriate tokenization improve performance paper propose novel method find appropriate tokenization give downstream model jointly optimize tokenizer model propose method restriction except use loss value compute downstream model train tokenizer thus apply propose method nlp task moreover propose method use explore appropriate tokenization already train model post process therefore propose method applicable various situations evaluate whether method contribute improve performance text classification three languages machine translation eight language pair experimental result show propose method improve performance determine appropriate tokenizations
train neural model morphological analysis generation lemmatization morphologically rich languages present method automatically extract substantially large amount train data fsts twenty-two languages seventeen endanger neural model follow tagset fsts order make possible use fallback systems together fsts source code model datasets release zenodo
estimate expect output quality generation systems central nlg paper qualify notion automatic metrics good humans estimate system level quality statistically humans unbiased high variance estimators metrics bias low variance estimators compare estimators error pairwise prediction generation system better use bootstrap measure error complicate predictions evaluate noisy human predict label instead grind truth metric predictions fluctuate base test set calculate apply bias variance noise decomposition adjust error noise free infinite test set set analysis compare adjust error metrics humans derive perfect segment level annotator unbiased estimators dependent number judgments collect mt identify two settings metrics outperform humans due statistical advantage variance number human judgments use small quality difference compare systems small data code reproduce analyse available https githubcom johntzwei metric statistical advantage
recently token level adaptive train achieve promise improvement machine translation cross entropy loss function adjust assign different train weight different tokens order alleviate token imbalance problem however previous approach use static word frequency information target language without consider source language insufficient bilingual task like machine translation paper propose novel bilingual mutual information bmi base adaptive objective measure learn difficulty target token perspective bilingualism assign adaptive weight accordingly improve token level adaptive train method assign larger train weight tokens higher bmi easy tokens update coarse granularity difficult tokens update fine granularity experimental result wmt14 english german wmt19 chinese english demonstrate superiority approach compare transformer baseline previous token level adaptive train approach analyse confirm method improve lexical diversity
deception detection task many applications direct physical computer mediate communication focus automatic deception detection text across culture view culture prism individualism collectivism dimension approximate culture use country proxy start point recent conclusions draw social psychology discipline explore differences usage specific linguistic feature deception across culture confirm attribute norms respect individualism collectivism divide also investigate universal feature set cross cultural text deception detection task exist evaluate predictive power different feature set approach create culture language aware classifiers experiment wide range n gram feature base phonology morphology syntax linguistic cue like word phoneme count pronouns use etc token embeddings conduct experiment eleven datasets five languages ie english dutch russian spanish romanian six countries us belgium india russia mexico romania apply two classification methods ie logistic regression fine tune bert model result show task fairly complex demand indications linguistic cue deception cultural origins consistent context diverse domains dataset settings language evident usage pronouns expression sentiment deceptive language result work show automatic deception detection across culture languages handle unify manner approach augment knowledge cultural differences domains interest
current dialogue summarization systems usually encode text number general semantic feature eg keywords topics gain powerful dialogue model capabilities however feature obtain via open domain toolkits dialog agnostic heavily rely human annotations paper show dialogpt pre train model conversational response generation develop unsupervised dialogue annotator take advantage dialogue background knowledge encode dialogpt apply dialogpt label three type feature two dialogue summarization datasets samsum ami employ pre train non pre train model summarize experimental result show propose method obtain remarkable improvements datasets achieve new state art performance samsum dataset
parse speak dialogue pose unique difficulties include disfluencies unmarked boundaries sentence like units previous work show prosody help parse disfluent speech tran et al two thousand and eighteen assume input parser already segment sentence like units sus true exist speech applications investigate prosody affect parser receive entire dialogue turn input turn base model instead gold standard pre segment sus su base model experiment english switchboard corpus find use transcripts alone turn base model trouble segment sus lead worse parse performance su base model however prosody effectively replace gold standard su boundaries prosody turn base model perform well su base model nine thousand and seventy-nine vs nine thousand and sixty-five f1 score respectively despite perform two task su segmentation parse rather one parse alone analysis show pitch intensity feature important corpus since allow model correctly distinguish su boundary speech disfluency distinction model otherwise struggle make
propose new dataset texrel playground study emergent communications particular relations comparison relations datasets texrel provide rapid train experimentation whilst sufficiently large avoid overfitting context emergent communications comparison use symbolic input texrel provide realistic alternative whilst remain efficient fast learn compare performance texrel relate relations dataset shapeworld provide baseline performance result texrel sender architectures receiver architectures end end architectures examine effect multitask learn context shape color relations accuracy topological similarity cluster precision investigate whether increase size latent mean space improve metrics compositionality carry case study use texrel reproduce result experiment recent paper use symbolic input use non symbolic input texrel instead
study problem learn name entity recognition ner tagger use noisy label multiple weak supervision source though cheap obtain label weak supervision source often incomplete inaccurate contradictory make difficult learn accurate ner model address challenge propose conditional hide markov model chmm effectively infer true label multi source noisy label unsupervised way chmm enhance classic hide markov model contextual representation power pre train language model specifically chmm learn token wise transition emission probabilities bert embeddings input tokens infer latent true label noisy observations refine chmm alternate train approach chmm alt fine tune bert ner model label infer chmm bert ner output regard additional weak source train chmm return experiment four ner benchmarks various domains show method outperform state art weakly supervise ner model wide margins
non autoregressive nar model show great promise machine translation use limit dependence knowledge distillation autoregressive model address issue seek understand distillation effective prior work suggest distil train data less complex manual translations base experiment levenshtein transformer mask predict nar model wmt14 german english task paper show different type complexity different impact reduce lexical diversity decrease reorder complexity help nar learn better alignment source target thus improve translation quality lexical diversity main reason distillation increase model confidence affect calibration different nar model differently
model conversational context play vital role emotion recognition conversation erc paper put forward novel idea encode utterances direct acyclic graph dag better model intrinsic structure within conversation design direct acyclic neural networknamely dag erc implement ideain attempt combine strengths conventional graph base neural model recurrence base neural modelsdag erc provide intuitive way model information flow long distance conversation background nearby contextextensive experiment conduct four erc benchmarks state art model employ baselines comparisonthe empirical result demonstrate superiority new model confirm motivation direct acyclic graph architecture erc
automate event extraction social science applications often require corpus level evaluations example aggregate text predictions across metadata unbiased estimate recall combine corpus level evaluation requirements real world social science set introduce indiapoliceevents corpus twenty-one thousand, three hundred and ninety-one sentence one thousand, two hundred and fifty-seven english language time india article events state gujarat march two thousand and two train annotators read label every document mention police activity events allow unbiased recall evaluations contrast datasets structure event representations gather annotations pose natural question evaluate shelf model three different task sentence classification document rank temporal aggregation target events present baseline result zero shoot bert base model fine tune natural language inference passage retrieval task novel corpus level evaluations annotation approach guide creation similar social science orient resources future
query focus summarization qfs model aim generate summaries source document answer give query previous work qfs consider query relevance criterion produce summary however study effect answer relevance summary generate process also important paper propose qfs bart model incorporate explicit answer relevance source document give query via question answer model generate coherent answer relate summaries furthermore model take advantage large pre train model improve summarization performance significantly empirical result debatepedia dataset show propose model achieve new state art performance
work investigate use interactively update label suggestions improve upon efficiency gather annotations task opinion mine german covid nineteen social media data develop guidelines conduct control annotation study social science students find suggestions model train small expert annotate dataset already lead substantial improvement term inter annotator agreement14 fleiss kappa annotation quality compare students receive label suggestions find label suggestions interactively train model lead improvement suggestions static model nonetheless analysis suggestion bias show annotators remain capable reflect upon suggest label general finally confirm quality annotate data transfer learn experiment different annotator group facilitate research opinion mine social media data release collect data consist two hundred expert two thousand, seven hundred and eighty-five student annotations
knn mt recently propose khandelwal et al 2020a successfully combine pre train neural machine translation nmt model token level k nearest neighbor knn retrieval improve translation accuracy however traditional knn algorithm use knn mt simply retrieve number nearest neighbor target token may prediction errors retrieve neighbor include noise paper propose adaptive knn mt dynamically determine number k target token achieve introduce light weight meta k network efficiently train train sample four benchmark machine translation datasets demonstrate propose method able effectively filter noise retrieval result significantly outperform vanilla knn mt model even noteworthy meta k network learn one domain could directly apply domains obtain consistent improvements illustrate generality method implementation open source https githubcom zhengxxn adaptive knn mt
effective method improve extremely low resource neural machine translation multilingual train improve leverage monolingual data create synthetic bilingual corpora use back translation method work focus closely relate languages uralic language family estonian finnish geographical regions find multilingual learn synthetic corpora increase translation quality every language pair data show transfer learn fine tune effective low resource machine translation achieve best result collect new parallel data voro north south saami present first result neural machine translation languages
automatic machine translation super efficient produce translations yet quality guarantee technique report introduce transmart practical human machine interactive translation system able trade translation quality efficiency compare exist publicly available interactive translation systems transmart support three key feature word level autocompletion sentence level autocompletion translation memory word level sentence level autocompletion transmart allow users interactively translate word manners rather strict manner leave right addition transmart potential avoid similar translation mistake use translate sentence history memory report present major function transmart algorithms achieve function use transmart apis evaluation result key function transmart publicly available homepage https transmartqqcom
word ambiguous ie convey distinct mean different contexts even mean unambiguous word context dependent phenomena present challenge nlp recently advent contextualized word embeddings lead success task involve lexical ambiguity word sense disambiguation however task directly evaluate well contextualized embeddings accommodate continuous dynamic nature word mean particularly way match human intuitions introduce raw c dataset grade human relatedness judgments one hundred and twelve ambiguous word context six hundred and seventy-two sentence pair total well human estimate sense dominance average inter annotator agreement assess use leave one annotator method seventy-nine show measure cosine distance compute use contextualized embeddings bert elmo correlate human judgments cosine distance also systematically underestimate similar humans find use sense word systematically overestimate similar humans find use different sense homonyms finally propose synthesis psycholinguistic theories mental lexicon computational model lexical semantics
synthetic data generation widely know boost accuracy neural grammatical error correction gec systems exist methods often lack diversity simplistic generate broad range grammatical errors make human writers work use error type tag automatic annotation tool errant guide synthetic data generation compare several model produce ungrammatical sentence give clean sentence error type tag use model build new large synthetic pre train data set error tag frequency distributions match give development set synthetic data set yield large consistent gain improve state art bea nineteen conll fourteen test set also show approach particularly effective adapt gec system train mix native non native english native english test set even surpass real train data consist high quality sentence pair
generative adversarial imitation learn gail model free algorithm show provide strong result imitate complex behaviors high dimensional environments paper utilize gail model text generation develop empathy base context aware conversational ai model use expert trajectory empathetic prompt response dialogues accurately exhibit correct empathetic emotion generate response generator gail model use gpt two sequential pre train language model train one hundred and seventeen million parameters forty gb internet data propose novel application approach use transfer learn fine tune gpt two model order generate concise user specific empathetic responses validate discriminator novel gail model utilize sentiment analysis history base reinforcement learn approach empathetically respond human interactions personalize manner find model response score various human generate prompt collect facebook empathetic dialogues dataset outperform baseline counterparts moreover model improve upon various history base conversational ai model develop recently model performance sustain conversation three interactions outperform similar conversational ai model
machine translation assess quality large amount automatic translations challenge automatic metrics reliable come high perform systems addition resort human evaluators expensive especially evaluate multiple systems overcome latter challenge propose novel application online learn give ensemble machine translation systems dynamically converge best systems take advantage human feedback available experiment wmt nineteen datasets show online approach quickly converge top three rank systems language pair consider despite lack human feedback many translations
paper address challenge learn procedural reason text answer question propose novel relational gate network learn filter key entities relationships learn contextual cross representations procedure question find answer relational gate network contain entity gate module relation gate module contextual interaction module modules help solve reason problem show model pairwise relationships help capture higher order relations find line reason cause effect procedural descriptions propose approach achieve state art result wiqa dataset
compare general news domain information extraction ie biomedical text require much broader domain knowledge however many previous ie methods utilize external knowledge inference due exponential growth biomedical publications model go beyond fix set parameters likely fall behind inspire humans look relevant information comprehend scientific text present novel framework utilize external knowledge joint entity relation extraction name keci knowledge enhance collective inference give input text keci first construct initial span graph represent initial understand text use entity linker form knowledge graph contain relevant background knowledge entity mention text make final predictions keci fuse initial span graph knowledge graph refine graph use attention mechanism keci take collective approach link mention span entities integrate global relational information local representations use graph convolutional network experimental result show framework highly effective achieve new state art result two different benchmark datasets biorelex bind interaction detection ade adverse drug event extraction example keci achieve absolute improvements four hundred and fifty-nine four hundred and ninety-one f1 score state art biorelex entity relation extraction task
contextualized word representations prove useful various natural language process task however remain unclear extent representations cover hand cod semantic information semantic frame specify semantic role arguments associate predicate paper focus verbs evoke different frame depend context investigate well contextualized word representations recognize difference frame verb evoke also explore type representation suitable semantic frame induction experiment compare seven different contextualized word representations two english frame semantic resources framenet propbank demonstrate several contextualized word representations especially bert variants considerably informative semantic frame induction furthermore examine extent contextualized representation verb estimate number frame verb evoke
recent study semantic frame induction show relatively high performance achieve use cluster base methods contextualized word embeddings however two potential drawbacks methods one focus much superficial information frame evoke verb tend divide instance verb many different frame cluster overcome drawbacks propose semantic frame induction method use mask word embeddings two step cluster experiment english framenet data demonstrate use mask word embeddings effective avoid much reliance surface information frame evoke verbs two step cluster improve number result frame cluster instance verb
retrieval base dialogue systems select best response many candidates although many state art model show promise performance dialogue response selection task still quite gap r1 r10 performance address propose leverage linguistic coordination phenomenon individuals tend develop similar linguistic behaviors conversation rerank n best candidates produce bert state art pre train language model result show improvement r1 compare bert baselines demonstrate utility repair machine generate output leverage linguistic theory
modern task orient semantic parse approach typically use seq2seq transformers map textual utterances semantic frame comprise intents slot model empirically strong specific strengths weaknesses largely remain unexplored work study bart xlm r two state art parsers across monolingual multilingual settings experiment yield several key result transformer base parsers struggle disambiguate intents slot surprisingly also produce syntactically valid frame though pre train imbue transformers syntactic inductive bias find ambiguity copy utterance span frame often lead tree invalidity indicate span extraction major bottleneck current parsers however silver line show transformer base parsers give sufficient indicators whether frame likely correct incorrect make easier deploy production settings
paper propose hierarchical transformer model vietnamese spell correction problem model consist multiple transformer encoders utilize character level word level detect errors make corrections addition facilitate future work vietnamese spell correction task propose realistic dataset collect real life texts problem compare method methods publicly available systems propose method outperform contemporary methods term recall precision f1 score demo version publicly available
knowledge facts typically represent relational triple observe commonsense facts represent triple whose form inconsistent expression language inconsistency put forward challenge pre train language model deal commonsense knowledge facts paper term knowledge deep commonsense knowledge conduct extensive exploratory experiment show deep commonsense knowledge occupy significant part commonsense knowledge conventional methods fail capture effectively propose novel method mine deep commonsense knowledge distribute sentence alleviate reliance conventional methods triple representation form knowledge experiment demonstrate proposal significantly improve performance mine deep commonsense knowledge
widely use pre train language model operate sequence tokens correspond word subword units encode text sequence tokens require tokenizer typically create independent artifact model token free model instead operate directly raw text bytes character many benefit process text language box robust noise minimize technical debt remove complex error prone text preprocessing pipelines since byte character sequence longer token sequence past work token free model often introduce new model architectures design amortize cost operate directly raw text paper show standard transformer architecture use minimal modifications process byte sequence carefully characterize trade off term parameter count train flop inference speed show byte level model competitive token level counterparts also demonstrate byte level model significantly robust noise perform better task sensitive spell pronunciation part contribution release new set pre train byte level transformer model base t5 architecture well code data use experiment
neural abstractive summarization methods often require large quantities label train data however label large amount summarization data often prohibitive due time financial expertise constraints limit usefulness summarization systems practical applications paper argue limitation overcome semi supervise approach consistency train leverage large amount unlabeled data improve performance supervise learn small corpus consistency regularization semi supervise learn regularize model predictions invariant small noise apply input article add noise unlabeled corpus help regularize consistency train framework obtain comparative performance without use full dataset particular verify leverage large amount unlabeled data decently improve performance supervise learn insufficient label dataset
parallel cross lingual summarization data scarce require model better use limit available cross lingual resources exist methods often adopt sequence sequence network multi task frameworks approach apply multiple decoders utilize specific task however independent decoders share parameters hence fail capture relationships discrete phrase summaries different languages break connections order transfer knowledge high resource languages low resource languages bridge connections propose novel multi task framework cross lingual abstractive summarization mclas low resource set employ one unify decoder generate sequential concatenation monolingual cross lingual summaries mclas make monolingual summarization task prerequisite cross lingual summarization cls task way share decoder learn interactions involve alignments summary pattern across languages encourage attain knowledge transfer experiment two cls datasets demonstrate model significantly outperform three baseline model low resource full dataset scenarios moreover depth analysis generate summaries attention head verify interactions learn well use mclas benefit cls task limit parallel resources
natural language process offer new insights language data across almost discipline domains allow us corroborate challenge exist knowledge primary hurdle widen participation use new research tool first lack cod skills students across k sixteen population large second lack knowledge nlp methods use answer question disciplinary interest outside linguistics computer science broaden participation nlp improve nlp literacy introduce new tool web base tool call natural language process four nlp4all intend purpose nlp4all help teachers facilitate learn nlp provide easy use interfaces nlp methods data analyse make possible non novice programmers learn nlp concepts interactively
mix initiative open domain dialogue require system pro actively introduce new topics one turn topic transition task explore system connect two topics cooperative coherent manner goal task generate bridge utterance connect new topic topic previous conversation turn especially interest commonsense explanations new topic relate mention first collect new dataset human one turn topic transition call otters explore different strategies use humans ask complete task notice use bridge utterance connect two topics approach use finally show exist state art text generation model adapt task examine performance baselines different split otters data
success research institutions heavily rely upon identify right researchers job researchers may need identify appropriate collaborators often across discipline students may need identify suitable supervisors project interest administrators may need match fund opportunities relevant researchers usually find potential collaborators institutions time consume manual search task prone bias paper propose novel query base framework search score explore research expertise automatically base upon process abstract academic publications give user query natural language framework find researchers relevant expertise make use domain specific knowledge base word embeddings also generate explanations recommendations evaluate framework institutional repository paper lead university use baselines artificial neural network transformer base model multilabel classification task identify author publication abstract also assess cross domain effectiveness framework separate research fund repository institution show simple method effective identify match satisfy desirable properties efficient
recognize gender bias major issue affect current translation technologies researchers primarily attempt mitigate work data front however whether algorithmic aspects concur exacerbate unwanted output remain far investigate work bring analysis gender bias automatic translation onto seemingly neutral yet critical component word segmentation segment methods influence ability translate gender certain segmentation approach penalize representation feminine linguistic mark address question compare five exist segmentation strategies target side speech translation systems result two language pair english italian french show state art sub word split bpe come cost higher gender bias light find propose combine approach preserve bpe overall translation quality leverage higher ability character base segmentation properly translate gender
investigate semantic knowledge language model lms focus one whether lms create categories linguistic environments base semantic monotonicity properties two whether categories play similar role lms human language understand use negative polarity item license case study introduce series experiment consist probe diagnostic classifiers dcs linguistic acceptability task well novel dc rank method tightly connect probe result inner work lm apply experimental pipeline lms train various filter corpora able gain stronger insights semantic generalizations acquire model
understand table important relevant task involve understand table structure well able compare contrast information within cells paper address challenge present new dataset task address goal share task semeval two thousand and twenty task nine fact verification evidence find tabular data scientific document sem tab facts dataset contain nine hundred and eighty-one manually generate table auto generate dataset one thousand, nine hundred and eighty table provide 180k statement 16m evidence annotations sem tab facts feature two sub task sub task goal determine statement support refute unknown relation table sub task b focus identify specific cells table provide evidence statement sixty-nine team sign participate task nineteen successful submissions subtask twelve successful submissions subtask b present result main find competition
neural language model exhibit impressive performance variety task internal reason may difficult understand prior art aim uncover meaningful properties within model representations via probe unclear faithfully probe portray information model actually use overcome limitations propose technique inspire causal analysis generate counterfactual embeddings within model experiment test technique produce evidence suggest bert base model use tree distance like representation syntax downstream prediction task
paper present work bioasq pipeline goal answer four type question summary yes factoids list goal empirically evaluate different modules involve feature extractor sentence selection block use pipeline test effectiveness module kinds question type perform error analysis define metrics useful future research relate bioasq pipeline critical improve performance train pipeline
identify understand quality phrase context fundamental task text mine challenge part task arguably lie uncommon emerge domain specific phrase infrequent nature phrase significantly hurt performance phrase mine methods rely sufficient phrase occurrences input corpus context aware tag model though restrict frequency heavily rely domain experts either massive sentence level gold label handcraft gazetteers work propose ucphrase novel unsupervised context aware quality phrase tagger specifically induce high quality phrase span silver label consistently co occur word sequence within document compare typical context agnostic distant supervision base exist knowledge base kbs silver label root deeply input domain context thus unique advantage preserve contextual completeness capture emerge kb phrase train conventional neural tagger base silver label usually face risk overfitting phrase surface name alternatively observe contextualized attention map generate transformer base neural language model effectively reveal connections word surface agnostic way therefore pair attention map silver label train lightweight span prediction model apply new input recognize unseen quality phrase regardless surface name frequency thorough experiment various task datasets include corpus level phrase rank document level keyphrase extraction sentence level phrase tag demonstrate superiority design state art pre train unsupervised distantly supervise methods
present bhaunicodex1e63acitra dialect map system south asia build database linguistic study languages region annotate topic location data analyse language coverage look towards applications typology visualise example datasets application mean useful feature map also serve new kind interactive bibliography linguists south asian languages
question answer qa english widely explore multilingual datasets relatively new several methods attempt bridge gap high low resourced languages use data augmentation translation cross lingual transfer project take step back study approach allow us take advantage exist resources order produce qa systems many languages specifically perform extensive analysis measure efficacy shoot approach augment automatic translations permutations context question answer pair addition make suggestions future dataset development efforts make better use fix annotation budget goal increase language coverage qa datasets systems code data reproduce experiment available https githubcom navidrajabi emqa
multiwoz one popular multi domain task orient dialog datasets contain 10k annotate dialogs cover eight domains widely accept benchmark various dialog task eg dialog state track dst natural language generation nlg end end e2e dialog model work identify overlook issue dialog state annotation inconsistencies dataset slot type tag inconsistently across similar dialogs lead confusion dst model propose automate correction issue present whop seventy dialogs additionally notice significant entity bias dataset eg cambridge appear fifty destination cities train domain entity bias potentially lead name entity memorization generative model may go unnoticed test set suffer similar entity bias well release new test set entities replace unseen entities finally benchmark joint goal accuracy jga state art dst baselines modify versions data experiment show annotation inconsistency corrections lead seven ten improvement jga hand observe twenty-nine drop jga model evaluate new test set unseen entities
aspect category detection acd sentiment analysis aim identify aspect categories mention sentence paper formulate acd shoot learn scenario however exist shoot learn approach mainly focus single label predictions methods work well acd task since sentence may contain multiple aspect categories therefore propose multi label shoot learn method base prototypical network alleviate noise design two effective attention mechanisms support set attention aim extract better prototypes remove irrelevant aspects query set attention compute multiple prototype specific representations query instance use compute accurate distance correspond prototypes achieve multi label inference learn dynamic threshold per instance policy network extensive experimental result three datasets demonstrate propose method significantly outperform strong baselines
grammatical error correction gec sequence label model enjoy fast inference compare sequence sequence model however inference sequence label gec model iterative process sentence pass model multiple round correction expose model sentence progressively fewer errors round traditional gec model learn sentence fix error rat couple iterative correction process cause mismatch train inference affect final performance order address mismatch propose gin like sequence label model consist grammatical error detector discriminator grammatical error labeler gumbel softmax sample generator sample real error distributions errors genuine compare traditional synthesize gec errors thus alleviate aforementioned mismatch allow better train result several evaluation benchmarks demonstrate propose approach effective improve previous state art baseline
news article lead bias common phenomenon usually dominate learn signal neural extractive summarizers severely limit performance data different even bias paper introduce novel technique demote lead bias make summarizer focus content semantics experiment two news corpora different degrees lead bias show method effectively demote model learn lead bias improve generality distribution data little performance loss distribution data
commit message document summarize source code change natural language good commit message clearly show source code change enhance collaboration developers therefore work develop model automatically write commit message end release 345k datasets consist code modification commit message six program languages python php go java javascript ruby similar neural machine translation nmt model use dataset fee code modification encoder input commit message decoder input measure result generate commit message bleu four also propose follow two train methods improve result generate commit message one method preprocessing input fee code modification encoder input two method use initial weight suitable code domain reduce gap contextual representation program language pl natural language nl train code dataset pre train weight available https githubcom graykode commit autosuggestions
work conduct find tokenization methods affect train result machine translation model work alphabet tokenization morpheme tokenization bpe tokenization apply korean source language english target language respectively comparison experiment conduct repeat fifty thousand epochs nine model use transformer neural network result measure bleu score experimental model model apply bpe tokenization korean morpheme tokenization english record three thousand, five hundred and seventy-three show best performance
intrinsic evaluation humans performance natural language generation model conduct overcome fact quality generate sentence fully represent extrinsic evaluation nevertheless exist intrinsic evaluations large score deviation accord evaluator criteria paper propose grammar accuracy evaluation gae provide specific evaluate criteria result analyze quality machine translation bleu gae confirm bleu score represent absolute performance machine translation model gae compensate shortcomings bleu flexible evaluation alternative synonyms change sentence structure
detect domain ood unknown intents user query essential task orient dialog system key challenge ood detection learn discriminative semantic feature traditional cross entropy loss focus whether sample correctly classify explicitly distinguish margins categories paper propose supervise contrastive learn objective minimize intra class variance pull together domain intents belong class maximize inter class variance push apart sample different class besides employ adversarial augmentation mechanism obtain pseudo diverse view sample latent space experiment two public datasets prove effectiveness method capture discriminative representations ood detection
paper present participation minitrue team exist two thousand and twenty-one challenge sexism detection social media task english spanish approach combine language model simple vote mechanism sexist label prediction three bert base model vote function use experimental result show final model vote function achieve best result among four model mean vote mechanism bring extra benefit system nevertheless also observe system robust data source languages
exist slot fill model recognize pre define domain slot type limit slot set practical application reliable dialogue system know know paper introduce new task novel slot detection nsd task orient dialogue system nsd aim discover unknown domain slot type strengthen capability dialogue system base domain train data besides construct two public nsd datasets propose several strong nsd baselines establish benchmark future work finally conduct exhaustive experiment qualitative analysis comprehend key challenge provide new guidance future directions
multi modal dialog model grow interest work propose frameworks resolve specific case multi modal dialog generation better mimic multi modal dialog generation real world dialog turn associate visual context take place specifically propose model mutual dependency text visual feature model need learn probability generate next dialog utterance give precede dialog utterances visual contexts also probability predict visual feature dialog utterance take place lead generate dialog utterance specific visual context observe significant performance boost vanilla model mutual dependency text visual feature model code available https githubcom shannonai openvidial
propose neuralwoz novel dialogue collection framework use model base dialogue simulation neuralwoz two pipelined model collector labeler collector generate dialogues one user goal instructions user context task constraints natural language two system api call result list possible query responses user request give knowledge base labeler annotate generate dialogue formulate annotation multiple choice problem candidate label extract goal instructions api call result demonstrate effectiveness propose method zero shoot domain transfer learn dialogue state track evaluation synthetic dialogue corpus generate neuralwoz achieve new state art improvements forty-four point joint goal accuracy average across domains improvements fifty-seven point zero shoot coverage multiwoz twenty-one dataset
event extraction ee considerably benefit pre train language model plms fine tune however exist pre train methods involve model event characteristics result develop ee model take full advantage large scale unsupervised data end propose cleve contrastive pre train framework ee better learn event knowledge large unsupervised data semantic structure eg amr obtain automatic parsers cleve contain text encoder learn event semantics graph encoder learn event structure respectively specifically text encoder learn event semantic representations self supervise contrastive learn represent word events closer unrelated word graph encoder learn event structure representations graph contrastive pre train parse event relate semantic structure two complementary representations work together improve conventional supervise ee unsupervised liberal ee require jointly extract events discover event schemata without annotate data experiment ace two thousand and five maven datasets show cleve achieve significant improvements especially challenge unsupervised set source code pre train checkpoints obtain https githubcom thu keg cleve
lack reliable automatic evaluation metrics major impediment development open domain dialogue systems various reference base metrics propose calculate score predict response small set reference however metrics show unsatisfactory correlations human judgments reference base metric reliability mainly depend two factor ability measure similarity predict response reference response well reliability give reference set yet discussions latter work attempt fill vacancy first clarify assumption reference base metrics high quality reference add reference set reliability metric increase next present reamsharp enhancement approach reference base evaluation metrics open domain dialogue systems prediction model design estimate reliability give reference set show predict result helpful augment reference set thus improve reliability metric experiment validate effectiveness prediction model reliability reference base metrics improve augment reference set
structure sentiment analysis attempt extract full opinion tuples text time task subdivide smaller smaller sub task eg target extraction target polarity classification argue division become counterproductive propose new unify framework remedy situation cast structure sentiment problem dependency graph parse nod span sentiment holders target expressions arc relations perform experiment five datasets four languages english norwegian basque catalan show approach lead strong improvements state art baselines analysis show refine sentiment graph syntactic dependency information improve result
despite recent advancements attention base deep learn architectures across majority natural language process task application remain limit low resource set lack pre train model languages study make first attempt investigate challenge adapt techniques extremely low resource language sumerian cuneiform one world oldest write languages attest least begin 3rd millennium bc specifically introduce first cross lingual information extraction pipeline sumerian include part speech tag name entity recognition machine translation curate interpretlr interpretability toolkit low resource nlp use alongside human attributions make sense model emphasize human evaluations gauge techniques notably components pipeline generalise language obtain interpretable execution techniques especially low resource set publicly release software model checkpoints novel dataset domain specific pre process promote research
though nearest neighbor machine translation knn mt citekhandelwal2020nearest prove introduce significant performance boost standard neural mt systems prohibitively slow since use entire reference corpus datastore nearest neighbor search mean step beam beam search search entire reference corpus knn mt thus two order slower vanilla mt model make hard apply real world applications especially online service work propose fast knn mt address issue fast knn mt construct significantly smaller datastore nearest neighbor search word source sentence fast knn mt first select nearest token level neighbor limit tokens query token decode step contrast use entire corpus datastore search space limit target tokens correspond previously select reference source tokens strategy avoid search whole datastore nearest neighbor drastically improve decode efficiency without loss performance fast knn mt two order faster knn mt two time slower standard nmt model fast knn mt enable practical use knn mt systems real world mt applicationsfootnotecode available urlhttps githubcom shannonai fast knn nmt
pre train contextualized language model prlms lead strong performance gain downstream natural language understand task however prlms still easily fool adversarial word substitution one challenge textual adversarial attack methods exist defence approach suffer notable performance loss complexities thus paper present compact performance preserve framework anomaly detection frequency aware randomization adfar detail design auxiliary anomaly detection classifier adopt multi task learn procedure prlms able distinguish adversarial input sample order defend adversarial word substitution frequency aware randomization process apply recognize adversarial input sample empirical result show adfar significantly outperform newly propose defense methods various task much higher inference speed remarkably adfar impair overall performance prlms code available https githubcom lilynlp adfar
understand linguistics morphology resource scarce code mix texts remain key challenge text process although word embed come handy support downstream task low resource languages plenty scopes improve quality language representation particularly code mix languages paper propose hit robust representation learn method code mix texts hit hierarchical transformer base framework capture semantic relationship among word hierarchically learn sentence level semantics use fuse attention mechanism hit incorporate two attention modules multi head self attention outer product attention module compute weight sum obtain attention weight evaluation hit one european spanish five indic hindi bengali tamil telugu malayalam languages across four nlp task eleven datasets suggest significant performance improvement various state art systems show adaptability learn representation across task transfer learn setup without fine tune
shapley value solution credit assignment problem cooperative game theory popular type explanation machine learn use explain importance feature embeddings even neurons nlp however leave one attention base explanations still predominate draw connection different methods formally prove save degenerate case attention weight leave one value shapley value textitattention flow post process variant attention weight obtain run max flow algorithm attention graph perhaps surprisingly prove attention flow indeed shapley value least layerwise level give many desirable theoretical qualities shapley value drive adoption among ml community argue nlp practitioners possible adopt attention flow explanations alongside traditional ones
pre train transformer language model show remarkable performance variety nlp task however recent research suggest phrase level representations model reflect heavy influence lexical content lack evidence sophisticate compositional phrase information investigate impact fine tune capacity contextualized embeddings capture phrase mean information beyond lexical content specifically fine tune model adversarial paraphrase classification task high lexical overlap sentiment classification task fine tune analyze phrasal representations control settings follow prior work find fine tune largely fail benefit compositionality representations though train sentiment yield small localize benefit certain model follow analyse identify confound cue paraphrase dataset may explain lack composition benefit task discuss potential factor underlie localize benefit sentiment train
paper first provide review state art emotional voice conversion research exist emotional speech databases motivate development novel emotional speech database esd address increase research need paper esd database make available research community esd database consist three hundred and fifty parallel utterances speak ten native english ten native chinese speakers cover five emotion categories neutral happy angry sad surprise twenty-nine hours speech data record control acoustic environment database suitable multi speaker cross lingual emotional voice conversion study case study implement several state art emotional voice conversion systems esd database paper provide reference study esd conjunction release
describe approach semeval two thousand and twenty-one task six detection persuasion techniques multimodal content memes system combine pretrained multimodal model clip chain classifiers also propose enrich data data augmentation technique submission achieve rank eight sixteen term f1 micro nine sixteen f1 macro test set
table text generation refer generate descriptive text key value table traditional autoregressive methods though generate text high fluency suffer low coverage poor faithfulness problems mitigate problems propose novel skeleton base two stage method combine autoregressive non autoregressive generations sana approach include one skeleton generation autoregressive pointer network select key tokens source table two edit base non autoregressive generation model produce texts via iterative insertion deletion operations integrate hard constraints skeleton non autoregressive model improve generation coverage source table thus enhance faithfulness conduct automatic human evaluations wikiperson wikibio datasets experimental result demonstrate method outperform previous state art methods automatic human evaluation especially coverage faithfulness particular achieve parent recall nine thousand, nine hundred and forty-seven wikiperson improve exist best result ten point
unsupervised commonsense question answer appeal since rely label task data among exist work popular solution use pre train language model score candidate choices directly condition question context however score language model easily affect irrelevant factor word frequencies sentence structure etc distract factor may mislead model choose wrong answer also make oversensitive lexical perturbations candidate answer paper present novel semantic base question answer method seqa unsupervised commonsense question answer instead directly score answer choice method first generate set plausible answer generative model eg gpt two use plausible answer select correct choice consider semantic similarity plausible answer choice devise simple yet sound formalism idea verify effectiveness robustness extensive experiment evaluate propose method four benchmark datasets method achieve best result unsupervised settings moreover attack textfooler synonym replacement seqa demonstrate much less performance drop baselines thereby indicate stronger robustness
sequence sequence learn neural network empirically prove effective framework chinese spell correction csc take sentence spell errors input output correct one however csc model may fail correct spell errors cover confusion set also encounter unseen ones propose method continually identify weak spot model generate valuable train instance apply task specific pre train strategy enhance model generate adversarial examples gradually add train set experimental result show adversarial train method combine pretraining strategy improve generalization robustness multiple csc model across three different datasets achieve stateof art performance csc task
language model integrate traditional symbolic operations flexible neural representations recurrent neural network grammars rnngs attract great attention scientific engineer perspectives however rnngs know harder scale due difficulty batch train paper propose effective batch rnngs every operation compute parallel tensors across multiple sentence pytorch implementation effectively employ gpu achieve x6 speedup compare exist c dynet implementation model independent auto batch moreover batch rnng also accelerate inference achieve x20 one hundred and fifty speedup beam search depend beam size finally evaluate syntactic generalization performance scale rnng lstm baseline base large train data 100m tokens english wikipedia broad coverage target syntactic evaluation benchmark rnng implementation available https githubcom aistairc rnng pytorch
fine tune transformer model unsupervised pre train reach high performance many different nlp task unfortunately transformers suffer long inference time greatly increase cost production limit factor deployment embed devices one possible solution use knowledge distillation solve problem transfer information large teacher model smaller student model need additional expensive pre train phase solution computationally expensive financially prohibitive smaller academic research group another solution use layer wise prune methods reach high compression rat transformer model avoid computational load pre train distillation stage price pay performance layer wise prune algorithms par state art knowledge distillation methods paper greedy layer prune glp introduce one outperform current state art layer wise prune two close performance gap compare knowledge distillation three use modest budget precisely methodology present possible prune evaluate competitive model whole glue benchmark budget three hundred source code available https githubcom deepopinion greedy layer prune
paper introduce semeval two thousand and twenty-one share task four read comprehension abstract mean recam share task design help evaluate ability machine represent understand abstract concepts give passage correspond question participate system expect choose correct answer five candidates abstract concepts cloze style machine read comprehension setup base two typical definitions abstractness ie imperceptibility nonspecificity task provide three subtasks evaluate participate model specifically subtask one aim evaluate well system model concepts directly perceive physical world subtask two focus model ability comprehend nonspecific concepts locate high hypernym hierarchy give context passage subtask three aim provide insights model generalizability two type abstractness semeval two thousand and twenty-one official evaluation period receive twenty-three submissions subtask one twenty-eight subtask two participate team additionally make twenty-nine submissions subtask three leaderboard competition website find https competitionscodalaborg competitions twenty-six thousand, one hundred and fifty-three data baseline code available https githubcom boyuanzheng010 semeval2021 read comprehension abstract mean
paper analyze interplay use offensive language mental health acquire publicly available datasets create offensive language identification depression detection train computational model compare use offensive language social media post write group individuals without self report depression diagnosis also look sample write group individuals whose post show sign depression accord recent relate study analysis indicate offensive language frequently use sample write individuals self report depression well individuals show sign depression result discuss open new avenues research politeness offensiveness mental health
computer aid translation cat use software assist human translator translation process prove useful enhance productivity human translators autocompletion suggest translation result accord text piece provide human translators core function cat two limitations previous research line first research work topic focus sentence level autocompletion ie generate whole translation sentence base human input word level autocompletion explore far second almost public benchmarks available autocompletion task cat might among reason research progress cat much slower compare automatic mt paper propose task general word level autocompletion gwlan real world cat scenario construct first public benchmark facilitate research topic addition propose effective method gwlan compare several strong baselines experiment demonstrate propose method give significantly accurate predictions baseline methods benchmark datasets
paper describe current lexical similarity analogy gold standards build conform certain ideas model design evaluate use topical relevance always important target notion information access tool relate language technology technologies prove useful start point much information technology use always align well use technologies put notably use case digital scholarship humanities social sciences paper argue systematic formulation requirements digital humanities social sciences explicit description assumptions underlie model design
neural lexicalize pcfgs l pcfgs show effective grammar induction however reduce computational complexity make strong independence assumption generation child word thus bilexical dependencies ignore paper propose approach parameterize l pcfgs without make implausible independence assumptions approach directly model bilexical dependencies meanwhile reduce learn representation complexities l pcfgs experimental result english wsj dataset confirm effectiveness approach improve run speed unsupervised parse performance
propose method generate paraphrase english question retain original intent use different surface form model combine careful choice train objective principled information bottleneck induce latent encode space disentangle mean form train encoder decoder model reconstruct question paraphrase mean exemplar surface form lead separate encode space use vector quantize variational autoencoder represent surface form set discrete latent variables allow us use classifier select different surface form test time crucially method require access external source target exemplars extensive experiment human evaluation show able generate paraphrase better tradeoff semantic preservation syntactic novelty compare previous methods
scarcity parallel data major obstacle train high quality machine translation systems low resource languages fortunately low resource languages linguistically relate similar high resource languages relate languages may share many lexical syntactic structure work exploit linguistic overlap facilitate translate low resource language monolingual data addition parallel data relate high resource language method nmt adapt combine denoising autoencoding back translation adversarial objectives utilize monolingual data low resource adaptation experiment seven languages three different language families show technique significantly improve translation low resource language compare translation baselines
paper present process build social listen system base aspect base sentiment analysis vietnamese create dataset build real application firstly create uit visfd vietnamese smartphone feedback dataset new benchmark corpus build base strict annotation scheme evaluate aspect base sentiment analysis consist eleven thousand, one hundred and twenty-two human annotate comment mobile e commerce freely available research purpose also present propose approach base bi lstm architecture fasttext word embeddings vietnamese aspect base sentiment task experiment show approach achieve best performances f1 score eight thousand, four hundred and forty-eight aspect task six thousand, three hundred and six sentiment task perform several conventional machine learn deep learn systems last least build sa2sl social listen system base best performance model dataset inspire social listen systems future
show neural machine translation nmt highly sensitive noisy parallel train sample prior work treat type mismatch source target noise result remain unclear sample mostly equivalent contain small number semantically divergent tokens impact nmt train close gap analyze impact different type fine grain semantic divergences transformer model show model train synthetic divergences output degenerate text frequently less confident predictions base find introduce divergent aware nmt framework use factor help nmt recover degradation cause naturally occur divergences improve translation quality model calibration en fr task
paper propose inverse adversarial train iat algorithm train neural dialogue systems avoid generic responses model dialogue history better contrast standard adversarial train algorithms iat encourage model sensitive perturbation dialogue history therefore learn perturbations give higher reward responses whose output probability reduce significantly dialogue history perturb model encourage generate diverse consistent responses penalize model generate response give perturb dialogue history model force better capture dialogue history generate informative responses experimental result two benchmark datasets show approach better model dialogue history generate diverse consistent responses addition point problem widely use maximum mutual information mmi base methods improve diversity dialogue response generation model demonstrate empirically
transfer learn pre train neural language model towards downstream task predominant theme nlp recently several researchers show deep nlp model learn non trivial amount linguistic knowledge capture different layer model investigate fine tune towards downstream nlp task impact learn linguistic knowledge carry study across popular pre train model bert roberta xlnet use layer neuron level diagnostic classifiers find glue task network rely core linguistic information preserve deeper network others forget linguistic information distribute pre train language model become localize lower layer post fine tune reserve higher layer task specific knowledge pattern vary across architectures bert retain linguistic information relatively deeper network compare roberta xlnet predominantly delegate lower layer
paper present comparison unsupervised methods hypernymy prediction ie predict word pair word fish cod hypernym hyponym importantly demonstrate across datasets english german predictions three methods weedsprec invcl slqs row strongly overlap highly correlate frequency base predictions contrast second order method slqs show overall lower accuracy make correct predictions others go wrong study confirm general need check frequency bias computational method order identify frequency unrelated effect
propose alternate approach quantify well language model learn natural language ask well match statistical tendencies natural language answer question analyze whether text generate language model exhibit statistical tendencies present human generate text train provide framework pair significance test evaluate fit language model trend find neural language model appear learn subset tendencies consider align much closely empirical trend propose theoretical distributions present fit different distributions highly dependent model architecture generation strategy concrete examples text generate nucleus sample scheme adhere closely type token relationship natural language text produce use standard ancestral sample text lstms reflect natural language distributions length stopwords symbols surprisingly well
faceted summarization provide brief document different perspectives readers quickly comprehend main point long document help structure outline however little research conduct subject partially due lack large scale faceted summarization datasets study present facetsum faceted summarization benchmark build emerald journal article cover diverse range domains different traditional document summary pair facetsum provide multiple summaries target specific section long document include purpose method find value analyse empirical result dataset reveal importance bring structure summaries believe facetsum spur advance summarization research foster development nlp systems leverage structure information long texts summaries
pre train text encoders bert variants recently achieve state art performances many nlp task effective pre train methods typically demand massive computation resources accelerate pre train electra train discriminator predict whether input token replace generator however new task binary classification less semantically informative study present new text encoder pre train method improve electra base multi task learn specifically train discriminator simultaneously detect replace tokens select original tokens candidate set develop two techniques effectively combine pre train task one use attention base network task specific head two share bottom layer generator discriminator extensive experiment glue squad datasets demonstrate effectiveness efficiency propose method
fine tune large pre train model task specific data achieve great success nlp however demonstrate majority information within self attention network redundant utilize effectively fine tune stage lead inferior result generalize obtain model domain distributions end propose simple yet effective data augmentation technique hiddencut better regularize model encourage learn generalizable feature specifically contiguous span within hide space dynamically strategically drop train experiment show hiddencut method outperform state art augmentation methods glue benchmark consistently exhibit superior generalization performances distribution challenge counterexamples publicly release code https githubcom gt salt hiddencut
open domain dialog systems user centric goal provide humans engage conversation experience user engagement one important metrics evaluate open domain dialog systems could also use real time feedback benefit dialog policy learn exist work detect user disengagement typically require hand label many dialog sample propose herald efficient annotation framework reframes train data annotation process denoising problem specifically instead manually label train sample first use set label heuristics label train sample automatically denoise weakly label data use shapley algorithm finally use denoised data train user engagement detector experiment show herald improve annotation efficiency significantly achieve eighty-six user disengagement detection accuracy two dialog corpora
bias amplify neural machine translation nmt model optimize speed evaluate generic test set use bleu investigate architectures techniques commonly use speed decode transformer base model greedy search quantization average attention network aans shallow decoder model show effect gendered noun translation construct new gender bias test set simplegen base gendered noun phrase single unambiguous correct answer find minimal overall bleu degradation apply speed optimizations observe gendered noun translation performance degrade much faster rate
gender bias word embeddings gradually become vivid research field recent years study field aim measurement debiasing methods english target language paper investigate gender bias static word embeddings unique perspective chinese adjectives train word representations different model gender bias behind vectors adjectives assess comparison produce result human score data set demonstrate gender bias encode word embeddings differentiate people attitudes
model pre train large scale regular text corpora often work well user generate data language style differ significantly mainstream text present context aware rule injection cari innovative method formality style transfer fst cari inject multiple rule end end bert base encoder decoder model learn select optimal rule base context intrinsic evaluation show cari achieve new highest performance fst benchmark dataset extrinsic evaluation show cari greatly improve regular pre train model performance several tweet sentiment analysis task
name entity recognition ner remain challenge entity mention discontinuous exist methods break recognition process several sequential step train predict condition golden intermediate result inference rely model output previous step introduce exposure bias solve problem first construct segment graph sentence node denote segment continuous entity part discontinuous entities edge link two nod belong entity nod edge generate respectively one stage grid tag scheme learn jointly use novel architecture name mac discontinuous ner reformulate non parametric process discover maximal cliques graph concatenate span clique experiment three benchmarks show method outperform state art sota result thirty-five percentage point improvement f1 achieve 5x speedup sota model
search health information online become customary consumers every day make need efficient reliable question answer systems press important contributor success rat systems ability fully understand consumers question however question frequently longer need mention peripheral information useful find relevant answer question summarization one potential solutions simplify long complex consumer question attempt find answer paper study task abstractive summarization real world consumer health question develop abstractive question summarization model leverage semantic interpretation question via recognition medical entities enable generation informative summaries towards propose multiple cloze task ie task file miss word give context identify key medical entities enforce model better coverage question focus recognition additionally infuse decoder input question type information generate question type drive summaries evaluate meqsum benchmark corpus framework outperform state art method one hundred and two rouge l point also conduct manual evaluation assess correctness generate summaries
task automatically detect hate speech social media gain attention give enormous volume content post daily human monitor hate speech unfeasible work propose new word level feature automatic hate speech detection hsd multiword expressions mwes mwes lexical units greater word idiomatic compositional mean propose integrate mwe feature deep neural network base hsd framework baseline hsd system rely universal sentence encoder use incorporate mwe feature create three branch deep neural network one branch use one mwe categories one mwe embeddings conduct experiment two hate speech tweet corpora different mwe categories two type mwe embeddings word2vec bert experiment demonstrate propose hsd system mwe feature significantly outperform baseline system term macro f1
table widely use various kinds document present information concisely understand table challenge problem require understand language table structure along numerical logical reason paper present systems solve task nine semeval two thousand and twenty-one statement verification evidence find table sem tab facts task consist two subtasks give table statement predict whether table support statement b predict cells table provide evidence statement fine tune tapas model extend bert architecture capture tabular structure subtasks show state art performance various table understand task subtask evaluate transfer learn standardize table single header row improve tapas performance subtask b evaluate different fine tune strategies improve tapas performance systems achieve f1 score six thousand, seven hundred and thirty-four subtask three way classification seven thousand, two hundred and eighty-nine subtask two way classification six thousand, two hundred and ninety-five subtask b
due great potential facilitate software development code generation attract increase attention recently generally dominant model seq2tree model convert input natural language description sequence tree construction action correspond pre order traversal abstract syntax tree ast however traversal order may suitable handle multi branch nod paper propose equip seq2tree model context base branch selector able dynamically determine optimal expansion order branch multi branch nod particularly since selection expansion order non differentiable multi step operation optimize selector reinforcement learn formulate reward function difference model losses obtain different expansion order experimental result depth analysis several commonly use datasets demonstrate effectiveness generality approach release code https githubcom deeplearnxmu cg rl
exist dialog state track dst model train dialog data random order neglect rich structural information dataset paper propose use curriculum learn cl better leverage curriculum structure schema structure task orient dialogs specifically propose model agnostic framework call schema aware curriculum learn dialog state track saclog consist preview module pre train dst model schema information curriculum module optimize model cl review module augment mispredicted data reinforce cl train show propose approach improve dst performance transformer base rnn base dst model trippy trade achieve new state art result woz20 multiwoz21
fix length summarization aim generate summaries preset number word character recent research incorporate length information word embeddings input recurrent decode unit cause compromise length controllability summary quality work present effective length control unit length attention lenatten break trade experimental result show lenatten bring improvements length controllability rogue score also great generalization ability task generate summary target length model seven hundred and thirty-two time better best perform length controllable summarizer length controllability cnn daily mail dataset
task rationalization aim extract piece input text rationales justify neural network predictions text classification task definition rationales represent key text piece use prediction thus similar classification feature distribution compare original input text however previous methods mainly focus maximize mutual information rationales label neglect relationship rationales input text address issue propose novel rationalization method match distributions rationales input text feature space output space empirically propose distribution match approach consistently outperform previous methods large margin data code available
unlike english letter chinese character rich specific mean usually mean word derive constituent character way several previous work syntactic parse propose annotate shallow word internal structure better utilize character level information work propose model deep internal structure chinese word dependency tree eleven label distinguish syntactic relationships first base newly compile annotation guidelines manually annotate word internal structure treebank wist consist 30k multi char word chinese penn treebank guarantee quality word independently annotate two annotators inconsistencies handle third senior annotator second present detail interest analysis wist reveal insights chinese word formation third propose word internal structure parse new task conduct benchmark experiment use competitive dependency parser finally present two simple ways encode word internal structure lead promise gain sentence level syntactic parse task
conventional tokenization methods chinese pretrained language model plms treat character indivisible token devlin et al two thousand and nineteen ignore characteristics chinese write system work comprehensively study influence three main factor chinese tokenization plm pronunciation glyph ie shape word boundary correspondingly propose three kinds tokenizers one shuowen mean talk word pronunciation base tokenizers two jiezi mean solve character glyph base tokenizers three word segment tokenizers tokenizers chinese word segmentation empirically compare effectiveness study tokenizers pretrain bert style language model evaluate model various downstream nlu task find shuowen jiezi tokenizers generally outperform conventional single character tokenizers chinese word segmentation show benefit preprocessing step moreover propose shuowen jiezi tokenizers exhibit significantly better robustness handle noisy texts code pretrained model publicly release facilitate linguistically inform chinese nlp
pre train language model prlm show powerful enhance broad range downstream task include various dialogue relate ones however prlms usually train general plain text common language model lm train objectives sufficiently capture dialogue exclusive feature due limitation train set immediate need fill gap specific dialogue task lm task unlikely collect huge dialogue data dialogue orient pre train paper propose three strategies simulate conversation feature general plain text propose method differ exist post train methods may yield general purpose prlm individualize detail task keep capability learn dialogue relate feature include speaker awareness continuity consistency result dialog prlm fine tune three public multi turn dialogue datasets help achieve significant consistent improvement plain prlms
paper present result main find semeval two thousand and twenty-one task one lexical complexity prediction provide participants augment version complex corpus shardlow et al two thousand and twenty complex english multi domain corpus word multi word expressions mwes annotate respect complexity use five point likert scale semeval two thousand and twenty-one task one feature two sub task sub task one focus single word sub task two focus mwes competition attract one hundred and ninety-eight team total fifty-four team submit official run test data sub task one thirty-seven sub task two
transformer base approach successfully use obtain state art accuracy natural language process nlp task semi structure table model architectures typically deep result slow train inference especially long input improve efficiency maintain high accuracy propose new architecture dot double transformer model decompose problem two sub task shallow prune transformer select top k tokens follow deep task specific transformer take input k tokens additionally modify task specific attention incorporate prune score two transformers jointly train optimize task specific loss run experiment three benchmarks include entailment question answer show small drop accuracy dot improve train inference time least fifty also show prune transformer effectively select relevant tokens enable end end model maintain similar accuracy slower baseline model finally analyse prune give insight impact task model
automatic dialogue coherence evaluation attract increase attention crucial develop promise dialogue systems however exist metrics two major limitations mostly train simplify two level set coherent vs incoherent humans give likert type multi level coherence score dub quantifiable b predict coherence score align actual human rat standards due absence human guidance train address limitations propose quantifiable dialogue coherence evaluation quantidce novel framework aim train quantifiable dialogue coherence metric reflect actual human rat standards specifically quantidce include two train stag multi level rank mlr pre train knowledge distillation kd fine tune mlr pre train new mlr loss propose enable model learn coarse judgement coherence degrees kd fine tune pretrained model finetuned learn actual human rat standards human annotate data advocate generalizability even limit fine tune data novel kd regularization introduce retain knowledge learn pre train stage experimental result show model train quantidce present stronger correlations human judgements state art metrics
recent years see paradigm shift name entity recognition ner systems sequence label span prediction despite preliminary effectiveness span prediction model architectural bias fully understand paper first investigate strengths weaknesses span prediction model use name entity recognition compare sequence label framework improve motivate us make complementary advantage systems base different paradigms reveal span prediction simultaneously serve system combiner recognize name entities different systems output experimentally implement one hundred and fifty-four systems eleven datasets cover three languages comprehensive result show effectiveness span prediction model serve base ner systems system combiners make code datasets available urlhttps githubcom neulab spanner well online system demo urlhttp spannersh model also deploy explainaboard platform allow users flexibly perform system combination top score systems interactive way urlhttp explainaboardnlpediaai leaderboard task ner
effectiveness neural language model derive entirely accurate model surface word co occurrence statistics model represent reason world describe bart t5 transformer language model identify contextual word representations function model entities situations evolve throughout discourse neural representations functional similarities linguistic model dynamic semantics support linear readout entity current properties relations manipulate predictable effect language generation result indicate prediction pretrained neural language model support least part dynamic representations mean implicit simulation entity state behavior learn text train data code data available https githubcom belindal state probe
multiplication social media platforms offer anonymity easy access online community formation online debate issue hate speech detection track become grow challenge society individual policy makers researchers despite efforts leverage automatic techniques automatic detection monitor performances still far satisfactory constantly call future research issue paper provide systematic review literature field focus natural language process deep learn technologies highlight terminology process pipeline core methods employ focal point deep learn architecture methodological perspective adopt prisma guideline systematic review last ten years literature acm digital library google scholar sequel exist survey limitations future research directions extensively discuss
weight finite state machine fundamental build block nlp systems withstand test time early use noisy channel model 1990s modern day neurally parameterized conditional random field work examine computation higher order derivatives respect normalization constant weight finite state machine provide general algorithm evaluate derivatives order previously describe literature case second order derivatives scheme run optimal mathcalofa2 n4 time alphabet size n number state algorithm significantly faster prior algorithms additionally approach lead significantly faster algorithm compute second order expectations covariance matrices gradients first order expectations
connection maximum span tree direct graph best dependency tree sentence exploit nlp community however many dependency parse scheme important detail approach span tree must exactly one edge emanate root work do efficiently solve problem find one best dependency tree research attempt extend solution find k best dependency tree arguably important extension larger proportion decode tree subject root constraint dependency tree indeed show rate root constraint violations increase average thirteen time decode k50 oppose k1 paper provide simplification k best span tree algorithm camerini et al one thousand, nine hundred and eighty simplification allow us obtain constant time speed original algorithm furthermore present novel extension algorithm decode k best dependency tree graph subject root constraint
study task long form opinion text generation face least two distinct challenge first exist neural generation model fall short coherence thus require efficient content plan second diverse type information need guide generator cover subjective objective content end propose dyploc generation framework conduct dynamic plan content generate output base novel design mix language model enrich generation diverse content propose use large pre train model predict relevant concepts generate claim experiment two challenge task newly collect datasets one argument generation reddit changemyview two write article use new york time opinion section automatic evaluation show model significantly outperform competitive comparisons human judge confirm generations coherent richer content
integrate extract knowledge web knowledge graph kgs facilitate task like question answer study relation integration aim align free text relations subject relation object extractions relations target kg address challenge free text relations ambiguous previous methods exploit neighbor entities relations additional context however predictions make independently mutually inconsistent propose two stage collective relation integration cori model first stage independently make candidate predictions second stage employ collective model access candidate predictions make globally coherent predictions improve collective model augment data portion target kg otherwise unused experiment result two datasets show cori significantly outperform baselines improve auc six hundred and seventy-seven seven hundred and forty-eight seven hundred and sixteen seven hundred and eighty respectively
online conversations cover vast amount information many different format abstractive text summarization primarily focus model solely news article research gap due part lack standardize datasets summarize online discussions address gap design annotation protocols motivate issue viewpoints assertions framework crowdsource four new datasets diverse online conversation form news comment discussion forums community question answer forums email thread benchmark state art model datasets analyze characteristics associate data create comprehensive benchmark also evaluate model widely use conversation summarization datasets establish strong baselines domain furthermore incorporate argument mine graph construction directly model issue viewpoints assertions present conversation filter noisy input show comparable improve result accord automatic human evaluations
recent years see numerous nlp datasets introduce evaluate performance fine tune model natural language understand task recent result large pretrained model though show many datasets largely saturate unlikely able detect progress kind datasets still effective discriminate among strong model kind datasets expect able detect future improvements measure uniformly across datasets draw item response theory evaluate twenty-nine datasets use predictions eighteen pretrained transformer model individual test examples find quoref hellaswag mc taco best suit distinguish among state art model snli mnli commitmentbank seem saturate current strong model also observe span selection task format use qa datasets like qamr squad20 effective differentiate strong weak model
computational footprint modern nlp systems grow become increasingly important arrive efficient model show employ graph convolutional document representation arrive question answer system perform comparably case exceed sota solutions use less five resources term trainable parameters currently stand major issue apply gcns nlp document representation paper show gcn enrich document representation greatly improve result see hotpotqa even use trivial topology model gqa perform admirably compare current sota require little preprocessing shao et al two thousand and twenty author suggest graph network necessary good performance multi hop qa paper suggest large language model necessary good performance show naive implementation gcn perform comparably sota model base pretrained language model
manual fact check scale well serve need internet issue compound non english contexts paper discuss claim match possible solution scale fact check define claim match task identify pair textual message contain claim serve one fact check construct novel dataset whatsapp tipline public group message alongside fact check claim first annotate contain claim like statements match potentially similar items annotate claim match dataset contain content high resource english hindi lower resource bengali malayalam tamil languages train embed model use knowledge distillation high quality teacher model order address imbalance embed quality low high resource languages dataset provide evaluations performance solution compare baselines exist state art multilingual embed model namely laser labse demonstrate performance exceed laser labse settings release annotate datasets codebooks train embed model allow research
introduce categorical modularity novel low resource intrinsic metric evaluate word embed quality categorical modularity graph modularity metric base k nearest neighbor graph construct embed vectors word fix set semantic categories goal measure proportion word nearest neighbor within categories use core set five hundred word belong fifty-nine neurobiologically motivate semantic categories twenty-nine languages analyze three word embed model per language fasttext muse subs2vec find moderate strong positive correlations categorical modularity performance monolingual task sentiment analysis word similarity calculation cross lingual task bilingual lexicon induction english overall suggest categorical modularity provide non trivial predictive information downstream task performance breakdowns correlations model suggest meta predictive properties semantic information loss well
paper outline use transformer network train translate math word problems equivalent arithmetic expressions infix prefix postfix notations compare result produce many neural configurations find configurations outperform previously report approach three four datasets significant increase accuracy twenty percentage point best neural approach boost accuracy thirty compare previous state art datasets
sota coreference resolution produce increasingly impressive score ontonotes benchmark however lack comparable data follow scheme genres make difficult evaluate generalizability open domain data paper provide dataset comprehensive evaluation show latest neural lm base end end systems degrade substantially domain make ontonotes like coreference dataset call ontogum publicly available convert gum english corpus cover twelve genres use deterministic rule evaluate thank rich syntactic discourse annotations gum able create largest human annotate coreference corpus follow ontonotes guidelines first evaluate consistency ontonotes scheme domain evaluation across twelve genres show nearly fifteen twenty degradation deterministic deep learn systems indicate lack generalizability covert overfitting exist coreference resolution model
modern sentence encoders use generate dense vector representations capture underlie linguistic characteristics sequence word include phrase sentence paragraph kinds representations ideal train classifier end task sentiment analysis question answer text classification different model propose efficiently generate general purpose sentence representations use pretraining protocols average commonly use efficient sentence encoder discrete cosine transform dct recently propose alternative capture underlie syntactic characteristics give text without compromise practical efficiency compare average however sentence encoders dct sentence encoder evaluate english end utilize dct encoder generate universal sentence representation different languages german french spanish russian experimental result clearly show superior effectiveness dct encode consistent performance improvements achieve strong baselines multiple standardize datasets
evidence base fact check aim verify truthfulness claim evidence extract textual source learn representation effectively capture relations claim evidence challenge recent state art approach develop increasingly sophisticate model base graph structure present simple model train sequence structure model enable inter sentence attentions different level benefit joint train result large scale dataset fact extraction verification fever show model outperform graph base approach yield one hundred and nine one hundred and forty-two improvements label accuracy fever score respectively best publish model
recent advancements transformer base model greatly improve ability question answer qa systems provide correct answer particular answer sentence selection as2 model core components retrieval base systems achieve impressive result generally effective model fail provide satisfy answer retrieve candidates poor quality even contain correct information as2 model train select best answer sentence among set candidates retrieve give question work propose generate answer set as2 top candidates rather select best candidate train sequence sequence transformer model generate answer candidate set test three english as2 datasets show improvement thirty-two absolute point accuracy state art
exist conversational recommendation cr systems usually suffer insufficient item information conduct short dialogue history unfamiliar items incorporate external information eg review potential solution alleviate problem give review often provide rich detail user experience different interest potential ideal resources provide high quality recommendations within informative conversation paper design novel end end framework namely review augment conversational recommender revcore review seamlessly incorporate enrich item information assist generate coherent informative responses detail extract sentiment consistent review perform review enrich entity base recommendations item suggestions well use review attentive encoder decoder response generation experimental result demonstrate superiority approach yield better performance recommendation conversation respond
discourse relations among arguments reveal logical structure debate conversation however prior work explicitly study sequence discourse relations influence claim impact paper empirically show discourse relations two arguments along context path essential factor identify persuasive power argument propose discoc inject fuse sentence level structural discourse information contextualized feature derive large scale language model experimental result extensive analysis show attention gate mechanisms explicitly model contexts texts indeed help argument impact classification task define durmus et al two thousand and nineteen discourse structure among context path claim classify boost performance
pre train language model plms achieve great success nlp however huge model size hinder applications many practical systems knowledge distillation popular technique compress plms learn small student model large teacher plm however knowledge learn single teacher may limit even bias result low quality student model paper propose multi teacher knowledge distillation framework name mt bert pre train language model compression train high quality student model multiple teacher plms mt bert design multi teacher co finetuning method jointly finetune multiple teacher plms downstream task share pool prediction layer align output space better collaborative teach addition propose multi teacher hide loss multi teacher distillation loss transfer useful knowledge hide state soft label multiple teacher plms student model experiment three benchmark datasets validate effectiveness mt bert compress plms
recent study report many machine read comprehension mrc model perform closely even better humans benchmark datasets however exist work indicate many mrc model may learn shortcuts outwit benchmarks performance unsatisfactory real world applications work attempt explore instead expect comprehension skills model learn shortcuts base observation large portion question current datasets shortcut solutions argue larger proportion shortcut question train data make model rely shortcut trick excessively investigate hypothesis carefully design two synthetic datasets annotations indicate whether question answer use shortcut solutions propose two new methods quantitatively analyze learn difficulty regard shortcut challenge question reveal inherent learn mechanism behind different performance two kinds question thorough empirical analysis show mrc model tend learn shortcut question earlier challenge question high proportion shortcut question train set hinder model explore sophisticate reason skills later stage train
transformer important text model however difficulty handle long document due quadratic complexity input text length order handle problem propose hierarchical interactive transformer hi transformer efficient effective long document model hi transformer model document hierarchical way ie first learn sentence representations learn document representations effectively reduce complexity meanwhile capture global document context model sentence specifically first use sentence transformer learn representations sentence use document transformer model global document context sentence representations next use another sentence transformer enhance sentence model use global document context finally use hierarchical pool method obtain document embed extensive experiment three benchmark datasets validate efficiency effectiveness hi transformer long document model
since language model use model wide variety languages natural ask whether neural architectures use task inductive bias towards model particular type languages investigation bias prove complicate due many variables appear experimental setup languages vary many typological dimension difficult single one two investigate without others act confounders propose novel method investigate inductive bias language model use artificial languages languages construct allow us create parallel corpora across languages differ typological feature investigate word order use train test language model constitute fully control causal framework demonstrate grammar engineer serve useful tool analyze neural model use method find commonly use neural architectures exhibit different inductive bias lstms display little preference respect word order transformers display clear preference order others find neither inductive bias lstm transformer appear reflect tendencies see attest natural languages
five years first publish proof concept direct approach speech translation st compete traditional cascade solutions light steady progress claim performance gap two close start question present systematic comparison state art systems representative two paradigms focus three language directions english german italian spanish conduct automatic manual evaluations exploit high quality professional post edit annotations multi faceted analysis one publicly available st benchmarks attest first time gap two paradigms close ii subtle differences observe behavior sufficient humans neither distinguish prefer one
model agnostic meta learn maml recently put forth strategy learn resource poor languages sample efficient fashion nevertheless properties languages often well represent available train hence argue iid assumption ingrain maml make ill suit cross lingual nlp fact decision theoretic framework maml interpret minimise expect risk across train languages uniform prior know bay criterion increase robustness outlier languages create two variants maml base alternative criteria minimax maml reduce maximum risk across languages neyman pearson maml constrain risk language maximum threshold criteria constitute fully differentiable two player game light propose new adaptive optimiser solve local approximation nash equilibrium evaluate model variants two popular nlp task part speech tag question answer report gain average minimum performance across low resource languages zero shoot settings compare joint multi source transfer vanilla maml
interpersonal verbs implicitly attribute causality either subject object therefore say carry implicit causality ic bias bias causal link infer narrative aid language comprehension investigate whether pre train language model plms encode ic bias use inference time find case albeit different degrees three distinct plm architectures however cause always need implicit explicitly state subordinate clause incongruent ic bias associate verb main clause lead delay human process hypothesize temporary challenge humans face integrate two contradict signal one lexical semantics verb one sentence level semantics would reflect higher error rat model task dependent causal link result study lend support hypothesis suggest plms tend prioritize lexical pattern higher order signal
purpose argumentative text support certain conclusion yet often omit expect readers infer rather appropriate read individual text rhetorical device limit accessibility browse many texts eg search engine social media scenarios explicit conclusion make good candidate summary argumentative text especially true conclusion informative emphasize specific concepts text paper introduce task generate informative conclusions first webis conclugen twenty-one compile large scale corpus one hundred and thirty-six thousand, nine hundred and ninety-six sample argumentative texts conclusions second two paradigms conclusion generation investigate one extractive abstractive nature latter exploit argumentative knowledge augment data via control cod finetuning bart model several subsets corpus third insights provide suitability corpus task differences two generation paradigms trade informativeness conciseness impact encode argumentative knowledge corpus code train model publicly available
recently significant progress study neural network translate text descriptions sql query despite achieve good performance public benchmarks exist text sql model typically rely lexical match word natural language nl question tokens table schemas may render model vulnerable attack break schema link mechanism work investigate robustness text sql model synonym substitution particular introduce spider syn human curated dataset base spider benchmark text sql translation nl question spider syn modify spider replace schema relate word manually select synonyms reflect real world question paraphrase observe accuracy dramatically drop eliminate explicit correspondence nl question table schemas even synonyms adversarially select conduct worst case adversarial attack finally present two categories approach improve model robustness first category approach utilize additional synonym annotations table schemas modify model input second category base adversarial train demonstrate categories approach significantly outperform counterparts without defense first category approach effective
emotion detection dialogues challenge often require identification thematic topics underlie conversation relevant commonsense knowledge intricate transition pattern affective state paper propose topic drive knowledge aware transformer handle challenge firstly design topic augment language model lm additional layer specialize topic detection topic augment lm combine commonsense statements derive knowledge base base dialogue contextual information finally transformer base encoder decoder architecture fuse topical commonsense information perform emotion label sequence prediction model experiment four datasets dialogue emotion detection demonstrate superiority empirically exist state art approach quantitative qualitative result show model discover topics help distinguish emotion categories
recently deep neural network dnns achieve great success semantically challenge nlp task yet remain unclear whether dnn model capture compositional mean aspects mean long study formal semantics investigate issue propose systematic generalization testbed base natural language semantics sygns whose challenge map natural language sentence multiple form scoped mean representations design account various semantic phenomena use sygns test whether neural network systematically parse sentence involve novel combinations logical expressions quantifiers negation experiment show transformer gru model generalize unseen combinations quantifiers negations modifiers similar give train instance form others also find generalization performance unseen combinations better form mean representations simpler data code sygns publicly available https githubcom verypluming sygns
sparse attention claim increase model interpretability assumption highlight influential input yet attention distribution typically representations internal model rather input suggest assumption may merit build recent work explore interpretability attention design set experiment help us understand sparsity affect ability use attention explainability tool three text classification task verify weak relationship input co index intermediate representations exist sparse attention otherwise find plausible mappings sparse attention distributions sparse set influential input avenues rather observe set induce sparsity may make less plausible attention use tool understand model behavior
natural language process nlp become important mean automatic recognition human traits state intoxication presence psychiatric disorder presence airway disorder state stress applications potential important pillar online help line may gradually introduce ehealth modules however nlp language specific languages dutch nlp model scarce result recent dutch nlp model low capture long range semantic dependencies sentence overcome present belabbert new dutch language model extend roberta architecture belabbert train large dutch corpus thirty-two gb web crawl texts apply belabbert classification psychiatric illnesses first evaluate strength text base classification use belabbert compare result exist robbert model compare performance belabbert audio classification psychiatric disorder finally brief exploration perform extend framework hybrid text audio base classification result show belabbert outperform current best text classification network dutch robbert belabbert also outperform classification base audio alone
work aim tackle challenge heterogeneous graph encode problem text sql task previous methods typically node centric merely utilize different weight matrices parameterize edge type one ignore rich semantics embed topological structure edge two fail distinguish local non local relations node end propose line graph enhance text sql lgesql model mine underlie relational feature without construct meta paths virtue line graph message propagate efficiently connections nod also topology direct edge furthermore local non local relations integrate distinctively graph iteration also design auxiliary task call graph prune improve discriminative capability encoder framework achieve state art result six hundred and twenty-eight glove seven hundred and twenty electra cross domain text sql benchmark spider time write
ethical aspects research language technologies receive much attention recently standard practice get study involve human subject review approve professional ethics committee board institution commonly see mention ethical approvals nlp research type research aspects study usually subject review rise concern discourse around ethics nlp also observe rise formal ethical review nlp study would imply heighten awareness ethical issue previously lack aim address question conduct detail quantitative qualitative analysis acl anthology well compare trend field relate discipline cognitive science machine learn data mine systems
dialogue essentially multi turn interaction among interlocutors effective evaluation metrics reflect dynamics interaction exist automatic metrics focus much turn level quality ignore dynamics end propose dynaeval unify automatic evaluation framework capable perform turn level evaluation also holistically consider quality entire dialogue dynaeval graph convolutional network gcn adopt model dialogue totality graph nod denote individual utterance edge represent dependency pair utterances contrastive loss apply distinguish well form dialogues carefully construct negative sample experiment show dynaeval significantly outperform state art dialogue coherence model correlate strongly human judgements across multiple dialogue evaluation aspects turn dialogue level
emotional support crucial ability many conversation scenarios include social interactions mental health support customer service chat follow reasonable procedures use various support skills help effectively provide support however due lack well design task corpora effective emotional support conversations research build emotional support dialog systems remain untouched paper define emotional support conversation esc task propose esc framework ground help skills theory construct emotion support conversation dataset esconv rich annotation especially support strategy help seeker supporter mode ensure corpus high quality conversations provide examples effective emotional support take extensive effort design train tutorials supporters several mechanisms quality control data collection finally evaluate state art dialog model respect ability provide emotional support result show importance support strategies provide effective emotional support utility esconv train emotional support systems
paper study end end construction nlp knowledge graph kg scientific paper focus extract four type relations evaluatedon task datasets evaluatedby task evaluation metrics well coreferent relate relations type entities instance f1 score coreferent f measure introduce novel methods relation type apply final framework scinlp kg thirty thousand nlp paper acl anthology build large scale kg facilitate automatically construct scientific leaderboards nlp community result experiment indicate result kg contain high quality information
language generation model democratization benefit many domains answer health relate question enhance education provide ai drive tutor service however language generation model democratization also make easier generate human like text scale nefarious activities spread misinformation target specific group hate speech thus essential understand people interact bots develop methods detect bot generate text paper show bot generate text detection methods robust across datasets model use information people respond rather use bot text directly also analyze linguistic alignment provide insight differences human human human bot conversations
representation degeneration problem contextual word representations cwrs hurt expressiveness embed space form anisotropic cone even unrelated word excessively positive correlations exist techniques tackle issue require learn process train model additional objectives mostly employ global assessment study isotropy quantitative analysis isotropy show local assessment could accurate due cluster structure cwrs base observation propose local cluster base method address degeneration issue contextual embed space show cluster include punctuations stop word local dominant directions encode structural information remove improve cwrs performance semantic task moreover find tense information verb representations dominate sense semantics show remove dominant directions verb representations transform space better suit semantic applications experiment demonstrate propose cluster base method mitigate degeneration problem multiple task
present novel model problem rank collection document accord semantic similarity source query document problem document document similarity rank study modern methods limit relatively short document rely existence grind truth similarity label yet common real world case similarity rank unsupervised problem similarity label unavailable moreover ideal model restrict document length hence introduce sdr self supervise method document similarity apply document arbitrary length importantly sdr effectively apply extremely long document exceed four thousand and ninety-six maximal token limit longformer extensive evaluations large document datasets show sdr significantly outperform alternatives across metrics accelerate future research unlabeled long document similarity rank additional contribution community herein publish two human annotate test set long document similarity evaluation sdr code datasets publicly available
exist software base energy measurements nlp model accurate consider complex interactions energy consumption model execution present irene interpretable extensible energy prediction system accurately predict inference energy consumption wide range transformer base nlp model irene construct model tree graph break nlp model modules break low level machine learn ml primitives irene predict inference energy consumption ml primitives function generalizable feature fine grain runtime resource usage irene aggregate low level predictions recursively predict energy module finally entire model experiment across multiple transformer model show irene predict inference energy consumption transformer model error seven compare grind truth contrast exist energy model see error fifty also show irene use conduct energy bottleneck analysis easily evaluate energy impact different architectural choices release code data https githubcom stonybrooknlp irene
grow body literature focus detail linguistic knowledge embed large pretrained language model exist work show non linguistic bias model drive model behavior away linguistic generalizations hypothesize compete linguistic process within language rather non linguistic model bias could obscure underlie linguistic knowledge test claim explore single phenomenon four languages english chinese spanish italian human behavior find similar across languages find cross linguistic variation model behavior show compete process language act constraints model behavior demonstrate target fine tune weight learn constraints uncover otherwise dormant linguistic knowledge model result suggest model need learn linguistic constraints language relative rank mismatch either produce non human like behavior
coreference resolution mostly investigate within single document scope show impressive progress recent years base end end model however challenge task cross document cd coreference resolution remain relatively explore recent model apply gold mention introduce first end end model cd coreference resolution raw text extend prominent model within document coreference cd set model achieve competitive result event entity coreference resolution gold mention importantly set first baseline result standard ecb dataset cd coreference resolution predict mention model simpler efficient recent cd coreference resolution systems use external resources
name entity recognition ner task identify span represent entities sentence whether entity span nest discontinuous ner task categorize flat ner nest ner discontinuous ner subtasks subtasks mainly solve token level sequence label span level classification however solutions hardly tackle three kinds ner subtasks concurrently end propose formulate ner subtasks entity span sequence generation task solve unify sequence sequence seq2seq framework base unify framework leverage pre train seq2seq model solve three kinds ner subtasks without special design tag schema ways enumerate span exploit three type entity representations linearize entities sequence propose framework easy implement achieve state art sota near sota performance eight english ner datasets include two flat ner datasets three nest ner datasets three discontinuous ner datasets
generate metaphors difficult task require understand nuanced relationships abstract concepts paper aim generate metaphoric sentence give literal expression replace relevant verbs guide conceptual metaphor theory propose control generation process encode conceptual mappings cognitive domains generate meaningful metaphoric expressions achieve develop two methods one use framenet base embeddings learn mappings domains apply lexical level cm lex two derive source target pair train control seq seq generation model cm bart assess methods automatic human evaluation basic metaphoricity conceptual metaphor presence show unsupervised cm lex model competitive recent deep learn metaphor generation systems cm bart outperform model automatic human evaluations
computational psycholinguistics various language model evaluate human read behavior eg eye movement build human like computational model however previous efforts focus almost exclusively english despite recent trend towards linguistic universal within general community order fill gap paper investigate whether establish result computational psycholinguistics generalize across languages specifically examine establish generalization lower perplexity language model human like language model japanese typologically different structure english experiment demonstrate establish generalization exhibit surprise lack universality namely lower perplexity always human like moreover discrepancy english japanese explore perspective non uniform information density overall result suggest cross lingual evaluation necessary construct human like computational model
interpretability important aspect trustworthiness model predictions transformer predictions widely explain attention weight ie probability distribution generate self attention unit head current empirical study provide shred evidence attention weight explanations prove unique recent study show theoretical justifications observation prove non identifiability attention weight give input head output attention weight generate unique call weight identifiable work provide deeper theoretical analysis empirical observations identifiability attention weight ignore previous work find attention weight identifiable currently perceive uncover hide role key vector however weight still prone non unique attentions make unfit interpretation tackle issue provide variant encoder layer decouple relationship key value vector provide identifiable weight desire length input prove applicability variations provide empirical justifications vary text classification task implementations available https githubcom declare lab identifiable transformers
much information nlp task really need transformer attention mechanism application time inference recent work know sparsity transformers float point within computation discretized fewer value minimal loss task accuracies however require retrain even create entirely new model expensive carbon emit focus optimizations require train systematically study full range typical attention value necessary inform design inference time quantization technique use prune log scale map produce eg twenty-three unique value task question answer sentiment analysis find nearly eighty attention value prune zero minimal ten relative loss accuracy use prune technique conjunction quantize attention value three bite format without retrain result eight accuracy reduction question answer fine tune roberta
adapter modules recently introduce efficient alternative fine tune nlp adapter tune consist freeze pretrained parameters model inject lightweight modules layer result addition small number task specific trainable parameters adapter tune investigate multilingual neural machine translation paper propose comprehensive analysis adapters multilingual speech translation st start different pre train model multilingual st train parallel data multilingual bart mbart train non parallel multilingual data show adapters use efficiently specialize st specific language pair low extra cost term parameters b transfer automatic speech recognition asr task mbart pre train model multilingual st task experiment show adapter tune offer competitive result full fine tune much parameter efficient
automatic summarization evaluation methods develop english routinely apply languages first attempt systematically quantify panlinguistic efficacy take summarization corpus eight different languages manually annotate generate summaries focus precision coverage recall base evaluate nineteen summarization evaluation metrics find use multilingual bert within bertscore perform well across languages level english
despite prominence neural abstractive summarization model know little actually form summaries understand decisions come propose two step method interpret summarization model decisions first analyze model behavior ablate full model categorize decoder decision one several generation modes roughly model behave like language model rely heavily input somewhere isolate decisions depend input explore interpret decisions use several different attribution methods compare techniques base ability select content reconstruct model predict token perturbations input thus reveal whether highlight attributions truly important generation next token machinery broadly useful even beyond summarization specifically demonstrate capability identify phrase summarization model memorize determine train pipeline memorization happen well study complex generation phenomena like sentence fusion per instance basis
romantic partner interact conflict influence feel end interaction predictive whether partner stay together long term hence understand emotions partner important yet current approach use include self report burdensome hence limit frequency data collection automatic emotion prediction could address challenge insights psychology research indicate partner behaviors influence emotions conflict interaction hence behavior partner could consider better predict partner emotion however yet investigate compare use partner behavior term emotion prediction performance work use bert extract linguistic feature ie partner say opensmile extract paralinguistic feature ie say data set three hundred and sixty-eight german speak swiss couple n seven hundred and thirty-six individuals videotape eight minutes conflict interaction laboratory base feature train machine learn model predict partner feel positive negative conflict interaction result show include behavior partner improve prediction performance furthermore men consider female partner speak important women consider male partner say important get better prediction performance work step towards automatically recognize partner emotion base behavior would enable better understand couple research therapy real world
many process psychology complex dyadic interactions two interact partner eg patient therapist intimate relationship partner nevertheless many basic question interactions difficult investigate dyadic process within person partner base multimodal aspects behavior unfold rapidly current analyse mainly base behavioral cod method whereby human coders annotate behavior base cod schema cod labor intensive expensive slow focus modalities current approach psychology use liwc analyze couple interactions however advance natural language process bert could enable development systems potentially automate behavioral cod turn could substantially improve psychological research work train machine learn model automatically predict positive negative communication behavioral cod three hundred and sixty-eight german speak swiss couple eight minute conflict interaction fine grain scale ten second sequence use linguistic feature paralinguistic feature derive opensmile result show simpler tf idf feature well complex bert feature perform better liwc add paralinguistic feature improve performance result suggest might time consider modern alternatives liwc de facto linguistic feature psychology prediction task couple research work step towards automate cod couple behavior could enhance couple research therapy utilize dyadic interactions well
recently various neural model multi party conversation mpc achieve impressive improvements variety task addressee recognition speaker identification response prediction however exist methods mpc usually represent interlocutors utterances individually ignore inherent complicate structure mpc may provide crucial interlocutor utterance semantics would enhance conversation understand process end present mpc bert pre train model mpc understand consider learn say unify model several elaborate self supervise task particularly task generally categorize one interlocutor structure model include reply utterance recognition identical speaker search pointer consistency distinction two utterance semantics model include mask share utterance restoration share node detection evaluate mpc bert three downstream task include addressee recognition speaker identification response selection experimental result show mpc bert outperform previous methods large margins achieve new state art performance three downstream task two benchmarks
relational fact extraction aim extract semantic triplets unstructured text work show relational fact extraction model organize accord graph orient analytical perspective efficient model adjacency list orient relational fact direct propose base analytical framework alleviate challenge error propagation sub task loss equilibrium direct employ novel adaptive multi task learn strategy dynamic sub task loss balance extensive experiment conduct two benchmark datasets result prove propose model outperform series state art sota model relational triplet extraction
recent work investigate interest question use pre train language model plms knowledge base answer open question however exist work limit use small benchmarks high test train overlap construct new dataset close book qa use squad investigate performance bart experiment show challenge bart remember train facts high precision also challenge answer close book question even relevant knowledge retain promise directions find include decouple knowledge memorize process qa finetune process force model recall relevant knowledge question answer
document level relation extraction docre model generally use graph network implicitly model reason skill ie pattern recognition logical reason coreference reason etc relate relation one entity pair document paper propose novel discriminative reason framework explicitly model paths reason skills entity pair document thus discriminative reason network design estimate relation probability distribution different reason paths base construct graph vectorized document contexts entity pair thereby recognize relation experimental result show method outperform previous state art performance large scale docre dataset code publicly available https githubcom xwjim drn
abstractive neural summarization model see great improvements recent years show rouge score generate summaries despite improve metrics limit understand strategies different model employ strategies relate understand language understand better run several experiment characterize one popular abstractive model pointer generator model see et al two thousand and seventeen use explicit copy generation switch control level abstraction generation vs extraction copy extractive bias dataset model utilize syntactic boundaries truncate sentence otherwise often copy verbatim modify copy generation switch force model generate simple paraphrase abilities reveal alongside factual inaccuracies hallucinations abstractive bias dataset model copy infrequently show similarly limit abstractive abilities line previous research result suggest abstractive summarization model lack semantic understand necessary generate paraphrase abstractive faithful source document
online game forums popular game players use communicate discuss strategy game even make friends however game forums also contain abusive harassment speech disturb threaten players therefore necessary automatically detect remove cyberbullying comment keep game forum clean friendly use cyberbullying dataset collect world warcraft wow league legends lol forums train classification model automatically detect whether comment player abusive result obtain eight thousand, two hundred and sixty-nine macro f1 score lol forum eight thousand, three hundred and eighty-six macro f1 score wow forum toxic bert model cyberbullying dataset
paper study automatically generate natural language text describe facts knowledge graph kg consider shoot set leverage excellent capacities pretrained language model plms language understand generation make three major technical contributions namely representation alignment bridge semantic gap kg encode plms relation bias kg linearization derive better input representations multi task learn learn correspondence kg text extensive experiment three benchmark datasets demonstrate effectiveness model kg text generation task particular model outperform comparison methods fully supervise shoot settings code datasets available https githubcom rucaibox shoot kg2text
countermeasures effectively fight ever increase hate speech online without block freedom speech great social interest natural language generation nlg uniquely capable develop scalable solutions however shelf nlg methods primarily sequence sequence neural model limit generate commonplace repetitive safe responses regardless hate speech eg please refrain use language irrelevant responses make ineffective de escalate hateful conversations paper design three module pipeline approach effectively improve diversity relevance propose pipeline first generate various counterspeech candidates generative model promote diversity filter ungrammatical ones use bert model finally select relevant counterspeech response use novel retrieval base method extensive experiment three representative datasets demonstrate efficacy approach generate diverse relevant counterspeech
modern model event causality identification eci mainly base supervise learn prone data lack problem unfortunately exist nlp relate augmentation methods directly produce available data require task solve data lack problem introduce new approach augment train data event causality identification iteratively generate new examples classify event causality dual learn framework one hand approach knowledge guide leverage exist knowledge base generate well form new sentence hand approach employ dual mechanism learnable augmentation framework interactively adjust generation process generate task relate sentence experimental result two benchmarks eventstoryline causal timebank show one method augment suitable task relate train data eci two method outperform previous methods eventstoryline causal timebank twenty-five twenty-one point f1 value respectively
current model event causality identification eci mainly adopt supervise framework heavily rely label data train unfortunately scale current annotate datasets relatively limit provide sufficient support model capture useful indicators causal statements especially hand new unseen case alleviate problem propose novel approach shortly name causerl leverage external causal statements event causality identification first design self supervise framework learn context specific causal pattern external causal statements adopt contrastive transfer strategy incorporate learn context specific causal pattern target eci model experimental result show method significantly outperform previous methods eventstoryline causal timebank twenty thirty-four point f1 value respectively
great research interest attract devise ai service able provide mental health support however lack corpora main obstacle research particularly chinese language paper propose psyqa chinese dataset psychological health support form question answer pair psyqa crawl chinese mental health service platform contain 22k question 56k long well structure answer base psychological counsel theories annotate portion answer texts typical strategies provide support present depth analysis lexical feature strategy pattern counsel answer also evaluate performance generate counsel answer generative pretrained model result show utilize strategies enhance fluency helpfulness generate answer still large space future research
multilingual pre train model achieve remarkable transfer performance pre train rich kinds languages model mbert pre train unlabeled corpora static contextual embeddings model could align well paper aim improve zero shoot cross lingual transfer performance align embeddings better propose pre train task name alignment language model alignlm use statistical alignment information prior knowledge guide bilingual word prediction evaluate method multilingual machine read comprehension natural language interface task result show alignlm improve zero shoot performance significantly mlqa xnli datasets
ability learn limit data shoot learn desirable often critical requirement nlp systems many exist methods poorly learn handful examples large pretrained language model recently show efficient shoot learners one approach shoot learn require finetuning model parameters augment language model input prim text typically construct use task specific descriptions examples work explore prim base shoot learn focus use examples prompt show present examples right order key generalization introduce pero prompt examples right order formulate shoot learn search set permutations train examples show pero learn generalize efficiently use ten examples contrast exist approach newline token natural choice separate examples prompt show learn new separator token potentially provide gain performance demonstrate effectiveness propose method task sentiment classification natural language inference fact retrieval finally analyze learn prompt reveal novel insights include idea two train examples right order alone provide competitive performance sentiment classification natural language inference
recent interest investigate shoot ner low resource target domain different label set compare resource rich source domain exist methods use similarity base metric however make full use knowledge transfer ner model parameters address issue propose template base method ner treat ner language model rank problem sequence sequence framework original sentence statement templates fill candidate name entity span regard source sequence target sequence respectively inference model require classify candidate span base correspond template score experiment demonstrate propose method achieve nine thousand, two hundred and fifty-five f1 score conll03 rich resource task significantly better fine tune bert one thousand and eighty-eight one thousand, five hundred and thirty-four one thousand, one hundred and seventy-three f1 score mit movie mit restaurant atis low resource task respectively
document level relation extraction challenge task sentence often require reason multiple sentence yet human annotators usually use small number sentence identify relationship give entity pair paper present embarrassingly simple effective method heuristically select evidence sentence document level easily combine bilstm achieve good performance benchmark datasets even better fancy graph neural network base methods release code https githubcom andrewzhe three sentence need
among ubiquitous multimodal data real world text modality generate human image reflect physical world honestly visual understand application machine expect understand image like human inspire propose novel self supervise learn method name text enhance visual deep infomax tvdim learn better visual representations fully utilize naturally exist multimodal data core idea self supervise learn maximize mutual information feature extract multiple view share context rational degree different previous methods consider multiple view single modality work produce multiple view different modalities jointly optimize mutual information feature pair intra modality inter modality consider information gap inter modality feature pair data noise adopt emphranking base contrastive learn optimize mutual information evaluation directly use pre train visual representations complete various image classification task experimental result show tvdim significantly outperform previous visual self supervise methods process set image
recent study strive incorporate various human rationales neural network improve model performance pay attention quality rationales exist methods distribute model focus distantly label rationale word entirely equally ignore potential important non rationale word distinguish importance different rationale word paper propose two novel auxiliary loss function make better use distantly label rationales encourage model maintain focus important word beyond label rationales pin alleviate redundant train non helpful rationales noirs experiment two representative classification task show propose methods push classification model effectively learn crucial clue non perfect rationales maintain ability spread focus unlabeled important word thus significantly outperform exist methods
frustratingly fragile nature neural network model make current natural language generation nlg systems prone backdoor attack generate malicious sequence could sexist offensive unfortunately little effort invest backdoor attack affect current nlg model defend attack work investigate problem two important nlg task machine translation dialogue generation give formal definition backdoor attack defense develop correspond benchmarks design methods attack nlg model achieve high attack success ask nlg model generate malicious sequence defend attack propose detect attack trigger examine effect delete replace certain word generation output find successful certain type attack discuss limitation work hope work raise awareness backdoor risk conceal deep nlg systems code data available https githubcom shannonai backdoornlg
paper present conceptually simple empirically powerful framework abstractive summarization simcls bridge gap learn objective evaluation metrics result currently dominate sequence sequence learn framework formulate text generation reference free evaluation problem ie quality estimation assist contrastive learn experimental result show minor modification exist top score systems simcls improve performance exist top perform model large margin particularly two hundred and fifty-one absolute improvement bart two hundred and fifty pegasus wrt rouge one cnn dailymail dataset drive state art performance new level open source cod result https githubcom yixinl7 simcls result propose model deploy explainaboard platform allow researchers understand systems fine grain way
exploitation syntactic graph sygs word context show beneficial distributional semantic model dsms level individual word representations derive phrasal representations via composition however notwithstanding potential performance benefit syntactically aware dsms propose date huge number parameters compare conventional dsms suffer data sparsity furthermore encode syg link ie syntactic relations largely limit linear map knowledge graph literature hand propose light weight model employ different geometric transformations gts encode edge knowledge graph kg work explore possibility adopt family model encode sygs furthermore investigate gt better encode syntactic relations representations use enhance phrase level composition via syntactic contextualisation
multi intent slu handle multiple intents utterance attract increase attention however state art joint model heavily rely autoregressive approach result two issue slow inference speed information leakage paper explore non autoregressive model joint multiple intent detection slot fill achieve fast accurate specifically propose global locally graph interaction network gl gin local slot aware graph interaction layer propose model slot dependency alleviate uncoordinated slot problem global intent slot graph interaction layer introduce model interaction multiple intents slot utterance experimental result two public datasets show framework achieve state art performance one hundred and fifteen time faster
pursuit natural language understand long stand interest track state change throughout narratives impressive progress make model state transaction centric dialogues procedural texts however problem less intensively study realm general discourse grind truth descriptions state may loosely define state change less densely distribute utterances paper propose turn simplify fully observable systems show properties sport events curated two thousand, two hundred and sixty-three soccer match include time stamp natural language commentary accompany discrete events team score goals switch players penalize card propose new task formulation give paragraph commentary game different timestamps system ask recognize occurrence game events domain allow rich descriptions state avoid complexities many real world settings initial point performance measurement include two baseline methods perspectives sentence classification temporal dependence current state art generative model respectively demonstrate even sophisticate exist methods struggle state track task definition state broaden non event chatter become prevalent
poetry one important art form human languages recently many study focus incorporate linguistic feature poetry style sentiment understand generation system however focus understand evaluate semantics poetry therefore propose novel task assess model semantic understand poetry poem match specifically task require model select one line chinese classical poetry among four candidates accord modern chinese translation line poetry construct dataset first obtain set parallel data chinese classical poetry modern chinese translation retrieve similar line poetry line poetry corpus negative choices name dataset chinese classical poetry match dataset ccpm release https githubcom thunlp aipoet ccpm hope dataset enhance study incorporate deep semantics understand generation system chinese classical poetry also preliminarily run two variants bert dataset baselines dataset
sentiment analysis text mine task determine polarity give text ie positiveness negativeness recently receive lot attention give interest opinion mine micro blogging platforms new form textual expressions present new challenge analyze text give use slang orthographic grammatical errors among others along challenge practical sentiment classifier able handle efficiently large workloads aim research identify text transformations lemmatization stem entity removal among others tokenizers eg word n grams tokens weight scheme impact accuracy classifier support vector machine train two spanish corpus methodology use exhaustively analyze combinations text transformations respective parameters find characteristics best perform classifiers common furthermore among different text transformations study introduce novel approach base combination word base n grams character base q grams result show novel combination word character produce classifier outperform traditional word base combination one thousand, one hundred and seventeen five hundred and sixty-two inegi tass fifteen dataset respectively
review efficientqa competition neurips two thousand and twenty competition focus open domain question answer qa systems take natural language question input return natural language answer aim competition build systems predict correct answer also satisfy strict disk memory budget memory budget design encourage contestants explore trade store large redundant retrieval corpora parameters large learn model report describe motivation organization competition review best submissions analyze system predictions inform discussion evaluation open domain qa
objectiveelectronic medical record emrs contain clinical narrative text great potential value medical researchers however information mix protect health information phi present risk patient clinician confidentiality paper present end end de identification framework automatically remove phi hospital discharge summaries materials methodsour corpus include six hundred hospital discharge summaries extract emrs two principal referral hospitals sydney australia end end de identification framework consist three components one annotation label phi six hundred hospital discharge summaries use five pre define categories person address date birth individual identification number phone fax number two model train evaluate ensembles name entity recognition ner model use three natural language process nlp toolkits stanza flair spacy balance imbalanced datasets three de identification remove phi hospital discharge summaries resultsthe final model framework ensemble combine six single model use balance imbalanced datasets train majority vote achieve nine thousand, eight hundred and sixty-six precision nine thousand, eight hundred and sixty-two recall nine thousand, eight hundred and sixty-four f1 score majority false positives false negative relate person category discussionour study show ensemble different model train use three different nlp toolkits upon balance imbalanced datasets achieve good result even relatively small corpus conclusionour end end framework provide robust solution de identify clinical narrative corpuses safely easily apply kind clinical narrative document
commonsense knowledge crucial artificial intelligence systems understand natural language previous commonsense knowledge acquisition approach typically rely human annotations example atomic text generation model example comet human annotation could provide high quality commonsense knowledge yet high cost often result relatively small scale low coverage hand generation model potential automatically generate knowledge nonetheless machine learn model often fit train data well thus struggle generate high quality novel knowledge address limitations previous approach paper propose alternative commonsense knowledge acquisition framework disco discourse commonsense automatically populate expensive complex commonsense knowledge affordable linguistic knowledge resources experiment demonstrate successfully convert discourse knowledge eventualities aser large scale discourse knowledge graph commonsense knowledge define atomic without additional annotation effort study suggest disco significantly outperform previous supervise approach term novelty diversity comparable quality total acquire 34m atomic like inferential commonsense knowledge populate atomic core part aser cod data available https githubcom hkust knowcomp disco commonsense
date recent work retrieval reader framework open domain qa focus either extractive generative reader exclusively paper study hybrid approach leverage strengths model apply novel techniques enhance extractive generative readers build upon recent pretrained neural language model find proper train methods provide large improvement previous state art model demonstrate simple hybrid approach combine answer readers efficiently take advantage extractive generative answer inference strategies outperform single model well homogeneous ensembles approach outperform previous state art model thirty-three twenty-seven point exact match naturalquestions triviaqa respectively
advent transformer arguably describe drive force behind many recent advance natural language process however despite sizeable performance improvements recently show model severely parameterized parameter inefficient computationally expensive train inspire success parameter share pretrained deep contextualized word representation encoders explore parameter share methods transformers specific focus encoder decoder model sequence sequence task neural machine translation perform analysis different parameter share reduction methods develop subformer parameter efficient transformer base model combine newly propose sandwich style parameter share technique design overcome deficiencies naive cross layer parameter share generative model self attentive embed factorization safe experiment machine translation abstractive summarization language model show subformer outperform transformer even use significantly fewer parameters
semantic parse task convert natural language utterances machine understandable mean representations logic form program languages train datasets semantic parse typically small due higher expertise require annotation nlp task result model application usually require additional prior knowledge build architecture algorithm increase dependency human experts hinder automation raise development maintenance cost practice work investigate whether generic transformer base seq2seq model achieve competitive performance minimal semantic parse specific inductive bias design exploit relatively large monolingual corpus target program language cheap mine web unlike parallel corpus achieve eight thousand and seventy-five exact match accuracy django three thousand, two hundred and fifty-seven bleu score conala sota best knowledge positive evidence highlight potentially easier path toward build accurate semantic parsers wild
riddle mystify puzzle question everyday concepts example riddle five finger alive ask concept glove solve riddle challenge cognitive process humans require complex commonsense reason abilities understand figurative language however currently commonsense reason datasets test abilities propose riddlesense novel multiple choice question answer challenge benchmarking higher order commonsense reason model first large dataset riddle style commonsense question answer distractors crowdsourced human annotators systematically evaluate wide range reason model point large gap best supervise model human performance point interest future research higher order commonsense reason computational creativity
adoption natural language generation nlg model leave individuals vulnerable generation harmful information memorize model conspiracy theories previous study examine conspiracy theories context social media evaluate presence new space generative language model work investigate capability language model generate conspiracy theory text specifically aim answer test pretrained generative language model memorization elicitation conspiracy theories without access model train data highlight difficulties task discuss context memorization generalization hallucination utilize new dataset consist conspiracy theory topics machine generate conspiracy theories help us discover many conspiracy theories deeply root pretrained language model experiment demonstrate relationship model parameters size temperature propensity generate conspiracy theory text result indicate need thorough review nlg applications release depth discussion drawbacks memorization generative language model
introduce voxpopuli large scale multilingual corpus provide 100k hours unlabelled speech data twenty-three languages largest open data date unsupervised representation learn well semi supervise learn voxpopuli also contain 18k hours transcribe speeches sixteen languages align oral interpretations five languages total 51k hours provide speech recognition baselines validate versatility voxpopuli unlabelled data semi supervise learn challenge domain settings release corpus https githubcom facebookresearch voxpopuli open license
recent work train neural retrievers open domain question answer openqa employ supervise unsupervised approach however remain unclear unsupervised supervise methods use effectively neural retrievers work systematically study retriever pre train first propose approach unsupervised pre train inverse cloze task mask salient span follow supervise finetuning use question context pair approach lead absolute gain two point previous best result top twenty retrieval accuracy natural question triviaqa datasets also explore two approach end end supervise train reader retriever components openqa model first approach reader consider retrieve document separately second approach reader consider retrieve document together experiment demonstrate effectiveness approach obtain new state art result natural question dataset obtain top twenty retrieval accuracy eighty-four improvement five point recent dpr model addition achieve good result answer extraction outperform recent model like realm rag three point scale end end train large model show consistent gain performance smaller model
pre train text text transformers achieve impressive performance across wide range nlp task naturally support zero shoot learn zsl use task description prompt input however approach potential limitations learn input output pair instance level instead learn solve task task level alternatively apply exist zsl methods text text transformers non trivial due text generation objective huge size address issue introduce hypter framework improve zero shoot transferability train hypernetwork generate task specific adapters task descriptions formulation enable learn task level greatly reduce number parameters use light weight adapters experiment two datasets demonstrate hypter improve upon fine tune baselines
multi hop reason ie reason across two document key ingredient nlp model leverage large corpora exhibit broad knowledge retrieve evidence passages multi hop model must contend fast grow search space across hop represent complex query combine multiple information need resolve ambiguity best order hop train passages tackle problems via baleen system improve accuracy robustness multi hop retrieval tame search space propose condense retrieval pipeline summarize retrieve passages hop single compact context model complex query introduce focus late interaction retriever allow different part query representation match disparate relevant passages lastly infer hop dependencies among unordered train passages devise latent hop order weak supervision strategy train retriever select sequence hop evaluate baleen retrieval two hop question answer many hop claim verification establish state art performance
many state art neural model design monotonicity reason perform poorly downward inference address shortcoming develop attentive tree structure neural network consist tree base long short term memory network tree lstm soft attention design model syntactic parse tree information sentence pair reason task self attentive aggregator use align representations premise hypothesis present model evaluate use monotonicity entailment dataset med show attempt explain model outperform exist model med
recurrent neural network rnns encode information suboptimal erroneous way impact quality representations base later elements sequence subsequently lead wrong predictions worse model performance humans challenge case like garden path sentence instance infamous horse race past barn fell lead language understand astray however still able correct representation accordingly recover new information encounter inspire propose augmentation standard rnns form gradient base correction mechanism way hope enable model dynamically adapt inner representation sentence add way correct deviations soon occur could therefore lead robust model use flexible representations even inference time conduct different experiment context language model impact use mechanism examine detail end look modifications base different kinds time dependent error signal influence model performance furthermore work contain study model confidence predictions train challenge test sample effect manipulation thereof lastly also study difference behavior novel model compare standard lstm baseline investigate error case detail identify point future research show propose approach come promise theoretical guarantee appeal intuition able produce minor improvements baseline due challenge practical application efficacy test model variants
various neural base methods propose far joint mention detection coreference resolution however exist work coreference resolution mainly dependent filter mention representation span largely neglect paper aim increase utilization rate data investigate whether eliminate span totally useless extent improve performance coreference resolution achieve propose mention representation refine strategy span highly relate mention well leverage use pointer network representation enhance notably utilize additional loss term work encourage diversity entity cluster experimental result document level conll two thousand and twelve share task english dataset show eliminate span indeed much effective approach achieve competitive result compare previous state art coreference resolution
aspect base sentiment analysis absa involve three fundamental subtasks aspect term extraction opinion term extraction aspect level sentiment classification early work focus solve one subtasks individually recent work focus solve combination two subtasks eg extract aspect term along sentiment polarities extract aspect opinion term pair wisely recently triple extraction task propose ie extract aspect term opinion term sentiment polarity triple sentence however previous approach fail solve subtasks unify end end framework paper propose complete solution absa construct two machine read comprehension mrc problems solve subtasks joint train two bert mrc model parameters share conduct experiment subtasks result several benchmark datasets demonstrate effectiveness propose framework significantly outperform exist state art methods
read write research paper one privilege abilities qualify researcher master however difficult new researchers egstudents fully grasp ability would fascinate could train intelligent agent help people read summarize paper perhaps even discover exploit potential knowledge clue write novel paper although exist work focus summarize emphie read knowledge give text generate emphie write text base give knowledge ability simultaneously read write still development typically require agent fully understand knowledge give text materials generate correct fluent novel paragraph challenge practice paper propose deep reader writer draw network consist textitreader extract knowledge graph kgs input paragraph discover potential knowledge graph text textitwriter generate novel paragraph textitreviewer review generate paragraph three different aspects extensive experiment show draw network outperform consider baselines several state art methods agenda agenda datasets code supplementary release https githubcom menggehe draw
recent years conversational recommender system crs receive much attention research community however exist study crs vary scenarios goals techniques lack unify standardize implementation comparison tackle challenge propose open source crs toolkit crslab provide unify extensible framework highly decouple modules develop crss base framework collect six commonly use human annotate crs datasets implement eighteen model include recent techniques graph neural network pre train model besides toolkit provide series automatic evaluation protocols human machine interaction interface test compare different crs methods project document release https githubcom rucaibox crslab
paper address challenge extract scientific reference patent approach problem sequence label task investigate merit bert model extraction long sequence reference patent scientific literature relevant study connection science industry prior work use front page citations analysis provide metadata patent archive paper build prior work use conditional random field crf flair reference extraction improve quality train data train three bert base model label data bert biobert scibert find improve train data lead large improvement quality train model addition bert model beat crf flair recall score around ninety-seven obtain cross validation best model label large collection thirty-three thousand patent extract citations match publications web science database extract fifty reference old train data methods seven hundred and thirty-five thousand reference total patent publication link follow research analyze type scientific work lead inventions
key component deep learn dl natural language process nlp word embeddings word embeddings effectively capture mean context word represent significantly improve performance downstream dl model various nlp task many exist word embeddings techniques capture context word base word co occurrence document text however often capture broader domain specific relationships concepts may crucial nlp task hand paper propose method integrate external knowledge medical terminology ontologies context capture word embeddings specifically use medical knowledge graph unify medical language system umls find connections clinical term cancer pathology report approach aim minimize distance connect clinical concepts evaluate propose approach use multitask convolutional neural network mt cnn extract six cancer characteristics site subsite laterality behavior histology grade dataset 900k cancer pathology report result show mt cnn model use domain inform embeddings outperform mt cnn use standard word2vec embeddings across task improvement overall micro macro f1 score 497and two hundred and twenty-five respectively
entity alignment ea task identify entities refer real world object locate different knowledge graph kgs entities align exist ea solutions treat separately generate alignment result rank list entities side nevertheless decision make paradigm fail take account interdependence among entities although recent efforts mitigate issue impose one one constraint alignment process still adequately model underlie interdependence result tend sub optimal fill gap work delve dynamics decision make process offer reinforcement learn rl base model align entities collectively rl framework devise coherence exclusiveness constraints characterize interdependence restrict collective alignment additionally generate precise input rl framework employ representative feature capture different aspects similarity entities heterogeneous kgs integrate adaptive feature fusion strategy proposal evaluate cross lingual mono lingual ea benchmarks compare state art solutions empirical result verify effectiveness superiority
political polarization us rise polarization negatively affect public sphere contribute creation ideological echo chamber paper focus address one factor contribute polarity polarize media introduce framework depolarize news article give article certain topic particular ideological slant eg liberal conservative framework first detect polar language article generate new article polar language replace neutral expressions detect polar word train multi attribute aware word embed model aware ideology topics 360k full length media article text generation propose new algorithm call text anneal depolarization algorithm tada tada retrieve neutral expressions word embed model decrease ideological polarity also preserve original argument text maintain grammatical correctness evaluate framework compare depolarize output model two modes fully automatic semi automatic ninety-nine stories span eleven topics base feedback one hundred and sixty-one human testers framework successfully depolarize nine hundred and one paragraph semi automatic mode seven hundred and eighty-three paragraph fully automatic mode furthermore eight hundred and twelve testers agree non polar content information well preserve seventy-nine agree depolarization harm semantic correctness compare original text depolarize text work show data drive methods help locate political polarity aid depolarization article
take advantage computationally lightweight high quality translators prompt consideration new applications address neglect languages locally run translators less popular languages may assist data project protect personal data may require specific compliance check post public translation api could render reasonable cost effective solutions do army local small scale pair translators like handle specialist dialect research illustrate translate two historically interest obfuscate languages one hacker speak l33t two reverse mirror write practice leonardo da vinci work generalize deep learn architecture translatable variants hacker speak lite medium hard vocabularies original contribution highlight fluent translator hacker speak fifty megabytes demonstrate generator augment future datasets greater million bilingual sentence pair long short term memory recurrent neural network lstm rnn extend previous work demonstrate english foreign translation service build little ten thousand bilingual sentence pair work solve equivalent translation problem twenty six additional non obfuscate languages rank order model proficiency quantitatively italian successful mandarin chinese challenge neglect languages method prototypes novel service smaller niche translations kabyle algerian dialect cover five seven million speakers one enterprise translators yet reach development one anticipate extension approach important dialects translate technical medical legal jargon process health record
small screenshots large videos document take bulk space modern smartphone document phone accumulate various source high storage capacity mobiles hundreds document accumulate short period however search manage document remain onerous task since search methods depend meta information text document paper showcase single modality insufficient classification present novel pipeline classify document device thus prevent private user data transfer server task integrate open source library optical character recognition ocr novel model architecture pipeline optimise model size necessary metric device inference benchmark classification model standard multimodal dataset food one hundred and one showcase competitive result previous state art thirty model compression
knowledge formal way understand world provide human level cognition intelligence next generation artificial intelligence ai one representations knowledge semantic relations entities effective way automatically acquire important knowledge call relation extraction sub task information extraction play vital role natural language process nlp purpose identify semantic relations entities natural language text date several study previous work document techniques base deep neural network dnns become prevail technique research especially supervise distant supervision methods base dnns popular reliable solutions article one introduce general concepts two give comprehensive overview dnns two point view supervise attempt improve standard systems distant supervision adopt dnns design sentence encoder de noise method three cover novel methods recent trend well discuss possible future research directions task
although open domain question answer qa draw great attention recent years require large amount resources build full system often difficult reproduce previous result due complex configurations paper introduce sf qa simple fair evaluation framework open domain qa sf qa framework modularizes pipeline open domain qa system make task easily accessible reproducible research group without enough compute resources propose evaluation framework publicly available anyone contribute code evaluations
give simple low resource method produce order embeddings ontologies embeddings map word vectors order relations word hypernymy hyponymy represent direct way method use sketch techniques particular countsketch dimensionality reduction also study methods merge ontologies particular medical domains order relations preserve give computational result medical ontologies wordnet show merge techniques effective embed yield accurate representation generic specialise domains
search one common platforms use seek information however users mostly get overload result whenever use platform resolve query nowadays direct answer query provide part search experience question answer qa retrieval process play significant role enrich search experience shelf semantic textual similarity model work fine well form search query performances degrade apply domain specific set incomplete grammatically ill form search query prevalence paper discuss framework calculate similarities give input query set predefined question retrieve question match use financial domain framework generalize domain specific search engine use domains well use siamese network six long short term memory lstm three model train classifier generate unnormalized normalize similarity score give pair question moreover question pair calculate three similarity score cosine similarity average word2vec embeddings fifteen cosine similarity sentence embeddings seven generate use roberta seventeen customize fuzzy match score finally develop metaclassifier use support vector machine nineteen combine five score detect give pair question similar benchmark model performance exist state art sota model quora question pair qqp dataset well dataset specific financial domain
work propose novel knowledge graph embed kge strategy call mofbiuse entities relations embed surface mofbius ring proposition strategy inspire classic toruse addition two arbitrary elements subject modulus operation sense toruse naturally guarantee critical boundedness embed vectors kge however nonlinear property addition operation torus ring uniquely derive modulus operation extent restrict expressiveness toruse generalization toruse mofbiuse also use modulus operation preserve closeness addition operation coordinate mofbius ring interact follow way colorred vector surface mofbius ring move along parametric trace go right opposite direction cycle hence mofbiuse assume much nonlinear representativeness toruse turn generate much precise embed result experiment mofbiuse outperform toruse classic embed strategies several key indicators
homonym identification important wsd require coarse grain partition sense goal project determine whether contextual information sufficient identify homonymous word capture context bert embeddings use oppose word2vec conflate sense one vector semcor leverage retrieve embeddings various cluster algorithms apply embeddings finally embeddings visualize lower dimensional space understand feasibility cluster process
steady need precisely extract structure knowledge web ie html document give web page extract structure object along various attribute interest eg price publisher author genre book facilitate variety downstream applications large scale knowledge base construction e commerce product search personalize recommendation consider web page render html dom tree exist approach formulate problem dom tree node tag task however either rely computationally expensive visual feature engineer incapable model relationship among tree nod paper propose novel transferable method simplify dom tree attribute extraction simpdom tackle problem efficiently retrieve useful context node leverage tree structure study two challenge experimental settings intra vertical shoot extraction ii cross vertical fewshot extraction domain knowledge evaluate approach extensive experiment swde public dataset show simpdom outperform state art sota method one hundred and forty-four f1 score also find utilize knowledge different vertical cross vertical extraction surprisingly useful help beat sota one hundred and thirty-seven
social media twitter facebook etc lead generate grow number comment contain users opinions sentiment analysis research deal comment extract opinions positive negative arabic language rich morphological language thus classical techniques english sentiment analysis use arabic word embed technique consider one successful methods gap morphological problem arabic many work do arabic sentiment analysis base word embed study focus variable parameters study discuss three parameters window size dimension vector negative sample arabic sentiment analysis use dbow dmpv architectures large corpus previous work generate learn word representations extract feature four binary classifiers logistic regression decision tree support vector machine naive bay use detect sentiment performance classifiers evaluate base precision recall f1 score
graph structure powerful tool model relationships textual elements graph word gow adopt many natural language task encode association term however gow provide document level relationships case connections document also essential identify sub events social media like twitter feature word document level useful supply different information event propose hybrid graph tweet get model combine word document level structure model tweet compress large amount raw data propose graph merge method utilize fasttext word embeddings reduce gow furthermore present novel method construct get reduce gow mutual information mi measure finally identify maximal cliques extract popular sub events model show promise result condense lexical level information capture keywords sub events
increase content availability internet difficult get notice become upmost priority blog writers get feedback creations confident impact article train machine learn model learn popular article style form vector space representations use various word embeddings popularity base clap tag
data augmentation widely use improve deep neural network many research field computer vision however less work do context text partially due discrete nature complexity natural languages paper propose improve standard maximum likelihood estimation mle paradigm incorporate self imitation learn phase automatic data augmentation unlike exist sentence level augmentation strategies apply specific model method general could easily adapt mle base train procedure addition framework allow task specific evaluation metrics design flexibly control generate sentence example term control vocabulary usage avoid nontrivial repetitions extensive experimental result demonstrate superiority method two synthetic several standard real datasets significantly improve relate baselines
beyond generate long topic coherent paragraph traditional caption task medical image report composition task pose task orient challenge require highly accurate medical term diagnosis multiple heterogeneous form information include impression find current methods often generate common sentence due dataset bias individual case regardless whether sentence properly capture key entities relationships limitations severely hinder applicability generalization capability medical report composition critical sentence lie descriptions abnormal diseases relatively rare moreover medical term appear one report often entangle co occur eg symptoms associate specific disease enforce semantic consistency medical term incorporate final report encourage sentence generation rare abnormal descriptions propose novel framework unify template retrieval sentence generation handle common rare abnormality ensure semantic coherency among detect medical term specifically approach exploit hybrid knowledge co reason explicit relationships among abnormal medical term induce visual attention learn topic representation encode better topic orient symptoms descriptions ii adaptive generation mode change template retrieval sentence generation accord contextual topic encoder experimental result two medical report benchmarks demonstrate superiority propose framework term human metrics evaluation
extreme multi label text classification xmc task find relevant label large label set nowadays deep learn base methods show significant success xmc however exist methods eg attentionxml x transformer etc still suffer one combine several model train predict one dataset two sample negative label statically process train label rank model reduce efficiency accuracy model address problems propose lightxml adopt end end train dynamic negative label sample lightxml use generative cooperative network recall rank label label recall part generate negative positive label label rank part distinguish positive label label network negative label sample dynamically label rank part train feed text representation extensive experiment show lightxml outperform state art methods five extreme multi label datasets much smaller model size lower computational complexity particular amazon dataset 670k label lightxml reduce model size seventy-two compare attentionxml
paper study response large model bert family incoherent input confuse model claim understand natural language define simple heuristics construct examples experiment show state art model consistently fail recognize ill form instead produce high confidence predictions consequence phenomenon model train sentence randomly permute word order perform close state art model alleviate issue show model explicitly train recognize invalid input robust attack without drop performance
work base submission competition hindi constraint conduct aaai2021 detection hostile post hindi social media platforms model present detection classification hostile post classify fake offensive hate defamation use relational graph convolutional network unlike exist work approach focus use semantic mean along contextutal information better classification result aaai2021 indicate propose model perform par google xlm roberta give dataset best submission rgcn achieve f1 score ninety-seven 7th rank coarse grain evaluation achieve best performance identify fake post among submissions challenge classification system xlm roberta secure 2nd rank fine grain classification
significance social media increase manifold past decades help people even remote corner world stay connect covid nineteen pandemic rag social media become relevant widely use ever along resurgence circulation fake news tweet demand immediate attention paper describe fake news detection system automatically identify whether tweet relate covid nineteen real fake part constraint covid19 fake news detection english challenge use ensemble model consist pre train model help us achieve joint 8th position leader board achieve f1 score nine thousand, eight hundred and thirty-one top score nine thousand, eight hundred and sixty-nine post completion competition able drastically improve system incorporate novel heuristic algorithm base username handle link domains tweet fetch f1 score nine thousand, eight hundred and eighty-three achieve state art result give dataset
multi hop knowledge base question answer kbqa aim find answer entities multiple hop away knowledge base kb entities question major challenge lack supervision signal intermediate step therefore multi hop kbqa algorithms receive feedback final answer make learn unstable ineffective address challenge propose novel teacher student approach multi hop kbqa task approach student network aim find correct answer query teacher network try learn intermediate supervision signal improve reason capacity student network major novelty lie design teacher network utilize forward backward reason enhance learn intermediate entity distributions consider bidirectional reason teacher network produce reliable intermediate supervision signal alleviate issue spurious reason extensive experiment three benchmark datasets demonstrate effectiveness approach kbqa task code reproduce analysis available https githubcom richardhgl wsdm2021nsm
real life applications heavily rely machine learn dialog systems demand domain detection methods intent classification model equip mechanism distinguish see intents unseen ones dialog agent capable reject latter avoid undesired behavior however despite increase attention pay task best practice domain intent detection yet fully establish paper conduct thorough comparison domain intent detection methods prioritize methods require access domain data train gather extremely time labor consume due lexical stylistic variation user utterances evaluate multiple contextual encoders methods prove efficient three standard datasets intent classification expand domain utterances main find show fine tune transformer base encoders domain data lead superior result mahalanobis distance together utterance representations derive transformer base encoders outperform methods wide margin establish new state art result datasets broader analysis show reason success lie fact fine tune transformer capable construct homogeneous representations domain utterances reveal geometrical disparity domain utterances turn mahalanobis distance capture disparity easily
amid pandemic covid nineteen world face unprecedented infodemic proliferation fake real information consider problematic consequences covid nineteen fake news bring scientific community put effort tackle contribute fight infodemic aim achieve robust model covid nineteen fake news detection task propose constraint two thousand and twenty-one fakenews nineteen take two separate approach one fine tune transformers base language model robust loss function two remove harmful train instance influence calculation evaluate robustness model evaluate different covid nineteen misinformation test set tweet nineteen understand model generalization ability first approach achieve nine thousand, eight hundred and thirteen weight f1 score w f1 share task whereas three thousand, eight hundred and eighteen w f1 tweet nineteen highest contrary perform influence data cleanse model ninety-nine cleanse percentage achieve five thousand, four hundred and thirty-three w f1 score tweet nineteen trade evaluate model two covid nineteen fake news test set suggest importance model generalization ability task step forward tackle covid nineteen fake news problem online social media platforms
identification fake news play prominent role ongoing pandemic impact multiple aspects day day life work present solution share task title covid19 fake news detection english score 50th place amongst one hundred and sixty-eight submissions solution within fifteen best perform solution propose solution employ heterogeneous representation ensemble adapt classification task via additional neural classification head comprise multiple hide layer paper consist detail ablation study display propose method behavior possible implications solution freely available urlhttps gitlabcom boshkokoloski covid19 fake news
due increase access clinical trial outcomes analysis researchers scientists able iterate improve upon relevant approach effectively however metrics relate result clinical trials typically follow standardization report make difficult researchers parse result different trials objective paper describe automate method utilize natural language process order describe probable core outcomes clinical trials order alleviate issue around disparate clinical trial outcomes nature process domain specific biobert employ order conduct multi class entity normalization task addition biobert unsupervised feature base approach make use encoder output embed representations outcomes label utilize finally cosine similarity calculate across vectors obtain semantic similarity method able harness domain specific context tokens learn embeddings biobert model well stable metric sentence similarity common outcomes identify use jaccard similarity classifications compile untenable pipeline automation process could conduct establish
social media platform convenient medium express personal thoughts share useful information fast concise ability reach millions effective place archive thoughts share artistic content receive feedback promote products etc despite numerous advantage platforms give boost hostile post hate speech derogatory remark post personal satisfaction political gain hostile post bully effect render entire platform experience hostile therefore detection hostile post important maintain social media hygiene problem pronounce languages like hindi low resources work present approach hostile text detection hindi language propose approach evaluate constraintaaai two thousand and twenty-one hindi hostility detection dataset dataset consist hostile non hostile texts collect social media platforms hostile post segregate overlap class fake offensive hate defamation evaluate host deep learn approach base cnn lstm bert multi label classification problem pre train hindi fast text word embeddings indicnlp facebook use conjunction cnn lstm model two variations pre train multilingual transformer language model mbert indicbert use show performance bert base model best moreover cnn lstm model also perform competitively bert base model
translate natural language query nlqs structure query language sql interfaces deploy relational databases challenge task widely study database community recently conventional rule base systems utilize series solutions pipeline deal step task namely stop word filter tokenization stem lemmatization parse tag translation recent work mostly focus translation step overlook earlier step use ad hoc solutions pipeline one critical challenge problems keyword map construct map tokens query relational database elements table attribute value etc define keyword map problem sequence tag problem propose novel deep learn base supervise approach utilize pos tag nlqs propose approach call textitdbtagger database tagger end end schema independent solution make practical various relational databases evaluate approach eight different datasets report new state art accuracy result nine hundred and twenty-four average result also indicate dbtagger faster counterparts ten thousand time scalable bigger databases
task orient conversation systems natural language generation systems generate sentence specific information relate conversation flow useful study focus language generation consider various information represent mean utterances multiple condition generation nlg mean representations condition sentence mean generally go two step sentence plan surface realization however propose simple one stage framework generate utterances directly mr mean representation model base gpt2 generate utterances flat condition slot value pair need determine structure sentence evaluate several systems e2e dataset six automatic metrics system simple method demonstrate comparable performance previous systems automate metrics addition use ten data set without techniques model achieve comparable performance show possibility perform zero shoot generation expand datasets
video sentiment analysis decision make process inherently complex involve fusion decisions multiple modalities cause cognitive bias inspire recent advance quantum cognition show sentiment judgment one modality could incompatible judgment another ie order matter jointly measure produce final decision thus cognitive process exhibit quantum like bias capture classical probability theories accordingly propose fundamentally new quantum cognitively motivate fusion strategy predict sentiment judgments particular formulate utterances quantum superposition state positive negative sentiment judgments uni modal classifiers mutually incompatible observables complex value hilbert space positive operator value measure experiment two benchmarking datasets illustrate model significantly outperform various exist decision level range state art content level fusion approach result also show concept incompatibility allow effective handle combination pattern include extreme case wrongly predict uni modal classifiers
work provide new insights transformer architecture particular best know variant bert first propose method measure degree non linearity different elements transformers next focus investigation fee forward network ffn inside transformers contain two three model parameters far receive much attention find ffns inefficient yet important architectural element simply replace attention block without degradation performance moreover study interactions layer bert show layer exhibit hierarchical structure extract feature fuzzy manner result suggest bert inductive bias towards layer commutativity find mainly due skip connections provide justification strong performance recurrent weight share transformer model
propose novel alignment mechanism deal procedural reason newly release multimodal qa dataset name recipeqa model solve textual cloze task read comprehension recipe contain image instructions exploit power attention network cross modal representations latent alignment space instructions candidate answer solve problem introduce constrain max pool refine max pool operation alignment matrix impose disjoint constraints among output model evaluation result indicate nineteen improvement baselines
marathi language one prominent languages use india predominantly speak people maharashtra past decade usage language online platforms tremendously increase however research natural language process nlp approach marathi text receive much attention marathi morphologically rich language use variant devanagari script write form work aim provide comprehensive overview available resources model marathi text classification evaluate cnn lstm ulmfit bert base model two publicly available marathi text classification datasets present comparative analysis pre train marathi fast text word embeddings facebook indicnlp use conjunction word base model show basic single layer model base cnn lstm couple fasttext embeddings perform par bert base model available datasets hope paper aid focus research experiment area marathi nlp
paper present deep learn model layer differentiate train method use share task constraint two thousand and twenty-one sub task covid19 fake news detection english hostile post detection hindi propose layer differentiate train procedure train pre train ulmfit arxiv180106146 model use special tokens annotate specific part tweet improve language understand gain insights model make tweet interpretable two submissions include modify roberta model simple random forest classifier propose approach score precision f1 score ninety-six million, seven hundred and twenty-eight thousand, nine hundred and seventy-two nine hundred and sixty-seven million, three hundred and twenty-four thousand, eight hundred and thirty-two respectively sub task covid19 fake news detection english also coarse grain hostility f1 score weight finegrained f1 score nine hundred and eight thousand, six hundred and forty-eight five hundred and thirty-three thousand, nine hundred and seven respectively sub task hostile post detection hindi propose approach rank 61st one hundred and sixty-four sub task covid19 fake news detection english 18th forty-five sub task hostile post detection hindi
consider problem multi label classification label lie hierarchy however unlike exist work hierarchical multi label classification assume label hierarchy know encourage recent success hyperbolic embeddings capture hierarchical relations propose jointly learn classifier parameters well label embeddings joint learn expect provide twofold advantage classifier generalize better leverage prior knowledge existence hierarchy label ii addition label co occurrence information label embed may benefit manifold structure input datapoints lead embeddings faithful label hierarchy propose novel formulation joint learn empirically evaluate efficacy result show joint learn improve baseline employ label co occurrence base pre train hyperbolic embeddings moreover propose classifiers achieve state art generalization standard benchmarks also present evaluation hyperbolic embeddings obtain joint learn show represent hierarchy accurately alternatives
social media platforms vulnerable fake news dissemination cause negative consequences panic wrong medication healthcare domain therefore important automatically detect fake news early stage get widely spread paper analyze impact incorporate content information prior knowledge credibility source model early detection fake news propose framework model feature use bert language model external source namely simple english wikipedia source reliability tag conduct experiment constraint datasets demonstrate benefit integrate feature early detection fake news healthcare domain
pandemic covid nineteen relevant fake news spread sky throughout social media believe without discrimination great trouble people life however universal language model may perform weakly fake news detection lack large scale annotate data sufficient semantic understand domain specific knowledge model train correspond corpora also mediocre insufficient learn paper propose novel transformer base language model fine tune approach fake news detection first token vocabulary individual model expand actual semantics professional phrase second adapt heat softmax loss distinguish hard mine sample common fake news disambiguation short text involve adversarial train improve model robustness last predict feature extract universal language model roberta domain specific model ct bert fuse one multiple layer perception integrate fine grain high level specific representations quantitative experimental result evaluate exist covid nineteen fake news dataset show superior performances compare state art methods among various evaluation metrics furthermore best weight average f1 score achieve nine thousand, nine hundred and two
entity link fundamental task natural language process deal lexical ambiguity texts important component entity link approach mention entity prior probability even though large number work entity link exist approach explicitly consider time aspect specifically temporality entity prior probability posit prior probability temporal nature affect performance entity link systems paper systematically study effect prior entity link performance temporal validity texts kbs
entity link el task automatically identify entity mention text resolve correspond entity reference knowledge base like wikipedia throughout past decade plethora el systems pipelines become available performance individual systems vary heavily across corpora languages domains link performance vary even different mention text corpus instance el approach better able deal short surface form others may perform better context information available end argue performance may optimise exploit result distinct el systems corpus thereby leverage individual strengths per mention basis paper introduce supervise approach exploit output multiple ready make el systems predict correct link per mention basis experimental result obtain exist grind truth datasets exploit three state art el systems show effectiveness approach capacity significantly outperform individual el systems well set baseline methods
twitter heavily use important channel communicate discuss events real time major events many uninformative tweet also publish rapidly many users make hard follow events paper address problem investigate machine learn methods automatically identify informative tweet among relevant target event examine traditional approach rich set handcraft feature state art approach automatically learn feature propose hybrid model leverage handcraft feature automatically learn ones experiment several large datasets real world events show latter approach significantly outperform former propose model perform best suggest highly effective mechanisms track mass events
propose design pattern tackle text rank problems dub expando mono duo empirically validate number ad hoc retrieval task different domains core design rely pretrained sequence sequence model within standard multi stage rank architecture expando refer use document expansion techniques enrich keyword representations texts prior invert index mono duo refer components reranking pipeline base pointwise model pairwise model rerank initial candidates retrieve use keyword search present experimental result ms marco passage document rank task trec two thousand and twenty deep learn track trec covid challenge validate design task achieve effectiveness near state art case use zero shoot approach exploit train data target task support replicability implementations design pattern open source pyserini ir toolkit pygaggle neural reranking library
propose new framework translation augment natural languages tanl solve many structure prediction language task include joint entity relation extraction nest name entity recognition relation classification semantic role label event extraction coreference resolution dialogue state track instead tackle problem train task specific discriminative classifiers frame translation task augment natural languages task relevant information easily extract approach match outperform task specific model task particular achieve new state art result joint entity relation extraction conll04 ade nyt ace2005 datasets relation classification fewrel tacred semantic role label conll two thousand and five conll two thousand and twelve accomplish use architecture hyperparameters task even train single model solve task time multi task learn finally show framework also significantly improve performance low resource regime thank better use label semantics
observe large scale language model capture undesirable societal bias eg relate race gender yet religious bias relatively unexplored demonstrate gpt three state art contextual language model capture persistent muslim violence bias probe gpt three various ways include prompt completion analogical reason story generation understand anti muslim bias demonstrate appear consistently creatively different use model severe even compare bias religious group instance muslim analogize terrorist twenty-three test case jewish map money five test case quantify positive distraction need overcome bias adversarial text prompt find use positive six adjectives reduce violent completions muslims sixty-six twenty still higher religious group
literature review focus use natural language generation nlg automatically detect generate persuasive texts extend previous research automatic identification persuasion text concentrate generative aspects conceptualize determinants persuasion five business focus categories benevolence linguistic appropriacy logical argumentation trustworthiness tool datasets allow nlg increase exist message persuasiveness previous research illustrate key aspects mention five categories research agenda study persuasive nlg develop review include analysis seventy seven article outline exist body knowledge show steady progress research field
truly real life data present strong excite challenge sentiment emotion research high variety possible wild properties make large datasets indispensable respect build robust machine learn model sufficient quantity data cover deep variety challenge modality force exploratory analysis interplay modalities yet make available context contribution present muse car first kind multimodal dataset data publicly available recently serve test bed 1st multimodal sentiment analysis challenge focus task emotion emotion target engagement trustworthiness recognition mean comprehensively integrate audio visual language modalities furthermore give thorough overview dataset term collection annotation include annotation tiers use year muse two thousand and twenty addition one sub challenge predict level trustworthiness participant outperform baseline model propose simple highly efficient multi head attention network exceed use multimodal fusion baseline around two ccc almost fifty improvement
dialog systems enrich external knowledge handle user query outside scope support databases apis paper follow baseline provide dstc9 track one propose three subsystems kdeak knowledgefactor ens gpt form pipeline task orient dialog system capable access unstructured knowledge specifically kdeak perform knowledge seek turn detection formulate problem natural language inference use knowledge dialogs databases faqs knowledgefactor accomplish knowledge selection task formulate factorize knowledge document retrieval problem three modules perform domain entity knowledge level analyse ens gpt generate response first process multiple knowledge snippets follow ensemble algorithm decide response solely derive gpt2 xl model regenerate combination top rank knowledge snippet experimental result demonstrate propose pipeline system outperform baseline generate high quality responses achieve least five thousand, eight hundred and seventy-seven improvement bleu four score
text style transfer gain increase attention research community recent years however propose approach vary many ways make hard assess individual contribution model components style transfer important component optimization technique use guide learn absence parallel train data work empirically compare dominant optimization paradigms provide supervision signal train backtranslation adversarial train reinforcement learn find backtranslation model specific limitations inhibit train style transfer model reinforcement learn show best performance gain adversarial train despite popularity offer advantage latter alternative work also experiment minimum risk train popular technique machine translation community knowledge empirically evaluate task style transfer fill research gap empirically show efficacy
text encoders base c dssm transformers demonstrate strong performance many natural language process nlp task low latency variants model also develop recent years order apply field sponsor search strict computational constraints however model panacea solve natural language understand nlu challenge pure semantic information data sufficient fully identify user intents propose textgnn model naturally extend strong twin tower structure encoders complementary graph information user historical behaviors serve natural guide help us better understand intents hence generate better language representations model inherit benefit twin tower model c dssm twinbert still use low latency environment achieve significant performance gain strong encoder counterpart baseline model offline evaluations online production system offline experiment model achieve fourteen overall increase roc auc one increase accuracy long tail low frequency ads online b test model show two hundred and three increase revenue per mille two hundred and thirty-two decrease ad defect rate
purpose project evaluate three language model name bert albert longformer question answer dataset call duorc language model task two input question context context paragraph entire document output answer base context goal perform grid search hyperparameter fine tune use duorc pretrained weight model take huggingface library different set hyperparameters use fine tune model use two versions duorc selfrc paraphraserc result show albert pretrained use squad1 dataset f1 score seven hundred and sixty-four accuracy score six thousand, eight hundred and fifty-two fine tune selfrc dataset longformer model pretrained use squad selfrc datasets f1 score five thousand, two hundred and fifty-eight accuracy score four thousand, six hundred and sixty fine tune paraphraserc dataset current result outperform result previous model duorc
model persuasive language potential better facilitate decision make process despite importance computational model persuasion still infancy largely due lack benchmark datasets provide quantitative label persuasive strategies expedite line research end introduce large scale multi domain text corpus model persuasive strategies good faith text request moreover design hierarchical weakly supervise latent variable model leverage partially label data predict associate persuasive strategies sentence supervision come overall document level label limit sentence level label experimental result show propose method outperform exist semi supervise baselines significantly publicly release code https githubcom gt salt persuasionstrategywvae
group people felt impact covid nineteen pandemic situation trigger anxiety bad everyone government role influential solve problems work program also many pros con public anxiety necessary detect anxiety improve government program increase public expectations study apply machine learn detect anxiety base social media comment regard government program deal pandemic concept adopt sentiment analysis detect anxiety base positive negative comment netizens machine learn methods implement include k nn bernoulli decision tree classifier support vector classifier random forest xg boost data sample use result crawl youtube comment data use amount four thousand, eight hundred and sixty-two comment consist negative positive data three thousand, two hundred and eleven one thousand, six hundred and fifty-one negative data identify anxiety positive data identify hope anxious machine learn process base feature extraction count vectorization tf idf result show sentiment data amount three thousand, eight hundred and eighty-nine nine hundred and seventy-three test train greatest accuracy random forest feature extraction vectorization count tf idf eight thousand, four hundred and ninety-nine eight thousand, two hundred and sixty-three respectively best precision test k nn best recall xg boost thus random forest best accurate detect someone anxiety base data social media
semantic text match model widely use community question answer information retrieval dialogue however model well address long form text match problem usually many noise set long form text match difficult exist semantic text match capture key match signal noisy information besides model computationally expensive simply use textual data indiscriminately match process tackle effectiveness efficiency problem propose novel hierarchical noise filter model paper namely match ignition basic idea plug well know pagerank algorithm transformer identify filter sentence word level noisy information match process noisy sentence usually easy detect sentence basic unit long form text directly use pagerank filter information base sentence similarity graph word need rely contexts express concrete mean propose jointly learn filter process match process reflect contextual dependencies word specifically word graph first build base attention score self attention block transformer keywords select apply pagerank graph way noisy word filter layer layer match process experimental result show match ignition outperform traditional text match model short text recent long form text match model also conduct detail analysis show match ignition efficiently capture important sentence word helpful long form text match
leaderboards ease model development many nlp datasets standardize evaluation delegate independent external repository adoption however far limit task reliably evaluate automatic manner work introduce genie extensible human evaluation leaderboard bring ease leaderboards text generation task genie automatically post leaderboard submissions crowdsourcing platforms ask human annotators evaluate various ax eg correctness conciseness fluency compare answer various automatic metrics introduce several datasets english genie represent four core challenge text generation machine translation summarization commonsense reason machine comprehension provide formal granular evaluation metrics identify areas future research make genie publicly available hope spur progress language generation model well automatic manual evaluation
work explore joint energy base model ebm train finetuning pretrained text encoders eg roberta natural language understand nlu task experiment show ebm train help model reach better calibration competitive strong baselines little loss accuracy discuss three variants energy function namely scalar hide sharp hide define top text encoder compare experiment due discreteness text data adopt noise contrastive estimation nce train energy base model make nce train effective train auto regressive noise model mask language model mlm objective
due success pre train model ptms people usually fine tune exist ptm downstream task ptms contribute maintain open source may suffer backdoor attack work demonstrate universal vulnerabilities ptms fine tune model easily control backdoor attack without knowledge downstream task specifically attacker add simple pre train task restrict output hide state trigger instance pre define target embeddings namely neuron level backdoor attack neuba attacker carefully design trigger correspond output hide state backdoor functionality eliminate fine tune experiment natural language process nlp computer vision cv task show neuba absolutely control predictions trigger instance influence model performance clean data finally find initialization resist neuba discuss several possible directions alleviate universal vulnerabilities find sound red alarm wide use ptms source code data access urlhttps githubcom thunlp neuba
supervise machine learn model evaluation strongly depend quality underlie dataset search relevant piece information may appear anywhere give passage however observe bias position correct answer text two popular question answer datasets use passage rank excessive favor earlier position inside passages unwanted artefact lead three common transformer base rank model ignore relevant part unseen passages concerningly evaluation set take bias distribution model overfitting bias overestimate true effectiveness work analyze position bias datasets contextualized representations effect retrieval result propose debiasing method retrieval datasets result show model train position bias dataset exhibit significant decrease rank effectiveness evaluate debiased dataset demonstrate mitigate position bias transformer base rank model equally effective bias debiased dataset well effective transfer learn set two differently bias datasets
recent improvements predictive quality natural language process systems often dependent substantial increase number model parameters lead various attempt compress model exist methods consider differences predictive power various model components generalizability compress model understand connection model compression distribution generalization define task compress language representation model perform best domain adaptation set choose address problem causal perspective attempt estimate textitaverage treatment effect eat model component single layer model predictions propose eat guide model compression scheme amoc generate many model candidates differ model components remove select best candidate stepwise regression model utilize eat predict expect performance target domain amoc outperform strong baselines forty-six sixty domain pair across two text classification task average improvement three f1 strongest baseline
abstractive text summarization process construct semantically relevant shorter sentence capture essence overall mean source text actually difficult time consume humans summarize manually large document text much work abstractive text summarization do english almost significant work report telugu abstractive text summarization would like propose abstractive text summarization approach telugu language use deep learn paper propose abstractive text summarization deep learn model telugu language propose architecture base encoder decoder sequential model attention mechanism apply model manually create dataset generate one sentence summary source text get good result measure qualitatively
annotate data become important bottleneck train accurate machine learn model especially areas require domain expertise recent approach deal issue propose use natural language explanations instead label individual data point thereby increase human annotators efficiency well decrease cost substantially paper focus task turn natural language descriptions python label function follow novel approach semantic parse pre train text text transformers series experiment approach achieve new state art semantic parse benchmark conala surpass previous best approach thirty-seven bleu point furthermore manually construct dataset natural language descriptions label function pair achieve bleu thirty-nine approach regard step stone towards model teach label natural language instead provide specific label sample code construct dataset model available https githubcom ypapanik t5 code generation
conversational passage retrieval rely question rewrite modify original question longer depend conversation history several methods question rewrite recently propose compare different retrieval pipelines bridge gap thoroughly evaluate question rewrite methods trec cast two thousand and nineteen two thousand and twenty datasets retrieval pipeline analyze effect different type question rewrite methods retrieval performance show combine question rewrite methods different type achieve state art performance datasets
present novel large scale dataset accompany machine learn model aim provide detail understand interplay visual content emotional effect explanations latter language contrast exist annotation datasets computer vision focus affective experience trigger visual artworks ask annotators indicate dominant emotion feel give image crucially also provide ground verbal explanation emotion choice demonstrate lead rich set signal objective content affective impact image create associations abstract concepts eg freedom love reference go beyond directly visible include visual similes metaphors subjective reference personal experience focus visual art eg paint artistic photograph prime example imagery create elicit emotional responses viewers dataset term artemis contain 439k emotion attributions explanations humans 81k artworks wikiart build data train demonstrate series caption systems capable express explain emotions visual stimuli remarkably caption produce systems often succeed reflect semantic abstract content image go well beyond systems train exist datasets collect dataset develop methods available https artemisdatasetorg
covid nineteen cause thousands deaths around world also result large international economic disruption identify pathways associate illness help medical researchers better understand properties condition process carry analyze medical record crucial develop tool model aid researchers process timely manner however medical record often unstructured clinical note pose significant challenge develop automate systems article propose pipeline aid practitioners analyze clinical note reveal pathways associate disease pipeline rely topological properties consist three step one pre process clinical note extract salient concepts two construct feature space patients characterize extract concepts finally three leverage topological properties distill available knowledge visualize result experiment publicly available dataset covid nineteen clinical note testify pipeline indeed extract meaningful pathways
citation recommendation important task assist scholars find candidate literature cite traditional study focus static model recommend citations explicitly distinguish differences paper cause temporal variations although researchers investigate chronological citation recommendation add time relate function model textual topics dynamically solutions hardly cope function generalization cold start problems information user profile isolate paper never cite rise fall science paradigms scientific topics tend change evolve time people would time preference cite paper since theoretical basis exist classical read publish old time new techniques propose recent paper explore chronological citation recommendation paper want predict time preference base user query probability distribution cite paper publish different time slice use time preference rank initial citation list obtain content base filter experimental result demonstrate task performance enhance time preference flexible add citation recommendation frameworks
multidisciplinary cooperation common research since social issue inevitably involve multiple discipline research article reference information especially citation content important representation communication among different discipline analyze distribution characteristics reference different discipline research article basic detect source refer information identify contributions different discipline work take article plos data characterize reference different discipline base citation content analysis cca first download two hundred and ten thousand, three hundred and thirty-four full text article plos collect information text citations identify discipline reference academic article characterize distribution reference analyze three characteristics namely number citations average cite intensity average citation length finally conclude distributions reference different discipline significantly different although reference come natural science humanities social sciences play important roles introduction background section article basic discipline mathematics mainly provide research methods article plos citations mention result discussion section article mainly discipline citations citations nurse medicine plos
human ability deep cognitive skills crucial development various real world applications process diverse abundant user generate input recent progress deep learn natural language process enable learn system reach human performance benchmarks require shallow semantics human ability still remain challenge even modern contextual embed model point many recent study exist machine comprehension datasets assume sentence level input lack casual motivational inferences could answer question answer bias present challenge novel task trope detection film effort create situation behavior understand machine tropes storytelling devices frequently use ingredients recipes creative work compare exist movie tag prediction task tropes sophisticate vary widely moral concept series circumstances embed motivations effect introduce new dataset tropes movie synopses timos five thousand, six hundred and twenty-three movie synopses ninety-five different tropes collect wikipedia style database tvtropes present multi stream comprehension network mulcom leverage multi level attention word sentence role relations experimental result demonstrate modern model include bert contextual embed movie tag prediction systems relational network perform thirty-seven human performance two thousand, three hundred and ninety-seven six thousand, four hundred and eighty-seven term f1 score mulcom outperform modern baselines fifteen fifty f1 score fifteen thirty mean average precision map score also provide detail analysis human evaluation pave ways future research
online peer peer support platforms enable conversations millions people seek provide mental health support successful web base mental health conversations could improve access treatment reduce global disease burden psychologists repeatedly demonstrate empathy ability understand feel emotions experience others key component lead positive outcomes supportive conversations however recent study show highly empathic conversations rare online mental health platforms paper work towards improve empathy online mental health support conversations introduce new task empathic rewrite aim transform low empathy conversational post higher empathy learn transformations challenge require deep understand empathy maintain conversation quality text fluency specificity conversational context propose partner deep reinforcement learn agent learn make sentence level edit post order increase express level empathy maintain conversation quality rl agent leverage policy network base transformer language model adapt gpt two perform dual task generate candidate empathic sentence add sentence appropriate position train reward transformations increase empathy post maintain text fluency context specificity diversity combination automatic human evaluation demonstrate partner successfully generate empathic specific diverse responses outperform nlp methods relate task like style transfer empathic dialogue generation work direct implications facilitate empathic conversations web base platforms
research construction traditional information science methodology taxonomy mostly conduct manually limit corpus researchers attempt summarize research methodology entities several abstract level generally three level however unable provide granular hierarchy moreover update methodology taxonomy traditionally slow process study collect full text academic paper relate information science first construct basic methodology taxonomy three level manual annotation word vectors research methodology entities train use full text data accordingly research methodology entities cluster basic methodology taxonomy expand use cluster result obtain methodology taxonomy level study provide new concepts construct methodology taxonomy information science propose methodology taxonomy semi automate detail conventional scheme speed taxonomy renewal enhance
participate dstc9 interactive dialogue evaluation track gunasekara et al two thousand and twenty sub task one knowledge ground dialogue sub task two interactive dialogue sub task one employ pre train language model generate topic relate responses propose response ensemble method response selection sub task2 propose novel dialogue plan model dpm capture conversation flow interaction humans also design integrate open domain dialogue system contain pre process dialogue model score model post process generate fluent coherent consistent humanlike responses tie 1st human rat also get highest meteor bert score sub task one rank 3rd interactive human evaluation sub task two
introduce hive four mat help interdisciplinary vocabulary engineer materials science automatic link data ontology application cover contextual background materials science share ontology infrastructures review knowledge extraction index process hive four mat vocabulary browse term search selection knowledge extraction index review plan integrate name entity recognition conclusion highlight next step relation extraction support better ontologies
propose tackle conditional text generation task especially require generate formulaic text splice together segment text retrieve neighbor source target pair unlike recent work condition retrieve neighbor encoder decoder set generate text token token leave right learn policy directly manipulate segment neighbor text ie insert replace form output standard techniques train policy require oracle derivation generation prove find shortest derivation reduce parse particular weight context free grammar find policies learn way allow interpretable table text headline generation competitive neighbor base token level policies automatic metrics though one dataset neighbor base policies underperform strong neighborless baseline case however generate splice faster
pretrained multilingual text encoders base neural transformer architectures multilingual bert mbert xlm achieve strong performance myriad language understand task consequently adopt go paradigm multilingual cross lingual representation learn transfer render cross lingual word embeddings clwes effectively obsolete however question remain extent find generalize one unsupervised settings two ad hoc cross lingual ir clir task therefore work present systematic empirical study focus suitability state art multilingual encoders cross lingual document sentence retrieval task across large number language pair contrast supervise language understand result indicate unsupervised document level clir setup relevance judgments ir specific fine tune pretrained encoders fail significantly outperform model base clwes sentence level clir demonstrate state art performance achieve however peak performance meet use general purpose multilingual text encoders shelf rather rely variants specialize sentence understand task
often language areas cognition whether two components object identical determine well form call constraints identity effect develop system learn well formedness examples easy enough build identify effect identity effect learn data without explicit guidance provide framework rigorously prove algorithms satisfy simple criteria make correct inference show broad class learn algorithms include deep feedforward neural network train via gradient base algorithms stochastic gradient descent adam method satisfy criteria dependent encode input broader circumstances able provide adversarial examples network necessarily classify incorrectly finally demonstrate theory computational experiment explore effect different input encode ability algorithms generalize novel input
data annotation play crucial role ensure name entity recognition ner project train right information learn produce accurate label challenge due complexity involve annotation label inconsistency multiple subsets data annotation eg train set test set multiple train subsets indicator label mistake work present empirical method explore relationship label consistency ner model performance use validate label consistency catch inconsistency multiple set ner data annotation experiment method identify label inconsistency test data scierc conll03 datasets two hundred and sixty-seven fifty-four label mistake validate consistency correct version datasets
large pre train multilingual model like mbert xlm r achieve state art result language understand task however well suit latency critical applications servers edge devices important reduce memory compute resources require model end propose pqrnn projection base embed free neural encoder tiny effective natural language process task without pre train pqrnns significantly outperform lstm model pre train embeddings despite 140x smaller number parameters outperform transformer baselines thereby showcasing parameter efficiency additionally show pqrnns effective student architectures distil large pre train language model perform careful ablations study effect pqrnn parameters data augmentation distillation settings mtop challenge multilingual semantic parse dataset pqrnn students achieve nine hundred and fifty-nine performance mbert teacher 350x smaller matis popular parse task pqrnn students average able get nine hundred and seventy-one teacher 350x smaller strong result suggest approach great latency sensitive applications able leverage large mbert like model
non autoregressive model boost efficiency neural machine translation parallelize decode cost effectiveness compare autoregressive counterparts paper claim syntactic semantic structure among natural language critical non autoregressive machine translation improve performance however structure rarely consider exist non autoregressive model inspire intuition propose incorporate explicit syntactic semantic structure languages non autoregressive transformer task neural machine translation moreover also consider intermediate latent alignment within target sentence better learn long term token dependencies experimental result two real world datasets ie wmt14 en de wmt16 en ro show model achieve significantly faster speed well keep translation quality compare several state art non autoregressive model
knowledge graph completion task expand knowledge graph base estimate possible entities proper nouns connect use set predefined relations verb predicate describe interconnections two things generally describe problem add new edge current network vertices edge traditional approach mainly focus use exist graphical information intrinsic graph train correspond embeddings describe information however think corpus relate entities also contain information positively influence embeddings better make predictions project try numerous ways use extract raw textual information help exist kg embed frameworks reach better prediction result mean add similarity function regularization part loss function result show make decent improvements baseline kg embed methods
code mixingcm frequently observe phenomenon use multiple languages utterance sentence cm mostly practice various social media platforms informal conversations sentiment analysis sa fundamental step nlp well study monolingual text code mix add challenge sentiment analysis due non standard representations paper propose meta embed transformer method sentiment analysis dravidian code mix dataset method use meta embeddings capture rich text representations use propose method task sentiment analysis dravidian languages code mix text achieve f1 score fifty-eight sixty-six give dravidian code mix data set code provide github https githubcom suman101112 fire two thousand and twenty dravidian codemix
relationship word sentence often tell us underlie semantic content document actual word individually work propose two novel algorithms call flexible lexical chain ii fix lexical chain ii algorithms combine semantic relations derive lexical chain prior knowledge lexical databases robustness distributional hypothesis word embeddings build block form single system short approach three main contributions set techniques fully integrate word embeddings lexical chain ii robust semantic representation consider latent relation word document iii lightweight word embeddings model extend natural language task intend assess knowledge pre train model evaluate robustness document classification task propose techniques test seven word embeddings algorithms use five different machine learn classifiers six scenarios document classification task result show integration lexical chain word embeddings representations sustain state art result even complex systems
change precondition variation inherent languages time new word enter lexicon others become obsolete exist word acquire new sense associate word correct mean historical context central challenge diachronic research historical corpora classical languages ancient greek latin typically come rich metadata exist model limit inability exploit contextual information beyond document timestamp embed base methods feature among current state art systems lack interpretative power contrast bayesian model provide explicit interpretable representations semantic change phenomena chapter build gasc recent computational approach semantic change base dynamic bayesian mixture model model evolution word sense time base distributional information lexical nature also text genres provide systematic comparison dynamic bayesian mixture model semantic change state art embed base model top provide full description mean change time show bayesian mixture model highly competitive approach detect binary semantic change ancient greek latin
knowledge graph completion refer predict miss triple approach achieve goal predict entities give entity relation predict miss triple via relation prediction end frame relation prediction problem multi label classification problem propose shallow neural model shallom accurately infer miss relations entities shallom analogous c bow approach predict central token p give surround tokens sof experiment indicate shallom outperform state art approach fb15k two hundred and thirty-seven wn18rr margins three eight absolute respectively require maximum train time eight minutes datasets ensure reproducibility result provide open source implementation include train evaluation script urlhttps githubcom dice group shallom
medical terminology normalization aim map clinical mention terminologies come knowledge base play important role analyze electronic health recordehr many downstream task paper focus chinese procedure terminology normalization expression terminologies various one medical mention may link multiple terminologies previous study explore methods multi class classification learn rankltr sort terminologies literature semantic information however information inadequate find right terminologies particularly multi implication case work propose combine recall rank framework solve problems framework compose multi task candidate generatormtcg keywords attentive rankerkar fusion blockfb mtcg utilize predict mention implication number recall candidates semantic similarity kar base bert keywords attentive mechanism focus keywords procedure sit procedure type fb merge similarity come mtcg kar sort terminologies different perspectives detail experimental analysis show propose framework remarkable improvement performance efficiency
multi head attention head mainstay transformer base model different methods propose classify role attention head base relations tokens high pair wise attention roles include syntactic tokens syntactic relation local nearby tokens block tokens sentence delimiter special cls sep tokens two main challenge exist methods classification standard score across study across functional roles b score often average quantities measure across sentence without capture statistical significance work formalize simple yet effective score generalize roles attention head employ hypothesis test score robust inference provide us right lens systematically analyze attention head confidently comment many commonly pose question analyze bert model particular comment co location multiple functional roles attention head distribution attention head across layer effect fine tune specific nlp task functional roles
use end end model speech translation st increasingly focus st community model condense previously cascade systems directly convert sound wave translate text however cascade model advantage include automatic speech recognition output useful variety practical st systems often display transcripts user alongside translations bridge gap recent work show initial progress feasibility end end model produce output however previous work look problem consecutive perspective leave uncertainty whether approach effective challenge stream set develop end end stream st model base translation approach compare standard cascade approach also introduce novel inference method joint case interleave transcript translation generation remove need use separate decoders evaluation across range metrics capture accuracy latency consistency show end end model statistically similar cascade model half number parameters also find systems provide strong translation quality low latency keep ninety-nine consecutive quality lag second
present depth analysis impact multi word suggestion choices neural language model user behaviour regard input text composition email write study first time compare different number parallel suggestions use native non native english writers explore trade efficiency vs ideation emerge recent literature build text editor prototype neural language model gpt two refine prestudy thirty people online study n156 people compose email four condition zero one three six parallel suggestions result reveal one benefit ideation cost efficiency suggest multiple phrase two non native speakers benefit suggestions three insights behaviour pattern discuss implications research design interactive suggestion systems vision support writers ai instead replace
since effective therapies exist alzheimer disease ad prevention become critical lifestyle factor change interventions analyze electronic health record ehr patients ad help us better understand lifestyle effect ad however lifestyle information typically store clinical narratives thus objective study demonstrate feasibility natural language process nlp model classify lifestyle factor eg physical activity excessive diet clinical texts automatically generate label train data use rule base nlp algorithm conduct weak supervision pre train bidirectional encoder representations transformers bert model weakly label train corpus model include bert base model pubmedbertabstracts full text pubmedbertonly abstract unify medical language system umls bert bio bert bio clinical bert perform two case study physical activity excessive diet order validate effectiveness bert model classify lifestyle factor ad model compare develop gold standard corpus gsc two case study pubmedbertabs model achieve best performance physical activity precision recall f one score ninety-six ninety-six ninety-six respectively regard classify excessive diet bio bert model show highest performance perfect precision recall f one score propose approach leverage weak supervision could significantly increase sample size require train deep learn model study also demonstrate effectiveness bert model extract lifestyle factor alzheimer disease clinical note
concept normalization free form texts crucial step every text mine pipeline neural architectures base bidirectional encoder representations transformers bert achieve state art result biomedical domain context drug discovery development clinical trials necessary establish efficacy safety drug investigate effectiveness transfer concept normalization general biomedical domain clinical trials domain zero shoot set absence label data propose simple effective two stage neural approach base fine tune bert architectures first stage train metric learn model optimize relative similarity mention concepts via triplet loss model train available label corpora scientific abstract obtain vector embeddings concept name entity mention texts second stage find closest concept name representation embed space give clinical mention evaluate several model include state art architectures dataset abstract real world dataset trial record interventions condition map drug disease terminologies extensive experiment validate effectiveness approach knowledge transfer scientific literature clinical trials
multi step ahead prediction language model challenge due discrepancy train test time process test time sequence predictor require make predictions give past predictions input instead past target provide train difference know exposure bias lead compound errors along generate sequence test time improve generalization neural language model address compound errors propose textitnearest neighbor replacement sample curriculum learn base method gradually change initially deterministic teacher policy stochastic policy token give time step replace sample nearest neighbor past target truncate probability proportional cosine similarity original word top k similar word allow learner explore alternatives current policy provide teacher sub optimal difficult learn propose method straightforward online require little additional memory requirements report find two language model benchmarks find propose method improve performance use conjunction schedule sample
indispensable component task orient dialogue systems dialogue state tracker keep track users intentions course conversation typical approach towards goal fill multiple pre define slot essential complete task although various dialogue state track methods propose recent years predict value slot separately fail consider correlations among slot paper propose slot self attention mechanism learn slot correlations automatically specifically slot token attention first utilize obtain slot specific feature dialogue context stack slot self attention apply feature learn correlations among slot conduct comprehensive experiment two multi domain task orient dialogue datasets include multiwoz twenty multiwoz twenty-one experimental result demonstrate approach achieve state art performance datasets verify necessity effectiveness take slot correlations consideration
good communication indubitably foundation effective teamwork time team develop communication style often exhibit entrainment conversational phenomena humans synchronize linguistic choices paper examine problem predict team performance embeddings learn multiparty dialogues team similar conflict score lie close one another vector space embeddings extract three type feature one dialogue act two sentiment polarity three syntactic entrainment although feature use effectively predict team performance utility vary teamwork phase separate dialogues players play cooperative game stag one early knowledge build two middle problem solve three late culmination unlike syntactic entrainment dialogue act sentiment embeddings effective classify team performance even initial phase find potential ramifications development conversational agents facilitate team
increase geospatial link open data adopt publish web need develop intuitive interfaces systems seamless efficient exploratory analysis rich heterogeneous multi modal datasets work gear towards improve exploration process earth observation eo link data develop natural language interface facilitate query question ask earth observation link data inherent spatio temporal dimension represent use geosparql paper seek study analyze use rnn base neural machine translation attention transform natural language question geosparql query specifically aim assess feasibility neural approach identify map spatial predicate natural language geosparql topology vocabulary extension include egenhofer rcc8 relations query execute triple store yield answer natural language question dataset consist mappings natural language question geosparql query corine land coverclc link data create train validate deep neural network experiment evident neural machine translation attention promise approach task translate spatial predicate natural language question geosparql query
recommender systems exploit interaction history estimate user preference heavily use wide range industry applications however static recommendation model difficult answer two important question well due inherent shortcomings exactly user like b user like item shortcomings due way static model learn user preference ie without explicit instructions active feedback users recent rise conversational recommender systems crss change situation fundamentally crs users system dynamically communicate natural language interactions provide unprecedented opportunities explicitly obtain exact preference users considerable efforts spread across disparate settings applications put develop crss exist model technologies evaluation methods crss far mature paper provide systematic review techniques use current crss summarize key challenge develop crss five directions one question base user preference elicitation two multi turn conversational recommendation strategies three dialogue understand generation four exploitation exploration trade off five evaluation user simulation research directions involve multiple research field like information retrieval ir natural language process nlp human computer interaction hci base research directions discuss future challenge opportunities provide road map researchers multiple communities get start area hope survey help identify address challenge crss inspire future research
objective project solve one major problems face people word process issue like trauma mild mental disability arth short form algorithm read handily arth self learn set algorithms intelligent way fulfil need read understand text effortlessly adjust accord need every user research project propagate two step first step algorithm try identify difficult word present text base two feature number syllables usage frequency use cluster algorithm analysis cluster algorithm label cluster accord difficulty level second step algorithm interact user aim test user comprehensibility text vocabulary level take automatically generate quiz algorithm identify cluster difficult user base result analysis mean perceive difficult word display next technology arth focus revival joy read among people poor vocabulary word process issue
multi label text classification mltc aim annotate document relevant label number candidate label real applications distribution label frequency often exhibit long tail ie label associate large number document aka head label large fraction label associate small number document aka tail label address challenge insufficient train data tail label classification propose head tail network httn transfer meta knowledge data rich head label data poor tail label meta knowledge map shoot network parameters many shoot network parameters aim promote generalizability tail classifiers extensive experimental result three benchmark datasets demonstrate httn consistently outperform state art methods code hyper parameter settings release reproducibility
engage argumentative discourse skilled human debaters tailor claim beliefs audience construct effective arguments recently field computational argumentation witness extensive effort address automatic generation arguments however exist approach perform audience specific adaptation work aim bridge gap study task belief base claim generation give controversial topic set beliefs generate argumentative claim tailor beliefs tackle task model people prior beliefs stances controversial topics extend state art text generation model generate claim condition beliefs automatic evaluation confirm ability approach adapt claim set give beliefs manual study additionally evaluate generate claim term informativeness likelihood utter someone respective belief result reveal limitations model users beliefs base stances demonstrate potential encode beliefs argumentative texts lay grind future exploration audience reach
work propose novel goal orient dialog task automatic symptom detection build system interact patients dialog detect collect clinical symptoms automatically save doctor time interview patient give set explicit symptoms provide patient initiate dialog diagnose system train collect implicit symptoms ask question order collect information make accurate diagnosis get reply patient question system also decide whether current information enough human doctor make diagnosis achieve goal propose two neural model train pipeline multi step reason task also build knowledge graph additional input improve model performance experiment show model significantly outperform baseline four discover sixty-seven implicit symptoms average limit number question
novel object caption zero shoot image caption task require describe object see train caption information available external object detectors key challenge select describe salient detect novel object input image paper focus challenge propose ecol r model encourage copy object label reinforce learn copy augment transformer model encourage accurately describe novel object label achieve via specialise reward function scst reinforcement learn framework rennie et al two thousand and seventeen encourage novel object mention maintain caption quality restrict scst train image detect object mention reference caption train ecol r model additionally improve copy mechanism via abstract label transfer knowledge know novel object type morphological selector determine appropriate inflect form novel object label result model set new state art nocaps agrawal et al two thousand and nineteen hold coco hendricks et al two thousand and sixteen benchmarks
pre train language model show improve performance many natural language task substantially although early focus model single language pre train recent advance result cross lingual visual pre train methods paper combine two approach learn visually ground cross lingual representations specifically extend translation language model lample conneau two thousand and nineteen mask region classification perform pre train three way parallel vision language corpora show fine tune multimodal machine translation model obtain state art performance also provide qualitative insights usefulness learn ground representations
grow political polarization american electorate last several decades widely study document administration president donald trump charge fake news make social news media mean unprecedented extent topic political communication use data november 3rd two thousand and twenty us presidential election recent work demonstrate viability use youtube social media ecosystem obtain insights extent us political polarization well relationship polarization nature content commentary provide different us news network work background paper look sharp transformation relationship news consumers fore fringe news media channel sixty-four days us presidential election violence take place us capitol january 6th paper make two distinct type contributions first introduce novel methodology analyze large social media data study dynamics social political news network viewers second provide insights actually happen regard us political social media channel viewerships volatile sixty-four day period
1st edition workshop mix initiative conversational systems microsecir2021 aim investigate collect novel ideas contributions field conversational systems oftentimes users fulfill information need use smartphones home assistants revolutionize way users access online information thus pose new challenge compare traditional search recommendation first edition micros particular focus mix initiative conversational systems indeed conversational systems need proactive propose answer also possible interpretations ambiguous vague request
entity alignment ea aim discover equivalent entities different knowledge graph kgs pivotal step integrate kgs increase knowledge coverage quality recent years witness rapid increase ea frameworks however state art solutions tend rely label data model train additionally work close domain set deal entities unmatchable address deficiencies offer unsupervised framework perform entity alignment open world specifically first mine useful feature side information kgs devise unmatchable entity prediction module filter unmatchable entities produce preliminary alignment result preliminary result regard pseudo label data forward progressive learn framework generate structural representations integrate side information provide comprehensive view alignment finally progressive learn framework gradually improve quality structural embeddings enhance alignment performance enrich pseudo label data alignment result previous round solution require label data effectively filter unmatchable entities comprehensive experimental evaluations validate superiority
proper identification grade level children read materials important step towards effective learn recent study readability assessment english domain apply modern approach natural language process nlp machine learn ml techniques automate process also need extract correct linguistic feature model readability formulas context filipino language limit work do one two especially consider language lexical complexity main feature paper explore use lexical feature towards improve development readability identification children book write filipino result show combine lexical feature lex consist type token ratio lexical density lexical variation foreign word count traditional feature trad use previous work sentence length average syllable length polysyllabic word word sentence phrase count increase performance readability model almost five margin forty-two four hundred and seventy-two analysis rank important feature show identify feature contribute term read complexity
model persuasion strategies predictors task outcome several real world applications receive considerable attention computational linguistics community however previous research fail account resist strategies employ individual foil persuasion attempt ground prior literature cognitive social psychology propose generalise framework identify resist strategies persuasive conversations instantiate framework two distinct datasets comprise persuasion negotiation conversations also leverage hierarchical sequence label neural architecture infer aforementioned resist strategies automatically experiment reveal asymmetry power roles non collaborative goal direct conversations benefit accrue incorporate resist strategies final conversation outcome also investigate role different resist strategies conversation outcome glean insights corroborate past find also make code dataset work publicly available https githubcom americast resper
contextualized representations pre train language model central achieve high performance downstream nlp task pre train bert lite bert albert model fine tune give state ofthe art result sentence pair regressions semantic textual similarity sts natural language inference nli although bert base model yield cls token vector reasonable sentence embed search optimal sentence embed scheme remain active research area computational linguistics paper explore sentence embed model bert albert particular take modify bert network siamese triplet network structure call sentence bert sbert replace bert albert create sentence albert salbert also experiment outer cnn sentence embed network sbert salbert evaluate performances sentence embed model consider use sts nli datasets empirical result indicate cnn architecture improve albert model substantially bert model sts benchmark despite significantly fewer model parameters albert sentence embed highly competitive bert downstream nlp evaluations
zero shoot cross lingual transfer supervise nlp task train corpus one language directly applicable another language without additional train source cross lingual transfer straightforward lexical overlap languages eg use script share subwords naturally force text embeddings occupy similar representation space recently introduce cross lingual language model xlm pretraining bring neural parameter share transformer style network important factor transfer paper aim validate hypothetically strong cross lingual transfer properties induce xlm pretraining particularly take xlm roberta xlmr experiment extend semantic textual similarity sts squad korquad machine read comprehension sentiment analysis alignment sentence embeddings various cross lingual settings result indicate presence cross lingual transfer pronounce sts sentiment analysis next mrc last complexity downstream task soften degree crosslingual transfer result empirically observe measure make code data publicly available
major scandals corporate history urge need regulatory compliance organizations need ensure control process comply relevant laws regulations policies however keep track constantly change legislation difficult thus organizations increasingly adopt regulatory technology regtech facilitate process end introduce regulatory information retrieval reg ir application document document information retrieval doc2doc ir query entire document make task challenge traditional ir query short furthermore compile release two datasets base relationships eu directives uk legislation experiment datasets use typical two step pipeline approach comprise pre fetcher neural ranker experiment various pre fetchers bm25 k nearest neighbor representations several bert model show fine tune bert model domain classification task produce best representations ir also show neural rankers perform due contradict supervision ie similar query document pair opposite label thus bias towards pre fetcher score interestingly apply date filter improve performance showcasing importance time dimension
disagreements pervasive human communication paper investigate make disagreement constructive end construct wikidisputes corpus seven four hundred and twenty-five wikipedia talk page conversations contain content dispute define task predict whether disagreements escalate mediation moderator evaluate feature base model linguistic markers previous work demonstrate performance improve use feature capture change linguistic markers throughout conversations oppose average value develop variety neural model show take account structure conversation improve predictive accuracy exceed feature base model assess best neural model term predictive accuracy uncertainty evaluate behaviour expose begin conversation find model accuracy improve uncertainty reduce model expose information
propose pre finetuning additional large scale learn stage language model pre train fine tune pre finetuning massively multi task learn around fifty datasets forty-eight million total label examples design encourage learn representations generalize better many different task show pre finetuning consistently improve performance pretrained discriminators egroberta generation model egbart wide range task sentence prediction commonsense reason mrc etc also significantly improve sample efficiency fine tune also show large scale multi task crucial pre finetuning hurt performance task use critical point usually fifteen performance improve linearly number task
users speak dialogue systems sds expect high quality interactions across wide range diverse topics however implementation sds capable respond every conceivable user utterance informative way challenge problem multi domain sds must necessarily identify deal domain ood utterances generate appropriate responses users always know advance domains sds handle address problem extend current state art multi domain sds estimate topic ood utterances use external knowledge representation wikipedia experimental result real human human dialogues show approach degrade domain prediction performance compare base model significantly joint train achieve accurate predictions nearest wikipedia article thirty compare benchmarks
open information extraction oie systems seek compress factual proposition sentence series n ary tuples tuples useful downstream task natural language process like knowledge base creation textual entailment natural language understand however current oie datasets limit size diversity introduce new dataset convert qa srl twenty dataset large scale oie dataset lsoie lsoie dataset twenty time larger next largest human annotate oie dataset construct evaluate several benchmark oie model lsoie provide baselines future improvements task lsoie data model code make publicly available
character link task link mention people conversations real world crucial understand conversations efficiency communication humans often choose use pronouns eg normal phrase eg girl rather name entities eg rachel speak language make link mention real people much challenge regular entity link task address challenge propose incorporate richer context coreference relations among different mention help link hand consider find coreference cluster trivial task could benefit global character information propose jointly solve two task specifically propose c2 joint learn model coreference resolution character link experimental result demonstrate c2 significantly outperform previous work task analyse conduct analyze contribution modules propose model effect hyper parameters
large datasets nlp suffer noisy label due erroneous automatic human annotation procedures study problem text classification label noise aim capture noise auxiliary noise model classifier first assign probability score train sample noisy label beta mixture model fit losses early epoch train use score selectively guide learn noise model classifier empirical evaluation two text classification task show approach improve baseline accuracy prevent fit noise
recent study machine read comprehension focus text level understand yet reach level human understand visual layout content real world document study introduce new visual machine read comprehension dataset name visualmrc wherein give question document image machine read comprehend texts image answer question natural language compare exist visual question answer vqa datasets contain texts image visualmrc focus develop natural language understand generation abilities contain thirty thousand pair question abstractive answer ten thousand document image source multiple domains webpages also introduce new model extend exist sequence sequence model pre train large scale text corpora take account visual layout content document experiment visualmrc show model outperform base sequence sequence model state art vqa model however performance still humans automatic evaluation metrics dataset facilitate research aim connect vision language understand
paper propose study language model multi task problem bring together three strand research multi task learn linguistics interpretability base hypotheses derive linguistic theory investigate whether language model adhere learn principles multi task learn train showcase idea analyse generalisation behaviour language model learn linguistic concept negative polarity items npis experiment demonstrate multi task set naturally emerge within objective general task language modellingwe argue insight valuable multi task learn linguistics interpretability research lead excite new find three domains
write language contain stylistic cue exploit automatically infer variety potentially sensitive author information adversarial stylometry intend attack model rewrite author text research propose several components facilitate deployment adversarial attack wild neither data target model accessible introduce transformer base extension lexical replacement attack show achieve high transferability train weakly label corpus decrease target model performance chance completely inconspicuous successful attack also prove notably less detectable humans framework therefore provide promise direction future privacy preserve adversarial attack
sponsor search auction crucial component modern search engines require set candidate bidwords advertisers place bid exist methods generate bidwords search query advertisement content however suffer data noise pair paper propose triangular bidword generation model trident take high quality data pair supervision signal indirectly guide bidword generation process propose model simple yet effective use bidword bridge search query advertisement generation search query advertisement bidword jointly learn triangular train framework alleviate problem train data bidword may noisy experimental result include automatic human evaluations show propose trident generate relevant diverse bidwords search query advertisements evaluation online real data validate effectiveness trident generate bidwords product search
lite bert albert introduce scale deep bidirectional representation learn natural languages due lack pretrained albert model korean language best available practice multilingual model resort back bert base model paper develop pretrain korealbert monolingual albert model specifically korean language understand introduce new train objective namely word order prediction wop use alongside exist mlm sop criteria architecture model parameters despite significantly fewer model parameters thus quicker train pretrained korealbert outperform bert counterpart six different nlu task consistent empirical result english lan et al korealbert seem improve downstream task performance involve multi sentence encode korean language pretrained korealbert publicly available encourage research application development korean nlp
clinical diagnosis aim assign diagnosis cod patient base clinical note play essential role clinical decision make consider manual diagnosis could error prone time consume many intelligent approach base clinical text mine propose perform automatic diagnosis however methods may achieve satisfactory result due follow challenge first diagnosis cod rare distribution extremely unbalance second exist methods challenge capture correlation diagnosis cod third lengthy clinical note lead excessive dispersion key information relate cod tackle challenge propose novel framework combine inheritance guide hierarchical assignment co occurrence graph propagation clinical automatic diagnosis specifically propose hierarchical joint prediction strategy address challenge unbalance cod distribution utilize graph convolutional neural network obtain correlation semantic representations medical ontology furthermore introduce multi attention mechanisms extract crucial information finally extensive experiment mimic iii dataset clearly validate effectiveness method
paper describe propose system aaai cad21 share task predict emphasis presentation slide specific task give content slide ask predict degree emphasis lay word slide propose two approach problem include bilstm elmo approach transformers base approach base roberta xlnet architectures achieve score five hundred and eighteen evaluation leaderboard rank us 3rd five hundred and forty-three post evaluation leaderboard rank us 1st time write paper
scenario base question answer sqa attract increase research interest compare well study machine read comprehension mrc sqa challenge task scenario may contain textual passage read also structure data like table ie tabular scenario base question answer tsqa ai applications tsqa answer multiple choice question high school exams require synthesize data multiple cells combine table texts domain knowledge infer answer support study task construct geotsqa dataset contain 1k real question contextualized tabular scenarios geography domain solve task extend state art mrc methods ttgen novel table text generator generate sentence variously synthesize tabular data feed downstream mrc method useful sentence sentence rank model fuse information scenario question domain knowledge approach outperform variety strong baseline methods geotsqa
method develop cnn base methods automatic icd cod base clinical text intensive care unit icu stay come shallow wide attention convolutional mechanism swim allow model learn local low level feature label key idea behind model design look presence informative snippets clinical text correlate code infer exist correspondence informative snippet convolution filter result evaluate approach mimic iii open access dataset icu medical record approach substantially outperform previous result top fifty medical code prediction mimic iii dataset attribute improvement swim wide architecture give model ability extensively learn unique feature different cod prove ablation experiment besides perform manual analysis performance imbalance different cod preliminary conclude characteristics determine difficulty learn specific cod conclusions present swim explainable cnn approach multi label document classification employ wide convolution layer learn local low level feature label yield strong improvements previous metrics icd nine code prediction task provide satisfactory explanations internal mechanics
today digital world increase focus soft skills reason many however main ones trace increase complexity labor market dynamics shift towards digitalisation digitalisation also increase focus soft skills since competencies hardly acquire artificial intelligence systems despite grow interest researchers struggle accurately define soft skill concept create complete share list soft skills therefore aim present paper development automate tool capable extract soft skills unstructured texts start initial seed list soft skills automatically collect set possible textual expressions refer soft skills thus create soft skills list do apply name entity recognition ner corpus scientific paper develop novel approach software application able perform automatic extraction soft skills text skillner measure performance tool consider different train model validate approach compare list soft skills skills label transversal esco european skills competence qualification occupation finally give first example skillner use identify relationships among esco job profile base soft skills share relationships among soft skills base job profile common final map soft skills job profile may help accademia achieve share clearer definition soft skills fuel future quantitative research topic
response kaggle covid nineteen open research dataset cord nineteen challenge propose three transformer base question answer systems use bert albert t5 model since cord nineteen dataset unlabeled evaluate question answer model performance two label question answer datasets textemdash covidqa covidgqa bert base qa system achieve highest f1 score two thousand, six hundred and thirty-two albert base qa system achieve highest exact match one thousand, three hundred and four however numerous challenge associate develop high performance question answer systems ongoing covid nineteen pandemic future pandemics end paper discuss challenge suggest potential solutions address
follow paper author present gin type model important stag development task emotion recognition text particular propose approach generate synthetic dataset possible emotions combinations base manually label incomplete data
paper add insights towards resources datasets use arabic offensive language research main goal paper guide researchers arabic offensive language select appropriate datasets base content create new arabic offensive language resources support complement available ones
natural language process branch computer science combine artificial intelligence linguistics aim analyze language element write speak software convert information consider language grammatical rule vocabulary diversity complexity study field somewhat understandable instance turkish interest language many ways examples agglutinative word structure consonant vowel harmony large number productive derivational morphemes practically infinite vocabulary derivation syntactic relations complex emphasis vocabulary phonological rule study interest feature turkish term natural language process mention addition summary info natural language process techniques systems various source develop turkish give
paper present fully automatic approach scansion classical greek hexameter verse particular paper describe algorithm use deterministic finite state automata local linguistic rule implement target search valid spondeus pattern addition weight finite state transducer correct complete partial analyse reject invalid candidates paper also detail result empirical evaluation annotation quality result approach hand annotate data show finite state approach provide quick linguistically sound analyse hexameter verse well efficient formalisation linguistic knowledge project code available see https githubcom anetschka greekscansion
paper propose deep learn base method identify segment clinical note correspond icd nine broad categories color cod respect seventeen icd nine categories propose medical segment colorer msc architecture pipeline framework work three stag one word categorization two phrase allocation three document classification msc use gate recurrent unit neural network grus map input document word multi label phrase allocations use statistical median map phrase allocation document multi label compute variable length segment color overlap phrase allocation probabilities cross level bidirectional contextual link identify adaptive context produce segment color train evaluate msc use document label mimic iii clinical note train conduct solely use document multi label without information phrase segment word addition color clinical note msc generate byproducts document multi label word tag creation icd9 category keyword list base segment color performance comparison msc byproduct document multi label versus methods whose purpose produce justifiable document multi label sixty-four vs five hundred and twenty-four micro average f1 score caml cnn attention multi label method evaluation msc segment color result medical practitioners independently assign color broad icd9 categories give sample forty color note sample fifty word relate category base word tag binary score evaluation median value eight hundred and thirty-three mean six hundred and thirty-seven
despite impressive vision language vl pretraining bert base encoder vl understand pretraining universal encoder decoder vl understand generation remain challenge difficulty originate inherently different peculiarities two discipline eg vl understand task capitalize unrestricted message pass across modalities generation task employ visual textual message pass paper start two stream decouple design encoder decoder structure two decouple cross modal encoder decoder involve separately perform type proxy task simultaneous vl understand generation pretraining moreover vl pretraining dominant way replace input visual word tokens mask tokens enforce multi modal encoder decoder reconstruct original tokens mask token involve fine tune downstream task alternative propose primary schedule sample strategy elegantly mitigate discrepancy via pretraining encoder decoder two pass manner extensive experiment demonstrate compel generalizability pretrained encoder decoder fine tune four vl understand generation downstream task source code available urlhttps githubcom yehli tden
practical sequence classification task natural language process often suffer low train data availability target class recent work towards mitigate problem focus transfer learn use embeddings pre train often unrelated task instance language model adopt alternative approach transfer learn ensemble relate task use prototypical network meta learn paradigm use intent classification case study demonstrate increase variability train task significantly improve classification performance apply data augmentation conjunction meta learn reduce sample bias make use conditional generator data augmentation train directly use meta learn objective simultaneously prototypical network hence ensure data augmentation customize task explore augmentation sentence embed space well prototypical embed space combine meta learn augmentation provide upto six hundred and forty-nine eight hundred and fifty-three relative f1 score improvements best perform systems five shoot ten shoot learn respectively
bridge performance gap high low resource languages focus much previous work typological feature databases world atlas language structure wals prime candidate data exist even low resource languages however previous work find minor benefit use typological information hypothesis model train cross lingual set pick typological cue input data thus overshadow utility explicitly use feature verify hypothesis blind model typological information investigate cross lingual share performance impact model base cross lingual architecture latent weight govern share languages learn train show prevent model exploit typology severely reduce performance control experiment reaffirm ii encourage share accord typology somewhat improve performance
deep neural network powerful statistical learners however predictions come explanation process analyze model explanation methods develop present novel explanation method call olm natural language process classifiers method combine occlusion language model techniques central explainability nlp respectively olm give explanations theoretically sound easy understand make several contributions theory explanation methods axioms explanation methods interest theoretical concept explore basics deduce methods introduce new axiom give intuition show contradict another exist axiom additionally point theoretical difficulties exist gradient base occlusion base explanation methods natural language process provide extensive argument evaluation explanation methods difficult compare olm explanation methods underline uniqueness experimentally finally investigate corner case olm discuss validity possible improvements
image caption focus generalize image draw distribution train set challenge problem generalize different distributions image recently nikolaus et al two thousand and nineteen introduce dataset assess compositional generalization image caption model evaluate ability describe image unseen adjective noun noun verb compositions work investigate different methods improve compositional generalization plan syntactic structure caption experiment show jointly model tokens syntactic tag enhance generalization rnn transformer base model also improve performance standard metrics
computational model political discourse task become increasingly important area research natural language process populist rhetoric rise across political sphere recent years however computational approach scarce due complex nature paper present new textitus vs dataset consist six thousand, eight hundred and sixty-one reddit comment annotate populist attitudes first large scale computational model phenomenon investigate relationship populist mindsets social group well range emotions typically associate set baseline two task relate populist attitudes present set multi task learn model leverage demonstrate importance emotion group identification auxiliary task
present textscvx2text framework text generation multimodal input consist video plus text speech audio order leverage transformer network show effective model language modality first convert set language embeddings learnable tokenizer allow approach perform multimodal fusion language space thus eliminate need ad hoc cross modal fusion modules address non differentiability tokenization continuous input eg video audio utilize relaxation scheme enable end end train furthermore unlike prior encoder model network include autoregressive decoder generate open end text multimodal embeddings fuse language encoder render approach fully generative make directly applicable different videox text problems without need design specialize network head task propose framework conceptually simple also remarkably effective experiment demonstrate approach base single architecture outperform state art three video base text generation task caption question answer audio visual scene aware dialog
surge user generate textual information recent increase use summarization algorithms provide overview extensive content traditional metrics evaluation algorithms eg rouge score rely match algorithmic summaries human generate ones however show textual content heterogeneous eg come different socially salient group exist summarization algorithms represent social group differently compare distribution original data mitigate adverse impact fairness preserve summarization algorithms also propose study consider normative notions fairness perspective writers content neglect readers perceptions underlie fairness notions bridge gap work study interplay fairness notions readers perceive textual summaries experiment show reader perception fairness often context sensitive moreover standard rouge evaluation metrics unable quantify perceive unfairness summaries end propose human loop metric automate graph base methodology quantify perceive bias textual summaries demonstrate utility quantify unfairness several summaries heterogeneous socio political microblog datasets
neuroscientists evaluate deep neural network natural language process possible candidate model language process brain model often train without explicit linguistic supervision show learn linguistic structure absence supervision man et al two thousand and twenty potentially question relevance symbolic linguistic theories model cognitive process warstadt bowman two thousand and twenty evaluate across two fmri datasets whether language model align better brain record attention bias annotations syntactic semantic formalisms use structure dependency minimal recursion semantic annotations find alignments improve significantly one datasets another dataset see mix result present extensive analysis result propose approach enable evaluation target hypotheses composition mean brain expand range possible scientific inferences neuroscientist could make open new opportunities cross pollination computational neuroscience linguistics
number work show gain incorporate source side symbolic syntactic semantic structure neural machine translation nmt much fewer work address decode structure propose general transformer base approach tree graph decode base generate sequence transition inspire similar approach use rnns dyer two thousand and sixteen experiment use propose decoder universal dependencies syntax english german german english english russian show improve performance standard transformer decoder well ablate versions modeltacltxtfootnoteall code implement present model release upon acceptance
paper describe method tune transformer base pretrained model adaptation reliable intelligence identification vietnamese snss problem also propose model combine bert base pretrained model metadata feature number comment number like image sns document improve result vlsp share task reliable intelligence identification vietnamese snss appropriate train techniques model able achieve nine thousand, three hundred and ninety-two roc auc public test set final version settle top two roc auc nine thousand, five hundred and thirteen private test set
paper describe joint effort telef onica research development automatic speech recognition systems albayzin two thousand and twenty challenge compare approach base either hybrid end end model hybrid model explore impact specaugment layer performance end end model use convolutional neural network gate linear units glus performance model also evaluate additional n gram language model improve word error rat inspect source separation methods extract speech noisy environment ie tv show precisely assess effect use neural base music separator name demucs fusion best systems achieve two thousand, three hundred and thirty-three wer official albayzin two thousand and twenty evaluations aside techniques use final submit systems also describe efforts retrieve high quality transcripts train
keyword spot particular wake word wuw detection important task voice assistants common issue voice assistants get easily activate background noise like music tv background speech accidentally trigger device paper propose speech enhancement se model adapt task wuw detection aim increase recognition rate reduce false alarm presence type noise se model fully convolutional denoising auto encoder waveform level train use log mel spectrogram waveform reconstruction losses together bce loss simple wuw classification network new database purposely prepare task recognize wuw challenge condition contain negative sample phonetically similar keyword database extend public databases exhaustive data augmentation simulate different noise environments result obtain concatenate se simple state art wuw detectors show se negative impact recognition rate quiet environments increase performance presence noise especially se wuw detector train jointly end end
differential privacy gain popularity machine learn strong privacy guarantee contrast privacy mitigation techniques k anonymity however apply differential privacy n gram count significantly degrade utility derive language model due large vocabularies propose differential privacy mechanism use public data prior bayesian setup provide tighter bound privacy loss metric epsilon thus better privacy utility trade off first transform count log space approximate distribution public private data gaussian posterior distribution evaluate softmax apply produce probability distribution technique achieve eighty-five reduction kl divergence compare previously know mechanisms epsilon equal one compare mechanism k anonymity n gram language model task show offer competitive performance large vocabulary size also provide superior privacy protection
natural language process nlp represent task automatic handle natural human language machinesthere large spectrum possible applications nlp help automate task like translate text one language retrieve summarize data huge repositories spam email filter identify fake news digital media find sentiment feedback people find political opinions view people various government policies provide effective medical assistance base past history record patient etc hindi official language india nearly six hundred and ninety-one million users india three hundred and sixty-six million rest world present number government private sector project researchers india abroad work towards develop nlp applications resources indian languages survey give report resources applications available hindi language nlp
text classification basic natural language process task wide range applications range sentiment analysis topic classification recently deep learn approach base cnn lstm transformers de facto approach text classification work highlight common issue associate approach show systems reliant important word present text useful classification limit train data discriminative train strategy approach tend ignore semantic mean sentence rather focus keywords important n grams propose simple black box technique shuttext present shortcomings model identify reliance model keywords involve randomly shuffle word sentence evaluate classification accuracy see common text classification datasets little effect shuffle high probability model predict original class also evaluate effect language model pretraining model try answer question around model robustness domain sentence show simple model base cnn lstm well complex model like bert questionable term syntactic semantic understand
work robust efficient text speech tts synthesis system name triple propose large scale online application key components triple one sequence sequence model adopt novel multi guidance attention transfer complementary advantage guide attention mechanisms basic attention mechanism without domain performance loss online service modification compare single attention mechanism multi guidance attention bring better naturalness long sentence synthesis also reduce word error rate two hundred and sixty-eight two new efficient multi band multi time vocoder framework reduce computational complexity twenty-eight ten gflop speed lpcnet 275x single cpu
affect preferences vary user demographics tap demographic information provide important cue users language preferences paper utilize user demographics propose empathbert demographic aware framework empathy prediction base bert several comparative experiment show empathbert surpass traditional machine learn deep learn model illustrate importance user demographics predict empathy distress user responses stimulative news article also highlight importance affect information responses develop affect aware model predict user demographic attribute
use language subject variation time well across social group knowledge domains lead differences even monolingual scenario variation word usage often call lexical semantic change lsc goal lsc characterize quantify language variations respect word mean measure distinct two language source people language model hardly data available task solutions involve unsupervised methods align two embeddings predict semantic change respect distance measure end propose self supervise approach model lexical semantic change generate train sample introduce perturbations word vectors input corpora show method use detection semantic change alignment method furthermore use choose landmark word use alignment lead substantial improvements exist techniques alignment illustrate utility techniques use experimental result three different datasets involve word different mean methods provide significant improvements also lead novel find lsc problem
entity link one essential task information extraction natural language understand entity link mainly consist two task recognition disambiguation name entities study address two task separately focus one moreover state art entity link algorithms either supervise poor performance absence annotate corpora language dependent appropriate multi lingual applications paper introduce unsupervised language independent entity disambiguation ulied utilize novel approach disambiguate link name entities evaluation ulied different english entity link datasets well available persian dataset illustrate ulied case outperform state art unsupervised multi lingual approach
customer satisfaction important factor create maintain long term relationships customers near real time identification potentially dissatisfy customers follow phone call provide organizations opportunity take meaningful interventions foster ongoing customer satisfaction loyalty work describe fully operational system develop large us company predict customer satisfaction follow incoming phone call system take input speech text transcriptions call predict call satisfaction report customers post call survey scale one ten ordinal subjective often highly skew nature predict survey score trivial task present several model challenge introduce graph neural network gnn approach take account comparative nature problem consider relative score among batch instead pair call train approach produce accurate predictions previous approach include standard regression classification model directly fit survey score call data propose approach easily generalize customer satisfaction prediction problems
recent developments natural language process nlp demonstrate large scale self supervise pre train extremely beneficial downstream task ideas adapt domains include analysis amino acid sequence proteins however date attempt protein sequence rely direct mask language model style pre train work design new adversarial pre train method proteins extend specialize similar advance nlp show compel result comparison traditional mlm pre train though development need ensure gain worth significant computational cost
use share private paradigm adversarial train significantly improve performances multi domain text classification mdtc model however two issue exist methods first instance multiple domains sufficient domain invariant feature extraction second align marginal distributions may lead fatal mismatch paper propose mixup regularize adversarial network mran address two issue specifically domain category mixup regularizations introduce enrich intrinsic feature share latent space enforce consistent predictions train instance learn feature domain invariant discriminative conduct experiment two benchmarks amazon review dataset fdu mtl dataset approach two datasets yield average accuracies eight thousand, seven hundred and sixty-four eight hundred and ninety respectively outperform relevant baselines
present truthbot one multilingual conversational chatbot design seek truth trustworthy verify information specific topics help users obtain information specific certain topics fact check information get recent news chatbot learn intent query train deep neural network data previous intents respond appropriately classify intent one class class implement separate module use either curated knowledge base search web obtain correct information topic chatbot currently set covid nineteen however bot easily customize topic specific responses experimental result show module perform significantly better closest competitor verify quantitatively several user base survey multiple languages truthbot deploy june two thousand and twenty currently run
recently multimodal transformer model gain popularity performance language vision task suggest learn rich visual linguistic representations focus zero shoot image retrieval task study three important factor impact quality learn representations pretraining data attention mechanism loss function pretraining model six datasets observe dataset noise language similarity downstream task important indicators model performance architectural analysis learn model multimodal attention mechanism outperform deeper model modality specific attention mechanisms finally show successful contrastive losses use self supervise learn literature yield similar performance gain use multimodal transformers
majority chinese character monophonic ietheir pronunciations unique thus induce easily use check table counterparts polyphonic character one pronunciation perform linguistic computation task relate speak mandarin chinese correct pronunciation polyphone must identify among several candidates accord context process call polyphone disambiguation key procedure grapheme phoneme g2p conversion step chinese text speech tts system problem well explore knowledge base learn base approach yet remain challenge due lack publicly available datasets complex language phenomenon concern polyphone paper propose novel semi supervise learn ssl framework mandarin chinese polyphone disambiguation potentially leverage unlimited unlabeled text data explore effect various proxy label strategies include entropy thresholding lexicon base label architecture pre train model electra combine convolution blstm layer fine tune task qualitative quantitative experiment demonstrate method achieve state art performance mandarin chinese polyphone disambiguation addition publish novel dataset specifically polyphone disambiguation task promote research
commonsense knowledge prove beneficial variety application areas include question answer natural language understand previous work explore collect commonsense knowledge triple automatically text increase coverage current commonsense knowledge graph investigate machine learn approach mine commonsense knowledge triple use dictionary term definitions input provide initial evaluation result start extract candidate triple use part speech tag pattern text compare performance three exist model triple score experiment show term definitions contain valid novel commonsense knowledge triple semantic relations also indicate challenge use exist triple score model
research area automatic essay grade aeg gear towards score essay holistically also work do score individual essay traits paper describe way score essay holistically use multi task learn mtl approach score essay holistically primary task score essay traits auxiliary task compare result single task learn stl approach use lstms bilstms also compare result auxiliary task task do aeg systems find traits work best different type essay conduct ablation test essay traits also report runtime number train parameters system find mtl base bilstm system give best result score essay holistically well perform well score essay traits
recent years witness significant improvement asr systems recognize speak utterances however still challenge task noisy domain data substitution deletion errors prevalent transcribe text errors significantly degrade performance downstream task work propose bert style language model refer phonemebert learn joint language model phoneme sequence asr transcript learn phonetic aware representations robust asr errors show phonemebert use downstream task use phoneme sequence additional feature also low resource setup asr transcripts downstream task phoneme information available evaluate approach extensively generate noisy data three benchmark datasets stanford sentiment treebank trec atis sentiment question intent classification task respectively result propose approach beat state art baselines comprehensively dataset
social media become popular percolate almost aspects daily live online post prove convenient individual users also foster fast spread various rumor rapid wide percolation rumor persistent adverse detrimental impact therefore researchers invest great efforts reduce negative impact rumor towards end rumor classification system aim detect track verify rumor social media systems typically include four components rumor detector ii rumor tracker iii stance classifier iv veracity classifier order improve state art rumor detection track verification propose vroc tweet level variational autoencoder base rumor classification system vroc consist co train engine train variational autoencoders vaes rumor classification components co train engine help vaes tune latent representations classifier friendly also show vroc able classify unseen rumor high level accuracy pheme dataset vroc consistently outperform several state art techniques observe unobserved rumor two hundred and sixty-nine term macro f1 score
paper study model base bilstm bert generate hashtags brazilian portuguese use ecommerce websites process corpus ecommerce review title products input generate hashtags output evaluate result use four quantitatives metrics nist bleu meteor crowdsourced score word cloud use qualitative metric besides computer meter metrics nist bleu meteor show bad result crowdsourced show amaze score conclude texts generate neural network promise use hashtags products ecommerce websites one code work available https githubcom augustocamargo text hashtag
revisit challenge problem resolve prepositional phrase pp attachment ambiguity date propose solutions either rule base explicit grammar rule direct resolve ambiguities statistical decision learn corpus label examples argue explicit commonsense knowledge base provide essential ingredient make good attachment decisions implement module name patch comm use variety conventional parsers make attachment decisions commonsense kb provide direct answer fall back general system infer knowledge base assertions manner similar way nlp systems handle vocabulary word result suggest commonsense knowledge base approach provide best worlds integrate rule base statistical techniques field increasingly come recognize importance explainability ai commonsense approach enable nlp developers better understand behavior systems facilitate natural dialogues end users
strong market advocacy benefit cannabis use improve mental health cannabis legalization priority among legislators however preliminary scientific research conclusively associate cannabis improve mental health study explore relationship depression consumption cannabis target social media corpus involve personal use cannabis intent derive potential mental health benefit use tweet contain association among three categories annotate domain experts reason effect addiction state art natural langauge process techniques fall short extract relationships cannabis phrase depression indicators seek address limitation use domain knowledge specifically drug abuse ontology addiction augment diagnostic statistical manual mental disorder lexicons mental health lack annotations due limit availability domain experts time use supervise contrastive learn conjunction gpt three train vast corpus achieve improve performance even limit supervision experimental result show method significantly extract cannabis depression relationships better state art relation extractor high quality annotations provide use nearest neighbor approach use learn representations use scientific community understand association cannabis depression better
character convey mean sequence character propose unsupervised distributional method learn abstract mean bear units sequence character rather segment sequence model discover continuous representations object sequence use recently propose architecture object discovery image call slot attention train model different languages evaluate quality obtain representations probe classifiers experiment show promise result ability units capture mean higher level abstraction
many applications machine learn certain categories examples may underrepresented train data cause systems underperform shoot case test time common remedy perform data augmentation duplicate underrepresented examples heuristically synthesize new examples remedy often fail cover full diversity complexity real examples propose data augmentation approach perform neural example extrapolation ex2 give handful exemplars sample distribution ex2 synthesize new examples also belong distribution ex2 model learn simulate example generation procedure data rich slice data apply underrepresented shoot slice apply ex2 range language understand task significantly improve state art methods multiple shoot learn benchmarks include relation extraction fewrel intent classification slot fill snip
semantic parse map natural language nl utterances logical form lfs underpin many advance nlp problems semantic parsers gain performance boost deep neural network inherit vulnerabilities adversarial examples paper provide empirical study robustness semantic parsers presence adversarial attack formally adversaries semantic parse consider perturb utterance lf pair whose utterances exactly mean original ones scalable methodology propose construct robustness test set base exist benchmark corpora result answer five research question measure sate art parsers performance robustness test set evaluate effect data augmentation
public datasets often use evaluate efficacy generalizability state art methods many task natural language process nlp however presence overlap train test datasets lead inflate result inadvertently evaluate model ability memorize interpret ability generalize addition data set may provide effective indicator performance methods real world scenarios identify leakage train data test data several publicly available datasets use evaluate nlp task include name entity recognition relation extraction study assess impact leakage model ability memorize versus generalize
recent advance language vision push forward research caption single image describe visual differences image pair suppose two image i1 i2 task generate description w12 compare exist methods directly model i1 i2 w12 map without semantic understand individuals paper introduce learn compare l2c model learn understand semantic structure two image compare learn describe one demonstrate l2c benefit comparison explicit semantic representations single image caption generalize better new test image pair outperform baseline automatic evaluation human evaluation bird word dataset
world open end non stationary constantly evolve thus talk talk change time inherent dynamic nature language come stark contrast current static language model paradigm construct train evaluation set overlap time periods despite recent progress demonstrate state art transformer model perform worse realistic setup predict future utterances beyond train period consistent pattern across three datasets two domains find increase model size alone key driver behind recent progress provide solution temporal generalization problem model continually update knowledge new information indeed slow degradation time hence give compilation ever larger language model train datasets combine grow list language model base nlp applications require date knowledge world argue right time rethink static language model evaluation protocol develop adaptive language model remain date respect ever change non stationary world
last years substantial increase prediction performances natural language process model text base supervise learn task especially deep learn model base transformer architecture vaswani et al two thousand and seventeen use transfer learn set contribute development transformer base model transfer learn potential achieve higher prediction accuracies relatively train data instance likely benefit social scientists seek accurate possible text base measure limit resources annotate train data enable social scientists leverage potential benefit research paper explain methods work might advantageous limitations additionally three transformer base model transfer learn bert devlin et al two thousand and nineteen roberta liu et al two thousand and nineteen longformer beltagy et al two thousand and twenty compare conventional machine learn algorithms three social science applications across evaluate task textual style train data set size conventional model consistently outperform transfer learn transformer base model thereby demonstrate potential benefit model bring text base social science research
develop high performance multilingualabstract mean representation amr sys tems project english amr annotationsto languages weak supervision weachieve goal bootstrapping transformer base multilingual word embeddings partic ular cross lingual roberta xlm r large develop novel technique forforeign text english amr alignment usingthe contextual word alignment en glish foreign language tokens wordalignment weakly supervise rely onthe contextualized xlm r word embeddingswe achieve highly competitive performancethat surpass best publish result forgerman italian spanish chinese
paper develop compositional vector base semantics positive transitive sentence quantum natural language process non english language ie persian compare parametrized quantum circuit two synonymous sentence two languages english persian consider grammarmeaning transitive sentence translate discocat diagram via zx calculus quantum circuit form also use bigraph method rewrite discocat diagram turn quantum circuit semantic side
seventh speak language world use bangla language online increase recent time hence become important analyze bangla text data maintain safe harassment free online place data make accessible article gather mark comment people public post celebrities government officials athletes facebook total amount collect comment forty-four thousand and one dataset compile aim develop ability machine differentiate whether comment bully expression help natural language process extent improper inappropriate comment comment label different categories harassment exploratory analysis different perspectives also include paper detail overview due scarcity data collection categorize bengali language comment dataset significant role research detect bully word identify inappropriate comment detect different categories bengali bully etc dataset publicly available https datamendeleycom datasets 9xjx8twk8p
october 14th two thousand and twenty researchers openai stanford institute human center artificial intelligence universities convene discuss open research question surround gpt three largest publicly disclose dense language model time meet take place chatham house rule discussants come variety research background include computer science linguistics philosophy political science communications cyber policy broadly discussion center around two main question one technical capabilities limitations large language model two societal effect widespread use large language model provide detail summary discussion organize two theme
identify user intents natural language utterances crucial step conversational systems extensively study supervise classification problem however practice new intents emerge deploy intent detection model thus model seamlessly adapt classify utterances see unseen intents unseen intents emerge deployment train data exist model target set rely heavily scarcely available train data overfit see intents data result bias misclassify utterances unseen intents see ones propose ride intent detection model leverage commonsense knowledge unsupervised fashion overcome issue train data scarcity ride compute robust generalizable relationship meta feature capture deep semantic relationships utterances intent label feature compute consider concepts utterance link intent label via commonsense knowledge extensive experimental analysis three widely use intent detection benchmarks show relationship meta feature significantly increase accuracy detect see unseen intents ride outperform state art model unseen intents
visual storytelling task generate relevant interest stories give image sequence work aim increase diversity generate stories preserve informative content image propose foster diversity informativeness generate story use concept selection module suggest set concept candidates utilize large scale pre train model convert concepts image full stories enrich candidate concepts commonsense knowledge graph create image sequence concept candidates propose obtain appropriate concepts graph propose two novel modules consider correlation among candidate concepts image concept correlation extensive automatic human evaluation result demonstrate model produce reasonable concepts enable model outperform previous model large margin diversity informativeness story retain relevance story image sequence
recently multimodal name entity recognition mner utilize image improve accuracy ner tweet however multimodal methods use attention mechanisms extract visual clue regardless whether text image relevant practically irrelevant text image pair account large proportion tweet visual clue unrelated texts exert uncertain even negative effect multimodal model learn paper introduce method text image relation propagation multimodal bert model integrate soft hard gate select visual clue propose multitask algorithm train mner datasets experiment deeply analyze change visual attention use text image relation propagation model achieve state art performance mner datasets
spell correction use detect correct orthographic mistake texts time traditional dictionary lookup string similarity methods suitable languages less complex structure english language however azerbaijani language complex structure due morphological structure derivation word plenty several word derive add suffice affix word therefore paper sequence sequence model attention mechanism use develop spell correction azerbaijani total twelve thousand wrong correct sentence pair use train model test one thousand real world misspell word f1 score result seventy-five distance zero ninety distance one ninety-six distance two
present arc da dataset direct answer open response freeform version arc ai2 reason challenge multiple choice dataset arc influential community multiple choice format unrepresentative real world question multiple choice format particularly susceptible artifacts arc da dataset address concern convert question direct answer format use combination crowdsourcing expert review result dataset contain two thousand, nine hundred and eighty-five question total eight thousand, four hundred and thirty-six valid answer question typically one valid answer arc da one first da datasets natural question often require reason appropriate question decompositions evident question describe conversion approach take appropriate evaluation metrics several strong model although high best score eighty-one genie six hundred and fourteen f1 six hundred and thirty-two rouge l still leave considerable room improvement addition dataset provide natural set new research explanation many question require reason construct answer hope dataset spur advance complex question answer community arc da available https allenaiorg data arc da
neural natural language generation nlg understand nlu model data hungry require massive amount annotate data competitive recent frameworks address bottleneck generative model synthesize weak label scale small amount train label expert curated rest data automatically annotate follow approach automatically construct large scale weakly label data fine tune gpt two employ semi supervise framework jointly train nlg nlu model propose framework adapt parameter update model accord estimate label quality e2e weather benchmarks show weakly supervise train paradigm effective approach low resource scenarios outperform benchmark systems datasets one hundred train data use
fine tune pre train language model plms demonstrate effectiveness various downstream nlp task recently however many low resource scenarios conventional fine tune strategies sufficiently capture important semantic feature downstream task address issue introduce novel framework name css lm improve fine tune phase plms via contrastive semi supervise learn specifically give specific task retrieve positive negative instance large scale unlabeled corpora accord domain level class level semantic relatedness task perform contrastive semi supervise learn retrieve unlabeled original label instance help plms capture crucial task relate semantic feature experimental result show css lm achieve better result conventional fine tune strategy series downstream task shoot settings outperform latest supervise contrastive fine tune strategies datasets source code available provide detail
last couple years attempt apply topological data analysis text particular natural language inference recent work tymochko et al suggest possibility capture notion logical shape text use topological delay embeddings technique derive dynamical systems apply word embeddings note reconstruct argument show use several old new examples problem connect logic topology text still much unsolved conclude clear answer question find circle circular argument point possible avenues exploration code use experiment also show
transformers emerge powerful tool broad range natural language process task key component drive impressive performance transformers self attention mechanism encode influence dependence tokens specific token beneficial quadratic complexity self attention input sequence length limit application longer sequence topic actively study community address limitation propose nystrofmformer model exhibit favorable scalability function sequence length idea base adapt nystrofm method approximate standard self attention ofn complexity scalability nystrofmformer enable application longer sequence thousands tokens perform evaluations multiple downstream task glue benchmark imdb review standard sequence length find nystrofmformer perform comparably case even slightly better standard self attention longer sequence task long range arena lra benchmark nystrofmformer perform favorably relative efficient self attention methods code available https githubcom mlpen nystromformer
word alignment essential stream cross lingual language understand generation task recently performance neural word alignment model exceed statistical model however heavily rely sophisticate translation model study propose super lightweight unsupervised word alignment slua model bidirectional symmetric attention train contrastive learn objective introduce agreement loss employ bind attention map alignments follow mirror like symmetry hypothesis experimental result several public benchmarks demonstrate model achieve competitive better performance compare state art word alignment significantly reduce train decode time average ablation analysis case study show superiority propose slua notably recognize model pioneer attempt unify bilingual word embed word alignments encouragingly approach achieve 164x speedup giza 50x parameter compression compare transformer base alignment methods release code facilitate community
capabilities natural language model train large scale data increase immensely past years downstream applications risk inherit bias contain model potential negative consequences especially marginalize group paper analyze occupational bias popular generative language model gpt two intersect gender five protect categories religion sexuality ethnicity political affiliation name origin use novel data collection pipeline collect 396k sentence completions gpt two find machine predict job less diverse stereotypical women men especially intersections ii fit two hundred and sixty-two logistic model show intersectional interactions highly relevant occupational associations iii give job gpt two reflect societal skew gender ethnicity us case pull distribution towards gender parity raise normative question language model learn
paper describe submission end end multi domain task completion dialog share task 9th dialog system technology challenge dstc nine participants share task build end end task completion dialog system evaluate human evaluation user simulator base automatic evaluation different traditional pipelined approach modules optimize individually suffer cascade failure propose end end dialog system one use generative pretraining two gpt two backbone jointly solve natural language understand dialog state track natural language generation task two adopt domain task adaptive pretraining tailor gpt two dialog domain finetuning three utilize heuristic pre post process rule greatly simplify prediction task improve generalizability four equip fault tolerance module correct errors inappropriate responses propose method significantly outperform baselines tie first place official evaluation make source code publicly available
intent detection slot fill two fundamental task build speak language understand slu system multiple deep learn base joint model demonstrate excellent result two task paper propose new joint model wheel graph attention network wheel gat able model interrelate connections directly intent detection slot fill construct graph structure utterances create intent nod slot nod direct edge intent nod provide utterance level semantic information slot fill slot nod also provide local keyword information intent experiment show model outperform multiple baselines two public datasets besides also demonstrate use bidirectional encoder representation transformer bert model boost performance slu task
recent work indicate many natural language understand reason datasets contain statistical cue may take advantage nlp model whose capability may thus grossly overestimate discover potential weakness model human design stress test propose expensive create generalize arbitrary model propose light weight general statistical profile framework icq see cue automatically identify possible bias multiple choice nlu datasets without need create additional test case evaluate blackbox test extent model may exploit bias
benchmark datasets significant impact accelerate research program language task paper introduce codexglue benchmark dataset foster machine learn research program understand generation codexglue include collection ten task across fourteen datasets platform model evaluation comparison codexglue also feature three baseline systems include bert style gpt style encoder decoder model make easy researchers use platform availability data baselines help development validation new methods apply various program understand generation problems
exist image retrieval systems use text query way user express look however fine grain image retrieval often require ability also express image content look text modality cumbersomely express localization preferences whereas point natural fit paper propose image retrieval setup new form multimodal query user simultaneously use speak natural language mouse trace empty canvas express characteristics desire target image describe simple modifications exist image retrieval model enable operate setup qualitative quantitative experiment show model effectively take spatial guidance account provide significantly accurate retrieval result compare text equivalent systems
mainstream image caption model rely convolutional neural network cnn image feature additional attention salient regions object generate caption via recurrent model recently scene graph representations image use augment caption model leverage structural semantics object entities relationships attribute several study note naive use scene graph black box scene graph generator harm image caption ing performance scene graph base caption mod els incur overhead explicit use image feature generate decent caption address challenge propose framework sg2caps utilize scene graph label competitive image caption ing performance basic idea close semantic gap two scene graph one derive input image one caption order achieve leverage spatial location object human object interaction hoi label additional hoi graph framework outperform exist scene graph caption model large margin cider score one hundred and ten vs seventy-one indicate scene graph promise representation image caption direct utilization scene graph label avoid expensive graph convolutions high dimensional cnn feature result 49fewer trainable parameters
model question answer dialogue agents summarization often interpret mean sentence rich context use mean new context take excerpt text problematic key piece may explicit local window isolate define problem sentence decontextualization take sentence together context rewrite interpretable context preserve mean describe annotation procedure collect data wikipedia corpus use data train model automatically decontextualize sentence present preliminary study show value sentence decontextualization user face task preprocessing systems perform document understand argue decontextualization important subtask many downstream applications definitions resources provide benefit task operate sentence occur richer context
word sense disambiguation wsd methods identify suitable mean word respect usage word specific context neural network base wsd approach rely sense annotate corpus since utilize lexical resources study utilize context relate gloss information target word model semantic relationship word set gloss propose senspick type stack bidirectional long short term memory lstm network perform wsd task experimental evaluation demonstrate senspick outperform traditional state art model benchmark datasets relative improvement thirty-five f one score improvement significant incorporate semantic relationships bring senspick lead position compare others
lexical inference context liic task recognize textual entailment two similar sentence ie sentence differ one expression therefore see variant natural language inference task focus lexical semantics formulate evaluate first approach base pretrained language model lms task shoot nli classifier ii relation induction approach base handcraft pattern express semantics lexical inference iii variant ii pattern automatically extract corpus approach outperform previous state art show potential pretrained lms liic extensive analysis investigate factor success failure three approach
multi turn dialogue read comprehension aim teach machine read dialogue contexts solve task response selection answer question major challenge involve noisy history contexts especial prerequisites commonsense knowledge unseen give material exist work mainly focus context response match approach work thus make first attempt tackle two challenge extract substantially important turn pivot utterances utilize external knowledge enhance representation context propose pivot orient deep selection model pod top transformer base language model dialogue comprehension detail model first pick pivot utterances conversation history accord semantic match candidate response question besides knowledge items relate dialogue context extract knowledge graph external knowledge pivot utterances external knowledge combine well design mechanism refine predictions experimental result four dialogue comprehension benchmark task show propose model achieve great improvements baselines series empirical comparisons conduct show selection strategies extra knowledge injection influence result
performance prediction task estimate system performance without perform experiment allow us reduce experimental burden cause combinatorial explosion different datasets languages task model paper make two contributions improve performance prediction nlp task first examine performance predictors holistic measure accuracy like f1 bleu also fine grain performance measure accuracy individual class examples second propose methods understand reliability performance prediction model two angle confidence intervals calibration perform analysis four type nlp task demonstrate feasibility fine grain performance prediction necessity perform reliability analysis performance prediction methods future make code publicly available urlhttps githubcom neulab reliable nlppp
canonical approach video language learn eg video question answer dictate neural model learn offline extract dense video feature vision model text feature language model feature extractors train independently usually task different target domains render fix feature sub optimal downstream task moreover due high computational overload dense video feature often difficult infeasible plug feature extractors directly exist approach easy finetuning provide remedy dilemma propose generic framework clipbert enable affordable end end learn video language task employ sparse sample single sparsely sample short clip video use train step experiment text video retrieval video question answer six datasets demonstrate clipbert outperform par exist methods exploit full length videos suggest end end learn sparsely sample clip often accurate use densely extract offline feature full length videos prove proverbial less principle videos datasets considerably different domains lengths range three second generic domain gif videos one hundred and eighty second youtube human activity videos show generalization ability approach comprehensive ablation study thorough analyse provide dissect factor lead success code publicly available https githubcom jayleicn clipbert
unsupervised approach extractive summarization usually rely notion sentence importance define semantic similarity sentence document propose new metrics relevance redundancy use pointwise mutual information pmi sentence easily compute pre train language model intuitively relevant sentence allow readers infer document content high pmi document redundant sentence infer summary high pmi summary develop greedy sentence selection algorithm maximize relevance minimize redundancy extract sentence show method outperform similarity base methods datasets range domains include news medical journal article personal anecdotes
language identification task identify document language applications like automatic spell checker selection language identification must use short string text message fragment work reproduce language identification architecture apple briefly sketch blog post confirm bi lstm model performance find outperform current open source language identifiers find language identification mistake due confusion relate languages
several contributions explore state art techniques text normalization problem inverse text normalization itn remain relatively unexplored best know approach leverage finite state transducer fst base model rely manually curated rule hence scalable propose efficient robust neural solution itn leverage transformer base seq2seq model fst base text normalization techniques data preparation show easily extend languages without need linguistic expert manually curate present hybrid framework integrate neural itn fst overcome common recoverable errors production environments empirical evaluations show propose solution minimize incorrect perturbations insertions deletions substitutions asr output maintain high quality even domain data transformer base model infuse pretraining consistently achieve lower wer across several datasets able outperform baselines english spanish german italian datasets
virtuosity language model like gpt three open new world possibility human ai collaboration write paper present framework generative language model conceptualize multiverse generators framework also apply human imagination core read write fiction call exploration commonality new form interfaces allow humans couple imagination ai write explore understand non linear fiction discuss early insights gain actively pursue approach develop test novel multiversal gpt three assist write interface
effective incorporation cross utterance information potential improve language model lms automatic speech recognition asr extract powerful robust cross utterance representations transformer lm tlm paper propose r tlm use hide state long short term memory lstm lm encode cross utterance information r tlm incorporate lstm module together segment wise recurrence transformer block addition lstm module output shortcut connection use fusion layer bypass lstm module also investigate propose system evaluate ami meet corpus eval2000 rt03 telephone conversation evaluation set best r tlm achieve nine six eight absolute wer reductions single utterance tlm baseline five three two absolute wer reductions strong cross utterance tlm baseline ami evaluation set eval2000 rt03 respectively improvements eval2000 rt03 support significance test r tlms find better lm score word recognition errors likely occur r tlm wer reduce interpolation lstm lm
paper explore distantly supervise relation extraction ds benefit use universal graph ug combination knowledge graph kg large scale text collection straightforward extension current state art neural model ds ug may lead degradation performance first report degradation associate difficulty learn ug propose two train strategies one path type adaptive pretraining sequentially train model different type ug paths prevent reliance single type ug path two complexity rank guide attention mechanism restrict attention span accord complexity ug path force model extract feature simple ug paths also complex ones experimental result biomedical nyt10 datasets prove robustness methods achieve new state art result nyt10 dataset code datasets use paper available https githubcom baodaiqin ugdsre
automatic speech recognition asr relevant area multiple settings provide natural communication mechanism applications users asrs often fail environments use language specific particular application domains strategies explore reduce errors close asrs post process particularly automatic spell check deep learn approach article explore use deep neural network refine result phonetic correction algorithm apply telesales audio database result exhibit reduction word error rate wer original transcription phonetic correction show viability deep learn model together post process correction strategies reduce errors make close asrs specific language domains
speak language understand slu systems extract transcriptions well semantics intent name entities speech essential components voice activate systems slu model either directly extract semantics audio compose pipelined automatic speech recognition asr natural language understand nlu model typically train via differentiable cross entropy losses even relevant performance metrics interest word semantic error rat work propose non differentiable sequence losses base slu metrics proxy semantic error use reinforce trick train asr slu model loss show custom sequence loss train state art open slu datasets lead six relative improvement asr nlu performance metrics large proprietary datasets also demonstrate semantic sequence loss train paradigm use update asr slu model without transcripts use semantic feedback alone
search engines leverage knowledge improve information access order effectively leverage knowledge search engines account context ie information user query thesis aim support search engines leverage knowledge account context first part thesis study make structure knowledge accessible user search engine proactively provide knowledge context enrich search result first task study retrieve descriptions knowledge facts text corpus next study automatically generate knowledge fact descriptions finally study contextualize knowledge facts automatically find facts relate query fact second part thesis study improve interactive knowledge gather focus conversational search user interact search engine gather knowledge large unstructured knowledge repositories focus multi turn passage retrieval instance conversational search propose model query resolution term classification task propose method address final part thesis focus search engine support professional writers news domain study support writers create event narratives explore knowledge corpus news article propose dataset construction procedure task rely exist news article simulate incomplete narratives relevant article study performance multiple rankers lexical semantic provide insights characteristics task
study utility lexical translation model ibm model one english text retrieval particular neural variants train end end use neural model1 aggregator layer apply context free contextualized query document embeddings new approach design neural rank system benefit effectiveness efficiency interpretability specifically show add interpretable neural model one layer top bert base contextualized embeddings one decrease accuracy efficiency two may overcome limitation maximum sequence length exist bert model context free neural model one less effective bert base rank model run efficiently cpu without expensive index time precomputation query time operations large tensors use model one produce best neural non neural run ms marco document rank leaderboard late two thousand and twenty
much previous work characterize language variation across internet social group focus type word use group extend type study employ bert characterize variation sense word well analyze two months english comment four hundred and seventy-four reddit communities specificity different sense cluster community combine specificity community unique word type use identify case social group language deviate norm validate metrics use user create glossaries draw sociolinguistic theories connect language variation trend community behavior find communities highly distinctive language medium size loyal highly engage users interact dense network
paper propose neural network architecture tackle query example user define keyword spot task multi head attention module add top multi layer gru effective feature extraction normalize multi head attention module propose feature aggregation also adopt softtriple loss combination triplet loss softmax loss showcase effectiveness demonstrate performance model internal datasets different languages public hey snip dataset compare performance model baseline system conduct ablation study show benefit component architecture propose work show solid performance preserve simplicity
prevail methods map large generative language model supervise task may fail sufficiently probe model novel capabilities use gpt three case study show zero shoot prompt significantly outperform shoot prompt suggest function shoot examples case better describe locate already learn task rather meta learn analysis motivate rethink role prompt control evaluate powerful language model work discuss methods prompt program emphasize usefulness consider prompt lens natural language explore techniques exploit capacity narratives cultural anchor encode nuanced intentions techniques encourage deconstruction problem components produce verdict inform encompass theory prompt program also introduce idea metaprompt seed model generate natural language prompt range task finally discuss general methods interact language model incorporate exist future benchmarks practical applications
pickup delivery service transaction classification base customer provide free text challenge problem involve association wide variety customer input fix set categories adapt various customer write style categorization important business help understand market need trend also assist build personalize experience different segment customers hence vital capture category information trend scale high precision recall paper focus specific use case single category drive transaction propose cost effective transaction classification approach base semi supervision knowledge distillation frameworks approach identify category transaction use free text input give customer use weak label notice performance gain similar use human annotate sample large internal dataset 20newsgroup dataset see roberta perform best categorization task use albert model 33x fewer parameters vis vis parameters roberta roberta teacher see performance similar roberta better performance unadapted albert framework albert student roberta teacher refer r albert paper model production use business understand change trend take appropriate decisions
workshop paper use empirical example ongoing fieldwork showcase complexity situatedness process make sense algorithmic result ie evaluate validate contextualize algorithmic output far research work focus sense make process data analytic learn environments classrooms train workshops multiple moments fieldwork suggest mean data analytics construct iterative reflexive dialogue data code assumptions prior knowledge algorithmic result data analytic result nothing short sociotechnical accomplishment one extremely difficult time impossible clearly distinguish human technical form data analytic work conclude paper set question would like explore workshop
back translation effective strategy improve performance neural machine translationnmt generate pseudo parallel data however several recent work find better translation quality pseudo parallel data necessarily lead better final translation model lower quality diverse data often yield stronger result paper propose novel method generate pseudo parallel data pre train back translation model method meta learn algorithm adapt pre train back translation model pseudo parallel data generate would train forward translation model well validation set evaluations standard datasets wmt en de fourteen wmt en fr fourteen well multilingual translation set method lead significant improvements strong baselines code make available
recommendation reason generation aim show sell point products customers play vital role attract customers attention well improve user experience simple effective way extract keywords directly knowledge base products ie attribute title recommendation reason however generate recommendation reason product knowledge naturally respond users interest fortunately e commerce websites exist user generate content user content short ie product question answer qa discussions reflect user care aspects therefore paper consider generate recommendation reason take account product attribute also customer generate product qa discussions reality adequate user content possible popular commodities whereas large sum long tail products new products gather sufficient number user content tackle problem propose user inspire multi source posterior transformer mspt induce model reflect users interest posterior multiple qa discussions module generate recommendation reason contain product attribute well user care aspects experimental result show model superior traditional generative model additionally analysis also show model focus user care aspects baselines
present novel large context end end automatic speech recognition e2e asr model effective train method base knowledge distillation common e2e asr model mainly focus utterance level process utterance independently transcribe hand large context e2e asr model take account long range sequential contexts beyond utterance boundaries well handle sequence utterances discourse conversations however transformer architecture recently achieve state art asr performance among utterance level asr systems yet introduce large context asr systems expect transformer architecture leverage effectively capture input speech contexts also long range sequential contexts beyond utterance boundaries therefore paper propose hierarchical transformer base large context e2e asr model combine transformer architecture hierarchical encoder decoder base large context model addition order enable propose model use long range sequential contexts also propose large context knowledge distillation distill knowledge pre train large context language model train phase evaluate effectiveness propose model propose train method japanese discourse asr task
recent years see proliferation attention mechanisms rise transformers natural language generation nlg previously state art nlg architectures rnn lstm run vanish gradient problems sentence grow larger distance position remain linear sequential computation hinder parallelization since sentence process word word transformers usher new era paper explore three major transformer base model namely gpt bert xlnet carry significant implications field nlg burgeon area bolster rapid developments attention mechanisms poetry generation summarization text generation derive benefit transformer base language model achieve groundbreaking result
paper present novel self supervise learn method handle conversational document consist transcribe text human human conversations one key technologies understand conversational document utterance level sequential label label estimate document utterance utterance manner main issue utterance level sequential label difficulty collect label conversational document manual annotations costly deal issue propose large context conversational representation learn lc crl self supervise learn method specialize conversational document self supervise learn task lc crl involve estimation utterance use surround utterances base large context language model way lc crl enable us effectively utilize unlabeled conversational document thereby enhance utterance level sequential label result experiment scene segmentation task use contact center conversational datasets demonstrate effectiveness propose method
paper first study apply deep mutual learn dml end end asr model dml multiple model train simultaneously collaboratively mimic throughout train process help attain global optimum prevent model make confident predictions previous study apply dml simple multi class classification problems study use complex sequence sequence map problems reason paper present method apply dml state art transformer base end end asr model particular propose combine dml recent representative train techniques ie label smooth schedule sample specaugment essential powerful end end asr model expect train techniques work well dml dml complementary characteristics experiment two setups japanese asr task large scale model compact model demonstrate dml improve asr performance model setups compare conventional learn methods include knowledge distillation also show combine dml exist train techniques effectively improve asr performance
transformer demonstrate great power learn contextual word representations multiple languages single model process multilingual sentence model learnable vector usually assign language call language embed language embed either add word embed attach begin sentence serve language specific signal transformer capture contextual representations across languages paper revisit use language embed identify several problems exist formulations investigate interaction language embed word embed self attention module find current methods reflect language specific word correlation well give find propose new approach call cross lingual language projection xlp replace language embed sentence xlp project word embeddings language specific semantic space project embeddings feed transformer model process language specific mean way xlp achieve purpose appropriately encode language multilingual transformer model experimental result show xlp freely significantly boost model performance extensive multilingual benchmark datasets cod model release https githubcom lsj2408 xlp
present coco lm new self supervise learn framework pretrains language model correct challenge errors contrast text sequence coco lm employ auxiliary language model mask predict tokens original text sequence create challenge pretraining input noise sample base likelihood auxiliary language model coco lm pretrains two task first task corrective language model learn correct auxiliary model corruptions recover original tokens second task sequence contrastive learn ensure language model generate sequence representations invariant noise transformations experiment glue squad benchmarks coco lm outperform recent pretraining approach various pretraining settings shoot evaluations higher pretraining efficiency analyse reveal coco lm advantage come challenge train signal contextualized token representations regularize sequence representations
rule base dialogue management still popular solution industrial task orient dialogue systems interpretablility however hard developers maintain dialogue logic scenarios get complex hand data drive dialogue systems usually end end structure popular academic research easier deal complex conversations methods require plenty train data behaviors less interpretable paper propose method leverage strength rule base data drive dialogue managers dm firstly introduce dm carina dialog system cds advance industrial dialogue system build microsoft propose model trigger design make dm trainable thus scalable scenario change furthermore integrate pre train model empower dm shoot capability experimental result demonstrate effectiveness strong shoot capability method
skip gram sg model learn word representation predict word surround center word unstructured text data however word context window contribute mean center word example less relevant word could context window hinder sg model learn better quality representation paper propose enhance version sg leverage context information produce word representation propose model contextual skip gram design predict contextual word center word context information simple idea help reduce impact irrelevant word train process thus enhance final performance
open domain table text generation notice unfaithful generation usually contain hallucinate content align input table record thus try evaluate generation faithfulness two entity centric metrics table record coverage ratio hallucinate entities text show strong agreement human judgements base metrics quantitatively analyze correlation train data quality generation fidelity indicate potential usage entity information faithful generation motivate find propose two methods faithful generation one augment train incorporate auxiliary entity information include augment plan base model unsupervised model two train instance selection base faithfulness rank show approach improve generation fidelity full dataset set shoot learn settings automatic human evaluations
paper describe participation uvailps group trec cast two thousand and twenty track passage retrieval pipeline consist initial retrieval module use bm25 ii rank module combine score bert rank model score machine comprehension model adjust passage retrieval important challenge conversational passage retrieval query often specify thus perform query resolution add miss context conversation history current turn query use quretec term classification query resolution model show best automatic manual run outperform correspond median run large margin
present first version system interactive generation theatre play script system base vanilla gpt two model several adjustments target specific issue encounter practice also list issue encounter plan solve future version system present system use generate theatre play script plan premiere february two thousand and twenty-one
covid nineteen pandemic sweep across world accompany tsunami fake news misinformation social media time reliable information vital public health safety covid nineteen relate fake news spread even faster facts time covid nineteen pandemic fake news intellectual confusion also place live people risk call immediate need contain spread misinformation social media introduce ctf first covid nineteen twitter fake news dataset label genuine fake tweet additionally propose cross sean cross stitch base semi supervise end end neural attention model leverage large amount unlabelled data cross sean partially generalise emerge fake news learn relevant external knowledge compare cross sean seven state art fake news detection methods observe achieve ninety-five f1 score ctf outperform best baseline nine also develop chrome sean cross sean base chrome extension real time detection fake tweet
standard approach incorporate linguistic information neural machine translation systems consist maintain separate vocabularies annotate feature incorporate eg pos tag dependency relation label embed aggregate subword word belong approach however easily accommodate annotation scheme dense every word propose method suit case show large improvements domain data comparable quality domain data experiment perform morphologically rich languages like basque german case low resource scenarios
availability large scale image caption visual question answer datasets contribute significantly recent successes vision language pre train however datasets often collect overrestrictive requirements inherit original target task eg image caption generation limit result dataset scale diversity take step push limit vision language pre train data relax data collection pipeline use conceptual caption 3m cc3m sharma et al two thousand and eighteen introduce conceptual 12m cc12m dataset twelve million image text pair specifically mean use vision language pre train perform analysis dataset benchmark effectiveness cc3m multiple downstream task emphasis long tail visual recognition result clearly illustrate benefit scale pre train data vision language task indicate new state art result nocaps conceptual caption benchmarks
key challenge abstractive summarization ensure factual consistency generate summary respect original document example state art model train exist datasets exhibit entity hallucination generate name entities present source document propose set new metrics quantify entity level factual consistency generate summaries show entity hallucination problem alleviate simply filter train data addition propose summary worthy entity classification task train process well joint entity summary generation approach yield improvements entity level metrics
clinical cod task assign set alphanumeric cod refer icd international classification diseases medical event base context capture clinical narrative latest version icd icd ten include seventy thousand cod labor intensive error prone task automatic icd cod medical report use machine learn gain significant interest last decade exist literature model problem multi label task nevertheless multi label approach challenge due extremely large label set size furthermore interpretability predictions essential endusers eg healthcare providers insurance company paper propose novel approach automatic icd cod reformulate extreme multi label problem simpler multi class problem use hierarchical solution make approach viable extensive data collection acquire phrase level human coder annotations supervise model learn specific relations input text predict icd cod approach employ two independently train network sentence tagger icd classifier stack hierarchically predict codeset medical report sentence tagger identify focus sentence contain medical event concept relevant icd cod use supervise attention mechanism icd classifier assign focus sentence icd code propose approach outperform strong baselines large margins twenty-three subset accuracy eighteen micro f1 fifteen instance base f one propose approach interpretability achieve implicitly learn attention score attribute prediction particular sentence word select human coders
idea nondeterministic typical hesitant fuzzy automata generalization fuzzy automata present costa bedregal paper present sufficient necessary condition typical hesitant fuzzy language compute nondeterministic typical hesitant fuzzy automata besides paper introduce new class typical hesitant fuzzy automata crisp transition show new class equivalent original class introduce costa bedregal
neural abstractive summarization study many piece literature achieve great success aid large corpora however encounter novel task one may always benefit transfer learn due domain shift problem overfitting could happen without adequate label examples furthermore annotations abstractive summarization costly often demand domain knowledge ensure grind truth quality thus grow appeal low resource abstractive summarization aim leverage past experience improve performance limit label examples target corpus paper propose utilize two knowledge rich source tackle problem large pre train model diverse exist corpora former provide primary ability tackle summarization task latter help discover common syntactic semantic information improve generalization ability conduct extensive experiment various summarization corpora different write style form result demonstrate approach achieve state art six corpora low resource scenarios seven trainable parameters compare previous work
fake news detection essential problem field natural language process benefit effective solution area manifold goodwill society surface level broadly match general problem text classification researchers propose various approach tackle fake news use simple well complex techniques paper try make comparison present deep learn techniques represent news instance vector space use combination common mathematical operations available vector space representations number experiment use various combinations permutations finally conclude sound analysis result evaluate reason result
address challenge problem natural language comprehension beyond plain text document introduce tilt neural network architecture simultaneously learn layout information visual feature textual semantics contrary previous approach rely decoder capable unify variety problems involve natural language layout represent attention bias complement contextualized visual information core model pretrained encoder decoder transformer novel approach achieve state art result extract information document answer question demand layout understand docvqa cord wikiops sroie time simplify process employ end end model
speech recognition systems spanish language google produce errors quite frequently use applications specific domain errors mostly occur recognize word new recognizer language model ad hoc domain article present algorithm use levenshtein distance phonemes reduce speech recognizer errors preliminary result show possible correct recognizer errors significantly use metric use dictionary specific phrase domain application despite design particular domains algorithm propose general application phrase must recognize explicitly define application without algorithm modify enough indicate algorithm set sentence must work algorithm complexity oftn number word transcript correct n number phrase specific domain
relation extraction use populate knowledge base important many applications prior datasets use train relation extraction model either suffer noisy label due distant supervision limit certain domains small train high capacity model constrain downstream applications relation extraction therefore introduce webred web relation extraction dataset strongly supervise human annotate dataset extract relationships variety text find world wide web consist 110k examples also describe methods use collect 200m examples pre train data task show combine pre train large weakly supervise dataset fine tune small strongly supervise dataset lead better relation extraction performance provide baselines new dataset present case importance human annotation improve performance relation extraction text find web
gpt three perform numerous task provide natural language prompt contain train examples show type shoot learn unstable choice prompt format train examples even order train examples accuracy vary near chance near state art demonstrate instability arise bias language model towards predict certain answer eg place near end prompt common pre train data mitigate first estimate model bias towards answer ask prediction give train prompt content free test input n fit calibration parameters prediction input uniform across answer diverse set task contextual calibration procedure substantially improve gpt three gpt two average accuracy three hundred absolute reduce variance across different choices prompt
bert model show significant success various natural language process task however due heavy model size high computational cost model suffer high latency fatal deployments resource limit devices tackle problem propose dynamic inference method bert via trainable gate variables apply input tokens regularizer bi modal property method show reduce computational cost glue dataset minimal performance drop moreover model adjust trade performance computational cost user specify hyperparameter
paper describe system submit team kbcnmujal task two share task hate speech offensive content identification indo european languages hasoc forum information retrieval evaluation december sixteen twenty two thousand and twenty hyderabad india datasets two dravidian languages viz malayalam tamil size four thousand observations share hasoc organizers datasets use train machine use different machine learn algorithms base classification regression model datasets consist tweet youtube comment two class label offensive offensive machine train classify social media message two categories appropriate n gram feature set extract learn specific characteristics hate speech text message feature model base tfidf weight n gram refer work respective experiment show feature word character combine model word character n grams could use identify term pattern offensive text content part hasoc share task test data set make available hasoc track organizers best perform classification model develop languages apply test datasets model give highest accuracy result train dataset malayalam language experiment predict categories respective test data system obtain f1 score seventy-seven similarly best perform model tamil language obtain f1 score eighty-seven work receive 2nd 3rd rank share task two malayalam tamil language respectively propose system name hasockbcnmujal
prosody speak word determine surround context incremental text speech synthesis synthesizer produce output access complete input full context often unknown result loss naturalness synthesize speech paper investigate whether use predict future text attenuate loss compare several test condition next future word unknown zero word b language model predict c randomly predict grind truth measure prosodic feature pitch energy duration find predict text provide significant improvements zero word lookahead slight gain random word lookahead confirm result perceptive test
joint event causality extraction challenge yet essential task information retrieval data mine recently pre train language model eg bert yield state art result dominate variety nlp task however model incapable impose external knowledge domain specific extraction consider prior knowledge frequent n grams represent effect events may benefit event causality extraction paper propose convolutional knowledge infusion frequent n grams different windows length within joint extraction framework knowledge infusion convolutional filter initialization help model capture intra event ie feature event cluster inter event ie associations across event cluster feature also boost train convergence experimental result benchmark datasets show model significantly outperform strong bertcsnn baseline
sentiment analysis vast area machine learn domain lot work do datasets analysis english language pakistan huge amount data roman urdu language scatter social sit include twitter youtube facebook similar applications study focus domain dataset gather youtube comment dataset contain comment people different pakistani dramas tv show dataset contain multi class classification group comment positive negative neutral sentiment study comparative analysis do five supervise learn algorithms include linear regression svm knn multi layer perceptron nai bay classifier accuracy recall precision f measure use measure performance result show accuracy svm sixty-four percent better rest list
social media often act breed ground different form offensive content low resource languages like tamil situation complex due poor performance multilingual language specific model lack proper benchmark datasets base share task offensive language identification dravidian languages eacl two thousand and twenty-one present exhaustive exploration different transformer model also provide genetic algorithm technique ensembling different model ensembled model train separately language secure first position tamil second position kannada first position malayalam sub task model cod provide
paper propose conditional adversarial network can framework explore relationship share feature label predictions impose discriminability share feature multi domain text classification mdtc propose introduce conditional domain discriminator model domain variance share feature representations class aware information simultaneously adopt entropy condition guarantee transferability share feature provide theoretical analysis framework show objective equivalent minimize total divergence among multiple joint distributions share feature label predictions therefore theoretically sound adversarial network discriminate multiple distributions evaluation result two mdtc benchmarks show outperform prior methods experiment demonstrate good ability generalize learn knowledge unseen domains
introduce new formalisation languages call keyboards consider set elementary operations write erase letter go right leave define keyboard set finite sequence operations call key correspond language set word obtain apply sequence key unlike classical model computation every key apply anytime define various class languages base different set elementary operations compare expressive power also compare well know class languages chomsky hierarchy obtain strict hierarchy languages whose expressivity orthogonal one aforementionned classical model nous introduisons une nouvelle repr esentation de langages les claviers se munit un ensemble op erations el ementaires ajout effacement une lettre eplacement droite gauche et efinit un clavier comme un ensemble de suit finies op erations el ementaires appel ees touch son langage sera l ensemble des mots obtenus en appliquant une suite quelconque de touch contrairement des model de calcul classiques tout les touch peuvent etre appliqu ees tout moment en premier lieu nous efinissons diff erentes class de claviers en faisant varier l ensemble des op erations el ementaires autoris ees et nous comparons l expressivit e des class de langages obtenues nous comparons egalement ces class la hi erarchie de chomsky nous obtenons que tout les class etudi ees sont diff erentes et nous caract erisons les class inclues dans les rationnels et les alg ebriques l expressivit e des claviers semble orthogonale celle des model evoqu es pr ec edemment
entities essential elements relation extraction task exhibit certain structure work formulate structure distinctive dependencies mention pair propose ssan incorporate structural dependencies within standard self attention mechanism throughout overall encode stage specifically design two alternative transformation modules inside self attention build block produce attentive bias adaptively regularize attention flow experiment demonstrate usefulness propose entity structure effectiveness ssan significantly outperform competitive baselines achieve new state art result three popular document level relation extraction datasets provide ablation visualization show entity structure guide model better relation extraction code publicly available
understand human language quantify identify intents entities even though classification methods rely label information often use comprehension language understand incredibly time consume tedious process generate high propensity supervise datasets paper present generation accurate intents correspond roman urdu unstructured data integrate corpus rasa nlu module intent classification embed knowledge graph rasa framework maintain dialog history semantic base natural language mechanism chatbot communication compare result work exist linguistic systems combine semantic technologies minimum accuracy intents generation sixty-four percent confidence response generation part minimum accuracy eight hundred and twenty-one percent maximum accuracy gain nine hundred and sixty-seven percent score refer log precision recall f1 measure intents summarize furthermore create confusion matrix represent intents ambiguously recognize approach
recent advancements natural language process citegpt2 citebert lead near human performance multiple natural language task paper seek understand whether similar techniques apply highly structure environment strict syntax rule specifically propose end end machine learn model code generation python language build top pre train language model demonstrate fine tune model perform well code generation task achieve bleu score twenty-two improvement forty-six reasonable sequence sequence baseline result relate code use train data process available github
increase usage clickbaits indonesian online news newsworthy article sometimes get bury among clickbaity news reliable lightweight tool need detect clickbaits go leverage state art natural language process model bert restful api base application develop study offload compute resources need train model cloud server client side application need send request api cloud server handle rest study propose design develop web base application detect clickbait indonesian use indobert language model application usage discuss available public use performance mean roc auc eighty-nine
pretraining bidirectional encoder representations transformers bert downstream nlp task non trival task pretrained five bert model differ size train set mixture formal informal arabic linguistic preprocessing intend support arabic dialects social media experiment highlight centrality data diversity efficacy linguistically aware segmentation also highlight data train step necessitate better model new model achieve new state art result several downstream task result model release community name qarib
propose unit unify transformer model simultaneously learn prominent task across different domains range object detection natural language understand multimodal reason base transformer encoder decoder architecture unit model encode input modality encoder make predictions task share decoder encode input representations follow task specific output head entire model jointly train end end losses task compare previous efforts multi task learn transformers share model parameters across task instead separately fine tune task specific model handle much higher variety task across different domains experiment learn seven task jointly eight datasets achieve strong performance task eight hundred and seventy-five fewer parameters code release mmf https mmfsh
neural dialogue model suffer low quality responses interact practice demonstrate difficulty generalization beyond train data recently knowledge distillation use successfully regularize student transfer knowledge teacher however teacher student train dataset tend learn similar feature representations whereas general knowledge find differences find general knowledge hinder unidirectional distillation student obey teacher may discard knowledge truly general refute teacher end propose novel train framework learn general knowledge line idea reach consensus ie find common knowledge beneficial different yet datasets diversify learn partner concretely train task divide group subtasks number students student assign one subtask optimize allocate subtask also imitate multi view feature representation aggregate students ie student peer induce students capture common knowledge among different subtasks alleviate fit students allocate subtasks enhance generalization extend unidirectional distillation bidirectional distillation encourage student student peer co evolve exchange complementary knowledge empirical result analysis demonstrate train framework effectively improve model generalization without sacrifice train efficiency
slot fill intent detection become significant theme field natural language understand even though slot fill intensively associate intent detection characteristics information require task different approach may fully aware problem addition balance accuracy two task effectively inevitable problem joint learn model paper continual learn interrelate model clim propose consider semantic information different characteristics balance accuracy intent detection slot fill effectively experimental result show clim achieve state art performace slot fill intent detection atis snip
study problem incorporate prior knowledge deep transformer base modeliebidirectional encoder representations transformers bert enhance performance semantic textual match task probe analyze bert already know solve task obtain better understand task specific knowledge bert need need analysis motivate us take different approach exist work instead use prior knowledge create new train task fine tune bert directly inject knowledge bert multi head attention mechanism lead us simple yet effective approach enjoy fast train stage save model train additional data task main task extensive experiment demonstrate propose knowledge enhance bert able consistently improve semantic textual match performance original bert model performance benefit salient train data scarce
era big data continuously time unknowingly leave behind digital trace browse share post like search watch listen online content aggregate digital trace provide powerful insights behavior preferences activities traits people many raise privacy concern around use aggregate digital trace undisputedly bring us many advance search engines learn users enable access unforeseen amount data knowledge information eg discovery previously unknown adverse drug reactions search engine log whether online service journalism digital forensics law research increasingly set explore large amount digital trace discover new information consider instance enron scandal hillary clinton email controversy panama paper case revolve around analyze search investigate explore turn upside large amount digital trace gain new insights knowledge information discovery task core find evidence activity real world dissertation revolve around discovery digital trace sit intersection information retrieval natural language process apply machine learn propose computational methods aim support exploration sense make process large collections digital trace focus textual trace eg email social media stream address two aspects central discovery digital trace
last decade large number knowledge graph kg information extraction approach propose albeit effective efforts disjoint collective strengths weaknesses effective kg information extraction ie study literature propose plumber first framework bring together research community disjoint ie efforts plumber architecture comprise thirty-three reusable components various kg information extraction subtasks coreference resolution entity link relation extraction use componentsplumber dynamically generate suitable information extraction pipelines offer overall two hundred and sixty-four distinct pipelineswe study optimization problem choose suitable pipelines base input sentence train transformer base classification model extract contextual embeddings input find appropriate pipeline study efficacy plumber extract kg triple use standard datasets two kgs dbpedia open research knowledge graph orkg result demonstrate effectiveness plumber dynamically generate kg information extraction pipelinesoutperforming baselines agnostics underlie kg furthermorewe provide analysis collective failure case study similarities synergies among integrate components discuss limitations
paper spiritual child two thousand and five lecture note kindergarten quantum mechanics show simple pictorial extension dirac notation allow several quantum feature easily express derive use language even kindergartner understand central approach use picture pictorial transformation rule understand derive feature quantum theory computation however approach leave many wonder beef word new approach capable produce new result simply aesthetically please way restate stuff already know aim sequel paper say beef highlight major result approach advocate kindergarten quantum mechanics apply tackle practical problems real quantum computers focus mainly become swiss army knife pictorial formalism zx calculus first look ideas behind zx calculus compare contrast usual quantum circuit formalism survey result past two years fall three categories one completeness rule zx calculus two state art quantum circuit optimisation result commercial open source quantum compilers rely zx three use zx translate real world stuff like natural language quantum circuit run today limit quantum hardware also take title literally outline ongoing experiment aim show zx calculus enable children cut edge quantum compute stuff anything would truly confirm kindergarten quantum mechanics joke
natural language process nlp today active field research innovation many applications need however big set data supervise learn suitably label train purpose include applications arabic language national dialects however open access label data set arabic dialects lack data science ecosystem lack burden innovation research field work present open data set social data content several arabic dialects data collect twitter social network consist 50k twit five five national dialects furthermore data label several applications namely dialect detection topic detection sentiment analysis publish data open access data encourage innovation encourage work field nlp arabic dialects social media selection model build use data set present paper along performances
hypothesize explicit integration contextual information multi task learn framework would emphasize significance context boost performance jointly learn name entity recognition ner relation extraction work prove hypothesis segment entities surround context build contextual representations use independent segment relation representation allow joint ner system achieve near state art sota performance ner task beat sota system end end ner four thousand, nine hundred and seven f1
natural language process nlp model current trend consist use increasingly extra data build best model possible imply expensive computational cost train time difficulties deployment worry model carbon footprint reveal critical problem future trend goal develop nlp model require extra data minimize train time paper explore markov chain model hide markov chain hmc pairwise markov chain pmc nlp segmentation task apply model three classic applications pos tag name entity recognition chunk develop original method adapt model text segmentation specific challenge obtain relevant performances short train execution time pmc achieve equivalent result obtain conditional random field crf one apply model task extra data use moreover pmc train time thirty time shorter crf ones validate model give objectives
nowadays neural network model achieve state art result many areas computer vision speech process sequential data especially natural language process nlp task recurrent neural network rnns extensions long short term memory lstm network gate recurrent unit gru among use model term term sequence process however many work create extensions improvements rnn focus develop ways sequential data process neural network term term way paper propose original hide neural markov chain hnmc framework new family sequential neural model base rnn hide markov model hmm probabilistic graphical model neural extension possible thank recent entropic forward backward algorithm hmm restoration propose three different model classic hnmc hnmc2 hnmc cn describe model whole construction compare classic rnn bidirectional rnn birnn model sequence label task chunk part speech tag name entity recognition every experiment whatever architecture embed method use one propose model best result show new neural sequential framework potential open way new model might eventually compete prevalent bilstm bigru
analysis online review attract great attention broad applications often time textual review couple numerical rat data work propose probabilistic model accommodate textual review overall rat consideration intrinsic connection joint sentiment topic prediction key propose method develop unify generative model topic model construct base review texts sentiment prediction obtain combine review texts overall rat inference model parameters obtain efficient gibbs sample procedure propose method enhance prediction accuracy review data achieve effective detection interpretable topics sentiments merit propose method elaborate case study amazon datasets simulation study
transformers arguably main workhorse recent natural language process research definition transformer invariant respect reorder input however language inherently sequential word order essential semantics syntax utterance paper provide overview common methods incorporate position information transformer model objectives survey showcase position information transformer vibrant extensive research area ii enable reader compare exist methods provide unify notation meaningful cluster iii indicate characteristics application take account select position encode iv provide stimuli future research
language vary across users interest field social media data word author user across interest may different mean eg cool sentiments eg fast however exist methods train user embeddings ignore variations across user interest product movie categories eg drama vs action study treat user interest domains empirically examine user language vary across user factor three english social media datasets propose user embed model account language variability user interest via multitask learn framework model learn user language variations without human supervision exist work mainly evaluate user embed extrinsic task propose intrinsic evaluation via cluster evaluate user embeddings extrinsic task text classification experiment three english language social media datasets show propose approach generally outperform baselines via adapt user factor
enormous amount discourse take place online pose challenge function civil inform public sphere efforts standardize online discourse data claimreview make available wealth new data potentially inaccurate claim review third party fact checker data could help would light nature online discourse role political elites amplify implications integrity online information ecosystem unfortunately semi structure nature much data present significant challenge come model reason online discourse key challenge relation extraction task determine semantic relationships name entities claim develop novel supervise learn method relation extraction combine graph embed techniques path traversal semantic dependency graph approach base intuitive observation knowledge entities along path subject object triple eg washingtondc unitedstatesofamerica provide useful information leverage extract semantic relation ie capitalof example potential application technique model online discourse show method integrate pipeline reason potential misinformation claim
automatic essay grade aeg process machine assign grade essay write response topic call prompt zero shoot aeg train system grade essay write new prompt present train data paper describe solution problem zero shoot automatic essay grade use cognitive information form gaze behaviour experiment show use gaze behaviour help improve performance aeg systems especially provide new essay write response new prompt score average almost five percentage point qwk
advancement technology accessibility internet individual revolutionize real time information liberty express thoughts without pass credibility check lead dissemination fake content ecosystem disastrous effect individuals society whole amplification fake news become rampant india debunk information often get republish replacement description claim depict different incidence curb fabricate stories necessary investigate deduplicates false claim make public majority study automatic fact check fake news detection restrict english country like india ten literate population speak english role regional languages spread falsity undermine paper introduce factdril first large scale multilingual fact check dataset regional indian languages collect exhaustive dataset across seven months cover eleven low resource languages propose dataset consist nine thousand and fifty-eight sample belong english five thousand, one hundred and fifty-five sample hindi remain eight thousand, two hundred and twenty-two sample distribute across various regional languages ie bangla marathi malayalam telugu tamil oriya assamese punjabi urdu sinhala burmese also present detail characterization three multi lingual multi media multi domain factdril accompany complete list vary attribute make unique dataset study lastly present potential use case dataset expect dataset valuable resource serve start point fight proliferation fake news low resource languages
mixup computer vision data augmentation technique use convex interpolations input data label enhance model generalization train however application mixup natural language understand nlu domain limit due difficulty interpolate text directly input space study propose mixup methods input manifold sentence embed level transformer architecture apply finetune bert model diverse set nlu task find mixup improve model performance well reduce test loss model calibration error fifty
sentiment analysis know one crucial task field natural language process convolutional neural network cnn one prominent model commonly use aim although convolutional neural network obtain remarkable result recent years still confront limitations firstly consider word sentence equal contributions sentence mean representation able extract informative word secondly require large number train data obtain considerable result many parameters must accurately adjust end convolutional neural network integrate hierarchical attention layer propose able extract informative word assign higher weight moreover effect transfer learn transfer knowledge learn source domain target domain aim improve performance also explore base empirical result propose model higher classification accuracy extract informative word also apply incremental transfer learn significantly enhance classification performance
many type distributional word embeddings weakly encode linguistic regularities directions difference jump jump similar direction walk walk several attempt make explain fact respond allen hospedales recent icml two thousand and nineteen theoretical explanation claim word2vec glove encode linguistic regularities whenever specific relation paraphrase hold four word involve regularity demonstrate explanation go paraphrase relations need explanation hold empirically
authorship analysis important subject field natural language process allow detection likely writer article news book message technique multiple use task relate authorship attribution detection plagiarism style analysis source misinformation etc focus paper explore limitations sensitiveness establish approach adversarial manipulations input end use establish techniques first develop experimental frame work author detection input perturbations next experimentally evaluate performance authorship detection model collection semantic preserve adversarial perturbations input narratives finally compare analyze effect different perturbation strategies input model configurations effect author detection model
research community propose copious modifications transformer architecture since introduce three years ago relatively see widespread adoption paper comprehensively evaluate many modifications share experimental set cover common use transformer natural language process surprisingly find modifications meaningfully improve performance furthermore transformer variants find beneficial either develop codebase use relatively minor change conjecture performance improvements may strongly depend implementation detail correspondingly make recommendations improve generality experimental result
present overview emotiongif2020 challenge hold 8th international workshop natural language process social media socialnlp conjunction acl two thousand and twenty challenge require predict affective reactions online texts include emotiongif dataset tweet label reaction categories novel dataset include 40k tweet reaction gifs due special circumstances year two thousand and twenty two round competition conduct total eighty-four team register task twenty-five team success fully submit entries evaluation phase first round thirteen team participate successfully second round top participants five team present technical report share code top score win team use recallk metric six thousand, two hundred and forty-seven
natural language process fast grow field artificial intelligence since transformer introduce google two thousand and seventeen large number language model bert gpt elmo inspire architecture model train huge datasets achieve state art result natural language understand however fine tune pre train language model much smaller datasets downstream task require carefully design pipeline mitigate problems datasets lack train data imbalanced data paper propose pipeline adapt general purpose roberta language model specific text classification task vietnamese hate speech detection first tune phobert dataset train model mask language model task employ encoder text classification order preserve pre train weight learn new feature representations utilize different train techniques layer freeze block wise learn rate label smooth experiment prove propose pipeline boost performance significantly achieve new state art vietnamese hate speech detection campaign seven thousand, two hundred and twenty-one f1 score
toxicity detection text popular nlp task recent years semeval two thousand and twenty-one task five toxic span detection focus detect toxic span within passages state art span detection approach employ various techniques broadly classify token classification span prediction approach paper explore simple versions approach performance task specifically use bert base model bert roberta spanbert approach also combine approach modify bring improvements toxic span prediction end investigate result four hybrid approach multi span spantoken lstm crf combination predict offset use union intersection additionally perform thorough ablative analysis analyze observe result best submission combination spanbert span predictor roberta token classifier predictions achieve f1 score six thousand, seven hundred and fifty-three test set best post eval f1 score six thousand, eight hundred and ninety-five intersection predict offset top three roberta token classification checkpoints approach improve performance three average share baseline model rnnsl spacy ner
article present methodologies semeval two thousand and twenty-one task four read comprehension abstract mean give fill blank type question correspond context task predict suitable word list five options three sub task within task imperceptibility subtask non specificity subtask ii intersection subtask iii use encoders transformers base model pre train mask language model mlm task build fill blank fitb model moreover model imperceptibility define certain linguistic feature model non specificity leverage information hypernyms hyponyms provide lexical database specifically non specificity try augmentation techniques statistical techniques also propose variants namely chunk vote max context take care input length restrictions bert etc additionally perform thorough ablation study use integrate gradients explain predictions sample best submissions achieve accuracies seven thousand, five hundred and thirty-one seven thousand, seven hundred and eighty-four test set subtask subtask ii respectively subtask iii achieve accuracies six thousand, five hundred and sixty-four six thousand, two hundred and twenty-seven
lack commonly use benchmark data set collection super glue wang et al two thousand and eighteen two thousand and nineteen evaluation non english pre train language model severe shortcoming current english centric nlp research concentrate large part research english neglect uncertainty transfer conclusions find english language languages evaluate performance german multilingual bert base model currently available via huggingface transformers library four task germeval17 workshop compare pre bert architectures wojatzki et al two thousand and seventeen schmitt et al two thousand and eighteen attia et al two thousand and eighteen well elmo base architecture biesialska et al two thousand and twenty bert base approach guhr et al two thousand and twenty observe improvements put relation similar task similar model pre bert vs bert base english language order draw tentative conclusions whether observe improvements transferable german potentially relate languages
privacy policies legal document describe practice organization company adopt handle personal data users policies legal document often write extensive legal jargon difficult understand though work do privacy policies none cater problem verify give privacy policy adhere data protection laws give country state aim bridge gap provide framework analyze privacy policies light various data protection laws general data protection regulation gdpr achieve firstly label privacy policies laws correlation scheme develop map content privacy policy appropriate segment law policy must conform check compliance privacy policy text correspond text law use nlp techniques use tool users would better equip understand personal data manage provide map gdpr pdpa laws easily incorporate already build pipeline
large language model become increasingly difficult train require computation time cost work present sru recurrent unit optional build attention exhibit state art model capacity train efficiency standard language model benchmarks enwik8 wiki one hundred and three billion word datasets model obtain better perplexity bits per character bpc use 3x 10x less train time cost compare top perform transformer model result reaffirm attention need complementary sequential model modules moreover fast recurrence little attention lead model architecture
chinese short text match fundamental task natural language process exist approach usually take chinese character word input tokens two limitations one chinese word polysemous semantic information fully utilize two model suffer potential issue cause word segmentation introduce hownet external knowledge base propose linguistic knowledge enhance graph transformer let deal word ambiguity additionally adopt word lattice graph input maintain multi granularity information model also complementary pre train language model experimental result two chinese datasets show model outperform various typical text match approach ablation study also indicate semantic information multi granularity information important text match model
improve efficiency transformer base language pre train important task nlp especially self attention module computationally expensive paper propose simple effective solution call emphlazyformer compute self attention distribution infrequently lazyformer compose multiple lazy block contain multiple transformer layer lazy block self attention distribution compute first layer reuse upper layer way cost computation could largely save also provide several train trick lazyformer extensive experiment demonstrate effectiveness propose method
paper present systems three subtasks semeval task4 read comprehension abstract mean recam explain algorithms use learn model process tune algorithms select best model inspire similarity recam task language pre train propose simple yet effective technology namely negative augmentation language model evaluation result demonstrate effectiveness propose approach model achieve 4th rank official test set subtask one subtask two accuracy eight hundred and seventy-nine accuracy nine hundred and twenty-eight respectively conduct comprehensive model analysis observe interest error case may promote future research
compute word sub word embeddings use fasttext sub word embeddings select byte pair encode bpe algorithm represent sub word evaluate biomedical word embeddings obtain better result previous versions show implication data obtain better representations
automate essay score aes cross disciplinary effort involve education linguistics natural language process nlp efficacy nlp model aes test ability evaluate long term dependencies extrapolate mean even text poorly write large pretrained transformer base language model dominate current state art many nlp task however computational requirements model make expensive deploy practice goal paper challenge paradigm nlp bigger better come aes evaluate performance several fine tune pretrained nlp model modest number parameters aes dataset ensembling model achieve excellent result fewer parameters pretrained transformer base model
challenge recognize name entities give text dynamic field recent years due advance neural network architectures increase compute power availability diverse label datasets deliver pre train highly accurate model task generally focus tag common entities domain specific use case require tag custom entities part pre train model solve either fine tune pre train model train custom model main challenge lie obtain reliable label train test datasets manual label would highly tedious task paper present pharmke text analysis platform focus pharmaceutical domain apply deep learn several stag thorough semantic analysis pharmaceutical article perform text classification use state art transfer learn model thoroughly integrate result obtain propose methodology methodology use create accurately label train test datasets use train model custom entity label task center pharmaceutical domain obtain result compare fine tune bert biobert model train dataset additionally pharmke platform integrate result obtain name entity recognition task resolve co reference entities analyze semantic relations every sentence thus set baseline additional text analysis task question answer fact extraction recognize entities also use expand knowledge graph generate dbpedia spotlight give pharmaceutical text
propose notation tensors name ax relieve author reader future implementers burden keep track order ax purpose also make easy extend operations low order tensors higher order ones eg extend operation image minibatches image extend attention mechanism multiple attention head brief overview notation illustrate several examples modern machine learn build block like attention convolution full model like transformers lenet finally give formal definitions describe extensions proposals build ideas many previous paper software libraries hope document encourage author use name tensors result clearer paper less bug prone implementations source code document find https githubcom namedtensor notation invite anyone make comment proposal submit issue pull request repository
transformer language model make tremendous stride natural language understand task however complexity natural language make challenge ascertain accurately model track world state underlie text motivate issue consider task language model game chess unlike natural language chess notations describe simple constrain deterministic domain moreover observe appropriate choice chess notation allow directly probe world state without require additional probe relate machinery find enough train data transformer language model learn track piece predict legal move high accuracy train solely move sequence b small train set provide access board state information train yield significant improvements c success transformer language model dependent access entire game history ie full attention approximate full attention result significant performance drop propose testbed benchmark future work development analysis transformer language model
hci nlp traditionally focus different evaluation methods hci involve small number people directly deeply nlp traditionally rely standardize benchmark evaluations involve larger number people indirectly present five methodological proposals intersection hci nlp situate context ml base nlp model goal foster interdisciplinary collaboration progress field emphasize field learn
natural language video localization nlvl aim locate target moment untrimmed video semantically correspond text query exist approach mainly solve nlvl problem perspective computer vision formulate rank anchor regression task methods suffer large performance degradation localize long videos work address nlvl new perspective ie span base question answer qa treat input video text passage propose video span localize network vslnet top standard span base qa framework name vslbase address nlvl vslnet tackle differences nlvl span base qa simple yet effective query guide highlight qgh strategy qgh guide vslnet search match video span within highlight region address performance degradation long videos extend vslnet vslnet l apply multi scale split concatenation strategy vslnet l first split untrimmed video short clip segment predict clip segment contain target moment suppress importance segment finally clip segment concatenate different confidences locate target moment accurately extensive experiment three benchmark datasets show propose vslnet vslnet l outperform state art methods vslnet l address issue performance degradation long videos study suggest span base qa framework effective strategy solve nlvl problem
international classification diseases icd de facto cod use globally clinical cod cod enable healthcare providers claim reimbursement facilitate efficient storage retrieval diagnostic information problem automatically assign icd cod approach literature multilabel classification use neural model unstructured data propose approach enhance performance neural model effectively train word vectors use routine medical data well external knowledge scientific article furthermore exploit geometric properties two set word vectors combine common dimensional space use meta embed techniques demonstrate efficacy approach multimodal set use unstructured structure information empirically show approach improve current state art deep learn architectures benefit ensemble model
abusive language online platforms major societal problem often lead important societal problems marginalisation underrepresented minorities many different form abusive language hate speech profanity cyber bully online platforms seek moderate order limit societal harm comply legislation create inclusive environment users within field natural language process researchers develop different methods automatically detect abusive language often focus specific subproblems narrow communities consider abusive language much differ context argue currently dichotomy type abusive language online platforms seek curb research efforts automatically detect abusive language thus survey exist methods well content moderation policies online platforms light suggest directions future work
several language applications often require word semantics core part process pipeline either precise mean inference semantic similarity multi sense embeddings se exploit important requirement se seek represent word distinct sense order resolve conflation mean word use different contexts previous work usually approach task train model large corpus often ignore effect usefulness semantic relations offer lexical resources however even large train data coverage possible word sense still issue addition considerable percentage contextual semantic knowledge never learn huge amount possible distributional semantic structure never explore paper leverage rich semantic structure wordnet use graph theoretic walk technique word sense enhance quality multi sense embeddings algorithm compose enrich texts original texts furthermore derive new distributional semantic similarity measure se prior ones adapt measure word sense disambiguation wsd aspect experiment report evaluation result eleven benchmark datasets involve wsd word similarity task show method enhance distributional semantic structure improve embeddings quality baselines despite small train data achieve state art performance datasets
detect attitudes express texts also know stance detection become important task detection false information online misinformation unintentionally false disinformation intentionally false spread deliberately malicious intent stance detection frame different ways include component fact check rumour detection detect previously fact check claim b task right prior efforts contrast stance detection relate social media task argumentation mine sentiment analysis survey examine relationship stance detection detection mis disinformation detection holistic viewpoint focus survey review analyse exist work area discuss lessons learn future challenge
past years meme become new way communication internet memes image embed text quickly spread hate offence violence classify memes challenge multimodal nature region specific interpretation share task organize develop model identify troll multimodal social media memes work present computational model develop part participation task train data come two form image embed tamil code mix text associate caption give english investigate visual textual feature use cnn vgg16 inception multilingual bert xlm roberta xlnet model multimodal feature extract combine image cnn resnet50 inception text long short term memory network feature via early fusion approach result indicate textual approach xlnet achieve highest weight f1 score fifty-eight enable model secure 3rd rank task
word embeddings show adept capture semantic syntactic regularities natural language text result representations find utility wide variety downstream content analysis task commonly word embed techniques derive distribute representation word base local context information however approach ignore rich amount explicit information present knowledge base problematic might lead poor representation word insufficient local context domain specific word furthermore problem become pronounce domain bio medicine presence domain specific word relatively high towards end project propose new word embed base model biomedical domain jointly leverage information available corpora domain knowledge order generate knowledge base power embeddings unlike exist approach propose methodology simple adept capture precise knowledge available domain resources accurate way experimental result biomedical concept similarity relatedness task validate effectiveness propose approach
paradigm representation learn transfer learn potential greatly enhance clinical natural language process work propose multi task pre train fine tune approach learn generalize transferable patient representations medical language model first pre train different relate high prevalence phenotypes fine tune downstream target task main contribution focus impact technique low prevalence phenotypes challenge task due dearth data validate representation pre train fine tune multi task pre train model low prevalence phenotypes include thirty-eight circulatory diseases twenty-three respiratory diseases seventeen genitourinary diseases find multi task pre train increase learn efficiency achieve consistently high performance across majority phenotypes important multi task pre train almost always either best perform model perform tolerably close best perform model property refer robust result lead us conclude multi task transfer learn architecture robust approach develop generalize transferable patient language representations numerous phenotypes
clinical case report write descriptions unique aspects particular clinical case play essential role share clinical experience atypical disease phenotypes new therapies however knowledge attempt develop end end system annotate index otherwise curate report paper propose novel computational resource platform create extract index query content clinical case report create foster environment sustainable resource support discovery enable researchers overcome challenge information science online video demonstration view https youtube q8owbqytjdc
development business adoption knowledge graph increase demand extract entities relations knowledge graph unstructured domain document make automatic knowledge extraction domain text quite meaningful paper propose knowledge extraction method base bert use extract knowledge point unstructured specific domain texts insurance clauses insurance industry automatically save manpower knowledge graph construction different commonly use methods base rule templates entity extraction model paper convert domain knowledge point question answer pair use text around answer document context method adopt bert base model similar bert squad read comprehension task model fine tune use directly extract knowledge point insurance clauses accord test result model performance good
entity alignment ea task discover entities refer real world object different knowledge graph kgs crucial step integrate multi source kgs majority exist embeddings base entity alignment methods embed entities relations vector space base relation triple kgs local alignment methods insufficiently consider multiple relations entities structure information kgs fully leverage paper propose novel framework base relation aware graph attention network capture interactions entities relations framework adopt self attention mechanism spread entity information relations aggregate relation information back entities furthermore propose global alignment algorithm make one one entity alignments fine grain similarity matrix experiment three real world cross lingual datasets show framework outperform state art methods
paper present deep bag sub emotions deepbose novel deep learn model depression detection social media model formulate internally compute differentiable bag feature bof representation incorporate emotional information achieve reinterpretation classical weight scheme like term frequency inverse document frequency probabilistic deep learn operations important advantage propose method train transfer learn paradigm useful enhance conventional bof model directly integrate deep learn architectures experiment perform erisk17 erisk18 datasets depression detection task result show deepbose outperform conventional bof representations competitive state art achieve f1 score positive class sixty-four erisk17 sixty-five erisk18
dialogue generation model face challenge produce generic repetitive responses unlike previous augmentation methods mostly focus token manipulation ignore essential variety within single sample use hard label propose promote generation diversity neural dialogue model via soft embed augmentation along soft label paper particularly select key input tokens fuse embeddings together embeddings semantic neighbor tokens new embeddings serve input model replace original one besides soft label use loss calculation result multi target supervision give input experimental result two datasets illustrate propose method capable generate diverse responses raw model remain similar n gram accuracy ensure quality generate responses
grow role social media shape public opinions beliefs across world increase attention identify counter problem hate speech social media hate speech online space serious manifestations include social polarization hate crimes prior work propose automate techniques detect hate speech online techniques primarily fail look beyond textual content moreover attempt make focus aspects interpretability model give social legal implications incorrect predictions work propose deep neural multi modal model detect hate speech effectively capture semantics text along socio cultural context particular hate expression make b provide interpretable insights decisions model perform thorough evaluation different model techniques demonstrate model able outperform exist state art hate speech classification approach finally show importance social cultural context feature towards unearth cluster associate different categories hate
empirical natural language process nlp systems application domains eg healthcare finance education involve interoperation among multiple components range data ingestion human annotation text retrieval analysis generation visualization establish unify open source framework support fast development sophisticate nlp workflows composable manner framework introduce uniform data representation encode heterogeneous result wide range nlp task offer large repository processors nlp task visualization annotation easily assemble full interoperability unify representation highly extensible framework allow plug custom processors external shelf nlp deep learn libraries whole framework deliver two modularized yet integratable open source project namely forte1 workflow infrastructure nlp function processors stave2 user interaction visualization annotation
progress query focus multi document summarization qmds limit lack sufficient largescale high quality train datasets present two qmds train datasets construct use two data augmentation methods one transfer commonly use single document cnn daily mail summarization dataset create qmdscnn dataset two mine search query log create qmdsir dataset two datasets complementary properties ie qmdscnn real summaries query simulate qmdsir real query simulate summaries cover real summary query aspects build abstractive end end neural network model combine datasets yield new state art transfer result duc datasets also introduce new hierarchical encoders enable efficient encode query together multiple document empirical result demonstrate data augmentation encode methods outperform baseline model automatic metrics well human evaluations along multiple attribute
paper present cognet knowledge base kb dedicate integrate three type knowledge one linguistic knowledge framenet schematically describe situations object events two world knowledge yago freebase dbpedia wikidata provide explicit knowledge specific instance three commonsense knowledge conceptnet describe implicit general facts model different type knowledge consistently introduce three level unify frame style representation architecture integrate free form commonsense knowledge structure knowledge propose strategy combine automate label crowdsourced annotation present cognet integrate one thousand semantic frame linguistic kbs twenty million frame instance world kbs well ninety thousand commonsense assertions commonsense kbs data easily query explore online platform free download rdf format utilization cc sa forty license demo data available http cognettop
combine representations word make sentence cohesive whole difficult since need account order word establish word present relate solution propose consist iteratively adjust context algorithm start presumably erroneous value context adjust value respect tokens hand order achieve representations word build combine symbolic embed positional encode single vectors algorithm iteratively weigh aggregate vectors use novel second order attention mechanism model report strong result several well know text classification task
code switch common phenomenon among people diverse lingual background widely use internet communication purpose paper present recurrent neural network combine attention model language identification code switch data english low resource roman urdu attention model enable architecture learn important feature languages hence classify code switch data demonstrate approach compare result state art model ie hide markov model conditional random field bidirectional lstm model evaluation use confusion matrix metrics show attention mechanism provide improve precision accuracy compare model
meta learn sufficiently validate beneficial low resource neural machine translation nmt however find meta train nmt fail improve translation performance domain unseen meta train stage paper aim alleviate issue propose novel meta curriculum learn domain adaptation nmt meta train nmt first learn similar curricula domain avoid fall bad local optimum early finally learn curricula individualities improve model robustness learn domain specific knowledge experimental result ten different low resource domains show meta curriculum learn improve translation performance familiar unfamiliar domains cod data freely available https githubcom nlp2ct meta curriculum
manage data information retrieval ir experiment challenge dataset documentation scatter across internet one obtain copy data numerous different data format work even basic format subtle dataset specific nuances need consider proper use help mitigate challenge introduce new robust lightweight tool irdatasets acquire manage perform typical operations datasets use ir primarily focus textual datasets use ad hoc search tool provide python command line interface numerous ir datasets benchmarks knowledge extensive tool kind integrations popular ir index experimentation toolkits demonstrate tool utility also provide documentation datasets irdatasets catalog https ir datasetscom catalog act hub information datasets use ir provide core information data benchmark provide well link detail information welcome community contributions intend continue maintain grow tool
paper show conditional inference tree ensembles suitable methods model linguistic variation earlier linguistic applications however claim suitability strongly increase combine prediction interpretation end develop statistical method prindt prediction interpretation decision tree introduce discuss present paper
recent study question answer qa conversational qa convqa emphasize role retrieval system first retrieve evidence large collection extract answer open retrieval convqa set typically assume question answerable single span text within particular passage span answer supervision signal thus derive whether system recover exact match grind truth answer span retrieve passages method refer span match weak supervision however information seek conversations challenge span match method since long answer especially freeform answer necessarily strict span passage therefore introduce learn weak supervision approach identify paraphrase span know answer passage experiment quac coqa datasets show span match weak supervisor handle conversations span answer less satisfactory result freeform answer generate people method flexible handle span answer freeform answer moreover method powerful combine span match method show complementary span match method also conduct depth analyse show insights open retrieval convqa weak supervision set
recent work sentiment analysis exploit text modality however millions hours video record post social media platforms everyday hold vital unstructured information exploit effectively gauge public perception multimodal sentiment analysis offer innovative solution computationally understand harvest sentiments videos contextually exploit audio visual textual cue paper firstly present first kind persian multimodal dataset comprise eight hundred utterances benchmark resource researchers evaluate multimodal sentiment analysis approach persian language secondly present novel context aware multimodal sentiment analysis framework simultaneously exploit acoustic visual textual cue accurately determine express sentiment employ decision level late feature level early fusion methods integrate affective cross modal information experimental result demonstrate contextual integration multimodal feature textual acoustic visual feature deliver better performance nine thousand, one hundred and thirty-nine compare unimodal feature eight thousand, nine hundred and twenty-four
paper introduce natural language understand nlu framework argumentative dialogue systems information seek opinion build domain approach distinguish multiple user intents identify system arguments user refer natural language utterances model applicable argumentative dialogue system allow user inform build opinion towards controversial topic order evaluate propose approach collect user utterances interaction respective system label intent reference argument extensive online study data collection include multiple topics two different user type native speakers uk non native speakers china evaluation indicate clear advantage utilize techniques baseline approach well robustness propose approach new topics different language proficiency well cultural background user
bert recent transformer base model achieve state art performance various nlp task paper investigate hardware acceleration bert fpga edge compute tackle issue huge computational complexity memory footprint propose fully quantize bert fq bert include weight activations softmax layer normalization intermediate result experiment demonstrate fq bert achieve 794x compression weight negligible performance loss propose accelerator tailor fq bert evaluate xilinx zcu102 zcu111 fpga achieve performance per watt three hundred and eighteen fps w 2891x 1272x intelr coretm i7 eight thousand, seven hundred cpu nvidia k80 gpu respectively
response generation task proper sentimental expressions obviously improve human like level responses however real application online systems high qps query per second indicator flow capacity line systems require dynamic vocabulary mechanism prove available improve speed generative model paper propose emotion control dialog response generation model base dynamic vocabulary mechanism experimental result show benefit model
uk volatile political environment years brexit leadership crises mark past five years work want understand global health emergency covid nineteen influence amount type topics abuse uk politicians receive engage public work want understand global health emergency covid nineteen influence amount type topics abuse uk politicians receive engage public work cover period june december two thousand and twenty analyse twitter abuse reply uk mps work follow analysis online abuse first four months covid nineteen pandemic uk paper examine overall abuse level new seven month period analyse reactions members different political party uk government relationship online abuse topics brexit government covid nineteen response policies social issue addition also examine presence conspiracy theories post abusive reply mps period find abuse level toward uk mps time high december two thousand and twenty fifty-four reply tweet send mps almost one higher two months precede general election departure trend see first four months pandemic mps tory party receive highest percentage abusive reply july two thousand and twenty onward stay five start september two thousand and twenty onward covid nineteen crisis deepen brexit negotiations eu start near completion
due high annotation cost make best use exist human create train data important research direction therefore carry systematic evaluation transferability bert base neural rank model across five english datasets previous study focus primarily zero shoot shoot transfer large dataset dataset small number query contrast collections substantial number query enable full shoot evaluation mode improve reliability result furthermore since source datasets licence often prohibit commercial use compare transfer learn train pseudo label generate bm25 scorer find train pseudo label possibly subsequent fine tune use modest number annotate query produce competitive better model compare transfer learn however need improve stability effectiveness shoot train case degrade performance pretrained model
sequentially stack self attention optional encoder decoder attention fee forward layer transformer achieve big success natural language process nlp many variants propose currently almost model assume layer order fix keep across data sample observe different data sample actually favor different order layer base observation work break assumption fix layer order transformer introduce instance wise layer reorder model structure instance wise order transformer iot model variant function reorder layer enable sample select better one improve model performance constraint almost number parameters achieve introduce light predictor negligible parameter inference cost decide capable favorable layer order input sequence experiment three task neural machine translation abstractive summarization code generation nine datasets demonstrate consistent improvements method show method also apply architectures beyond transformer code release github
choice parameter share strategy multilingual machine translation model determine optimally parameter space use hence directly influence ultimate translation quality inspire linguistic tree show degree relatedness different languages new general approach parameter share multilingual machine translation suggest recently main idea use expert language hierarchies basis multilingual architecture closer two languages parameters share work test idea use transformer architecture show despite success previous work problems inherent train hierarchical model demonstrate case carefully choose train strategy hierarchical architecture outperform bilingual model multilingual model full parameter share
inductive link prediction entities train inference stag different show promise complete continuously evolve knowledge graph exist model inductive reason mainly focus predict miss link learn logical rule however many exist approach take account semantic correlations relations commonly see real world knowledge graph address challenge propose novel inductive reason approach namely tact effectively exploit topology aware correlations relations entity independent manner tact inspire observation semantic correlation two relations highly correlate topological structure knowledge graph specifically categorize relation pair several topological pattern propose relational correlation network rcn learn importance different pattern inductive link prediction experiment demonstrate tact effectively model semantic correlations relations significantly outperform exist state art methods benchmark datasets inductive link prediction task
abstract mean representation amr provide many information sentence semantic relations coreferences name entity relation one representation however research amr parse indonesian sentence fairly limit paper develop system aim parse indonesian sentence use machine learn approach base zhang et al work system consist three step pair prediction label prediction graph construction pair prediction use dependency parse component get edge word amr result pair prediction pass label prediction process use supervise learn algorithm predict label edge amr use simple sentence dataset gather article news article sentence model achieve smatch score eight hundred and twenty simple sentence test data
transformer architectures bring fundamental change computational linguistic field dominate recurrent neural network many years success also imply drastic change cross modal task language vision many researchers already tackle issue paper review critical milestones field well overall trend transformer architecture incorporate visuolinguistic cross modal task furthermore discuss current limitations speculate upon prospect find imminent
introduce task change narrative point view character assign narrative perspective different one originally use writer result shift narrative point view alter read experience use tool fiction write generate type text range educational self help self diagnosis introduce benchmark dataset contain wide range type narratives annotate change point view deictic first second person anaphoric third person describe pipeline process raw text rely neural architecture mention selection evaluations new benchmark dataset show propose architecture substantially outperform baselines generate mention less ambiguous natural
typeshift tool visualize linguistic pattern time type production language production complex process draw linguistic cognitive motor skills visualize holistic trend type process typeshift aim elucidate often noisy information signal use represent type pattern word level character level accomplish enable researcher compare contrast specific linguistic phenomena compare individual type session multiple group average finally although typeshift originally design type data easy adapt accommodate speech data well web demo available https angoodkindshinyappsio typeshift source code access https githubcom angoodkind typeshift
negation scope resolution extensively research problem use locate word affect negation cue sentence recent work show simply finetuning transformer base architectures yield state art result task work look negation scope resolution cloze style task sentence context cue word query also introduce novel cloze style attention mechanism call orthogonal attention inspire self attention first propose framework develop orthogonal attention variants propose four orthogonal attention variants oa c oa ca oa oa emb use orthogonal attention layer top xlnet backbone outperform finetuned xlnet state art negation scope resolution achieve best result date four datasets experiment bioscope abstract bioscope full paper sfu review corpus sem two thousand and twelve dataset sherlock
surprisal theory provide unify framework understand many phenomena sentence process hale two thousand and one levy 2008a posit word conditional probability give prior context fully determine process difficulty problematically claim one local statistic word frequency also show affect process even conditional probability give context hold constant ask whether local statistics role process whether word frequency special case present first clear evidence complex local statistics word bigram trigram probability also affect process independently surprisal find suggest significant independent role local statistics process motivate research new generalizations surprisal also explain local statistical information outsized effect
word2vec glove word embed model play key roles current state art result achieve natural language process design give significant unique vectorized representations word entities model prove efficiently extract similarities establish relationships reflect semantic contextual mean among word entities african languages represent thirty-one worldwide speak languages recently subject lot research however best knowledge currently none word embed model languages word entities none languages study paper describe glove word2vec poincar e embeddings functionalities build word2vec poincar e word embed model fon nobiin show promise result test applicability transfer learn model landmark african languages jointly involve mitigate scarcity resources attempt provide linguistic social interpretations result main contribution arouse interest create word embed model proper african languages ready use significantly improve performances natural language process downstream task official repository implementation https githubcom bonaventuredossou afrivec
text classification widely study problem broad applications many real world problems number texts train classification model limit render model prone overfitting address problem propose ssl reg data dependent regularization approach base self supervise learn ssl ssl unsupervised learn approach define auxiliary task input data without use human provide label learn data representations solve auxiliary task ssl reg supervise classification task unsupervised ssl task perform simultaneously ssl task unsupervised define purely input texts without use human provide label train model use ssl task prevent model overfitted limit number class label classification task experiment seventeen text classification datasets demonstrate effectiveness propose method
due rapid emergence short videos requirement content understand creation video caption task receive increase attention recent years paper convert traditional video caption task new paradigm ie open book video caption generate natural language prompt video content relevant sentence limit video address open book video caption problem propose novel retrieve copy generate network pluggable video text retriever construct retrieve sentence hint train corpus effectively copy mechanism generator introduce extract expressions multi retrieve sentence dynamically two modules train end end separately flexible extensible framework coordinate conventional retrieval base methods orthodox encoder decoder methods draw diverse expressions retrieve sentence also generate natural accurate content video extensive experiment several benchmark datasets show propose approach surpass state art performance indicate effectiveness promise propose paradigm task video caption
large pre train language model show encode large amount world commonsense knowledge parameters lead substantial interest methods extract knowledge past work knowledge extract take manually author query gather paraphrase use separate pipeline work propose method automatically rewrite query bertese paraphrase query directly optimize towards better knowledge extraction encourage meaningful rewrite add auxiliary loss function encourage query correspond actual language tokens empirically show approach outperform compete baselines obviate need complex pipelines moreover bertese provide insight type language help language model perform knowledge extraction
transformer powerful tool many natural language task base self attention mechanism encode dependence tokens specific token computation self attention bottleneck due quadratic time complexity various approach reduce time complexity approximation matrix one nystromformer author use nystrom base method approximation softmax nystrom method generate fast approximation large scale symmetric positive semidefinite spsd matrix use columns spsd matrix however since nystrom approximation low rank spectrum spsd matrix decay slowly nystrom approximation low accuracy alternative method propose approximation much stronger error bind nystrom method time complexity nystromformer ofleftnright
nlp text classification one primary problems try solve use language analyse indisputable lack label train data make harder task low resource languages like amharic task collect label annotate make valuable kind data encourage junior researchers school machine learn practitioners implement exist classification model language short paper aim introduce amharic text classification dataset consist 50k news article categorize six class dataset make available easy baseline performances encourage study better performance experiment
medical systems general patient treatment decisions outcomes particular affect bias base gender demographic elements language model increasingly apply medicine grow interest build algorithmic fairness process impact patient care much work address question focus bias encode language model statistical estimate relationships concepts derive distant read corpora build work investigate word choices make healthcare practitioners language model interact regard bias identify remove gendered language two clinical note datasets describe new debiasing procedure use bert base gender classifiers show minimal degradation health condition classification task low medium level bias removal via data augmentation finally compare bias semantically encode language model bias empirically observe health record work outline interpretable approach use data augmentation identify reduce potential bias natural language process pipelines
many specialize domains remain untouched deep learn large label datasets require expensive expert annotators address bottleneck within legal domain introduce contract understand atticus dataset cuad new dataset legal contract review cuad create dozens legal experts atticus project consist thirteen thousand annotations task highlight salient portion contract important human review find transformer model nascent performance performance strongly influence model design train dataset size despite promise result still substantial room improvement one large specialize nlp benchmarks annotate experts cuad serve challenge research benchmark broader nlp community
task long form question answer lfqa involve retrieve document relevant give question use generate paragraph length answer many model recently propose lfqa show paper task formulation raise fundamental challenge regard evaluation dataset creation currently preclude meaningful model progress demonstrate challenge first design new system rely sparse attention contrastive retriever learn achieve state art performance eli5 lfqa dataset system top public leaderboard detail analysis reveal several trouble trend one system generate answer actually ground document retrieve two eli5 contain significant train validation overlap least eighty-one eli5 validation question occur paraphrase form train set three rouge l informative metric generate answer quality easily game four human evaluations use text generation task unreliable lfqa offer suggestions mitigate issue hope lead rigorous lfqa research meaningful progress future
code summarization generation empower conversion program language pl natural language nl code translation avail migration legacy code one pl another paper introduce plbart sequence sequence model capable perform broad spectrum program language understand generation task plbart pre train extensive collection java python function associate nl text via denoising autoencoding experiment code summarization english language code generation code translation seven program languages show plbart outperform rival state art model moreover experiment discriminative task eg program repair clone detection vulnerable code detection demonstrate plbart effectiveness program understand furthermore analysis reveal plbart learn program syntax style eg identifier name convention logical flow eg block inside else block equivalent else block crucial program semantics thus excel even limit annotations
pretrained text encoders bert apply increasingly various natural language process nlp task recently demonstrate significant performance gain however recent study demonstrate existence social bias pretrained nlp model although prior work make progress word level debiasing improve sentence level fairness pretrained encoders still lack exploration paper propose first neural debiasing method pretrained sentence encoder transform pretrained encoder output debiased representations via fair filter fairfil network learn fairfil introduce contrastive learn framework minimize correlation filter embeddings bias word also preserve rich semantic information original sentence real world datasets fairfil effectively reduce bias degree pretrained text encoders continuously show desirable performance downstream task moreover post hoc method require retrain text encoders enlarge fairfil application space
large scale transformer base language model lms demonstrate impressive capabilities open text generation however control generate text properties topic style sentiment challenge often require significant change model architecture retrain fine tune model new supervise data paper present novel approach topical language generation tlg combine pre train lm topic model information cast problem use bayesian probability formulation topic probabilities prior lm probabilities likelihood topical language generation probability posterior learn model derive topic probability distribution user provide document natural structure furthermore extend model introduce new parameters function influence quantity topical feature present generate text feature would allow us easily control topical properties generate text experimental result demonstrate model outperform state art result coherency diversity fluency faster decode
cross lingual word embeddings clwe prove useful many cross lingual task however exist approach learn clwe include ones contextual embeddings sense agnostic work propose novel framework align contextual embeddings sense level leverage cross lingual signal bilingual dictionaries operationalize framework first propose novel sense aware cross entropy loss model word sense explicitly monolingual elmo bert model pretrained sense aware cross entropy loss demonstrate significant performance improvement word sense disambiguation task propose sense alignment objective top sense aware cross entropy loss cross lingual model pretraining pretrain cross lingual model several language pair english german spanish japanese chinese compare best baseline result cross lingual model achieve fifty-two two hundred and nine one hundred and twenty-nine average performance improvements zero shoot cross lingual ner sentiment classification xnli task respectively
number morphology base word embed model introduce recent years however evaluation mostly limit english know morphologically simple language paper explore whether extent incorporate morphology word embeddings improve performance downstream nlp task case morphologically rich russian language nlp task choice pos tag chunk ner russian language mostly solve use morphology without understand semantics word experiment show morphology base embeddings train skipgram objective outperform exist embed model fasttext moreover complex morphology unaware model bert allow achieve significantly greater performance task presumably require understand word morphology
knowledge graph question answer kgqa systems base machine learn algorithms require thousands question answer pair train examples natural language process pipelines need module fine tune paper present novel qa approach dub tebaqa approach learn answer question base graph isomorphisms basic graph pattern sparql query learn basic graph pattern efficient due small number possible pattern novel paradigm reduce amount train data necessary achieve state art performance tebaqa also speed domain adaption process transform qa system development task much smaller easier data compilation task evaluation tebaqa achieve state art performance qald eight deliver comparable result qald nine lc quad v1 additionally perform fine grain evaluation complex query deal aggregation superlative question well ablation study highlight future research challenge
frame involve positive negative presentation argument issue depend audience goal speaker entman one thousand, nine hundred and eighty-three differences lexical frame focus work large effect people opinions beliefs make progress towards reframing arguments positive effect create dataset method task use lexical resource connotations create parallel corpus propose method argument reframing combine controllable text generation positive connotation post decode entailment component denotation result show method effective compare strong baselines along dimension fluency mean trustworthiness reduction fear
many covid nineteen patients develop prolong symptoms infection include fatigue delirium headache long term health impact condition still clear necessary develop way follow patients monitor health status support timely intervention treatment lack sufficient human resources follow patients propose novel smart chatbot solution back machine learn collect information ie generate digital diary personalize manner article describe design framework components prototype
pipelined nlp systems largely supersede end end neural model yet nearly commonly use model still require explicit tokenization step recent tokenization approach base data derive subword lexicons less brittle manually engineer tokenizers techniques equally suit languages use fix vocabulary may limit model ability adapt paper present canine neural encoder operate directly character sequence without explicit tokenization vocabulary pre train strategy operate either directly character optionally use subwords soft inductive bias use finer grain input effectively efficiently canine combine downsampling reduce input sequence length deep transformer stack encode context canine outperform comparable mbert model twenty-eight f1 tydi qa challenge multilingual benchmark despite twenty-eight fewer model parameters
recent study indicate nlu model prone rely shortcut feature prediction without achieve true language understand result model fail generalize real world distribution data work show word nlu train set model long tail distribution two find one nlu model strong preference feature locate head long tail distribution two shortcut feature pick early iterations model train two observations employ formulate measurement quantify shortcut degree train sample base shortcut measurement propose shortcut mitigation framework ltgr suppress model make overconfident predictions sample large shortcut degree experimental result three nlu benchmarks demonstrate long tail distribution explanation accurately reflect shortcut learn behavior nlu model experimental analysis indicate ltgr improve generalization accuracy ood data preserve accuracy distribution data
covid nineteen pandemic global crisis test every society expose critical role local politics crisis response unite state strong partisan divide result polarization individual behaviors divergent policy adoption across regions better understand divide characterize compare pandemic narratives democratic republican politicians social media use novel computational methods include computational frame analysis semantic role analysis analyze tweet politicians yous include president members congress state governors systematically uncover contrast narratives term topics frame agents shape narratives find democrats narrative tend concern pandemic well financial social support republicans discuss political entities china use contrast frame semantic roles democrats emphasize government role respond pandemic republicans emphasize roles individuals support small businesses party narratives also include shout out followers blame party find concretely expose gap elusive consensus two party methodologies may apply computationally study narratives various domains
build socially intelligent agent involve many challenge one track agent mental state transition teach agent make rational decisions guide utility like human towards end propose incorporate mental state parser utility model dialogue agents hybrid mental state parser extract information dialogue event observations maintain graphical representation agent mind meanwhile utility model rank model learn human preferences crowd source social commonsense dataset social iqa empirical result show propose model attain state art performance dialogue action emotion prediction task fantasy text adventure game dataset light also show example case demonstrate textiti propose mental state parser assist agent decision ground context like locations object textitii utility model help agent make reasonable decisions dilemma best knowledge first work build socially intelligent agent incorporate hybrid mental state parser discrete events continuous dialogues parse human like utility model
conversations social media sm increasingly use investigate social issue web online harassment rumor spread issue common thread research use adversarial reactions eg reply point factual inaccuracies rumor though adversarial reactions prevalent online conversations infer adverse view stance text reply difficult require complex natural language process nlp model moreover conventional nlp model stance mine need label data supervise learn get label conversations challenge conversations topic topics change time challenge make learn stance difficult nlp problem research first create new stance dataset comprise three different topics label users opinions topics pro con users stance reply others post favor oppose find limitations supervise approach propose weakly supervise approach predict stance twitter reply novel method allow use smaller number hashtags generate weak label twitter reply compare supervise learn method improve mean f1 macro eight hand label dataset without use hand label examples train set show applicability propose method covid nineteen relate conversations twitter
paper investigate whether power model pre train text data bert transfer general token sequence classification applications verify pre train model transferability test pre train model one text classification task mean tokens mismatch two real world non text token sequence classification data include amino acid sequence dna sequence music find even non text data model pre train text converge faster randomly initialize model test performance pre train model merely slightly worse model design specific task
pretrained language model significantly improve performance stream language understand task include extractive question answer provide high quality contextualized word embeddings however learn question answer model still need large scale data annotation specific domains work propose cooperative self play learn framework regex question generation answer regex build upon mask answer extraction task interactive learn environment contain answer entity recognizer question generator answer extractor give passage mask entity generator generate question around entity extractor train extract mask entity generate question raw texts framework allow train question generation answer model text corpora without annotation leverage reinforcement learn technique reward generate high quality question improve answer extraction model performance experiment result show regex outperform state art sota pretrained language model zero shoot approach standard question answer benchmarks yield new sota performance zero shoot set
model often attend salient word evolve throughout train approximate model train two stage process early train attention weight uniform model learn translate individual input word co occur frequently later model learn attend correct output know translate formalize define model property knowledge translate individual word ktiw eg know translate claim drive learn attention claim support fact attention mechanism learn ktiw learn word co occurrence statistics way around particularly construct train distribution make ktiw hard learn learn attention fail model even learn simple task copy input word output approximation explain model sometimes attend salient word inspire toy example multi head attention model overcome hard train distribution improve learn dynamics rather expressiveness
multimodal sentiment analysis currently identify significance variety domains purpose sentiment analysis different aspects distinguish modalities correspond one target process analyze work propose target aspect base multimodal sentiment analysis tabmsa first time furthermore attention capsule extraction multi head fusion network ef net task tabmsa devise multi head attention mha base network resnet one hundred and fifty-two employ deal texts image respectively integration mha capsule network aim capture interaction among multimodal input addition target aspect information context image also incorporate sentiment deliver evaluate propose model two manually annotate datasets experimental result demonstrate effectiveness propose model new task
effectively apply robots work environments assist humans essential develop evaluate visual ground vg affect machine performance occlude object however current vg work limit work environments offices warehouse object usually occlude due space utilization issue work propose novel ocid ref dataset feature refer expression segmentation task refer expressions occlude object ocid ref consist three hundred and five thousand, six hundred and ninety-four refer expressions two thousand, three hundred scenes provide rgb image point cloud input resolve challenge occlusion issue argue crucial take advantage 2d 3d signal resolve challenge occlusion issue experimental result demonstrate effectiveness aggregate 2d 3d signal refer occlude object still remain challenge modern visual ground systems ocid ref publicly available https githubcom lluma ocid ref
predict user intent detect correspond slot text two key problems natural language understand nlu context zero shoot learn task typically approach either use representations pre train multilingual transformers mbert machine translate source data know target language fine tune work focus particular scenario target language unknown train goal propose novel method augment monolingual source data use multilingual code switch via random translations enhance transformer language neutrality fine tune downstream task method also help discover novel insights code switch different language families around world impact performance target language experiment benchmark dataset multiatis yield average improvement forty-two accuracy intent task eighteen f1 slot task use method state art across eight different languages furthermore present application method crisis informatics use new human annotate tweet dataset slot fill english haitian creole collect haiti earthquake disaster
build effective neural machine translation nmt model low resourced morphologically rich african indigenous languages open challenge besides issue find available resources lot work put preprocessing tokenization recent study show standard tokenization methods always adequately deal grammatical diacritical tonal properties african languages couple extremely low availability train sample hinder production reliable nmt model paper use fon language case study revisit standard tokenization methods introduce word expressions base web tokenization human involve super word tokenization strategy create better representative vocabulary train furthermore compare tokenization strategy others fon french french fon translation task
infants acquire word phonemes unsegmented speech signal use segmentation cue distributional prosodic co occurrence cue many pre exist computational model represent process tend focus distributional prosodic cue paper propose nonparametric bayesian probabilistic generative model call prosodic hierarchical dirichlet process hide language model prosodic hdp hlm prosodic hdp hlm extension hdp hlm consider prosodic distributional cue within single integrative generative model conduct three experiment different type datasets demonstrate validity propose method result show prosodic daa successfully use prosodic cue outperform method solely use distributional cue main contributions study follow one develop probabilistic generative model time series data include prosody potentially double articulation structure two propose prosodic daa derive inference procedure prosodic hdp hlm show prosodic daa discover word directly continuous human speech signal use statistical information prosodic information unsupervised manner three show prosodic cue contribute word segmentation naturally distribute case word ie follow zipf law
document level relation extraction aim discover relations entities across whole document build dependency entities different sentence document remain great challenge current approach either leverage syntactic tree construct document level graph aggregate inference information different sentence paper build cross sentence dependencies infer compositional relations inter sentence mention adopt aggressive link strategy intermediate relations reason document level graph mention convolution notice generalization problem na instance cause incomplete annotation worsen fully connect mention pair improve rank loss propose attend problem experiment show connections different mention crucial document level relation extraction enable model extract meaningful higher level compositional relations
study text representation methods use deep model current methods word level embed character level embed scheme treat texts either sequence atomic word sequence character methods either ignore word morphologies word boundaries overcome limitations propose convert texts two representations develop sent2matrix method method allow explicit incorporation word morphologies boundaries couple novel serpentine pad method sent2matrix method lead interest visualization one character sequence fold two serpentine manifold notably method first attempt represent texts two format experimental result text classification task show method consistently outperform prior embed methods
transparency become key robotics ai necessary evaluate methods transparency provide include automatically generate natural language nl explanations explore parallel generation explanations much study field evaluation natural language generation nlg specifically investigate nlg evaluation measure map well explanations present exban corpus crowd source corpus nl explanations bayesian network run correlations compare human subjective rat nlg automatic measure find embed base automatic nlg evaluation methods bertscore bleurt higher correlation human rat compare word overlap metrics bleu rouge work implications explainable ai transparent robotic autonomous systems
present formal framework development family discriminative learn algorithms probabilistic context free grammars pcfgs base generalization criterion h first propose h criterion objective function growth transformations optimization method allow us develop final expressions estimation parameters pcfgs second generalize h criterion take account set reference interpretations set compete interpretations propose new family objective function allow us develop expressions estimation transformations pcfgs
multimodal pre train propel great advancement vision language research large scale pre train model although successful fatefully suffer slow inference speed due enormous computation cost mainly cross modal attention transformer architecture apply real life applications latency computation demand severely deter practical use pre train model paper study image text retrieval itr mature scenario vl application widely study even prior emergence recent pre train model propose simple yet highly effective approach lightningdot accelerate inference time itr thousands time without sacrifice accuracy lightningdot remove time consume cross modal attention pre train three novel learn objectives extract feature index offline employ instant dot product match rank significantly speed retrieval process fact lightningdot achieve new state art across multiple itr benchmarks flickr30k coco multi30k outperform exist pre train model consume 1000x magnitude computational hours code pre train checkpoints available https githubcom intersun lightningdot
paper study zero shoot cross lingual transfer vision language model specifically focus multilingual text video search propose transformer base model learn contextualized multilingual multimodal embeddings zero shoot set empirically demonstrate performance degrade significantly query multilingual text video model non english sentence address problem introduce multilingual multimodal pre train strategy collect new multilingual instructional video dataset multihowto100m pre train experiment vtt show method significantly improve video search non english languages without additional annotations furthermore multilingual annotations available method outperform recent baselines large margin multilingual text video search vtt vatex well multilingual text image search multi30k model multi howto100m available http githubcom berniebear multi ht100m
automatically accurately identify user intents fill associate slot speak language critical success dialogue systems traditional methods require manually define domain intent slot schema ask many domain experts annotate correspond utterances upon neural model train procedure bring challenge information share hinder schema data sparsity open domain dialogue systems tackle challenge explore new task automatic intent slot induction propose novel domain independent tool design coarse fine three step procedure include role label concept mine pattern mine rcap one role label extract keyphrases users utterances classify quadruple coarsely define intent roles via sequence label two concept mine cluster extract intent role mention name abstract fine grain concepts three pattern mine apply apriori algorithm mine intent role pattern automatically infer intent slot use coarse grain intent role label fine grain concepts empirical evaluations real world domain domain datasets show one rcap generate satisfactory slu schema outperform state art supervise learn method two rcap directly apply domain datasets gain least seventy-six improvement f1 score intent detection forty-one improvement f1 score slot fill three rcap exhibit power generic intent slot extractions less manual effort open pathways schema induction new domains unseen intent slot discovery generalizable dialogue systems
entity synonyms discovery crucial entity leverage applications however exist study suffer several critical issue one input mention may vocabulary oov may come different semantic space entities two connection mention entities may hide establish surface match three entities rarely appear due long tail effect tackle challenge facilitate knowledge graph propose novel entity synonyms discovery framework name emphkgsynnet specifically pre train subword embeddings mention entities use large scale domain specific corpus learn knowledge embeddings entities via joint transc transe model importantly obtain comprehensive representation entities employ specifically design emphfusion gate adaptively absorb entities knowledge information semantic feature conduct extensive experiment demonstrate effectiveness emphkgsynnet leverage knowledge graph experimental result show emphkgsynnet improve state art methods one hundred and forty-seven term hits3 offline evaluation outperform bert model eighty-three positive feedback rate online b test entity link module question answer system
word use talk current epidemiological crisis social media inform us conceptualize pandemic react development paper provide extensive explorative analysis discourse covid nineteen report twitter change time focus first wave pandemic base extensive corpus tweet produce 20th march 1st july two thousand and twenty first show topics associate development pandemic change time use topic model second show sentiment polarity language use tweet change relatively positive valence first lockdown toward negative valence correspondence reopen third show average subjectivity tweet increase linearly fourth popular frequently use figurative frame war change real riot fight enter discourse
search available reliable official understandable information trivial task due scatter information across internet availability lack governmental communication channel communicate african dialects languages paper introduce artificial intelligence power chatbot crisis communication would omnichannel multilingual multi dialectal present work modify starspace embed tailor african dialects question answer task along architecture propose chatbot system description different layer english french arabic tunisian igboyorub hausa use languages dialects quantitative qualitative evaluation result obtain real deploy covid nineteen chatbot result show users satisfy conversation chatbot meet customer need
work empirically demonstrate ability text graph convolutional network text gcn outperform traditional natural language process benchmarks task semi supervise swahili news classification particular focus experimentation sparsely label semi supervise context representative practical constraints face low resourced african languages follow result introduce variant text gcn model utilise bag word embed rather naive one hot encode reduce memory footprint text gcn whilst demonstrate similar predictive performance
aspect category detection acd one challenge task aspect base sentiment analysis problem purpose task identify aspect categories mention user generate review set pre define categories paper investigate performance various monolingual pre train language model compare multilingual model vietnamese aspect category detection problem conduct experiment two benchmark datasets restaurant hotel domain experimental result demonstrate effectiveness monolingual phobert model others two datasets also evaluate performance multilingual model base combination whole semeval two thousand and sixteen datasets languages vietnamese dataset best knowledge research study first attempt perform various available pre train language model aspect category detection task utilize datasets languages base multilingual model
shoot learn draw researchers attention overcome problem data scarcity recently large pre train language model show great performance shoot learn various downstream task question answer machine translation nevertheless little exploration make achieve shoot learn fact check task however fact check important problem especially amount information online grow exponentially every day paper propose new way utilize powerful transfer learn ability language model via perplexity score notable strength methodology lie capability shoot learn two train sample methodology already outperform major class baseline absolute ten f1 macro metric across multiple datasets experiment empirically verify plausibility rather surprise usage perplexity score context fact check highlight strength shoot methodology compare strong fine tune base baseline model moreover construct publicly release two new fact check datasets relate covid nineteen
recent work show supervise model often exploit data artifacts achieve good test score performance severely degrade sample outside train distribution contrast set gardneret al two thousand and twenty quantify phenomenon perturb test sample minimal way output label modify contrast set create manually require intensive annotation effort present novel method leverage rich semantic input representation automatically generate contrast set visual question answer task method compute answer perturb question thus vastly reduce annotation cost enable thorough evaluation model performance various semantic aspects eg spatial relational reason demonstrate effectiveness approach gqa dataset semantic scene graph image representation find despite gqa compositionality carefully balance label distribution two high perform model drop thirteen seventeen accuracy compare original test set finally show automatic perturbation apply train set mitigate degradation performance open door robust model
modern litigation fraud investigators often face overwhelm number document must review throughout matter majority legal case fraud investigators know beforehand exactly look find addition fraudsters may use deception hide behaviour intentions use code word effectively mean fraud investigators look needle haystack without know needle look like part larger research program use framework expedite investigation process apply text mine machine learn techniques structure framework use three well know methods fraud investigations fraud triangle ii golden w investigation question iii analysis compete hypotheses framework possible automatically organize investigative data easier investigators find answer typical investigative question research focus one components framework identification usage code word fraudsters novel annotate synthetic data set create contain code word hide normal email communication subsequently range machine learn techniques employ detect code word show state art bert model significantly outperform methods task result demonstrate deep neural language model reliably f1 score nine apply fraud investigations detection code word
ability transformers perform precision task question answer natural language inference nli summarise enable rank one best paradigm address natural language process nlp task nli one best scenarios test architectures due knowledge require understand complex sentence establish relationships hypothesis premise nevertheless model suffer incapacity generalise domains difficulties face multilingual interlingual scenarios lead pathway literature address issue involve design train extremely large architectures lead unpredictable behaviours establish barriers impede broad access fine tune paper propose new architecture call siamese inter lingual transformer silt efficiently align multilingual embeddings natural language inference allow unmatched language pair process silt leverage siamese pre train multi lingual transformers freeze weight two input sentence attend later combine matrix alignment method experimental result carry paper evidence silt allow reduce drastically number trainable parameters allow inter lingual nli achieve state art performance common benchmarks make code dataset available https githubcom jahuerta92 siamese inter lingual transformer
follow success dot product attention transformers numerous approximations recently propose address quadratic complexity respect input length however approximations thus far ignore contribution textitvalue vectors quality approximation work argue research efforts direct towards approximate true output attention sub layer include value vectors propose value aware objective show theoretically empirically optimal approximation value aware objective substantially outperform optimal approximation ignore value context language model moreover show choice kernel function compute attention similarity substantially affect quality sparse approximations kernel function less skew affect value vectors
paper provide roadmap explore question imbue learn agents ability understand generate contextually relevant natural language service achieve goal hypothesize two key components create agents interactivity environment ground show vital part language learn humans posit interactive narratives environments choice train agents game simulations agent interact world natural language perceive act upon talk world use textual descriptions command dialogue exist intersection natural language process storytelling sequential decision make discuss unique challenge text game puzzle like structure combine natural language state action space provide knowledge representation commonsense reason exploration beyond challenge describe far progress realm interactive narratives apply adjacent problem domains applications provide interest challenge well extensions discuss far describe three detail one evaluate ai system commonsense understand automatically create interactive narratives two adapt abstract text base policies include modalities vision three enable multi agent human ai collaboration share situate worlds
rise social media lead increase comment online forums however still exist invalid comment informative users moreover comment also quite toxic harmful people paper create dataset constructive toxic speech detection name uit victsd vietnamese constructive toxic speech detection dataset ten thousand human annotate comment task propose system constructive toxic speech detection state art transfer learn model vietnamese nlp phobert system obtain f1 score seven thousand, eight hundred and fifty-nine five thousand, nine hundred and forty classify constructive toxic comment respectively besides implement various baseline model traditional machine learn deep neural network base model evaluate dataset result solve several task online discussions develop framework identify constructiveness toxicity vietnamese social media comment automatically
pretrained language model lm drive impressive gain morpho syntactic semantic task ability model discourse pragmatic phenomena less clear step towards better understand discourse model capabilities propose sentence intrusion detection task examine performance broad range pretrained lms detection task english lack dataset task introduce insted novel intruder sentence detection dataset contain one hundred and seventy thousand document construct english wikipedia cnn news article experiment show pretrained lms perform impressively domain evaluation experience substantial drop cross domain set indicate limit generalisation capacity result novel linguistic probe dataset show substantial room improvement especially cross domain set
distributionally robust optimization dro provide framework train machine learn model able perform well collection relate data distributions uncertainty set do solve min max game model train minimize maximum expect loss among distributions uncertainty set careful design uncertainty set critical success dro procedure previous work limit relatively simple alternatives keep min max optimization problem exactly tractable f divergence ball paper argue instead use neural generative model characterize worst case distribution allow flexible problem specific selection uncertainty set however simple conceptually approach pose number implementation optimization challenge circumvent issue propose relaxation kl constrain inner maximization objective make dro problem amenable gradient base optimization large scale generative model develop model selection heuristics guide hyper parameter search toy settings realistic nlp task find propose approach yield model robust comparable baselines
gpts traditional fine tune fail achieve strong result natural language understand nlu show gpts better comparable similar size berts nlu task novel method p tune employ trainable continuous prompt embeddings knowledge probe lama benchmark best gpt recover sixty-four p1 world knowledge without additional text provide test time substantially improve previous best twenty percentage point superglue benchmark gpts achieve comparable sometimes better performance similar size berts supervise learn importantly find p tune also improve berts performance shoot supervise settings largely reduce need prompt engineer consequently p tune outperform state art approach shoot superglue benchmark
neural network prone learn spurious correlations bias datasets thus vulnerable make inferences new target domain prior work reveal spurious pattern via post hoc model explanations compute importance input feature eliminate unintended model behaviors regularize importance score human knowledge however regularization technique lack flexibility coverage since importance score towards pre define list feature adjust complex human knowledge feature interaction pattern generalization hardly incorporate work propose refine learn model collect human provide compositional explanations model failure case describe generalizable rule spurious pattern explanation train examples match regularize tackle challenge regularization coverage additionally introduce regularization term feature interaction support complex human rationale refine model demonstrate effectiveness propose approach two text classification task show improve performance target domain refinement
computer science perspective address line hate speech challenge task attract attention industry mainly social media platform owners academia chapter provide overview state art data science approach define hate speech task solve mitigate phenomenon address task limit investigation mostly semi automatic detection hate speech task majority exist computer science work focus finally summarize challenge open problems current data science research future directions field aim prepare easily understandable report capable promote multidisciplinary character hate speech research researchers domains eg psychology sociology thus take advantage knowledge achieve computer science domain also contribute back help improve computer science address urgent socially relevant issue prevalence hate speech social media
study demonstrate viability deploy bert style model serverless environments production set since freely available pre train model large deploy way utilize knowledge distillation fine tune model proprietary datasets two real world task sentiment analysis semantic textual similarity result obtain model tune specific domain deployable serverless environments subsequent performance analysis show solution result latency level acceptable production use also cost effective approach small medium size deployments bert model without infrastructure overhead
conditional random field crf base neural model among performant methods solve sequence label problems despite great success crf shortcoming occasionally generate illegal sequence tag eg sequence contain tag immediately tag forbid underlie bio tag scheme work propose mask conditional random field mcrf easy implement variant crf impose restrictions candidate paths train decode phase show propose method thoroughly resolve issue bring consistent improvement exist crf base model near zero additional cost
acoustic word embeddings awe fix dimensional representations variable length speech segment zero resource languages label data available one awe approach use unsupervised autoencoder base recurrent model another recent approach use multilingual transfer supervise awe model train several well resourced languages apply unseen zero resource language consider recent contrastive learn loss use purely unsupervised multilingual transfer settings firstly show term unsupervised term discovery system use contrastive self supervision result improvements previous unsupervised monolingual awe model secondly consider multilingual awe model adapt specific zero resource language use discover term find self supervise contrastive adaptation outperform adapt multilingual correspondence autoencoder siamese awe model give best overall result word discrimination task six zero resource languages
goal summary concisely state important information document principle mind introduce new reference free summary evaluation metrics use pretrained language model estimate information share document summary metrics modern take shannon game method summary quality score propose decades ago replace human annotators language model also view metrics extension blanc recently propose approach summary quality measurement base performance language model without help summary use gpt two empirically verify introduce metrics correlate human judgement base coverage overall quality five summary dimension
study problem entity relation extraction presence symbolic domain knowledge knowledge take form ontology define relations permissible arguments previous approach set integrate knowledge learn approach either self train approximations lose precise mean logical expressions contrast approach employ semantic loss capture precise mean logical sentence maintain probability distribution possible state guide model solutions minimize constraint violations focus low data regimes show semantic loss outperform baselines wide margin
use deep learn techniques grow across various field past decade complaints opaqueness black box model increase result increase focus transparency deep learn model work investigate various methods improve interpretability deep neural network natural language process nlp task include machine translation sentiment analysis provide comprehensive discussion definition term textitinterpretability various aspects begin work methods collect summarise survey associate local interpretation divide three categories one explain model predictions relate input feature two explain natural language explanation three probe hide state model word representations
exist curriculum learn approach neural machine translation nmt require sample sufficient amount easy sample train data early train stage always achievable low resource languages amount train data limit address limitation propose novel token wise curriculum learn approach create sufficient amount easy sample specifically model learn predict short sub sequence begin part target sentence early stage train sub sequence gradually expand train progress new curriculum design inspire cumulative effect translation errors make latter tokens difficult predict begin ones extensive experiment show approach consistently outperform baselines five language pair especially low resource languages combine approach sentence level methods improve performance high resource languages
present graph base translation model translate source graph target string source graph construct dependency tree extra link non syntactic phrase connect inspire phrase base model first introduce translation model segment graph sequence disjoint subgraphs generate translation combine subgraph translations leave right use beam search however similar phrase base model model weak phrase reorder therefore introduce model base synchronous node replacement grammar learn recursive translation rule provide two implementations model different restrictions source graph parse efficiently experiment chinese english german english show graph base model significantly better correspond sequence tree base baselines
despite important progress conversational systems often generate dialogues sound unnatural humans conjecture reason lie different train test condition agents train control lab set test wild train learn generate utterance give human dialogue history hand test must interact hence deal noisy data propose fill gap train model mix batch contain sample human machine generate dialogues assess validity propose method guesswhat visual referential game
paper evaluate performance several modern subword segmentation methods low resource neural machine translation set compare segmentations produce apply bpe token sentence level morphologically base segmentations lmvr morsel evaluate translation task english nepali sinhala kazakh predict use morphologically base segmentation methods would lead better performance set however compare bpe find consistent reliable differences emerge segmentation methods morphologically base methods outperform bpe case perform best tend vary across task performance segmentation methods often statistically indistinguishable
recent work unsupervised question answer show model train procedurally generate question answer pair achieve performance competitive supervise methods work consider task unsupervised read comprehension present method perform test time learn ttl give context text passage without require train large scale human author datasets contain textitcontext question answer triplets method operate directly single test context use self supervision train model synthetically generate question answer pair infer answer unseen human author question context method achieve accuracies competitive fully supervise methods significantly outperform current unsupervised methods ttl methods smaller model also competitive current state art unsupervised read comprehension
paper propose novel lightweight relation extraction approach structural block drive convolutional neural learn specifically detect essential sequential tokens associate entities dependency analysis name structural block encode block block wise inter block wise representation utilize multi scale cnns one eliminate noisy irrelevant part sentence meanwhile two enhance relevant block representation block wise inter block wise semantically enrich representation method advantage independent long sentence context since encode sequential tokens within block boundary experiment two datasets ie semeval2010 kbp37 demonstrate significant advantage method particular achieve new state art performance kbp37 dataset comparable performance state art semeval2010 dataset
non autoregressive transformer promise text generation model however current non autoregressive model still fall behind autoregressive counterparts translation quality attribute accuracy gap lack dependency model among decoder input paper propose cnat learn implicitly categorical cod latent variables non autoregressive decode interaction among categorical cod remedy miss dependencies improve model capacity experiment result show model achieve comparable better performance machine translation task compare several strong baselines
sentiment analysis one fundamental task natural language process popular languages like english arabic russian mandarin also indian languages hindi bengali tamil see significant amount work area however marathi language third popular language india still lag behind due absence proper datasets paper present first major publicly available marathi sentiment analysis dataset l3cubemahasent curated use tweet extract various maharashtrian personalities twitter account dataset consist sixteen thousand distinct tweet classify three broad class viz positive negative neutral also present guidelines use annotate tweet finally present statistics dataset baseline classification result use cnn lstm ulmfit bert base deep learn model
word embeddings basic build block modern nlp pipelines efforts make learn rich efficient interpretable embeddings large generic datasets available public domain however embeddings limit applicability small corpora specific domains automotive manufacture maintenance support etc work present comprehensive notion interpretability word embeddings propose novel method generate highly interpretable efficient embeddings domain specific small corpus report evaluation result result word embeddings demonstrate novel feature enhance interpretability
various robustness evaluation methodologies different perspectives propose different natural language process nlp task methods often focus either universal task specific generalization capabilities work propose multilingual robustness evaluation platform nlp task textflint incorporate universal text transformation task specific transformation adversarial attack subpopulation combinations provide comprehensive robustness analysis textflint enable practitioners automatically evaluate model aspects customize evaluations desire line code guarantee user acceptability text transformations linguistically base provide human evaluation one textflint generate complete analytical report well target augment data address shortcomings model robustness validate textflint utility perform large scale empirical evaluations sixty-seven thousand evaluations state art deep learn model classic supervise methods real world systems almost model show significant performance degradation include decline fifty bert prediction accuracy task aspect level sentiment classification name entity recognition natural language inference therefore call robustness include model evaluation promote healthy development nlp technology
automatically describe image use natural sentence important task support visually impair people inclusion onto internet still big challenge require understand relation object present image attribute action involve visual interpretation methods need linguistic model also necessary verbally describe semantic relations problem know image caption although many datasets propose literature majority contain english caption whereas datasets caption describe languages scarce recently movement call pracegover arise internet stimulate users social media publish image tag pracegover add short description content thus inspire movement propose pracegover multi modal dataset portuguese caption base post instagram first large dataset image caption portuguese freely annotate image caption dataset bring additional challenge problem first contrast popular datasets ms coco caption pracegover one reference image also mean variance reference sentence length significantly greater ms coco caption two characteristics contribute make dataset interest due linguistic aspect challenge introduce image caption problem publicly share dataset https githubcom gabrielsantosrv pracegover
recognize relations entities pivotal task relational learn learn relation representations distantly label datasets difficult abundant label noise complicate expressions human language paper aim learn predictive interpretable robust relation representations distantly label data effective different settings include supervise distantly supervise shoot learn instead solely rely supervision noisy label propose learn prototypes relation contextual information best explore intrinsic semantics relations prototypes representations feature space abstract essential semantics relations entities sentence learn prototypes base objectives clear geometric interpretation prototypes unit vectors uniformly disperse unit ball statement embeddings center end correspond prototype vectors surface ball approach allow us learn meaningful interpretable prototypes final classification result several relation learn task show model significantly outperform previous state art model demonstrate robustness encoder interpretability prototypes extensive experiment
process mine focus analysis record event data order gain insights true execution business process foundational process mine techniques treat data sequence abstract events advance techniques depend availability specific kinds information resources organizational mine business object artifact centric analysis however information generally readily available rather associate events ad hoc manner often even part unstructured textual attribute give size complexity event log call automate support extract process information thereby enable advance process mine techniques paper present approach achieve call semantic role label event data combine analysis textual attribute value base state art language model novel attribute classification technique manner approach extract information eight semantic roles per event demonstrate approach efficacy quantitative evaluation use broad range event log demonstrate usefulness extract information case study
speech affect recognition problem extract emotional affect audio data low resource languages corpora rear affect recognition difficult task cross corpus settings present approach model train high resource language fine tune recognize affect low resource language train model corpus set savee emovo urdu iemocap achieve baseline accuracy six thousand and forty-five six thousand, eight hundred and five eight thousand and thirty-four five thousand, six hundred and fifty-eight percent respectively capture diversity affect languages cross corpus evaluations discuss detail find accuracy improve add domain target data train data finally show performance improve low resource language speech affect recognition achieve uar six thousand, nine hundred and thirty-two six hundred and eighty-two urdu italian speech affect
artificial write permeate live due recent advance large scale transformer base language model lms bert variants gpt two three others use pretrained model fine tune specific task researchers extend state art many nlp task show capture linguistic knowledge also retain general knowledge implicitly present data successes excite unfortunately lms train unfiltered text corpora suffer degenerate bias behaviour well establish show recent improvements lms also store ethical moral value society actually bring moral dimension surface value capture geometrically direction embed space reflect well agreement phrase social norms implicitly express train texts provide path attenuate even prevent toxic degeneration lms since one rate non normativity arbitrary phrase without explicitly train lm task moral dimension use moral compass guide even lms towards produce normative text show
recent developments natural language process lead introduction state art neural language model enable unsupervised transferable learn use different pretraining objectives model achieve excellent result downstream nlp task various domain adaptation techniques improve performance domain specific task compare analyze pretrained neural language model xlnet autoregressive bert autoencoder legal task result show xlnet model perform better sequence classification task legal opinions classification whereas bert produce better result ner task use domain specific pretraining additional legal vocabulary adapt bert model legal domain prepare multiple variants bert model use methods combination compare variants bert model specialize legal domain conclude additional pretraining vocabulary techniques enhance bert model performance legal opinions classification task additional legal vocabulary improve bert performance ner task combine pretraining vocabulary techniques improve final result legal vocab bert model give best result legal opinions task outperform larger pretrained general language model ie bert base xlnet base
recent work aspect level sentiment classification demonstrate efficacy incorporate syntactic structure dependency tree graph neural networksgnn approach usually vulnerable parse errors better leverage syntactic information face unavoidable errors propose simple yet effective graph ensemble technique graphmerge make use predictions differ ent parsers instead assign one set model parameters dependency tree first combine dependency relations different parse apply gnns result graph allow gnn mod els robust parse errors additional computational cost help avoid overparameterization overfitting gnn layer stack introduce connectivity ensemble graph experiment semeval two thousand and fourteen task four acl fourteen twitter datasets show graphmerge model outperform model single dependency tree also beat ensemble mod els without add model parameters
authorship attribution aa task find owner give text important widely study research topic many applications recent work show deep learn methods could achieve significant accuracy improvement aa task nevertheless propose methods represent user post use single type feature eg word bi grams adopt text classification approach address task furthermore methods offer limit explainability aa result paper address limitations propose deepstyle novel embed base framework learn representations users salient write style conduct extensive experiment two real world datasets twitter weibo experiment result show deepstyle outperform state art baselines aa task
online hate speech important issue break cohesiveness online social communities even raise public safety concern societies motivate rise issue researchers develop many traditional machine learn deep learn methods detect hate speech online social platforms automatically however methods consider single type textual feature eg term frequency use word embeddings approach neglect rich textual information could utilize improve hate speech detection paper propose deephate novel deep learn model combine multi faceted text representations word embeddings sentiments topical information detect hate speech online social platforms conduct extensive experiment evaluate deephate three large publicly available real world datasets experiment result show deephate outperform state art baselines hate speech detection task also perform case study provide insights salient feature best aid detect hate speech online social platforms
automate hate speech detection social media challenge task recently gain significant traction data mine natural language process community however exist methods adopt supervise approach depend heavily annotate hate speech datasets imbalanced often lack train sample hateful content paper address research gap propose novel multitask learn base model angrybert jointly learn hate speech detection sentiment classification target identification secondary relevant task conduct extensive experiment augment three commonly use hate speech detection datasets experiment result show angrybert outperform state art single task learn multitask learn baselines conduct ablation study case study empirically examine strengths characteristics angrybert model show secondary task able improve hate speech detection
take step towards address representation african continent nlp research create first large publicly available high quality dataset name entity recognition ner ten african languages bring together variety stakeholders detail characteristics languages help researchers understand challenge languages pose ner analyze datasets conduct extensive empirical evaluation state art methods across supervise transfer learn settings release data code model order inspire future research african nlp
research devote low resource veps karelian languages algorithms assign part speech tag word grammatical properties word present article algorithms use morphological dictionaries lemma part speech set grammatical feature gramset know word form algorithms base analogy hypothesis word suffix likely inflectional model part speech gramset accuracy algorithms evaluate compare three hundred and thirteen thousand vepsian sixty-six thousand karelian word use verify accuracy algorithms special function design assess quality result develop algorithms nine hundred and twenty-four vepsian word eight hundred and sixty-eight karelian word assign correct part speech develop algorithm nine hundred and fifty-three vepsian word nine hundred and seven karelian word assign correct gramset algorithm morphological semantic tag texts closely relate inseparable corpus process describe paper
report present analysis hashtags use italian cultural heritage institutions promote communicate cultural content covid nineteen lock period italy several activities support engage users propose use social media activities present one hashtags help aggregate content create community specific topics result show one side italian institutions proactive adapt pandemic scenario side users react positively increase participation propose activities
standard automatic metrics bleu problematic document level mt evaluation neither distinguish document level improvements translation quality sentence level ones identify specific discourse phenomena cause translation errors address problems propose automatic metric blond document level machine translation evaluation blond take discourse coherence consideration calculate recall distance check point phrase tag provide comprehensive evaluation score combine n gram extensive comparisons blond exist evaluation metrics conduct illustrate critical distinctions experimental result show blond much higher document level sensitivity respect previous metrics human evaluation also reveal high pearson r correlation value blond score manual quality judgments
current state art approach cross modal retrieval process text visual input jointly rely transformer base architectures cross attention mechanisms attend word object image offer unmatched retrieval performance model one typically pretrained scratch thus less scalable two suffer huge retrieval latency inefficiency issue make impractical realistic applications address crucial gap towards improve efficient cross modal retrieval propose novel fine tune framework turn pretrained text image multi modal model efficient retrieval model framework base cooperative retrieve rerank approach combine one twin network separately encode items corpus enable efficient initial retrieval two cross encoder component nuanced ie smarter rank retrieve small set items also propose jointly fine tune two components share weight yield parameter efficient model experiment series standard cross modal retrieval benchmarks monolingual multilingual zero shoot setups demonstrate improve accuracy huge efficiency benefit state art cross encoders
success large scale pre train multilingual model natural language process nlp recent years see proliferation large web mine text datasets cover hundreds languages however date systematic analysis quality publicly available datasets whether datasets actually contain content languages claim represent work manually audit quality two hundred and five language specific corpora release five major public datasets ccaligned paracrawl wikimatrix oscar mc4 audit correctness language cod sixth jw300 find lower resource corpora systematic issue least fifteen corpora completely erroneous significant fraction contain less fifty sentence acceptable quality similarly find eighty-two corpora mislabeled use nonstandard ambiguous language cod demonstrate issue easy detect even non speakers languages question supplement human judgements automatic analyse inspire analysis recommend techniques evaluate improve multilingual corpora discuss risk come low quality data release
problem solve understand problem one seek solve essential initial step paper propose computational methods facilitate problem understand task recognize unknown specifications long math problems focus topic probability experimental result show learn model yield strong result task promise first step towards human interpretable modular approach understand long math problems
problem knowledge base visual question answer involve answer question require external knowledge addition content image knowledge typically come variety form include visual textual commonsense knowledge use knowledge source however also increase chance retrieve irrelevant noisy facts make difficult comprehend facts find answer address challenge propose multi modal answer validation use external knowledge mavex idea validate set promise answer candidates base answer specific knowledge retrieval contrast exist approach search answer vast collection often irrelevant facts approach aim learn knowledge source trust answer candidate validate candidate use source consider multi modal set rely textual visual knowledge resources include image search use google sentence wikipedia article concepts conceptnet experiment ok vqa challenge knowledge base vqa dataset demonstrate mavex achieve new state art result
automatic speech recognition asr imperfect process result certain mismatch asr output text compare plain write text transcriptions plain text data use train systems speak language understand asr prove strategy reduce say mismatch prevent degradations hallucinate asr output would give gold transcription prior work domain focus model errors phonetic level use lexicon convert phone word usually accompany fst language model present novel end end model directly predict hallucinate asr word sequence output condition input word sequence well correspond phoneme sequence improve prior publish result recall errors domain asr system transcription unseen data well domain asr system transcriptions audio unrelated task additionally explore scenario limit characterization data test asr system obtainable verify extrinsic validity method also use hallucinate asr errors augment train speak question classifier find enable robustness real asr errors downstream task scarce even zero task specific audio available train time
propose tough mention recall tmr metrics supplement traditional name entity recognition ner evaluation examine recall specific subsets tough mention unseen mention whose tokens token type combination observe train type confusable mention token sequence multiple entity type test data demonstrate usefulness metrics evaluate corpora english spanish dutch use five recent neural architectures identify subtle differences performance bert flair two english ner corpora identify weak spot performance current model spanish conclude tmr metrics enable differentiation otherwise similar score systems identification pattern performance would go unnoticed overall precision recall f1
neural keyphrase generation model recently attract much interest due ability output absent keyphrases keyphrases appear source text paper discuss usefulness absent keyphrases information retrieval ir perspective show commonly draw distinction present absent keyphrases make explicit enough introduce finer grain categorization scheme shed light impact absent keyphrases scientific document retrieval scheme find fraction around twenty word make keyphrases actually serve document expansion small fraction word behind much gain observe retrieval effectiveness also discuss propose scheme offer new angle evaluate output neural keyphrase generation model
present level proliferation fake bias propagandistic content online make impossible fact check every single suspicious claim article either manually automatically thus many researchers shift attention higher granularity aim profile entire news outlets make possible detect likely fake news moment publish simply check reliability source source factuality also important element systems automatic fact check fake news detection need assess reliability evidence retrieve online political bias detection western political landscape predict leave center right bias equally important topic experience similar shift towards profile entire news outlets moreover clear connection two highly bias media less likely factual yet two problems address separately survey review state art media profile factuality bias argue need model jointly discuss interest recent advance use different information source modalities go beyond text article target news outlet publish finally discuss current challenge outline future research directions
ensure faithful interaction data representation humanities depend expert construct grind truths
introduce delft factoid question answer system combine nuance depth knowledge graph question answer approach broader coverage free text delft build free text knowledge graph wikipedia entities nod sentence entities co occur edge question delft find subgraph link question entity nod candidates use text sentence edge create dense high coverage semantic graph novel graph neural network reason free text graph combine evidence nod via information along edge sentence select final answer experiment three question answer datasets show delft answer entity rich question better machine read base model bert base answer rank memory network delft advantage come high coverage free text knowledge graph double dbpedia relations novel graph neural network reason rich noisy free text evidence
query example well know information retrieval task document choose user search query goal retrieve relevant document large collection however document often cover multiple aspects topic address scenario introduce task faceted query example users also specify finer grain aspect addition input query document focus application task scientific literature search envision model able retrieve scientific paper analogous query scientific paper along specifically choose rhetorical structure elements one solution problem work rhetorical structure elements refer facets indicate background method result aspects scientific paper introduce describe expert annotate test collection evaluate model train perform task test collection consist diverse set fifty query document draw computational linguistics machine learn venues carefully follow annotation guideline use trec depth k pool k one hundred two hundred and fifty result data collection consist grade relevance score high annotation agreement data freely available research purpose
unsupervised cluster aim discover semantic categories data accord distance measure representation space however different categories often overlap representation space begin learn process pose significant challenge distance base cluster achieve good separation different categories end propose support cluster contrastive learn sccl novel framework leverage contrastive learn promote better separation assess performance sccl short text cluster show sccl significantly advance state art result benchmark datasets three eleven improvement accuracy four fifteen improvement normalize mutual information furthermore quantitative analysis demonstrate effectiveness sccl leverage strengths bottom instance discrimination top cluster achieve better intra cluster inter cluster distance evaluate grind truth cluster label
pioneer work field inductive inference gold one thousand, nine hundred and sixty-seven prove set contain finite languages least one infinite language fix alphabet learnable exact sense within framework angluin one thousand, nine hundred and eighty provide complete characterization learnability language families mathematically concept exact learn classical set see use particular type metric learn limit short research note use niyogi extend version theorem blum blum one thousand, nine hundred and seventy-five existence lock data set prove necessary condition learnability limit family languages give metric recover gold theorem special case moreover language family assume contain finite languages condition also become sufficient learnability limit
multilingual language model show decent performance multilingual cross lingual natural language understand task however power multilingual model code switch task fully explore paper study effectiveness multilingual language model understand capability adaptability mix language set consider inference speed performance number parameters measure practicality conduct experiment three language pair name entity recognition part speech tag compare exist methods use bilingual embeddings multilingual meta embeddings find suggest pre train multilingual model necessarily guarantee high quality representations code switch use meta embeddings achieve similar result significantly fewer parameters
text base game simulate worlds interact players use natural language recent work use testbed autonomous language understand agents motivation understand mean word semantics key component humans understand reason act worlds however remain unclear extent artificial agents utilize semantic understand text end perform experiment systematically reduce amount semantic information available learn agent surprisingly find agent capable achieve high score even complete absence language semantics indicate currently popular experimental setup model may poorly design understand leverage game texts remedy deficiency propose inverse dynamics decoder regularize representation space encourage exploration show improve performance several game include zork discuss implications find design future agents stronger semantic understand
online discussion forums prevalent easily accessible thus allow people share ideas opinions post message discussion thread forum thread significantly grow length become difficult participants newcomers exist grasp main ideas mitigate problem study aim create automatic text summarizer online forums present hierarchical unify deep neural network build sentence thread representations forum summarization scheme bi lstm derive representation comprise information whole sentence whole thread whereas cnn capture informative feature respect context sentence thread attention mechanism apply top cnn highlight high level representations carry important information contribute desirable summary extensive performance evaluation conduct three datasets two real life online forums one news dataset result reveal propose model outperform several competitive baselines
work test performance two bidirectional transformer base language model bert spanbert predict directionality causal pair textual content preliminary result show predict direction inter sentence implicit causal relations challenge spanbert perform better bert causal sample longer span length also introduce crest framework unify collection scatter datasets causal relations
speech recognition see surge interest research last decade machine learn model speech recognition either require large train datasets lot storage memory combine prominence english number one language audio data available mean languages currently lack good speech recognition model method present paper show create train model speech recognition language highly accurate also require little storage memory train data compare traditional model allow train model recognize language deploy edge devices mobile phone car display fast real time speech recognition
short text popular avenue share feedback opinions review social media e commerce platforms etc many company need extract meaningful information may include thematic content well semantic polarity short texts understand users behaviour however obtain high quality sentiment associate human interpretable theme still remain challenge short texts paper develop eljst embed enhance generative joint sentiment topic model discover coherent diverse topics short texts use markov random field regularizer see generalisation skip gram base model leverage higher order semantic information appear word embed self attention weight graphical model result show average improvement ten topic coherence five topic diversification baselines finally eljst help understand users behaviour granular level explain bring significant value service healthcare industries often deal customers
introduce functorial language model principled way compute probability distributions word sequence give monoidal functor grammar mean yield method train categorical compositional distributional discocat model raw text data provide proof concept implementation discopy python toolbox monoidal categories
conventional machine read comprehension mrc well address pattern match ability commonsense reason remain gap humans machine previous methods tackle problem enrich word representations via pre train knowledge graph embeddings kge however make limit use large number connections nod knowledge graph kg could pivotal cue build commonsense reason chain paper propose plug play module incorporate connection information commonsense reason piecer beyond enrich word representations knowledge embeddings piecer construct joint query passage graph explicitly guide commonsense reason knowledge orient connections word piecer high generalizability since plug suitable position mrc model experimental result record large scale public mrc dataset require commonsense reason show piecer introduce stable performance improvements four representative base mrc model especially low resource settings
many case machine learn research suggest development train data might higher relevance choice model classifiers thus data augmentation methods develop improve classifiers artificially create train data nlp challenge establish universal rule text transformations provide new linguistic pattern paper present evaluate text generation method suitable increase performance classifiers long short texts achieve promise improvements evaluate short well long text task enhancement text generation method simulate low data regime additive accuracy gain one thousand, five hundred and fifty-three achieve current track construct regimes universally applicable also show major improvements several real world low data task four hundred and eighty-four f1 score since evaluate method many perspectives also observe situations method might suitable discuss implications pattern successful application approach different type datasets
demonstrate transformer base model redesign order capture inductive bias across task different granularities perform inference zero shoot manner specifically show sentence level transformers modify effective sequence labelers token level without direct supervision compare range diverse previously propose methods generate token level label present simple yet effective modify attention layer significantly advance current state art
automatic multiple choice question generation mcqg useful yet challenge task natural language process nlp task automatic generation correct relevant question textual data despite usefulness manually create sizeable meaningful relevant question time consume challenge task teachers paper present nlp base system automatic mcqg computer base test examination cbtewe use nlp technique extract keywords important word give lesson material validate system perverse five lesson materials use check effectiveness efficiency system manually extract keywords teacher compare auto generate keywords result show system capable extract keywords lesson materials set examinable question outcome present user friendly interface easy accessibility
research area vision language encompass challenge topics seek connect visual textual information video text problem one topics goal connect input video textual description connection mainly make retrieve significant descriptions corpus generate new one give context video two ways represent essential task computer vision natural language process communities call text retrieval video task video caption description task two task substantially complex predict retrieve single sentence image spatiotemporal information present videos introduce diversity complexity regard visual content structure associate language descriptions review categorize describe state art techniques video text problem cover main video text methods ways evaluate performance analyze report benchmark datasets create show drawbacks strengths problem requirements also show impressive progress researchers make dataset analyze despite progress video text conversion still unsolved state art techniques still long way achieve human like performance generate retrieve video descriptions cover several significant challenge field discuss future research directions
sentiment analysis important task understand social media content like customer review twitter facebook feed etc multilingual communities around world large amount social media text characterize presence code switch thus become important build model handle code switch data however annotate code switch data scarce need unsupervised model algorithms propose general framework call unsupervised self train show applications specific use case sentiment analysis code switch data use power pre train bert model initialization fine tune unsupervised manner use pseudo label produce zero shoot transfer test algorithm multiple code switch languages provide detail analysis learn dynamics algorithm aim answer question unsupervised model understand code switch languages learn representations unsupervised model compete well supervise counterparts performance reach within one seven weight f1 score compare supervise model train two class problem
recent study big data analytics natural language process develop automatic techniques analyze sentiment social media information addition grow user base social media high volume post also provide valuable sentiment information predict price fluctuation cryptocurrency research direct predict volatile price movement cryptocurrency analyze sentiment social media find correlation previous work develop analyze sentiment english social media post propose method identify sentiment chinese social media post popular chinese social media platform sina weibo develop pipeline capture weibo post describe creation crypto specific sentiment dictionary propose long short term memory lstm base recurrent neural network along historical cryptocurrency price movement predict price trend future time frame conduct experiment demonstrate propose approach outperform state art auto regressive base model one hundred and eighty-five precision one hundred and fifty-four recall
neural predictive model achieve remarkable performance improvements various natural language process task however neural predictive model suffer lack explainability predictions limit practical utility paper propose neural predictive approach make prediction generate correspond explanation simultaneously leverage knowledge entail explanations additional distillation signal efficient learn conduct preliminary study chinese medical multiple choice question answer english natural language inference commonsense question answer task experimental result show propose approach generate reasonable explanations predictions even small scale train corpus propose method also achieve improve prediction accuracy three datasets indicate make predictions benefit generate explanation decision process
unsupervised nlp model represent word single point single region semantic space exist multi sense word embeddings represent longer word sequence like phrase sentence propose novel embed method text sequence phrase sentence sequence represent distinct set multi mode codebook embeddings capture different semantic facets mean codebook embeddings view cluster center summarize distribution possibly co occur word pre train word embed space introduce end end trainable neural model directly predict set cluster center input text sequence test time experiment show per sentence codebook embeddings significantly improve performances unsupervised sentence similarity extractive summarization benchmarks phrase similarity experiment discover multi facet embeddings provide interpretable semantic representation outperform single facet baseline
amid discussion green ai see explainability neglect explore possibility efficiently approximate computationally expensive explainers end propose task feature attribution model address empirical explainers empirical explainers learn data predict attribution map expensive explainers train test empirical explainers language domain find model expensive counterparts well fraction cost could thus mitigate computational burden neural explanations significantly applications tolerate approximation error
nlp ghana open source non profit organization aim advance development adoption state art nlp techniques digital language tool ghanaian languages problems paper first present motivation necessity efforts organization introduce popular ghanaian languages present state nlp ghana present nlp ghana organization outline aim scope work methods employ contributions make thus far nlp community ghana
present parallel machine translation train corpus english akuapem twi twenty-five thousand, four hundred and twenty-one sentence pair use transformer base translator generate initial translations akuapem twi later verify correct necessary native speakers eliminate occurrence translationese addition six hundred and ninety-seven higher quality crowd source sentence provide use evaluation set downstream natural language process nlp task typical use case larger human verify dataset train machine translation model akuapem twi higher quality six hundred and ninety-seven crowd source dataset recommend test dataset machine translation english twi twi english model furthermore twi part crowd source data may also use task representation learn classification etc fine tune transformer translation model train corpus report benchmarks crowd source test set
work retrain distil bert language model walmart voice shop assistant retail domain specific data also inject universal syntactic dependencies improve performance model natural language understand nlu components voice assistants available today heavily dependent language model various task generic language model bert roberta useful domain independent assistants limitations cater specific domain example shop domain token horizon mean brand instead literal mean generic model able capture subtleties work retrain distil version bert language model retail domain specific data walmart voice shop assistant also include universal dependency base feature retrain process improve performance model downstream task evaluate performance retrain language model four downstream task include intent entity detection sentiment analysis voice title shorten proactive intent suggestion observe increase performance downstream task one hundred and thirty-one average
unsupervised neural machine translation unmt beneficial especially low resource languages dravidian family however unmt systems tend fail realistic scenarios involve actual low resource languages recent work propose utilize auxiliary parallel data achieve state art result work focus unsupervised translation english kannada low resource dravidian language additionally utilize limit amount auxiliary data english relate dravidian languages show unify write systems essential unsupervised translation dravidian languages explore several model architectures use auxiliary data order maximize knowledge share enable unmt distant language pair experiment demonstrate crucial include auxiliary languages similar focal language kannada furthermore propose metric measure language similarity show serve good indicator select auxiliary languages
transformer network revolutionize nlp representation learn since introduce though great effort make explain representation transformers widely recognize understand sufficient one important reason lack enough visualization tool detail analysis paper propose use dictionary learn open black box linear superpositions transformer factor visualization demonstrate hierarchical semantic structure capture transformer factor eg word level polysemy disambiguation sentence level pattern formation long range dependency pattern confirm conventional prior linguistic knowledge rest relatively unexpected may provide new insights hope visualization tool bring knowledge better understand transformer network work
podcast track new text retrieval conference trec two thousand and twenty podcast track design encourage research podcast information retrieval nlp research communities track consist two share task segment retrieval summarization base dataset one hundred thousand podcast episodes metadata audio automatic transcripts release concurrently track track generate considerable interest attract hundreds new registrations trec fifteen team mostly disjoint search summarization make final submissions assessment deep learn dominant experimental approach search experiment summarization paper give overview task result participants experiment track return trec two thousand and twenty-one two task incorporate slight modifications response participant feedback
transformer base language model change modern natural language process nlp landscape high resource languages english chinese russian etc however technology yet exist ghanaian language paper introduce first model twi akan widely speak ghanaian language specific contribution research work development several pretrained transformer language model akuapem asante dialects twi pave way advance application areas name entity recognition ner neural machine translation nmt sentiment analysis sa part speech pos tag specifically introduce four different flavour abena bert model akan fine tune set akan corpora bako bert akan knowledge train scratch open source model hug face model hub demonstrate use via simple sentiment classification example
visual events composition temporal action involve actors spatially interact object develop computer vision model reason compositional spatio temporal events need benchmarks analyze progress uncover shortcomings exist video question answer benchmarks useful often conflate multiple source error one accuracy metric strong bias model exploit make difficult pinpoint model weaknesses present action genome question answer agqa new benchmark compositional spatio temporal reason agqa contain 192m unbalance question answer pair 96k videos also provide balance subset 39m question answer pair three order magnitude larger exist benchmarks minimize bias balance answer distributions type question structure although human evaluators mark eight thousand, six hundred and two question answer pair correct best model achieve four thousand, seven hundred and seventy-four accuracy addition agqa introduce multiple train test split test various reason abilities include generalization novel compositions indirect reference compositional step use agqa evaluate modern visual reason systems demonstrate best model barely perform better non visual baselines exploit linguistic bias none exist model generalize novel compositions unseen train
ground natural language instructions web perform previously unseen task enable accessibility automation introduce task dataset train ai agents open domain step step instructions originally write people build russ rapid universal support service tackle problem russ consist two model first bert lstm pointers parse instructions thingtalk domain specific language design ground natural language web ground model retrieve unique ids webpage elements request thingtalk russ may interact user dialogue eg ask address execute web operation eg click button inside web runtime augment train synthesize natural language instructions map thingtalk dataset consist eighty different customer service problems help websites total seven hundred and forty-one step step instructions correspond action russ achieve seven hundred and sixty-seven end end accuracy predict agent action single instructions outperform state art model directly map instructions action without thingtalk user study show russ prefer actual users web navigation
paper present submit system semeval two thousand and twenty-one task four read comprehension abstract mean system use large pre train language model encoder additional dual multi head co attention layer strengthen relationship passages question answer pair follow current state art model duma main difference stack passage question question passage attention modules instead calculate parallelly simulate consider process also add layer normalization module improve performance model furthermore incorporate know knowledge abstract concepts retrieve definitions candidate answer wordnet fee model extra input system call wordnet enhance dual multi head co attention wn duma achieve eight thousand, six hundred and sixty-seven eight thousand, nine hundred and ninety-nine accuracy official blind test set subtask one subtask two respectively
paper propose generative language model call afriki approach base lstm architecture train small corpus contemporary fiction aim promote human creativity use model author tool explore machine loop afrikaans poetry generation knowledge first study attempt creative text generation afrikaans
context importance feature model software product line consider model management variability objective define protocol conduct systematic map study summarize synthesize evidence reason algorithms feature model method application protocol conduct systematic map study accord guidelines k petersen result validate protocol conduct systematic map study conclusions initial find show detail review different reason algorithms feature model need
khmer text write leave right optional space space serve word boundary instead use readability functional purpose word segmentation prior step downstream task part speech pos tag thus robustness pos tag highly depend word segmentation conventional khmer pos tag two stage process begin word segmentation actual tag word afterward work joint word segmentation pos tag approach use single deep learn model propose word segmentation pos tag perform spontaneously propose model train test use publicly available khmer pos dataset validation suggest performance joint model par conventional two stage pos tag
propose novel interpretable framework cross lingual content flag significantly outperform prior work term predictive performance average inference time framework base nearest neighbour architecture interpretable design moreover easily adapt new instance without need retrain scratch unlike prior work encode texts also label neighbourhood space yield better accuracy ii use bi encoder instead cross encoder save computation time evaluation result ten different datasets abusive language detection eight languages show sizable improvements state art well speed inference time
abuse internet important societal problem time millions internet users face harassment racism personal attack type abuse across various platforms psychological effect abuse individuals profound last consequently past years substantial research effort towards automate abusive language detection field nlp position paper discuss role model users online communities play abuse detection specifically review analyze state art methods leverage user community information enhance understand detection abusive language explore ethical challenge incorporate user community information lay considerations guide future research finally address topic explainability abusive language detection propose properties explainable method aim exhibit describe user community information facilitate realization properties discuss effective operationalization explainability view properties
common tool word alignment rely large amount parallel sentence usually process accord one ibm model algorithms train data however machine translation mt systems especially neural mt nmt able produce word alignments use train attention head convenient word alignment theoretically viable byproduct attention base nmt also able provide decoder score translate sentence pair summarize different approach word alignment extract alignment score explore ways score extract nmt focus infer word alignment score base output sentence token probabilities compare extraction alignment score attention conclude aggregate source alignment score simple fee forward network achieve best result combine alignment extractors use
increase occurrence form negative effect misinformation social media platforms necessitate misinformation detection tool currently work do address covid nineteen misinformation however misinformation detection tool forty distinct indigenous ugandan languages paper address gap present basic language resources misinformation detection data set base code mix luganda english message source facebook twitter social media platforms several machine learn methods apply misinformation detection data set develop classification model detect whether code mix luganda english message contain misinformation ten fold cross validation evaluation classification methods experimental misinformation detection task show discriminative multinomial naive bay dmnb method achieve highest accuracy f measure seven thousand, eight hundred and nineteen seven thousand, seven hundred and ninety respectively also support vector machine bag ensemble classification model achieve comparable result result promise since machine learn model base n gram feature misinformation detection dataset
since advent online social media platforms twitter facebook useful health relate study conduct use information post online participants personal health relate issue mental health self harm depression study users often share stories platforms online users resort share empathy support online communities crucial help affect individuals preliminary analysis show content relate non suicidal self injury nssi proliferate twitter thus use twitter collect relevant data analyse proffer ways support users prone nssi behaviour approach utilise custom crawler retrieve relevant tweet self report users relevant organisations interest combat self harm textual analysis identify six major categories self harm users consist inflict anti self harm support seekers recover pro self harm risk inflict category dominate collection engagement perspective show online users respond information post self harm support organisations twitter note engage organisations apply useful technique uncover organisations strategy online participants show strong inclination towards online post associate mental health relate attribute study base premise social media use tool support proactive measure ease negative impact self harm consequently proffer ways prevent potential users engage self harm support affect users set recommendations support research dataset make available interest researchers
recently increase interest multilingual automatic speech recognition asr speech recognition system cater multiple low resource languages take advantage low amount label corpora multiple languages multilingualism become common today world increase interest code switch asr well code switch multiple languages freely interchange within single sentence sentence success low resource multilingual code switch asr often depend variety languages term acoustics linguistic characteristics well amount data available carefully consider build asr system challenge would like focus build multilingual code switch asr systems two different subtasks relate total seven indian languages namely hindi marathi odia tamil telugu gujarati bengali purpose provide total six hundred hours transcribe speech data comprise train test set languages include two code switch language pair hindi english bengali english also provide baseline recipe task wer three thousand and seventy-three three thousand, two hundred and forty-five test set multilingual code switch subtasks respectively
present novel approach detect translations ot ut part adequacy error check translation evaluation restrict machine translation mt output specifically target applications human generate translation pipeline goal system identify ot ut errors human translate video subtitle high error recall achieve without reference translations learn model synthesize train data compare various classification network train embeddings pre train language model best hybrid network gru cnn achieve eight hundred and ninety-three accuracy high quality human annotate evaluation data eight languages
recent neural base relation extraction approach though achieve promise improvement benchmark datasets report vulnerability towards adversarial attack thus far efforts mostly focus generate adversarial sample defend adversarial attack little know difference normal adversarial sample work take first step leverage salience base method analyze adversarial sample observe salience tokens direct correlation adversarial perturbations find adversarial perturbations either tokens exist train set superficial cue associate relation label extent approach unveil character adversarial sample release open source testbed diagnoseadv
media bias lead increase political polarization thus need automatic mitigation methods grow exist mitigation work display article multiple news outlets provide diverse news coverage without neutralize bias inherent display article therefore propose new task single neutralize article generation multiple bias article facilitate efficient access balance unbiased information paper compile new dataset neuws define automatic evaluation metric provide baselines multiple analyse serve solid start point propose task lastly obtain human evaluation demonstrate alignment metric human judgment
low resource african languages fully benefit progress neural machine translation lack data motivate challenge compare zero shoot learn transfer learn multilingual learn three bantu languages shona isixhosa isizulu english main target english isizulu translation thirty thousand sentence pair twenty-eight average size corpora show importance language similarity performance english isizulu transfer learn base english isixhosa english shona parent model whose bleu score differ fifty-two demonstrate multilingual learn surpass transfer learn zero shoot learn dataset bleu score improvements relative baseline english isizulu model ninety-nine sixty-one twenty respectively best model also improve previous sota bleu score ten
poetry generation long challenge artificial intelligence scope japanese poetry generation many researchers pay attention haiku generation focus waka generation explore creative potential natural language generation systems japanese poetry creation propose novel waka generation model wakavt automatically produce waka poems give user specify keywords firstly additive mask base approach present satisfy form constraint secondly structure transformer variational autoencoder integrate enhance quality generate content specifically obtain novelty diversity wakavt employ sequence latent variables effectively capture word level variability waka data improve linguistic quality term fluency coherence meaningfulness propose fuse multilevel self attention mechanism properly model hierarchical linguistic structure waka best knowledge first investigate waka generation model base transformer variational autoencoder objective subjective evaluation result demonstrate model outperform baselines significantly
introduce new semi supervise time series anomaly detection algorithm use deep reinforcement learn drl active learn efficiently learn adapt anomalies real world time series data model call rlad make assumption underlie mechanism produce observation sequence continuously adapt detection model base experience anomalous pattern addition require manual tune parameters outperform state art methods compare unsupervised semi supervise across several figure merit specifically outperform best unsupervised approach factor one hundred and fifty-eight f1 score one label around 44x another real world dataset one label compare rlad seven deep learn base algorithms across two common anomaly detection datasets around 3m data point twenty-eight two hundred and sixty-five anomalieswe outperform across several important performance metrics
work support development language technology languages africa provide wikidata derive resource name list correspond common entity type person location organization first mine wikidata name list approach emphasize scalability replicability address data quality issue languages use latin script produce list contain approximately nineteen million name across twenty-eight african languages describe data process use produce limitations provide software data public use finally discuss ethical considerations produce resource others kind
paper present submission semeval two thousand and twenty-one task five toxic span detection purpose task detect span make text toxic complex labour several reason firstly intrinsic subjectivity toxicity secondly due toxicity always come single word like insult offend sometimes whole expressions form word may toxic individually follow idea focus single word multi word expressions study impact use multi depth distilbert model use embeddings different layer estimate final per token toxicity quantitative result show use information multiple depths boost performance model finally also analyze best model qualitatively
claim verification task predict veracity write statements evidence previous large scale datasets model task classification ignore need retrieve evidence construct research purpose may representative real world need paper introduce novel claim verification dataset instance derive search engine query yield ten thousand, nine hundred and eighty-seven claim annotate evidence represent real world information need claim annotate evidence full wikipedia article section sentence level granularity annotation allow comparison two complementary approach verification stance classification evidence extraction follow entailment recognition comprehensive evaluation find significant difference accuracy two approach enable systems use evidence extraction summarize rationale end user maintain accuracy predict claim veracity challenge claim evidence document contain hundreds sentence dataset present interest challenge capture previous work evidence transfer learn experiment release code data support research task
neural machine translation distillation steal scenarios goal preserve performance target model teacher highest score hypothesis teacher model commonly use train new model student reference translations also available better hypotheses respect reference upsampled poor hypotheses either remove undersampled paper explore importance sample method landscape prune hypothesis upsampling undersampling deduplication combination english czech english german mt model use standard mt evaluation metrics show careful upsampling combination original data lead better performance compare train original synthesize data direct combination
recent text generation model easy generate relevant fluent text give text lack causal reason ability change part give text counterfactual story rewrite recently propose task test causal reason ability text generation model require model predict correspond story end condition modify counterfactual one previous work show traditional sequence sequence model well handle problem often capture spurious correlations original counterfactual end instead causal relations condition end address issue propose sketch customize generation model guide causality implicate condition end sketch stage skeleton extract remove word conflict counterfactual condition original end customize stage generation model use fill proper word skeleton guidance counterfactual condition way obtain counterfactual end relevant original end consistent counterfactual condition experimental result show propose model generate much better end compare traditional sequence sequence model
information extraction narrative clinical note useful patient care well secondary use medical data research clinical purpose many study focus information extraction english clinical texts less deal clinical note languages english study test feasibility use shelf information extraction algorithms identify medical concepts italian clinical note use metamap map medical concepts unify medical language system umls study address two question q1 understand would possible properly map medical term find clinical note relate semantic group disorder italian umls resources q2 investigate would feasible use metamap extract medical concepts italian clinical note result exp1 show italian umls metathesaurus source cover ninety-one medical term disorder semantic group find study dataset even metamap build analyze texts write english work properly also texts write italian metamap identify correctly half concepts italian clinical note use metamap annotation italian clinical note instead simple text search improve result fifteen percentage point metamap show recall precision f measure fifty-three ninety-eight sixty-nine respectively failures due impossibility metamap generate italian meaningful variants metamap performance annotate automatically translate english clinical note line find literature similar recall seventy-five f measure eighty-three even higher precision ninety-five
propose new framework understand represent relate salient events video use visual semantic role label represent videos set relate events wherein event consist verb multiple entities fulfill various roles relevant event study challenge task semantic role label videos vidsrl introduce vidsitu benchmark large scale video understand data source 29k ten second movie clip richly annotate verb semantic roles every two second entities co reference across events within movie clip events connect via event event relations clip vidsitu draw large collection movies sim3k choose complex sim42 unique verbs within video well diverse sim200 verbs one hundred annotations provide comprehensive analysis dataset comparison publicly available video understand benchmarks several illustrative baselines evaluate range standard video recognition model code dataset available vidsituorg
paper study effect order depth mention nest name entity recognition ner model ner essential task extraction biomedical information nest entities common since medical concepts assemble form larger entities conventional ner systems predict disjoint entities thus iterative model nest ner use multiple predictions enumerate entities impose predefined order largest smallest smallest largest design order agnostic iterative model procedure choose custom order train prediction accommodate task propose modification transformer architecture take account entities predict previous step provide set experiment study model capabilities effect order performance finally show smallest largest order give best result
present error analysis neural upos taggers evaluate use gold standard tag large positive contribution parse performance use predict upos tag either harm performance offer negligible improvement evaluate neural dependency parsers implicitly learn word type relate errors taggers make explain minimal impact use predict tag parsers also present short analysis contexts result reductions tag performance mask upos tag base errors make taggers tease away contribution upos tag taggers succeed fail classify correctly impact tag errors
social media play pivotal role disseminate news across globe act platform people express opinions variety topics covid nineteen vaccination drive across globe accompany wide variety express opinions often color emotions extract corpus twitter post relate covid nineteen vaccination create two class lexical categories emotions influence factor use unsupervised word embeddings track longitudinal change latent space lexical categories five countries strong vaccine roll program ie india usa brazil uk australia nearly six hundred thousand vaccine relate tweet unite state india analyze overall understand situation around world time period eight months june two thousand and twenty january two thousand and twenty-one cosine distance lexical categories use create similarity network modules use community detection algorithms demonstrate negative emotions like hesitancy towards vaccines high correlation health relate effect misinformation associations form major module highest importance network form january two thousand and twenty-one millions vaccines administer relationship emotions influence factor find variable across countries extract visualize propose framework may helpful guide design effective vaccine campaign use policymakers model vaccine uptake
two thousand and twenty coronavirus pandemic heighten need flag coronavirus relate misinformation fact check group take verify misinformation internet explore stories report fact check group politifact poynter snopes january june two thousand and twenty characterise six story cluster analyse time series story validity trend level agreement across sit break story cluster granular story type propose unique automate method bert classifier use classify diverse story source fact check stories tweet
fingerspell word sign letter letter important component american sign language previous work automatic fingerspell recognition assume boundaries fingerspell regions sign videos know beforehand paper consider task fingerspell detection raw untrimmed sign language videos important step towards build real world fingerspell recognition systems propose benchmark suite evaluation metrics reflect effect detection downstream fingerspell recognition task addition propose new model learn detect fingerspell via multi task train incorporate pose estimation fingerspell recognition transcription along detection compare model several alternatives model outperform alternative approach across metrics establish state art benchmark
paper present two approach lindenmayer systems rule base generative approach focus l systems thue rewrite systems constraint base model theoretic approach rule abandon favour condition allowable expressions language pullum two thousand and nineteen argue possible least subset l systems languages generate map string admissibility condition three laws local tree admissibility condition cf rogers one thousand, nine hundred and ninety-seven equivalent define model languages work construct structure assume superficial constraints expressions define set constraints well form expressions specific l languages must satisfy see l systems methods distinguish turn satisfy model
availability open source software play remarkable role automatic speech recognition asr kaldi instance widely use develop state art offline online asr systems paper describe exkaldi rt online asr toolkit implement base kaldi python language exkaldi rt provide tool provide real time audio stream pipeline extract acoustic feature transmit packets remote connection estimate acoustic probabilities neural network online decode similar function available build kaldi key feature exkaldi rt completely work python language easy use interface online asr system developers exploit original research example apply neural network base signal process acoustic model train deep learn frameworks perform benchmark experiment minimum librispeech corpus show exkaldi rt could achieve competitive asr performance real time
propose fly data augmentation method automatic speech recognition asr use alignment information generate effective train sample method call align data augmentation ada asr replace transcribe tokens speech representations align manner generate previously unseen train pair speech representations sample audio dictionary extract train corpus inject speaker variations train examples transcribe tokens either predict language model augment data pair semantically close original data randomly sample strategies result train pair improve robustness asr train experiment seq seq architecture show ada apply top specaugment achieve nine twenty-three four fifteen relative improvements wer specaugment alone librispeech 100h librispeech 960h test datasets respectively
identification categorization social media post generate disasters crucial reduce suffer affect people however lack label data significant bottleneck learn effective categorization system disaster motivate us study problem unsupervised domain adaptation uda previous disaster label data source current disaster target however amount label data available limit restrict learn capabilities model handle challenge utilize limit label data along abundantly available unlabeled data generate source disaster propose novel two part graph neural network first part extract domain agnostic global information construct token level graph across domains second part preserve local instance level semantics experiment show propose method outperform state art techniques two hundred and seventy-four weight f1 score average two standard public dataset area disaster management also report experimental result granular actionable multi label classification datasets disaster domain first time outperform bert three hundred average wrt weight f1 additionally show approach retain performance limit label data available
introduce shoot transfer learn method keyword spot language leverage open speech corpora nine languages automate extraction large multilingual keyword bank use train embed model five train examples fine tune embed model keyword spot achieve average f1 score seventy-five keyword classification one hundred and eighty new keywords unseen embed model nine languages embed model also generalize new languages achieve average f1 score sixty-five five shoot model two hundred and sixty keywords sample across thirteen new languages unseen embed model investigate stream accuracy five shoot model two contexts keyword spot keyword search across four hundred and forty keywords twenty-two languages achieve average stream keyword spot accuracy eight hundred and fifty-two false acceptance rate twelve observe promise initial result keyword search
autoregressive ar model attention base encoder decoder model rnn transducer achieve great success speech recognition predict output sequence condition previous tokens acoustic encode state inefficient gpus non autoregressive nar model get rid temporal dependency output tokens predict entire output tokens least one step however nar model still face two major problems one hand still great gap performance nar model advance ar model hand difficult nar model train converge address two problems propose new model name two step non autoregressive transformertsnat improve performance accelerate convergence nar model learn prior knowledge parameters share ar model furthermore introduce two stage method inference process improve model performance greatly experiment conduct public chinese mandarin dataset asiehll one result show tsnat achieve competitive performance ar model outperform many complicate nar model
paper introduce timers new open source dataset speak english command common voice control use case involve number describe gap exist speak language understand datasets timers fill design creation dataset experiment number asr base end end baseline model code make available part speechbrain toolkit
automatic speech recognition asr technologies today primarily optimize give datasets thus change application environment eg acoustic condition topic domains may inevitably degrade performance collect new data describe new environment fine tune system naturally lead higher error rat earlier datasets refer catastrophic forget concept lifelong learn lll aim enable machine sequentially learn new task new datasets describe change real world without forget previously learn knowledge thus bring attention paper report knowledge first effort extensively consider analyze use various approach lll end end e2e asr include propose novel methods save data past domains mitigate catastrophic forget problem overall relative reduction two hundred and eighty-seven wer achieve compare fine tune baseline sequentially learn three different benchmark corpora first step toward highly desire asr technologies capable synchronize continuously change real world
design conceptually sound metamodels embody proper semantics relation application domain particularly tedious model drive engineer metamodels define complex relationships domain concepts crucial modeler define concepts thoroughly consistent respect application domain propose approach assist modeler design metamodel recommend relevant domain concepts several model scenarios approach require extract knowledge domain hand design completion rule instead design fully data drive approach use deep learn model able abstract domain concepts learn structural lexical metamodel properties corpus thousands independent metamodels evaluate approach test set contain one hundred and sixty-six metamodels unseen model train five thousand test sample preliminary result show train model able provide accurate top five list relevant recommendations concept rename scenarios although promise result less compel scenario iterative construction metamodel part conservative strategy use evaluate recommendations
infer meta information table column headers relationships columns active research topic data management find many table miss information paper study problem annotate table columns ie predict column type relationships columns use information table show multi task learn approach call doduo train use pre train language model task outperform individual learn approach experimental result show doduo establish new state art performance two benchmarks column type prediction column relation prediction task forty one hundred and nineteen improvements respectively also establish doduo already perform previous state art performance minimal number tokens eight tokens per column
significance social media increase manifold past decades help people even remote corner world stay connect advent technology digital media become relevant widely use ever along resurgence circulation fake news tweet demand immediate attention paper describe novel fake news detection system automatically identify whether news item real fake extension work constraint covid nineteen fake news detection english challenge use ensemble model consist pre train model follow statistical feature fusion network along novel heuristic algorithm incorporate various attribute present news items tweet like source username handle url domains author statistical feature propose framework also quantify reliable predictive uncertainty along proper class output confidence level classification task evaluate result covid nineteen fake news dataset fakenewsnet dataset show effectiveness propose algorithm detect fake news short news content well news article obtain best f1 score nine thousand, eight hundred and ninety-two covid nineteen dataset f1 score nine thousand and seventy-three fakenewsnet dataset
often use perturbations regularize neural model neural encoder decoders previous study apply schedule sample bengio et al two thousand and fifteen adversarial perturbations sato et al two thousand and nineteen perturbations methods require considerable computational time thus study address question whether approach efficient enough train time compare several perturbations sequence sequence problems respect computational time experimental result show simple techniques word dropout gal ghahramani two thousand and sixteen random replacement input tokens achieve comparable better score recently propose perturbations even though simple methods faster code publicly available https githubcom takase rethinkperturbations
many neural network speaker recognition systems model speaker use fix dimensional embed vector embeddings generally compare use either linear 2nd order score recently handle utterance specific uncertainty work propose score representations way capture uncertainty enroll test asymmetry additional non linear information achieve incorporate 2nd stage neural network know decision network part end end train regimen particular propose concept decision residual network involve use compact decision network leverage cosine score model residual signal need additionally present modification generalize end end softmax loss function better target separation different speaker score observe significant performance gain two techniques
english speech text stt machine learn task acoustic model conventionally train uncase latin character necessary orthography capitalization punctuation denormalization non standard word impute separate post process model add complexity limit performance many format task benefit semantic information present acoustic signal absent transcription propose new stt task end end neural transcription fully format text target label present baseline conformer base model train corpus five thousand hours professionally transcribe earn call achieve cer seventeen contribution stt research community release corpus free non commercial use https datasetskenshocom datasets scribe
multi talker scenarios meet conversations speech process systems usually require transcribe audio well identify speakers downstream applications since overlap speech common case conventional approach usually address problem cascade fashion involve speech separation speech recognition speaker identification train independently paper propose stream unmixing recognition identification transducer surit new framework deal problem end end stream fashion surit employ recurrent neural network transducer rnn backbone speech recognition speaker identification validate idea librispeechmix dataset multi talker dataset derive librispeech present encourage result
reason information multiple part passage derive answer open challenge read comprehension model paper present approach reason complex question decompose simpler subquestions take advantage single span extraction read comprehension model derive final answer accord instructions predefined reason template focus subtraction base arithmetic question evaluate approach subset drop dataset show approach competitive state art interpretable require little supervision
present speechstew speech recognition model train combination various publicly available speech recognition datasets ami broadcast news common voice librispeech switchboard fisher tedlium wall street journal speechstew simply mix datasets together without special weight balance datasets speechstew achieve sota near sota result across variety task without use external language model result include ninety wer ami ihm forty-seven wer switchboard eighty-three wer callhome thirteen wsj significantly outperform prior work strong external language model also demonstrate speechstew learn powerful transfer learn representations fine tune speechstew noisy low resource speech dataset chime six achieve three hundred and eighty-nine wer without language model compare three hundred and eighty-six wer strong hmm baseline language model
commonsense knowledge acquisition reason long core artificial intelligence problem however past lack scalable methods collect commonsense knowledge paper propose develop principles collect commonsense knowledge base selectional preference generalize definition selectional preference one hop linguistic syntactic relations higher order relations linguistic graph unlike previous commonsense knowledge definition eg conceptnet selectional preference sp knowledge rely statistical distribution linguistic graph efficiently accurately acquire unlabeled corpus modern tool follow principle develop large scale eventuality linguistic term cover activity state event base knowledge graph aser eventuality represent dependency graph relation discourse relation define shallow discourse parse higher order selectional preference collect linguistic graph reflect various kinds commonsense knowledge moreover motivate observation humans understand events abstract observe events higher level thus transfer knowledge new events propose conceptualization module significantly boost coverage aser total aser contain four hundred and thirty-eight million eventualities six hundred and forty-eight million edge eventualities conceptualization probase selectional preference base concept instance relational knowledge base concept graph contain fifteen million conceptualize eventualities two hundred and twenty-four million edge detail analysis provide demonstrate quality collect data apis tool available https githubcom hkust knowcomp aser
recent years witness prosperity legal artificial intelligence development technologies paper propose novel legal application legal provision prediction lpp aim predict relate legal provision affairs formulate task challenge knowledge graph completion problem require text understand also graph reason end propose novel text guide graph reason approach collect amount real world legal provision data guangdong government service website construct legal dataset call legallpp extensive experimental result dataset show approach achieve better performance compare baselines code dataset available urlhttps githubcom zjunlp legalpp reproducibility
numerous new dialog domains create every day collect data domains extremely costly since involve human interactions therefore essential develop algorithms adapt different domains efficiently build data drive dialog model recent research domain adaption focus give model better initialization rather optimize adaptation process propose efficient domain adaptive task orient dialog system model incorporate meta teacher model emphasize different impact generate tokens respect context first train base dialog model meta teacher model adversarially meta learn set rich resource domains meta teacher learn quantify importance tokens different contexts across different domains adaptation meta teacher guide dialog model focus important tokens order achieve better adaptation efficiency evaluate model two multi domain datasets multiwoz google schema guide dialogue achieve state art performance
important understand advertise comedies dog whistle politics however computational research hinder lack available datasets paper propose large diverse chinese dataset create understand computational linguistics perspective formulate task understand provide quantitative qualitative analysis test word embed similarity pretrained language model experiment suggest task require deep language understand common sense world knowledge thus good testbed pretrained language model help model perform better task code available https githubcom jetrunner dogwhistle data leaderboard available https competitionscodalaborg competitions thirty thousand, four hundred and fifty-one
word vector embeddings show contain amplify bias data extract consequently many techniques propose identify mitigate attenuate bias word representations paper utilize interactive visualization increase interpretability accessibility collection state art debiasing techniques aid present visualization embed representations debiasing system verb open source web base visualization tool help users gain technical understand visual intuition inner work debiasing techniques focus geometric properties particular verb offer easy follow use case explore effect debiasing techniques geometry high dimensional word vectors help understand various debiasing techniques change underlie geometry verb decompose technique interpretable sequence primitive transformations highlight effect word vectors use dimensionality reduction interactive visual exploration verb design target natural language process nlp practitioners design decision make systems top word embeddings also researchers work fairness ethics machine learn systems nlp also serve visual medium education help nlp novice understand mitigate bias word embeddings
paper describe approach toxic span detection problem semeval two thousand and twenty-one task five propose bertoxic system fine tune pre train bert model locate toxic text span give text utilize additional post process step refine boundaries post process step involve one label character offset consecutive toxic tokens toxic two assign toxic label word least one token label toxic experiment show two post process step improve performance model four hundred and sixteen test set also study effect data augmentation ensemble model strategies system system significantly outperform provide baseline achieve f1 score six hundred and eighty-three place lone pine 17th place ninety-one team competition code make available https githubcom yakoob khan toxic span detection
paper describe acquisition preprocessing segmentation alignment amharic english parallel corpus useful machine translation resourced language amharic corpus larger previously compile corpora release research purpose train neural machine translation phrase base statistical machine translation model use corpus automatic evaluation neural machine translation model outperform phrase base statistical machine translation model
powerful sentence encoders train multiple languages rise systems capable embed wide range linguistic properties vector representations explicit probe task use verify presence specific linguistic properties unclear whether vector representations manipulate indirectly steer properties investigate use geometric map embed space transform linguistic properties without tune pre train sentence encoder decoder validate approach three linguistic properties use pre train multilingual autoencoder analyze result monolingual cross lingual settings
paper present favalon functional program language build premise lambda calculus use interactive replacement favalon seamlessly integrate type versions exist libraries command use type inference flexible runtime type metadata techniques employ shell link command together much favalon syntax customizable via user define function allow extend anyone familiar command line furthermore favalon type inference engine separate runtime library easily repurposed applications
video question answer vidqa evaluation metrics limit single word answer select phrase fix set phrase metrics limit vidqa model application scenario work leverage semantic roles derive video descriptions mask certain phrase introduce vidqap pose vidqa fill phrase task enable evaluation answer phrase compute relative improvement predict answer compare empty string reduce influence language bias vidqa datasets retrieve video different answer question facilitate research construct activitynet srl qa charades srl qa benchmark extend three vision language model perform extensive analysis ablative study guide future work
metaphors widely use political rhetoric effective frame device efficacy specific metaphors war metaphor political discourse document study often rely small number hand cod instance metaphor use larger scale topic agnostic study require establish general persuasiveness metaphors device would light broader pattern guide persuasiveness paper present large scale data drive study metaphors use political discourse conduct study publicly available dataset 85k post make four hundred and twelve us politicians facebook public page feb two thousand and seventeen contributions threefold show evidence metaphor use correlate ideological lean complex ways depend concurrent political events win lose elections show post metaphors elicit engagement audience overall even control various socio political factor gender political party affiliation finally demonstrate metaphoricity indeed reason increase engagement post fine grain linguistic analysis metaphorical vs literal usages five hundred and thirteen word across 70k post
purpose study present herein develop machine learn algorithm base natural language process automatically detect whether patient cardiac failure healthy condition use physician note research data warehouse chu sainte justine hospital first word representation learn technique employ use bag word bow term frequency inverse document frequency tfidf neural word embeddings word2vec representation technique aim retain word semantic syntactic analysis critical care data help enrich mutual information word representation lead advantage appropriate analysis step second machine learn classifier use detect patients condition either cardiac failure stable patient create word representation vector space previous step machine learn approach base supervise binary classification algorithm include logistic regression lr gaussian naive bay gaussiannb multilayer perceptron neural network mlpnn technically mainly optimize empirical loss train classifiers result automatic learn algorithm would accomplish draw high classification performance include accuracy acc precision pre recall rec f1 score f1 result show combination tfidf mlpnn always outperform combinations overall performance case without feature selection propose framework yield overall classification performance acc pre rec f1 eighty-four eighty-two eighty-five eighty-three respectively significantly feature selection well apply overall performance would finally improve four evaluation
paper propose joint clinical natural language representation learn supervise classification framework base machine learn detect concept label clinical narratives chu sainte justine hospital chusj novel framework jointly discover distributional syntactic latent semantic representation learn contextual clinical narrative input learn knowledge representation label contextual output supervise classification first effective representation learn approach small data set mix numeric value texts four different methods apply capture numerical vital sign value different representation learn approach use discover rich structure clinical narrative data second automatic encounter disease prediction case cardiac failure binary classifiers iteratively train learn knowledge representation process data precede step multilayer perceptron neural network outperform discriminative generative classifiers consequently propose framework yield overall classification performance accuracy recall precision eighty-nine eighty-eight eighty-nine respectively furthermore generative autoencoder ae learn algorithm propose leverage sparsity reduction affirmatively ae algorithm overperforming sparsity reduction techniques classifier performances successfully achieve ninety-one ninety-one ninety-one respectively accuracy recall precision
commit message important impact software development especially work large team multiple developers different style write may often involve project reason may difficult maintain strict pattern write informative commit message frequent issue message descriptive enough paper apply neural machine translation nmt techniques convert code diffs commit message present improve sketch base encoder task split approach three part firstly focus find suitable nmt baseline problem secondly show performance nmt model improve train examples contain specific file type lastly introduce novel sketch base neural model inspire recent approach use code generation show sketch base encoder significantly outperform exist state art solutions result highlight improvement relevant especially java source code file examine two different datasets introduce recent years task
take first step towards multilingual style transfer create release xformal benchmark multiple formal reformulations informal text brazilian portuguese french italian result xformal suggest state art style transfer approach perform close simple baselines indicate style transfer even challenge move multilingual
widespread adoption deep model motivate press need approach interpret network output facilitate model debug instance attribution methods constitute one mean accomplish goals retrieve train instance may lead particular prediction influence function koh liang two thousand and seventeen provide machinery quantify effect perturb individual train instance would specific test prediction however even approximate computationally expensive degree may prohibitive many case might simpler approach eg retrieve train examples similar give test point perform comparably work evaluate degree different potential instance attribution agree respect importance train sample find simple retrieval methods yield train instance differ identify via gradient base methods ifs nonetheless exhibit desirable characteristics similar complex attribution methods code methods experiment paper available https githubcom successar instanceattributionsnlp
vision language navigation vln require agent navigate remote location basis natural language instructions set photo realistic panoramas exist methods take word instructions discrete view panorama minimal unit encode however require model match different textual landmarks instructions eg tv table view feature work propose object inform sequential bert encode visual perceptions linguistic instructions fine grain level namely object word facilitate match visual textual entities hence know sequential bert enable visual textual clue interpret light temporal context crucial multi round vln task additionally enable model identify relative direction eg leave right front back navigable location room type eg bedroom kitchen current final navigation goal namely know information widely mention instructions imply desire next final locations extensive experiment demonstrate effectiveness compare several state art methods three indoor vln task reverie ndh r2r
paper propose automatic chinese text categorization method solve emergency event report classification problem since bidirectional encoder representations transformers bert achieve great success natural language process domain employ derive emergency text feature study overcome data imbalance problem distribution emergency event categories novel loss function propose improve performance bert base model meanwhile avoid impact extreme learn rate adabound optimization algorithm achieve gradual smooth transition adam sgd employ learn parameters model verify feasibility effectiveness propose method chinese emergency text dataset collect internet employ compare benchmarking methods propose method achieve best performance term accuracy weight precision weight recall weight f1 value therefore promise employ propose method real applications smart emergency management systems
investigate video aid grammar induction learn constituency parser unlabeled text correspond video exist methods multi modal grammar induction focus learn syntactic grammars text image pair promise result show information static image useful induction however videos provide even richer information include static object also action state change useful induce verb phrase paper explore rich feature eg action object scene audio face ocr speech videos take recent compound pcfg model baseline propose multi modal compound pcfg model mmc pcfg effectively aggregate rich feature different modalities propose mmc pcfg train end end outperform individual modality previous state art systems three benchmarks ie didemo youcook2 msrvtt confirm effectiveness leverage video information unsupervised grammar induction
dialogue successful alignment speakers different linguistic level work consider dialogue occur interlocutors engage collaborative learn task explore performance learn ie task success relate dialogue alignment process main contribution work propose new measure automatically study alignment consider completely spontaneous speak dialogues among children context collaborative learn activity measure alignment consider children use expressions relate task hand follow action expressions link task success focus expressions relate task give us insight way children use potentially unfamiliar terminology relate task first find work discovery measure propose capture elements lexical alignment context measure find team bad performance often align late dialogue achieve task success late follow instructions action also find interlocutors exhibit hesitation phenomena measure look fillers introduce expressions pertain task exhibit hesitation accept expression role clarification lastly show information management markers measure discourse marker oh occur general vicinity follow action automatically infer instructions however good performers tend marker closer action measure still reflect fine grain aspects learn dialogue even conclude overall link final measure learn
dialogue state track crucial part multi domain task orient dialogue systems responsible extract information user utterances present novel architecture utilize powerful generative model gpt two generate slot value one one causally time utilize graph attention network enable inter slot information exchange exploit inter slot relations correlations model achieve five thousand, four hundred and eighty-six joint accuracy multiwoz twenty retain performance five thousand and forty-three sparse supervision train session level annotations one hundred and forty-three full train set use conduct detail analyse demonstrate significance use graph model task show experiment propose graph modules indeed help capture inter slot relations
large language model lead state art accuracies across range task however train large model efficiently challenge two reason gpu memory capacity limit make impossible fit large model single gpu even multi gpu server b number compute operations require train model result unrealistically long train time new methods model parallelism tensor pipeline parallelism propose address challenge unfortunately naive usage lead fundamental scale issue thousands gpus due various reason eg expensive cross node communication idle periods wait devices work show compose different type parallelism methods tensor pipeline data parallelism scale thousands gpus achieve two order magnitude increase size model efficiently train compare exist systems survey techniques pipeline parallelism propose novel interleave pipeline parallelism schedule improve throughput ten comparable memory footprint compare previously propose approach quantitatively study trade off tensor pipeline data parallelism provide intuition configure distribute train large model approach allow us perform train iterations model one trillion parameters five hundred and two petaflop three thousand and seventy-two gpus achieve per gpu throughput fifty-two peak previous efforts train similar size model achieve much lower throughput thirty-six theoretical peak code open source https githubcom nvidia megatron lm
chinese character decomposition use feature enhance machine translation mt model combine radicals character word level model recent work investigate ideograph stroke level embed however question remain different decomposition level chinese character representations radical stroke best suit mt investigate impact chinese decomposition embed detail ie radical stroke intermediate level well decompositions represent mean original character sequence carry analysis automate human evaluation mt furthermore investigate combination decompose multiword expressions mwes enhance model learn mwe integration mt see decade exploration however decompose mwes previously explore
emotion recognition conversations important step various virtual chat bots require opinion base feedback like social media thread online support many applications current emotion recognition conversations model face issue like loss contextual information two dialogues conversation b failure give appropriate importance significant tokens utterance c inability pass emotional information previous utterancesthe propose model advance contextual feature extraction adcofe address issue perform unique feature extraction use knowledge graph sentiment lexicons phrase natural language level word position embed utterances experiment emotion recognition conversations dataset show adcofe beneficial capture emotions conversations
large pretrained language model like gpt three acquire surprise ability perform zero shoot classification zsc example classify review sentiments prompt language model review question review positive context ask predict whether next word yes however model specialize answer prompt address weakness propose meta tune train model specialize answer prompt still generalize unseen task create train data aggregate forty-three exist datasets annotate four hundred and forty-one label descriptions total unify question answer qa format meta tune model outperform size qa model label unseen task forecast performance would improve even larger model therefore measure zsc performance non specialize language model might underestimate true capability community wide efforts aggregate datasets unify format help build model understand prompt better
give database schema text sql aim translate natural language question correspond sql query setup cross domain traditional semantic parse model struggle adapt unseen database schemas improve model generalization capability rare unseen schemas propose new architecture shadowgnn process schemas abstract semantic level ignore name semantic items databases abstract schemas exploit well design graph projection neural network obtain delexicalized representation question schema base domain independent representations relation aware transformer utilize extract logical link question schema finally sql decoder context free grammar apply challenge text sql benchmark spider empirical result show shadowgnn outperform state art model annotate data extremely limit ten train set shadowgnn get absolute five performance gain show powerful generalization ability implementation open source urlhttps githubcom wowcz shadowgnn
meta learn learn learn technique help overcome resource scarcity cross lingual nlp problems enable fast adaptation new task apply model agnostic meta learn maml task cross lingual dependency parse train model diverse set languages learn parameter initialization adapt quickly new languages find meta learn pre train significantly improve upon performance language transfer standard supervise learn baselines variety unseen typologically diverse low resource languages shoot learn setup
paper introduce fresada french satire data set compose eleven thousand, five hundred and seventy article news domain order avoid report unreasonably high accuracy rat due learn characteristics specific publication source divide sample train validation test train publication source distinct validation test publication source give rise cross domain cross source satire detection task employ two classification methods baselines new data set one base low level feature character n grams one base high level feature average camembert word embeddings additional contribution present unsupervised domain adaptation method base regard pairwise similarities give dot product train sample validation sample feature include domain specific feature attain significant improvements character n grams camembert embeddings
keyword extraction call identify word phrase express main concepts texts best huge amount texts create every day time electronic infrastructure practically impossible humans study manage volume document however need efficient effective access document evident various purpose weblogs news technical note almost long texts reader seek understand concepts topics keywords decide read full text aim use combine approach consist two model graph centrality feature textural feature follow graph centralities degree betweenness eigenvector closeness centrality use optimally combine extract best keyword among candidate keywords extract propose method also another approach introduce distinguish keywords among candidate phrase consider separate keyword evaluate propose method seven datasets name semeval2010 semeval2017 inspec fao30 thesis100 pak2018 wikinews use result report precision recall f measure
document describe pragmatic approach migrate organisation computer system towards new system could evolve forever address whole organisation integrate governance aspects important purely technical aspects human resources call tender similar migration imply one start green field
offensive language pervasive social media individuals frequently take advantage perceive anonymity computer mediate communication use engage behavior many would consider real life automatic identification offensive content online important task gain attention recent years task model supervise classification problem systems train use dataset contain post annotate respect presence form abusive offensive content objective study provide description classification system build semeval two thousand and nineteen task six offenseval system classify tweet either offensive offensive sub task classify offensive tweet categories sub task b c train machine learn deep learn model along data preprocessing sample techniques come best result model discuss include naive bay svm logistic regression random forest lstm
adversarial train show improve generalization performance deep learn model various natural language process task exist work usually formulate adversarial train zero sum game solve alternate gradient descent ascent algorithms formulation treat adversarial defend players equally undesirable defend player contribute generalization performance address issue propose stackelberg adversarial train salt formulate adversarial train stackelberg game formulation induce competition leader follower follower generate perturbations leader train model subject perturbations different conventional adversarial train salt leader advantageous position leader move recognize strategy follower take anticipate follower outcomes consideration leader advantage enable us improve model fit unperturbed data leader strategic information capture stackelberg gradient obtain use unroll algorithm experimental result set machine translation natural language understand task show salt outperform exist adversarial train baselines across task
although self supervise pre train transformer model result revolutionize natural language process nlp applications achievement state art result regard various benchmarks process still vulnerable small imperceptible permutations originate legitimate input intuitively representations similar feature space subtle input permutations large variations occur different mean motivate us investigate learn robust textual representation contrastive manner however non trivial obtain oppose semantic instance textual sample study propose disentangle contrastive learn method separately optimize uniformity alignment representations without negative sample specifically introduce concept momentum representation consistency align feature leverage power normalization conform uniformity experimental result nlp benchmarks demonstrate approach obtain better result compare baselines well achieve promise improvements invariance test adversarial attack code available https githubcom zjunlp dcl
knowledge graph suffer sparsity degrade quality representations generate various methods abundance textual information throughout web many exist knowledge base align information across diverse data source remain challenge literature previous work partially address issue enrich knowledge graph entities base hard co occurrence word present entities knowledge graph external text achieve soft augmentation propose knowledge graph enrichment embed framework name edge give original knowledge graph first generate rich noisy augment graph use external texts semantic structural level distill relevant knowledge suppress introduce noise design graph alignment term share embed space original graph augment graph enhance embed learn augment graph regularize locality relationship target entity base negative sample experimental result four benchmark datasets demonstrate robustness effectiveness edge link prediction node classification
semantic parse use sequence sequence model allow parse deeper representations compare traditional word tag base model spite advantage widespread adoption model real time conversational use case stymie higher compute requirements thus higher latency work propose non autoregressive approach predict semantic parse tree efficient seq2seq model architecture combine non autoregressive prediction convolutional neural network achieve significant latency gain parameter size reduction compare traditional rnn model novel architecture achieve eighty-one reduction latency top dataset retain competitive performance non pretrained model three different semantic parse datasets code available https githubcom facebookresearch pytext
semantic role label srl aim extract arguments predicate input sentence traditional srl fail analyze dialogues work every single sentence ellipsis anaphora frequently occur dialogues address problem propose conversational srl task argument dialogue participants phrase dialogue history current sentence exist srl datasets sentence level manually annotate semantic roles three thousand chit chat dialogues twenty-seven thousand, one hundred and ninety-eight sentence boost research direction experiment show traditional srl systems even help coreference resolution rewrite perform poorly analyze dialogues model dialogue histories participants greatly help performance indicate adapt srl conversations promise universal dialogue understand initial study apply csrl two mainstream conversational task dialogue response generation dialogue context rewrite also confirm usefulness csrl
recursive neural network rvnn show useful learn sentence representations help achieve competitive performance several natural language inference task however recent rvnn base model fail learn simple grammar meaningful semantics intermediate tree representation work propose attention mechanism tree lstms learn meaningful explainable parse tree structure also demonstrate superior performance propose model natural language inference semantic relatedness sentiment analysis task compare state art rvnn base methods present detail qualitative quantitative analysis learn parse tree show discover linguistic structure explainable semantically meaningful grammatically correct recent approach source code paper available https githubcom atul04 explainable latent structure use attention
new word regularly introduce communities yet word persist community lexicon among many factor contribute lexical change focus understudy effect social network conduct large scale analysis 80k neologisms four thousand, four hundred and twenty online communities across decade use poisson regression survival analysis study demonstrate community network structure play significant role lexical change apart overall size properties include dense connections lack local cluster external contact promote lexical innovation retention unlike offline communities topic base communities experience strong lexical level despite increase contact accommodate niche word work provide support sociolinguistic hypothesis lexical change partially shape structure underlie network also uncover find specific online communities
discrete adversarial attack symbolic perturbations language input preserve output label lead prediction error attack extensively explore purpose evaluate model robustness utility improve robustness limit offline augmentation ie give train model attack use generate perturb adversarial examples model train exactly work address gap leverage discrete attack online augmentation adversarial examples generate every step adapt change nature model also consider efficient attack base random sample unlike prior work base expensive search base procedures second contribution provide general formulation multiple search base attack past work propose new attack base best first search surprisingly find random sample lead impressive gain robustness outperform commonly use offline augmentation lead speedup train time 10x furthermore online augmentation search base attack justify higher train cost significantly improve robustness three datasets last show propose algorithm substantially improve robustness compare prior methods
contrastive learn emerge powerful representation learn method facilitate various downstream task especially supervise data limit construct efficient contrastive sample data augmentation key success unlike vision task data augmentation method contrastive learn investigate sufficiently language task paper propose novel approach construct contrastive sample language task use text summarization use sample supervise contrastive learn gain better text representations greatly benefit text classification task limit annotations improve method mix sample different class add extra regularization name mix sum regularization addition cross entropy loss experiment real world text classification datasets amazon five yelp five ag news demonstrate effectiveness propose contrastive learn framework summarization base data augmentation mix sum regularization
answer selection involve many natural language process applications dialog systems question answer qa important yet challenge task practice since conventional methods typically suffer issue ignore diverse real world background knowledge paper extensively investigate approach enhance answer selection model external knowledge knowledge graph kg first present context knowledge interaction learn framework knowledge aware neural network knn learn qa sentence representations consider tight interaction external knowledge kg textual information develop two kinds knowledge aware attention mechanism summarize context base knowledge base interactions question answer handle diversity complexity kg information propose contextualized knowledge aware attentive neural network ckann improve knowledge representation learn structure information via customize graph convolutional network gcn comprehensively learn context base knowledge base sentence representation via multi view knowledge aware attention mechanism evaluate method four widely use benchmark qa datasets include wikiqa trec qa insuranceqa yahoo qa result verify benefit incorporate external knowledge kg show robust superiority extensive applicability method
propose future discriminators generation fudge flexible modular method control text generation give pre exist model g generate text distribution interest fudge enable condition desire attribute example formality require access g output logits fudge learn attribute predictor operate partial sequence use predictor output adjust g original probabilities show fudge model term correspond bayesian decomposition conditional distribution g give attribute moreover fudge easily compose predictors multiple desire attribute evaluate fudge three task couplet completion poetry topic control language generation formality change machine translation observe gain three task
current state art model hiagm hierarchical text classification two limitations first correlate text sample label dataset contain irrelevant information second consider statistical constraint label representations learn structure encoder constraints representation learn prove helpful previous work paper propose htcinfomax address issue introduce information maximization include two modules text label mutual information maximization label prior match first module model interaction text sample grind truth label explicitly filter irrelevant information second one encourage structure encoder learn better representations desire characteristics label better handle label imbalance hierarchical text classification experimental result two benchmark datasets demonstrate effectiveness propose htcinfomax
paper introduce unifiedm2 general purpose misinformation model jointly model multiple domains misinformation single unify setup model train handle four task detect news bias clickbait fake news verify rumor group task together unifiedm2learns richer representation misinformation lead state art comparable performance across task furthermore demonstrate unifiedm2 learn representation helpful shoot learn unseen misinformation task datasets model generalizability unseen events
show name entity recognition ner could benefit incorporate long distance structure information capture dependency tree believe type feature contextual information capture linear sequence structure information capture dependency tree may complement however exist approach largely focus stack lstm graph neural network graph convolutional network gcns build improve ner model exact interaction mechanism two type feature clear performance gain appear significant work propose simple robust solution incorporate type feature synergized lstm syn lstm clearly capture two type feature interact conduct extensive experiment several standard datasets across four languages result demonstrate propose model achieve better performance previous approach require fewer parameters analysis demonstrate model capture longer dependencies compare strong baselines
beam search go method decode auto regressive machine translation model yield consistent improvements term bleu concern find output high model likelihood thus agnostic whatever end metric score practitioners care aim establish whether beam search replace powerful metric drive search technique end explore numerous decode algorithms include rely value function parameterised neural network report result variety metrics notably introduce monte carlo tree search mcts base method showcase competitiveness provide blueprint use mcts fruitfully language applications open promise future directions find algorithm best heavily depend characteristics goal metric believe extensive experiment analysis inform research area
recent publications automatic speech recognition asr strong focus attention encoder decoder aed architectures work well large datasets tend overfit apply low resource scenarios one solution tackle issue generate synthetic data train text speech system tts additional text available successfully apply many publications aed systems present novel approach silence correction data pre process tts systems increase robustness train corpora target asr applications work show successful application synthetic data aed systems also test method highly optimize state art hybrid asr system competitive monophone base system use connectionist temporal classification ctc show later systems addition synthetic data minor effect still outperform aed systems large margin librispeech 100h achieve final word error rate thirty-three one hundred hybrid system clean noisy test set surpass previous state art systems include unlabeled audio data
global spread covid nineteen cause pandemics widely discuss evident large number scientific article amount user generate content social media paper aim compare academic communication social communication pandemic perspective communication preference differences aim provide information ongoing research global pandemics thereby eliminate knowledge barriers information inequalities academic social communities first collect full text metadata pandemic relate article twitter data mention article second extract analyze topics sentiment tendencies article relate tweet finally conduct pandemic relate differential analysis academic community social community mine result data generate pandemic communication preferences eg information need attitude tendencies researchers public respectively research result fifty thousand, three hundred and thirty-eight article nine hundred and twenty-seven thousand, two hundred and sixty-six correspond tweet mention article reveal communication differences global pandemics academic social communities regard consistency research recognition preferences particular research topics analysis large scale pandemic relate tweet also confirm communication preference differences two communities
continual learn become increasingly important enable nlp model constantly learn gain knowledge time previous continual learn methods mainly design preserve knowledge previous task without much emphasis well generalize model new task work propose information disentanglement base regularization method continual learn text classification propose method first disentangle text hide space representations generic task representations specific individual task regularize representations differently better constrain knowledge require generalize also introduce two simple auxiliary task next sentence prediction task id prediction learn better generic specific representation space experiment conduct large scale benchmarks demonstrate effectiveness method continual text classification task various sequence lengths state art baselines publicly release code https githubcom gt salt idbr
describe straight forward approach task five six two thousand and twenty-one social media mine health applications smm4h share task system base fine tune distill bert task well first fine tune model task explore much fine tune necessary accurately classify tweet contain self report covid nineteen symptoms task five whether tweet relate covid nineteen self report non personal report literature news mention virus task six
e commerce company large product selections organization group products meaningful ways important create great customer shop experience cultivate authoritative brand image one important way group products identify family product variants variants mostly slight yet distinct differences eg color pack size paper introduce novel approach identify product variants combine constrain cluster tailor nlp techniques eg extraction product family name unstructured product title identification products similar model number achieve superior performance compare exist baseline use vanilla classification approach addition design algorithm meet certain business criteria include meet high accuracy requirements wide range categories eg appliances decor tool build materials etc well prioritize interpretability model make accessible understandable business partner
state art deep neural network require large scale label train data often expensive obtain available many task weak supervision form domain specific rule show useful settings automatically generate weakly label train data however learn weak rule challenge due inherent heuristic noisy nature additional challenge rule coverage overlap prior work weak supervision consider instance cover weak rule thus leave valuable unlabeled data behind work develop weak supervision framework astra leverage available data give task end leverage task specific unlabeled data self train model student consider contextualized representations predict pseudo label instance may cover weak rule develop rule attention network teacher learn aggregate student pseudo label weak rule label condition fidelity underlie context instance finally construct semi supervise learn objective end end train unlabeled data domain specific rule small amount label data extensive experiment six benchmark datasets text classification demonstrate effectiveness approach significant improvements state art baselines
unesco world heritage list whl identify exceptionally valuable cultural natural heritage preserve mankind whole evaluate justify outstanding universal value ouv nomination whl essentially important property inscribe yet complex task even experts since criteria mutually exclusive furthermore manual annotation heritage value currently dominant field knowledge demand time consume impede systematic analysis authoritative document term implications heritage management study apply state art nlp model build classifier new real world dataset contain official ouv justification statements seek explainable scalable less bias automation tool facilitate nomination evaluation monitor process world heritage properties label smooth innovatively adapt transform task smoothly multi class multi label classification add prior inter class relationship knowledge label improve performance baselines study show best model fine tune bert ulmfit reach nine hundred and forty-three top three accuracy promise develop apply heritage research practice
translate text language unknown text author dub outbound translation modern need user experience significant room improvement beyond basic machine translation facility demonstrate show three ways user confidence outbound translation well overall final quality affect backward translation quality estimation alignment source paraphrase paper describe experiment outbound translation english czech estonian examine effect propose feedback module focus quality machine translation systems influence find user perception success show backward translation feedback mix effect whole process increase user confidence produce translation objective quality
text retrieval use learn dense representations recently emerge promise alternative traditional text retrieval use sparse bag word representations one recent work garner much attention dense passage retriever dpr technique propose karpukhin et al two thousand and twenty end end open domain question answer present replication study work start model checkpoints provide author otherwise independent implementation group pyserini ir toolkit pygaggle neural text rank library although experimental result largely verify claim original paper arrive two important additional find contribute better understand dpr first appear original author report effectiveness bm25 baseline hence also dense sparse hybrid retrieval result second incorporate evidence retriever improve answer span score technique able improve end end question answer effectiveness use exactly model original work
machine learn ml widely use automatically extract meaningful information electronic health record ehr support operational clinical financial decision make however ml model require large number annotate examples provide satisfactory result possible healthcare scenarios due high cost clinician label data active learn al process select informative instance label expert train supervise algorithm demonstrate effectiveness al multi label text classification clinical domain context apply set well know al methods help automatically assign icd nine cod mimic iii dataset result show selection informative instance provide satisfactory classification significantly reduce train set eighty-three total instance conclude al methods significantly reduce manual annotation cost preserve model performance
major advancement performance machine translation model make possible part thank availability large scale parallel corpora languages world existence corpora rare emakhuwa language speak mozambique like african languages low resource nlp term lack computational linguistic resources best knowledge parallel corpora include emakhuwa already exist paper describe creation emakhuwa portuguese parallel corpus collection texts jehovah witness website variety source include african story book website universal declaration human right mozambican legal document dataset contain forty-seven thousand, four hundred and fifteen sentence pair amount six hundred and ninety-nine thousand, nine hundred and seventy-six word tokens emakhuwa eight hundred and seventy-seven thousand, five hundred and ninety-five word tokens portuguese normalization process remain complete corpus make freely available research use
recent advance open domain story generation lack reliable automatic evaluation metrics become increasingly imperative issue hinder fast development story generation accord conduct research regard learnable evaluation metrics promise accurate assessments higher correlations human judgments critical bottleneck obtain reliable learnable evaluation metric lack high quality train data classifiers efficiently distinguish plausible implausible machine generate stories previous work rely textitheuristically manipulate plausible examples mimic possible system drawbacks repetition contradiction irrelevant content text level textitunnatural textitoversimplify characteristics implausible machine generate stories propose tackle issue generate comprehensive set implausible stories use plot structure representations controllable factor use generate stories since plot compact structure easier manipulate generate text target undesirable properties time maintain grammatical correctness naturalness generate sentence improve quality generate implausible stories apply adversarial filter procedure present citetzellers2018swag select nuanced set implausible texts experiment show evaluation metrics train generate data result reliable automatic assessments correlate remarkably better human judgments compare baselines
paper propose question answer qa benchmark spatial reason natural language text contain realistic spatial phenomena cover prior work challenge state art language model lm propose distant supervision method improve task specifically design grammar reason rule automatically generate spatial description visual scenes correspond qa pair experiment show pretraining lms automatically generate data significantly improve lms capability spatial understand turn help better solve two external datasets babi boolq hope work foster investigations sophisticate model spatial reason text
translate close text know advance severely low resource language leverage massive source parallelism word give text one hundred and twenty-four source languages translate severely low resource language use one thousand line low resource data without external help firstly propose systematic method rank choose source languages close low resource language call linguistic definition language family family origin famo call empirical definition higher rank languages use metrics family choice famc secondly build iteratively pretrained multilingual order preserve lexiconized transformer ipml train one thousand line thirty-five low resource data translate name entities correctly build massive lexicon table two thousand, nine hundred and thirty-nine bible name entities one hundred and twenty-four source languages include many occur cover sixty-six severely low resource languages moreover also build novel method combine translations different source languages one use english hypothetical low resource language get two hundred and thirty-nine bleu increase multilingual baseline one hundred and three bleu increase asymmetric baseline bible dataset get four hundred and twenty-eight bleu score portuguese english translation medical emea dataset also good result real severely low resource mayan language eastern pokomchi
languages powerful solutions coordination problems provide stable share expectations word say correspond beliefs intentions head yet language use variable non stationary social environment require linguistic representations flexible old word acquire new ad hoc partner specific mean fly paper introduce hierarchical bayesian theory convention formation aim reconcile long stand tension two basic observations specifically argue central computational problem communication simply transmission classical formulations learn adaptation multiple timescales account rapid learn within dyadic interactions allow coordination partner specific common grind social conventions stable priors abstract away interactions multiple partner present new empirical data alongside simulations show model provide cognitive foundation explain several phenomena pose challenge previous account one convergence efficient refer expressions across repeat interaction partner two gradual transfer partner specific common grind novel partner three influence communicative context conventions eventually form
threat online misinformation hard overestimate adversaries rely range tool cheap fake sophisticate deep fake motivate threat scenario image use context support certain narrative express caption prior datasets detect image text inconsistency solve blind model due linguistic cue introduce text manipulation propose dataset image text unmanipulated mismatch introduce several strategies automatic retrieval suitable image give caption capture case relate semantics inconsistent entities well match entities inconsistent semantic context large scale automatically generate newsclippings dataset require model jointly analyze modalities reason entity mismatch well semantic mismatch text image news media
exponential rise online social media enable creation distribution consumption information unprecedented rate however also lead burgeon various form online abuse increase case online antisemitism become one major concern socio political consequences unlike major form online abuse like racism sexism etc online antisemitism study much machine learn perspective best knowledge present first work direction automate multimodal detection online antisemitism task pose multiple challenge include extract signal across multiple modalities contextual reference handle multiple aspects antisemitism unfortunately exist publicly available benchmark corpus critical task hence collect label two datasets three thousand, one hundred and two three thousand, five hundred and nine social media post twitter gab respectively present multimodal deep learn system detect presence antisemitic content specific antisemitism category use text image post perform extensive set experiment two datasets evaluate efficacy propose system finally also present qualitative analysis study
understand voluminous historical record provide clue past various aspects social political issue even natural science facts however generally difficult fully utilize historical record since document write modern language part content damage time result restore damage unrecognizable part well translate record modern languages crucial task response present multi task learn approach restore translate historical document base self attention mechanism specifically utilize two korean historical record ones voluminous historical record world experimental result show approach significantly improve accuracy translation task baselines without multi task learn addition present depth exploratory analysis translate result via topic model uncover several significant historical events
work address issue miss modalities arise visual question answer difference prediction task find novel method solve task hand address miss modality grind truth answer present test time use privilege knowledge distillation scheme deal issue miss modality order efficiently first introduce model big teacher take image question answer triplet input outperform baseline use combination model distill knowledge target network student take image question pair input experiment model vizwiz vqa v2 answer difference datasets show extensive experimentation ablation performances method diverse possibility future research
paper propose novel graph learn framework phrase ground image develop sequential dense graph model exist work capture coarse grain context fail distinguish diversity context among phrase image regions contrast pay special attention different motifs imply context scene graph devise disentangle graph network integrate motif aware contextual information representations besides adopt interventional strategies feature structure level consolidate generalize representations finally cross modal attention network utilize fuse intra modal feature phrase compute similarity regions select best ground one validate efficiency disentangle interventional graph network dign series ablation study model achieve state art performance flickr30k entities referit game benchmarks
propose parameter share method transformers vaswani et al two thousand and seventeen propose approach relax widely use technique share parameters one layer layer universal transformers dehghani et al two thousand and nineteen increase efficiency computational time propose three strategies sequence cycle cycle rev assign parameters layer experimental result show propose strategies efficient parameter size computational time moreover indicate propose strategies also effective configuration use many train data recent wmt competition
attention key component ubiquitous pre train language model learn focus relevant piece information transformer base architectures prove capable tackle several task sometimes even surpass single task counterparts better understand phenomenon conduct structural analysis new purpose question answer model introduce surprisingly model retain single task performance even absence strong transfer effect task attention head importance score observe attention head specialize particular task head conducive learn others multi task single task settings
paper summarize participation laboratoire informatique image et interaction l3i laboratory university la rochelle recognize ultra fine grain entities rufes track within text analysis conference tac series evaluation workshops participation rely two neural base model one base pre train fine tune language model stack transformer layer fine grain entity extraction one box model within document entity coreference observe approach great potential increase performance fine grain entity recognition thus future work envision enhance ability model follow additional experiment deeper analysis result
detect humor challenge task since word might share multiple valences depend context word even use offensive expressions neural network architectures base transformer obtain state art result several natural language process task especially text classification adversarial learn combine techniques multi task learn aid neural model learn intrinsic properties data work describe adversarial multi task network amtl humor use detect rate humor offensive texts task seven semeval two thousand and twenty-one branch model focus solve relate task consist bilstm layer follow capsule layer top bertweet use generate contextualized embeddings best model consist ensemble test configurations achieve nine thousand, five hundred and sixty-six f1 score nine thousand, four hundred and seventy accuracy task 1a obtain rmse score six thousand, two hundred five thousand, three hundred and eighteen task 1b two respectively
advent direct model automatic speech recognition asr formerly prevalent frame wise acoustic model base hide markov model hmm diversify number model architectures like encoder decoder attention model transducer model segmental model direct hmm transducer model stay frame level model definition segmental model define level label segment directly soft attention base model avoid explicit alignment transducer segmental approach internally model alignment either segment hypotheses implicitly emit call blank symbols work prove widely use class rnn transducer model segmental model direct hmm equivalent therefore show equal model power show blank probabilities translate segment length probabilities vice versa addition provide initial experiment investigate decode beam prune compare time synchronous label segment synchronous search strategies properties use underlie model
paper would light impact fine tune social media data internal representations neural language model focus bot detection twitter key task mitigate counteract automatic spread disinformation bias social media investigate use pre train language model tackle detection tweet generate bot human account base exclusively content unlike general trend benchmarks like glue bert generally outperform generative transformers like gpt gpt two classification task regular text observe fine tune generative transformers bot detection task produce higher accuracies analyze architectural components transformer study effect fine tune hide state output representations among find show part syntactical information distributional properties capture bert pre train lose upon fine tune generative pre train approach manage preserve properties
discontinuous constituent parsers always lag behind continuous approach term accuracy speed presence constituents discontinuous yield introduce extra complexity task however discontinuous tree convert continuous variant reorder tokens base propose reduce discontinuous parse continuous problem directly solve shelf continuous parser end develop pointer network capable accurately generate continuous token arrangement give input sentence define bijective function recover original order experiment main benchmarks two continuous parsers prove approach par accuracy purely discontinuous state art algorithms considerably faster
choice negative examples important noise contrastive estimation recent work find hard negative highest score incorrect examples model effective practice use without formal justification develop analytical tool understand role hard negative specifically view contrastive loss bias estimator gradient cross entropy loss show theoretically empirically set negative distribution model distribution result bias reduction also derive general form score function unify various architectures use text retrieval combine hard negative appropriate score function obtain strong result challenge task zero shoot entity link
gow two thousand and twelve dual lexicon model suggest primary purpose word mediate mappings acoustic phonetic input form linguistic representation motivate evidence functional image aphasia behavioral result model argue existence two parallel wordform store dorsal ventral process stream paper test hypothesis complex systematic map sound articulation dorsal stream pose different computational pressure feature set arbitrary map sound mean test hypothesis create two deep convolutional neural network cnns dorsal network train identify individual speak word ventral network train map onto semantic class extract pattern network activation penultimate level network test well feature generate network support generalization linguistic categorization associate dorsal versus ventral process stream preliminary result show model successfully learn task secondary generalization test show ventral cnn outperform dorsal cnn semantic task concreteness classification dorsal cnn outperform ventral cnn articulation task classification onset phoneme class syllable length result consistent hypothesis divergent process demand ventral dorsal process stream impose computational pressure development multiple lexica
automatically evaluate text base non task orient dialogue systems ie chatbots remain open problem previous approach suffer challenge range poor correlation human judgment poor generalization often require gold standard reference comparison human annotate data extend exist evaluation methods propose metric base linguistic feature may able maintain good correlation human judgment interpretable without require gold standard reference human annotate data support proposition measure analyze various linguistic feature dialogues produce multiple dialogue model find feature behaviour consistent know properties model test similar across domains also demonstrate approach exhibit promise properties zero shoot generalization new domains relate task evaluate response relevance
problem answer question use knowledge pre train language model lms knowledge graph kgs present two challenge give qa context question answer choice methods need identify relevant knowledge large kgs ii perform joint reason qa context kg work propose new model qa gnn address challenge two key innovations relevance score use lms estimate importance kg nod relative give qa context ii joint reason connect qa context kg form joint graph mutually update representations graph neural network evaluate qa gnn commonsenseqa openbookqa datasets show improvement exist lm lmkg model well capability perform interpretable structure reason eg correctly handle negation question
rapid development nlp research leaderboards emerge one tool track performance various systems various nlp task effective goal extent generally present rather simplistic one dimensional view submit systems communicate holistic accuracy number paper present new conceptualization implementation nlp evaluation explainaboard addition inherit functionality standard leaderboard also allow researchers diagnose strengths weaknesses single system eg best perform system bad ii interpret relationships multiple systems eg system outperform system b combine systems b c iii examine prediction result closely eg common errors make multiple systems contexts particular errors occur explainaboard deploy urlhttp explainaboardnlpediaai additionally release interpretable evaluation code urlhttps githubcom neulab explainaboard output file three hundred systems forty datasets nine task motivate output drive research future
language model lms must safe equitable responsibly deploy practice safety mind numerous detoxification techniques eg dathathri et al two thousand and twenty krause et al two thousand and twenty propose mitigate toxic lm generations work show current detoxification techniques hurt equity decrease utility lms language use marginalize group eg african american english minority identity mention particular perform automatic human evaluations text generation quality lms condition input different dialects group identifiers find detoxification make lms brittle distribution shift especially language use marginalize group identify failures stem detoxification methods exploit spurious correlations toxicity datasets overall result highlight tension controllability distributional robustness lms
speak language understand slu system usually consist various pipeline components component heavily rely result upstream ones example intent detection id slot fill sf require upstream automatic speech recognition asr transform voice text case upstream perturbations eg asr errors environmental noise careless user speak propagate id sf model thus deteriorate system performance therefore well perform sf id model expect noise resistant extent however exist model train clean data cause textitgap clean data train real world inference bridge gap propose method perspective domain adaptation high low quality sample embed similar vector space meanwhile design denoising generation model reduce impact low quality sample experiment widely use dataset ie snip large scale house dataset ten million train examples demonstrate method outperform baseline model real world noisy corpus also enhance robustness produce high quality result noisy environment source code release
frame political issue influence policy public opinion even though public play key role create spread frame little know ordinary people social media frame political issue create new dataset immigration relate tweet label multiple frame typologies political communication theory develop supervise model detect frame demonstrate users ideology region impact frame choices message frame influence audience responses find commonly use issue generic frame obscure important ideological regional pattern reveal immigration specific frame furthermore frame orient towards human interest culture politics associate higher user engagement large scale analysis complex social linguistic phenomenon contribute nlp social science research
develop conversational recommendation system design help users navigate set limit options find best choice unlike many internet scale systems use singular set search term return rank list options amongst thousands system use multi turn user dialog deeply understand users preferences system respond context users specific immediate feedback make sequential recommendations envision system would highly useful situations intrinsic constraints find right restaurant within walk distance right retail item within limit inventory research prototype instantiate former use case leverage real data google place yelp zomato evaluate system similar system incorporate user feedback sixteen person remote study generate sixty-four scenario base search journey recommendation system successfully trigger saw increase efficiency higher confidence rat respect final user choice also find users prefer system seventy-five compare baseline
vector representations become central element semantic language model lead mathematical overlap many field include quantum theory compositionality core goal representations give representations wet fish concept wet fish represent position paper survey question two point view first consider question whether explicit mathematical representation successful use tool within linear algebra whether mathematical tool need second consider whether semantic vector composition explicitly describe mathematically whether model internal side effect train neural network third newer question whether compositional model implement quantum computer give fundamentally linear nature quantum mechanics propose question relate survey may help highlight candidate operations future quantum implementation
natural language prompt recently use coax pretrained language model perform ai task use fill blank paradigm petroni et al two thousand and nineteen shoot extrapolation paradigm brown et al two thousand and twenty example language model retain factual knowledge train corpora extract ask fill blank sentential prompt however prompt come explore idea learn prompt gradient descent either fine tune prompt take previous work start random initialization prompt consist soft word ie continuous vectors necessarily word type embeddings language model furthermore task optimize mixture prompt learn prompt effective ensemble across multiple english lms task approach hugely outperform previous methods show implicit factual knowledge language model previously underestimate moreover knowledge cheap elicit random initialization nearly good inform initialization
possible explanation impressive performance mask language model mlm pre train model learn represent syntactic structure prevalent classical nlp pipelines paper propose different explanation mlms succeed downstream task almost entirely due ability model higher order word co occurrence statistics demonstrate pre train mlms sentence randomly shuffle word order show model still achieve high accuracy fine tune many downstream task include task specifically design challenge model ignore word order model perform surprisingly well accord parametric syntactic probe indicate possible deficiencies test representations syntactic information overall result show purely distributional information largely explain success pre train underscore importance curating challenge evaluation datasets require deeper linguistic knowledge
propose task narrative reorderingnareor involve rewrite give story different narrative order preserve plot semantic temporal aspects present dataset nareorc one thousand human rewrite stories within rocstories non linear order conduct detail analysis propose novel initial task specific train methods evaluation metrics perform experiment nareorc use gpt two transformer model conduct extensive human evaluation demonstrate nareor challenge task potential exploration
automatic icd cod task assign cod international classification diseases icd medical note cod describe state patient multiple applications eg computer assist diagnosis epidemiological study icd cod challenge task due complexity length medical note unlike general trend language process transformer model report reach high performance task investigate detail icd cod use pubmedbert state art transformer model biomedical language understand find difficulty fine tune model long piece text main limitation bert base model icd cod run extensive experiment show despite gap current state art pretrained transformers reach competitive performance use relatively small portion text point better methods aggregate information long texts main need improve bert base icd cod
paper contribute new state art sota semantic textual similarity sts compare combine number recently propose sentence embed methods sts propose novel simple ensemble knowledge distillation scheme improve previous approach experiment demonstrate model train learn average embed space multiple ensemble students outperform individual model high robustness utilize distillation method combination previous methods significantly improve sota unsupervised sts proper hyperparameter tune previous methods improve supervise sota score
solve math word problems mwps important challenge problem natural language process exist approach solve mwps require full supervision form intermediate equations however label every math word problem correspond equations time consume expensive task order address challenge equation annotation propose weakly supervise model solve math word problems require final answer supervision approach problem first learn generate equation use problem description final answer use train supervise mwp solver propose compare various weakly supervise techniques learn generate equations directly problem description answer extensive experiment demonstrate even without use equations supervision approach achieve accuracy five hundred and sixty standard math23k dataset also curate release new dataset mwps english consist ten thousand, two hundred and twenty-seven instance suitable train weakly supervise model
conversational search paradigm introduce step change traditional search paradigm allow users interact search agents multi turn natural fashion conversation flow naturally usually center around target field knowledge work propose knowledge drive answer generation approach open domain conversational search conversation wide entities knowledge graph use bias search answer generation first conversation specific knowledge graph extract top passages retrieve transformer base ranker entities knowledge graph use bias search answer generator transformer towards information rich concise answer conversation specific bias compute identify relevant passages accord salient entities particular conversation experiment show propose approach successfully exploit entities knowledge along conversation outperform set baselines search answer generation task
social media become increasingly prominent day day live increasingly important detect informative content prevent spread disinformation unverified rumour many sophisticate successful model propose literature often compare older nlp baselines svms cnns lstms paper examine performance broad set modern transformer base language model show basic fine tune model competitive even significantly outperform recently propose state art methods present framework baseline create evaluate new methods misinformation detection study comprehensive set benchmark datasets discuss potential data leakage need careful design experiment understand datasets account confound variables extreme case example show classify base first three digits tweet ids contain information date give state art performance commonly use benchmark dataset fake news detection twitter16 provide simple tool detect problem suggest step mitigate future datasets
vital step towards widespread adoption neural retrieval model resource efficiency throughout train index query workflows neural ir community make great advancements train effective dual encoder dense retrieval dr model recently dense text retrieval model use single vector representation per query passage score match enable low latency first stage retrieval nearest neighbor search increasingly common train approach require enormous compute power either conduct negative passage sample continuously update refresh index require large batch size batch negative sample instead rely compute capability introduce efficient topic aware query balance margin sample technique call tas balance cluster query train sample query cluster per batch train lightweight six layer dr model novel dual teacher supervision combine pairwise batch negative teachers method trainable single consumer grade gpu forty-eight hours oppose common configuration 8x v100s show tas balance train method achieve state art low latency 64ms per query result two trec deep learn track query set evaluate ndcg10 outperform bm25 forty-four plainly train dr nineteen doct5query eleven previous best dr model five additionally tas balance produce first dense retriever outperform every method recall cutoff trec dl allow resource intensive rank model operate fewer passages improve result
paper propose recent research paradigm task event detection ed cast question answer qa problem possibility multiple answer support entities extraction event trigger thus transform task identify answer span context also focus surround entities architecture base pre train fine tune language model input context augment entities mark different level position type finally argument roles experiment ace2005 corpus demonstrate propose paradigm viable solution ed task significantly outperform state art model moreover prove methods also able extract unseen event type
despite widespread use natural language process nlp task word embeddings criticize inherit unintended gender bias train corpora programmer closely associate man homemaker closely associate woman gender bias also show propagate downstream task
recently argue encoder decoder model make interpretable replace softmax function attention sparse variants work introduce novel simple method achieve sparsity attention replace softmax activation relu show sparsity naturally emerge formulation train stability achieve layer normalization either specialize initialization additional gate function model call rectify linear attention rela easy implement efficient previously propose sparse attention mechanisms apply rela transformer conduct experiment five machine translation task rela achieve translation performance comparable several strong baselines train decode speed similar vanilla attention analysis show rela deliver high sparsity rate head diversity induce cross attention achieve better accuracy respect source target word alignment recent sparsified softmax base model intriguingly rela head also learn attend nothing ie switch query possible sparsified softmax alternatives
multimodal sentiment analysis muse two thousand and twenty-one challenge focus task sentiment emotion well physiological emotion emotion base stress recognition comprehensively integrate audio visual language biological signal modalities purpose muse two thousand and twenty-one bring together communities different discipline mainly audio visual emotion recognition community signal base sentiment analysis community symbol base health informatics community present four distinct sub challenge muse wilder muse stress focus continuous emotion valence arousal prediction muse send participants recognise five class valence arousal muse physio novel aspect physiological emotion predict years challenge utilise muse car dataset focus user generate review introduce ulm tsst dataset display people stressful depositions paper also provide detail state art feature set extract datasets utilisation baseline model long short term memory recurrent neural network sub challenge competitive baseline participants set namely test report concordance correlation coefficient ccc four thousand, six hundred and sixteen ccc muse wilder four thousand, seven hundred and seventeen ccc muse stress four thousand, six hundred and six ccc muse physio muse send f1 score three thousand, two hundred and eighty-two obtain
describe interpretability illusion arise analyze bert model activations individual neurons network may spuriously appear encode single simple concept fact encode something far complex effect hold linear combinations activations trace source illusion geometric properties bert embed space well fact common text corpora represent narrow slice possible english sentence provide taxonomy model learn concepts discuss methodological implications interpretability research especially importance test hypotheses multiple data set
goal orient dialogue systems interact real word environments often encounter noisy data work investigate robust goal orient dialogue systems noisy data specifically analysis consider intent classification ic slot label sl model form basis dialogue systems collect test suite six common phenomena find live human bot conversations abbreviations case misspell morphological variants paraphrase synonyms show phenomena degrade ic sl performance state art bert base model use synthetic data augmentation improve ic sl model robustness real world noise one hundred and fifteen ic one hundred and seventy-three point sl average across noise type make suite noisy test data public enable research robustness dialog systems
representations large pretrained model bert encode range feature monolithic vectors afford strong predictive accuracy across multitude downstream task paper explore whether possible learn disentangle representations identify exist subnetworks within pretrained model encode distinct complementary aspect representations concretely learn binary mask transformer weight hide units uncover subsets feature correlate specific factor variation eliminate need train disentangle model scratch particular task evaluate method respect ability disentangle representations sentiment genre movie review toxicity dialect tweet syntax semantics combine mask magnitude prune find identify sparse subnetworks within bert strongly encode particular aspects eg toxicity weakly encode others eg race moreover despite learn mask find disentanglement via mask perform well often better previously propose methods base variational autoencoders adversarial train
significant memory computational requirements large deep neural network restrict application edge devices knowledge distillation kd prominent model compression technique deep neural network knowledge train large teacher model transfer smaller student model success knowledge distillation mainly attribute train objective function exploit soft target information also know dark knowledge besides give regular hard label train set however show literature larger gap teacher student network difficult train use knowledge distillation address shortcoming propose improve knowledge distillation method call anneal kd feed rich information provide teacher soft target incrementally efficiently anneal kd technique base gradual transition anneal soft target generate teacher different temperatures iterative process therefore student train follow anneal teacher output step step manner paper include theoretical empirical evidence well practical experiment support effectiveness anneal kd method comprehensive set experiment different task image classification cifar ten one hundred nlp language inference bert base model glue benchmark consistently get superior result
classical information retrieval systems bm25 rely exact lexical match carry search efficiently invert list index recent neural ir model shift towards soft semantic match query document term lose computation efficiency exact match systems paper present coil contextualized exact match retrieval architecture bring semantic lexical match coil score base overlap query document tokens contextualized representations new architecture store contextualized token representations invert list bring together efficiency exact match representation power deep language model experimental result show coil outperform classical lexical retrievers state art deep lm retrievers similar smaller latency
paper investigate correct chinese text errors type mistake miss redundant character common chinese native speakers exist model base detect correct framework correct mistake character errors deal miss redundant character reason lengths sentence correction lead inconsistence model input output although seq2seq base sequence tag methods provide solutions problem achieve relatively good result english context perform well chinese context accord experimental result work propose novel detect correct framework alignment agnostic mean handle text align non align occasion also serve cold start model annotate data provide experimental result three datasets demonstrate method effective achieve best performance among exist publish model
neural information retrieval ir model promise mainly semantic match capabilities ameliorate well know synonymy polysemy problems word base symbolic approach however power neural model dense representations come cost inefficiency limit use ranker sparse representations hand help enhance symbolic latent term representations yet take advantage invert index efficiency amenable symbolic ir techniques around decades order transcend trade sparse representations symbolic latent term base dense representations propose ultra high dimensional uhd representation scheme equip directly controllable sparsity high dimensionality attempt make mean dimension less entangle polysemous dense embeddings sparsity allow efficiency vector calculations also possibility make individual dimension attributable interpretable concepts model uhd bert maximize benefit ultra high dimensional uhd sparse representations base bert language model adopt bucket method method different segment embed horizontal bucket embeddings multiple layer bert vertical bucket select merge diverse linguistic aspects represent additional important benefit highly disentangle high dimensional efficient sparse representations neural approach harmonize well study symbolic ir techniques eg invert index pseudo relevance feedback bm25 enable us build powerful efficient neuro symbolic information retrieval system
generate paragraph diverse content important many applications exist generation model produce similar content homogenize contexts due fix leave right sentence order idea permute sentence order improve content diversity multi sentence paragraph propose novel framework permgen whose objective maximize expect log likelihood output paragraph distributions respect possible sentence order permgen use hierarchical positional embed design new procedures train decode candidate rank sentence permute generation experiment three paragraph generation benchmarks demonstrate permgen generate diverse output higher quality exist model
name entity recognition ner pre train language model overestimate focus dataset bias solve current benchmark datasets however bias hinder generalizability necessary address real world situations weak name regularity plenty unseen mention alleviate use dataset bias make model fully exploit data propose debiasing method bias model replace pointwise mutual information pmi enhance generalization ability outperform domain performance approach enable debias highly correlate word label benchmark datasets reflect informative statistics via subword frequency alleviate class imbalance positive negative examples long name complex structure entities method predict entities debiasing conjunction special character extensive experiment general biomedical domains demonstrate effectiveness generalization capabilities pmi
emotion dynamics model significant task emotion recognition conversation aim predict conversational emotions build empathetic dialogue systems exist study mainly develop model base recurrent neural network rnns benefit power recently develop pre train strategies better token representation learn conversations seriously hard distinguish dependency interlocutors emotional influence among interlocutors simply assemble feature top rnns paper develop series bert base model specifically capture inter interlocutor intra interlocutor dependencies conversational emotion dynamics concretely first substitute bert rnns enrich token representations flat structure bert f bert apply link utterances conversation directly hierarchically structure bert h bert employ distinguish interlocutors link utterances importantly spatial temporal structure bert namely st bert propose determine emotional influence among interlocutors finally conduct extensive experiment two popular emotion recognition conversation benchmark datasets demonstrate propose model attain around five ten improvement state art baselines respectively
propose effective consistency train framework enforce train model predictions give original perturb input similar add discrete noise would incur highest divergence predictions virtual adversarial discrete noise obtain replace small portion tokens keep original semantics much possible efficiently push train model decision boundary moreover perform iterative refinement process alleviate degrade fluency perturb sentence due conditional independence assumption experimental result show propose method outperform consistency train baselines text edit paraphrase continuous noise semi supervise text classification task robustness benchmark
paper describe submissions social media mine health smm4h2021 share task participate two tasks1 classification extraction normalization adverse drug effect ade mention english tweet task one two classification covid nineteen tweet contain symptomstask six approach first task use language representation model roberta binary classification head second task use bertweet base roberta fine tune perform pre train model task model place top custom domain specific process pipeline system rank first among submissions subtask 1a f1 score sixty-one subtask 1b system obtain f1 score fifty improvements eight f1 score average across submissions bertweet model achieve f1 score ninety-four smm4h two thousand and twenty-one task six
introduce novel embed model name noke aim integrate co occurrence among entities relations graph neural network improve knowledge graph completion ie link prediction give knowledge graph noke construct single graph consider entities relations individual nod noke compute weight edge among nod base co occurrence entities relations next noke utilize vanilla gnns update vector representations entity relation nod adopt score function produce triple score comprehensive experimental result show noke obtain state art result three new challenge difficult benchmark datasets codex knowledge graph completion demonstrate power simplicity effectiveness
machine learn bring strike advance multilingual natural language process capabilities past year example latest techniques improve state art performance xtreme multilingual benchmark thirteen point sizeable gap human level performance remain improvements easier achieve task others paper analyze current state cross lingual transfer learn summarize lessons learn order catalyze meaningful progress extend xtreme xtreme r consist improve set ten natural language understand task include challenge language agnostic retrieval task cover fifty typologically diverse languages addition provide massively multilingual diagnostic suite fine grain multi dataset evaluation capabilities interactive public leaderboard gain better understand model
post process static embed beenshown improve performance lexical sequence level task however post process contextualized embeddings study problem work question usefulness post process contextualized embeddings obtain different layer pre train language model specifically standardize individual neuron activations use z score min max normalization remove top principle components use top method additionally apply unit length normalization word representations diverse set pre train model show post process unwrap vital information present representations lexical task word similarity analogyand sequence classification task find raise interest point relation theresearch study use contextualized representations suggest z score normalization essential step consider use application
stance detection concern classification writer viewpoint towards target different task variants eg stance tweet vs full article stance respect claim vs implicit topic moreover task definitions vary include label inventory data collection annotation protocol aspects hinder cross domain study require change standard domain adaptation approach paper perform depth analysis sixteen stance detection datasets explore possibility cross domain learn moreover propose end end unsupervised framework domain prediction unseen user define label particular combine domain adaptation techniques mixture experts domain adversarial train label embeddings demonstrate sizable performance gain strong baselines domain ie see target ii domain ie unseen target finally perform exhaustive analysis cross domain result highlight important factor influence model performance
prevalence large pre train language model lead significant improvements performance nlp systems recent research demonstrate model inherit societal bias extant natural language paper explore simple method probe pre train language model gender bias use effect multi lingual study gender bias towards politicians construct dataset 250k politicians countries world quantify adjective verb usage around politicians name function gender conduct study seven languages across six different language model architectures result demonstrate stance towards politicians pre train language model highly dependent language use finally contrary previous find study suggest larger language model tend significantly gender bias smaller ones
multiple instance learn mil become standard learn paradigm distantly supervise relation extraction dsre however due relation extraction perform bag level mil significant hardware requirements train couple large sentence encoders deep transformer neural network paper propose novel sample method dsre relax hardware requirements propose method limit number sentence batch randomly sample sentence bag batch however come cost lose valid sentence bag alleviate issue cause random sample use ensemble train model prediction demonstrate effectiveness approach use propose learn set fine tune bert widely nyt dataset approach significantly outperform previous state art methods term auc pn metrics
obtain high quality sentence embeddings pretrained language model plms must either augment additional pretraining objectives finetuned large set label text pair latter approach typically outperform former require great human effort generate suitable datasets sufficient size paper show large plms leverage obtain high quality embeddings without require label data finetuning modifications pretraining objective utilize generative abilities plms generate entire datasets label text pair scratch use regular finetuning much smaller model fully unsupervised approach outperform strong baselines several english semantic textual similarity datasets
neural machine translation nmt model conventionally train token level negative log likelihood nll guarantee generate translations optimize select sequence level evaluation metric multiple approach propose train nmt bleu reward order directly improve metric however report gain bleu translate real quality improvement limit application industry recently become clear community bleu low correlation human judgment deal state art model lead emerge model base evaluation metrics new metrics show much higher human correlation paper investigate whether beneficial optimize nmt model state art model base metric bleurt propose contrastive margin loss fast stable reward optimization suitable large nmt model experiment perform automatic human evaluations compare model train smooth bleu bleurt baseline model result show reward optimization bleurt able increase metric score large margin contrast limit gain train smooth bleu human evaluation show model train bleurt improve adequacy coverage translations code available via https githubcom naver ai metricmt
despite show increasingly human like conversational abilities state art dialogue model often suffer factual incorrectness hallucination knowledge roller et al two thousand and twenty work explore use neural retrieval loop architectures recently show effective open domain qa lewis et al 2020b izacard grave two thousand and twenty knowledge ground dialogue task arguably challenge require query base complex multi turn dialogue context generate conversationally coherent responses study various type architectures multiple components retrievers rankers encoder decoders goal maximize knowledgeability retain conversational ability demonstrate best model obtain state art performance two knowledge ground conversational task model exhibit open domain conversational capabilities generalize effectively scenarios within train data verify human evaluations substantially reduce well know problem knowledge hallucination state art chatbots
recent commonsense reason task typically discriminative nature model answer multiple choice question certain context discriminative task limit fail adequately evaluate model ability reason explain predictions underlie commonsense knowledge also allow model use reason shortcuts right right reason work present explagraphs new generative structure commonsense reason task associate dataset explanation graph generation stance prediction specifically give belief argument model predict whether argument support counter belief also generate commonsense augment graph serve non trivial complete unambiguous explanation predict stance explanation graph dataset collect via crowdsourcing novel collect judge refine graph collection framework improve graph quality via multiple round verification refinement significant eighty-three graph contain external commonsense nod diverse structure reason depths also propose multi level evaluation framework check structural semantic correctness generate graph plausibility human write graph experiment state art text generation model like bart t5 generate explanation graph observe large gap human performance thereby encourage useful future work new commonsense graph base explanation generation task
paper reformulate relation extraction task mask language model propose novel adaptive prompt base finetuning approach propose adaptive label word selection mechanism scatter relation label variable number label tokens handle complex multiple label space introduce auxiliary entity discriminator object encourage model focus context representation learn extensive experiment benchmark datasets demonstrate approach achieve better performance shoot supervise set
legal english sublanguage important everyone everyone understand pretrained model become best practice among current deep learn approach different problems would waste even danger model apply practice without knowledge sublanguage law paper raise issue propose trivial solution introduce bertlaw legal sublanguage pretrained model paper experiment demonstrate superior effectiveness method compare baseline pretrained model
product quantization pq popular approach maximum inner product search mips widely use ad hoc retrieval recent study propose differentiable pq embed quantization modules train jointly however lack depth understand appropriate joint train objectives improvements non differentiable baselines consistently positive reality work propose search orient product quantization sopq novel train objective mcl formulate minimization mcl query key match probability maximize differentiable pq besides vcs protocol design facilitate minimization mcl sql leverage relax dependency label data extensive experiment four real world datasets validate effectiveness propose methods
natural language process nlp research combine study universal principles basic science apply science target specific use case settings however process exchange basic nlp applications often assume emerge naturally result many innovations go unapplied many important question leave unstudied describe new paradigm translational nlp aim structure facilitate process basic apply nlp research inform one another translational nlp thus present third research paradigm focus understand challenge pose application need challenge drive innovation basic science technology design show many significant advance nlp research emerge intersection basic principles application need present conceptual framework outline stakeholders key question translational research framework provide roadmap develop translational nlp dedicate research area identify general translational principles facilitate exchange basic apply research
machine learn model offer excellent predictive performance often lack interpretability necessary support integrate human machine decision make clinical medicine high risk settings domain experts may unwilling trust model predictions without explanations work explainable ai must balance compete objectives along two different ax one explanations must balance faithfulness model decision make plausibility domain expert two domain experts desire local explanations individual predictions global explanations behavior aggregate propose train proxy model mimic behavior train model provide fine grain control trade off evaluate approach task assign icd cod clinical note demonstrate explanations proxy model faithful replicate train model behavior
detect fix bug two important yet frustrate part software development cycle exist bug detection tool base mainly static analyzers rely mathematical logic symbolic reason program execution detect common type bug fix bug typically leave developer work introduce deepdebug data drive program repair approach learn detect fix bug java methods mine real world github repositories frame bug patch sequence sequence learn task consist two step denoising pretraining ii supervise finetuning target translation task show pretraining source code program improve number patch find thirty-three compare supervise train scratch domain adaptive pretraining natural language code improve accuracy another thirty-two refine standard accuracy evaluation metric non deletion deletion fix show best model generate seventy-five non deletion fix previous state art contrast prior work attain best result generate raw code oppose work abstract code tend benefit smaller capacity model finally observe subtle improvement add syntax embeddings along standard positional embeddings well add auxiliary task predict token syntactic class despite focus java approach language agnostic require general purpose parser tree sitter
combination multilingual pre train representations cross lingual transfer learn one effective methods build functional nlp systems low resource languages however extremely low resource languages without large scale monolingual corpora pre train sufficient annotate data fine tune transfer learn remain study challenge task moreover recent work show multilingual representations surprisingly disjoint across languages bring additional challenge transfer onto extremely low resource languages paper propose metaxl meta learn base framework learn transform representations judiciously auxiliary languages target one bring representation space closer effective transfer extensive experiment real world low resource languages without access large scale monolingual corpora large amount label data task like cross lingual sentiment analysis name entity recognition show effectiveness approach code metaxl publicly available githubcom microsoft metaxl
conduct empirical evaluation extrapolation performance condition scalar control input like desire output length desire edit input sentence desire sentiment across three text generation task specifically examine zero shoot set model ask generalize range control value see train focus evaluate popular embed methods scalar input include learnable sinusoidal embeddings well simpler approach surprisingly find indicate simplest strategy use scalar input directly without encode reliably allow successful extrapolation
learn new language remain date cognitive task require considerable diligence willpower recent advance tool notwithstanding paper propose broccoli new paradigm aim reduce require effort seamlessly embed vocabulary learn users everyday information diet achieve inconspicuously switch choose word encounter user translation target language thus see word context user assimilate new vocabulary without much conscious effort validate approach careful user study find efficacy lightweight broccoli approach competitive traditional memorization base vocabulary learn low cognitive overhead manifest pronounce decrease learners usage mnemonic learn strategies compare traditional learn finally establish language pattern typical information diet compatible space repetition strategies thus enable efficient use broccoli paradigm overall work establish feasibility novel powerful install forget approach embed language acquisition
online domain specific customer service applications many company struggle deploy advance nlp model successfully due limit availability noise datasets prior research demonstrate potential migrate large open domain pretrained model domain specific task appropriate pretraining strategies yet rigorously evaluate social media customer service settings especially multilingual condition address gap collect multilingual social media corpus contain customer service conversations 865k tweet compare various pipelines pretraining finetuning approach apply five different end task show pretraining generic multilingual transformer model domain dataset finetuning specific end task consistently boost performance especially non english settings
pretrained language model demonstrate strong performance nlp task fine tune small task specific datasets hence autoregressive model constitute ideal agents operate text base environments language understand generative capabilities essential nonetheless collect expert demonstrations environments time consume endeavour introduce two stage procedure learn small set demonstrations improve interact environment show language model fine tune twelve expert demonstrations simple reinforcement learn algorithm achieve fifty-one absolute improvement success rate exist methods alfworld environment
millions repetitive code snippets submit code repositories every day search large codebases use simple natural language query would allow programmers ideate prototype develop easier faster although exist methods show good performance search cod natural language description contain keywords code still far behind search cod base semantic mean natural language query semantic structure code recent years natural language program language research communities create techniques embed vector space work leverage efficacy embed model use simple lightweight two layer neural network task semantic code search show model learn inherent relationship embed space probe scope improvement empirically analyze embed methods analysis show quality code embed model bottleneck model performance discuss future directions study area
extend language model structural modifications vision language vandl pretraining successful ways make vandl model grind vision language potential applications advance model include multi modal machine read comprehension model multi modal dialogue model require language ability upon ground although language capability crucial applications impact extend visual capabilities language capabilities fully understand paper investigate visual extension affect language capability vandl model use glue benchmark find visual extension cause decrease language capability vandl pretraining greater impact structural modifications decrease result suggest need study pretraining maintain possible improve model language capability
low resource settings model transfer help overcome lack label data many task domains however predict useful transfer source challenge problem even similar source might lead unexpected negative transfer result thus rank methods base task text similarity may sufficient identify promise source tackle problem propose method automatically determine many source exploit study effect model transfer sequence label across various domains task show methods base model similarity support vector machine able predict promise source result performance increase twenty-four f1 point
citation content analysis seek understand citations base language use make citation key issue citation content analysis look linguistic structure characterize distinct class citations purpose understand intent function citation previous work focus model linguistic feature first draw conclusions language structure unique class citation function base performance classification task inter annotator agreement study start large sample pre classify citation corpus two million citations class scite smart citation dataset support dispute mention citations analyze corpus linguistics order reveal unique statistically significant language structure belong type citation generate comparison table citation type present number interest linguistic feature uniquely characterize citation type find within citation collocate low correlation citation type sentiment additionally find subjectivity citation collocate across class low find suggest sentiment collocate predictor citation function due low subjectivity opinion express mode understand citations implicit previous citation sentiment analysis literature inappropriate instead suggest citations better understand claim make devices citation type explain understand two claim compare present approach hope inspire similar corpus linguistic study citations derive robust theory citation empirical basis use citation corpora
recent advance use retrieval components external knowledge source show impressive result variety downstream task natural language process explore use unstructured external knowledge source image correspond caption improve visual question answer vqa first train novel alignment model embed image caption space achieve substantial improvement performance image caption retrieval wrt similar methods second show retrieval augment multi modal transformers use train alignment model improve result vqa strong baselines conduct extensive experiment establish promise approach examine novel applications inference time hot swap indices
natural language inference nli model know learn bias artefacts within train data impact well model generalise unseen datasets previous de bias approach focus prevent model learn bias instead provide model information human would approach task aim encourage model learn feature generalise better domain datasets use natural language explanations supervise model attention weight encourage attention pay word present explanations first time show train human generate explanations simultaneously improve performance distribution distribution nli whereas relate work robustness involve trade two train human explanations encourage model attend broadly across sentence pay attention word premise less attention stop word punctuation supervise model attend word humans believe important create robust better perform nli model
develop unify multilingual translation model key topic machine translation research however exist approach suffer performance degradation multilingual model yield inferior performance compare ones train separately rich bilingual data attribute performance degradation two issue multilingual embed conflation multilingual fusion effect address two issue propose pam transformer model augment defusion adaptation multilingual machine translation specifically pam consist embed layer adapters shift word intermediate representations towards language specific ones extensive experiment result iwslt opus one hundred wmt benchmarks show method outperform several strong competitors include series adapter multilingual knowledge distillation
major challenge neuroscience machine learn development useful tool understand complex information process systems one tool probe ie supervise model relate feature interest activation pattern arise biological artificial neural network neuroscience pave way use model numerous study conduct recent decades work draw insights neuroscience help guide probe research machine learn highlight two important design choices probe direction expressivity relate choices research goals argue specific research goals play paramount role design probe encourage future probe study explicit state goals
despite high accuracy pretrained transformer network text classification persist issue significant complexity make hard interpret recent research focus develop feature score methods identify part input important model make particular prediction use explanation ie rationale limitation approach assume particular feature score method use across instance dataset use predefined fix length might optimal across instance address propose method extract variable length explanations use set different feature score methods instance level method inspire word erasure approach assume faithful rationale prediction one highest divergence model output distribution use full text text remove rationale particular instance evaluation four standard text classification datasets show method consistently provide faithful explanations compare previous fix length fix feature score methods rationale extraction
pre train language model lm become go text representation encoders prior research use deep lms encode text sequence sentence passages single dense vector representations dense representations use efficient text comparison embed base retrieval however dense encoders suffer low resource situations many techniques develop solve problem despite success much know happen paper show one lie readiness lm expose knowledge dense representation fine tune term optimization readiness validate theory present condenser general pre train architecture base transformer lms improve dense optimization readiness show fine tune condenser significantly improve performance small noisy train set
introduce data augmentation technique base byte pair encode bert like self attention model boost performance speak language understand task compare evaluate method range augmentation techniques encompass generative model vaes performance boost techniques synonym replacement back translation show method perform strongly domain intent classification task voice assistant user study focus utterance naturalness semantic similarity
knowledge graph become increasingly popular supplemental information represent structural relations entities knowledge graph embed methods kge use various downstream task eg knowledge graph completion include triple classification link prediction however knowledge graph also include much sensitive information train set vulnerable privacy attack paper conduct one attack ie membership inference attack four standard kge methods explore privacy vulnerabilities knowledge graph experimental result four benchmark knowledge graph datasets show privacy attack reveal membership information leakage kge methods
present text2app framework allow users create functional android applications natural language specifications conventional method source code generation try generate source code directly impractical create complex software overcome limitation transform natural language abstract intermediate formal language represent application substantially smaller number tokens intermediate formal representation compile target source cod abstraction program detail allow seq2seq network learn complex application structure less overhead order train sequence model introduce data synthesis method ground human survey demonstrate text2app generalize well unseen combination app components capable handle noisy natural language instructions explore possibility create applications highly abstract instructions couple system gpt three large pretrained language model source code ready run demo notebook demo video publicly available urlhttp text2appgithubio
transformer base architectures recently use task answer question table order improve accuracy task specialize pre train techniques develop apply millions open domain web table paper propose two novel approach demonstrate one achieve superior performance table qa task without even use specialize pre train techniques first model call rci interaction leverage transformer base architecture independently classify row columns identify relevant cells model yield extremely high accuracy find cell value recent benchmarks second model propose call rci representation provide significant efficiency advantage online qa systems table materialize embeddings exist table experiment recent benchmarks prove propose methods effectively locate cell value table ninety-eight hit1 accuracy wikisql lookup question also interaction model outperform state art transformer base approach pre train large table corpora tapas tabert achieve thirty-four one thousand, eight hundred and eighty-six additional precision improvement standard wikisql benchmark
present simple effective approach leverage wikipedia neural machine translation well cross lingual task image caption dependency parse without use direct supervision external parallel data supervise model target language show first sentence title link wikipedia page well cross lingual image caption strong signal seed parallel data extract bilingual dictionaries cross lingual word embeddings mine parallel text wikipedia final model achieve high bleu score close sometimes higher strong supervise baselines low resource languages eg supervise bleu forty versus one hundred and twenty-one model english kazakh moreover tailor wikily translation model unsupervised image caption cross lingual dependency parser transfer image caption train multi task machine translation image caption pipeline arabic english arabic train data wikily translation english caption data caption result arabic slightly better supervise model dependency parse translate large amount monolingual text use artificial train data annotation projection framework show model outperform recent work cross lingual transfer dependency parsers
propose neural string edit distance model string pair classification sequence generation base learn string edit distance modify original expectation maximization learn edit distance algorithm differentiable loss function allow us integrate neural network provide contextual representation input test method cognate detection transliteration grapheme phoneme conversion show trade performance interpretability single framework use contextual representations difficult interpret match performance state art string pair classification model use static embeddings minor modification loss function force interpretability expense accuracy drop
although pretrained language model ptlms show contain significant amount world knowledge still produce inconsistent answer question probe even use specialize train techniques reduce inconsistency result hard identify model actually believe world goal reduce problem systems globally consistent accurate answer approach add memory component beliefbank record model answer two mechanisms use improve consistency among beliefs first reason component weight sit solver improve consistency flip answer significantly clash others second feedback component query model use know beliefs context show control experimental set two mechanisms improve accuracy consistency significant first step towards endow model evolve memory allow construct coherent picture world
despite recent monumental advance field many natural language process nlp model still struggle perform adequately noisy domains propose novel probabilistic embed level method improve robustness nlp model method robust embeddings via distributions red incorporate information noisy tokens surround context obtain distributions embed vectors express uncertainty semantic space fully deterministic method evaluate method number downstream task use exist state art model presence natural synthetic noise demonstrate clear improvement embed approach robustness literature
representation learn method consider stable consistently generate similar representation give data across multiple run word embed methods wems class representation learn methods generate dense vector representation word give text data central idea paper explore stability measurement wems use intrinsic evaluation base word similarity experiment three popular wems word2vec glove fasttext stability measurement investigate effect five parameters involve train model perform experiment use four real world datasets different domains wikipedia news song lyric european parliament proceed also observe effect wem stability three downstream task cluster pos tag fairness evaluation experiment indicate amongst three wems fasttext stable follow glove word2vec
recent years conversational agents provide natural convenient access useful information people daily life along broad new research topic conversational question answer qa among popular conversational qa task conversational open domain qa require retrieve relevant passages web extract exact answer practical less study main challenge well capture fully explore historical context conversation facilitate effective large scale retrieval current work mainly utilize history question refine current question enhance representation yet relations history answer current answer conversation also critical task totally neglect address problem propose novel graph guide retrieval method model relations among answer across conversation turn particular utilize passage graph derive hyperlink connect passages contain history answer potential current answer retrieve relevant passages subsequent answer extraction moreover order collect complementary information historical context also propose incorporate multi round relevance feedback technique explore impact retrieval context current question understand experimental result public dataset verify effectiveness propose method notably f1 score improve five eleven predict history answer true history answer respectively
study multi answer retrieval explore problem require retrieve passages cover multiple distinct answer give question task require joint model retrieve passages model repeatedly retrieve passages contain answer cost miss different valid answer prior work focus single answer retrieval limit reason set passages jointly paper introduce jpr joint passage retrieval model focus reranking model joint probability retrieve passages jpr make use autoregressive reranker select sequence passages equip novel train decode algorithms compare prior approach jpr achieve significantly better answer coverage three multi answer datasets combine downstream question answer improve retrieval enable larger answer generation model since need consider fewer passages establish new state art
impressive milestones achieve text match adopt cross attention mechanism capture pertinent semantic connections two sentence however cross attention mechanisms focus word level link two input neglect importance contextual information propose context aware interaction network coin properly align two sequence infer semantic relationship specifically interaction block include one context aware cross attention mechanism effectively integrate contextual information two gate fusion layer flexibly interpolate align representations apply multiple stack interaction block produce alignments different level gradually refine attention result experiment two question match datasets detail analyse confirm effectiveness model
recent years vision language research shift study task require complex reason interactive question answer visual common sense reason question answer plausibility prediction however datasets use problems fail capture complexity real input multimodal environments ambiguous natural language request diverse digital domains introduce mobile app task iterative feedback motif dataset natural language command greatest number interactive environments date motif first contain natural language request interactive environments satisfiable obtain follow question subset enable research task uncertainty resolution perform initial feasibility classification experiment reach f1 score three hundred and seventy-three verify need richer vision language representations improve architectures reason task feasibility
ability automatically extract knowledge graph kg give collection document long stand problem artificial intelligence one way assess capability task slot fill give entity query form entity slot system ask fill slot generate extract miss value relevant passage passages capability crucial create systems automatic knowledge base population become ever increase demand especially enterprise applications recently promise direction evaluate language model way would evaluate knowledge base task slot fill suitable intent recent advancements field try solve task end end fashion use retrieval base language model model like retrieval augment generation rag show surprisingly good performance without involve complex information extraction pipelines however result achieve model two slot fill task kilt benchmark still level require real world information extraction systems paper describe several strategies adopt improve retriever generator rag order make better slot filler kgi0 system available https githubcom ibm retrieve write slot fill reach top one position kilt leaderboard rex zsre dataset large margin
cryptic crosswords dominant english language crossword variety unite kingdom solve expert humans use flexible creative intelligence knowledge language cryptic clue read like fluent natural language adversarially compose two part definition wordplay cipher require sub word character level manipulations promise target evaluate advance nlp systems seek process language creative human like ways present dataset cryptic crossword clue major newspaper use benchmark train sequence sequence model solve also develop relate benchmarks guide development approach challenge task show performance substantially improve use novel curriculum learn approach model pre train relate task involve eg unscramble word train solve cryptics however even curricular approach generalize novel clue type way humans cryptic crosswords remain challenge nlp systems potential source future innovation
qdmr mean representation complex question decompose question sequence atomic step state art qdmr parsers use common sequence sequence seq2seq approach qdmr structure fundamentally describe label relations span input question thus dependency base approach seem appropriate task work present qdmr parser base dependency graph dgs nod graph word edge describe logical relations correspond different computation step propose non autoregressive graph parser graph edge compute simultaneously b seq2seq parser use gold graph auxiliary supervision find graph parser lead moderate reduction performance forty-seven forty-four 16x speed inference time due non autoregressive nature parser improve sample complexity compare seq2seq model second seq2seq model train auxiliary graph supervision better generalization new domains compare seq2seq model also perform better question long sequence computation step
distant supervision ds well establish technique create large scale datasets relation extraction without use human annotations however research ds mostly limit english language constrain single language inhibit utilization large amount data languages could allow extraction diverse facts recently dataset multilingual ds release however analysis reveal propose dataset exhibit unrealistic characteristics one lack sentence express relation two sentence give entity pair express exactly one relation show characteristics lead gross overestimation model performance response propose new dataset dis rex alleviate issue dataset fifteen million sentence span across four languages thirty-six relation class one relation na class also modify widely use bag attention model encode sentence use mbert provide first benchmark result multilingual ds unlike compete dataset show dataset challenge leave enough room future research take place field
goal context open domain textual question answer qa explain answer list support textual evidence rationales also show evidence lead answer systematic way could do new opportunities understand debug system reason would become possible approach generate explanations form entailment tree namely tree entailment step facts know intermediate conclusions final answer train model skill create entailmentbank first dataset contain multistep entailment tree node tree typically two facts compose together produce new conclusion give hypothesis question answer define three increasingly difficult explanation task generate valid entailment tree give relevant sentence leave gold entailment tree b relevant irrelevant sentence c corpus show strong language model partially solve task identify several new directions improve performance work significant provide new type dataset multistep entailments baselines offer new avenue community generate richer systematic explanations
present new corpus situate interactive multimodal conversations simmc twenty aim build successful multimodal assistant agent specifically dataset feature 11k task orient dialogs 117k utterances user virtual assistant shop domain fashion furniture ground situate photo realistic vr scenes dialogs collect use two phase pipeline first generate simulate dialog flow via novel multimodal dialog simulator propose follow manual paraphrase generate utterances paper provide depth analysis collect dataset describe detail four main benchmark task propose simmc twenty preliminary analysis baseline model highlight new challenge simmc twenty dataset bring suggest new directions future research dataset code make publicly available
contextualized representations base neural language model further state art various nlp task despite great success nature representations remain mystery paper present empirical property representations average approximate first principal component specifically experiment show average representations share almost direction first principal component matrix whose columns representations believe explain average representation always simple yet strong baseline examinations show property also hold challenge scenarios example representations model right random initialization therefore conjecture property intrinsic distribution representations necessarily relate input structure realize representations empirically follow normal distribution dimension assume true demonstrate empirical property fact derive mathematically
recently bert realize significant progress sentence match via word level cross sentence attention however performance significantly drop use siamese bert network derive two sentence embeddings fall short capture global semantic since word level attention two sentence absent paper propose dual view distil bertdvbert sentence match sentence embeddings method deal sentence pair two distinct view ie siamese view interaction view siamese view backbone generate sentence embeddings interaction view integrate cross sentence interaction multiple teachers boost representation ability sentence embeddings experiment six sts task show method outperform state art sentence embed methods significantly
embed matrices key components neural natural language process nlp model responsible provide numerical representations input tokensfootnotein paper word subwords refer textittokens term textitembedding refer embeddings input paper analyze impact utility matrices context neural machine translation nmt show detract syntactic semantic information word embeddings run nmt systems random embeddings damage initially sound also show incorporate limit amount task specific knowledge fully train embeddings boost performance nmt systems find demonstrate exchange negligible deterioration performance nmt model run partially random embeddings work structure mean minimal memory requirement longer need store large embed table significant gain industrial device settings evaluate embeddings translate english german french achieve 53x compression rate despite considerably smaller architecture model case even able outperform state art baselines
despite availability large datasets pretrained model state art question answer model remain susceptible variety adversarial attack still far obtain human level language understand one propose way forward dynamic adversarial data collection human annotator attempt create examples model loop fail however approach come higher cost per sample slower pace annotation model adversarial data require annotator effort generate work investigate several answer selection question generation filter methods form synthetic adversarial data generation pipeline take human generate adversarial sample unannotated text create synthetic question answer pair model train synthetic human generate data outperform model train synthetic adversarial data obtain state art result adversarialqa dataset overall performance gain 37f1 furthermore find train synthetic adversarial data improve model generalisation across domains non adversarial data demonstrate gain nine twelve datasets mrqa lastly find model become considerably difficult beat human adversaries drop macro average validate model error rate one hundred and seventy-six eighty-eight compare non augment model
transformer base pre train language model significantly improve performance various natural language process nlp task recent years effective prevalent model usually prohibitively large resource limit deployment scenarios thread research thus work apply network prune techniques pretrain finetune paradigm widely adopt nlp however exist prune result benchmark transformers bert remarkable prune result literature convolutional neural network cnns particular common wisdom prune cnn state sparse prune technique compress model obtain reduce number channel layer elsen et al two thousand and twenty zhu gupta two thousand and seventeen exist work sparse prune bert yield inferior result small dense counterparts tinybert jiao et al two thousand and twenty work aim fill gap study knowledge transfer lose pre train fine tune prune process propose knowledge aware sparse prune process achieve significantly superior result exist literature show first time sparse prune compress bert model significantly reduce number channel layer experiment multiple data set glue benchmark show method outperform lead competitors twenty time weight flop compression neglectable loss prediction accuracy
transformers state art model nlp map give input sequence vectors output sequence vectors however model permutation equivariant additive position embeddings input use supply information order input tokens task additional additive segment embeddings use denote different type input sentence recent work propose variations positional encode relative position encode achieve better performance work systematic study compare different position encode understand reason differences performance demonstrate simple yet effective way encode position segment transformer model propose method perform par sota glue xtreme wmt benchmarks save computation cost
large pretrained generative model like gpt three often suffer hallucinate non existent incorrect content undermine potential merit real applications exist work usually attempt detect hallucinations base correspond oracle reference sentence document level however grind truth reference may readily available many free form text generation applications sentence document level detection may fail provide fine grain signal would prevent fallacious content real time first step address issue propose novel token level reference free hallucination detection task associate annotate dataset name hades hallucination detection dataset create dataset first perturb large number text segment extract english language wikipedia verify crowd source annotations mitigate label imbalance annotation utilize iterative model loop strategy conduct comprehensive data analyse create multiple baseline model
image caption conventionally rely reference base automatic evaluations machine caption compare caption write humans stark contrast reference free manner humans assess caption quality paper report surprise empirical find clip radford et al two thousand and twenty-one cross modal model pretrained 400m imagecaption pair web use robust automatic evaluation image caption without need reference experiment span several corpora demonstrate new reference free metric clipscore achieve highest correlation human judgements outperform exist reference base metrics like cider spice information gain experiment demonstrate clipscore tight focus image text compatibility complementary exist reference base metrics emphasize text text similarities thus also present reference augment version refclipscore achieve even higher correlation beyond literal description task several case study reveal domains clipscore perform well clip art image alt text rat also relatively weaker vs reference base metrics eg news caption require richer contextual knowledge
day day question come variety answer type current question answer qa literature fail adequately address answer diversity question end present gooaq large scale dataset variety answer type dataset contain five million question three million answer collect google gooaq question collect semi automatically google search engine use autocomplete feature result naturalistic question practical interest nonetheless short express use simple language gooaq answer mine google responses collect question specifically answer box search result yield rich space answer type contain textual answer short long well structure ones collections benchmarkt5 model gooaq observe line recent work lm strong performance gooaq short answer question heavily benefit annotate data however b quality generate coherent accurate responses question require long responses question less reliant observe annotate data mainly support pre train release gooaq facilitate research improve qa diverse response type
keyphrase generation aim summarize long document collection salient phrase deep neural model demonstrate remarkable success task capable predict keyphrases even absent document however abstractiveness acquire expense substantial amount annotate data paper present novel method keyphrase generation autokeygen without supervision human annotation motivate observation absent keyphrase one document appear place whole part first construct phrase bank pool phrase corpus phrase bank draw candidate absent keyphrases document partial match process rank type candidates combine lexical semantic level similarities input document moreover utilize top rank candidates train deep generative model absent keyphrases extensive experiment demonstrate autokeygen outperform unsupervised baselines even beat strong supervise methods certain case
build robust question answer systems need ability verify whether answer question truly correct good enough context imperfect qa datasets explore use natural language inference nli way achieve goal nli inherently require premise document context contain necessary information support hypothesis propose answer question leverage large pre train model recent prior datasets construct powerful question converter decontextualization modules reformulate qa instance premise hypothesis pair high reliability combine standard nli datasets nli examples automatically derive qa train data train nli model judge correctness qa model propose answer show nli approach generally improve confidence estimation qa model across different domains evaluate selective qa set careful manual analysis predictions nli model show identify case qa model produce right answer wrong reason answer verify address aspects question
language model train ever text researchers turn largest corpora available unlike type datasets nlp large unlabeled text corpora often present minimal documentation best practice document establish work provide first documentation colossal clean crawl corpus c4 raffel et al two thousand and twenty dataset create apply set filter single snapshot common crawl begin high level summary data include distributions text come write give detail analysis salient part data include frequent source text eg patentsgooglecom contain significant percentage machine translate ocr text effect filter data disproportionately remove text aae evidence benchmark nlp dataset examples contain text release web interface interactive index copy dataset encourage community continuously explore report additional find
propose new general train technique attention mechanisms base virtual adversarial train vat vat compute adversarial perturbations unlabeled data semi supervise set attention mechanisms report previous study vulnerable perturbations empirical experiment reveal technique one provide significantly better prediction performance compare conventional adversarial train base techniques also vat base techniques semi supervise set two demonstrate stronger correlation word importance better agreement evidence provide humans three gain performance increase amount unlabeled data
sentiment analysis important task natural language process nlp exist state art methods supervise learn paradigm however human annotations scarce thus leverage weak supervision sentiment analysis paper propose posterior regularization framework variational approach weakly supervise sentiment analysis better control posterior distribution label assignment intuition behind posterior regularization extract opinion word two document semantically similar posterior distributions two document similar experimental result show posterior regularization improve original variational approach weakly supervise sentiment analysis performance stable smaller prediction variance
prim handful train sample large pretrained language model gpt three show competitive result compare fully supervise fine tune large pretrained language model demonstrate order sample provide difference near state art random guess performance essentially permutations fantastic analyse phenomenon detail establish present across model size even largest current model relate specific subset sample give good permutation one model transferable another one could use development set determine permutations performant would deviate shoot set require additional annotate data instead use generative nature language model construct artificial development set base entropy statistics candidate permutations set identify performant prompt method improve upon gpt family model average thirteen relative across eleven different establish text classification task
privacy preservation crucial component real world application yet applications rely machine learn backends challenge model often capture designer may envision result potential leakage sensitive information example emotion recognition model susceptible learn pattern target variable sensitive variables pattern maliciously purpose obtain protect information paper concentrate use interpretable methods evaluate model efficacy preserve privacy respect sensitive variables focus saliency base explanations explanations highlight regions input text allow us understand model explanations shift model train preserve privacy show certain commonly use methods seek preserve privacy might align human perception privacy preservation also show induce spurious correlations model input primary well secondary task even improvement evaluation metric significant correlations hence lead false assurances perceive privacy model especially use cross corpus condition conduct crowdsourcing experiment evaluate inclination evaluators choose particular model give task model explanations provide find correlation interpretation differences sociolinguistic bias use proxy user trust
event schemas structure knowledge source define typical real world scenarios eg go airport present framework efficient human loop construction schema library base novel mechanism schema induction well craft interface allow non experts program complex event structure associate work release machine readable resource schema library two hundred and thirty-two detail event schemas describe distinct typical scenario term relevant sub event structure happen scenario participants play role scenario fine grain type participant imply relational constraints custom annotation interface schemablocks event schemas available online
paper present simcse simple contrastive learn framework greatly advance state art sentence embeddings first describe unsupervised approach take input sentence predict contrastive objective standard dropout use noise simple method work surprisingly well perform par previous supervise counterparts hypothesize dropout act minimal data augmentation remove lead representation collapse draw inspiration recent success learn sentence embeddings natural language inference nli datasets incorporate annotate pair nli datasets contrastive learn use entailment pair positives contradiction pair hard negative evaluate simcse standard semantic textual similarity sts task unsupervised supervise model use bert base achieve average seven hundred and forty-five eight hundred and sixteen spearman correlation respectively seventy-nine forty-six point improvement compare previous best result also show contrastive learn theoretically regularize pre train embeddings anisotropic space uniform better align positive pair supervise signal available
interpretable system complex open domain reason need interpretable mean representation natural language excellent candidate extremely expressive easy humans understand however manipulate natural language statements logically consistent ways hard model precise yet robust enough handle variation information express paper describe parapattern method build model generate logical transformations diverse natural language input without direct human supervision use bart base model lewis et al two thousand and twenty generate result apply particular logical operation one premise statements crucially largely automate pipeline scrap construct suitable train examples wikipedia paraphrase give model ability handle lexical variation evaluate model use target contrast set well domain sentence compositions qasc dataset khot et al two thousand and twenty result demonstrate operation model accurate flexible
large scale language model gpt three excellent shoot learners allow control via natural text prompt recent study report prompt base direct classification eliminate need fine tune lack data inference scalability paper propose novel data augmentation technique leverage large scale language model generate realistic text sample mixture real sample also propose utilize soft label predict language model effectively distil knowledge large scale language model create textual perturbations simultaneously perform data augmentation experiment diverse classification task show method hugely outperform exist text augmentation methods ablation study qualitative analysis provide insights approach
humans learn new language task efficiently machine conceivably leverage prior experience knowledge learn task paper explore whether cross task generalization ability acquire apply build better shoot learners across diverse nlp task introduce crossfit task setup study cross task shoot learn ability standardize see unseen task split data access different learn stag evaluation protocols addition present nlp shoot gym repository one hundred and sixty shoot nlp task cover diverse task categories applications convert unify text text format empirical analysis reveal shoot learn ability unseen task improve via upstream learn stage use set see task additionally advantage last medium resource scenarios thousands train examples available also observe selection upstream learn task significantly influence shoot performance unseen task ask analysis task similarity transferability
current nlp model predominantly train pretrain finetune pipeline model first pretrained large text corpus mask language model mlm objective finetuned downstream task prior work show insert intermediate pre train phase heuristic mlm objectives resemble downstream task significantly improve final performance however still unclear one case intermediate pre train helpful two whether hand craft heuristic objectives optimal give task three whether mlm policy design one task generalizable beyond task paper perform large scale empirical study investigate effect various mlm policies intermediate pre train crucially introduce methods automate discovery optimal mlm policies learn mask model either direct supervision meta learn downstream task investigate effect use heuristic directly supervise meta learn mlm policies intermediate pretraining eight select task across three categories close book qa knowledge intensive language task abstractive summarization notably show learn mask policies outperform heuristic mask name entities triviaqa mask policies learn one task positively transfer task certain case
grow quantity complexity data pose challenge humans consume information respond timely manner businesses domains rapidly change rule regulations failure identify change costly contrast expert analysis development domain specific ontology taxonomies use task base approach fulfil specific information need within new domain specifically propose extract task base information incoming instance data pipeline construct state art nlp technologies include bi lstm crf model entity extraction attention base deep semantic role label automate verb base relationship extractor use automatically extract instance level semantic structure instance combine larger domain specific knowledge graph produce new timely insights preliminary result validate manually show methodology effective extract specific information complete end use case
order interpret communicative intents utterance need ground something outside language ground world modalities paper argue dialogue clarification mechanisms make explicit process interpret communicative intents speaker utterances ground various modalities dialogue situate paper frame dialogue clarification mechanisms understudy research problem key miss piece giant jigsaw puzzle natural language understand discuss theoretical background practical challenge pose problem propose recipe obtain ground annotations conclude highlight ethical issue need address future work
deep model improve state art supervise unsupervised learn example deep embed cluster dec greatly improve unsupervised cluster performance use stack autoencoders representation learn however one weakness deep model local neighborhood structure original space necessarily preserve latent space preserve local geometry various methods propose supervise semi supervise learn literature eg spectral cluster label propagation use graph laplacian regularization paper combine strength deep representation learn measure propagation mp kl divergence base graph regularization method originally use semi supervise scenario main assumption mp two data point close original space likely belong class measure kl divergence class membership distribution take assumption unsupervised learn scenario propose deep embed cluster aid measure propagation decamp model evaluate decamp short text cluster task three public datasets decamp perform competitively state art baselines include baselines use additional data generate word embeddings use cluster process example stackoverflow dataset decamp achieve cluster accuracy seventy-nine five higher exist baselines empirical result suggest decamp effective method unsupervised learn
paper describe system submit team biggreen lcp two thousand and twenty-one predict lexical complexity english word give context assemble feature engineer base model deep neural network model found bert bert perform competitively feature engineer base model help extreme case eg separate instance easy neutral difficulty handcraft feature comprise breadth lexical semantic syntactic novel phonological measure visualizations bert attention map offer insight potential feature transformers model may learn fine tune lexical complexity prediction ensembled predictions score reasonably well single word subtask demonstrate harness perform well multi word expression subtask
scatter factor circular universality firstly introduce barker et al two thousand and twenty word w call k universal natural number k every word length k w alphabet occur scatter factor w call circular k universal conjugate w k universal word youu1cdots un call scatter factor w obtain w delete part w ie exist possibly empty word v1dotsvn1 wv1u1v2cdots vnunvn1 work prove two problems leave open aforementioned paper namely generalisation one main theorems arbitrary alphabets slight modification another theorem characterise circular universality universality way present deep insights behaviour remainder call arch factorisation hebrard repetitions word consider
traditional goal orient dialogue systems rely various components natural language understand dialogue state track policy learn response generation train component require annotations hard obtain every new domain limit scalability systems similarly rule base dialogue systems require extensive write maintenance rule scale either end end dialogue systems hand require module specific annotations need large amount data train overcome problems demo present alexa conversations new approach build goal orient dialogue systems scalable extensible well data efficient components system train data drive manner instead collect annotate conversations train generate use novel dialogue simulator base seed dialogues specifications apis entities provide developer approach provide box support natural conversational phenomena like entity share across turn users change mind conversation without require developers provide dialogue flow exemplify approach use simple pizza order task showcase value reduce developer burden create robust experience finally evaluate system use typical movie ticket book task show dialogue simulator essential component system lead fifty improvement turn level action signature prediction accuracy
existence multiple datasets sarcasm detection prompt us apply transfer learn exploit commonality adversarial neural transfer ant framework utilize multiple loss term encourage source domain target domain feature distributions similar optimize domain specific performance however objectives may conflict lead optimization difficulties sometimes diminish transfer propose generalize latent optimization strategy allow different losses accommodate improve train dynamics propose method outperform transfer learn meta learn baselines particular achieve one thousand and two absolute performance gain previous state art isarcasm dataset
causal inference process capture effect relationship among variables exist work focus deal structure data mine causal relationship among factor unstructured data like text less examine great importance especially legal domain paper propose novel graph base causal inference gci framework build causal graph fact descriptions without much human involvement enable causal inference facilitate legal practitioners make proper decisions evaluate framework challenge similar charge disambiguation task experimental result show gci capture nuance fact descriptions among multiple confuse charge provide explainable discrimination especially shoot settings also observe causal knowledge contain gci effectively inject powerful neural network better performance interpretability
extract temporal relations eg concurrent among events crucial natural language understand previous study mainly rely neural network learn effective feature manual craft linguistic feature temporal relation extraction usually fail context two events complex wide inspire examination available temporal relation annotations human like cognitive procedures propose new temporal graph transformer network one explicitly find connection two events syntactic graph construct one two continuous sentence two automatically locate indicative temporal cue path two event mention well surround concepts syntactic graph new temporal orient attention mechanism experiment matres tb dense datasets show approach significantly outperform previous state art methods end end temporal relation extraction temporal relation classification
work show process build large scale train set digital digitize collections national library result bidirectional encoder representations transformers bert base language model norwegian outperform multilingual bert mbert model several token sequence classification task norwegian bokmaal norwegian nynorsk model also improve mbert performance languages present corpus english swedish danish languages include corpus weight degrade moderately keep strong multilingual properties therefore show build high quality model within memory institution use somewhat noisy optical character recognition ocr content feasible hope pave way memory institutions follow
news article revision histories potential give us novel insights across vary field linguistics social sciences work present knowledge first publicly available dataset news article revision histories textitnewsedits dataset multilingual contain one million, two hundred and seventy-eight thousand, eight hundred and four article four million, six hundred and nine thousand, four hundred and thirty versions twenty-two english french language newspaper source base three countries across version pair count one hundred and nine million add sentence eighty-nine million change sentence sixty-eight million remove sentence within change sentence derive seventy-two million atomic edit textitnewsedits knowledge largest corpus revision histories domain
journalists obtain lead story ideas read large corpora government record court case propose bill etc however small percentage record interest document propose model newsworthiness aim surface interest document train model automatically label corpora publish newspaper article predict whether article front page article ie textbfnewsworthy ie textbfless newsworthy transfer model unlabeled corpora court case bill city council meet minutes rank document corpora newsworthiness fine tune roberta model achieve ninety-three auc performance heldout label document eighty-eight auc expert validate unlabeled corpora provide interpretation visualization model
sentiment prediction remain challenge unresolved task various research field include psychology neuroscience computer science stem high degree subjectivity limit input source effectively capture actual sentiment even challenge text base input meanwhile rise deep learn unprecedented large volume data pave way artificial intelligence perform impressively accurate predictions even human level reason draw inspiration propose coverage base sentiment subsentence extraction system estimate span input text recursively feed information back network predict subsentence consist auxiliary information express sentiment important build block enable vivid epic sentiment delivery within scope paper natural language process task text summarisation qanda approach outperform state art approach large margin subsentence prediction ie average jaccard score seventy-two eighty-nine evaluation design rigorous experiment consist twenty-four ablation study finally learn lessons return community share software package public dataset reproduce result present paper
recent years online shop gain momentum become important venue customers wish save time simplify shop process key advantage shop online ability read customers say products interest work aim maintain advantage situations extreme brevity need example shop voice suggest novel task extract single representative helpful sentence set review give product select sentence meet two condition first helpful purchase decision second opinion express support multiple reviewers task closely relate task multi document summarization product review domain differ objective level conciseness collect dataset english sentence helpfulness score via crowd source demonstrate reliability despite inherent subjectivity involve next describe complete model extract representative helpful sentence positive negative sentiment towards product demonstrate outperform several baselines
classify seven months worth belgian covid relate tweet use multilingual bert relate governments covid measure classify tweet state opinion belgian government curfew measure strict ok loose examine change topics discuss view express time reference date relate events implementation new measure covid nineteen relate announcements media
use crowdworkers nlp research grow rapidly tandem exponential increase research production machine learn ai ethical discussion regard use crowdworkers within nlp research community typically confine scope issue relate labor condition fair pay draw attention lack ethical considerations relate various task perform workers include label evaluation production find final rule common ethical framework use researchers anticipate use online crowdsourcing platforms data collection result gap spirit practice human subject ethics nlp research enumerate common scenarios crowdworkers perform nlp task risk harm thus recommend researchers evaluate risk consider three ethical principles set belmont report also clarify common misconceptions regard institutional review board irb application hope paper serve reopen discussion within community regard ethical use crowdworkers
recently significant advance neural methods tackle knowledge intensive task open domain question answer qa advance fuel combine large pre train language model learnable retrieval document majority model use separate encoders learn query representation passage representation retriever additional encoder downstream task use separate encoders stage task occupy lot memory make difficult scale large number task paper propose novel retrieval optimize multi task rom framework jointly train self supervise task knowledge retrieval extractive question answer rom approach present unify generalizable framework enable scale efficiently multiple task vary level supervision optimization choices different learn schedule without change model architecture also provide flexibility change encoders without change architecture system use framework achieve comparable better performance recent methods qa drastically reduce number parameters
automatic unreliable news detection research problem great potential impact recently several paper show promise result large scale news datasets model use article without resort fact check mechanism retrieve support evidence work take closer look datasets provide valuable resources future research observe number problems may lead result generalize realistic settings specifically show selection bias data collection lead undesired artifacts datasets addition systems train predict level individual article overlap article source train evaluation data provide strong confound factor model exploit presence confound factor model achieve good performance directly memorize site label map instead model real task unreliable news detection observe significant drop ten accuracy model test clean split train test source overlap use observations experimental result provide practical suggestions create reliable datasets unreliable news detection task suggest future dataset creation include simple model difficulty bias probe future model development use clean non overlap site date split
integrate external knowledge commonsense reason task show progress resolve knowledge gap task knowledge integration yield peak performance critical select knowledge graph kg well align give task objective present approach assess well candidate kg correctly identify accurately fill gap reason task call kg task match show kg task match three phase knowledge task identification knowledge task alignment knowledge task integration also analyze transformer base kg task model via commonsense probe measure much knowledge capture model kg integration empirically investigate kg match socialiqa siqa sap et al 2019b physical iqa piqa bisk et al two thousand and twenty mcscript20 ostermann et al two thousand and nineteen datasets three diverse kgs atomic sap et al 2019a conceptnet speer et al two thousand and seventeen automatically construct instructional kg base wikihow koupaee wang two thousand and eighteen methods able demonstrate atomic event inference focus kg best match siqa mcscript20 taxonomic conceptnet wikihow base kgs best match piqa across three analysis phase verify methods find human evaluation
paper describe method use transformer base language model tlms understand public opinion social media post approach train set gpt model several covid nineteen tweet corpora reflect populations users distinctive view use prompt base query probe model reveal insights bias opinions users demonstrate approach use produce result resemble poll public diverse social political public health issue result covid nineteen tweet data show transformer language model promise tool help us understand public opinions social media scale
human semantic cognition proper name name refer individual entities harder learn retrieve common nouns seem case machine learn algorithms linguistic distributional reason behaviour investigate depth far tackle issue show semantic distinction proper name common nouns reflect linguistic distributions employ original task distributional semantics doppelganger test extensive set model new dataset novel aficionados dataset result indicate distributional representations different individual entities less clearly distinguishable common nouns outcome intriguingly mirror human cognition
image collection object attribute represent web relationships among interconnect object scene graph emerge new modality structure graphical representation image scene graph encode object nod connect via pairwise relations edge support question answer scene graph propose graphvqa language guide graph neural network framework translate execute natural language question multiple iterations message pass among graph nod explore design space graphvqa framework discuss trade different design choices experiment gqa dataset show graphvqa outperform state art model large margin eight thousand, eight hundred and forty-three vs nine thousand, four hundred and seventy-eight
product descriptions e commerce websites often suffer miss important aspects clarification question generation cqgen promise approach help alleviate problem unlike traditional qgen assume existence answer context generate question accordingly cqgen mimic user behaviors ask unstated information generate cqs serve sanity check proofread help e commerce merchant identify potential miss information advertise product improve consumer experience consequently due variety possible user background use case information need quite diverse also specific detail topic previous work assume generate one cq per context result tend generic thus propose task diverse cqgen also tackle challenge specificity propose new model name kpcnet generate cqs keyword prediction condition deal task automatic human evaluation two datasets home kitchen office show kpcnet generate specific question promote better group level diversity several compete baselines
paper describe magicpai system semeval two thousand and twenty-one task seven hahackathon detect rat humor offense task aim detect whether text humorous humorous four subtasks competition paper mainly present solution multi task learn model base adversarial examples task 1a 1b specifically first vectorize clean dataset add perturbation obtain robust embed representations correct loss via confidence level finally perform interactive joint learn multiple task capture relationship whether text humorous humorous final result show effectiveness system
question answer semi structure table see semantic parse task significant practical push boundary natural language understand exist research mainly focus understand content unstructured evidence eg news natural language sentence document task verification structure evidence table chart databases still less explore paper describe sattiy team system semeval two thousand and twenty-one task nine statement verification evidence find table sem tab fact competition aim verify statements find evidence table scientific article promote proper interpretation surround article paper exploit ensemble model pre train language model table tapas tabert task adjust result base rule extract task b finally leaderboard attain f1 score eight thousand, four hundred and ninety-six seven thousand, seven hundred and thirty-two task two way three way evaluation respectively f1 score four thousand, eight hundred and fifty-six task b
paper present pali team win system semeval two thousand and twenty-one task two multilingual cross lingual word context disambiguation fine tune xlm roberta model solve task word context disambiguation ie determine whether target word two contexts contain mean implementation first specifically design input tag emphasize target word contexts second construct new vector fine tune embeddings xlm roberta fee fully connect network output probability whether target word context mean new vector attain concatenate embed cls token embeddings target word contexts train explore several trick ranger optimizer data augmentation adversarial train improve model prediction consequently attain first place four cross lingual task
work nlp make assumption desirable develop solutions native language question consequently strong trend towards build native language model even low resource languages paper question development explore idea simply translate data english thereby enable use pretrained large scale english language model demonstrate empirically large english language model couple modern machine translation outperform native language model scandinavian languages exception finnish assume due inferior translation quality result suggest machine translation mature technology raise serious counter argument train native language model low resource languages paper therefore strive make provocative important point english language model improve unprecedented pace turn improve machine translation empirical environmental stand point effective translate data low resource languages english build language model languages
twitter sentiment analysis often focus predict polarity tweet attract increase attention last years particular rise deep learn dl paper propose new task predict predominant sentiment among first order reply give tweet therefore create retweet large dataset tweet reply manually annotate sentiment label strong baseline propose two stage dl base method first create automatically label train data apply standard sentiment classifier tweet reply aggregate predictions original tweet rationale individual errors make classifier likely cancel aggregation step second use automatically label data supervise train neural network predict reply sentiment original tweet result classifier evaluate new retweet dataset show promise result especially consider train without manually label data dataset baseline implementation publicly available
bidirectional mask transformers become core theme current nlp landscape despite impressive benchmarks recur theme recent research question model capacity syntactic generalization work seek address question add supervise token level supertagging objective standard unsupervised pretraining enable explicit incorporation syntactic bias network train dynamics approach straightforward implement induce marginal computational overhead general enough adapt variety settings apply methodology lassy large automatically annotate corpus write dutch experiment suggest syntax aware model perform par establish baselines despite lassy large one order magnitude smaller commonly use corpora
recent years natural language process nlp model achieve phenomenal success linguistic semantic task like text classification machine translation cognitive dialogue systems information retrieval via natural language understand nlu natural language generation nlg feat primarily attribute due seminal transformer architecture lead design bert gpt ii iii etc although large size model achieve unprecedented performances come high computational cost consequently recent nlp architectures utilize concepts transfer learn prune quantization knowledge distillation achieve moderate model size keep nearly similar performances achieve predecessors additionally mitigate data size challenge raise language model knowledge extraction perspective knowledge retrievers build extricate explicit data document large corpus databases greater efficiency accuracy recent research also focus superior inference provide efficient attention longer input sequence paper summarize examine current state art sota nlp model employ numerous nlp task optimal performance efficiency provide detail understand function different architectures taxonomy nlp design comparative evaluations future directions nlp
though pre train language model bert xlnet rapidly advance state art many nlp task implicit semantics rely surface information word corpus intuitively background knowledge influence efficacy understand inspire common sense focus improve model pretraining leverage explicit knowledge different recent research optimize pretraining model knowledge mask strategies propose simple general method combine explicit knowledge pretraining specific first match knowledge facts knowledge graph kg add knowledge injunction layer transformer directly without change architecture present study seek find direct impact explicit knowledge transformer per train conduct experiment various datasets different downstream task experimental result show solely add external knowledge transformer improve learn performance many nlp task
international classification disease icd cod procedure refer tag medical note diagnosis cod show effective crucial bill system medical sector currently icd cod assign clinical note manually likely many errors moreover train skilled coders also require time human resources therefore automate icd code determination process important task advancement artificial intelligence theory computational hardware machine learn approach emerge suitable solution automate process project apply transformer base architecture capture interdependence among tokens document use code wise attention mechanism learn code specific representations entire document finally feed separate dense layer correspond code prediction furthermore handle imbalance code frequency clinical datasets employ label distribution aware margin ldam loss function experimental result mimic iii dataset show propose model outperform baselines significant margin particular best set achieve micro auc score nine hundred and twenty-three compare eight hundred and sixty-eight bidirectional recurrent neural network also show use code wise attention mechanism model provide insights prediction thus support clinicians make reliable decisions code available online https githubcom biplob1ly transicd
classification model use input data predict likelihood subsequent input data fall predetermine categories perform effective classifications model require large datasets train become common practice utilize synthetic data boost performance machine learn model report use synthetic data build model detect problems rarely occur example create synthetic data help model identify deteriorate oil line common practice machine learn practitioners generate synthetic data rotate flip crop image increase volume image data train convolutional neural network purpose paper explore create utilize synthetic nlp data improve performance natural language process machine learn classification model paper use yelp pizza restaurant review dataset transfer learn fine tune pre train gpt two transformer model generate synthetic pizza review data combine synthetic data original genuine data create new joint dataset new combine model significantly outperform original model accuracy precision
social scientists psychologists take interest understand people express emotions sentiments deal catastrophic events natural disasters political unrest terrorism covid nineteen pandemic catastrophic event raise number psychological issue depression give abrupt social change lack employment rise covid nineteen case stricter lock down people express sentiments social media provide deep understand people physiologically react catastrophic events paper use deep learn base language model via long short term memory lstm recurrent neural network sentiment analysis twitter focus rise novel case india use lstm model global vector glove word representation build language model review sentiments express selective months cover major peak new case two thousand and twenty present framework focus multi label sentiment classification use lstm model glove embed one sentiment express result show majority tweet positive high level optimism rise covid nineteen case india find number tweet significantly lower towards peak new case find optimistic joke tweet mostly dominate monthly tweet much lower number negative sentiments express could imply majority generally positive annoy towards way pandemic handle authorities peak reach
automatically recommend relevant law article give legal case attract much attention greatly release human labor search large database laws however current research support coarse grain recommendation relevant article predict whole without explain specific fact article relevant since one case form many support facts traverse verify correctness recommendation result time consume believe learn fine grain correspondence single fact law article crucial accurate trustworthy ai system motivation perform pioneer study create corpus manually annotate fact article correspondences treat learn text match task propose multi level match network address help model better digest content law article parse article form premise conclusion pair random forest experiment show parse form yield better performance result model surpass popular text match baselines furthermore compare previous research find establish fine grain fact article correspondences improve recommendation accuracy large margin best system reach f1 score nine hundred and sixty-three make great potential practical use also significantly boost downstream task legal decision prediction increase f1 score one hundred and twenty-seven
covid nineteen devastate world since end two thousand and nineteen continue play significant role major national worldwide events consequently news wake leave life unaffected earn world attention social media platforms serve vehicle global conversation covid nineteen particular many people use sit order express feel experience observations pandemic provide multi faceted analysis critical properties exhibit conversations social media regard novel coronavirus pandemic present framework analysis mine track critical content characteristics social media conversations around pandemic focus twitter reddit gather large scale dataset covid nineteen social media conversations analyse cover track potential report virus acquisition symptoms conversation topics language complexity measure time region across unite state also present bert base model recognize instance hateful tweet covid nineteen conversations achieve lower error rate state art performance result provide empirical validation effectiveness propose framework demonstrate social media data efficiently leverage provide public health experts inexpensive thorough insight course outbreak
memory constrain settings like iot devices network data pipelines advantageous smaller contextual embeddings investigate efficacy project contextual embed data bert onto manifold use nonlinear dimensionality reduction techniques compress embeddings particular propose novel post process approach apply combination isomap pca find geodesic distance estimations estimate shortest path riemannian manifold isomap k nearest neighbor graph bolster performance compress embeddings comparable original bert embeddings one dataset find despite twelve fold dimensionality reduction compress embeddings perform within one original bert embeddings downstream classification task addition find approach work particularly well task reliant syntactic data compare linear dimensionality reduction result show promise novel geometric approach achieve lower dimensional text embeddings exist transformers pave way data specific application specific embed compressions
strategies improve train prediction quality weakly supervise machine learn model vary much tailor specific task integrate specific model architecture work propose software framework knodle treat weak data annotations deep learn model methods improve weakly supervise train separate modular components standardize interfaces independent part account data model agnostic weak supervision method development still allow train process access fine grain information data set characteristics match heuristic rule well elements deep learn model ultimately use prediction hence framework encompass wide range train methods improve weak supervision range methods look correlations rule output class independently machine learn model train result label harness interplay neural network weakly label data illustrate benchmarking potential framework performance comparison several reference implementations selection datasets already available knodle
currently widespread neural network architecture train language model call bert lead improvements various nlp task general larger number parameters bert model better result obtain nlp task unfortunately memory consumption train duration drastically increase size model though article investigate various train techniques smaller bert model evaluate five public german ner task two introduce article combine different methods bert variants like albert roberta relative positional encode addition propose two new fine tune techniques lead better performance cse tag modify form lcrf furthermore introduce new technique call wwa reduce bert memory usage lead small increase performance
argue explainable artificial intelligence must possess rationale decisions able infer purpose observe behaviour able explain decisions context audience understand intend address issue present four novel contributions firstly define arbitrary task term perceptual state discuss two extremes domain possible solutions secondly define intensional solution optimal definitions intelligence describe purpose task agent possess rationale decisions term purpose express perceptual symbol system ground hardware thirdly communicate rationale require natural language mean encode decode perceptual state propose theory mean acquire language agent model world language describe rather language utterances humans predictive value agent goals agent imbue utterances mean term goals perceptual state context peircean semiotics community agents must share rough approximations sign referents interpretants order communicate mean exist context intent communicate humans agent must comparable experience goals agent learn intensional solutions compel objective function somewhat analogous human motivators hunger pain may capable explain rationale term intent term audience understand intend form approximation perceptual state humans
recently research mental health condition use public online data include reddit surge nlp health research report user characteristics important judge generalisability find paper show exist nlp methods yield information clinical demographic identity characteristics almost 20k reddit users self report bipolar disorder diagnosis population consist slightly feminine masculine gendered mainly young middle age us base adults often report additional mental health diagnose compare general reddit statistics epidemiological study additionally paper carefully evaluate methods discuss ethical issue
social media contain unfiltered unique information potentially great value case misinformation also great harm regard biomedical topics false information particularly dangerous methods automatic fact check fake news detection address problem apply biomedical domain social media yet aim fill research gap annotate corpus one thousand, two hundred tweet implicit explicit biomedical claim latter also span annotations claim phrase corpus sample relate covid nineteen measles cystic fibrosis depression develop baseline model detect tweet contain claim automatically analyse reveal biomedical tweet densely populate claim forty-five corpus sample contain one thousand, two hundred tweet focus domains mention baseline classification experiment embed base classifiers bert base transfer learn demonstrate detection challenge however show acceptable performance identification explicit expressions claim implicit claim tweet challenge detect
recent neural base aspect base sentiment analysis approach though achieve promise improvement benchmark datasets report suffer poor robustness encounter confounder non target aspects paper take causal view address issue propose simple yet effective method namely sentiment adjustment senta apply backdoor adjustment disentangle confound factor experimental result aspect robustness test set arts dataset demonstrate approach improve performance maintain accuracy original test set
memo describe ntr tsu submission sigtyp two thousand and twenty-one share task predict language ids speech speak language identification lid important step multilingual automate speech recognition asr system pipeline many low resource endanger languages single speaker record may available demand need domain speaker invariant language id systems memo show convolutional neural network self attentive pool layer show promise result language identification task
disinformation fake news pose detrimental effect individuals society recent years attract broad attention fake news detection majority exist fake news detection algorithms focus mine news content surround exogenous context discover deceptive signal endogenous preference user decide spread piece fake news ignore confirmation bias theory indicate user likely spread piece fake news confirm exist beliefs preferences users historical social engagements post provide rich information users preferences toward news great potential advance fake news detection however work explore user preference fake news detection somewhat limit therefore paper study novel problem exploit user preference fake news detection propose new framework upfd simultaneously capture various signal user preferences joint content graph model experimental result real world datasets demonstrate effectiveness propose framework release code data benchmark gnn base fake news detection https githubcom safe graph gnn fakenews
massive open online course moocs become popular choice e learn thank great flexibility however due large number learners diverse background tax offer real time support learners may post feel confusion struggle respective mooc forums large volume post high workloads mooc instructors unlikely instructors identify learners require intervention problem study natural language process nlp problem recently know challenge due imbalance data complex nature task paper explore first time bayesian deep learn learner base text post two methods monte carlo dropout variational inference new solution assess need instructor interventions learner post compare model base propose methods probabilistic model baseline non bayesian model similar circumstances different case apply prediction result suggest bayesian deep learn offer critical uncertainty measure supply traditional neural network add explainability trust robustness ai crucial education base applications additionally achieve similar better performance compare non probabilistic neural network well grant lower variance
major challenge research non english machine read question answer qa lack annotate datasets paper present germanquad dataset thirteen thousand, seven hundred and twenty-two extractive question answer pair improve reproducibility dataset creation approach foster qa research languages summarize lessons learn evaluate reformulation question answer pair way speed annotation process extractive qa model train germanquad significantly outperform multilingual model also show machine translate train data fully substitute hand annotate train data target language finally demonstrate wide range applications germanquad adapt germandpr train dataset dense passage retrieval dpr train evaluate first non english dpr model
infographics document design effectively communicate information use combination textual graphical visual elements work explore automatic understand infographic image use visual question answer techniqueto end present infographicvqa new dataset comprise diverse collection infographics along natural language question answer annotations collect question require methods jointly reason document layout textual content graphical elements data visualizations curate dataset emphasis question require elementary reason basic arithmetic skills finally evaluate two strong baselines base state art multi modal vqa model establish baseline performance new task dataset code leaderboard make available http docvqaorg
ontologies comprise concepts attribute relationships form quintessential backbone many knowledge base ai systems systems manifest form question answer dialogue number business analytics master data management applications efforts towards populate domain specific ontologies examine role document structure learn ontological relationships concepts document corpus inspire ideas hypernym discovery explainability method perform fifteen point accurate stand alone r gcn model task
aim expand shoot relations coverage knowledge graph kgs shoot knowledge graph completion fkgc recently gain research interest exist model employ shoot relation multi hop neighbor information enhance semantic representation however noise neighbor information might amplify neighborhood excessively sparse neighbor available represent shoot relation moreover model infer complex relations one many one n many one n one many many n n previous knowledge graph completion approach require high model complexity large amount train instance thus infer complex relations shoot scenario difficult fkgc model due limit train instance paper propose shoot relational learn global local framework address issue global stage novel gate attentive neighbor aggregator build accurately integrate semantics shoot relation neighborhood help filter noise neighbor even kg contain extremely sparse neighborhoods local stage meta learn base transh mtransh method design model complex relations train model shoot learn fashion extensive experiment show model outperform state art fkgc approach frequently use benchmark datasets nell one wiki one compare strong baseline model metar model achieve five shoot fkgc performance improvements eighty nell one twenty-eight wiki one metric hits10
take first step address task automatically generate shellcodes ie small piece code use payload exploitation software vulnerability start natural language comment assemble release novel dataset shellcodeia32 consist challenge common assembly instructions natural language descriptions experiment standard methods neural machine translation nmt establish baseline performance level task
build natural language understand nlu modules task orient speak dialogue systems sds involve definition intents entities collection task relevant data annotate data intents entities repeat process add functionality enhancement sds work showcase intent bulk label system sds developers interactively label augment train data unlabeled utterance corpora use advance cluster visual label methods extend deep align cluster work better backbone bert model explore techniques select seed data label develop data balance method use oversampling technique utilize paraphrase model also look effect data augmentation cluster process result show achieve ten gain cluster accuracy datasets use combination techniques finally extract utterance embeddings cluster model plot data interactively bulk label sample reduce time effort data label whole dataset significantly
record clinical encounter extensive complex thus place premium tool extract summarize relevant information paper introduce task generate discharge summaries clinical encounter summaries set need faithful traceable scale multiple long document motivate use extract abstract summarization cascade introduce two new measure faithfulness hallucination rate evaluation task complement exist measure fluency informativeness result across seven medical section five model show summarization architecture support traceability yield promise result sentence rewrite approach perform consistently measure use faithfulness faithfulness adjust f3 diverse range generate section
automate metaphor detection challenge task identify metaphorical expressions word sentence tackle problem adopt pre train contextualized model eg bert roberta end propose novel metaphor detection model namely metaphor aware late interaction bert melbert model leverage contextualized word representation also benefit linguistic metaphor identification theories distinguish contextual literal mean word empirical result demonstrate melbert outperform several strong baselines four benchmark datasets ie vua eighteen vua twenty moh x trofi
recommender systems assist legal professionals find relevant literature support case despite importance profession legal applications reflect latest advance recommender systems representation learn research simultaneously legal recommender systems typically evaluate small scale user study without public available benchmark datasets thus study limit reproducibility address gap research practice explore set state art document representation methods task retrieve semantically relate us case law evaluate text base eg fasttext transformers citation base eg deepwalk poincar e hybrid methods compare total twenty-seven methods use two silver standards annotations two thousand, nine hundred and sixty-four document silver standards newly create open case book wikisource reuse open license facilitate reproducibility experiment show document representations average fasttext word vectors train legal corpora yield best result closely follow poincar e citation embeddings combine fasttext poincar e hybrid manner improve overall result besides overall performance analyze methods depend document length citation count coverage recommendations make source code model datasets publicly available https githubcom malteos legal document similarity
unsupervised image caption challenge task aim generate caption without supervision image sentence pair image sentence draw different source object label detect image previous work pseudo caption ie sentence contain detect object label assign give image focus previous work alignment input image pseudo caption sentence level however pseudo caption contain many word irrelevant give image work investigate effect remove mismatch word image sentence alignment determine make task difficult propose simple gate mechanism train align image feature reliable word pseudo caption detect object label experimental result show propose method outperform previous methods without introduce complex sentence level learn objectives combine sentence level alignment method previous work method improve performance result confirm importance careful alignment word level detail
naturally occur bracket answer fragment natural language question hyperlinks webpages reflect human syntactic intuition regard phrasal boundaries availability approximate correspondence syntax make appeal distant information source incorporate unsupervised constituency parse noisy incomplete address challenge develop partial bracket aware structure ramp loss learn experiment demonstrate distantly supervise model train naturally occur bracket data accurate induce syntactic structure compete unsupervised systems english wsj corpus model achieve unlabeled f1 score six hundred and eighty-nine constituency parse
recently much progress natural language process drive deep contextualized representations pretrained large corpora typically fine tune pretrained model specific downstream task base single view learn however inadequate sentence interpret differently different perspectives therefore work propose text text multi view learn framework incorporate additional view text generation view typical single view passage rank model empirically propose approach help rank performance compare single view counterpart ablation study also report paper
extract pattern useful information natural language datasets challenge task especially deal data write language different english like italian machine deep learn together natural language process nlp techniques widely spread improve lately provide plethora useful methods address supervise unsupervised problems textual information propose reckonition nlp base system industrial accidents work prevention reckonition mean provide natural language understand cluster inference result joint partnership italian national institute insurance accidents work inail obtain result show ability process textual data write italian describe industrial accidents dynamics consequences
investigate inference variable length cod domains computer science noisy information transmission information retrieval storage topics traditionally mostly constant length codewords act study rely upon two concepts independent close set focus word relations whose image compute apply peculiar combinations deletion insertion substitution particular characterizations variable length cod maximal families tau independent tau close cod provide
introduce dynabench open source platform dynamic dataset creation model benchmarking dynabench run web browser support human model loop dataset creation annotators seek create examples target model misclassify another person paper argue dynabench address critical need community contemporary model quickly achieve outstanding performance benchmark task nonetheless fail simple challenge examples falter real world scenarios dynabench dataset creation model development model assessment directly inform lead robust informative benchmarks report four initial nlp task illustrate concepts highlight promise platform address potential objections dynamic benchmarking new standard field
researcher write good research statement crucial cost lot time effort help researchers paper propose research statement generation rsg task aim summarize one research achievements help prepare formal research statement task conduct comprehensive attempt include corpus construction method design performance evaluation first construct rsg dataset sixty-two research statements correspond one thousand, two hundred and three publications due limitation resources propose practical rsg method identify researcher research directions topic model cluster techniques extract salient sentence neural text summarizer finally experiment show method outperform baselines better content coverage coherence
large pre train language model lms demonstrate remarkable ability shoot learners however success hinge largely scale model parameters degree make challenge train serve paper propose new approach name efl turn small lms better shoot learners key idea approach reformulate potential nlp task entailment one fine tune model little eight examples demonstrate propose method naturally combine unsupervised contrastive learn base data augmentation method ii easily extend multilingual shoot learn systematic evaluation eighteen standard nlp task demonstrate approach improve various exist sota shoot learn methods twelve yield competitive shoot performance five hundred time larger model gpt three
present zero resource speech challenge two thousand and twenty-one ask participants learn language model directly audio without text label challenge base libri light dataset provide 60k hours audio english audio book without associate text provide pipeline baseline system consist encoder base contrastive predictive cod cpc quantizer k mean standard language model bert lstm metrics evaluate learn representations acoustic abx discrimination lexical spot word syntactic acceptability judgment semantic level similarity judgment present overview eight submit systems four group discuss main result
automatic hate speech detection online social network important open problem natural language process nlp hate speech multidimensional issue strongly dependant language cultural factor despite relevance research topic almost exclusively devote english supervise learn resources label datasets nlp tool create language consider large portion users worldwide speak languages english important need create efficient approach multilingual hate speech detection work propose address problem multilingual hate speech detection perspective transfer learn goal determine knowledge one particular language use classify language determine effective ways achieve propose hate specific data representation evaluate effectiveness general purpose universal representations unlike propose model train massive amount data focus cross lingual set one need classify hate speech one language without access label data language show use simple yet specific multilingual hate representations improve classification result explain qualitative analysis show specific representation able capture common pattern hate speech present different languages proposal constitute best knowledge first attempt construct multilingual specific task representations despite simplicity model outperform previous approach experimental setups find orient future solutions toward use domain specific representations
knowledge representation learn receive lot attention past years success exist methods heavily rely quality knowledge graph entities triplets tend learn less expressive power fortunately many knowledge graph construct various source representations could contain much information propose adversarial embed transfer network atransn transfer knowledge one teacher knowledge graph target one align entity set without explicit data leakage specifically add soft constraints align entity pair neighbour exist knowledge representation learn methods handle problem possible distribution differences teacher target knowledge graph introduce adversarial adaption module discriminator module evaluate degree consistency embeddings align entity pair consistency score use weight soft constraints necessary acquire relations triplets teacher knowledge graph utilize entity representations knowledge graph completion result show atransn achieve better performance baselines without transfer three datasets cn3l wk3l dwy100k ablation study demonstrate atransn bring steady consistent improvement different settings extension combine knowledge graph embed algorithms extension three teacher graph display promise generalization adversarial transfer network
current large scale language model politically bias result data train potentially cause serious problems deploy real world settings paper describe metrics measure political bias gpt two generation propose reinforcement learn rl framework mitigate political bias generate text use reward word embeddings classifier rl framework guide debiased generation without access train data require model retrain empirical experiment three attribute sensitive political bias gender location topic methods reduce bias accord metrics human evaluation maintain readability semantic coherence
recent work show graph structure sentence generate dependency parsers potential improve event detection however often leverage edge dependencies word discard dependency label eg nominal subject treat underlie graph edge homogeneous work propose novel framework incorporate dependencies label use recently propose technique call graph transformer network gtn integrate gtns leverage dependency relations two exist homogeneous graph base model demonstrate improvement f1 score ace dataset
paper describe machine learn approach annotate analyze data curation work log icpsr large social sciences data archive systems study track curation work coordinate team decision make icpsr repository staff use systems organize prioritize document curation work do datasets make promise resources study curation work impact data reuse especially combination data usage analytics key challenge however classify similar activities measure associate impact metrics paper contribute one schema data curation activities two computational model identify curation action work log descriptions three analysis frequent data curation activities icpsr time first propose schema data curation action help us analyze impact curation work use schema annotate set data curation log contain record data transformations project management decisions complete repository staff finally train text classifier detect frequency curation action large set work log approach support analysis curation work document work log systems important step toward study relationship research data curation data reuse
natural language process nlp systems prove vulnerable backdoor attack whereby hide feature backdoors train language model may activate specific input call trigger trick model produce unexpected behaviors paper create covert natural trigger textual backdoor attack textithidden backdoors trigger fool modern language model human inspection deploy hide backdoors two state art trigger embed methods first approach via homograph replacement embed trigger deep neural network visual spoof lookalike character replacement second approach use subtle differences text generate language model real natural text produce trigger sentence correct grammar high fluency demonstrate propose hide backdoors effective across three downstream security critical nlp task representative modern human centric nlp systems include toxic comment detection neural machine translation nmt question answer qa two hide backdoor attack achieve attack success rate asr least ninety-seven injection rate three toxic comment detection nine hundred and fifty-one asr nmt less five inject data finally nine thousand, one hundred and twelve asr qa update twenty-seven poison data sample model previously train ninety-two thousand and twenty-four sample twenty-nine able demonstrate adversary high success rate attack maintain functionality regular users trigger inconspicuous human administrators
make online purchase become important customer read product review carefully make decision base however review lengthy may contain repeat sometimes irrelevant information help decision make paper introduce mrcbert novel unsupervised method generate summaries product review leverage machine read comprehension ie mrc approach extract relevant opinions generate rat wise aspect wise summaries review mrcbert show obtain reasonable performance use exist model transfer learn useful learn limit low resource scenarios demonstrate result review product electronics category amazon review dataset approach unsupervised require domain specific dataset product review dataset train fine tune instead use squad v11 dataset fine tune bert mrc task since mrcbert require task specific dataset easily adapt use domains
find appropriate word convey concepts ie lexical access essential effective communication reverse dictionaries fulfill need help individuals find word could relate specific concept idea best knowledge resource available persian language paper compare four different architectures implement persian reverse dictionary predict evaluate model use phraseword tuples extract persian dictionaries available online namely amid moein dehkhoda phrase describe word give phrase model suggest relevant word term ability convey concept model consider perform well correct word one top suggestions experiment show model consist long short term memory lstm units enhance additive attention mechanism enough produce suggestions comparable case better word original dictionary study also reveal model sometimes produce synonyms word output lead us introduce new metric evaluation reverse dictionaries call synonym accuracy account percentage time event produce word synonym occur assessment best model use new metric also indicate least sixty-two time produce accurate result within top one hundred suggestions
large scale pre train model like bert obtain great success various natural language process nlp task still challenge adapt math relate task current pre train model neglect structural feature semantic correspondence formula context address issue propose novel pre train model namely textbfmathbert jointly train mathematical formulas correspond contexts addition order capture semantic level structural feature formulas new pre train task design predict mask formula substructures extract operator tree opt semantic structural representation formulas conduct various experiment three downstream task evaluate performance mathbert include mathematical information retrieval formula topic classification formula headline generation experimental result demonstrate mathbert significantly outperform exist methods three task moreover qualitatively show pre train model effectively capture semantic level structural information formulas best knowledge mathbert first pre train model mathematical formula understand
end end approach sequence task become increasingly popular yet complex sequence task like speech translation systems cascade several model train sub task show superior suggest compositionality cascade systems simplify learn enable sophisticate search capabilities work present end end framework exploit compositionality learn searchable hide representations intermediate stag sequence model use decompose sub task hide intermediate improve use beam search enhance overall performance also incorporate external model intermediate stag network score adapt towards domain data one instance propose framework multi decoder model speech translation extract searchable hide intermediate speech recognition sub task model demonstrate aforementioned benefit outperform previous state art around six three bleu two test set fisher callhome around three four bleu english german english french test set must c
digital era almost every discipline people use automate systems generate information represent document format different natural languages result grow interest towards better solutions find organize analyze document paper propose system cluster amharic text document use encyclopedic knowledge ek neural word embed ek enable representation relate concepts neural word embed allow us handle contexts relatedness cluster process text document pass preprocessing stag enrich text document feature extract document map ek word embed model tf idf weight vector enrich feature generate finally text document cluster use popular spherical k mean algorithm propose system test amharic text corpus amharic wikipedia data test result show use ek word embed document cluster improve average accuracy use ek furthermore change size class significant effect accuracy
bag word model sense word multiple mean eg bank use either river bank institution sense represent probability distributions context word sense prevalence represent probability distribution sense may change time model measure kind sense change challenge due typically high dimensional parameter space sparse datasets recently publish corpus ancient greek texts contain expert annotate sense label select target word automatic sense annotation word kosmos mean decoration order world use test case recent work relate generative model monte carlo methods adapt exist generative sense change model develop simpler model main effect sense time give mcmc methods bayesian inference model efficient exist methods carry automatic sense annotation snippets contain kosmos use model measure time evolution three sense prevalence far aware first analysis data within class generative model consider quantify uncertainty return credible set evolve sense prevalence good agreement give expert annotation
important sociable recommendation dialog systems perform task content social content engage users gain favor addition understand user preferences provide satisfy recommendation systems must able generate coherent natural social conversations user traditional dialog state track apply systems track attribute social content address challenge propose deux novel attribute guide framework create better user experience accomplish movie recommendation task deux module keep track movie attribute eg favorite genres actorsetc user utterances system responses allow system introduce new movie attribute social content deux multiple value attribute type suit recommendation task since user may like multiple genres instance experiment suggest deux outperform baselines consistent fit user preferences better provide engage chat experience approach use similar problems sociable task orient dialog system
rise internet make major source information unfortunately information online true thus number fact check initiatives launch manual automatic present contribution regard whatthewikifact system automatic claim verification use wikipedia system predict veracity input claim show evidence retrieve part verification process show confidence score list relevant wikipedia article together detail information article include phrase use retrieve relevant sentence contain stances respect input claim associate probabilities
state art pre train model show memorise facts perform well limit amount train data gain better understand model learn study generalisation memorisation capabilities noisy low resource scenarios find train model almost unaffected label noise possible reach near optimal performances even extremely noisy datasets conversely also find completely fail test low resource task shoot learn rare entity recognition mitigate limitations propose novel architecture base bert prototypical network improve performance low resource name entity recognition task
many current artificial general intelligence agi natural language process nlp architectures possess general conversational intelligence either deal language unable convey knowledge form similar human language without manual labor intensive methods template base customization paper propose new technique automatically generate grammatically valid sentence use link grammar database natural language generation method far outperform current state art baselines may serve final component proto agi question answer pipeline understandably handle natural language material
technology assist review tar refer iterative active learn workflows document review high recall retrieval hrr task tar research commercial tar software apply linear model logistic regression support vector machine lexical feature transformer base model supervise tune find improve effectiveness many text classification task suggest use tar indeed find pre train bert model reduce review volume thirty tar workflows simulate rcv1 v2 newswire collection contrast find linear model outperform bert simulate legal discovery topics jeb bush e mail collection suggest match transformer pre train corpora task domain important generally appreciate additionally show right language model fine tune task collection start active learn critical little much fine tune result performance worse linear model even rcv1 v2
readers responses literature receive scant attention computational literary study rise social media offer opportunity capture segment responses data drive analysis responses provide new critical insight people read post discuss individual book goodreads social media platform host user discussions popular literature refer review consist plot summaries opinions quote mixture since review write readers computationally model allow one discover overall non professional discussion space work include aggregate summary work plot implicit rank importance events readers impressions main character develop pipeline interlock computational tool extract representation reader generate share narrative model use corpus review five popular novels discover readers distillation main storylines novel understand relative importance character well readers vary impressions character make three important contributions study infinite vocabulary network automatically derive narrative network include meta actants ii new sequence algorithm rev2seq generate consensus sequence events base partial trajectories aggregate review iii new impressions algorithm sent2imp provide finer non trivial multi modal insight readers opinions character
covid nineteen pandemic people start discuss pandemic relate topics social media subreddit textitr covid19positive number topics discuss share include experience get positive test result stories presumably get infect question ask regard pandemic disease study try understand linguistic perspective nature discussions subreddit find differences linguistic characteristics eg psychological emotional reason across three different categories topics also classify post different categories use sota pre train language model classification model use pandemic relate research social media
pre train text encoders draw sustain attention natural language process nlp show capability obtain promise result different task recent study illustrate external self supervise signal knowledge extract unsupervised learn n grams beneficial provide useful semantic evidence understand languages chinese improve performance various downstream task accordingly enhance encoders paper propose pre train n gram enhance encoders large volume data advance techniques train moreover try extend encoder different languages well different domains confirm architecture applicable vary circumstances new state art performance observe long list nlp task across languages domains
anthology speak languages today inundate textual information necessitate development automatic summarization model manuscript propose extractor paraphraser base abstractive summarization system exploit semantic overlap oppose predecessors focus syntactic information overlap model outperform state art baselines term rouge meteor word mover similarity wms establish superiority propose system via extensive ablation experiment also challenge summarization capabilities state art pointer generator network pgn thorough experimentation show pgn paraphraser contrary prevail notion summarizer illustrate incapability accumulate information across multiple sentence
taxonomies important ingredient knowledge organization serve backbone sophisticate knowledge representations intelligent systems formal ontologies however build taxonomies manually costly endeavor hence automatic methods taxonomy induction good alternative build large scale taxonomies paper propose tiemb approach automatic unsupervised class subsumption axiom extraction knowledge base use entity text embeddings apply approach webisa database database subsumption relations extract large portion world wide web extract class hierarchies person place domain
unfold tremendous amount audiovisual data upload daily social media platforms effective topic model techniques need exist work tend apply variants topic model text data set paper aim develop topic extractor video transcriptions model improve coherence exploit neural word embeddings graph base cluster method unlike typical topic model approach work without know true number topics experimental result real life multimodal data set muse car demonstrate approach extract coherent meaningful topics outperform baseline methods furthermore successfully demonstrate generalisability approach pure text review data set
recent years sentiment analysis emotion classification two abundantly use techniques field natural language process nlp although sentiment analysis emotion classification use commonly applications analyze customer review popularity candidates contest elections comment various sport events however study examine application epidemic outbreak detection early outbreak detection key deal epidemics effectively however traditional ways outbreak detection time consume inhibit prompt response respective departments social media platforms twitter facebook instagram etc allow users express thoughts relate different aspects life therefore serve substantial source information situations propose study exploit bilingual urdu english data twitter news websites relate dengue epidemic pakistan sentiment analysis emotion classification perform acquire deep insights data set gain fair idea relate epidemic outbreak machine learn deep learn algorithms use train implement model execution task comparative performance model evaluate use accuracy precision recall f1 measure
portray emotion trustworthiness know increase appeal video content however causal relationship signal online user engagement well understand limit understand partly due scarcity emotionally annotate data vary modalities express user engagement online contribution utilise large dataset youtube review videos include ca six hundred hours dimensional arousal valence trustworthiness annotations investigate feature extract signal various user engagement indicators include view like dislike ratio well sentiment comment identify positive negative influence single feature well interpretable pattern dimension relate user engagement result demonstrate smaller boundary range fluctuations arousal lead increase user engagement furthermore extract time series feature reveal significant p005 correlations dimension count signal mean arousal number peak valence absolute energy trustworthiness effective combination feature outline approach aim automatically predict several user engagement indicators user engagement prediction paradigm compare feature semi automatic cross task automatic task specific feature selection methods select feature set appear outperform usage feature eg use feature achieve one hundred and fifty-five like per day lp mean absolute error valence improve semi automatic automatic selection one hundred and thirty-three one hundred and twenty-three lp respectively data mean nine hundred and seventy-two lp std two thousand, eight hundred and seventy-five lp
bert base model currently use solve nearly natural language process nlp task often achieve state art result therefore nlp community conduct extensive research understand model design effective efficient train procedures several ablation study investigate train bert like model carry vast majority concern english language train procedure design english universal applicable especially typologically different languages therefore paper present first ablation study focus polish unlike isolate english language fusional language design thoroughly evaluate pretraining procedure transfer knowledge multilingual monolingual bert base model addition multilingual model initialization factor possibly influence pretraining also explore ie train objective corpus size bpe dropout pretraining length base propose procedure polish bert base language model herbert train model achieve state art result multiple downstream task
simultaneous translation start translate sentence receive word source sentence vital role many scenarios although previous prefix prefix framework consider suitable simultaneous translation achieve good performance still two inevitable drawbacks high computational resource cost cause need train separate model latency k insufficient ability encode information target token attend specific source prefix propose novel framework adopt simple effective decode strategy design full sentence model within framework train single full sentence model achieve arbitrary give latency save computational resources besides competence full sentence model encode whole sentence decode strategy enhance information maintain decode state real time experimental result show method achieve better translation quality baselines four directions zhrightarrowen enrightarrowro enleftrightarrowde
emerge line research explainable nlp creation datasets enrich human annotate explanations rationales use build evaluate model step wise inference explanation generation capabilities human annotate explanations use grind truth inference lack systematic assessment consistency rigour attempt provide critical quality assessment explanation gold standards xgss nli propose systematic annotation methodology name explanation entailment verification eev quantify logical validity human annotate explanations application eev three mainstream datasets reveal surprise conclusion majority explanations appear coherent surface represent logically invalid arguments range incomplete contain clearly identifiable logical errors conclusion confirm inferential properties explanations still poorly formalise understand additional work line research necessary improve way explanation gold standards construct
experience information need users want engage expert often turn information retrieval system search engine instead classical information retrieval systems answer information need directly instead provide reference hopefully authoritative answer successful question answer systems offer limit corpus create demand human experts neither timely scalable large pre train language model contrast capable directly generate prose may responsive information need present dilettantes rather experts true understand world prone hallucinate crucially incapable justify utterances refer support document corpus train paper examine ideas classical information retrieval large pre train language model synthesize evolve systems truly deliver promise expert advice
computational sign language research lack large scale datasets enable creation useful reallife applications date research limit prototype systems small domains discourse eg weather forecast address issue push field forward release six datasets comprise one hundred and ninety hours footage larger domain news twenty hours footage annotate deaf experts interpreters make publicly available research purpose paper share dataset collection process tool develop enable alignment sign language video subtitle well baseline translation result underpin future research
current work nlp utilize deep learn require lot train data computational power paper investigate strengths genetic algorithms gas extractive summarization hypothesize gas could construct efficient solutions summarization task due relative customizability relative deep learn model do build vocabulary set word represent array weight optimize set weight ga weight use build overall weight sentence pass threshold extraction result show ga able learn weight representation could filter excessive vocabulary thus dictate sentence importance base common english word
keyphrases concisely summarize high level topics discuss document categorize present keyphrase explicitly appear source text absent keyphrase match contiguous subsequence highly semantically relate source exist keyphrase generation approach synchronously generate present absent keyphrases without explicitly distinguish two categories paper select guide generate sgg approach propose deal present absent keyphrase generation separately different mechanisms specifically sgg hierarchical neural network consist point base selector low layer concentrate present keyphrase generation selection guide generator high layer dedicate absent keyphrase generation guider middle transfer information selector generator experimental result four keyphrase generation benchmarks demonstrate effectiveness model significantly outperform strong baselines present absent keyphrases generation furthermore extend sgg title generation task indicate extensibility natural language generation task
introduce biomedical informatics bmi students natural language process nlp require balance technical depth practical know address application focus need develop set three activities introduce introductory bmi students information retrieval nlp cover document representation strategies language model tf idf bert activities provide students hand experience target towards common use case introduce fundamental components nlp workflows wide variety applications
natural language process nlp methods analyze legal text offer legal scholars practitioners range tool allow empirically analyze law large scale however researchers seem struggle come identify ethical limit use nlp systems acquire genuine insights law systems predictive capacity paper set number ways think systematically issue place emphasis three crucial normative parameters best knowledge underestimate current debate importance academic freedom b existence wide diversity legal ethical norms domestically even internationally c threat moralism research relate computational law three parameters provide specific recommendations legal nlp community discussion structure around study real life scenario prompt recent debate legal nlp research community
present hare new task reader feedback use optimize document summaries personal interest normal flow read task relate interactive summarization personalize summaries produce follow long feedback stage users may read sentence many time however process severely interrupt flow read make impractical leisurely read propose gather minimally invasive feedback read process adapt user interest augment document real time build recent advance unsupervised summarization evaluation propose suitable metric task use evaluate variety approach approach range simple heuristics preference learn analysis provide insight important task human evaluation additionally support practicality hare code reproduce work available https githubcom tannerbohn honeasyouread
paper address problem sentiment analysis jopara code switch language guarani spanish first collect corpus guarani dominant tweet discuss difficulties find quality data even relatively easy annotate task sentiment analysis train set neural model include pre train language model explore whether perform better traditional machine learn ones low resource setup transformer architectures obtain best result despite consider guarani pre train traditional machine learn model perform close due low resource nature problem
along covid nineteen pandemic infodemic false mislead information emerge complicate covid nineteen response efforts social network sit facebook twitter contribute largely spread rumor conspiracy theories hate xenophobia racism prejudice combat spread fake news researchers around world still make considerable efforts build share covid nineteen relate research article model datasets paper release aracovid19 mfh manually annotate multi label arabic covid nineteen fake news hate speech detection dataset dataset contain ten thousand, eight hundred and twenty-eight arabic tweet annotate ten different label label design consider aspects relevant fact check task tweet check worthiness positivity negativity factuality confirm annotate dataset practical utility use train evaluate several classification model report obtain result though dataset mainly design fake news detection also use hate speech detection opinion news classification dialect identification many task
emojis establish popular mean communication online message despite apparent ubiquity image base tokens however interpretation ambiguity may allow unique use emojis appear paper present first examination emoji usage hacktivist group via study anonymous collective twitter research aim identify whether anonymous affiliate evolve approach use emojis compare large dataset anonymous tweet baseline tweet dataset randomly sample twitter users use computational qualitative analysis compare emoji usage utilise word2vec language model examine semantic relationships emojis identify clear distinctions emoji emoji relationships anonymous users explore emojis use mean convey emotions find despite little commonality emoji emoji semantic tie anonymous emoji usage display similar pattern emotional purpose emojis baseline twitter users finally explore textual context emojis occur find although similarities exist emoji usage anonymous baseline twitter datasets anonymous users appear adopt specific interpretations certain emojis include use emojis mean express adoration infatuation towards notable anonymous affiliate find indicate emojis appear retain considerable degree similarity within anonymous account compare typical twitter users however sign emoji usage anonymous account evolve somewhat gain additional group specific associations reveal new insights behaviours unusual collective
semantic match central significance answer selection task aim select correct answer give question candidate answer pool useful method employ neural network attention generate sentence representations way information pair sentence mutually influence computation representations work effective architecturemulti size neural network attention mechanism msnnis introduce answer selection task architecture capture level language granularities parallel various size filter compare single layer cnn multi layer cnns meanwhile extend sentence representations attention mechanism thus contain information different type question empirical study three various benchmark task answer selection demonstrate efficacy propose model benchmarks superiority competitors experimental result show one multi size neural network msnn useful method capture abstract feature different level granularities single multi layer cnns two attention mechanism better strategy derive informative representations three msnn better architecture answer selection task moment
present fairly large potential idiomatic expression pie dataset natural language process nlp english challenge nlp systems regard task machine translation mt word sense disambiguation wsd information retrieval make imperative label idioms dataset class work best author knowledge first idioms corpus class idioms beyond literal general idioms classification particular follow class label dataset metaphor simile euphemism parallelism personification oxymoron paradox hyperbole irony literal many past efforts limit corpus size class sample dataset contain twenty thousand, one hundred sample almost one thousand, two hundred case idioms mean ten class sense corpus may also extend researchers meet specific need corpus part speech pos tag nltk library classification experiment perform corpus obtain baseline comparison among three common model include bert model give good result also make publicly available corpus relevant cod work nlp task
natural language process feature additive explanation methods quantify independent contribution input token towards model decision compute rank correlation attention weight score produce small sample methods previous analyse seek either invalidate support role attention base explanations faithful plausible measure salience investigate measure rank correlation reliably conclude comprehensively compare feature additive methods include attention base explanations across several neural architectures task case find none choose methods agree therefore argue rank correlation largely uninformative measure quality feature additive methods additionally range conclusions practitioner may draw single explainability algorithm limit
paper present multilingual covid nineteen analysis method cmta detect observe spread misinformation disease within texts cmta propose data science ds pipeline apply machine learn model process classify dense cnn analyze mbert multilingual micro texts ds pipeline data preparation task extract feature multilingual textual data categorize specific information class ie false partly false mislead cmta pipeline experiment multilingual micro texts tweet show misinformation spread across different languages assess performance cmta put perspective perform comparative analysis cmta eight monolingual model use detect misinformation comparison show cmta surpass various monolingual model suggest use general method detect misinformation multilingual micro texts cmta experimental result show misinformation trend covid nineteen different languages first pandemic months
name entity recognition ner study extensively earlier algorithms base sequence label like hide markov model hmm conditional random field crf follow neural network base deep learn model recently bert show new state art accuracy sequence label task like ner short article study various approach task specific ner task specific ner two components identify intent piece text like search query label query task specific name entities example consider task label target store locations search query could enter search box speak device like alexa google home store locations highly ambiguous sometimes difficult differentiate say location non location example pickup order orange store orange store location buy orange target orange fruit explore difficulty multi task learn call global local transfer information jointly learn query intent ie store lookup name entities use multiple loss function bert base model find interest result
era pre train language model transformers de facto choice model architectures recent research show promise entirely convolutional cnn architectures explore use pre train fine tune paradigm context language model convolutional model competitive transformers pre train paper investigate research question present several interest find across extensive set experiment eight datasets task find cnn base pre train model competitive outperform transformer counterpart certain scenarios albeit caveats overall find outline paper suggest conflate pre train architectural advance misguide advance consider independently believe research pave way healthy amount optimism alternative architectures
adapt pre train neural model downstream task become standard practice obtain high quality model work propose novel model adaptation paradigm adapt prune prune neural connections pre train model optimise performance target task remain connections weight intact formulate adapt prune optimisation problem differentiable loss propose efficient algorithm prune model prove algorithm near optimal standard assumptions apply algorithm adapt bert glue task result suggest method prune fifty weight bert yield similar performance compare fine tune full model also compare method state art prune methods study topological differences obtain sub network
paper reversibility executable interval temporal logic itl specifications investigate itl allow reason systems term behaviours represent non empty sequence state allow specification systems different level abstraction high level specification term properties instance safety liven properties concrete level one specify system term program construct one execute concrete specification ie test simulate behaviour system paper formalise notion executability itl specifications itl also reflection operator allow reason reverse behaviours investigate reversibility executable itl specifications ie one use reflection operator reverse concrete behaviour particular system
constrain optimization solvers integer linear program ilp cornerstone explainable natural language inference inception ilp base approach provide way encode explicit controllable assumptions cast natural language inference abductive reason problem solver construct plausible explanation give hypothesis constrain base solvers provide explanations often limit use explicit constraints integrate part broader deep neural architectures contrast state art transformer base model learn data implicitly encode complex constraints however model intrinsically black box paper present novel framework name partial explainer diff explainer combine best worlds cast constrain optimization part deep neural network via differentiable convex optimization fine tune pre train transformers downstream explainable nlp task demonstrate efficacy framework transform constraints present tupleilp integrate sentence embed transformers task explainable science qa experiment show approx ten improvement non differentiable solver still provide explanations support inference
concept text natural language generation task express input mean representation natural language previous approach task able generalise rare unseen instance rely delexicalisation input however often require input appear verbatim output text pose challenge multilingual settings task expand generate output text multiple languages give input paper explore application multilingual model concept text propose language agnostic delexicalisation novel delexicalisation method use multilingual pretrained embeddings employ character level post edit model inflect word correct form relexicalisation experiment across five datasets five languages show multilingual model outperform monolingual model concept text framework outperform previous approach especially low resource languages
recently retrieval model base dense representations gradually apply first stage document retrieval task show better performance traditional sparse vector space model obtain high efficiency basic structure model bi encoder case however simple structure may serious information loss encode document since query agnostic address problem design method mimic query document iterative cluster process represent document multiple pseudo query ie cluster centroids boost retrieval process use approximate nearest neighbor search library also optimize match function two step score calculation procedure experimental result several popular rank qa datasets show model achieve state art result
build abduction base explanations chine learn develop method compute local explanations neural network model natural language process nlp explanations comprise subset word put text satisfy two key feature optimality wrt user define cost function length explanation robustness ensure prediction invariance bound perturbation embed space leave word present two solution algorithms respectively base implicit hit set maximum universal subsets introduce number algorithmic improvements speed convergence hard instance show method con figure different perturbation set bed space use detect bias predictions enforce include exclude constraints bias term well enhance exist heuristic base nlp explanation frameworks anchor evaluate framework three widely use sentiment analysis task texts to100words sst twitter imdb datasetsdemonstrating effectiveness derive explanations
neural text generation model likely suffer low diversity problem various decode strategies train base methods propose promote diversity exploit contextual feature rarely consider incorporate syntactic structure clue work propose use linguistic annotation ie part speech pos guide text generation detail introduce pos guide softmax posg softmax explicitly model two posterior probabilities next pos ii next token vocabulary target pos pos guide sample strategy propose address low diversity problem enrich diversity pos extensive experiment human evaluations demonstrate compare exist state art methods propose methods generate diverse text maintain comparable quality
show transformer encoder architectures massively speed limit accuracy cost replace self attention sublayers simple linear transformations mix input tokens linear transformations along simple nonlinearities fee forward layer sufficient model semantic relationships several text classification task perhaps surprisingly find replace self attention sublayer transformer encoder standard unparameterized fourier transform achieve ninety-two accuracy bert glue benchmark pre train run seven time faster gpus twice fast tpus result model name fnet scale efficiently long input match accuracy accurate efficient transformers long range arena benchmark train run faster across sequence lengths gpus relatively shorter sequence lengths tpus finally fnet light memory footprint particularly efficient smaller model size fix speed accuracy budget small fnet model outperform transformer counterparts
successful debut natural language process transformer architectures become de facto standard many domains obstacle deployment new modalities architectural configuration optimal depth width ratio show dramatically vary across data type eg 10x larger image language theoretically predict existence embed rank bottleneck limit contribution self attention width transformer expressivity thus directly tie input vocabulary size rank optimal depth width ratio since small vocabulary size rank dictate add advantage depth width empirically demonstrate existence bottleneck implications depth width interplay transformer architectures link architecture variability across domains often gloss usage different vocabulary size embed rank different domains additional benefit rank bottleneck framework allow us identify size redundancies twenty-five fifty lead nlp model albert t5
gcomm step towards develop robust platform foster research ground language acquisition challenge realistic set comprise two grid environment set agents stationary speaker mobile listener connect via communication channel expose continuous array task partially observable set key solve task lie agents develop linguistic abilities utilize efficiently explore environment speaker listener access information provide different modalities ie speaker input natural language instruction contain target task specifications listener input grid view must rely complete assign task however way achieve develop use form communication gcomm provide several tool study different form communication assess generalization
data scarcity low resource languages become bottleneck build robust neural machine translation systems fine tune multilingual pre train model eg mbart liu et al two thousand and twenty translation task good approach low resource languages however performance greatly limit unseen languages translation pair paper present continual pre train cpt framework mbart effectively adapt unseen languages first construct noisy mix language text monolingual corpus target language translation pair cover source target languages continue pre train mbart reconstruct original monolingual text result show method consistently improve fine tune performance upon mbart baseline well strong baselines across test low resource translation pair contain unseen languages furthermore approach also boost performance translation pair languages see original mbart pre train code available https githubcom zliucr cpt nmt
propose vadec multi task framework exploit correlation categorical dimensional model emotion representation better subjectivity analysis focus primarily effective detection emotions tweet jointly train multi label emotion classification multi dimensional emotion regression thereby utilize inter relatedness task co train especially help improve performance classification task outperform strongest baselines thirty-four eleven thirty-nine gain jaccard accuracy macro f1 micro f1 score respectively ait dataset also achieve state art result one hundred and thirteen gain average six different metrics senwave dataset regression task vadec train senwave achieve seventy-six one hundred and sixty-five gain pearson correlation score current state art emobank dataset valence v dominance affect dimension respectively conclude work case study covid nineteen tweet post indians help establish efficacy propose solution
rapid development social media change lifestyle people simultaneously provide ideal place publish disseminate rumor severely exacerbate social panic trigger crisis social trust early content base methods focus find clue text user profile rumor detection recent study combine stances users comment news content capture difference true false rumor although user stance effective rumor detection manual label process time consume labor intensive limit application utilize facilitate rumor detection paper first finetune pre train bert model small label dataset leverage model annotate weak stance label users comment data overcome problem mention propose novel stance aware reinforcement learn framework srlf select high quality label stance data model train rumor detection stance selection rumor detection task optimize simultaneously promote task mutually conduct experiment two commonly use real world datasets experimental result demonstrate framework outperform state art model significantly confirm effectiveness propose framework
achieve human level performance machine read comprehension mrc datasets longer challenge help powerful pre train language model plms however necessary provide answer prediction explanation improve mrc system reliability especially real life applications paper propose new benchmark call expmrc evaluate explainability mrc systems expmrc contain four subsets include squad cmrc two thousand and eighteen race c3 additional annotations answer evidence mrc systems require give correct answer also explanation use state art pre train language model build baseline systems adopt various unsupervised approach extract evidence without human annotate train set experimental result show model still far human performance suggest expmrc challenge resources available https githubcom ymcui expmrc
knowledge intensive task question answer often require assimilate information different section large input book article collections propose readtwice simple effective technique combine several strengths prior approach model long range dependencies transformers main idea read text small segment parallel summarize segment memory table use second read text show method outperform model comparable size several question answer qa datasets set new state art challenge narrativeqa task question entire book source code pre train checkpoints readtwice find https google research readtwice
increase availability textual corpora data fetch social network fuel huge production work base model propose psychologist robert plutchik often refer simply plutchik wheel relate research range annotation task description emotions detection tool visualisation emotions traditionally carry use popular layouts bar plot table however sub optimal classic representation plutchik wheel follow principles proximity opposition pair emotions spatial proximity model also semantic proximity adjacent emotions elicit complex emotion primary dyad trigger together spatial opposition semantic opposition well positive emotions opposite negative emotions common layouts fail preserve feature mention need visually allow comparisons different corpora blink eye hard basic design solutions introduce pyplutchik python library specifically design visualisation plutchik emotions texts corpora pyplutchik draw plutchik flower emotion petal size much emotion detect annotate corpus also represent three degrees intensity notably pyplutchik allow users display also primary secondary tertiary opposite dyads compact intuitive way substantiate claim pyplutchik outperform classic visualisations display plutchik emotions showcase examples display library compel feature
field machine learn well train model assume able recover train label ie synthetic label predict model close grind truth label possible inspire propose self guide curriculum strategy encourage learn neural machine translation nmt model follow recovery criterion cast recovery degree train example learn difficulty specifically adopt sentence level bleu score proxy recovery degree different exist curricula rely linguistic prior knowledge third party language model choose learn difficulty suitable measure degree knowledge mastery nmt model experiment translation benchmarks include wmt14 englishrightarrowgerman wmt17 chineserightarrowenglish demonstrate approach consistently improve translation performance strong baseline transformer
commonly observe problem state art abstractive summarization model generate summaries factually inconsistent input document fact automatic summarization may produce plausible sound yet inaccurate summaries major concern limit wide application paper present approach address factual consistency summarization first propose efficient automatic evaluation metric measure factual consistency next propose novel learn algorithm maximize propose metric model train extensive experiment confirm method effective improve factual consistency even overall quality summaries judge automatic metrics human evaluation
form common type document real life carry rich information textual content organizational structure realize automatic process form word group relation extraction two fundamental crucial step preliminary process optical character reader ocr word group aggregate word belong semantic entity relation extraction predict link semantic entities exist work treat two individual task two task correlate reinforce group process refine integrate representation correspond entity link process give feedback group performance purpose acquire multimodal feature textual data layout information build end end model multitask train combine word group relation extraction enhance performance task validate propose method real world fully annotate noisy scan benchmark funsd extensive experiment demonstrate effectiveness method
transformer model multi head attention require cache intermediate result efficient inference generation task however cache bring new memory relate cost prevent leverage larger batch size faster speed propose memory efficient lossless attention call el attention address issue avoid heavy operations build multi head key value requirements use cache el attention construct ensemble attention result expand query keep key value share produce result multi head attention less gpu memory faster inference speed conduct extensive experiment transformer bart gpt two summarization question generation task result show el attention speed exist model 16x 53x without accuracy loss
data augmentation approach effectively improve performance multimodal machine learn paper introduce generative model data augmentation leverage correlations among multiple modalities different conventional data augmentation approach apply low level operations deterministic heuristics method propose learn augmentation sampler generate sample target modality condition observe modalities variational auto encoder framework additionally propose model able quantify confidence augment data generative probability jointly update downstream pipeline experiment visual question answer task demonstrate effectiveness propose generative model able boost strong updn base model state art performance
coincidence initial algebras ias final coalgebras fcs phenomenon underpin various important result theoretical computer science paper identify general fibrational condition ia fc coincidence namely fiber initial algebra base category identify coalgebras fiber coinductive predicate fibrational ia fc coincidence allow one use coinductive witness invariants verify inductive properties liven general fibrational theory feature technical condition stability chain colimits extend framework presence monadic effect restrict fibrations complete lattice value predicate practical benefit categorical theory exemplify new upside witness notions three verification problems probabilistic liven acceptance model check respect bottom tree automata
automate predictions require explanations interpretable humans one type explanation rationale ie selection input feature relevant text snippets model compute outcome however single overall selection provide complete explanation eg weigh several aspects decisions end present novel self interpretable model call conrat inspire human explanations high level decisions often base key concepts conrat extract set text snippets concepts infer ones describe document explain outcome linear aggregation concepts two regularizers drive conrat build interpretable concepts addition propose two techniques boost rationale predictive performance experiment single multi aspect sentiment classification task show conrat first generate concepts align human rationalization use overall label outperform state art methods train aspect label independently
rise personal assistants make conversational question answer convqa popular mechanism user system interaction state art methods convqa knowledge graph kgs learn crisp question answer pair find popular benchmarks reality however train data hard come users would rarely mark answer explicitly correct wrong work take step towards natural learn paradigm noisy implicit feedback via question reformulations reformulation likely trigger incorrect system response whereas new follow question could positive signal previous turn answer present reinforcement learn model term conquer learn conversational stream question reformulations conquer model answer process multiple agents walk parallel kg walk determine action sample use policy network policy network take question along conversational context input train via noisy reward obtain reformulation likelihood evaluate conquer create release convref benchmark 11k natural conversations contain around 205k reformulations experiment show conquer successfully learn answer conversational question noisy reward signal significantly improve state art baseline
machine understand user utterances conversational systems utmost importance enable engage meaningful conversations users entity link el one mean text understand prove efficacy various downstream task information retrieval paper study entity link conversational systems develop better understand el conversational set entail analyze large number dialogues exist conversational datasets annotate reference concepts name entities personal entities use crowdsourcing base annotate dialogues identify main characteristics conversational entity link report performance traditional el systems conversational entity link dataset conel present extension methods better fit conversational set resources release paper include annotate datasets detail descriptions crowdsourcing setups well annotations produce various el systems new resources allow investigation role entities conversations different document isolate short text utterances like query tweet complement exist conversational datasets
analogies play central role human commonsense reason ability recognize analogies eye see ear hear sometimes refer analogical proportion shape structure knowledge understand language surprisingly however task identify analogies yet receive much attention language model era paper analyze capabilities transformer base language model unsupervised task use benchmarks obtain educational settings well commonly use datasets find shelf language model identify analogies certain extent struggle abstract complex relations result highly sensitive model architecture hyperparameters overall best result obtain gpt two roberta configurations use bert able outperform word embed model result raise important question future work extent pre train language model capture knowledge abstract semantic relations
cross lingual text representations gain popularity lately act backbone many task unsupervised machine translation cross lingual information retrieval name however evaluation representations difficult domains beyond standard benchmarks due necessity obtain domain specific parallel language data across different pair languages paper propose automatic metric evaluate quality cross lingual textual representations use image proxy pair image text evaluation dataset experimentally backretrieval show highly correlate grind truth metrics annotate datasets analysis show statistically significant improvements baselines experiment conclude case study recipe dataset without parallel cross lingual data illustrate judge cross lingual embed quality backretrieval validate outcome small human study
generalization central problem machine learn especially data limit use prior information enforce constraints principled way encourage generalization work propose leverage prior information embed pretrained language model lm improve generalization intent classification slot label task limit train data specifically extract prior knowledge pretrained lm form synthetic data encode prior implicitly fine tune lm generate augment language contain text also encode intent label slot label generate synthetic data use train classifier later since generate data may contain noise rephrase learn generate data learn noisy label utilize mixout regularization classifier prove effectiveness resist label noise generate data empirically method demonstrate superior performance outperform baseline large margin
human language describe system make textituse finite mean express unlimited array thoughts particular interest aspect compositionality whereby mean compound language expression deduce mean constituent part artificial agents develop compositional communication protocols akin human language make seamlessly generalize unseen combinations study recognize role curiosity enable linguistic development children paper seek use intrinsic feedback induce systematic unambiguous protolanguage demonstrate compositionality enable agents interact unseen object also transfer skills one task another zero shoot set textitcan agent train pull push twice pull twice
humour detection sentence interest challenge task last years attempt highlight humour detection research conduct use traditional approach embed eg word2vec glove recently bert sentence embed also use task paper propose framework humour detection short texts take news headline propose framework iben attempt extract information write text via use different layer bert several trials weight assign different layer bert model extract information send bi gru neural network embed matrix utilize properties external embed model multi kernel convolution neural network also employ extract higher level sentence representations framework perform well task humour detection
paper describe contribution semeval two thousand and twenty task seven assess humor edit news headline present method base deep neural network recent years quite attention devote humor production perception team kdehumor employ recurrent neural network model include bi directional lstms bilstms moreover utilize state art pre train sentence embed techniques analyze performance method demonstrate contribution component architecture
work present new approach unsupervised abstractive summarization base maximize combination coverage fluency give length constraint introduce novel method encourage inclusion key term original document summary key term mask original document must fill coverage model use current generate summary novel unsupervised train procedure leverage coverage model along fluency model generate score summaries test popular news summarization datasets method outperform previous unsupervised methods two r one point approach result competitive supervise methods model attain higher level abstraction copy passages roughly two time shorter prior work learn compress merge sentence without supervision
work describe automatic news chatbot draw content diverse set news article create conversations user news key components system include automatic organization news article topical chatrooms integration automatically generate question conversation novel method choose question present avoid repetitive suggestions describe algorithmic framework present result usability study show news readers use system successfully engage multi turn conversations specific news stories
defeasible reason mode reason conclusions overturn take account new evidence commonly use method cognitive science logic literature handcraft argumentation support inference graph humans find inference graph useful reason construct scale difficult paper automatically generate inference graph transfer learn another nlp task share kind reason inference graph support automate metrics human evaluation find method generate meaningful graph defeasible inference task human accuracy task improve twenty consult generate graph find open excite new research avenues case machine reason help human reason dataset two hundred and thirty thousand influence graph defeasible query locate https tinyurlcom defeasiblegraphs
propose cascade neural model perform sentence classification phrase recognition triple extraction automatically structure scholarly contributions nlp publications identify important contribution sentence paper use bert base classifier positional feature subtask one bert crf model use recognize characterize relevant phrase contribution sentence subtask two categorize triple several type base whether elements express text address type use separate bert base classifiers well rule subtask three system officially rank second phase one evaluation first part phase two evaluation fix submission error pharse one approach yield best result overall paper addition system description also provide analysis result highlight strengths limitations make code publicly available https githubcom liu hy nlp contrib graph
gender bias stereotype recently raise significant ethical concern natural language process however progress detection evaluation gender bias natural language understand inference limit require investigation work propose evaluation methodology measure bias construct challenge task involve pair gender neutral premise gender specific hypothesis use challenge task investigate state art nli model presence gender stereotype use occupations find suggest three model bert roberta bart train mnli snli datasets significantly prone gender induce prediction errors also find debiasing techniques augment train dataset ensure gender balance dataset help reduce bias certain case
domain ood input detection vital task orient dialogue system since acceptance unsupported input could lead incorrect response system paper propose outflip method generate domain sample use domain train dataset automatically white box natural language attack method hotflip revise generate domain sample instead adversarial examples evaluation result show integrate outflip generate domain sample train dataset could significantly improve intent classification model domain detection performance
wikipedia online encyclopedia available two hundred and eighty-five languages compose extremely relevant knowledge base kb could leverage automatic systems several purpose however structure organisation information prone automatic parse understand therefore necessary structure knowledge goal current shinra2020 ml task leverage wikipedia page order categorise correspond entities across two hundred and sixty-eight hierarchical categories belong extend name entity ene ontology work propose three distinct model base contextualised embeddings yield multilingual bert explore performances linear layer without explicit usage ontology hierarchy gate recurrent units gru layer also test several pool strategies leverage bert embeddings selection criteria base label score able achieve good performance across large variety languages include see fine tune process zero shoot languages
medical article provide current state art treatments diagnostics many medical practitioners professionals exist public databases medline contain twenty-seven million article make difficult extract relevant content without use efficient search engines information retrieval tool crucial order navigate provide meaningful recommendations article treatments classify article broader medical topics improve retrieval relate article set medical label consider mesinesp task order several thousands label decs cod fall extreme multi label classification problem heterogeneous highly hierarchical structure medical topics make task manually classify article extremely laborious costly therefore crucial automate process classification typical machine learn algorithms become computationally demand large number label achieve better recall datasets become unsolved problem work present priberam participation bioasq task mesinesp address large multi label classification problem use four different model support vector machine svm customise search engine priberam search bert base classifier svm rank ensemble previous model result demonstrate three individual model perform well best performance achieve ensemble grant priberam 6th place present challenge make 2nd best team
climate change burn issue time sustainable development goal sdg thirteen unite nations demand global climate action realize urgency two thousand and fifteen paris world leaders sign agreement commit take voluntary action reduce carbon emissions however scale magnitude climate action process vary globally especially develop develop countries therefore parliament social media debate discussions climate change gather data wide range source essential policy design implementation downside currently mechanisms pool worldwide disperse knowledge emerge structure unstructured data source paper thematically discuss nlp techniques could employ climate policy research contribute society good large particular exemplify symbiosis nlp climate policy research via four methodologies first one deal major topics relate climate policy use automate content analysis investigate opinions sentiments major actors narratives towards climate policy second methodology third technique explore climate actors beliefs towards pro anti climate orientation finally discuss develop climate knowledge graph present theme paper argue create knowledge platform would help formulation holistic climate policy effective climate action knowledge platform would integrate policy actors vary opinions different social sectors like government business civil society scientific community research outcome add value effective climate action policymakers make inform decisions look diverse public opinion comprehensive platform
pursuit deeper understand model behaviour recent impetus develop suit probe aim diagnose model beyond simple metrics like accuracy bleu paper take step back ask important timely question reliable diagnostics provide insight model train setups critically examine three recent diagnostic test pre train language model find likelihood base representation base model diagnostics yet reliable previously assume base empirical find also formulate recommendations practitioners researchers
paper describe n xkt neural encode base explanatory knowledge transfer novel method automatic transfer explanatory knowledge neural encode mechanisms demonstrate n xkt able improve accuracy generalization science question answer qa specifically leverage facts background explanatory knowledge corpora n xkt model show clear improvement zero shoot qa furthermore show n xkt fine tune target qa dataset enable faster convergence accurate result systematic analysis conduct quantitatively analyze performance n xkt model impact different categories knowledge zero shoot generalization task
project attempt build question answer system news domain passages news article anyone ask question build span base model use attention mechanism model predict answer question position start end tokens paragraph train model use stanford question answer squad twenty dataset1 well squad twenty systems must answer question possible also determine answer support paragraph abstain answer model architecture comprise three layer embed layer rnn layer attention layer embed layer use glove universal sentence encoder rnn layer build variations rnn layer include bi lstm stack lstm build attention layer use context question attention also improvise innovative bidirectional attention layer best perform model use glove embed combine bi lstm context question attention achieve f1 score thirty-three thousand and ninety-five thirty-three thousand and ninety-four respectively also leverage transfer learn build transformer base model use bert bert base model achieve f1 score fifty-seven thousand, five hundred and thirteen forty-nine thousand, seven hundred and sixty-nine respectively conclude bert model superior aspects answer various type question
advent large pre train language model give rise rapid progress field natural language process nlp performance model standard benchmarks scale size compression techniques knowledge distillation key make practical present mate kd novel text base adversarial train algorithm improve performance knowledge distillation mate kd first train mask language model base generator perturb text maximize divergence teacher student logits use knowledge distillation student train original perturb train sample evaluate algorithm use bert base model glue benchmark demonstrate mate kd outperform competitive adversarial learn data augmentation baselines glue test set six layer roberta base model outperform bert large
paper compare bert squad ab3p abbreviation definition identification adi task adi input text output short form abbreviations acronyms long form expansions bert reranking improve bert without reranking fail reach ab3p rule base baseline bert miss reranking introduce two new feature charmatch freq first feature identify opportunities take advantage character constraints acronyms second feature identify opportunities take advantage frequency constraints across document
increase amount evidence case little data target language train different language yield surprisingly good result however currently establish guidelines choose train source language attempt solve issue thoroughly analyze state art multilingual model try determine impact good transfer languages oppose majority multilingual nlp literature train english group almost thirty languages show look particular syntactic feature two four time helpful predict performance aggregate syntactic similarity find importance syntactic feature strongly differ depend downstream task single feature good performance predictor nlp task result one expect target language l1 single language l2 best choice nlp task instance bulgarian best source language french pos tag russian ner thai nli discuss important linguistic feature affect transfer quality use statistical machine learn methods
medical dialogue generation aim provide automatic accurate responses assist physicians obtain diagnosis treatment suggestions efficient manner medical dialogues two key characteristics relevant response generation patient state symptoms medication physician action diagnosis treatments medical scenarios large scale human annotations usually available due high cost privacy requirements hence current approach medical dialogue generation typically explicitly account patient state physician action focus implicit representation instead propose end end variational reason approach medical dialogue generation able deal limit amount label data introduce patient state physician action latent variables categorical priors explicit patient state track physician policy learn respectively propose variational bayesian generative approach approximate posterior distributions patient state physician action use efficient stochastic gradient variational bay estimator optimize derive evidence lower bind two stage collapse inference method propose reduce bias model train physician policy network compose action classifier two reason detectors propose augment reason ability conduct experiment three datasets collect medical platforms experimental result show propose method outperform state art baselines term objective subjective evaluation metrics experiment also indicate propose semi supervise reason method achieve comparable performance state art fully supervise learn baselines physician policy learn
contract element extraction cee novel task automatically identify extract legally relevant elements contract date payments legislation reference contract automatic methods task view sequence label problem dramatically reduce human labor however contract genres element type may vary widely significant challenge sequence label task transfer knowledge one domain another ie cross domain cee cross domain cee differ cross domain name entity recognition ner two important ways first contract elements far fine grain name entities hinder transfer extractors second extraction zone cross domain cee much larger cross domain ner result contexts elements different domains diverse propose framework bi directional feedback clause element relation network bi fleet cross domain cee task address challenge bi fleet three main components one context encoder two clause element relation encoder three inference layer incorporate invariant knowledge element clause type clause element graph construct across domains hierarchical graph neural network adopt clause element relation encoder reduce influence context variations multi task framework bi directional feedback scheme design inference layer conduct clause classification element extraction experimental result cross domain ner cee task show bi fleet significantly outperform state art baselines
diversify enrich generate dialogue responses knowledge ground dialogue investigate recent years despite success exist methods mainly follow paradigm retrieve relevant sentence large corpus augment dialogues explicit extra information time resource consume paper propose knowexpert end end framework bypass retrieval process inject prior knowledge pre train language model lightweight adapters best knowledge first attempt tackle task rely solely generation base approach experimental result show knowexpert perform comparably retrieval base baselines demonstrate potential propose direction
judge readability text many important applications instance perform text simplification source read material language learners paper present five hundred and eighteen participant study investigate scroll behaviour relate readability text make dataset publicly available show one statistically significant differences way readers interact text depend text level two measure use predict readability text three background reader impact read interactions factor contribute text difficulty
work introduce corpus satire detection romanian news gather fifty-five thousand, six hundred and eight public news article multiple real satirical news source compose one largest corpora satire detection regardless language one romanian language provide official split text sample train news article belong different source test news article thus ensure model achieve high performance simply due overfitting conduct experiment two state art deep neural model result set strong baselines novel corpus result show machine level accuracy satire detection romanian quite low seventy-three test set compare human level accuracy eighty-seven leave enough room improvement future research
conversational artificial intelligence convai systems attract much academic commercial attention recently make significant progress front however little exist work discuss systems develop deploy social good paper briefly review progress community make towards better convai systems reflect exist technologies help advance social good initiatives various angle unique convai yet become common knowledge community discuss challenge ahead convai systems better help us achieve goals highlight risk involve development deployment real world
ai chatbots make vast stride technology improvement recent years already operational many industries advance natural language process techniques base deep network efficiently process user request carry function chatbots gain traction applicability healthcare attractive proposition due reduce economic people cost overburden system however healthcare bots require safe medically accurate information capture deep network yet capable due user text speech variations knowledge symbolic structure suit accurate reason handle natural language process directly thus paper study effect combine knowledge neural representations chatbot safety accuracy understand
paper study efficiency transfer bert learn low complexity model like bilstm bilstm attention shallow cnns use sentiment analysis sst two dataset also compare complexity inference bert model lower complexity model underline importance techniques enable high performance nlp model edge devices like mobiles tablets mcu development board like raspberry pi etc enable excite new applications
recent advance large scale pre train gpt three allow seemingly high quality text generate give prompt however generation systems often suffer problems hallucinate facts inherently design incorporate useful external information ground generation model appear offer remedy train typically rely rarely available parallel data correspond information relevant document provide context propose framework alleviate data constraint jointly train ground generator document retriever language model signal model learn reward retrieval document highest utility generation attentively combine use mixture experts moe ensemble generate follow text demonstrate generator retriever take advantage joint train work synergistically produce informative relevant text prose dialogue generation
commonsense reason aim incorporate set commonsense facts retrieve commonsense knowledge graph ckg draw conclusion ordinary situations dynamic nature commonsense knowledge postulate model capable perform multi hop reason new situations feature also result large scale sparse knowledge graph reason process need predict relations new events however exist approach area limit consider ckgs limit set facts thus render unfit reason new unseen situations events paper present neural symbolic reasoner capable reason large scale dynamic ckgs logic rule reason ckgs learn train model addition provide interpretable explanation learn logic rule help generalise prediction newly introduce events experimental result task link prediction ckgs prove effectiveness model outperform state art model
recent study neural network pre train weight ie bert mainly focus low dimensional subspace embed vectors compute input word contexts locate work propose new approach find regularize remainder space refer manifold access word specifically synthesize manifold embeddings base two embeddings obtain actually observe word utilize fine tune network discriminator train detect whether input embed locate inside manifold simultaneously generator optimize produce new embeddings easily identify manifold discriminator two modules successfully collaborate unify end end manner regularize manifold extensive evaluation various text classification benchmarks demonstrate effectiveness approach well good compatibility exist data augmentation techniques aim enhance manifold
proposal large scale datasets facilitate research deep neural model news summarization deep learn also potentially useful speak dialogue summarization benefit range real life scenarios include customer service management medication track end propose dialogsum large scale label dialogue summarization dataset conduct empirical analysis dialogsum use state art neural summarizers experimental result show unique challenge dialogue summarization speak term special discourse structure coreferences ellipsis pragmatics social common sense require specific representation learn technologies better deal
write coherent engage story easy creative writers use knowledge worldview put disjoint elements together form coherent storyline work rework iteratively toward perfection automate visual storytelling vist model however make poor use external knowledge iterative generation attempt create stories paper introduce pr vist framework represent input image sequence story graph find best path form storyline pr vist take path learn generate final story via iterative train process framework produce stories superior term diversity coherence humanness per automatic human evaluations ablation study show plot rework contribute model superiority
paper elaborate notion uncertainty context annotation large text corpora specifically focus limit historical languages uncertainty might due inherent properties language example linguistic ambiguity overlap categories linguistic description could also cause lack annotation expertise examine annotation uncertainty detail identify source deepen understand nature different type uncertainty encounter daily annotation practice moreover practical implications theoretical find also discuss last least article see attempt reconcile perspectives main scientific discipline involve corpus project linguistics computer science develop unify view highlight potential synergies discipline
hybrid data combine tabular textual content eg financial report quite pervasive real world however question answer qa hybrid data largely neglect exist research work extract sample real financial report build new large scale qa dataset contain tabular textual data name tat qa numerical reason usually require infer answer addition subtraction multiplication division count comparison sort compositions propose novel qa model term tagop capable reason table text adopt sequence tag extract relevant cells table along relevant span text infer semantics apply symbolic reason set aggregation operators arrive final answer tagopachieves five hundred and eighty inf1 one hundred and eleven absolute increase previous best baseline model accord experiment tat qa result still lag far behind performance expert human ie908 f1 demonstrate tat qa challenge serve benchmark train test powerful qa model address hybrid form data
higher order methods dependency parse partially fully address issue edge dependency tree construct text span subtree level rather word level shortcoming incorrect span cover correspond tree root certain word though word correctly link head paper propose new method dependency parse address issue propose method construct dependency tree directly model span span word subtree subtree relations consist two modules text span proposal module propose candidate text span represent subtree dependency tree denote root start end span link module construct link propose span use machine read comprehension mrc framework backbone formalize span link module mrc setup one span use query extract text span subtree link propose method come follow merit one address fundamental problem edge dependency tree construct subtrees two mrc framework allow method retrieve miss span span proposal stage lead higher recall eligible span extensive experiment ptb ctb universal dependencies ud benchmarks demonstrate effectiveness propose method able achieve new sota performances ptb ud benchmarks competitive performances previous sota model ctb dataset code available https githubcom shannonai mrc dependency parse
discocat model coecke et al two thousand and ten prove valuable tool study compositional aspects language level semantics strong dependency pregroup grammars pose important restrictions first prevent large scale experimentation due absence pregroup parser second limit expressibility model context free grammars paper solve problems reformulate discocat passage combinatory categorial grammar ccg category semantics start show standard categorial grammars express biclosed category rule emerge curry uncurrying identity proceed model permutation induce rule exploit symmetry compact close category encode word mean provide proof concept method convert alice wonderland discocat form corpus make available community
paper provide account port text data mine course online summer two thousand and twenty result covid nineteen pandemic improve second pilot run describe course adapt two pilot run teach techniques use improve students learn community build online also provide information relentless feedback collect course help us adapt teach one session next one pilot next discuss lessons learn promote use innovative teach techniques apply digital digital badge pair program break room teach natural language process course beginners students different background
text sql task seq seq model often lead sub optimal performance due limitations architecture paper present simple yet effective approach adapt transformer base seq seq model robust text sql generation instead induce constraint decoder reformat task slot fill propose train seq seq model schema aware denoising sead consist two denoising objectives train model either recover input predict output two novel erosion shuffle noise denoising objectives act auxiliary task better model structural data s2s generation addition improve propose clause sensitive execution guide eg decode strategy overcome limitation eg decode generative model experiment show propose method improve performance seq seq model schema link grammar correctness establish new state art wikisql benchmark result indicate capacity vanilla seq seq architecture text sql may estimate
talkmoves innovative application design support k twelve mathematics teachers reflect continuously improve instructional practice application combine state art natural language process capabilities automate speech recognition automatically analyze classroom record provide teachers personalize feedback use specific type discourse aim broaden deepen classroom conversations mathematics specific discourse strategies refer talk move within mathematics education community prior research document ways systematic use discourse strategies positively impact student engagement learn article describe talkmoves application cloud base infrastructure manage process classroom record interface provide teachers feedback use talk move individual teach episodes present series model architectures develop study conduct develop best perform transformer base model f1 seven hundred and ninety-three also discuss several technical challenge need address work real world speech language data noisy k twelve classrooms
natural language contexts display logical regularities respect substitutions relate concepts capture functional order theoretic property call monotonicity certain class nli problems result entailment label depend context monotonicity relation substitute concepts build previous techniques aim improve performance nli model problems consistent performance across upward downward monotone contexts still seem difficult attain even state art model end reframe problem context monotonicity classification make compatible transformer base pre train nli model add task train pipeline furthermore introduce sound complete simplify monotonicity logic formalism describe treatment contexts abstract units use notions formalism adapt target challenge set investigate whether intermediate context monotonicity classification task aid nli model performance examples exhibit monotonicity reason
graph text generation benefit pre train language model plms achieve better performance structure graph encoders however fail fully utilize structure information input graph paper aim improve performance pre train language model propose structure graph text model two step fine tune mechanism first fine tune model wikipedia adapt graph text generation addition use traditional token position embeddings encode knowledge graph kg propose novel tree level embed method capture inter dependency structure input graph new approach significantly improve performance text generation metrics english webnlg two thousand and seventeen dataset
dialogue state track essential part goal orient dialogue systems state track model often fail handle unseen service paper propose sgd qa simple extensible model schema guide dialogue state track base question answer approach propose multi pass model share single encoder domain information dialogue utterance domain description represent query dialogue utterance serve context model improve performance unseen service least 16x compare single pass baseline model sgd dataset sgd qa show competitive performance compare state art multi pass model significantly efficient term memory consumption train performance provide thorough discussion model ablation study error analysis
conversational information seek cis play increasingly important role connect people information due lack suitable resource previous study cis limit study theoretical conceptual frameworks laboratory base user study particular aspect cis eg ask clarify question work make efforts facilitate research cis three aspects one formulate pipeline cis six sub task intent detection id keyphrase extraction ke action prediction ap query selection qs passage selection ps response generation rg two release benchmark dataset call wizard search engine wise allow comprehensive depth research aspects cis three design neural architecture capable train evaluate jointly separately six sub task devise pre train fine tune learn scheme reduce requirements wise scale make full use available data report useful characteristics cis base statistics wise also show best perform model variant isable achieve effective cis indicate several metrics release dataset code well evaluation script facilitate future research measure improvements important research direction
influencers key nature network information propagation social media influencers particularly important political discourse engagement issue may derive legitimacy either solely partly online operation offline sphere expertise entertainers journalists etc quantify influencers political engagement polarity use google universal sentence encoder use encode tweet 6k influencers 26k indian politicians political crises india obtain aggregate vector representations influencers base tweet embeddings alongside retweet graph help compute stance polarity influencers respect political issue find covid nineteen confluence influencers side government three contentious issue around citizenship kashmir statehood farmers protest mainly government align fan account amplify incumbent position propose method offer insight political schisms present day india also offer mean study influencers polarization contexts
give video video ground aim retrieve temporal moment semantically correspond language query work propose parallel attention network sequence match seqpan address challenge task multi modal representation learn target moment boundary prediction design self guide parallel attention module effectively capture self modal contexts cross modal attentive information video text inspire sequence label task natural language process split grind truth moment begin inside end regions propose sequence match strategy guide start end boundary predictions use region label experimental result three datasets show seqpan superior state art methods furthermore effectiveness self guide parallel attention module sequence match module verify
neural machine translation nmt currently exhibit bias produce translations short overgenerating frequent word show poor robustness copy noise train data domain shift recent work tie shortcomings beam search de facto standard inference algorithm nmt eikema aziz two thousand and twenty propose use minimum bay risk mbr decode unbiased sample instead paper empirically investigate properties mbr decode number previously report bias failure case beam search find mbr still exhibit length token frequency bias owe mt metrics use utility function mbr also increase robustness copy noise train data domain shift
draft follow ackermannian lower bind reachability problem vector addition systems state vass recently announce czerwi nski orlikowski independently result announce leroux significantly different proof provide simplification former construction thus improve lower bind vass fix dimension czerwi nski orlikowski prove fk hardness dimension 6k leroux dimension 4k9 simplify construction yield fk hardness already dimension 3k2
web search query rather ambiguous paris hilton mean find latest news celebrity find specific hotel paris worldwide twenty parises propose solve ambiguity problem derive entity base query interpretations give query task link suitable part query semantically compatible entities background knowledge base suggest approach identify reasonable interpretations query base contain entities focus effectiveness also efficiency since web search response time exceed hundreds milliseconds approach propose use query segmentation pre process step find promise segment base skeletons skeletons enhance interpretations link contain segment entities knowledge base rank interpretations final step experimental comparison corpus two thousand, eight hundred query show approach better interpretation accuracy better run time previously effective query entity link methods
word vector representations open new opportunities extract useful information unstructured text define word vector make easy machine learn algorithms understand text extract information word vector representations use many applications word synonyms word analogy syntactic parse many others glove base word contexts matrix vectorization ef fective vector learn algorithm improve previous vector learn algorithms however glove model fail explicitly consider order word appear within contexts paper multiple methods incorporate word order glove word embeddings propose experimental result show word order vector weave word embeddings approach outperform unmodified glove natural lan guage task analogy completion word similarity weave direct concatenation slightly outperform glove word similarity task increase average rank two however greatly improve glove baseline word analogy task achieve average three thousand, six hundred and thirty-four improvement accuracy
current storytelling systems focus ongenerating stories coherent plot regard less narration style impor tant controllable text generation fore propose new task stylize story gen eration namely generate stories speci fied style give lead context tacklethe problem propose novel generationmodel first plan stylize keywordsand generate whole story theguidance keywords besides pro pose two automatic metrics evaluate theconsistency generate story andthe specify style experiment demonstratesthat model controllably generateemo tion drive orevent drive stories base onthe rocstories dataset mostafazadeh et al2016 study present insights stylizedstory generation research
background clear language make communication easier two party layman may difficulty communicate professional due understand specialize term common domain healthcare rare find layman knowledgeable medical terminology lead poor understand condition treatment bridge gap several professional vocabularies ontologies create map laymen medical term professional medical term vice versa objective many present vocabularies build manually semi automatically require large investments time human effort consequently slow growth vocabularies paper present automatic method enrich laymen vocabularies benefit able apply vocabularies domain methods entirely automatic approach use machine learn specifically global vectors word embeddings glove corpus collect social media healthcare platform extend enhance consumer health vocabularies chv approach improve chv incorporate synonyms hyponyms wordnet ontology basic glove novel algorithms incorporate wordnet evaluate use two laymen datasets national library medicine nlm open access consumer health vocabulary oac chv medlineplus healthcare vocabulary result result show glove able find new laymen term f score four thousand, eight hundred and forty-four furthermore enhance glove approach outperform basic glove average f score sixty-one relative improvement twenty-five furthermore enhance glove show statistical significance two grind truth datasets p001
heterogeneity sentence exist sequence sequence task machine translation sentence largely vary mean grammatical structure may increase difficulty convergence train network paper introduce model resolve heterogeneity sequence sequence task multi filter gaussian mixture autoencoder mgmae utilize autoencoder learn representations input representations output encoder lie latent space whose dimension hide dimension encoder representations train data latent space use train gaussian mixtures latent space representations divide several mixtures gaussian distributions filter decoder tune fit data one gaussian distributions specifically gaussian correspond one filter filter responsible heterogeneity within gaussian thus heterogeneity train data resolve comparative experiment conduct geo query dataset english french translation experiment show compare traditional encoder decoder model network achieve better performance sequence sequence task machine translation question answer
recent years internet users report adverse drug events ade social media blog health forums large volume report pharmacovigilance seek resort nlp monitor outlets propose first time use spanbert architecture task ade extraction new version popular bert transformer show improve capabilities multi token text span validate hypothesis experiment two datasets smm4h cadec different text typologies tweet blog post find spanbert combine crf outperform competitors
paper revisit math word problemsmwps cross lingual multilingual perspective construct mwp solvers pretrained multilingual language model use sequence sequence model copy mechanism compare mwp solvers perform cross lingual multilingual scenarios facilitate comparison cross lingual performance first adapt large scale english dataset mathqa counterpart chinese dataset math23k extend several english datasets bilingual datasets machine translation plus human annotation experiment show mwp solvers may transfer different language even target expressions operator set constants cross lingual multilingual case better generalize problem type exist source language target language
introduce first study automatic detoxification russian texts combat offensive language kind textual style transfer use instance process toxic content social media much work do english language field never solve russian language yet test two type model unsupervised approach base bert architecture perform local corrections supervise approach base pretrained language gpt two model compare several baselines addition describe evaluation setup provide train datasets metrics automatic evaluation result show test approach successfully use detoxification although room improvement
transformer base model demonstrate excellent capabilities capture pattern structure natural language generation achieve state art result many task paper present transformer base model multi turn dialog response generation solution base hybrid approach augment transformer base generative model novel retrieval mechanism leverage memorize information train data via k nearest neighbor search system evaluate two datasets make customer assistant dialogs taskmaster one release google hold high quality goal orient conversational data proprietary dataset collect real customer service call center achieve better bleu score strong baselines
important part artificial intelligence ai question answer qa aim generate answer question phrase natural language substantial progress open domain question answer qa systems still struggle answer question involve geographic entities concepts require spatial operations paper discuss problem geographic question answer geoqa first investigate reason geographic question difficult answer analyze challenge geographic question discuss uniqueness geographic question compare general qa review exist work geoqa classify type question address base survey provide generic classification framework geographic question finally conclude work point unique future research directions geoqa
consider problem collectively detect multiple events particularly cross sentence settings key deal problem encode semantic information model event inter dependency document level paper reformulate seq2seq task propose multi layer bidirectional network mlbinet capture document level association events semantic information simultaneously specifically bidirectional decoder firstly devise model event inter dependency within sentence decode event tag vector sequence secondly information aggregation module employ aggregate sentence level semantic event tag information finally stack multiple bidirectional decoders fee cross sentence information form multi layer bidirectional tag architecture iteratively propagate information across sentence show approach provide significant improvement performance compare current state art result
exist multilingual machine translation approach mainly focus english centric directions non english directions still lag behind work aim build many many translation system emphasis quality non english language directions intuition base hypothesis universal cross language representation lead better multilingual translation performance end propose method train method obtain single unify multilingual translation model mcolt empower two techniques contrastive learn scheme close gap among representations different languages ii data augmentation multiple parallel monolingual data align token representations english centric directions mcolt achieve competitive even better performance strong pre train model mbart tens wmt benchmarks non english directions mcolt achieve improvement average ten bleu compare multilingual baseline
event detection ed aim detect event trigger word sentence classify specific event type real world applications ed typically sufficient label data thus formulate shoot learn problem tackle issue low sample diversity shoot ed propose novel knowledge base shoot event detection method use definition base encoder introduce external event knowledge knowledge prior event type furthermore external knowledge typically provide limit imperfect coverage event type introduce adaptive knowledge enhance bayesian meta learn method dynamically adjust knowledge prior event type experiment show method consistently substantially outperform number baselines least fifteen absolute f1 point shoot settings
distantly supervise ds relation extraction attract much attention past years utilize large scale auto label data however evaluation long problem previous work either take costly inconsistent methods manually examine small sample model predictions directly test model auto label data check produce much fifty-three wrong label entity pair level popular nyt10 dataset problem lead inaccurate evaluation also make hard understand leave improve research ds evaluate ds model credible way build manually annotate test set two ds datasets nyt10 wiki20 thoroughly evaluate several competitive model especially latest pre train ones experimental result show manual evaluation indicate different conclusions automatic ones especially unexpected observations eg pre train model achieve dominate performance susceptible false positives compare previous methods hope manual test set novel observations help advance future ds research
recent years abstractive text summarization multimodal input start draw attention due ability accumulate information different source modalities generate fluent textual summary however exist methods use short videos visual modality short summary grind truth therefore perform poorly lengthy videos long grind truth summary additionally exist benchmark dataset generalize task videos vary lengths paper introduce aviate first large scale dataset abstractive text summarization videos diverse duration compile presentations well know academic conferences like ndss icml neurips etc use abstract correspond research paper reference summaries ensure adequate quality uniformity grind truth propose name factorize multi modal transformer base decoder language model inherently capture intra modal inter modal dynamics within various input modalities text summarization task name utilize increase number self attentions capture multimodality perform significantly better traditional encoder decoder base network extensive experiment illustrate name achieve significant improvement baselines qualitative quantitative evaluations exist how2 dataset short videos newly introduce aviate dataset videos diverse duration beat best baseline two datasets one hundred and thirty-nine two hundred and seventy-four rouge l point respectively
today see ever increase number clinical note contain clinical result image textual descriptions patient health state data analyze employ cater novel service help people domain experts common healthcare task however many technologies deep learn tool like word embeddings start investigate recently many challenge remain open come healthcare domain applications address challenge propose use deep learn word embeddings identify sixteen morbidity type within textual descriptions clinical record purpose use deep learn model base bidirectional long short term memory lstm layer exploit state art vector representations data word embeddings employ pre train word embeddings namely glove word2vec word embeddings train target domain furthermore compare performances deep learn approach traditional tf idf use support vector machine multilayer perceptron baselines obtain result seem latter outperform combination deep learn approach use word embeddings preliminary result indicate specific feature make dataset bias favour traditional machine learn approach
extensive research target dependent sentiment classification tsc lead strong classification performances domains author tend explicitly express sentiment specific entities topics review social media investigate tsc news article much less research domain despite importance news essential information source individual societal decision make article introduce newstsc manually annotate dataset explore tsc news article investigate characteristics sentiment news contrast popular tsc domains find sentiment news express less explicitly dependent context readership require greater degree interpretation extensive evaluation find state art tsc perform worse news article domains average recall avgrec six hundred and ninety-eight newstsc compare avgrev seven hundred and fifty-six eight hundred and twenty-two establish tsc datasets reason include incorrectly resolve relation target sentiment bear phrase context dependence major improvement previous news tsc find bert natural language understand capabilities capture less explicit sentiment use news article
emerge recipe achieve state art effectiveness neural document rank involve utilize large pre train language model eg bert evaluate individual passages document aggregate output pool additional transformer layer major drawback approach high query latency due cost evaluate every passage document bert make matter worse high inference cost latency vary base length document longer document require time computation address challenge adopt intra document cascade strategy prune passages candidate document use less expensive model call esm run score model expensive effective call etm find best train esm short efficient student model via knowledge distillation etm short effective teacher model eg bert prune allow us run etm model smaller set passages whose size vary document length experiment ms marco trec deep learn track benchmarks suggest propose intra document cascade rank model idcm lead four hundred lower query latency provide essentially effectiveness state art bert base document rank model
distributional semantics deeply change last decades first predict model steal thunder traditional count ones recently replace many nlp applications contextualized vectors produce transformer neural language model although extensive body research devote distributional semantic model dsm evaluation still lack thorough comparison respect test model semantic task benchmark datasets moreover previous work mostly focus task drive evaluation instead explore differences way model represent lexical semantic space paper perform comprehensive evaluation type distributional vectors either produce static dsms obtain average contextualized vectors generate bert first investigate performance embeddings several semantic task carry depth statistical analysis identify major factor influence behavior dsms result show allege superiority predict base model apparent real surely ubiquitous ii static dsms surpass contextualized representations context semantic task datasets furthermore borrow cognitive neuroscience methodology representational similarity analysis rsa inspect semantic space generate distributional model rsa reveal important differences relate frequency part speech lexical items
present simplify task agnostic multi modal pre train approach accept either video text input variety end task exist pre train task specific adopt either single cross modal encoder require modalities limit use retrieval style end task complex multitask learn two unimodal encoders limit early cross modal fusion instead introduce new pretraining mask scheme better mix across modalities eg force mask text predict closest video embeddings also maintain separability eg unimodal predictions sometimes require without use input experimental result show strong performance across wider range task previous methods often outperform task specific pre train
general data protection regulation gdpr become standard law data protection many countries currently twelve countries adopt regulation establish gdpr like regulation however evaluate differences similarities gdpr like regulations time consume need lot manual effort legal experts moreover gdpr like regulations different countries write languages lead difficult task since legal experts know languages essential paper investigate simple natural language process nlp approach tackle problem first extract chunk information gdpr like document form structure data natural language next use nlp methods compare document measure similarity finally manually label small set data evaluate approach empirical result show bert model cosine similarity outperform baselines data code publicly available
distantly supervision automatically generate plenty train sample relation extraction however also incur two major problems noisy label imbalanced train data previous work focus reduce wrongly label relations false positives explore miss relations cause incompleteness knowledge base false negative furthermore quantity negative label overwhelmingly surpass positive ones previous problem formulations paper first provide thorough analysis challenge cause negative data next formulate problem relation extraction positive unlabeled learn task alleviate false negative problem thirdly propose pipeline approach dub textscrere perform sentence level relation detection subject object extraction achieve sample efficient train experimental result show propose method consistently outperform exist approach remain excellent performance even learn large quantity false positive sample
probe model devise investigate encode knowledge eg syntactic structure contextual representations probe often design simplicity lead restrictions probe design may allow full exploitation structure encode information one restriction linearity examine case structural probe hewitt man two thousand and nineteen aim investigate encode syntactic structure contextual representations learn linear transformations observe structural probe learn metric able kernelize develop novel non linear variant identical number parameters test six languages find radial basis function rbf kernel conjunction regularization achieve statistically significant improvement baseline languages imply least part syntactic knowledge encode non linearly conclude discuss rbf kernel resemble bert self attention layer speculate resemblance lead rbf base probe stronger performance
recently unsupervised parse syntactic tree gain considerable attention prototypical approach unsupervised parse employ reinforcement learn auto encoders however mechanism ensure learn model leverage well understand language grammar propose approach utilize generic linguistic knowledge language present form syntactic rule thus induce better syntactic structure introduce novel formulation take advantage syntactic grammar rule independent base system achieve new state art result two benchmarks datasets mnli wsj source code paper available https githubcom anshuln diorawithrules
text generation become one important yet challenge task natural language process nlp resurgence deep learn greatly advance field neural generation model especially paradigm pretrained language model plms paper present overview major advance achieve topic plms text generation preliminaries present general task definition briefly describe mainstream architectures plms text generation core content discuss adapt exist plms model different input data satisfy special properties generate text summarize several important fine tune strategies text generation finally present several future directions conclude paper survey aim provide text generation researchers synthesis pointer relate research
logical reason closely relate human cognition vital importance human understand texts recent years witness increase attentions machine logical reason abilities however previous study commonly apply ad hoc methods model pre define relation pattern link name entities consider global knowledge components relate commonsense without local perception complete facts events methodology obviously insufficient deal complicate logical structure therefore argue natural logic units would group backbone constituents sentence subject verb object form facts cover global local knowledge piece necessary basis logical reason beyond build ad hoc graph propose general convenient fact drive approach construct supergraph top newly define fact units enhance supergraph explicit guidance local question option interactions experiment two challenge logical reason benchmark datasets reclor logiqa show propose model textscfocal reasoner outperform baseline model dramatically also smoothly apply downstream task mutual dialogue reason dataset achieve competitive result
biology offer many examples large scale complex concurrent systems many process take place parallel compete resources influence behavior scalable model biological systems continue active field research paper introduce new approach base event b state base formal method refinement central ingredient allow us check model consistency step step automate way approach base function lead elegant concise model method demonstrate approach construct knowledge largest ever build event b model describe erbb signal pathway key evolutionary pathway significant role development many type cancer event b model erbb pathway describe one thousand, three hundred and twenty molecular reactions two hundred and forty-two events
cloud native application cnapp distribute system collection independent components micro service interact via communication protocols give rise present abstract architecture cnapp dynamically configurable acyclic direct multi graph vertices microservices edge protocols generic mechanisms reconfigurations evidently correspond higher level function functionals imply also internal abstract architecture microservice collection event trigger serverless function include function implement protocols dynamically compose event dependent data flow graph generic mechanisms compositions correspond calculus functionals relations
contemporary approach perception plan estimation control allow robots operate robustly remote surrogates uncertain unstructured environments opportunity robots operate isolation also alongside humans complex environments natural language provide efficient flexible medium humans communicate collaborative robots significant progress statistical methods natural language understand robots able interpret diverse array free form navigation manipulation mobile manipulation command however contemporary approach require detail prior spatial semantic map robot environment model space possible referents utterance consequently methods fail robots deploy new previously unknown partially observe environments particularly mental model environment differ human operator robot paper provide comprehensive description novel learn framework allow field service robots interpret correctly execute natural language instructions priori unknown unstructured environments integral approach use language sensor infer spatial topological semantic information implicit natural language utterances exploit information learn distribution latent environment model incorporate distribution probabilistic language ground model infer distribution symbolic representation robot action space use imitation learn identify belief space policy reason environment behavior distributions evaluate framework variety different navigation mobile manipulation experiment
present first large scale corpus entity resolution email conversations cerec corpus consist six thousand and one email thread enron email corpus contain thirty-six thousand, four hundred and forty-eight email message sixty thousand, three hundred and eighty-three entity coreference chain annotation carry two step process minimal manual effort experiment carry evaluate different feature performance four baselines create corpus task mention identification coreference resolution best performance five hundred and ninety-two f1 report highlight room improvement depth qualitative quantitative error analysis present understand limitations baselines consider
introduce novel top end end formulation document level discourse parse rhetorical structure theory rst framework formulation consider discourse parse sequence split decisions token boundaries use seq2seq network model split decisions framework facilitate discourse parse scratch without require discourse segmentation prerequisite rather yield segmentation part parse process unify parse model adopt beam search decode best tree structure search space high score tree extensive experiment standard english rst discourse treebank demonstrate parser outperform exist methods good margin end end parse parse gold segmentation importantly without use handcraft feature make faster easily adaptable new languages domains
event detection ed aim identify event trigger word give text classify event type current methods ed rely heavily train instance almost ignore correlation event type hence tend suffer data scarcity fail handle new unseen event type address problems formulate ed process event ontology population link event instance pre define event type event ontology propose novel ed framework entitle ontoed ontology embed enrich event ontology linkages among event type induce event event correlations base event ontology ontoed leverage propagate correlation knowledge particularly data rich data poor event type furthermore ontoed apply new unseen event type establish linkages exist ones experiment indicate ontoed predominant robust previous approach ed especially data scarce scenarios
pre train language model prlms demonstrate superior performance due strong ability learn universal language representations self supervise pre train however even help powerful prlms still challenge effectively capture task relate knowledge dialogue texts enrich correlations among speaker aware utterances work present spider structural pre train dialogue reader capture dialogue exclusive feature simulate dialogue like feature propose two train objectives addition original lm objectives one utterance order restoration predict order permute utterances dialogue context two sentence backbone regularization regularize model improve factual correctness summarize subject verb object triplets experimental result widely use dialogue benchmarks verify effectiveness newly introduce self supervise task
neural machine translation nmt model essentially joint language model condition source sentence partial translation therefore nmt model naturally involve mechanism language model lm predict next token base partial translation despite success nmt still suffer hallucination problem generate fluent inadequate translations main reason nmt pay excessive attention partial translation neglect source sentence extent namely overconfidence lm accordingly define margin nmt lm calculate subtract predict probability lm nmt model token margin negatively correlate overconfidence degree lm base property propose margin base token level objective mto margin base sentencelevel objective mso maximize margin prevent lm overconfident experiment wmt14 english german wmt19 chinese english wmt14 english french translation task demonstrate effectiveness approach one hundred and thirty-six one hundred and fifty sixty-three bleu improvements respectively compare transformer baseline human evaluation verify approach improve translation adequacy well fluency
abusive language massive problem online social platforms exist abusive language detection techniques particularly ill suit comment contain heterogeneous abusive language pattern ie abusive non abusive part due part lack datasets explicitly annotate heterogeneity abusive language tackle challenge provide annotate dataset abusive language eleven thousand comment youtube account heterogeneity dataset separately annotate comment whole individual sentence comprise comment propose algorithm use supervise attention mechanism detect categorize abusive content use multi task learn empirically demonstrate challenge use traditional techniques heterogeneous content comparative gain performance propose approach state art methods
recently sequence sequence model make remarkable progress task keyphrase generation kg concatenate multiple keyphrases predefined order target sequence train however keyphrases inherently unordered set rather order sequence impose predefined order introduce wrong bias train highly penalize shift order keyphrases work propose new train paradigm one2set without predefining order concatenate keyphrases fit paradigm propose novel model utilize fix set learn control cod condition generate set keyphrases parallel solve problem correspondence prediction target train propose k step target assignment mechanism via bipartite match greatly increase diversity reduce duplication ratio generate keyphrases experimental result multiple benchmarks demonstrate approach significantly outperform state art methods
commonsense generation challenge task generate plausible sentence describe everyday scenario use provide concepts requirement reason commonsense knowledge compositional generalization ability even puzzle strong pre train language generation model propose novel framework use retrieval methods enhance pre train fine tune commonsense generation retrieve prototype sentence candidates concept match use auxiliary input fine tune boost performance trainable sentence retriever demonstrate experimentally large scale commongen benchmark approach achieve new state art result
contend hate speech social media one challenge social problems time various type anti social behavior social media foremost aggressive behavior cause many social issue affect social live mental health social media users paper propose end end ensemble base architecture automatically identify classify aggressive tweet tweet classify three categories covertly aggressive overtly aggressive non aggressive propose architecture ensemble smaller subnetworks able characterize feature embeddings effectively demonstrate qualitatively smaller subnetworks able learn unique feature best model ensemble capsule network result six hundred and fifty-two f1 score facebook test set result performance gain ninety-five trac two thousand and eighteen winners code model weight publicly available https githubcom parthpatwa hater genius aggression classification use capsule network
propose new task dataset common problem social science research upsampling coarse document label fine grain label span pose problem question answer format answer provide fine grain label provide benchmark dataset baselines socially impactful task identify exact crowd size protest demonstrations unite state give order magnitude information protest attendance small sample fine grain examples english language news text evaluate several baseline model include zero shoot result rule base question answer model shoot model fine tune small set document weakly supervise model use larger set coarsely label document find rule base model initially outperform zero shoot pre train transformer language model fine tune small subset twenty-five examples substantially improve sample performance also demonstrate method fine tune transformer span coarse label perform similarly rule base approach work contribute social scientists ability generate data understand cause successes collective action
accord freud word originally magic day word retain much ancient magical power word behaviors transform problems solve way use word reveal intentions goals value novel tool text analysis help understand magical power word power multiply combine study social network ie analysis relationships among social units special issue international journal information management entitle combine social network analysis text mine theory practice include heterogeneous innovative research nexus text mine social network analysis aim enrich work intersection field still lag behind theoretical empirical methodological foundations nine article accept inclusion special issue present methods tool business applications summarize editorial introduction
prior work prove translation memory tm boost performance neural machine translation nmt contrast exist work use bilingual corpus tm employ source side similarity search memory retrieval propose new framework use monolingual memory perform learnable memory retrieval cross lingual manner framework unique advantage first cross lingual memory retriever allow abundant monolingual data tm second memory retriever nmt model jointly optimize ultimate translation goal experiment show propose method obtain substantial improvements remarkably even outperform strong tm augment nmt baselines use bilingual tm own ability leverage monolingual data model also demonstrate effectiveness low resource domain adaptation scenarios
present four type neural language model train large historical dataset book english publish one thousand, seven hundred and sixty one thousand, nine hundred comprise fifty-one billion tokens language model architectures include static word2vec fasttext contextualized model bert flair architecture train model instance use whole dataset additionally train separate instance text publish one thousand, eight hundred and fifty two static model four instance consider different time slice bert model already use various downstream task consistently improve performance paper describe model create outline reuse potential
educational content label proper knowledge components kcs particularly useful teachers content organizers however manually label educational content labor intensive error prone address challenge prior research propose machine learn base solutions auto label educational content limit success work significantly improve prior research one expand input type include kc descriptions instructional video title problem descriptions ie three type prediction task two double granularity prediction one hundred and ninety-eight three hundred and eighty-five kc label ie practical set much harder multinomial classification problem three improve prediction accuracies five twenty-three use task adaptive pre train bert outperform six baselines four propose simple evaluation measure recover fifty-six seventy-three mispredicted kc label cod data set experiment available athttps githubcom tbs17 tapt bert
article present description systems part participation share task namely artificial intelligence legal assistance aila two thousand and nineteen integral event forum information retrieval evaluation two thousand and nineteen outcomes track would helpful automation work process indian judiciary system manual work procedures documentation level lower higher court judiciary system complex nature systems produce part track would assist law practitioners would helpful common men kind track also open path research natural language process nlp judicial domain track define two problems task one identify relevant prior case give situation task two identify relevant statutes give situation tackle propose approach base bm25 doc2vec per result declare task organizers 3rd modest position task one task two respectively
present algorithm base multi layer transformers identify adverse drug reactions adr social media data model rely properties problem characteristics contextual word embeddings extract two view document classifier train view label set unlabeled document use initializer new classifier view finally initialize classifier view train use initial train examples evaluate model largest publicly available adr dataset experiment testify model significantly outperform transformer base model pretrained domain specific data
present report evaluate contextualizing hate speech classifiers post hoc explanation paper within scope ml reproducibility challenge two thousand and twenty work focus aspects constitute paper method validity state result follow section describe paper relate work algorithmic frameworks experiment evaluations
paper explore task difficulty controllable question generation dcqg aim generate question require difficulty level previous research task mainly define difficulty question whether correctly answer question answer qa system lack interpretability controllability work redefine question difficulty number inference step require answer argue question generation qg systems stronger control logic generate question end propose novel framework progressively increase question difficulty step step rewrite guidance extract reason chain dataset automatically construct facilitate research extensive experiment conduct test performance method
learn high quality sentence representations benefit wide range natural language process task though bert base pre train language model achieve high performance many downstream task native derive sentence representations prove collapse thus produce poor performance semantic textual similarity sts task paper present consert contrastive framework self supervise sentence representation transfer adopt contrastive learn fine tune bert unsupervised effective way make use unlabeled texts consert solve collapse issue bert derive sentence representations make applicable downstream task experiment sts datasets demonstrate consert achieve eight relative improvement previous state art even comparable supervise sbert nli incorporate nli supervision achieve new state art performance sts task moreover consert obtain comparable result one thousand sample available show robustness data scarcity scenarios
exist emotion aware conversational model usually focus control response content align specific emotion class whereas empathy ability understand concern feel experience others hence critical learn cause evoke users emotion empathetic respond aka emotion cause gather emotion cause online environments leverage counsel strategies develop empathetic chatbot utilize causal emotion information real world online dataset verify effectiveness propose approach compare chatbot several sota methods use automatic metrics expert base human judgements well user base online evaluation
modern multi document summarization mds methods base transformer architectures generate state art summaries lack explainability focus graph base transformer model mds gain recent popularity aim improve explainability graph base mds analyze attention weight graph base mds graphsum vertices represent textual units edge form similarity graph units compare graphsum performance utilize different textual units e sentence versus paragraph two news benchmark datasets namely wikisum multinews experiment show paragraph level representations provide best summarization performance thus subsequently focus oanalysisn analyze paragraph level attention weight graphsum multi head decode layer order improve explainability transformer base mds model reference metric calculate rouge score input paragraph sentence generate summary indicate source origin information via text similarity observe high correlation attention weight reference metric especially later decode layer transformer architecture finally investigate generate summaries follow pattern positional bias extract paragraph provide information generate summary result show high correlation position summary source origin
lottery ticket hypothesis suggest parametrized network consist lottery ticket train certain collection ie subnetwork match performance full model paper study collection ticket refer win ticket extremely parametrized model eg pre train language model observe certain compression ratios generalization performance win ticket match also exceed full model particular observe phase transition phenomenon compression ratio increase generalization performance win ticket first improve deteriorate certain threshold refer ticket threshold super ticket show phase transition task model dependent model size become larger train data set become smaller transition become pronounce experiment glue benchmark show super ticket improve single task fine tune nine point bert base ten point bert large term task average score also demonstrate adaptively share super ticket across task benefit multi task learn
evolve gender neutral frame involuntary celibate identity concept incels come refer online community men bear antipathy towards women society large perceive inability find maintain sexual relationships explore incel language use reddit global online message board contextualize incel community online expressions misogyny real world act violence perpetrate women assemble around three million comment incel theme reddit channel analyze temporal dynamics data drive rank order glossary phrase belong emergent incel lexicon study reveal generation normalization extensive cod misogynist vocabulary service group identity
semeval task four aim find proper option multiple candidates resolve task machine read comprehension exist approach propose concat question option together form context aware model however argue straightforward concatenation provide coarse grain context mrc task ignore specific position option relative question paper propose novel mrc model fill options question produce fine grain context define summary better reveal relationship option question conduct series experiment give dataset result show approach outperform counterparts large extent
article give overview plagiarism domain focus academic plagiarism article define plagiarism explain origin term well plagiarism relate term identify extent plagiarism domain focus plagiarism subdomain text document give overview current classifications taxonomies propose comprehensive classification accord several criteria origin purpose technical implementation consequence complexity detection accord number linguistic source article suggest new classification academic plagiarism describe sort methods plagiarism type categories approach phase plagiarism detection classification methods algorithms plagiarism detection title article explicitly target academic community sufficiently general interdisciplinary useful many professionals like software developers linguists librarians
introduction transformer neural network change landscape natural language process nlp last years far none visualization systems yet manage examine facets transformers give us motivation current work propose new nlp transformer context sensitive visualization method leverage exist nlp tool find significant group tokens word greatest effect output thus preserve context original text first use sentence level dependency parser highlight promise word group dependency parser create tree relationships word sentence next systematically remove adjacent non adjacent tuples emphn tokens input text produce several new texts tokens miss result texts pass pre train bert model classification output compare full text difference activation strength record modify texts produce largest difference target classification output neuron select combination remove word consider influential model output finally influential word combinations visualize heatmap
covid nineteen pandemic drive ever greater demand tool enable efficient exploration biomedical literature although semi structure information result concept recognition detection define elements clinical trials eg pico criteria commonly use support literature search contributions abstraction remain poorly understand especially relation text base retrieval study compare result retrieve standard search engine filter use clinically relevant concepts relations analysis base annotations trec covid share task obtain quantitative well qualitative insights characteristics relational concept base literature exploration importantly find relational concept selection filter original retrieve collection way decrease proportion unjudged document increase precision mean user likely expose larger number relevant document
despite exist pioneer work sign language translation slt non trivial obstacle ie limit quantity parallel sign text data tackle parallel data bottleneck propose sign back translation signbt approach incorporate massive speak language texts slt train text gloss translation model first back translate monolingual text gloss sequence pair sign sequence generate splice piece estimate gloss sign bank feature level finally synthetic parallel data serve strong supplement end end train encoder decoder slt framework promote slt research contribute csl daily large scale continuous slt dataset provide speak language translations gloss level annotations topic revolve around people daily live eg travel shop medical care likely slt application scenario extensive experimental result analysis slt methods report csl daily propose sign back translation method obtain substantial improvement previous state art slt methods
backdoor attack kind insidious security threat machine learn model inject backdoor train victim model produce adversary specify output input embed predesigned trigger behave properly normal input inference sort emergent attack backdoor attack natural language process nlp investigate insufficiently far know almost exist textual backdoor attack methods insert additional content normal sample trigger cause trigger embed sample detect backdoor attack block without much effort paper propose use syntactic structure trigger textual backdoor attack conduct extensive experiment demonstrate syntactic trigger base attack method achieve comparable attack performance almost one hundred success rate insertion base methods possess much higher invisibility stronger resistance defenses result also reveal significant insidiousness harmfulness textual backdoor attack code data paper obtain https githubcom thunlp hiddenkiller
distributional semantics base neural approach cornerstone natural language process surprise connections human mean representation well recent transformer base language model prove capable produce contextual word representations reliably convey sense specific information simply product self supervision prior work show contextual representations use accurately represent large sense inventory sense embeddings extent distance base solution word sense disambiguation wsd task outperform model train specifically task still remain much understand use neural language model nlms produce sense embeddings better harness nlm mean representation abilities work introduce principled approach leverage information layer nlms inform probe analysis fourteen nlm variants also emphasize versatility sense embeddings contrast task specific model apply several sense relate task besides wsd demonstrate improve performance use propose approach prior work focus sense embeddings finally discuss unexpected find regard layer model performance variations potential applications downstream task
sememe define minimum semantic unit linguistics sememe knowledge base skbs comprise word annotate sememes enable sememes apply natural language process far large body research showcased unique advantage effectiveness skbs various task however languages skbs manual construction skbs time consume labor intensive tackle challenge propose simple fully automatic method build skb via exist dictionary use method build english skb french skb conduct comprehensive evaluations intrinsic extrinsic perspectives experimental result demonstrate automatically build english skb even superior hownet widely use skb take decades build manually english french skbs bring obvious performance enhancement multiple downstream task code data paper except copyright dictionaries obtain https githubcom thunlp dictskb
medical entity retrieval integral component understand communicate information across various health systems current approach tend work well specific medical domains generalize poorly unseen sub specialties increase concern public health crisis new medical condition drug treatments come light frequently zero shoot retrieval challenge due high degree ambiguity variability medical corpora make difficult build accurate similarity measure mention concepts medical knowledge graph kg however contain rich semantics include large number synonyms well curated graphical structure take advantage valuable information propose suite learn task design train efficient zero shoot entity retrieval model without require human annotation knowledge graph enrich architecture significantly outperform common zero shoot benchmarks include bm25 clinical bert seven thirty higher recall across multiple major medical ontologies umls snomed icd ten
extensive work argue favour pay crowd workers wage least equivalent yous federal minimum wage meanwhile research collect high quality annotations suggest use qualification require workers previously complete certain number task requesters pay fairly require workers complete large number task already workers need complete substantial amount poorly pay work earn fair wage analysis worker discussions guidance researchers estimate workers spend approximately two hundred and twenty-five months full time effort poorly pay task order get qualifications need better pay task discuss alternatives qualification conduct study correlation qualifications work quality two nlp task find possible reduce burden workers still collect high quality data
recently huge interest dialog systems interest also develop field medical domain researchers focus build dialog system medical domain research focus multi turn dialog system train multi turn dialog data difficult gather huge amount multi turn conversational data medical domain verify professionals trust however several frequently ask question faqs single turn qa pair information verify experts use build multi turn dialog system
performance state art neural rankers deteriorate substantially expose noisy input apply new domain paper present novel method fine tune neural rankers significantly improve robustness domain data query perturbations specifically contrastive loss compare data point representation space combine standard rank loss fine tune use relevance label denote similar dissimilar pair allow model learn underlie match semantics across different query document pair lead improve robustness experiment four passage rank datasets propose contrastive fine tune method obtain improvements robustness query reformulations noise perturbations zero shoot transfer bert bart base rankers additionally experiment show contrastive fine tune outperform data augmentation robustifying neural rankers
neural machine translation nmt model achieve state art performance many translation benchmarks active research field nmt knowledge distillation widely apply enhance model performance transfer teacher model knowledge train sample however previous work rarely discuss different impact connections among sample serve medium transfer teacher knowledge paper design novel protocol effectively analyze different impact sample compare various sample partition base protocol conduct extensive experiment find teacher knowledge better knowledge specific sample may even hurt whole performance knowledge distillation finally address issue propose two simple yet effective strategies ie batch level global level selections pick suitable sample distillation evaluate approach two large scale machine translation task wmt fourteen english german wmt nineteen chinese english experimental result show approach yield one hundred and twenty-eight eighty-nine bleu point improvements transformer baseline respectively
internet social media offer firm novel ways manage market strategy gain competitive advantage group users express internet particular topic product brand frequently call virtual tribe e tribe however automatic tool identify study characteristics virtual tribes towards aim paper present tribefinder system reveal twitter users tribal affiliations analyze tweet language use show potential instrument provide example consider three specific tribal macro categories alternative realities lifestyle recreation addition discuss different characteristics identify tribe term use language social interaction metrics tribefinder illustrate importance adopt new lens study virtual tribes crucial firm properly design market strategy scholars extend prior market research
arguably visual perception conversational agents physical world key way exhibit human like intelligence image ground conversation thus propose address challenge exist work focus explore multimodal dialog model grind conversation give image paper take step study image ground conversation fully open end set pair dialog image assume available specifically present maria neural conversation agent power visual world experience retrieve large scale image index maria consist three flexible components ie text image retriever visual concept detector visual knowledge ground response generator retriever aim retrieve correlate image dialog image index visual concept detector extract rich visual knowledge image response generator ground extract visual knowledge dialog context generate target response extensive experiment demonstrate maria outperform previous state art methods automatic metrics human evaluation generate informative responses visual commonsense physical world
background knowledge graph kgs especially medical knowledge graph often significantly incomplete necessitate demand medical knowledge graph completion medkgc medkgc find new facts base exit knowledge kgs path base knowledge reason algorithm one important approach task type method receive great attention recent years high performance interpretability fact traditional methods path rank algorithm pra take paths entity pair atomic feature however medical kgs sparse make difficult model effective semantic representation extremely sparse path feature sparsity medical kgs mainly reflect long tail distribution entities paths previous methods merely consider context structure paths knowledge graph ignore textual semantics symbols path therefore performance improve due two aspects entity sparseness path sparseness address issue paper propose two novel path base reason methods solve sparsity issue entity path respectively adopt textual semantic information entities paths medkgc use pre train model bert combine textual semantic representations entities relationships model task symbolic reason medical kg numerical compute issue textual semantic representation
incorporate external knowledge name entity recognition ner systems widely study generic domain paper focus clinical domain limit data accessible interpretability important recent advancement technology acceleration clinical trials result discovery new drug procedures well medical condition factor motivate towards build robust zero shoot ner systems quickly adapt new medical terminology propose auxiliary gazetteer model fuse ner system result better robustness interpretability across different clinical datasets gazetteer base fusion model data efficient achieve seventeen micro f1 gain i2b2 dataset use twenty train data bring forty-seven micro f1 gain novel entity mention never present train moreover fusion model able quickly adapt new mention gazetteers without train gain propose fusion model transferable relate datasets
find cod give natural language query isb eneficial productivity software developers future progress towards better semantic match query code require richer supervise train resources remedy introduce cosqa datasetit include twenty thousand, six hundred and four label pair natural language query cod annotate least three human annotators introduce contrastive learn method dub coclr enhance query code match work data augmenter bring artificially generate train instance show evaluate codexglue codebert model train cosqa improve accuracy code question answer fifty-one incorporate coclr bring improvement one hundred and five
propose measure fine grain domain relevance degree term relevant broad eg computer science narrow eg deep learn domain measurement crucial many downstream task natural language process handle long tail term build core anchor semantic graph use core term rich description information bridge vast remain fringe term semantically support fine grain domain without rely match corpus supervision develop hierarchical core fringe learn learn core fringe term jointly semi supervise manner contextualized hierarchy domain reduce expensive human efforts employ automatic annotation hierarchical positive unlabeled learn approach apply big small domains cover head tail term require little human effort extensive experiment demonstrate methods outperform strong baselines even surpass professional human performance
grow ubiquity internet thingsiot complex logic program resource constrain iot devices almost exclusively use c program language c provide low level control memory lack number high level program abstractions higher order function polymorphism strong static type memory safety automatic memory management present hailstorm statically type purely functional program language attempt address problem high level program language strict type discipline support feature like higher order function tail recursion automatic memory management program iot devices declarative manner applications run devices tend heavily dominate hailstorm track side effect likei type system use resource type choice allow us explore design purely functional standalone language area common embed functional core imperative language borrow combinators arrowized frp discrete time semantics design full set combinators work progress drive examples far evaluate hailstorm write standard examples literature earthquake detection railway cross system various clock systems also run examples grisp embed systems board generation erlang
field natural language understand experience exponential progress last years impressive result several task success motivate researchers study underlie knowledge encode model despite attempt understand semantic capabilities successful often lead non conclusive contradictory conclusions among different work via probe classifier extract underlie knowledge graph nine influential language model last years include word embeddings text generators context encoders probe base concept relatedness ground wordnet result reveal model encode knowledge suffer several inaccuracies furthermore show different architectures train strategies lead different model bias conduct systematic evaluation discover specific factor explain concepts challenge hope insights motivate development model capture concepts precisely
automate system could assist judge predict outcome case would help expedite judicial process system practically useful predictions system explainable promote research develop system introduce ildc indian legal document corpus ildc large corpus 35k indian supreme court case annotate original court decisions portion corpus separate test set annotate gold standard explanations legal experts base ildc propose task court judgment prediction explanation cjpe task require automate system predict explainable outcome case experiment battery baseline model case predictions propose hierarchical occlusion base model explainability best prediction model accuracy seventy-eight versus ninety-four human legal experts point towards complexity prediction task analysis explanations propose algorithm reveal significant difference point view algorithm legal experts explain judgments point towards scope future research
recent progress neural machine translation nmt make possible translate successfully monolingual language pair large parallel data exist pre train model improve performance even although exist work translate code mix settings one pair include text two languages still unclear recent success nmt language model exactly mean translate code mix text investigate one context namely mt code mix modern standard arabic egyptian arabic msaea english develop model different condition employ standard end end sequence sequence s2s transformers train scratch ii pre train s2s language model lms able acquire reasonable performance use msa en parallel data s2s model train scratch also find lms fine tune data various arabic dialects help msaea en task work context share task machine translation code switch best model achieve bf2572 bleu place us first official share task evaluation msaea en
natural language process nlp find data augmentation techniques produce high quality human interpretable examples always challenge recently leverage knn augment examples retrieve large repositories unlabelled sentence make step toward interpretable augmentation inspire paradigm introduce minimax knn sample efficient data augmentation strategy tailor knowledge distillation kd exploit semi supervise approach base kd train model augment data contrast exist knn augmentation techniques blindly incorporate sample method dynamically select subset augment sample maximize kl divergence teacher student model step aim extract efficient sample ensure augment data cover regions input space maximum loss value evaluate technique several text classification task demonstrate minimax knn consistently outperform strong baselines result show minimax knn require fewer augment examples less computation achieve superior performance state art knn base augmentation techniques
many exist conversation model base encoder decoder framework focus ways make encoder complicate enrich context vectors increase diversity informativeness generate responses however approach face two problems first decoder simple effectively utilize previously generate information tend generate duplicate self contradict responses second complex encoder tend generate diverse incoherent responses complex context vectors may deviate original semantics context work propose conversation model name think teamwork generation hover around impressive noticeable keywords make decoder complicate avoid generate duplicate self contradict responses model simplify context vectors increase coherence generate responses reasonable way model propose teamwork generation framework semantics extractor compare baselines automatic human evaluation show advantage model
data augmentation effective way improve performance many neural text generation model however current data augmentation methods need define choose proper data map function map original sample augment sample work derive objective formulate problem data augmentation text generation task without use augment data construct specific map function propose objective efficiently optimize apply popular loss function text generation task convergence rate guarantee experiment five datasets two text generation task show approach approximate even surpass popular data augmentation methods
ascent fully automate methodology extract consolidate commonsense assertions web content nguyen et al www two thousand and twenty-one advance traditional triple base commonsense knowledge representation capture semantic facets like locations purpose composite concepts ie subgroups relate aspects subject demo present web portal allow users understand construction process explore content observe impact use case question answer demo website introductory video available online
language model like bert spanbert pretrained open domain data obtain impressive gain various nlp task paper probe effectiveness domain adaptive pretraining objectives downstream task particular three objectives include novel objective focus model predicate argument relations evaluate two challenge dialogue understand task experimental result demonstrate domain adaptive pretraining proper objectives significantly improve performance strong baseline task achieve new state art performances
simple technique accelerate inference large scale pre train model early exit gain much attention nlp community allow sample exit early internal classifiers without pass entire model exist work usually train internal classifiers independently employ exit strategy decide whether exit base confidence current internal classifier however none work take full advantage fact internal classifiers train solve task therefore use construct ensemble paper show novel objective function train ensemble internal classifiers naturally induce perspective ensemble learn information theory propose train objective consist two term one accuracy diversity internal classifiers contrast objective use prior work exactly accuracy term train objective therefore optimize accuracy diversity propose simple vote base strategy consider predictions past internal classifiers infer correct label decide whether exit experimental result various nlp task show propose objective function vote base strategy achieve better accuracy speed trade off
large scale model learn fix dimensional cross lingual sentence representations like large scale model learn fix dimensional cross lingual sentence representations like laser artetxe schwenk 2019b lead significant improvement performance downstream task however increase modifications base large scale model usually impractical due memory limitations work introduce lightweight dual transformer architecture two layer generate memory efficient cross lingual sentence representations explore different train task observe current cross lingual train task leave lot desire shallow architecture ameliorate propose novel cross lingual language model combine exist single word mask language model newly propose cross lingual token level reconstruction task augment train task introduction two computationally lite sentence level contrastive learn task enhance alignment cross lingual sentence representation space compensate learn bottleneck lightweight transformer generative task comparisons compete model cross lingual sentence retrieval multilingual document classification confirm effectiveness newly propose train task shallow model
recent work xu et al two thousand and twenty suggest numeral systems different languages shape functional need efficient communication information theoretic sense take learn theoretic approach show efficient communication emerge via reinforcement learn framework two artificial agents play lewis signal game goal convey numeral concept agents gradually learn communicate use reinforcement learn result numeral systems show efficient information theoretic framework regier et al two thousand and fifteen gibson et al two thousand and seventeen also show similar human numeral systems type result thus provide mechanistic explanation via reinforcement learn recent result xu et al two thousand and twenty potentially generalize semantic domains
despite achievements large scale multimodal pre train approach cross modal retrieval eg image text retrieval remain challenge task bridge semantic gap two modalities previous study mainly focus word region alignment object level lack match linguistic relation among word visual relation among regions neglect relation consistency impair contextualized representation image text pair hinder model performance interpretability paper first propose novel metric intra modal self attention distance isd quantify relation consistency measure semantic distance linguistic visual relations response present inter modal alignment intra modal self attentions iais regularize train method optimize isd calibrate intra modal self attentions two modalities mutually via inter modal alignment iais regularizer boost performance prevail model flickr30k ms coco datasets considerable margin demonstrate superiority approach
performance efficiency crucial factor sequence label task many real world scenarios although pre train model ptms significantly improve performance various sequence label task computational cost expensive alleviate problem extend recent successful early exit mechanism accelerate inference ptms sequence label task however exist early exit mechanisms specifically design sequence level task rather sequence label paper first propose simple extension sentence level early exit sequence label task reduce computational cost also propose token level early exit mechanism allow partial tokens exit early different layer consider local dependency inherent sequence label employ window base criterion decide token whether exit token level early exit bring gap train inference introduce extra self sample fine tune stage alleviate extensive experiment three popular sequence label task show approach save sixty-six seventy-five inference cost minimal performance degradation compare competitive compress model distilbert approach achieve better performance speed ratios 2x 3x 4x
nlp community currently invest lot research resources development deep learn model train data make lot progress clear model learn kinds spurious pattern social bias annotation artifacts algorithmic solutions far limit success alternative actively discuss careful design datasets deliver specific signal position paper map arguments data curation argue fundamentally point moot curation already happen change world question much think want invest process
social network platforms generally use share positive constructive insightful content however recent time people often get expose objectionable content like threat identity attack hate speech insult obscene texts offensive remark bully exist work toxic speech detection focus binary classification differentiate toxic speech among small set categories paper describe system propose team cisco semeval two thousand and twenty-one task five toxic span detection first share task focus detect span text attribute toxicity english language approach problem primarily two ways sequence tag approach dependency parse approach sequence tag approach tag token sentence particular tag scheme best perform architecture approach also prove best perform architecture overall f1 score six thousand, nine hundred and twenty-two thereby place us 7th final evaluation phase leaderboard also explore dependency parse approach extract span input sentence supervision target span boundaries rank span use biaffine model finally also provide detail analysis result model performance paper
paper aim improve abstractive dialogue summarization quality time enable granularity control model two primary components stag one two stage generation strategy generate preliminary summary sketch serve basis final summary summary sketch provide weakly supervise signal form pseudo label interrogative pronoun categories key phrase extract use constituency parser two simple strategy control granularity final summary model automatically determine control number generate summary sentence give dialogue predict highlight different text span source text model achieve state art performance largest dialogue summarization corpus samsum high five thousand and seventy-nine rouge l score addition conduct case study show competitive human evaluation result controllability human annotate summaries
number problems process sound natural language well areas reduce simultaneously read input sequence write output sequence generally different length well develop methods produce output sequence base entirely know input however efficient methods enable transformations line exist paper introduce architecture learn reinforcement make decisions whether read token write another token architecture able transform potentially infinite sequence line experimental study compare state art methods neural machine translation produce slightly worse translations transformer outperform autoencoder attention even though architecture translate texts line thereby solve difficult problem reference methods
deep learn dl base language model achieve high performance various benchmarks natural language inference nli time symbolic approach nli receive less attention approach symbolic dl advantage weaknesses however currently method combine system solve task nli merge symbolic deep learn methods propose inference framework call neurallog utilize monotonicity base logical inference engine neural network language model phrase alignment framework model nli task classic search problem use beam search algorithm search optimal inference paths experiment show joint logic neural inference system improve accuracy nli task achieve state art accuracy sick med datasets
common ground process create maintain mutual understand critical aspect sophisticate human communication various task settings propose exist literature mostly focus create common grind static context ignore aspect maintain overtime dynamic context work propose novel task set study ability create maintain common grind dynamic environments base minimal task formulation collect large scale dataset five thousand, six hundred and seventeen dialogues enable fine grain evaluation analysis various dialogue systems dataset analyse highlight novel challenge introduce set usage complex spatio temporal expressions create maintain common grind finally conduct extensive experiment assess capabilities baseline dialogue system discuss future prospect research
aspect sentiment classification asc aim determine sentiments express towards different aspects sentence state art asc model achieve remarkable performance recently show suffer issue robustness particularly two common scenarios domains test train data different domain scenario test data adversarially perturb adversarial scenario asc model may attend irrelevant word neglect opinion expressions truly describe diverse aspects tackle challenge paper hypothesize position bias ie word closer concern aspect would carry higher degree importance crucial build robust asc model reduce probability mis attend accordingly propose two mechanisms capture position bias namely position bias weight position bias dropout flexibly inject exist model enhance representations classification experiment conduct domain adversarial datasets demonstrate propose approach largely improve robustness effectiveness current model
effectively perform task next word prediction long short term memory network lstms must keep track many type information information directly relate next word identity secondary eg discourse level feature feature downstream word correlate secondary information appear lstm representations even though part emphexplicitly supervise prediction task contrast reinforcement learn rl techniques explicitly supervise representations predict secondary information show beneficial inspire success propose predictive representation learn prl explicitly constrain lstms encode specific predictions like might need learn implicitly show prl one significantly improve two strong language model methods two converge quickly three perform better data limit work show explicitly encode simple predictive task facilitate search effective language model
translation natural language source code help software development enable developers comprehend ideate search write computer program natural language despite grow interest industry research community task often difficult due lack large standard datasets suitable train deep neural model standard noise removal methods evaluation benchmarks leave researchers collect new small scale datasets result inconsistencies across publish work study present codesc large parallel dataset compose forty-two million java methods natural language descriptions extensive analysis identify remove prevail noise pattern dataset demonstrate proficiency codesc two complementary task code description pair code summarization code search show dataset help improve code search twenty-two achieve new state art code summarization furthermore show codesc effectiveness pre train fine tune setup open possibilities build pretrained language model java facilitate future research release dataset data process tool benchmark urlhttps githubcom csebuetnlp codesc
exist visual question answer vqa systems tend overly rely language bias hence fail reason visual clue address issue propose novel language prior feedback lpf objective function balance proportion answer loss value total vqa loss lpf firstly calculate modulate factor determine language bias use question branch lpf assign self adaptive weight train sample train process reweighting mechanism lpf ensure total vqa loss reshape balance form mean sample require certain visual information predict efficiently use train method simple implement model agnostic end end trainable conduct extensive experiment result show lpf one bring significant improvement various vqa model two achieve competitive performance bias sensitive vqa cp v2 benchmark
sluice resolution problem system need output correspond antecedents wh ellipses antecedents elide content behind wh word implicitly refer use contexts previous work frame sluice resolution question answer set outperform precede work large margins ellipsis question referentially dependent expressions anaphoras retrieve correspond antecedents like answer question output piece clarify information however task fully solve therefore want investigate make sluice resolution differ question answer fill error gap also present result use recent state art question answer systems improve previous work eight thousand, six hundred and one nine thousand and thirty-nine f1
neural multimodal machine translation mmt system one aim perform better translation extend conventional text translation model multimodal information many recent study report improvements equip model multimodal module despite controversy whether improvements indeed come multimodal part revisit contribution multimodal information mmt devise two interpretable mmt model surprise although model replicate similar gain recently develop multimodal integrate systems achieve model learn ignore multimodal information upon investigation discover improvements achieve multimodal model text counterparts fact result regularization effect report empirical find highlight importance mmt model interpretability discuss find benefit future research
work scholarly document process assume information process trustworthy factually correct however always case two core challenge address one ensure scientific publications credible eg claim make without support evidence relevant support evidence provide two scientific find misrepresent distort outright misreported communicate journalists general public present first step towards address problems outline remain challenge
despite well develop cut edge representation learn language language representation model usually focus specific level linguistic units work introduce universal language representation learn ie embeddings different level linguistic units text quite diverse lengths uniform vector space propose train objective misad utilize meaningful n grams extract large unlabeled corpus simple effective algorithm pre train language model empirically verify well design pre train scheme may effectively yield universal language representation bring great convenience handle multiple layer linguistic object unify way especially model achieve highest accuracy analogy task different language level significantly improve performance downstream task glue benchmark question answer dataset
neural dialogue generation model train one hot target distribution suffer confidence issue lead poor generation diversity widely report literature although exist approach label smooth alleviate issue fail adapt diverse dialog contexts paper propose adaptive label smooth adalabel approach adaptively estimate target label distribution time step different contexts maximum probability predict distribution use modify soft target distribution produce novel light weight bi directional decoder module result target distribution aware previous future contexts adjust avoid train dialogue model model train end end manner extensive experiment two benchmark datasets show approach outperform various competitive baselines produce diverse responses
prune effective method reduce memory footprint computational cost associate large natural language process model however current approach either explore head prune limit prune ratio focus unstructured prune negligible effect real inference time power consumption address challenge develop novel multilevel structure prune mlpruning framework use three different level structure prune head prune row prune block wise sparse prune propose use learnable top k threshold employ adaptive regularization adjust regularization magnitude adaptively select appropriate prune ratios different weight matrices also propose two step pipeline combine block wise prune head row prune achieve high structure prune ratios minimum accuracy degradation empirical result show bertbase textapprox20 remain weight achieve accuracy comparable full model qqp mnli squad textapprox369x speedup framework open sourcedcitecodebase
differentiable architecture search dart successfully apply many vision task however directly use dart transformers memory intensive render search process infeasible end propose multi split reversible network combine dart specifically devise backpropagation reconstruction algorithm need store last layer output relieve memory burden dart allow us search larger hide size candidate operations evaluate search architecture three sequence sequence datasets ie wmt fourteen english german wmt fourteen english french wmt fourteen english czech experimental result show network consistently outperform standard transformers across task moreover method compare favorably big size evolve transformers reduce search computation order magnitude
neural model automate fact verification achieve promise result thank availability large human annotate datasets however new domain require fact verification create dataset manually write claim link support evidence expensive develop qacg framework train robust fact verification model use automatically generate claim support refute unverifiable evidence wikipedia qacg generate question answer pair evidence convert different type claim experiment fever dataset show qacg framework significantly reduce demand human annotate train data zero shoot scenario qacg improve roberta model f1 fifty seventy-seven equivalent performance 2k manually curated examples qacg code publicly available
hyperbolic neural network show great potential model complex data however exist hyperbolic network completely hyperbolic encode feature hyperbolic space yet formalize operations tangent space euclidean subspace origin hyperbolic space hybrid method greatly limit model ability network paper propose fully hyperbolic framework build hyperbolic network base lorentz model adapt lorentz transformations include boost rotation formalize essential operations neural network moreover also prove linear transformation tangent space use exist hyperbolic network relaxation lorentz rotation include boost implicitly limit capabilities exist hyperbolic network experimental result four nlp task show method better performance build shallow deep network code release facilitate follow research
document level mt model still far satisfactory exist work extend translation unit single sentence multiple sentence however study show enlarge translation unit whole document supervise train transformer fail paper find failure cause overfitting stick around local minima train analysis show increase complexity target source attention reason failure solution propose g transformer introduce locality assumption inductive bias transformer reduce hypothesis space attention target source experiment show g transformer converge faster stably transformer achieve new state art bleu score non pretraining pre train settings three benchmark datasets
multi source sequence generation msg important kind sequence generation task take multiple source include automatic post edit multi source translation multi document summarization etc msg task suffer data scarcity problem recent pretrained model prove effective low resource downstream task transfer pretrained sequence sequence model msg task essential although directly finetuning pretrained model msg task concatenate multiple source single long sequence regard simple method transfer pretrained model msg task conjecture direct finetuning method lead catastrophic forget solely rely pretrained self attention layer capture cross source information sufficient therefore propose two stage finetuning method alleviate pretrain finetune discrepancy introduce novel msg model fine encoder learn better representations msg task experiment show approach achieve new state art result wmt17 ape task multi source translation task use wmt14 test set adapt document level translation framework outperform strong baselines significantly
transformers advance field natural language process nlp variety important task cornerstone transformer architecture multi head attention mha mechanism model pairwise interactions elements sequence despite massive success current framework ignore interactions among different head lead problem many head redundant practice greatly waste capacity model improve parameter efficiency formulate mha latent variable model probabilistic perspective present cascade head collide attention coda explicitly model interactions attention head hierarchical variational distribution conduct extensive experiment demonstrate coda outperform transformer baseline six perplexity textttwikitext one hundred and three language model six bleu textttwmt14 en de machine translation due improvements parameter efficiencyfootnoteour implementation publicly available urlhttps githubcom lzhengisme coda
translation quality estimation critical reduce post edit efforts machine translation cross lingual corpus clean research problem quality estimation qe aim directly estimate quality translation give pair source target sentence highlight word need corrections without reference golden translations paper propose verdi novel framework word level sentence level post edit effort estimation bilingual corpora verdi adopt two word predictors enable diverse feature extract pair sentence subsequent quality estimation include transformer base neural machine translation nmt model pre train cross lingual language model xlm exploit symmetric nature bilingual corpora apply model level dual learn nmt predictor handle primal task dual task simultaneously weight share lead stronger context prediction ability single direction nmt model take advantage dual learn scheme design novel feature directly encode translate target information without rely source context extensive experiment conduct wmt20 qe task demonstrate method beat winner competition outperform baseline methods great margin use sentence level score provide verdi clean parallel corpus observe benefit model performance train efficiency
span extraction read comprehension model make tremendous advance enable availability large scale high quality train datasets despite rapid progress widespread application extractive read comprehension datasets languages english remain scarce create sufficient amount train data language costly even impossible alternative create large scale high quality monolingual span extraction train datasets develop multilingual model approach systems transfer target language without require train data language paper order solve scarce availability extractive read comprehension train data target language propose multilingual extractive read comprehension approach call xlrc simultaneously model exist extractive read comprehension train data multilingual environment use self adaptive attention multilingual attention specifically firstly construct multilingual parallel corpora translate exist extractive read comprehension datasets ie cmrc two thousand and eighteen target language ie chinese different language families ie english secondly enhance final target representation adopt self adaptive attention saa combine self attention inter attention extract semantic relations pair target source languages furthermore propose multilingual attention mla learn rich knowledge various language families experimental result show model outperform state art baseline ie robertalarge cmrc two thousand and eighteen task demonstrate effectiveness propose multi lingual model approach show potentials multilingual nlp task
document level event extraction aim recognize event information whole piece article exist methods effective due two challenge task target event arguments scatter across sentence b correlation among events document non trivial model paper propose heterogeneous graph base interaction model tracker git solve aforementioned two challenge first challenge git construct heterogeneous graph interaction network capture global interactions among different sentence entity mention second git introduce tracker module track extract events hence capture interdependency among events experiment large scale dataset zheng et al two thousand and nineteen show git outperform previous methods twenty-eight f1 analysis reveal git effective extract multiple correlate events event arguments scatter across document code available https githubcom runxinxu git
knowledge graph prove effective model structure information conceptual knowledge especially medical domain however lack high quality annotate corpora remain crucial problem advance research applications task order accelerate research domain specific knowledge graph medical domain introduce diakg high quality chinese dataset diabetes knowledge graph contain twenty-two thousand and fifty entities six thousand, eight hundred and ninety relations total implement recent typical methods name entity recognition relation extraction benchmark evaluate propose dataset thoroughly empirical result show diakg challenge exist methods analysis conduct discuss future research direction improvements hope release dataset assist construction diabetes knowledge graph facilitate ai base applications
paper explore character drive story continuation story emerge character first second person narration well dialogue require model select language consistent character persona relationships character follow advance story hypothesize multi task model train character dialogue plus character relationship information improve transformer base story continuation end extend critical role dungeons dragons dataset rameshkumar bailey two thousand and twenty consist dialogue transcripts people collaboratively tell story play role play game dungeons dragons automatically extract relationships pair interact character well personas series ablations lend evidence hypothesis show multi task model use character relationships improve story continuation accuracy strong baselines
site reliability engineer sres play key role issue identification resolution issue report sres come together virtual room collaboration platform triage issue leave behind wealth information use later triaging similar issue however usability conversations offer challenge due noisy ii unlabelled paper present novel approach issue artefact extraction noisy conversations minimal label data propose combination unsupervised supervise model minimum human intervention leverage domain knowledge predict artefacts small amount conversation data use fine tune already pretrained language model artefact prediction large amount conversation data experimental result dataset show propose ensemble unsupervised supervise model better use either one individually
mixture experts moe model achieve promise result outrageous large amount parameters constant computation cost thus become trend model scale still mystery moe layer bring quality gain leverage parameters sparse activation work investigate several key factor sparse expert model observe load imbalance may significant problem affect model quality contrary perspectives recent study number sparsely activate experts k expert capacity c top k rout significantly make difference context furthermore take step forward propose simple method call expert prototyping split experts different prototypes apply k top one rout strategy improve model quality maintain constant computational cost exploration extremely large scale model reflect effective train larger model push model scale one trillion parameters implement solely four hundred and eighty nvidia v100 32gb gpus comparison recent sotas two thousand and forty-eight tpu core propose giant model achieve substantial speedup convergence size baseline
sequence sequence model provide viable new approach generative summarization allow model longer limit simply select recombine sentence original text however model three drawbacks grasp detail original text often inaccurate text generate model often repetitions difficult handle word beyond word list paper propose new architecture combine reinforcement learn adversarial generative network enhance sequence sequence attention model first use hybrid pointer generator network copy word directly source text contribute accurate reproduction information without sacrifice ability generators generate new word second use intra temporal intra decoder attention penalize summarize content thus discourage repetition apply model propose covid nineteen paper title summarization task achieve close approximations current model roueg bring better readability
availability large scale datasets drive development neural model create summaries single document generic purpose use summarization system users often specific intents various language realizations depend information need range single keyword long narrative compose multiple question exist summarization systems however often either fail support act robustly query focus summarization task introduce laqsum first unify text summarization system learn latent query document abstractive summarization exist query form deep generative framework system jointly optimize latent query model conditional language model allow users plug play query type test time despite learn generic summarization data require optimization downstream summarization task system robustly outperform strong comparison systems across summarization benchmarks different query type document settings target domains
paraphrase detection important number applications include plagiarism detection authorship attribution question answer text summarization text mine general etc paper give performance overview various type corpus base model especially deep learn dl model task paraphrase detection report result eight model lsi tf idf word2vec doc2vec glove fasttext elmo use evaluate three different public available corpora microsoft research paraphrase corpus clough stevenson webis crowd paraphrase corpus two thousand and eleven great number experiment decide appropriate approach text pre process hyper parameters sub model selection exist eg skipgram vs cbow distance measure semantic similarity paraphrase detection threshold find researchers use deep learn model show dl model competitive traditional state art approach potential develop
propose piglet model learn physical commonsense knowledge interaction use knowledge grind language factorize piglet physical dynamics model separate language model dynamics model learn object also glass cup break throw plastic ones use interface language model give us unify model linguistic form ground mean piglet read sentence simulate neurally might happen next communicate result literal symbolic representation natural language experimental result show model effectively learn world dynamics along communicate able correctly forecast happen next give english sentence eighty time outperform 100x larger text text approach ten likewise natural language summaries physical interactions also judge humans accurate lm alternatives present comprehensive analysis show room future work
answer complex question long document require aggregate multiple piece evidence predict answer paper propose multi hop retrieval method dochopper answer compositional question long document step dochopper retrieve paragraph sentence embed document mix retrieve result query update query next step contrast many retrieval base methods eg rag realm query augment token sequence instead augment numerically combine another neural representation mean model end end differentiable demonstrate utilize document structure largely improve question answer retrieval performance long document experiment dochopper three different qa task require read long document answer compositional question discourse entailment reason factual qa table text information seek qa academic paper dochopper outperform baseline model achieve state art result datasets additionally dochopper efficient inference time three hundred and ten time faster baselines
memes one popular type content use spread information online influence large number people rhetorical psychological techniques task detection persuasion techniques texts image detect persuasive techniques memes consist three subtasks multi label classification use textual content b multi label classification span identification use textual content c multi label classification use visual textual content paper propose transfer learn approach fine tune bert base model different modalities also explore effectiveness ensembles model train different modalities achieve f1 score five hundred and seventy four hundred and eighty-two five hundred and twenty-one correspond subtasks
name entity recognition ner fundamental component many applications web search voice assistants although deep neural network greatly improve performance ner due requirement large amount train data deep neural network hardly scale many languages industry set tackle challenge cross lingual ner transfer knowledge rich resource language languages low resources pre train multilingual language model instead use train data target languages cross lingual ner rely train data source languages optionally add translate train data derive source languages however exist cross lingual ner methods make good use rich unlabeled data target languages relatively easy collect industry applications address opportunities challenge paper describe novel practice microsoft leverage large amount unlabeled data target languages real production settings effectively extract weak supervision signal unlabeled data develop novel approach base ideas semi supervise learn reinforcement learn empirical study three benchmark data set verify approach establish new state art performance clear edge ner techniques report paper way become fundamental component web rank entity pane answer trigger question answer microsoft bing search engine moreover techniques also serve part speak language understand module commercial voice assistant plan open source code prototype framework deployment
large scale pre train past two years witness significant performance boost visual question answer vqa task though rapid progress make remain unclear whether state art sota vqa model robust encounter test examples wild study introduce adversarial vqa new large scale vqa benchmark collect iteratively via adversarial human model loop procedure new benchmark present several interest find surprisingly dataset collection find non expert annotators successfully attack sota vqa model relative ease ii test variety sota vqa model new dataset highlight fragility find large scale pre train model adversarial train methods achieve far lower performance achieve standard vqa v2 dataset iii consider data augmentation dataset use improve performance robust vqa benchmarks iv present detail analysis dataset provide valuable insights challenge bring community hope adversarial vqa serve valuable benchmark use future work test robustness develop vqa model dataset publicly available https adversarialvqa githubio
multimodal machine translation mmt enrich source text visual information translation gain popularity recent years several pipelines propose direction yet task lack quality datasets illustrate contribution visual modality translation systems paper propose system multimodal translation task wat two thousand and twenty-one english hindi propose use mbart pretrained multilingual sequence sequence model textual translations bring visual information textual domain extract object tag image enhance input multimodal task also explore robustness system systematically degrade source text finally achieve bleu score four hundred and forty-six five hundred and sixteen test set challenge set task
paper present coarse fine question answer cfqa system base reinforcement learn efficiently process document different lengths choose appropriate action system design use actor critic base deep reinforcement learn model achieve multi step question answer compare previous qa model target datasets mainly contain either short long document multi step coarse fine model take merit multiple system modules handle short long document system hence obtain much better accuracy faster train speed compare current state art model test model four qa datasets wikereading wikireading long cnn squad demonstrate thirteen seventeen accuracy improvements 15x 34x train speed up comparison baselines use state art model
sofgaard two thousand and twenty obtain result suggest fraction tree occur test data isomorphic tree train set account non trivial variation parser performance similar statistical analyse nlp result base evaluate linear regressions however study methodological issue undertake use small sample size lead unreliable result present replication study also bin sentence length find small subset sentence vary performance respect graph isomorphism correlation observe parser performance graph isomorphism wild disappear control covariants however control experiment covariants keep fix observe strong correlation suggest conclusions draw statistical analyse like need temper control experiment complement readily tease factor apart
present novel method relation extraction single sentence map sentence two give entities canonical fact knowledge graph kg especially presume sentential set context single sentence often sparse paper introduce kgpool method address sparsity dynamically expand context additional facts kg learn representation facts entity alias entity descriptions etc use neural methods supplement sentential context unlike exist methods statically use expand facts kgpool condition expansion sentence study efficacy kgpool evaluate different neural model kgs wikidata nyt freebase experimental evaluation standard datasets show feed kgpool representation graph neural network overall method significantly accurate state art methods
classify core textual components scientific paper title author body text etc critical first step automate scientific document understand previous work show use elementary layout information ie token 2d position page lead accurate classification introduce new methods incorporate visual layout structure vila eg group page texts text line text block language model improve performance show vila approach simply add special tokens denote boundaries layout structure model input lead one hundred and forty-five f1 score improvements token classification task moreover design hierarchical model h vila encode layout structure record seventy efficiency boost without hurt prediction accuracy experiment conduct newly curated evaluation suite s2 vlue novel metric measure vila awareness new dataset cover nineteen scientific discipline gold annotations pre train weight benchmark datasets source code available https githubcom allenai vilahttps githubcom allenai vila
state art open domain question answer systems use neural retrieval model encode passages continuous vectors extract knowledge source however retrieval model often require large memory run massive size passage index paper introduce binary passage retriever bpr memory efficient neural retrieval model integrate learn hash technique state art dense passage retriever dpr represent passage index use compact binary cod rather continuous vectors bpr train multi task objective two task efficient candidate generation base binary cod accurate reranking base continuous vectors compare dpr bpr substantially reduce memory cost 65gb 2gb without loss accuracy two standard open domain question answer benchmarks natural question triviaqa code train model available https githubcom studio ousia bpr
exist name entity recognition ner approach base sequence label model focus capture local context dependencies however way take one sentence input prevent model non sequential global context useful especially local context information limit ambiguous end propose model call global context enhance document level ner gcdoc leverage global contextual information two level ie word sentence word level document graph construct model wider range dependencies word obtain enrich contextual representation word via graph neural network gnn avoid interference noise information propose two strategies first apply epistemic uncertainty theory find tokens whose representations less reliable thereby help prune document graph selective auxiliary classifier propose effectively learn weight edge document graph reduce importance noisy neighbour nod sentence level appropriately model wider context beyond single sentence employ cross sentence module encode adjacent sentence fuse current sentence representation via attention gate mechanisms extensive experiment two benchmark ner datasets conll two thousand and three ontonotes fifty english dataset demonstrate effectiveness propose model model reach f1 score nine thousand, two hundred and twenty-two nine thousand, three hundred and forty bert conll two thousand and three dataset eight thousand, eight hundred and thirty-two nine thousand and forty-nine bert ontonotes fifty dataset achieve new state art performance
many task orient dialogue systems use deep reinforcement learn drl learn policies respond user appropriately complete task successfully train drl agents diverse dialogue trajectories prepare well rare user request unseen situations one effective diversification method let agent interact diverse set learn user model however trajectories create artificial user model may contain generation errors quickly propagate agent policy thus important control quality diversification resist noise paper propose novel dialogue diversification method task orient dialogue systems train simulators method intermittent short extension ensemble see constrain intensity interact ensemble diverse user model effectively control quality diversification evaluations multiwoz dataset show see successfully boost performance several state art drl dialogue agents
knowledge distillation kd commonly use construct synthetic data train non autoregressive translation nat model however exist discrepancy low frequency word distil original data lead errors predict low frequency word alleviate problem directly expose raw data nat leverage pretraining analyze direct alignments find kd make low frequency source word align target deterministically fail align sufficient low frequency word target source accordingly propose reverse kd rejuvenate alignments low frequency target word make authentic synthetic data combine complementary approach new train strategy boost nat performance conduct experiment five translation benchmarks two advance architectures result demonstrate propose approach significantly universally improve translation quality reduce translation errors low frequency word encouragingly approach achieve two hundred and eighty-two three hundred and thirty-nine bleu point wmt14 english german wmt16 romanian english datasets respectively code data train model available urlhttps githubcom longyuewangdcu rlfw nat
deploy real world machine learn applications often subject uncontrolled even potentially malicious input domain input lead unpredictable output sometimes catastrophic safety issue prior study domain detection require domain task label limit supervise classification scenarios work tackle problem detect domain sample unsupervised domain data utilize latent representations pre train transformers propose simple yet effective method transform feature across layer construct domain detectors efficiently two domain specific fine tune approach propose boost detection accuracy empirical evaluations relate methods two datasets validate method greatly improve domain detection ability general scenario
although deep neural network widely employ prove effective sentiment analysis task remain challenge model developers assess model erroneous predictions might exist prior deployment deploy emergent errors hard identify prediction run time impossible trace back source address gap paper propose error detection framework sentiment analysis base explainable feature perform global level feature validation human loop assessment follow integration global local level feature contribution analysis experimental result show give limit human loop intervention method able identify erroneous model predictions unseen data high precision
commonsense reason intuitive humans long term challenge artificial intelligence ai recent advancements pretrained language model show promise result several commonsense benchmark datasets however reliability comprehensiveness benchmarks towards assess model commonsense reason ability remain unclear end introduce new commonsense reason benchmark dataset comprise natural language true false statements sample pair complementary counterpart result 4k sentence pair propose pairwise accuracy metric reliably measure agent ability perform commonsense reason give situation dataset crowdsourced enhance adversarial model loop setup incentivize challenge sample facilitate systematic analysis commonsense capabilities design dataset along dimension knowledge domains reason scenarios numeracy experimental result demonstrate strongest baseline unifiedqa 3b fine tune achieve seventy-one standard accuracy fifty-one pairwise accuracy well human performance ninety-five metrics dataset available https githubcom pluslabnlp com2sense
infer social relations dialogues vital build emotionally intelligent robots interpret human language better act accordingly model social network graph name socaog consistency relations among group leverage attribute inference cue moreover formulate sequential structure prediction task propose alpha beta gamma strategy incrementally parse socaog dynamic inference upon incoming utterance alpha process predict attribute relations condition semantics dialogues ii beta process update social relations base relate attribute iii gamma process update individual attribute base interpersonal social relations empirical result dialogre moviegraph show model infer social relations accurately state art methods moreover ablation study show three process complement case study demonstrate dynamic relational inference
sentiment analysis sa become extensive research area recent years impact diverse field include ecommerce consumer business politics drive increase adoption usage social media platforms challenge extract topics sentiments unsupervised short texts emerge contexts may contain figurative word strident data co existence many possible mean single word phrase contribute obtain incorrect topics prior research base specific theme rhetoric focus content clean dataset work report effectiveness bertbidirectional encoder representations transformers sentiment classification task raw live dataset take popular microblogging platform demonstrate novel bert framework propose show enhance performance obtainable combine latent topics contextual bert embeddings numerical experiment conduct ensemble forty-two thousand datasets use nimbleboxai platform hardware configuration consist nvidia tesla k80cuda four core cpu 15gb ram run isolate google cloud platform instance empirical result show model improve performance add topics bert accuracy rate nine thousand and eighty-one sentiment classification use bert propose approach
fact verification challenge task require simultaneously reason aggregate multiple retrieve piece evidence evaluate truthfulness claim exist approach typically explore semantic interaction claim evidence different granularity level fail capture topical consistency reason process believe crucial verification ii aggregate multiple piece evidence equally without consider implicit stances claim thereby introduce spurious information alleviate issue propose novel topic aware evidence reason stance aware aggregation model accurate fact verification follow four key properties one check topical consistency claim evidence two maintain topical coherence among multiple piece evidence three ensure semantic similarity global topic information semantic representation evidence four aggregate evidence base implicit stances claim extensive experiment conduct two benchmark datasets demonstrate superiority propose model several state art approach fact verification source code obtain https githubcom jasenchn tarsa
introduce collection recognize textual entailment rte datasets focus figurative language leverage five exist datasets annotate variety figurative language simile metaphor irony frame twelve thousand, five hundred rte exampleswe evaluate well state art model train popular rte datasets capture different aspects figurative language result analyse indicate model might sufficiently capture figurative language struggle perform pragmatic inference reason world knowledge ultimately datasets provide challenge testbed evaluate rte model
texts convey sophisticate knowledge however texts also convey sensitive information despite success general purpose language model domain specific mechanisms differential privacy dp exist text sanitization mechanisms still provide low utility curse high dimensional text representation companion issue utilize sanitize texts downstream analytics also explore paper take direct approach text sanitization insight consider sensitivity similarity via new local dp notion sanitize texts also contribute sanitization aware pretraining fine tune enable privacy preserve natural language process bert language model promise utility surprisingly high utility boost success rate inference attack
rural regions several develop countries access quality healthcare medical infrastructure professional diagnosis largely unavailable many regions gradually gain access internet infrastructure although strong enough connection allow sustain communication medical practitioner several deaths result lack medical access absence patient previous health record unavailability information indigenous languages easily prevent paper describe approach leverage phenomenal progress machine learn nlp natural language process techniques design model low resource multilingual preliminary first point contact medical assistant contribution include define nlp pipeline require name entity recognition language agnostic sentence embed natural language translation information retrieval question answer generative pre train final query process obtain promise result pipeline preliminary result ehr electronic health record analysis text summarization medical practitioners peruse diagnosis nlp pipeline aim provide preliminary medical information user claim supplant diagnosis qualify medical practitioners use input subject matter experts compile large corpus pre train fine tune biobert base nlp model specific task expect recent advance nlp architectures several efficient privacy preserve model impact solution improve individual task performance
essential component dialogue systems multi turn response selection aim pick optimal response among set candidates improve dialogue fluency paper investigate three problems current response selection approach especially generation base conversational agents exist approach often formulate sentence score problem consider relationships responses ii exist model tend select undesirable candidates large overlap dialogue history iii negative instance train mainly construct random sample corpus whereas generate candidates practice typically closer distribution address problems create new dataset call convai2 propose new response selector call global selector experimental result show global selector train convai2 noticeable improvements accuracy inference speed
image caption model lately show impressive result apply standard datasets switch real life scenarios however constitute challenge due larger variety visual concepts cover exist train set reason novel object caption noc recently emerge paradigm test caption model object unseen train phase paper present novel approach noc learn select relevant object image regardless adherence train set constrain generative process language model accordingly architecture fully attentive end end trainable also incorporate constraints perform experiment hold coco dataset demonstrate improvements state art term adaptability novel object caption quality
open end nature visual caption make challenge area evaluation majority propose model rely specialize train improve human correlation result limit adoption generalizability explainabilty introduce typicality new formulation evaluation root information theory uniquely suit problems lack definite grind truth typicality serve framework develop novel semantic comparison sparcs well referenceless fluency evaluation metrics course analysis two separate dimension fluency naturally emerge style capture metric spurt grammar capture form grammatical outlier penalties extensive experiment ablation study benchmark datasets show decompose dimension semantics fluency provide greater system level insight captioner differences propose metrics along combination smurf achieve state art correlation human judgment compare rule base evaluation metrics
language model lm automatic speech recognition asr usually incorporate utterance level contextual information domains like voice assistants however additional context time utterance speak provide rich input signal introduce attention mechanism train neural speech recognition language model text non linguistic contextual data apply large de identify dataset utterances collect popular voice assistant platform method reduce perplexity seventy relative standard lm incorporate contextual information evaluate utterances extract long tail dataset method improve perplexity ninety relative standard lm twenty-eight relative compare state art model contextual lm
adversarial attack expose important blind spot deep learn systems word sentence level attack scenarios mostly deal find semantic paraphrase input fool nlp model character level attack typically insert typos input stream commonly think easier defend via spell correction modules work show standard spellchecker approach pruthi et al two thousand and nineteen train defend insertions deletions swap perform poorly character level benchmark recently propose eger benz two thousand and twenty include challenge attack visual phonetic perturbations miss word segmentations contrast show untrained iterative approach combine context independent character level information context dependent information bert mask language model perform par human crowd workers amazon mechanical turk amt supervise via three shoot learn
crowdworker construct natural language inference nli datasets find contain statistical artifacts associate annotation process allow hypothesis classifiers achieve better random performance poliak et al two thousand and eighteen gururanganet et al two thousand and eighteen tsuchiya two thousand and eighteen investigate whether mednli physician annotate dataset premise extract clinical note contain artifacts romanov shivade two thousand and eighteen find entail hypotheses contain generic versions specific concepts premise well modifiers relate responsiveness duration probability neutral hypotheses feature condition behaviors co occur condition premise contradiction hypotheses feature explicit negation premise implicit negation via assertion good health adversarial filter demonstrate performance degrade evaluate difficult subset provide partition information recommendations alternative dataset construction strategies knowledge intensive domains
study calibration question answer estimate whether model correctly predict answer question unlike prior work mainly rely model confidence score calibrator incorporate information input example eg question evidence context together data augmentation via back translation simple approach achieve five ten gain calibration accuracy read comprehension benchmarks furthermore present first calibration study open retrieval set compare calibration accuracy retrieval base span prediction model answer generation model approach show consistent gain calibrators rely model confidence simple efficient calibrator easily adapt many task model architectures show robust gain settings
quadratic computational memory complexities transformer attention mechanism limit scalability model long sequence paper propose luna linear unify nest attention mechanism approximate softmax attention two nest linear attention function yield linear oppose quadratic time space complexity specifically first attention function luna pack input sequence sequence fix length pack sequence unpack use second attention function compare traditional attention mechanism luna introduce additional sequence fix length input additional correspond output allow luna perform attention operation linearly also store adequate contextual information perform extensive evaluations three benchmarks sequence model task long context sequence model neural machine translation mask language model large scale pretraining competitive even better experimental result demonstrate effectiveness efficiency luna compare variety
automatically extract key information scientific document potential help scientists work efficiently accelerate pace scientific progress prior work consider extract document level entity cluster relations end end raw scientific text improve literature search help identify methods materials give problem despite importance task exist work scientific information extraction sciie consider extraction solely base content individual paper without consider paper place broader literature contrast prior work augment text representations leverage complementary source document context citation graph referential link cite cite paper test set english language scientific document show simple ways utilize structure content citation graph lead significant gain different scientific information extraction task task combine observe sizable improvement end end information extraction state art suggest potential future work along direction release software tool facilitate citation aware sciie development
incorporate syntax neural approach nlp multitude practical scientific benefit instance language model syntax aware likely able produce better sample even discriminative model like bert syntax module could use core nlp task like unsupervised syntactic parse rapid progress recent years arguably spur empirical success parse read predict architecture shen et al 2018a later simplify order neuron lstm shen et al two thousand and nineteen notably first time neural approach able successfully perform unsupervised syntactic parse evaluate various metrics like f one score however even heuristic much less fully mathematical understand architectures work lag severely behind work answer representational question raise architectures shen et al 2018a two thousand and nineteen well transition base syntax aware language model dyer et al two thousand and sixteen kind syntactic structure current neural approach syntax represent concretely grind question sandbox probabilistic context free grammars pcfgs identify key aspect representational power approach amount directionality context predictor access force make parse decision show limit context either bound unidirectional pcfgs approach represent max likelihood parse conversely context unlimited represent max likelihood parse pcfg
knowledge base kbs text often contain complementary knowledge kbs store structure knowledge support long range reason text store comprehensive timely knowledge unstructured way separately embed individual knowledge source vector space demonstrate tremendous successes encode respective knowledge jointly embed reason knowledge source fully leverage complementary information still largely open problem conduct large scale systematic investigation align kb text embeddings joint reason set novel evaluation framework two evaluation task shoot link prediction analogical reason evaluate array kb text embed alignment methods also demonstrate alignment infuse textual information kb embeddings accurate link prediction emerge entities events use covid nineteen case study
investigate problem chinese grammatical error correction cgec present new framework name tail tail textbfttt non autoregressive sequence prediction address deep issue hide cgec consider tokens correct convey directly source target error position estimate correct base bidirectional context information thus employ bert initialize transformer encoder backbone model conduct information model convey consider rely position substitution handle variable length correction case various operations substitution deletion insertion local paraphrase require jointly therefore conditional random field crf layer stack tail conduct non autoregressive sequence prediction model token dependencies since tokens correct easily predict convey target model may suffer severe class imbalance issue alleviate problem focal loss penalty strategies integrate loss function moreover besides typical fix length error correction datasets also construct variable length corpus conduct experiment experimental result standard datasets especially variable length datasets demonstrate effectiveness ttt term sentence level accuracy precision recall f1 measure task error detection correction
paper implement compare seven different data augmentation strategies task automatic score children ability understand others thoughts feel desire mindreading recruit domain experts annotate augment sample determine extent strategy preserve original rat also carry multiple experiment measure much augmentation strategy improve performance automatic score systems determine capabilities automatic systems generalize unseen data create uk mind twenty new corpus children performance test mindreading consist ten thousand, three hundred and twenty question answer pair obtain new state art performance mind ca corpus improve macro f1 score six point result indicate number train examples quality augmentation strategies affect performance systems task specific augmentations generally outperform task agnostic augmentations automatic augmentations base vectors glove fasttext perform worst find systems train mind ca generalize well uk mind twenty demonstrate data augmentation strategies also improve performance unseen data
recent years chatbots empower engage social conversations humans potential elicit people disclose personal experience opinions emotions however extent people respond chabots self disclosure remain less know work design social chatbot three self disclosure level conduct small talk provide relevant recommendations people three hundred and seventy-two mturk participants randomize one four group different self disclosure level converse chatbot two topics movies covid nineteen find people self disclosure level strongly reciprocal chatbot self disclosure level chatbots self disclosure also positively impact engagement users perception bot lead effective recommendation participants enjoy agree recommendations
problem categorize short speech sentence accord semantic feature high accuracy subject study natural language process study data set create sample classify forty-six different categories use examples consist sentence take chat conversations company customer representatives company website visitors primary purpose automatically tag question request visitors accurate way forty-six predetermine categories use chat application generate meaningful answer question ask website visitors different bert model one gpt two model pre train turkish prefer classification performances relevant model analyze detail report accordingly
emotion recognition conversations erc gain increase attention develop empathetic machine recently many approach devote perceive conversational context deep learn model however approach insufficient understand context due lack ability extract integrate emotional clue work propose novel contextual reason network dialoguecrn fully understand conversational context cognitive perspective inspire cognitive theory emotion design multi turn reason modules extract integrate emotional clue reason module iteratively perform intuitive retrieve process conscious reason process imitate human unique cognitive think extensive experiment three public benchmark datasets demonstrate effectiveness superiority propose model
generative linguistic steganography mainly utilize language model apply steganographic sample stegosampling generate high security steganographic text stegotext however previous methods generally lead statistical differences conditional probability distributions stegotext natural text bring security risk paper ensure security present novel provably secure generative linguistic steganographic method adg recursively embed secret information adaptive dynamic group tokens accord probability give shelf language model prove security adg mathematically also conduct extensive experiment three public corpora verify imperceptibility experimental result reveal propose method able generate stegotext nearly perfect security
reply suggestion model help users process email chat faster previous work study english reply suggestion instead present mrs multilingual reply suggestion dataset ten languages mrs use compare two families model one retrieval model select reply fix set two generation model produce reply scratch therefore mrs complement exist cross lingual generalization benchmarks focus classification sequence label task build generation model retrieval model baselines mrs two model different strengths monolingual set require different strategies generalize across languages mrs publicly available https githubcom zhangmozhi mrs
ability capture complex linguistic structure long term dependencies among word passage essential relation extraction task graph neural network gnns one mean encode dependency graph show effective prior work however relatively little attention pay receptive field gnns crucial task extremely long text require discourse understand work leverage idea graph pool propose mirror graph convolution network gnn model pool unpooling structure tailor task pool branch reduce graph size enable gnn obtain larger receptive field within fewer layer unpooling branch restore pool graph original resolution token level task experiment two discourse level relation extraction datasets demonstrate effectiveness method show significant improvements prior methods especially model long term dependencies necessary moreover propose clause match cm novel graph pool method merge nod base dependency relations graph cm largely reduce graph size retain main semantics input text
video ground dialogue system require understand dialogue contain semantic dependencies turn turn video contain visual cue spatial temporal scene variations build dialogue systems challenge problem involve complex multimodal temporal input study independently hard exist datasets exist benchmarks enough annotations help analyze dialogue systems understand linguistic visual reason capability limitations isolation benchmarks also explicitly design minimize bias model exploit without actual reason address limitations paper present diagnostic dataset test range reason abilities videos dialogues dataset design contain minimal bias detail annotations different type reason question require include cross turn video interval track dialogue object track use dataset analyze several dialogue system approach provide interest insights abilities limitations total dataset contain ten instance ten round dialogues sim11k synthetic videos result 100k dialogues 1m question answer pair code dataset make public
zero shoot learn zsl aim understand unseen categories train examples class level descriptions improve discriminative power zsl model visual learn process unseen categories inspiration psychology human creativity produce novel art first propose cizsl v1 creativity inspire model generative zsl relate zsl human creativity observe zsl recognize unseen creativity create likable unseen introduce learn signal inspire creativity literature explore unseen space hallucinate class descriptions encourage careful deviation visual feature generations see class allow knowledge transfer see unseen class second cizsl v2 propose improve version cizsl v1 generative zero shoot learn cizsl v2 consist investigation additional inductive losses unseen class along semantic guide discriminator empirically show consistently cizsl losses improve generative zsl model challenge task generalize zsl noisy text cub nabirds datasets also show advantage approach attribute base zsl awa2 apy sun datasets also show cizsl v2 improve performance compare cizsl v1
text image retrieval essential task cross modal information retrieval ie retrieve relevant image large unlabelled dataset give textual query paper propose visualsparta novel visual text sparse transformer match model show significant improvement term accuracy efficiency visualsparta capable outperform previous state art scalable methods mscoco flickr30k also show achieve substantial retrieve speed advantage ie one million image index visualsparta use cpu get 391x speedup compare cpu vector search 54x speedup compare vector search gpu acceleration experiment show speed advantage even get bigger larger datasets visualsparta efficiently implement invert index best knowledge visualsparta first transformer base text image retrieval model achieve real time search large scale datasets significant accuracy improvement compare previous state art methods
current open domain question answer systems often follow retriever reader architecture retriever first retrieve relevant passages reader read retrieve passages form answer paper propose simple effective passage reranking method name reader guide reranker rider involve train reranks retrieve passages solely base top predictions reader reranking show rider despite simplicity achieve ten twenty absolute gain top one retrieval accuracy one four exact match gain without refine retriever reader addition rider without train outperform state art transformer base supervise rerankers remarkably rider achieve four hundred and eighty-three natural question dataset six hundred and sixty-four triviaqa dataset one thousand and twenty-four tokens seventy-eight passages average use reader input passage reranking
neural entity type model typically represent fine grain entity type vectors high dimensional space space well suit model type complex interdependencies study ability box embeddings embed concepts dimensional hyperrectangles capture hierarchies type even relationships define explicitly ontology model represent type entity mention box mention context feed bert base model embed mention box space essentially model leverage typological clue present surface text hypothesize type representation mention box containment use derive posterior probability mention exhibit give type conditional probability relations type compare approach vector base type model observe state art performance several entity type benchmarks addition competitive type performance box base model show better performance prediction consistency predict supertype subtype together confidence ie calibration demonstrate box base model capture latent type hierarchies better vector base model
recent time bert base transformer model become inseparable part tech stack text process model similar progress observe speech domain multitude model observe state art result use audio transformer model encode speech beg question audio transformer model learn moreover although standard methodology choose last layer embed downstream task optimal choice try answer question two recent audio transformer model mockingjay wave2vec20 compare comprehensive set language delivery structure feature include audio fluency pronunciation feature additionally probe audio model understand textual surface syntax semantic feature compare bert exhaustive settings native non native synthetic read spontaneous speech datasets
recently attract much attention build reliable name entity recognition ner systems use limit annotate data nearly exist work heavily rely domain specific resources external lexicons knowledge base however domain specific resources often available meanwhile difficult expensive construct resources become key obstacle wider adoption tackle problem work propose novel robust domain adaptive approach rdaner low resource ner use cheap easily obtainable resources extensive experiment three benchmark datasets demonstrate approach achieve best performance use cheap easily obtainable resources deliver competitive result state art methods use difficultly obtainable domainspecific resources code corpora find https githubcom houking rdaner
emojis become ubiquitous digital communication due visual appeal well ability vividly convey human emotion among factor grow prominence emojis social media instant message also lead increase need systems tool operate text contain emojis study assess support consider test set tweet emojis base perform series experiment investigate ability prominent nlp text process tool adequately process particular consider tokenization part speech tag well sentiment analysis find show many tool still notable shortcomings operate text contain emojis
broader disclosive transparency truth clarity communication regard function ai systems widely consider desirable unfortunately nebulous concept difficult define quantify previous work suggest trade exist greater disclosive transparency user confusion much information cloud reader understand system description mean address issue connect disclosive transparency replication room think experiment person describe system attempt convey requisite information third party reconstruct set degree necessary information convey represent description transparency level expertise need third party correspond potential user confusion introduce two neural language model base probabilistic metrics model factor demonstrate correlate user expert opinions system transparency make valid objective proxy finally apply metrics study relationships transparency confusion user perceptions corpus nlp demo abstract
paper present detail study improve visual representations vision language vl task develop improve object detection model provide object centric representations image compare widely use emphbottom top model citeanderson2018bottom new model bigger better design vl task pre train much larger train corpora combine multiple public annotate object detection datasets therefore generate representations richer collection visual object concepts previous vl research focus mainly improve vision language fusion model leave object detection model improvement untouched show visual feature matter significantly vl model experiment fee visual feature generate new object detection model transformer base vl fusion model oscar citeli2020oscar utilize improve approach short pre train vl model fine tune wide range downstream vl task result show new visual feature significantly improve performance across vl task create new state art result seven public benchmarks release new object detection model public
availability large amount data compel computation power make deep learn model much popular text classification sentiment analysis deep neural network achieve competitive performance task train naive text representations word count term frequency binary matrix embeddings however many representations result input space dimension order vocabulary size enormous lead blow number parameters learn computational cost become infeasible scale domains require retain colossal vocabulary work propose use singular value decomposition transform high dimensional input space lower dimensional latent space show neural network train lower dimensional space able retain performance savor significant reduction computational complexity many situations also outperform classical neural network train native input space
large scale pretrained language model show thrill generation capabilities especially generate consistent long text thousands word ease however users model control prefix sentence certain global aspects generate text challenge simultaneously achieve fine grain controllability preserve state art unconditional text generation capability paper first propose new task name outline story o2s test bed fine grain controllable generation long text generate multi paragraph story cascade events ie sequence outline events guide subsequent paragraph generation create dedicate datasets future benchmarks build state art keyword extraction techniques finally propose extremely simple yet strong baseline method o2s task fine tune pre train language model augment sequence outline story pair simple language model objective method introduce new parameters perform architecture modification except several special tokens delimiters build augment sequence extensive experiment various datasets demonstrate state art conditional story generation performance model achieve better fine grain controllability user flexibility paper among first ones knowledge propose model create datasets task outline story work also instantiate research interest fine grain controllable generation open domain long text control input represent short text
investigate large scale latent variable model lvms neural story generation explore application open domain long text objectives two thread generation effectiveness controllability lvms especially variational autoencoder vae achieve effective controllable generation exploit flexible distributional latent representations recently transformers variants achieve remarkable effectiveness without explicit latent representation learn thus lack satisfy controllability generation paper advocate revive latent variable model essentially power representation learn era transformers enhance controllability without hurt state art generation effectiveness specifically integrate latent representation vectors transformer base pre train architecture build conditional variational autoencoder cvae model components encoder decoder variational posterior build top pre train language model gpt2 specifically paper experiment demonstrate state art conditional generation ability model well excellent representation learn capability controllability
study novel task video question answer generation vqag challenge video question answer video qa task multimedia due expensive data annotation cost many widely use large scale video qa datasets video qa msvd qa msrvtt qa automatically annotate use caption question generation capqg input caption instead video caption neither fully represent video always practically available crucial generate question answer pair base video via video question answer generation vqag exist video text v2t approach despite take video input generate question alone work propose novel model generator pretester network focus two components one joint question answer generator jqag generate question correspond answer allow video question answer train two pretester pt verify generate question try answer check pretested answer model propose answer grind truth answer evaluate system two available large scale human annotate video qa datasets achieve state art question generation performances furthermore use generate qa pair video qa task surpass supervise baselines apply generate question video qa applications surpass supervise baselines use generate question pre train strategy outperform capqg transfer learn approach employ semi supervise twenty fully supervise learn annotate data experimental result suggest novel perspectives video qa train
neural network often parameterized hence benefit aggressive regularization conventional regularization methods dropout weight decay leverage structure network input hide state result conventional methods less effective methods leverage structure spatialdropout dropblock randomly drop value certain contiguous areas hide state set zero although locations dropout areas random pattern spatialdropout dropblock manually design fix propose learn dropout pattern method controller learn generate dropout pattern every channel layer target network convnet transformer target network train dropout pattern result validation performance use signal controller learn show method work well image recognition cifar ten imagenet well language model penn treebank wikitext two learn dropout pattern also transfer different task datasets language model penn treebank engligh french translation wmt two thousand and fourteen code available
end end e2e speaker attribute automatic speech recognition sa asr model propose recently jointly perform speaker count speech recognition speaker identification model achieve low speaker attribute word error rate sa wer monaural overlap speech comprise unknown number speakers however e2e model approach susceptible mismatch train test condition yet investigate whether e2e sa asr model work well record much longer sample see train work first apply know decode technique develop perform single speaker asr long form audio e2e sa asr task propose novel method use sequence sequence model call hypothesis stitcher model take multiple hypotheses obtain short audio segment extract original long form input output fuse single hypothesis propose several architectural variations hypothesis stitcher model compare conventional decode methods experiment use librispeech libricss corpora show propose method significantly improve sa wer especially long form multi talker record
automatically construct taxonomy find many applications e commerce web search one critical challenge data business scope grow real applications new concepts emerge need add exist taxonomy previous approach focus taxonomy expansion ie find appropriate hypernym concept taxonomy new query concept paper formulate new task taxonomy completion discover hypernym hyponym concepts query propose triplet match network tmn find appropriate pair give query concept tmn consist one primal scorer multiple auxiliary scorers auxiliary scorers capture various fine grain signal eg query hypernym query hyponym semantics primal scorer make holistic prediction triplet base internal feature representations auxiliary scorers also innovative channel wise gate mechanism retain task specific information concept representations introduce boost model performance experiment four real world large scale datasets show tmn achieve best performance taxonomy completion task previous taxonomy expansion task outperform exist methods
continual relation extraction important task focus extract new facts incrementally unstructured text give sequential arrival order relations task prone two serious challenge namely catastrophic forget order sensitivity propose novel curriculum meta learn method tackle two challenge continual relation extraction combine meta learn curriculum learn quickly adapt model parameters new task reduce interference previously see task current task design novel relation representation learn method distribution domain range type relations representations utilize quantify difficulty task construction curricula moreover also present novel difficulty base metrics quantitatively measure extent order sensitivity give model suggest new ways evaluate model robustness comprehensive experiment three benchmark datasets show propose method outperform state art techniques code available anonymous github repository https githubcom wutong8023 aaaicml
topic model widely use analysis techniques cluster document surface thematic elements text corpora model remain challenge optimize often require human loop approach domain experts use knowledge steer adjust however fragility incompleteness opacity model mean even minor change could induce large potentially undesirable change result model paper conduct simulation base analysis human center interactions topic model objective measure sensitivity topic model common class user action find user interactions impact differ magnitude often negatively affect quality result model way difficult user evaluate suggest incorporation sensitivity multiverse analyse topic model interfaces surface overcome deficiencies
paper describe system aaai two thousand and twenty-one share task covid nineteen fake news detection english achieve 3rd position weight f1 score nine thousand, eight hundred and fifty-nine test set specifically propose ensemble method different pre train language model bert roberta ernie etc various train strategies include warm uplearning rate schedule k fold cross validation also conduct extensive analysis sample correctly classify code available athttps githubcom archersama 3rd solution covid19 fake news detection english
several domains crucial store manipulate data whose origin need completely traceable guarantee consistency trustworthiness reliability data typically ethical legal reason also important guarantee properties also carry data compose process new data article present main requirements theorethical problems arise design system support data capabilities present architecture implement system well prototype develop pharo
main aim study assessment discussion model hand write arabic segmentation framework propose base three step pre process segmentation evaluation pre process step morphological operators apply connect gap cgs write word gap happen pen lift write scan document convert image binary type segmentation step first remove small diacritics bound connect component segment offline word huge data utilize propose model apply variety handwrite style compatible real life applications consequently automatic evaluation stage select randomly one thousand, one hundred and thirty-one image iesk ardb database segment sub word small gap connect model performance evaluation reach eighty-eight standard grind truth database propose model achieve highest accuracy compare relate work
voice assistants become ubiquitous increasingly expect support perform well wide variety use case across different domains present domain aware rescoring framework suitable achieve domain adaptation second pass rescoring production settings framework fine tune domain general neural language model several domains use lstm base domain classification model select appropriate domain adapt model use second pass rescoring domain aware rescoring improve word error rate twenty-four slot word error rate forty-one three individual domains shop navigation music compare domain general rescoring improvements obtain maintain accuracy general use case
large fraction textual data available today contain various type noise ocr noise digitize document noise due informal write style users microblogging sit enable task search retrieval classification available data need robust algorithms text normalization ie clean different kinds noise text several efforts towards clean normalize noisy text however many exist text normalization methods supervise require language dependent resources large amount train data difficult obtain propose unsupervised algorithm text normalization need train data human intervention propose algorithm applicable text different languages handle machine generate human generate noise experiment several standard datasets show text normalization propose algorithm enable better retrieval stance detection compare use several baseline text normalization methods implementation algorithm find https githubcom ranarag unsupclean
identify adverse hostile content web particularly social media become problem paramount interest recent years ever increase popularity fine tune pretrained transformer base encoder model classifier head gradually become new baseline natural language classification task work explore gain attribute task adaptive pretraining tapt prior fine tune transformer base architectures specifically study two problems namely coarse binary classification hindi tweet hostile b fine grain multi label classification tweet four categories hate fake offensive defamation build architecture take emojis segment hashtags consideration classification able experimentally showcase performance upgrade due tapt system team name irel iiit rank first hostile post detection hindi share task f1 score nine thousand, seven hundred and sixteen coarse grain detection weight f1 score six thousand, two hundred and ninety-six fine grain multi label classification provide blind test corpora
provide personalize explanations recommendations help users understand underlie insight recommendation result helpful effectiveness transparency persuasiveness trustworthiness recommender systems current explainable recommendation model mostly generate textual explanations base pre define sentence templates however expressiveness power template base explanation sentence limit pre define expressions manually define expressions require significant human efforts motivate problem propose generate free text natural language explanations personalize recommendation particular propose hierarchical sequence sequence model hss personalize explanation generation different conventional sentence generation nlp research great challenge explanation generation e commerce recommendation sentence user review explanation purpose solve problem propose auto denoising mechanism base topical item feature word sentence generation experiment various e commerce product domains show approach improve recommendation accuracy also explanation quality term offline measure feature word coverage research one initial step grant intelligent agents ability explain base natural language sentence
embody instruction follow challenge problem require agent infer sequence primitive action achieve goal environment state complex language visual input action learn realistic environments directives alfred recently propose benchmark problem consist step step natural language instructions achieve subgoals compose ultimate high level goal key challenge task include localize target locations navigate visual input ground language instructions visual appearance object address challenge study augment agent field view navigation subgoals multiple view angle train agent predict relative spatial relation target location timestep also improve language ground introduce pre train object detection module model pipeline empirical study show approach exceed baseline model performance
automatic text summarization widely study important task natural language process traditionally various feature engineer machine learn base systems propose extractive well abstractive text summarization recently deep learn base specifically transformer base systems immensely popular summarization cognitively challenge task extract summary worthy sentence laborious express semantics brief abstractive summarization complicate paper specifically look problem summarize scientific research paper multiple domains differentiate two type summaries namely laysumm short summary capture essence research paper layman term restrict overtly specific technical jargon b longsumm much longer detail summary aim provide specific insights various ideas touch upon paper leverage latest transformer base model systems simple intuitive base specific paper section contribute human summaries two type describe evaluations gold standard summaries use rouge metrics prove effectiveness approach blind test corpora system rank first third longsumm laysumm task respectively
desirable property learn systems effective interpretable towards goal recent model propose first generate extractive explanation input text generate prediction explanation call explain predict model model primarily consider task input supervision signal learn extractive explanation effectively integrate rationales data additional inductive bias improve task performance propose novel yet simple approach expred use multi task learn explanation generation phase effectively trade explanation prediction losses use another prediction network extract explanations optimize task performance conduct extensive evaluation approach three diverse language datasets fact verification sentiment classification qa find substantially outperform exist approach
decade since two thousand and ten successes artificial intelligence forefront computer science technology vector space model solidify position forefront artificial intelligence time quantum computers become much powerful announcements major advance frequently news mathematical techniques underlie areas common sometimes realize vector space take position axiomatic heart quantum mechanics 1930s adoption key motivation derivation logic probability linear geometry vector space quantum interactions particles model use tensor product also use express object operations artificial neural network paper describe common mathematical areas include examples use artificial intelligence ai particularly automate reason natural language process nlp techniques discuss include vector space scalar products subspaces implication orthogonal projection negation dual vectors density matrices positive operators tensor products application areas include information retrieval categorization implication model word sense disambiguation inference knowledge base semantic composition approach potentially implement quantum hardware many practical step implementation early stag already realize explain common mathematical tool help researchers ai quantum compute exploit overlap recognize explore new directions along way
accord probability rank principle prp rank document decrease order probability relevance lead optimal document rank ad hoc retrieval prp hold two condition meet c1 model well calibrate c2 probabilities relevance report certainty know however deep neural network dnns often well calibrate several source uncertainty thus c1 c2 might satisfy neural rankers give success neural learn rank l2r approach especially bert base approach first analyze circumstances deterministic ie output point estimate neural rankers calibrate motivate find use two techniques model uncertainty neural rankers lead propose stochastic rankers output predictive distribution relevance oppose point estimate experimental result ad hoc retrieval task conversation response rank reveal bert base rankers robustly calibrate stochastic bert base rankers yield better calibration ii uncertainty estimation beneficial risk aware neural rank ietaking account uncertainty rank document predict unanswerable conversational contexts
many computer scientists use aggregate answer online workers represent grind truth prior work show aggregation methods majority vote effective measure relatively objective feature subjective feature semantic connotation online workers know optimize hourly earn tend deteriorate quality responses work longer paper aim address issue propose quality aware semantic data annotation system observe timely feedback workers performance quantify quality score better inform online workers maintain quality label throughout extend period time validate effectiveness propose annotation system evaluate performance base expert label dataset ii demonstrate machine learn task lead consistent learn behavior seventy eighty accuracy result suggest system researchers collect high quality answer subjective semantic feature large scale
researchers worldwide seek repurpose exist drug discover new drug counter disease cause severe acute respiratory syndrome coronavirus two sars cov two promise source candidates study molecules report scientific literature drug like context coronavirus research report project leverage human artificial intelligence detect reference drug like molecules free text engage non expert humans create corpus label text use label corpus train name entity recognition model employ train model extract ten thousand, nine hundred and twelve drug like molecules covid nineteen open research dataset challenge cord nineteen corpus one hundred and ninety-eight thousand, eight hundred and seventy-five paper performance analyse show automate extraction model achieve performance par non expert humans
sufficient amount annotate data usually require fine tune pre train language model downstream task unfortunately attain label data costly especially multiple language varieties dialects propose self train pre train language model zero shoot scenarios improve performance data scarce varieties use resources data rich ones demonstrate utility approach context arabic sequence label use language model fine tune modern standard arabic msa predict name entities ne part speech pos tag several dialectal arabic da varieties show self train indeed powerful improve zero shoot msa da transfer large texttildelow ten f1 ner two accuracy pos tag acquire even better performance shoot scenarios limit amount label data conduct ablation study show performance boost observe directly result unlabeled da examples use self train work open opportunities develop da model exploit msa resources extend languages task code fine tune model access https githubcom mohammadkhalifa zero shoot arabic dialects
paper propose discrete knowledge graph kg embed dkge method project kg entities relations ham space base computationally tractable discrete optimization algorithm solve formidable storage computation cost challenge traditional continuous graph embed methods convergence dkge guarantee theoretically extensive experiment demonstrate dkge achieve superior accuracy classical hash function map effective continuous embeddings discrete cod besides dkge reach comparable accuracy much lower computational complexity storage compare many continuous graph embed methods
despite impressive performance standard benchmarks deep neural network often brittle deploy real world systems consequently recent research focus test robustness model result diverse set evaluation methodologies range adversarial attack rule base data transformations work identify challenge evaluate nlp systems propose solution form robustness gym rg simple extensible evaluation toolkit unify four standard evaluation paradigms subpopulations transformations evaluation set adversarial attack provide common platform evaluation robustness gym enable practitioners compare result four evaluation paradigms click easily develop share novel evaluation methods use build set abstractions validate robustness gym utility practitioners conduct real world case study sentiment model team reveal performance degradations eighteen verify robustness gym aid novel research analyse perform first study state art commercial academic name entity link nel systems well fine grain analysis state art summarization model nel commercial systems struggle link rare entities lag academic counterparts ten state art summarization model struggle examples require abstraction distillation degrade nine robustness gym find https robustnessgymcom
logical reason task symbols learn arithmetic operations computer program evaluations become challenge deep learn particular even state art neural network fail achieve textitout distribution ood generalization symbolic reason task whereas humans easily extend learn symbolic rule resolve difficulty propose neural sequence grid seq2grid module input preprocessor automatically segment align input sequence grid module output grid via novel differentiable map neural network structure take grid input resnet textcnn jointly train module end end fashion extensive experiment show neural network module input preprocessor achieve ood generalization various arithmetic algorithmic problems include number sequence prediction problems algebraic word problems computer program evaluation problems state art sequence transduction model moreover verify module enhance textcnn solve babi qa task without external memory
present eventplus temporal event understand pipeline integrate various state art event understand components include event trigger type detection event argument detection event duration temporal relation extraction event information especially event temporal knowledge type common sense knowledge help people understand stories evolve provide predictive hint future events eventplus first comprehensive temporal event understand pipeline provide convenient tool users quickly obtain annotations events temporal information user provide document furthermore show eventplus easily adapt domains eg biomedical domain make eventplus publicly available facilitate event relate information extraction downstream applications
impact user satisfaction policy learn task orient dialogue systems long subject research interest current model estimate user satisfaction either treat context short texts product review ii rely turn feature instead distribute semantic representations work adopt deep neural network use distribute semantic representation learn estimate user satisfaction conversations evaluate impact model context length network moreover show propose hierarchical network outperform state art quality estimators furthermore show apply network infer reward function partial observable markov decision process pomdp yield great improvement task success rate
automatic height age estimation speakers use acoustic feature widely use purpose human computer interaction forensics etc work propose novel approach use attention mechanism build end end architecture height age estimation attention mechanism combine long short term memorylstm encoder able capture long term dependencies input acoustic feature modify conventionally use attention calculate context vectors sum attention across timeframes introduce modify context vector take account total attention across encoder units well give us new cross attention mechanism apart also investigate multi task learn approach jointly estimate speaker height age train test model timit corpus model outperform several approach literature achieve root mean square error rmse 692cm and634cm male female heights respectively rmse 785years 875years male females age respectively track attention weight allocate different phone find vowel phone important whistlestop phone least important estimation task
desirable text speech system take account environment synthetic speech present provide appropriate context dependent output user paper present compare various approach generate different speak style namely normal lombard whisper speech use limit data follow systems propose assess one pre train fine tune model style two lombard whisper speech conversion signal process base approach three multi style generation use single model base speaker verification model mean opinion score ab preference listen test show one generate high quality speech pre train fine tune approach speak style two although speaker verification sv model explicitly train discriminate different speak style lombard whisper voice use pre train system sv model use style encoder generate different style embeddings input tacotron system also show result synthetic lombard speech significant positive impact intelligibility gain
differences political ideology increasingly appear impediment successful bipartisan communication local leadership example recent empirical find show conservatives less likely adhere covid nineteen health directives behavior direct contradiction past research indicate conservatives rule abide prefer avoid loss prevention motivate liberals reconcile disconnect recent empirical find past research use insights gather press release millions tweet mobility data capture local movement retail grocery workplace park transit domains covid nineteen shelter place order find conservatives adhere health directives express fear virus order better understand phenomenon analyze official citizen communications find press release local federal government along number confirm covid nineteen case lead increase expressions fear twitter
describe machine aid script curator masc system human machine collaborative script author script produce masc include one english descriptions sub events comprise larger complex event two event type events three record entities expect participate multiple sub events four temporal sequence sub events masc automate portion script creation process suggestions event type link wikidata sub events may forget illustrate automations useful script writer case study script
recent advance neural network base language model lead successful deployments model improve user experience various applications demonstrate strong performance language model come along ability memorize rare train sample pose serious privacy threats case model train confidential user content work introduce methodology investigate identify user content train data could leak strong realistic threat model propose two metrics quantify user level data leakage measure model ability produce unique sentence fragment within train data metrics enable compare different model train data term privacy demonstrate approach extensive numerical study rnn transformer base model illustrate propose metrics utilize investigate efficacy mitigations like differentially private train api harden
automatic speech recognition asr systems evaluate use word error rate wer calculate compare number errors grind truth transcription asr system calculation however require manual transcription speech signal obtain grind truth since transcribe audio signal costly process automatic wer evaluation e wer methods develop automatically predict wer speech system rely transcription speech signal feature wer continuous variable previous work show posit e wer classification problem effective regression however convert classification set approach suffer heavy class imbalance paper propose new balance paradigm e wer classification set within paradigm also propose wer bert bert base architecture speech feature e wer furthermore introduce distance loss function tackle ordinal nature e wer classification propose approach paradigm evaluate librispeech dataset commercial black box asr system google cloud speech text api result experiment demonstrate wer bert establish new state art automatic wer estimation
quantify confidence conversely uncertainty prediction highly desirable trait automatic system improve robustness usefulness downstream task paper investigate confidence estimation end end automatic speech recognition asr previous work address confidence measure lattice base asr current machine learn research mostly focus confidence measure unstructured deep learn however asr systems increasingly build upon deep end end methods little work try develop confidence measure context fill gap provide extensive benchmark popular confidence methods four well know speech datasets two challenge overcome adapt exist methods work structure data sequence obtain confidences coarser level predictions word instead tokens result suggest strong baseline obtain scale logits learn temperature follow estimate confidence negative entropy predictive distribution finally sum pool aggregate word level
investigate solve cross corpus news recommendation unseen users future problem traditional content base recommendation techniques often fail luckily real world recommendation service publisher eg daily news may accumulate large corpus lot consumers use newly deploy publisher eg political news take advantage exist corpus propose transfer learn model dub trnews news recommendation transfer knowledge source corpus target corpus tackle heterogeneity different user interest different word distributions across corpora design translator base transfer learn strategy learn representation map source target corpora learn translator use generate representations unseen users future show experiment real world datasets trnews better various baselines term four metrics also show translator effective among exist transfer strategies
malicious software threats detection gain importance subdomain information security due expansion ict applications daily settings major challenge design develop anti malware systems coverage detection particularly development dynamic analysis methods detect polymorphic metamorphic malware efficiently present study propose methodological framework detect malicious code analyze run trace output long short term memory lstm develop model run trace malicious benign portable executable pe file create dataset run trace output obtain dynamic analysis pe file obtain dataset instruction format sequence call instruction sequence model ism split first dataset basic block obtain second one call basic block sequence model bsm experiment show ism achieve accuracy eight thousand, seven hundred and fifty-one false positive rate one thousand, eight hundred and thirty-four bsm achieve accuracy nine thousand, nine hundred and twenty-six false positive rate two hundred and sixty-two
sarcasm linguistic expression often use communicate opposite say usually something unpleasant intention insult ridicule inherent ambiguity sarcastic expressions make sarcasm detection difficult work focus detect sarcasm textual conversations various social network platforms online media end develop interpretable deep learn model use multi head self attention gate recurrent units multi head self attention module aid identify crucial sarcastic cue word input recurrent units learn long range dependencies cue word better classify input text show effectiveness approach achieve state art result multiple datasets social network platforms online media model train use propose approach easily interpretable enable identify sarcastic cue input text contribute final classification score visualize learn attention weight sample input texts showcase effectiveness interpretability model
recently transformer base language model bert show tremendous performance improvement range natural language process task however language model usually computation expensive memory intensive inference result difficult deploy resource restrict devices improve inference performance well reduce model size maintain model accuracy propose novel quantization method name kdlsq bert combine knowledge distillation kd learn step size quantization lsq language model quantization main idea method kd technique leverage transfer knowledge teacher model student model exploit lsq quantize student model quantization train process extensive experiment result glue benchmark squad demonstrate propose kdlsq bert perform effectively different bite eg two bite sim eight bite quantization also outperform exist bert quantization methods even achieve comparable performance full precision base line model obtain 149x compression ratio code public available
withtheadventofsocialmediatherehasbeenanextremely rapid increase content share online consequently propagation fake news hostile message social media platforms also skyrocket paper address problem detect hostile fake content devanagari hindi script multi class multi label problem use nlp techniques build model make use abusive language detector couple feature extract via hindi bert hindi fasttext model metadata model achieve ninety-seven f1 score coarse grain evaluation hostility detection task additionally build model identify fake news relate covid nineteen english tweet leverage entity information extract tweet along textual representations learn word embeddings achieve ninety-three f1 score english fake news detection task
paper describe method annotation epidemiological information animal disease relate news article annotation guidelines generic aim embrace animal zoonotic infectious diseases regardless pathogen involve way transmission eg vector bear airborne contact framework rely successive annotation sentence news article annotator evaluate sentence specific epidemiological context correspond publication news article
conventional model visual question answer vqa explore deterministic approach various type image feature question feature attention mechanisms however exist modalities explore addition image question pair bring extra information model work propose latent variable model vqa extra information eg caption answer categories incorporate latent variables improve inference turn benefit question answer performance experiment vqa v20 benchmarking dataset demonstrate effectiveness propose model improve strong baselines especially rely extensive language vision pre train
task video question answer videoqa consist answer natural language question video serve proxy evaluate performance model scene sequence understand methods design videoqa date end end deep learn architectures struggle complex temporal causal reason provide limit transparency reason step present hyster hybrid spatio temporal event reasoner reason physical events videos model leverage strength deep learn methods extract information video frame reason capabilities explainability symbolic artificial intelligence answer set program framework define method base general temporal causal physics rule transfer across task apply model clevrer dataset demonstrate state art result question answer accuracy work set foundations incorporation inductive logic program field videoqa
end end model achieve impressive result task automatic speech recognition asr low resource asr task however label data hardly satisfy demand end end model self supervise acoustic pre train already show amaze asr performance transcription still inadequate language model end end model work fuse pre train acoustic encoder wav2vec20 pre train linguistic encoder bert end end asr model fuse model need learn transfer speech language fine tune limit label data length two modalities match monotonic attention mechanism without additional parameters besides fully connect layer introduce hide map modalities propose schedule fine tune strategy preserve utilize text context model ability pre train linguistic encoder experiment show effective utilize pre train modules model achieve better recognition performance callhome corpus fifteen hours end end model
log base cyber threat hunt emerge important solution counter sophisticate cyber attack however exist approach require non trivial efforts manual query construction overlook rich external knowledge threat behaviors provide open source cyber threat intelligence oscti bridge gap build threatraptor system facilitate cyber threat hunt computer systems use oscti build upon mature system audit frameworks threatraptor provide one unsupervised light weight accurate nlp pipeline extract structure threat behaviors unstructured oscti text two concise expressive domain specific query language tbql hunt malicious system activities three query synthesis mechanism automatically synthesize tbql query extract threat behaviors four efficient query execution engine search big system audit log data
policy specification process human initialize robot behaviour turn warm start policy optimization via reinforcement learn rl policy specification design inherently collaborative process modern methods base learn demonstration deep rl lack model interpretability accessibility classify current state art methods policy specification rely black box model insufficient mean collaboration non expert users model provide mean inspect policies learn agent focus create usable modality teach robot behaviour paper propose novel machine learn framework enable humans one specify natural language interpretable policies form easy understand decision tree two leverage policies warm start reinforcement learn three outperform baselines lack natural language initialization mechanism train approach collect first kind corpus map free form natural language policy descriptions decision tree base policies show novel framework translate natural language decision tree ninety-six ninety-seven accuracy hold corpus across two domains respectively finally validate policies initialize natural language command able significantly outperform relevant baselines p one benefit natural language base warm start technique
paper consider problem leverage textual descriptions improve generalization control policies new scenarios unlike prior work space assume access form prior knowledge connect text state observations learn symbol ground control policy simultaneously challenge due lack concrete supervision incorrect ground result worse performance policies use text develop new model emma entity mapper multi modal attention use multi modal entity condition attention module allow selective focus relevant sentence manual entity environment emma end end differentiable learn latent ground entities dynamics text observations use environment reward source supervision empirically test model design new framework one thousand, three hundred and twenty game collect text manuals free form natural language via crowd source demonstrate emma achieve successful zero shoot generalization unseen game new dynamics obtain significantly higher reward compare multiple baselines ground acquire emma also robust noisy descriptions linguistic variation
paper propose unify pre train approach call unispeech learn speech representations unlabeled label data supervise phonetic ctc learn phonetically aware contrastive self supervise learn conduct multi task learn manner resultant representations capture information correlate phonetic structure improve generalization across languages domains evaluate effectiveness unispeech cross lingual representation learn public commonvoice corpus result show unispeech outperform self supervise pretraining supervise transfer learn speech recognition maximum one hundred and thirty-four one hundred and seventy-eight relative phone error rate reductions respectively average test languages transferability unispeech also demonstrate domain shift speech recognition task ie relative word error rate reduction six previous approach
study analyze impact covid nineteen pandemic subjective well measure twitter data indicators japan italy turn overall subjective well drop one hundred and seventeen italy eighty-three japan first nine months two thousand and twenty compare last two months two thousand and nineteen even compare historical mean index data science approach try identify possible cause drop consider several explanatory variables include climate air quality data number covid nineteen case deaths facebook covid flu symptoms global survey google trend data coronavirus relate search google mobility data policy intervention measure economic variables google trend proxies well health stress proxy variables base big data show simple static regression model able capture complexity well therefore propose dynamic elastic net approach show different group factor may impact well different periods even short time length show country specific aspects finally structural equation model analysis try address causal relationships among covid nineteen factor subjective well show overall prolong mobility restrictionsflu covid like symptoms economic uncertainty social distance news pandemic negative effect subjective well
remain aware fast evolve cyber threat landscape open source cyber threat intelligence oscti receive grow attention community commonly knowledge threats present vast number oscti report despite press need high quality oscti exist oscti gather management platforms however primarily focus isolate low level indicators compromise hand higher level concepts eg adversary tactics techniques procedures relationships overlook contain essential knowledge threat behaviors critical uncover complete threat scenario bridge gap propose securitykg system automate oscti gather management securitykg collect oscti report various source use combination ai nlp techniques extract high fidelity knowledge threat behaviors construct security knowledge graph securitykg also provide ui support various type interactivity facilitate knowledge graph exploration
investigate self attention mechanism bert fine tune scenario classification scientific article taxonomy research discipline observe self attention focus word highly relate domain article particularly small subset vocabulary word tend receive attention compare evaluate subset attend word feature selection methods normally use text classification order characterize self attention possible feature selection approach use conceptnet grind truth also find attend word relate research field article however conventional feature selection methods still better option learn classifiers scratch result suggest self attention identify domain relevant term discriminatory information bert encode contextualized output classification layer also raise question whether inject feature selection methods self attention mechanism could optimize single sequence classification use transformers
open domain conversational search assistants aim answer user question open topics conversational manner paper show transformer architecture achieve state art result key ir task leverage creation conversational assistants engage open domain conversational search single yet informative answer particular propose open domain abstractive conversational search agent pipeline address two major challenge first conversation context aware search second abstractive search answer generation address first challenge conversation context model query rewrite method unfold context conversation specific moment search correct answer answer pass transformer base ranker improve retrieval performance second challenge tackle recent abstractive transformer architectures generate digest top relevant passages experiment show transformers deliver solid performance across task conversational search outperform best trec cast two thousand and nineteen baseline
recent advance automatic speech recognition asr achieve accuracy level comparable human transcribers lead researchers debate machine reach human performance previous work focus english language modular hide markov model deep neural network hmm dnn systems paper perform comprehensive benchmarking end end transformer asr modular hmm dnn asr human speech recognition hsr arabic language dialects hsr evaluate linguist performance lay native speaker performance new dataset collect part study asr end end work lead one hundred and twenty-five two hundred and seventy-five three hundred and thirty-eight wer new performance milestone mgb2 mgb3 mgb5 challenge respectively result suggest human performance arabic language still considerably better machine absolute wer gap thirty-six average
deep learn model susceptible adversarial examples imperceptible perturbations original input result adversarial attack model analysis attack state art transformers nlp help improve robustness model adversarial input paper present adv olm black box attack method adapt idea occlusion language model olm current state art attack methods olm use rank word sentence later substitute use word replacement strategies experimentally show approach outperform attack methods several text classification task
us food drug administration fda actively promote use real world data rwd drug development rwd generate important real world evidence reflect real world clinical environment treatments use meanwhile artificial intelligence ai especially machine deep learn ml dl methods increasingly use across many stag drug development process advancements ai also provide new strategies analyze large multidimensional rwd thus conduct rapid review article past twenty years provide overview drug development study use ai rwd find popular applications adverse event detection trial recruitment drug repurposing also discuss current research gap future opportunities
computers get powerful integrate daily live focus increasingly shift towards human friendly interfaces make automatic speech recognition asr central player ideal mean interaction machine consequently interest speech technology grow last years systems propose higher accuracy level achieve even surpass textithuman accuracy asr systems become increasingly powerful computational complexity also increase hardware support keep pace paper propose technique improve energy efficiency performance asr systems focus low power hardware edge devices focus optimize dnn base acoustic model evaluation observe main bottleneck state art asr systems leverage run time information beam search reduce energy execution time acoustic model evaluation two hundred and fifty-six two hundred and fifty-nine respectively negligible accuracy loss
artificial intelligence provide backbone many tool people use around world recent work bring attention algorithms power ai free politics stereotype bias work area focus ways ai exacerbate exist inequalities discrimination little work study governments actively shape train data describe censorship affect development wikipedia corpuses text data regularly use pre train input nlp algorithms show word embeddings train baidu baike online chinese encyclopedia different associations adjectives range concepts democracy freedom collective action equality people historical events china regularly block uncensored counterpart chinese language wikipedia examine implications discrepancies study use downstream ai applications paper show government repression censorship self censorship may impact train data applications draw
speaker diarization task label audio video record class correspond speaker identity short task identify speak early years speaker diarization algorithms develop speech recognition multi speaker audio record enable speaker adaptive process also gain value stand alone application time provide speaker specific meta information downstream task audio retrieval recently rise deep learn technology drive force revolutionary change research practice across speech application domains past decade rapid advancements make speaker diarization paper review historical development speaker diarization technology also recent advancements neural speaker diarization approach also discuss speaker diarization systems integrate speech recognition applications recent surge deep learn lead way jointly model two components complementary consider excite technical trend believe valuable contribution community provide survey work consolidate recent developments neural methods thus facilitate progress towards efficient speaker diarization
textual explanations prove help improve user satisfaction machine make recommendations however current mainstream solutions loosely connect learn explanation learn recommendation example often separately model rat prediction content generation task work propose strengthen connection enforce idea sentiment alignment recommendation correspond explanation train time two learn task join latent sentiment vector encode recommendation module use make word choices explanation generation train inference time explanation module require generate explanation text match sentiment predict recommendation module extensive experiment demonstrate solution outperform rich set baselines recommendation explanation task especially improve quality generate explanations importantly user study confirm generate explanations help users better recognize differences recommend items understand item recommend
newspapers trustworthy media people get reliable credible information compare source hand social media often spread rumor mislead news get traffic attention careful characterization evaluation interpretation newspaper data provide insight intrigue passionate social issue monitor big social incidence study analyze large set spatio temporal bangladeshi newspaper data relate covid nineteen pandemic methodology include volume analysis topic analysis automate classification sentiment analysis news article get insight covid nineteen pandemic different sectors regions bangladesh period time analysis help government organizations figure challenge arise society due pandemic step take immediately post pandemic period government ally come together address crisis future keep problems mind
paper propose two intuitive metrics skew stereotype quantify analyse gender bias present contextual language model tackle winobias pronoun resolution task find evidence gender stereotype correlate approximately negatively gender skew box model suggest trade two form bias investigate two methods mitigate bias first approach online method effective remove skew expense stereotype second inspire previous work elmo involve fine tune bert use augment gender balance dataset show reduce skew stereotype relative unaugmented fine tune counterpart however find exist gender bias benchmarks fully probe professional bias pronoun resolution may obfuscate cross correlations manifestations gender prejudice code available online https githubcom 12kleingordon34 nlpmastersproject
work limit number require attention inference hop memory augment neural network propose online adaptive approach call a2p mann exploit small neural network classifier adequate number attention inference hop input query determine technique result elimination large number unnecessary computations extract correct answer addition lower computations a2p mann suggest prune weight final fc fully connect layer end two prune approach one negligible accuracy loss controllable loss final accuracy develop efficacy technique assess use twenty question answer qa task babi dataset analytical assessment reveal average forty-two fewer computations compare baseline mann cost less one accuracy loss addition use along previously publish zero skip technique computation count reduction sixty-eight achieve finally propose approach without zero skip implement cpu gpu platforms forty-three runtime reduction achieve
autoregressive sequence generation model achieve state art performance areas like machine translation image caption model autoregressive generate word condition previously generate word lead heavy latency inference recently non autoregressive decode propose machine translation speed inference time generate word parallel typically model use word level cross entropy loss optimize word independently however learn process fail consider sentence level consistency thus result inferior generation quality non autoregressive model paper propose simple efficient model non autoregressive sequence generation nag novel train paradigm counterfactuals critical multi agent learn cmal cmal formulate nag multi agent reinforcement learn system element position target sequence view agents learn cooperatively maximize sentence level reward mscoco image caption benchmark nag method achieve performance comparable state art autoregressive model bring 139x decode speedup wmt14 en de machine translation dataset method outperform cross entropy train baseline sixty bleu point achieve greatest decode speedup 1746x
distant weak supervision allow obtain large amount label train data quickly cheaply automatic annotations tend contain high amount errors popular technique overcome negative effect noisy label noise model underlie noise process model work study quality estimate noise model theoretical side derive expect error noise model apart evaluate theoretical result commonly use synthetic noise also publish noisyner new noisy label dataset nlp domain obtain realistic distant supervision technique provide seven set label differ noise pattern evaluate different noise level instance parallel clean label available make possible study scenarios small amount gold standard data leverage theoretical result correspond experiment give insights factor influence noise model estimation like noise distribution sample technique
conventional algorithmic fairness west centric see sub group value methods paper de center algorithmic fairness analyse ai power india base thirty-six qualitative interview discourse analysis algorithmic deployments india find several assumptions algorithmic fairness challenge find india data always reliable due socio economic factor ml makers appear follow double standards ai evoke unquestioning aspiration contend localise model fairness alone window dress india distance model oppress communities large instead imagine algorithmic fairness india provide roadmap contextualise data model empower oppress communities enable fair ml ecosystems
emergence communication systems agents learn play referential signal game realistic image attract lot attention recently majority work focus use fix pretrained image feature extraction network potentially bias information agents learn communicate work consider signal game set sender agent must communicate information image receiver must select correct image many distractors investigate effect feature extractor weight task solve visual semantics learn model first demonstrate extent use pretrained feature extraction network inductively bias visual semantics convey emergent communication channel quantify visual semantics induce go explore ways inductive bias introduce encourage emergence semantically meaningful communication without need form supervise pretraining visual feature extractor impose various augmentations input image additional task game aim induce visual representations capture conceptual properties image experiment demonstrate communication systems capture visual semantics learn completely self supervise manner play right type game work bridge gap emergent communication research self supervise feature learn
train machine learn model meaningful order easy sample hard ones use curriculum learn provide performance improvements standard train approach base random data shuffle without additional computational cost curriculum learn strategies successfully employ areas machine learn wide range task however necessity find way rank sample easy hard well right pace function introduce difficult data limit usage curriculum approach survey show limit tackle literature present different curriculum learn instantiations various task machine learn construct multi perspective taxonomy curriculum learn approach hand consider various classification criteria build hierarchical tree curriculum learn methods use agglomerative cluster algorithm link discover cluster taxonomy end provide interest directions future work
expressive text encoders rnns transformer network center nlp model recent work effort focus sentence level task capture dependencies word single sentence pair sentence however certain task argumentation mine require account longer texts complicate structural dependencies deep structure prediction general framework combine complementary strengths expressive neural encoders structure inference highly structure domains nevertheless need arise go beyond sentence work rely combine output score independently train classifiers one main reason constrain inference come high computational cost paper explore use randomize inference alleviate concern show efficiently leverage deep structure prediction expressive neural encoders set task involve complicate argumentative structure
vision language navigation wayfinding agents enhance exploit automatically generate navigation instructions however exist instruction generators comprehensively evaluate automatic evaluation metrics use develop validate use human wayfinders show generators perform par slightly better template base generator far worse human instructors furthermore discover bleu rouge meteor cider ineffective evaluate ground navigation instructions improve instruction evaluation propose instruction trajectory compatibility model operate without reference instructions model show highest correlation human wayfinding outcomes score individual instructions rank instruction generation systems reference instructions available recommend use spice
able parse code switch cs utterances spanishenglish hindienglish essential democratize task orient semantic parse systems certain locales work focus spanglish spanishenglish release dataset cstop contain five thousand, eight hundred cs utterances alongside semantic parse examine cs generalizability various cross lingual xl model exhibit advantage pre train xl language model data one language present focus improve pre train model case english corpus alongside either zero cs train instance available propose two data augmentation methods zero shoot shoot settings fine tune use translate align augment use generation model follow match filter combine shoot set improvements decrease initial thirty point accuracy gap zero shoot full data settings two thirds
work investigate problems semantic parse shoot learn set set provide utterance logical form pair per new predicate state art neural semantic parsers achieve less twenty-five accuracy benchmark datasets k one tackle problem propose apply designate meta learn method train model ii regularize attention score alignment statistics iii apply smooth technique pre train result method consistently outperform baselines one two shoot settings
introduce task historical text summarisation document historical form language summarise correspond modern language fundamentally important routine historians digital humanities researchers never automate compile high quality gold standard text summarisation dataset consist historical german chinese news hundreds years ago summarise modern german chinese base cross lingual transfer learn techniques propose summarisation model train even cross lingual historical modern parallel data benchmark state art algorithms report automatic human evaluations distinguish historic modern language summarisation task standard cross lingual summarisation ie modern modern language highlight distinctness value dataset demonstrate transfer learn approach outperform standard cross lingual benchmarks task
propose method online news stream cluster variant non parametric stream k mean algorithm model use combination sparse dense document representations aggregate document cluster similarity along multiple representations make cluster decision use neural classifier weight document cluster similarity model learn use novel adaptation triplet loss linear classification objective show use suitable fine tune objective external knowledge pre train transformer model yield significant improvements effectiveness contextual embeddings cluster model achieve new state art standard stream cluster dataset english document
hate speech become major content moderation issue online social media platforms give volume velocity online content production impossible manually moderate hate speech relate content platform paper utilize multi task multi lingual approach base recently propose transformer neural network solve three sub task hate speech sub task part two thousand and nineteen share task hate speech offensive content hasoc identification indo european languages expand submission competition utilize multi task model train use three approach multi task learn separate task head b back translation c multi lingual train finally investigate performance various model identify instance transformer base model perform differently better show possible utilize different combine approach obtain model generalize easily different languages task trade slight accuracy case much reduce inference time compute cost open source update version hasoc two thousand and nineteen code new improvements https githubcom socialmediaie mtmlhatespeech
taxonomy hierarchically structure knowledge graph play crucial role machine intelligence taxonomy expansion task aim find position new term exist taxonomy capture emerge knowledge world keep taxonomy dynamically update previous taxonomy expansion solutions neglect valuable information bring hierarchical structure evaluate correctness merely add edge downgrade problem node pair score mini path classification paper propose hierarchy expansion framework hef fully exploit hierarchical structure properties maximize coherence expand taxonomy hef make use taxonomy hierarchical structure multiple aspects hef utilize subtrees contain relevant nod self supervision data complete comparison parental sibling relations ii hef adopt coherence model module evaluate coherence taxonomy subtree integrate hypernymy relation detection several tree exclusive feature iii hef introduce fit score position selection explicitly evaluate path level selections take full advantage parental relations interchange information disambiguation self correction extensive experiment show better exploit hierarchical structure optimize taxonomy coherence hef vastly surpass prior state art three benchmark datasets average improvement four hundred and sixty-seven accuracy three hundred and twenty-three mean reciprocal rank
paper introduce large scale korean speech dataset call vote400 use analyze recognize voice elderly people dataset include three hundred hours continuous dialog speech one hundred hours read speech record elderly people age sixty-five years preliminary experiment show speech recognition system train vote400 outperform conventional systems speech recognition elderly people voice work multi organizational effort lead etri mind lab inc purpose advance speech recognition performance elderly care robots
understand mean text fundamental challenge natural language understand nlu research ideal nlu system process language way exclusive single task dataset keep mind introduce novel knowledge drive semantic representation approach english text leverage verbnet lexicon able map syntax tree text commonsense mean represent use basic knowledge primitives general purpose knowledge represent approach use build reason base nlu system also provide justification apply approach construct two nlu applications present square semantic base question answer reason engine stacack stateful conversational agent use commonsense knowledge systems work truly understand natural language text process provide natural language explanations responses maintain high accuracy
scientific literature tend grow function fund interest give field mine literature reveal trend may immediately apparent cord nineteen corpus represent grow corpus scientific literature associate covid nineteen examine intersection set candidate therapeutics identify drug repurposing study temporal instance cord nineteen corpus determine possible find measure change associate time propose techniques use could form basis tool pre screen new candidate therapeutics early research process
recent advance deep learn techniques enable machine generate cohesive open end text prompt sequence word context model empower many downstream applications conversation bots automatic storytelling show generate texts exhibit social bias systematically study benchmark social bias open end language generation introduce bias open end language generation dataset bold large scale dataset consist twenty-three thousand, six hundred and seventy-nine english text generation prompt bias benchmarking across five domains profession gender race religion political ideology also propose new automate metrics toxicity psycholinguistic norms text gender polarity measure social bias open end text generation multiple angle examination text generate three popular language model reveal majority model exhibit larger social bias human write wikipedia text across domains result highlight need benchmark bias open end language generation caution users language generation model downstream task cognizant embed prejudice
author stylize rewrite task rewrite input text particular author style recent work area leverage transformer base language model denoising autoencoder setup generate author stylize text without rely parallel corpus data however approach limit lack explicit control target attribute entirely data drive paper propose director generator framework rewrite content target author style specifically focus certain target attribute show propose framework work well even limit size target author corpus experiment corpora consist relatively small size text author three distinct author show significant improvements upon exist work rewrite input texts target author style quantitative qualitative analyse show model better mean retention result fluent generations
evolution social media platforms empower everyone access information easily social media users easily share information rest world may sometimes encourage spread fake news result undesirable consequences work train model identify health news relate covid nineteen pandemic real fake model achieve high f1 score nine thousand, eight hundred and sixty-four model achieve second place leaderboard tail first position narrow margin five point
machine learn seek identify encode body knowledge within provide datasets however data encode subjective content determine possible outcomes model train subjectivity enable marginalisation part society term social bias seek remove paper contextualise discourse bias ml community subjective choices development process consideration choices data model development construct subjectivity bias represent model argue address mitigate bias near impossible data ml model object mean make step development pipeline data selection annotation model train analysis accordingly find prevalent discourse bias limit ability address social marginalisation recommend conscientious accept de bias methods correct fraction bias
last years three major topics receive increase interest deep learn nlp conversational agents bring three topics together create amaze digital customer experience indeed deploy production solve real world problems something innovative disruptive introduce new portuguese financial domain language representation model call berta berta uncase bert base train scratch data ita virtual assistant chatbot solution novel contribution berta pretrained language model require less data reach state art performance three nlp task generate smaller lighter model make deployment feasible develop three task validate model information retrieval frequently ask question faq ita bank sentiment analysis virtual assistant data ner solution propose task real world solutions production environment usage specialist model prove effective compare google bert multilingual dprquestionencoder facebook available hug face berta improve performance twenty-two faq retrieval mrr metric twenty-one sentiment analysis f1 score forty-four ner f1 score also represent sequence sixty-six fewer tokens compare shelf model
recent study field machine translation mt natural language process nlp show exist model amplify bias observe train data amplification bias language technology mainly examine respect specific phenomena gender bias work go beyond study gender mt investigate bias amplification might affect language broader sense hypothesize algorithmic bias ie exacerbation frequently observe pattern combination loss less frequent ones exacerbate societal bias present current datasets could also lead artificially impoverish language machine translationese assess linguistic richness lexical morphological level translations create different data drive mt paradigms phrase base statistical pb smt neural mt nmt experiment show loss lexical morphological richness translations produce investigate mt paradigms two language pair enfr enes
propose simple method automatic speech recognition asr fine tune bert language model lm train large scale unlabeled text data generate rich contextual representations assumption give history context sequence powerful lm narrow range possible choices speech signal use simple clue hence compare conventional asr systems train powerful acoustic model scratch believe speech recognition possible simply fine tune bert model initial study demonstrate effectiveness propose idea aishell dataset show stack simple top bert yield reasonable performance
guess game prototypical instance learn interact paradigm work investigate well artificial agent benefit play guess game later ask perform novel nlp downstream task visual question answer vqa propose two ways exploit play guess game one supervise learn scenario agent learn mimic successful guess game two novel way agent play call self play via iterate experience learn spiel evaluate ability procedures generalize domain evaluation show increase accuracy seven hundred and seventy-nine compare competitors evaluation suite compguesswhat transfer evaluation show improve performance vqa tdiuc dataset term harmonic average accuracy five hundred and thirty-one thank fine grain object representations learn via spiel
segmentation email functional zone also dub email zone relevant preprocessing step nlp task deal email however despite multilingual character email applications previous literature regard email zone corpora systems develop essentially english paper analyse exist email zone corpora propose new multilingual benchmark compose six hundred and twenty-five email portuguese spanish french moreover introduce okapi first multilingual email segmentation model base language agnostic sentence encoder besides generalize well unseen languages model competitive current english benchmarks reach new state art performances domain adaptation task english
non parallel text style transfer attract increase research interest recent years despite successes transfer style base encoder decoder framework current approach still lack ability preserve content even logic original sentence mainly due large unconstrained model space simplify assumptions latent embed space since language intelligent product humans certain grammars limit rule base model space nature relieve problem require reconcile model capacity deep neural network intrinsic model constraints human linguistic rule end propose method call graph transformer base auto encoder gtae model sentence linguistic graph perform feature extraction style transfer graph level maximally retain content linguistic structure original sentence quantitative experiment result three non parallel text style transfer task show model outperform state art methods content preservation achieve comparable performance transfer accuracy sentence naturalness
federate learn fl promise approach distribute compute well distribute data provide level privacy compliance legal frameworks make fl attractive consumer healthcare applications area actively explore study examine fl context larger language model lack comprehensive review robustness across task architectures number clients relevant factor paper explore fine tune transformer base language model federate learn set evaluate three popular bert variants different size bert albert distilbert number text classification task sentiment analysis author identification perform extensive sweep number clients range thirty-two evaluate impact distribute compute task performance federate average set find suggest large size evaluate model generally prohibitive federate train find different model handle federate average vary degree notably distilbert converge significantly slower larger number clients circumstances even collapse chance level performance investigate issue present interest perspective future research
two thousand and sixteen unite kingdom uk citizens vote leave european union eu officially implement two thousand and twenty period uk residents experience great deal uncertainty around uk continue relationship eu many people use social media platforms express emotions critical event sentiment analysis recently consider important tool detect mental well twitter content however detect psychological distress status political relate tweet challenge task due lack explicit sentence describe depressive anxiety status address problem paper leverage transfer learn approach sentiment analysis measure non clinical psychological distress status brexit tweet framework transfer knowledge learn self report psychological distress tweet source domain detect distress status brexit tweet target domain framework apply domain adaptation technique decrease impact negative transfer source target domains paper also introduce brexit distress index use detect level psychological distress individuals brexit tweet design experiment include data domains propose model able detect non clinical psychological distress status brexit tweet accuracy sixty-six sixty-two source target domains respectively
paper propose novel end end sequence sequence speak language understand model use attention mechanism reliably select contextual acoustic feature order hypothesize semantic content initial architecture capable extract pronounce word concepts acoustic span design test shallow fusion language model system reach one hundred and thirty-six concept error rate cer one hundred and eighty-five concept value error rate cver french media corpus achieve absolute twenty-eight point reduction compare state art original model propose hypothesize concepts value transduction reach one hundred and fifty-four cer two hundred and sixteen cver without new type context
present universal neural vocoder base parallel wavenet additional condition network call audio encoder universal vocoder offer real time high quality speech synthesis wide range use case test forty-three internal speakers diverse age gender speak twenty languages seventeen unique style seven voice five style expose train show propose universal vocoder significantly outperform speaker dependent vocoders overall also show propose vocoder outperform several exist neural vocoder architectures term naturalness universality find consistent test three hundred open source voice
distantly supervise relation extraction effective method scale large corpora suffer noisy label exist approach try alleviate noise multi instance learn provide additional information manage recognize mainly top frequent relations neglect long tail propose redsandt relation extraction distant supervision transformers novel distantly supervise transformer base method manage capture wider set relations highly informative instance label embeddings exploit bert pre train model relationship label entities respectively guide redsandt focus solely relational tokens fine tune bert structure input include sub tree connect entity pair entities type use extract informative vectors shape label embeddings also use attention mechanism instance reduce noise finally represent sentence concatenate relation instance embeddings experiment nyt ten dataset show redsandt capture broader set relations higher confidence achieve state art auc four hundred and twenty-four
pharmaceutical company rely often external source innovation boost discovery research productivity however depth knowledge external innovation may translate successful product launch still require order better understand best leverage innovation ecosystem analyze pre approval publication histories fda approve new molecular entities nmes new biologic entities nbes launch thirteen top research pharma company last decade two thousand and six two thousand and sixteen find academic institutions contribute majority pre approval publications publication subject matter closely align strengths respective innovator find also true candidate drug terminate phase three volume literature molecules substantially less approve drug may suggest approve drug often associate robust dataset provide large number institute collectively result analysis support hypothesis collaborative research innovation environment span across academia industry government highly conducive successful drug approvals
paper provide detail description hitachi jhu system submit third dihard speech diarization challenge system output ensemble result five subsystems two x vector base subsystems two end end neural diarization base subsystems one hybrid subsystem refine system five subsystems become competitive complementary dover lap base system combination achieve diarization error rat one thousand, one hundred and fifty-eight one thousand, four hundred and nine track one full core one thousand, six hundred and ninety-four two thousand and one track two full core respectively result second place task challenge
privacy important concern build statistical model data contain personal information differential privacy offer strong definition privacy use solve several privacy concern dwork et al two thousand and fourteen multiple solutions propose differentially private transformation datasets contain sensitive information however transformation algorithms offer poor utility natural language process nlp task due noise add process paper address issue provide utility preserve differentially private text transformation algorithm use auto encoders algorithm transform text offer robustness attack produce transformations high semantic quality perform well downstream nlp task prove theoretical privacy guarantee algorithm assess privacy leakage membership inference attacksmia shokri et al two thousand and seventeen model train transform data result show propose model perform better mia attack offer lower degradation utility underlie transformation process compare exist baselines
paper propose open source production first production ready speech recognition toolkit call wenet new two pass approach implement unify stream non stream end end e2e speech recognition single model main motivation wenet close gap research production e2e speechrecognition model wenet provide efficient way ship asr applications several real world scenarios main difference advantage open source e2e speech recognition toolkits toolkit new two pass method implement method propose dynamic chunk base attention strategy transformer layer allow arbitrary right context length modify hybrid ctc attention architecture inference latency could easily control change chunk size ctc hypotheses rescored attention decoder get final result experiment aishell one dataset use wenet show model achieve five hundred and three relative character error rate cer reduction non stream asr compare standard non stream transformer model quantification model perform reasonable rtf latency
work present advancements control articulatory speech synthesis engine textitviz pink trombone hand gesture interface translate continuous finger movements wrist flexion continuous speech use vocal tract area function base articulatory speech synthesis use cyberglove ii eighteen sensors capture kinematic information wrist individual finger order control virtual tongue coordinate bend value sensors utilize fit spline tongue model smoothen noisy value outliers consider upper palate fix spline model dynamically move lower surface tongue vocal tract compute 1d area functional value feed pink trombone generate continuous speech sound therefore learn manipulate one wrist finger one learn produce speech sound one hand without need use vocal tract
introduce gem live benchmark natural language generation nlg evaluation metrics measure progress nlg rely constantly evolve ecosystem automate metrics datasets human evaluation standards due move target new model often still evaluate divergent anglo centric corpora well establish flaw metrics disconnect make challenge identify limitations current model opportunities progress address limitation gem provide environment model easily apply wide set task evaluation strategies test regular update benchmark help nlg research become multilingual evolve challenge alongside model paper serve description data organize share task acl two thousand and twenty-one workshop invite entire nlg community participate
article evaluate first experience generate artificial children voice costa rican accent use technique statistical parametric speech synthesis base hide markov model process record voice sample use learn model fundamentals technique use subjective evaluation result perception group people describe result show intelligibility result evaluate isolate word lower voice record group participate children similarly detection age gender speak person significantly affect artificial voice relative record natural voice result show need obtain larger amount data addition become numerical reference future developments result new data process improve result technique
speech emotion recognition vital contributor next generation human computer interaction hci however current exist small scale databases limit development relate research paper present lssed challenge large scale english speech emotion dataset data collect eight hundred and twenty subject simulate real world distribution addition release pre train model base lssed promote development speech emotion recognition also transfer relate downstream task mental health analysis data extremely difficult collect finally experiment show necessity large scale datasets effectiveness pre train model dateset release https githubcom tobefans lssed
many methods exist condition model output task instructions retrieve document user provide explanations feedback rather rely solely examples task input output approach use valuable additional data improve model correctness align learn model human priors meanwhile grow body evidence suggest language model one store large amount knowledge parameters two perform inference task textual input test time result raise possibility task humans explain model task already know could infer paper study circumstances explanations individual data point improve model performance order carefully control important properties data explanations introduce synthetic dataset experiment also make use three exist datasets explanations e snli tacred semeval first give formal framework available model approach explanation data use model input target prior argue promise role explanation data model input propose use retrieval base method show solve synthetic task accuracies upwards ninety-five baselines without explanation data achieve sixty-five accuracy identify properties datasets retrieval base model fail three exist datasets find improvements explanation retrieval draw find synthetic task suggest least one six precondition successful model fail hold datasets code publicly available https githubcom peterbhase explanationroles
word vector representations enable machine encode human language speak language understand process confusion2vec motivate human speech production perception word vector representation encode ambiguities present human speak language addition semantics syntactic information confusion2vec provide robust speak language representation consider inherent human language ambiguities paper propose novel word vector space estimation unsupervised learn lattices output automatic speech recognition asr system encode word confusion2vec vector space constituent subword character n grams show subword encode help better represent acoustic perceptual ambiguities human speak language via information model lattice structure asr output usefulness propose confusion2vec representation evaluate use semantic syntactic acoustic analogy word similarity task also show benefit subword model acoustic ambiguity representation task speak language intent detection result significantly outperform exist word vector representations evaluate erroneous asr output demonstrate confusion2vec subword model eliminate need retrain adapt natural language understand model asr transcripts
propose novel attention base self supervise approach identify claim worthy sentence fake news article important first step automate fact check leverage aboutness headline content use attention mechanism task identify claim use downstream task claim verification release benchmark dataset manually select compel article veracity label associate evidence work go beyond stylistic analysis identify content influence reader belief experiment three datasets show strength model data code available https githubcom architapathak self supervise claimidentification
one important challenge apply deep learn electronic health record ehr complexity multimodal structure ehr usually contain mixture structure cod unstructured free text data sparse irregular longitudinal feature doctor utilize make decisions deep learn regime determine different modality representations fuse together difficult problem often address handcraft model intuition work extend state art neural architecture search nas methods propose multimodal fusion architecture search mufasa simultaneously search across multimodal fusion strategies modality specific architectures first time demonstrate empirically mufasa method outperform establish unimodal nas public ehr data comparable computation cost addition mufasa produce architectures outperform transformer evolve transformer compare baselines ccs diagnosis code prediction discover model improve top five recall eighty-eight ninety-one demonstrate ability generalize ehr task study top architecture depth provide empirical evidence mufasa improvements derive ability customize model data modality find effective fusion strategies
topic detection process determine topics collection textual data one topic detection methods cluster base method assume centroids topics cluster method advantage process data negative representations therefore cluster method allow combination broader representation learn method paper adopt deep learn topic detection use deep autoencoder fuzzy c mean call deep autoencoder base fuzzy c mean dfcm encoder autoencoder perform lower dimensional representation learn fuzzy c mean group lower dimensional representation identify centroids autoencoder decoder transform back centroids original representation interpret topics simulation show dfcm improve coherence score eigenspace base fuzzy c mean efcm comparable lead standard methods ie nonnegative matrix factorization nmf latent dirichlet allocation lda
exist methods vision language learn typically require design task specific architectures objectives task example multi label answer classifier visual question answer region scorer refer expression comprehension language decoder image caption etc alleviate hassle work propose unify framework learn different task single architecture language model objective ie multimodal conditional text generation model learn generate label text base visual textual input seven popular vision language benchmarks include visual question answer refer expression comprehension visual commonsense reason previously model discriminative task generative approach single unify architecture reach comparable performance recent task specific state art vision language model moreover generative approach show better generalization ability question rare answer also show framework allow multi task learn single architecture single set parameters achieve similar performance separately optimize single task model code publicly available https githubcom j min vl t5
data text generation dtg subfield natural language generation aim transcribe structure data natural language descriptions field recently boost use neural base generators exhibit one side great syntactic skills without need hand craft pipelines side quality generate text reflect quality train data realistic settings offer imperfectly align structure text pair consequently state art neural model include mislead statements usually call hallucinations output control phenomenon today major challenge dtg problem address paper previous work deal issue instance level use alignment score table reference pair contrast propose finer grain approach argue hallucinations rather treat word level specifically propose multi branch decoder able leverage word level label learn relevant part train instance label obtain follow simple efficient score procedure base co occurrence analysis dependency parse extensive evaluations via automate metrics human judgment standard wikibio benchmark show accuracy alignment label effectiveness propose multi branch decoder model able reduce control hallucinations keep fluency coherence generate texts experiment degrade version totto show model could successfully use noisy settings
natural language process methods apply variety music study draw connection music language paper expand approach investigate textitchord embeddings apply two case study address two key question one musical information chord embeddings capture two might musical applications benefit analysis show capture similarities chord adhere important relationships describe music theory first case study demonstrate use chord embeddings next chord prediction task yield predictions closely match experience musicians second case study show potential benefit use representations task relate musical stylometrics
purpose develop high throughput multi label annotators body chest abdomen pelvis compute tomography ct report apply across variety abnormalities organs disease state approach use dictionary approach develop rule base algorithms rba extraction disease label radiology text report target three organ systems lungs pleura liver gallbladder kidneys ureters four diseases per system base prevalence dataset expand algorithms beyond pre define keywords attention guide recurrent neural network rnn train use rba extract label classify report positive one diseases normal organ system confound effect model performance evaluate use random initialization pre train embed well different size train datasets performance evaluate use receiver operate characteristic roc area curve auc two thousand, one hundred and fifty-eight manually obtain label result model extract disease label two hundred and sixty-one thousand, two hundred and twenty-nine radiology report one hundred and twelve thousand, five hundred and one unique subject pre train model outperform random initialization across diseases train dataset size reduce performance robust except diseases relatively small number case pre train classification aucs achieve ninety-five five disease outcomes across three organ systems conclusions label extract pipeline able encompass variety case diseases generalize beyond strict rule exceptional accuracy method easily adapt enable automate label hospital scale medical data set train image base disease classifiers
modern mathematics build idea proof translatable formal proof whose validity objective question decidable computer yet practice proof informal may omit many detail agent consider proof valid trust could expand machine verifiable proof proof validity thus become subjective matter lead debate may difficult settle hence concept valid proof well define process establish validity complex multi agent problem introduce sprig protocol sprig allow agents propose verify succinct informative proof decentralize fashion trust establish agents able request detail proof step debate arise must isolate detail proof persist go machine level detail automatically settle structure bounties stake set incentivize agents act good faith propose game theoretic discussion sprig show agents various type information interact lead proof tree appropriate level detail invalidation wrong proof discuss resilience various attack analyze simplify model characterize equilibria compute agents level trust sprig design run smart contract blockchain platform allow anonymous agents participate verification debate contribute information smart contract mediate interactions settle debate guarantee bounties stake pay specify sprig enable new applications issuance bounties open problems creation derivatives market allow agents inject information pertain proof
performance degradation automatic speech recognition asr system commonly observe test acoustic condition different train hence essential make asr systems robust various environmental distortions background noise reverberations multi stream paradigm improve robustness take account handle variety unseen single stream condition inter stream dynamics previously practical two stage train strategy propose within multi stream end end asr stage two formulate multi stream model feature stage one universal feature extractor ufe paper extension introduce two stage augmentation scheme focus mismatch scenarios stage one augmentation aim address single stream input varieties data augmentation techniques stage two time mask apply temporal mask ufe feature randomly select stream simulate diverse stream combinations inference also present adaptive connectionist temporal classification ctc fusion help hierarchical attention mechanisms experiment conduct two datasets dirha ami multi stream scenario compare previous train strategy substantial improvements report relative word error rate reductions two hundred and ninety-seven five hundred and ninety-three across several unseen stream combinations
provide computer systems ability understand generate natural language long challenge engineer recent progress natural language process nlp like gpt three language model release openai make possible extent paper explore possibility rationalise email communication use gpt three first demonstrate technical feasibility understand incoming email generate responses draw literature discipline software engineer well data science second apply knowledge business study software engineer identify ways tackle challenge encounter third argue economic viability solution analyse cost market demand conclude apply gpt three rationalise email communication feasible technically economically
present simple efficient auxiliary loss function automatic speech recognition asr base connectionist temporal classification ctc objective propose objective intermediate ctc loss attach intermediate layer ctc encoder network intermediate ctc loss well regularize ctc train improve performance require small modification code small overhead train inference respectively addition propose combine intermediate ctc loss stochastic depth train apply combination recently propose conformer network evaluate propose method various corpora reach word error rate wer ninety-nine wsj corpus character error rate cer fifty-two aishell one corpus respectively base ctc greedy search without language model especially aishell one task comparable state art asr systems base auto regressive decoder beam search
minimum linear arrangement problem mla consist find map pi vertices graph integers minimize sumuvin epiyou piv tree various algorithms available solve problem polynomial time best know run subquadratic time nv exist variants mla arrangements constrain certain class projectivity iordanskii later hochberg stallmann hs put forward ofn time algorithms solve problem arrangements constrain planar also consider linear arrangements root tree constrain projective gildea temperley gt sketch algorithm projectivity constraint claim run ofn provide justification cost contrast park levy claim gt algorithm run ofn log dmax dmax maximum degree provide sufficient detail correct error hs algorithm planar case show relationship projective case derive algorithm projective case run undoubtlessly ofn time
many households include children use voice personal assistants vpa amazon alexa children benefit rich functionalities vpas third party apps also expose new risk vpa ecosystem eg inappropriate content information collection study risk vpas pose children build natural language process nlp base system automatically interact vpa apps analyze result conversations identify content risky children identify twenty-eight child direct apps risky content maintain grow dataset thirty-one thousand, nine hundred and sixty-six non overlap app behaviors collect three thousand, four hundred and thirty-four alexa apps find suggest although voice apps design children subject policy requirements intensive vet children still vulnerable risky content conduct user study show parent concern vpa apps inappropriate content ask personal information many parent aware risky apps either type exist finally identify new threat users vpa apps confound utterances voice command share multiple apps may user invoke interact different app intend identify four thousand, four hundred and eighty-seven confound utterances include five hundred and eighty-one share child direct non child direct apps
automate speech recognition asr task challenge domain especially low data scenarios audio examples main problem train asr systems data low resource marginalize languages paper present approach mitigate lack train data employ automate curriculum learn combination adversarial bandit approach inspire reinforcement learn goal approach optimize train sequence mini batch rank level difficulty compare asr performance metrics random train sequence discrete curriculum test approach truly low resource language show bandit framework good improvement baseline transfer learn model
paper present novel multi channel speech extraction system simultaneously extract multiple clean individual source mixture noisy reverberant environments propose method build improve multi channel time domain speech separation network employ speaker embeddings identify extract multiple target without label permutation ambiguity efficiently inform speaker information extraction model propose new speaker condition mechanism design additional speaker branch receive external speaker embeddings experiment two channel whamr data show propose system improve nine relative source separation performance strong multi channel baseline increase speech recognition accuracy sixteen relative baseline
whatsapp popular message app world due popularity whatsapp become powerful cheap tool political campaign widely use two thousand and nineteen indian general election use connect voters large scale along campaign report whatsapp also become breed grind harmful speech various protect group religious minorities many message attempt instil fear among population specific minority community accord research inter group conflict fear speech message could last impact might lead real offline violence paper perform first large scale study fear speech across thousands public whatsapp group discuss politics india curate new dataset try characterize fear speech dataset observe users write fear speech message use various events symbols create illusion fear among reader target community build model classify fear speech observe current state art nlp model perform well task fear speech message tend spread faster could potentially go undetected classifiers build detect traditional toxic speech due low toxic nature finally use novel methodology target users facebook ads conduct survey among users whatsapp group understand type users consume share fear speech believe work open new research question different tackle hate speech research community traditionally involve
transformers powerful neural architectures allow integrate different modalities use attention mechanisms paper leverage neural transformer architectures multi channel speech recognition systems spectral spatial information collect different microphones integrate use attention layer multi channel transformer network mainly consist three part channel wise self attention layer csa cross channel attention layer cca multi channel encoder decoder attention layer eda csa cca layer encode contextual relationship within channel across time respectively channel attend output csa cca feed eda layer help decode next token give precede ones experiment show far field house dataset method outperform baseline single channel transformer well super directive neural beamformers cascade transformers
writers poets singers usually create compositions one breath text revisit adjust modify rephrase even multiple time order better convey mean emotions feel author want express amongst noble write arts poetry probably one need elaborate since composition formally respect predefined meter rhyme scheme paper propose framework generate poems repeatedly revisit correct humans order improve overall quality frame problem revise poems context reinforcement learn particular use proximal policy optimization model generate poems scratch learn progressively adjust generate text order match target criterion evaluate approach case match rhyme scheme without information word responsible create rhyme coherently alter poem word propose framework general appropriate reward shape apply text generation problems
widespread use toxic language online platforms increasingly use automate systems leverage advance natural language process automatically flag remove toxic comment however automate systems detect moderate toxic language provide feedback users let alone provide avenue recourse users make actionable change present work recast interactive open source web tool visualize model toxic predictions provide alternative suggestions flag toxic language work also provide users new path recourse use automate moderation tool recast highlight text responsible classify toxicity allow users interactively substitute potentially toxic phrase neutral alternatives examine effect recast via two large scale user evaluations find recast highly effective help users reduce toxicity detect model users also gain stronger understand underlie toxicity criterion use black box model enable transparency recourse addition find users focus optimize language model instead judgement imply incentive goal deploy automate model model cease effective classifiers toxicity compare human annotations open discussion toxicity detection model work work effect future online discourse
modern wake word detection systems usually rely neural network acoustic model transformers recently show superior performance lstm convolutional network various sequence model task better temporal model power however clear whether advantage still hold short range temporal model like wake word detection besides vanilla transformer directly applicable task due non stream nature quadratic time space complexity paper explore performance several variants chunk wise stream transformers tailor wake word detection recently propose lf mmi system include look ahead next chunk gradient stop different positional embed methods add layer dependency chunk experiment mobvoi wake word dataset demonstrate propose transformer model outperform baseline convolution network twenty-five average false rejection rate false alarm rate comparable model size still maintain linear complexity wrt sequence length
two dimensional 2d numerical approach vocal tract vt model afford better balance low computational cost accurate render acoustic wave propagation however require high spatio temporal resolution numerical scheme precise estimation acoustic formants simulation run time expense recently propose new vt acoustic model technique know 25d finite difference time domain 25d fdtd extend exist 2d fdtd approach add tube depth acoustic wave solver work first simulate acoustic output new model show comparable 2d fdtd realistic 3d fem vt model low spatio temporal resolution next radiation model develop include circular baffle around vt head geometry transfer function radiation model analyze use five different vocal tract shape vowel sound e
natural language video description nlvd recently receive strong interest computer vision natural language process nlp multimedia autonomous robotics communities state art sota approach obtain remarkable result test benchmark datasets however approach poorly generalize new datasets addition none exist work focus process input nlvd systems visual textual work present extensive study deal role visual input evaluate respect overall nlp performance achieve perform data augmentation visual component apply common transformations model camera distortions noise light camera position typical real world operative scenarios sne base analysis propose evaluate effect consider transformations overall visual data distribution study consider english subset microsoft research video description msvd dataset use commonly nlvd observe dataset contain relevant amount syntactic semantic errors errors amend manually new version dataset call msvd v2 use experimentation msvd v2 dataset release help gain insight nlvd problem
attention base pre train language model gpt two bring considerable progress end end dialogue model however also present considerable risk task orient dialogue lack knowledge ground diversity address issue introduce modify train objectives language model finetuning employ massive data augmentation via back translation increase diversity train data examine possibilities combine data multiples source improve performance target dataset carefully evaluate contributions human automatic methods model achieve state art performance multiwoz data show competitive performance human evaluation
generative flow diffusion model predominantly train ordinal data example natural image paper introduce two extensions flow diffusion categorical data language image segmentation argmax flow multinomial diffusion argmax flow define composition continuous distribution normalize flow argmax function optimize model learn probabilistic inverse argmax lift categorical data continuous space multinomial diffusion gradually add categorical noise diffusion process generative denoising process learn demonstrate method outperform exist dequantization approach text model model image segmentation map log likelihood
technical growths empower numerous revolutions educational system acquaint technology classroom elevate learn experience nowadays web base learn get much popularity paper describe web base learn effectiveness towards students one prime factor education learn system feedback beneficial learn must use effectively paper work machine learn techniques like logistic regression lr support vector machine svm naive bay nb decision tree dt apply web base learn emphasis give sentiment present feedback students also work two type feature extraction technique fets namely count vector cvr bag word bow term frequency inverse document frequency tf idf vector research study goal propose lr svm nb dt model classify presence student feedback dataset sfb improve accuracy clean dataset feature extraction techniques sfb one significant concern among student sentimental analysis
platforms support online commentary social network news sit increasingly leverage machine learn assist moderation efforts process typically provide feedback author would help contribute accord community guidelines prohibitively time consume human moderators computational approach still nascent work focus model help suggest rephrase toxic comment civil manner inspire recent progress unpaired sequence sequence task self supervise learn model introduce call cae t5 cae t5 employ pre train text text transformer fine tune denoising cyclic auto encoder loss experiment largest toxicity detection dataset date civil comment model generate sentence fluent better preserve initial content compare earlier text style transfer systems compare use several score systems human evaluation
inspire inductive transfer learn computer vision many efforts make train contextualized language model boost performance natural language process task model mostly train large general domain corpora news book wikipediaalthough pre train generic language model well perceive semantic syntactic essence language structure exploit real world domain specific scenario still need practical considerations take account token distribution shift inference time memory simultaneous proficiency multiple task paper focus legal domain present different language model strain general domain corpora best customize multiple legal document review task compare efficiencies respect task performances present practical considerations
pre train representations become crucial many nlp perception task representation learn nlp transition train raw text without human annotations visual vision language representations still rely heavily curated train datasets expensive require expert knowledge vision applications representations mostly learn use datasets explicit class label imagenet openimages vision language popular datasets like conceptual caption mscoco clip involve non trivial data collection clean process costly curation process limit size datasets hence hinder scale train model paper leverage noisy dataset one billion image alt text pair obtain without expensive filter post process step conceptual caption dataset simple dual encoder architecture learn align visual language representations image text pair use contrastive loss show scale corpus make noise lead state art representations even simple learn scheme visual representation achieve strong performance transfer classification task imagenet vtab align visual language representations also set new state art result flickr30k mscoco benchmarks even compare sophisticate cross attention model representations also enable cross modality search complex text text image query
present emotion extraction speech important issue due diverse applications hence become absolutely necessary obtain model take consideration speak style person vocal tract information timbral qualities congenital information regard voice speech production system nonlinear system like real world systems hence need arise model speech information use nonlinear techniques work model articulation system use nonlinear multifractal analysis multifractal spectral width scale exponents reveal essentially complexity associate speech signal take multifractal spectrums well distinguishable low fluctuation region case different emotions source characteristics quantify help different non linear model like multi fractal detrended fluctuation analysis wavelet transform modulus maxima result obtain study give good result emotion cluster
task sequential sentence classification enable semantic structure research paper enhance academic search engines support researchers find explore research literature effectively however previous work investigate potential transfer learn datasets different scientific domains task yet propose uniform deep learn architecture multi task learn improve sequential sentence classification scientific texts across domains exploit train data multiple domains contributions summarise follow one tailor two common transfer learn methods sequential transfer learn multi task learn evaluate performance sequential sentence classification two present multi task model able recognise semantically relate class different datasets thus support manual comparison assessment different annotation scheme three unify approach capable handle datasets contain either abstract full paper without feature engineer demonstrate model train datasets different scientific domains benefit one another use propose multi task learn architecture approach outperform state art three benchmark datasets
already know auditory visual stimulus able convey emotions human mind different extent strength intensity emotional arousal vary depend type stimulus choose study try investigate emotional arousal cross modal scenario involve auditory visual stimulus study source characteristics robust fractal analytic technique call detrended fluctuation analysis dfa 2d analogue use characterize three three standardize audio video signal quantify scale exponent correspond positive negative valence find significant difference scale exponents correspond two different modalities detrended cross correlation analysis dcca also apply decipher degree cross correlation among individual audio visual stimulus first kind study propose novel algorithm emotional arousal classify cross modal scenario use source audio visual signal also attempt correlation
end end e2e speak language understand slu infer semantics directly speech signal without cascade automatic speech recognizer asr natural language understand nlu module however pair utterance record correspond semantics may always available sufficient train e2e slu model real production environment paper propose unify well optimize e2e asr encoder speech pre train language model encoder language transformer decoder unify speech language pre train model slp continually enhance limit label data target domain use conditional mask language model mlm objective thus effectively generate sequence intent slot type slot value give input speech inference experimental result two public corpora show approach e2e slu superior conventional cascade method also outperform present state art approach e2e slu much less pair data
rapid evolution social media fake news become significant social problem address timely manner use manual investigation motivate numerous study automate fake news detection study explore supervise train model different modalities eg text image propagation network news record identify fake news however performance techniques generally drop news record come different domains eg politics entertainment especially domains unseen rarely see train motivation empirically show news record different domains significantly different word usage propagation pattern furthermore due sheer volume unlabelled news record challenge select news record manual label domain coverage label dataset maximize hence work one propose novel framework jointly preserve domain specific cross domain knowledge news record detect fake news different domains two introduce unsupervised technique select set unlabelled informative news record manual label ultimately use train fake news detection model perform well many domains minimize label cost experiment show integration propose fake news model selective annotation approach achieve state art performance cross domain news datasets yield notable improvements rarely appear domains news datasets
software log analysis help maintain health software solutions ensure compliance security exist software systems consist heterogeneous components emit log various format typical solution unify log use manually build parsers laborious instead explore possibility automate parse task employ machine translation mt create tool generate synthetic apache log record use train recurrent neural network base mt model model evaluation real world log show model learn apache log format parse individual log record median relative edit distance actual real world log record mt prediction less equal twenty-eight thus show log parse use mt approach promise
paper propose vara tts non autoregressive non ar text speech tts model use deep variational autoencoder vdvae residual attention mechanism refine textual acoustic alignment layer wisely hierarchical latent variables different temporal resolutions vdvae use query residual attention module leverage coarse global alignment previous attention layer extra input follow attention layer produce refine version alignment amortize burden learn textual acoustic alignment among multiple attention layer outperform use single attention layer robustness utterance level speak speed factor compute jointly train speak speed predictor take mean pool latent variables coarsest layer input determine number acoustic frame inference experimental result show vara tts achieve slightly inferior speech quality ar counterpart tacotron two order magnitude speed inference outperform analogous non ar model bvae tts term speech quality
question text image answer raise distinctive issue ai note discuss problem unanswerable question vqa visual question answer qa visual question answer ai generally
present novel interactive learn protocol enable train request fulfil agents verbally describe activities protocol give rise new family interactive learn algorithms offer complementary advantage traditional algorithms like imitation learn il reinforcement learn rl develop algorithm practically implement protocol employ train agents two challenge request fulfil problems use purely language description feedback empirical result demonstrate strengths algorithm compare rl baselines sample efficient compare il baselines achieve competitive success rat require feedback providers agent specific expertise also provide theoretical guarantee algorithm certain assumptions teacher environment
open domain question answer model directly leverage question answer qa pair close book qa cbqa model qa pair retrievers show promise term speed memory compare conventional model retrieve read text corpora qa pair retrievers also offer interpretable answer high degree control trivial update test time new knowledge however model lack accuracy retrieve read systems substantially less knowledge cover available qa pair relative text corpora like wikipedia facilitate improve qa pair model introduce probably ask question paq large resource 65m automatically generate qa pair introduce new qa pair retriever repaq complement paq find paq preempt cache test question enable repaq match accuracy recent retrieve read model whilst significantly faster use paq train cbqa model outperform comparable baselines five trail repaq fifteen indicate effectiveness explicit retrieval repaq configure size 500mb speed 1k question per second whilst retain high accuracy lastly demonstrate repaq strength selective qa abstain answer likely incorrect enable repaq back expensive state art model lead combine system accurate 2x faster state art model alone
present open predicate query language opql method construct virtual kb vkb train entirely text large knowledge base kbs indispensable wide range industry applications question answer recommendation typically kbs encode world knowledge structure readily accessible form derive laborious human annotation efforts unfortunately extremely high precision kbs inevitably highly incomplete automate methods enrich far inaccurate instead opql construct vkb encode index set relation mention way naturally enable reason train without structure supervision demonstrate opql outperform prior vkb methods two different kb reason task additionally use external memory integrate language model opql lm lead improvements two open domain question answer task
attention popular effective mechanism artificial neural network base sequence sequence model survey paper comprehensive review different attention model use develop automatic speech recognition systems provide paper focus development evolution attention model offline stream speech recognition within recurrent neural network transformer base architectures
attention base encoder decoder aed model achieve promise performance speech recognition however decoder predict text tokens character word autoregressive manner difficult aed model predict tokens parallel make inference speed relatively slow believe encoder already capture whole speech utterance token level relationship implicitly predict token without explicitly autoregressive language model prediction token rely tokens parallel prediction tokens sequence realizable base idea propose non autoregressive speech recognition model call laso listen attentively spell model consist encoder decoder position dependent summarizer pds three modules base basic attention block encoder extract high level representations speech pds use positional encode correspond tokens convert acoustic representations token level representations decoder capture token level relationships self attention mechanism last probability distribution vocabulary compute token position therefore speech recognition formulate position wise classification problem propose cross modal transfer learn method refine semantics large scale pre train language model bert improve performance
second year trec deep learn track goal study ad hoc rank large train data regime document retrieval task passage retrieval task hundreds thousands human label train query evaluate use single shoot trec style evaluation give us picture rank methods work best large data available much comprehensive relevance label small number test query year evidence rankers bert style pretraining outperform rankers large data regime
model parallelism become necessity train modern large scale deep language model work identify new orthogonal dimension exist model parallel approach possible perform pipeline parallelism within single train sequence transformer base language model thank autoregressive property enable fine grain pipeline compare previous work key idea design terapipe high performance token level pipeline parallel algorithm synchronous model parallel train transformer base language model develop novel dynamic program base algorithm calculate optimal pipelining execution scheme give specific model cluster configuration show terapipe speed train 50x largest gpt three model one hundred and seventy-five billion parameters aws cluster forty-eight p316xlarge instance compare state art model parallel methods
domain air traffic control atc systems efforts train practical automatic speech recognition asr model always face problem small train sample since collection annotation speech sample expert domain dependent task work novel train approach base pretraining transfer learn propose address issue improve end end deep learn model develop address specific challenge asr atc domain unsupervised pretraining strategy first propose learn speech representations unlabeled sample certain dataset specifically mask strategy apply improve diversity sample without lose general pattern subsequently transfer learn apply fine tune pretrained optimize baseline model finally achieve supervise asr task virtue common terminology use atc domain transfer learn task regard sub domain adaption task transfer model optimize use joint corpus consist baseline sample new transcribe sample target dataset joint corpus construction strategy enrich size diversity train sample important address issue small transcribe corpus addition speed perturbation apply augment new transcribe sample improve quality speech corpus three real atc datasets use validate propose asr model train strategies experimental result demonstrate asr performance significantly improve three datasets absolute character error rate one third achieve supervise train applicability propose strategies asr approach also validate
change neural architectures foster significant breakthroughs language model computer vision unfortunately novel architectures often require think choice hyperparameters eg learn rate warmup schedule momentum coefficients maintain stability optimizer optimizer instability often result poor parameter initialization avoid architecture specific initialization scheme paper present gradinit automate architecture agnostic method initialize neural network gradinit base simple heuristic variance network layer adjust single step sgd adam result smallest possible loss value adjustment do introduce scalar multiplier variable front parameter block optimize variables use simple numerical scheme gradinit accelerate convergence test performance many convolutional architectures without skip connections even without normalization layer also enable train original post ln transformer machine translation without learn rate warmup wide range learn rat momentum coefficients code available https githubcom zhuchen03 gradinit
despite great success high level synthesis hls tool observe several unresolved challenge one high level abstraction program style hls sometimes conceal optimization opportunities two exist hls tool provide flexible trade pareto solutions among different objectives constraints three actual quality result rtl design hard predict address challenge propose end end framework namelyironman primary goal enable flexible automate design space exploration dse provide either optimal solutions user specify constraints various trade off among different objectives different type resources area latency dse either require tedious manual efforts achievable attain goals exist hls tool three components ironman one gpp highly accurate graph neural network base performance resource predictor two rlmd reinforcement learn base multi objective dse engine explore optimal resource allocation strategy provide pareto solutions different objectives three ct code transformer assist rlmd gpp extract data flow graph original hls c c automatically generate synthesizable code hls directives experimental result show one gpp achieve high prediction accuracy reduce prediction errors hls tool 109x resource utilization 57x time two rlmd obtain optimal pareto solutions outperform genetic algorithm simulate anneal one hundred and twenty-seven one hundred and twenty-nine respectively three ironman able find optimize solutions perfectly match various dsp constraints 254x fewer dsps 6x shorter latency hls tool 400x faster heuristic algorithms hls tool
increase interest develop personalize task orient dialogue systems tdss previous work personalize tdss often assume complete user profile available even users unrealistic one everyone will expose profile due privacy concern two rich user profile may involve large number attribute eg gender age taste paper study personalize tdss without assume user profile complete propose cooperative memory network comemnn novel mechanism gradually enrich user profile dialogues progress simultaneously improve response selection base enrich profile comemnn consist two core modules user profile enrichment upe dialogue response selection drs former enrich incomplete user profile utilize collaborative information neighbor users well current dialogues latter use enrich profile update current user query encode useful information base personalize response user request select conduct extensive experiment personalize babi dialogue benchmark datasets find comemnn able enrich user profile effectively result improvement three hundred and six term response selection accuracy compare state art methods also test robustness comemnn incompleteness user profile randomly discard attribute value user profile even discard fifty attribute value comemnn able match performance best perform baseline without discard user profile show robustness comemnn
biomedical question answer qa gain increase attention capability provide users high quality information vast scientific literature although increase number biomedical qa datasets recently make available resources still rather limit expensive produce transfer learn via pre train language model lms show promise approach leverage exist general purpose knowledge however finetuning large model costly time consume often yield limit benefit adapt specific theme specialise domains covid nineteen literature bootstrap domain adaptation propose simple yet unexplored approach call biomedical entity aware mask bem encourage mask language model learn entity centric knowledge base pivotal entities characterize domain hand employ entities drive lm fine tune result strategy downstream process applicable wide variety mask lms require additional memory components neural architectures experimental result show performance par state art model several biomedical qa datasets
online conversations go many directions turn poorly due antisocial behavior others turn positively benefit research improve online space focus primarily detect reduce antisocial behavior yet know little positive outcomes online conversations increase prosocial outcome simply lack antisocial behavior something examine conversational feature lead prosocial outcomes within online discussions introduce series new theory inspire metrics define prosocial outcomes mentor esteem enhancement use corpus 26m reddit conversations show outcomes forecast initial comment online conversation best model provide relative twenty-four improvement human forecast performance rank conversations predict outcome result indicate platforms use early cue algorithmic rank early conversations prioritize better outcomes
methods materials investigate transferability neural network base de identification sys tems without domain generalization use two domain generalization approach novel approach joint domain learn jdl develop paper state art domain general ization approach common specific decomposition csd literature first measure trans ferability single external source second use two external source evaluate whether domain generalization improve transferability de identification model across domains rep resent different note type institution third use two external source domain train data study whether external source data useful even case sufficient domain train data available finally investigate transferability de identification mod els across institutions result conclusions find transferability single external source give inconsistent sults use additional external source consistently yield f1 score approximately eighty domain generalization always helpful improve transferability also find external source useful even case domain train data available reduce amount need domain train data improve performance transferability across institutions differ note type annotation label external source different institution also useful improve performance
paper multilingual end end framework call atcspeechnet propose tackle issue translate communication speech human readable text air traffic control atc systems propose framework focus integrate multilingual automatic speech recognition asr one model end end paradigm develop convert speech waveform text directly without feature engineer lexicon order make deficiency handcraft feature engineer cause atc challenge speech representation learn srl network propose capture robust discriminative speech representations raw wave self supervise train strategy adopt optimize srl network unlabeled data predict speech feature ie wave feature end end architecture improve complete asr task grapheme base model unit apply address multilingual asr issue face problem small transcribe sample atc domain unsupervised approach mask prediction apply pre train backbone network asr model unlabeled data feature feature process finally integrate srl asr end end multilingual asr framework formulate supervise manner able translate raw wave text one model ie wave text experimental result atcspeech corpus demonstrate propose approach achieve high performance small label corpus less resource consumption four hundred and twenty label error rate fifty-eight hour transcribe corpus compare baseline model propose approach obtain one hundred relative performance improvement enhance increase size transcribe sample
recent work demonstrate reasonable success representation learn hypercomplex space specifically fully connect layer quaternions 4d hypercomplex number replace real value matrix multiplications fully connect layer hamilton products quaternions enjoy parameter save one four learnable parameters achieve comparable performance various applications however one key caveat hypercomplex space exist predefined dimension 4d 8d 16d restrict flexibility model leverage hypercomplex multiplications end propose parameterizing hypercomplex multiplications allow model learn multiplication rule data regardless whether rule predefined result method subsume hamilton product also learn operate arbitrary nd hypercomplex space provide architectural flexibility use arbitrarily one n learnable parameters compare fully connect layer counterpart experiment applications lstm transformer model natural language inference machine translation text style transfer subject verb agreement demonstrate architectural flexibility effectiveness propose approach
conversational machine read systems need interpret natural language rule answer high level question may qualify va health care benefit ask follow clarification question whose answer necessary answer original question however exist work assume rule text provide user question neglect essential retrieval step real scenarios work propose investigate open retrieval set conversational machine read open retrieval set relevant rule texts unknown system need retrieve question relevant evidence collection rule texts answer users high level question accord multiple retrieve rule texts conversational manner propose mudern multi passage discourse aware entailment reason network extract condition rule texts discourse segmentation conduct multi passage entailment reason answer user question directly ask clarification follow question inquiry information create sharc dataset mudern achieve state art performance outperform exist single passage conversational machine read model well new multi passage conversational machine read baseline large margin addition conduct depth analyse provide new insights new set model
deep learn techniques achieve high accuracy computer vision task however accuracy suffer considerably face domain change ie soon use domain differ train domain example road sign recognition model train recognize road sign germany perform poorly countries different road sign standards like china propose contrakg neuro symbolic approach enable cross domain transfer learn base prior knowledge domain context knowledge graph serve medium encode prior knowledge transform dense vector representation via embed methods use five phase train pipeline train deep neural network adjust visual embed space accord domain invariant embed space knowledge graph base contrastive loss function allow neural network incorporate train data different target domains already represent knowledge graph conduct series empirical evaluations determine accuracy approach result show contrakg significantly accurate conventional approach deal domain change transfer learn setup network train domains contrakg achieve twenty-one higher accuracy test source domain fifteen test target domain compare standard approach moreover ten target data train achieve accuracy cross entropy base model train full target data
develop novel approach conformal prediction target task limit data available train conformal prediction identify small set promise output candidates place single prediction guarantee set contain correct answer high probability train data limit however predict set easily become unusably large work obtain substantially tighter prediction set maintain desirable marginal guarantee cast conformal prediction meta learn paradigm exchangeable collections auxiliary task conformalization algorithm simple fast agnostic choice underlie model learn algorithm dataset demonstrate effectiveness approach across number shoot classification regression task natural language process computer vision computational chemistry drug discovery
large majority american adults get least news internet even though many online news products goal inform users news lack scalable reliable tool measure well achieve goal therefore resort noisy proxy metrics eg click rat read time track performance first step towards measure news informedness scale study problem quiz style multiple choice question generation may use survey users knowledge recent news particular formulate problem two sequence sequence task question answer generation qag distractor incorrect answer generation dg introduce newsquizqa first dataset intend quiz style question answer generation contain 20k human write question answer pair 5k news article summaries use dataset propose series novel techniques apply large pre train transformer encoder decoder model namely pegasus t5 task question answer generation distractor generation show model outperform strong baselines use automate metrics human raters provide case study run weekly quiz real world users via google survey platform course two months find users generally find automatically generate question educational enjoyable finally serve research community release newsquizqa dataset
news recommendation call deep insights news article underlie semantics therefore pretrained language model plms like bert roberta may substantially contribute recommendation quality however extremely challenge news recommenders train together big model learn news recommenders require intensive news encode operations whose cost prohibitive plms use news encoder paper propose novel framework speedyfeed efficiently train plms base news recommenders superior quality speedyfeed highlight light weight encode pipeline give rise three major advantage firstly make intermedia result fully reusable train workflow remove repetitive redundant encode operations secondly improve data efficiency train workflow non informative data eliminate encode thirdly save cost leverage simplify news encode compact news representation extensive experiment show speedyfeed lead 100times acceleration train process enable big model train efficiently effectively massive user data well train plms base model speedyfeed demonstrate highly competitive performance outperform state art news recommenders significant margins speedyfeed also model agnostic framework potentially applicable wide spectrum content base recommender systems therefore whole framework open source facilitate progress relate areas
text classifiers core many nlp applications use variety algorithmic approach software paper describe facebook determine give piece text anything hashtag post belong narrow topic covid nineteen fully define topic evaluate classifier performance employ human guide iterations keyword discovery require label data covid nineteen build two set regular expressions one sixty-six languages ninety-nine precision recall fifty two eleven common languages precision ninety recall ninety regular expressions enable low latency query multiple platforms response challenge like covid nineteen fast revisions comparisons dnn classifier show explainable result higher precision recall less overfitting learn apply narrow topic classifiers
medical visual question answer med vqa tremendous potential healthcare however development technology hinder lack publicly available high quality label datasets train evaluation paper present large bilingual dataset slake comprehensive semantic label annotate experience physicians new structural medical knowledge base med vqa besides slake include richer modalities cover human body part currently available dataset show slake use facilitate development evaluation med vqa systems dataset download http wwwmed vqacom slake
event coreference continue challenge problem information extraction absence external knowledge base events coreference become cluster task rely effective representations context event mention appear recent advance contextualized language representations prove successful many task however use event link limit present three part approach one use representations derive pretrained bert model two train neural classifier three drive simple cluster algorithm create coreference chain achieve state art result model two standard datasets within document event coreference task establish new standard third newer dataset
graph convolutional network gcns powerful architecture representation learn make predictions document naturally occur graph eg citation social network data contain sensitive personal information document people profile relationships edge prone privacy leak gcns adversary might reveal original input train model although differential privacy dp offer well found privacy preserve framework gcns pose theoretical practical challenge due train specifics address challenge adapt differentially private gradient base train gcns investigate impact various privacy budget dataset size two optimizers experimental setup five nlp datasets two languages show certain model choices privacy preserve gcns perform ninety non private variants formally guarantee strong privacy measure
interest offensive content identification social media grow substantially recent years previous work deal mostly post level annotations however identify offensive span useful many ways help cop important challenge present mud multilingual system detect offensive span texts mud feature pre train model python api developers user friendly web base interface detail description mud components present paper
web scale repositories products patent scientific paper offer opportunity create automate systems scour millions ideas assist users discover inspirations solutions yet common representation ideas form raw textual descriptions lack important structure require support creative innovation prior work point importance functional structure capture mechanisms purpose inventions allow users discover structural connections across ideas creatively adapt exist technologies however use functional representations either coarse limit expressivity dependent curated knowledge base poor coverage significant manual effort users help bridge gap unlock potential large scale idea mine propose novel computational representation automatically break products fine grain functional facets train model extract facets challenge real world corpus invention descriptions represent product set facet embeddings design similarity metrics support granular match functional facets across ideas use build novel functional search capability enable expressive query mechanisms purpose construct graph capture hierarchical relations purpose mechanisms across entire corpus products use graph help problem solvers explore design space around focal problem view relate problem perspectives empirical user study approach lead significant boost search accuracy quality creative inspirations outperform strong baselines state art representations product texts fifty sixty
two common paradigms end end speech recognition connectionist temporal classification ctc attention base encoder decoder aed model argue latter better suit learn implicit language model test hypothesis measure temporal context sensitivity evaluate model perform constrain amount contextual information audio input find aed model indeed context sensitive gap close add self attention ctc model furthermore two model perform similarly contextual information constrain finally contrast previous research result show ctc model highly competitive wsj librispeech without help external language model
paper propose enhance approach create dedicate corpus algerian arabic newspapers comment develop approach enhance exist approach enrichment available corpus inclusion annotation step follow model annotate train test evaluate revise matter approach corpus create collect comment web sit three well know algerian newspapers three classifiers support vector machine naive bay k nearest neighbor use classification comment positive negative class identify influence stem obtain result classification test without stem obtain result show stem enhance considerably classification due nature algerian comment tie algerian arabic dialect promise result constitute motivation us improve approach especially deal non arabic sentence especially dialectal french ones
reliable automatic evaluation dialogue systems interactive environment long overdue ideal environment evaluate dialog systems also know turing test need involve human interaction usually affordable large scale experiment though researchers attempt use metrics eg perplexity bleu language generation task model base reinforcement learn methods eg self play evaluation automatic evaluation methods show weak correlation actual human evaluation practice bridge gap propose new framework name enigma estimate human evaluation score base recent advance policy evaluation reinforcement learn enigma require handful pre collect experience data therefore involve human interaction target policy evaluation make automatic evaluations feasible importantly enigma model free agnostic behavior policies collect experience data see detail section two significantly alleviate technical difficulties model complex dialogue environments human behaviors experiment show enigma significantly outperform exist methods term correlation human evaluation score
introduce content base document alignment approach cda efficient method align multilingual web document base content create parallel train data machine translation mt systems operate industrial level cda work two step project document web domain share multilingual space ii align base similarity representations space leverage lexical translation model build vector representations use tf idf cda achieve performance comparable state art systems wmt sixteen bilingual document alignment share task benchmark operate multilingual space besides create two web scale datasets examine robustness cda industrial set involve twenty-eight languages millions document experiment show cda robust cost effective significantly superior process large noisy web data ii scale new low resourced languages
recent surge complex attention base deep learn architectures lead extraordinary result various downstream nlp task english language however research resource constrain morphologically rich indian vernacular languages relatively limit paper proffer team sppuakah solution techdofication two thousand and twenty subtask 1f focus coarse grain technical domain identification short text document marathi devanagari script base indian language avail large dataset hand hybrid cnn bilstm attention ensemble model propose competently combine intermediate sentence representations generate convolutional neural network bidirectional long short term memory lead efficient text classification experimental result show propose model outperform various baseline machine learn deep learn model give task give best validation accuracy eight thousand, nine hundred and fifty-seven f1 score eight thousand, eight hundred and seventy-five furthermore solution result best system submission subtask give test accuracy six thousand, four hundred and twenty-six f1 score six thousand, one hundred and fifty-seven transcend performances team well baseline system give organizers share task
ability quickly learn small quantity oftraining data widen range machine learn applications paper propose data efficient image caption model visualgpt leverage linguistic knowledge large pretrained language modellm crucial challenge balance use visual information image prior linguistic knowledge acquire pretraining design novel self resurrect encoder decoder attention mechanism quickly adapt pretrained lm language decoder ona small amount domain train data propose self resurrect activation unit produce sparse activations reduce susceptibility zero gradients train propose model visualgpt one five one mscoco conceptual caption train data condition outperform best baseline model one hundred and eight cider ms coco upto fifty-four cider conceptual caption visual gpt achieve state art result iu x ray medical report generation dataset best knowledge first work improve data efficiency image caption utilize lm pretrained unimodal data code available https githubcom vision cair visualgpt
work present novel pipeline demonstrate achievable combine effort state art approach specifically propose novel r2 d2 rank twice read twice pipeline compose retriever passage reranker extractive reader generative reader simple way combine furthermore previous work often come massive index external document scale order tens gib work present simple approach prune content massive index open domain qa system altogether index os library components fit 6gib docker image retain eight original index content lose three accuracy
product title compression voice mobile commerce well study problem several supervise model propose far however model two major limitations design generate compressions dynamically base cue inference time transfer well different categories test time address shortcomings model title compression meta learn problem ask learn title compression model give one example compression adopt unsupervised approach meta train propose automatic task generation algorithm model observe label generation process outcome four unobserved process create parameterized approximations four latent process get principled way generate random compression rule treat different task main meta learner use two model m1 m2 m1 task agnostic embed generator whose output feed m2 task specific label generator pre train m1 novel unsupervised segment rank prediction task allow us treat m1 segment generator also learn rank segment meta train process experiment sixteen thousand crowd generate meta test examples show unsupervised meta train regime able acquire learn algorithm different task see one example task show model train end end black box meta learner outperform non parametric approach best model obtain f1 score eight thousand, four hundred and twelve beat baseline large margin twenty-five f1 point
knowledge graph embed kge model learn project symbolic entities relations continuous vector space base observe triplets however exist kge model make proper trade graph context model complexity make still far satisfactory paper propose lightweight framework name lightcake context aware kge lightcake explicitly model graph context without introduce redundant trainable parameters use iterative aggregation strategy integrate context information entity relation embeddings generic framework use many simple kge model achieve excellent result finally extensive experiment public benchmarks demonstrate efficiency effectiveness framework
tms pattern recognition approach use finite state machine learn propositional logic represent pattern addition natively interpretable provide competitive accuracy various task paper increase compute power tms propose first order logic base framework herbrand semantics result tm relational take advantage logical structure appear natural language learn rule represent action consequences relate real world outcome logic program horn clauses bring structure view unstructured data close domain question answer first order representation produce 10x compact kbs along increase answer accuracy nine thousand, four hundred and eighty-three nine thousand, nine hundred and forty-eight approach robust towards erroneous miss superfluous information distil aspects text important real world understand
leave wing authoritarianism remain far less understand right wing authoritarianism contribute literature former typically rely survey use new social media analytics approach use list sixty term provide exploratory sketch outline political ideology tribal equalitarianism origins 19th 20th century social philosophy use analyse english corpus google book eight million book scrap unique tweet twitter n two hundred and two thousand, eight hundred and fifty-two conduct series investigations discern extent ideology cohesive amongst public reveal signatures authoritarianism grow popularity though exploratory result provide evidence leave wing authoritarianism two form one uniquely conservative moral signature amongst ostensible liberals use measure moral foundations theory two substantial prevalence anger relative anxiety sadness general result indicate worldview grow popularity increasingly cohesive show signatures authoritarianism
automatic speech recognition asr widely use consumer electronics asr greatly improve utility accessibility technology usually output word sequence without punctuation result ambiguity infer user intent first present transformer base approach punctuation prediction achieve eight improvement iwslt two thousand and twelve ted task beat previous state art one next describe multimodal model learn text audio achieve eight improvement text algorithm internal dataset audio transcriptions finally present approach learn model use contextual dropout allow us handle variable amount future context test time
evaluation journals quality one dominant theme bibliometrics since journals primary venue vet distribution scholarship many criticisms quantify journal impact bibliometrics include disciplinary differences among journals source materials use time windows inclusion work measure skewness citation distributions lariviere sugimoto two thousand and nineteen however despite various attempt remediate newly propose indicators sjr snip eigenfactor walters two thousand and seventeen indicators still remain base citation count fail acknowledge critical differences type citation make whether support dispute work quantify journal impact various program suggest apply encompass citation content analysis within bibliometrics project citation content analysis do scale need order supplement quantitate journal citation analysis scite citation index produce use citation index contain citation type base citation function support dispute mention present initial result statistical characterization citations journals base citation function also present initial result characterize ratio support dispute receive journal potential indicator quality show two interest result ratio support dispute correlate total citations distribution ratio skew show normal distribution conclude proposal future research use citation analysis qualify citation function well implications perform bibliometrics task research evaluation information retrieval use citation function
chatbots ai assistants claim importance today life main reason behind adopt technology connect user understand requirements fulfill achieve cost heavy train data complex learn model work carry propose simple algorithm model implement different field work scope propose model convert human language text computer understandable sql query model require data relate specific field save data space model perform linear computation hence solve computational complexity work also define stag new methodology implement previous method adopt fulfill requirement stage two datasets available online use work atis dataset wikisql work compare computation time among two datasets also compare accuracy paper work basic natural language process task like semantic parse ner part speech tend achieve result simple methods
modern automatic speech recognition asr systems achieve high performance term recognition accuracy however perfectly accurate transcript still challenge read due disfluency filter word errata common speak communication many downstream task human readers rely output asr system therefore errors introduce speaker asr system alike propagate next task pipeline work propose asr post process model aim transform incorrect noisy asr output readable text humans downstream task leverage metadata extraction mde corpus construct task specific dataset study since dataset small propose novel data augmentation method use two stage train strategy fine tune roberta pre train model construct test set model outperform production two step pipeline base post process method large margin one thousand, three hundred and twenty-six readability aware wer ra wer one thousand, seven hundred and fifty-three bleu metrics human evaluation also demonstrate method generate human readable transcripts baseline method
semantic embeddings advance state art countless natural language process task various extensions multimodal domains visual semantic embeddings propose power visual semantic embeddings come distillation enrichment information machine learn inner work poorly understand shortage analysis tool address problem generalize notion probe task visual semantic case end discuss formalization probe task embeddings image caption pair ii define three concrete probe task within general framework iii train classifiers probe properties iv compare various state art embeddings lens propose probe task experiment reveal twelve increase accuracy visual semantic embeddings compare correspond unimodal embeddings suggest text image dimension represent former complement
grow prevalence psychological interventions vital measure rate effectiveness psychological care assist train supervision quality assurance service traditionally quality assessment address human raters evaluate record sessions along specific dimension often codify construct relevant approach domain however cost prohibitive time consume method lead poor feasibility limit use real world settings facilitate process develop automate competency rat tool able process raw record audio session analyze speak say health professional use language provide therapy focus use case specific type psychotherapy call motivational interview system give comprehensive feedback therapist include information dynamics session eg therapist vs client talk time low level psychological language descriptors eg type question ask well high level behavioral construct eg extent therapist understand clients perspective describe platform performance use dataset five thousand record draw deployment real world clinical set use assist train new therapists widespread use automate psychotherapy rat tool may augment experts capabilities provide avenue effective train skill improvement eventually lead positive clinical outcomes
veracity essential key research development innovative products live emotion analysis verification nullify deceit make complainers live chat corroborate message end message apps promote honest conversation users main concept behind emotion artificial intelligent verifier license decline message accountability compare variegate emotions chat app users recognize facial expressions text prediction paper propose emotion intelligent live detector act honest arbiter distribute facial emotions label namely happiness sadness surprise hate separately predict label message text classification finally compare label declare message fraud bonafide emotion detection deploy convolutional neural network cnn use minixception model text prediction select support vector machine svm natural language process probability classifier due receive best accuracy train dataset apply support vector machine svm random forest classifier naive bay classifier logistic regression
automatic speech recognition asr area grow academic commercial interest due high demand applications use provide natural communication method common general purpose asr systems fail applications use domain specific language various strategies use reduce error provide context modify language model post process correction methods article explore use evolutionary process generate optimize context specific application domain well different correction techniques base phonetic distance metrics result show viability genetic algorithm tool context optimization add post process correction base phonetic representations reduce errors recognize speech
recurrent transducer model emerge promise solution speech recognition current next generation smart devices transducer model provide competitive accuracy within reasonable memory footprint alleviate memory capacity constraints devices however model access parameters chip memory every input time step adversely effect device battery life limit usability low power devices address transducer model memory access concern optimize model architecture design novel recurrent cell design demonstrate model energy cost dominate access model weight chip memory ii transducer model architecture pivotal determine number access chip memory model size good proxy iii transducer model optimizations novel recurrent cell reduce chip memory access 45x model size 2x minimal accuracy impact
anomalies failures large computer systems cloud impact large number users communicate compute store information therefore timely accurate anomaly detection necessary reliability security safe operation mitigation losses increasingly important systems recently evolution software industry open several problems need tackle include one address software evolution due software upgrade two solve cold start problem data system interest available paper propose framework anomaly detection log data major troubleshoot source system information end utilize pre train general purpose language model preserve semantics log message map log vector embeddings key idea representations log robust less invariant change log therefore result better generalization anomaly detection model perform several experiment cloud dataset evaluate different language model obtain numerical log representations bert gpt two xl robustness evaluate gradually alter log message simulate change semantics result show propose approach achieve high performance robustness open possibilities future research direction
explainable nlp exnlp increasingly focus collect human annotate explanations explanations use downstream three ways data augmentation improve performance predictive task loss signal train model produce explanations predictions mean evaluate quality model generate explanations review identify three predominant class explanations highlight free text structure organize literature annotate type point learn date give recommendations collect exnlp datasets future
natural language process algorithms make incredible progress still struggle apply distribution examples address challenge underexplored version domain adaptation problem algorithm train several source domains apply examples unseen domain unknown train time particularly examples label unlabeled knowledge target domain available algorithm train time present pada prompt base autoregressive domain adaptation algorithm base t5 model give test example pada first generate unique prompt condition prompt label example respect nlp task prompt sequence unrestricted length consist pre define domain relate feature drfs characterize source domains intuitively prompt unique signature map test example semantic space span source domains experiment three task text classification sequence tag total fourteen multi source adaptation scenarios pada substantially outperform strong baselines
explore use residual network neural attention argument mine particular link prediction method propose make assumptions document argument structure propose residual architecture exploit attention multi task learn make use ensemble evaluate challenge data set consist user generate comment well two datasets consist scientific publications user generate content dataset model outperform state art methods rely domain knowledge scientific literature datasets achieve result comparable yield bert base approach much smaller model size
reference essential part wikipedia statement wikipedia reference paper explore creation collection reference new wikipedia article editors perspective map workflow editors create new article emphasise select reference
paper propose mixspeech simple yet effective data augmentation method base mixup automatic speech recognition asr mixspeech train asr model take weight combination two different speech feature eg mel spectrograms mfcc input recognize text sequence two recognition losses use combination weight apply mixspeech two popular end end speech recognition model include las listen attend spell transformer conduct experiment several low resource datasets include timit wsj hkust experimental result show mixspeech achieve better accuracy baseline model without data augmentation outperform strong data augmentation method specaugment recognition task specifically mixspeech outperform specaugment relative per improvement one hundred and six timit dataset achieve strong wer forty-seven wsj dataset
social media digitalise massive amount users cognitions term timelines emotional content big data open unprecedented opportunities investigate cognitive phenomena like perception personality information diffusion require suitable interpretable frameworks since social media data come users mind worthy candidates challenge cognitive network model cognition give structure mental conceptual associations work outline cognitive network science open new quantitative ways understand cognition online media like reconstruct users semantically emotionally frame events contextual knowledge unavailable machine learn ii investigate conceptual salience prominence knowledge structure social discourse iii study users personality traits like openness experience curiosity creativity language post iv bridge cognitive emotional content social dynamics via multilayer network compare mindsets influencers followers advancements combine cognitive network computer science understand cognitive mechanisms digital real world settings come limitations concern representativeness individual variability data integration aspects discuss along ethical implications manipulate socio cognitive data future read cognitions network social media expose cognitive bias amplify online platforms relevantly inform policy make education market massive complex cognitive trend
quantum natural language process qnlp deal design implementation nlp model intend run quantum hardware paper present result first nlp experiment conduct noisy intermediate scale quantum nisq computers datasets size one hundred sentence exploit formal similarity compositional model mean coecke et al two thousand and ten quantum theory create representations sentence natural map quantum circuit use representations implement successfully train two nlp model solve simple sentence classification task quantum hardware describe detail main principles process challenge experiment way accessible nlp researchers thus pave way practical quantum natural language process
modern natural language process nlp methods employ self supervise pretraining objectives mask language model boost performance various application task pretraining methods frequently extend recurrence adversarial linguistic property mask recently contrastive learn objectives contrastive self supervise train objectives enable recent successes image representation pretraining learn contrast input input pair augment image either similar dissimilar however nlp automate creation text input augmentations still challenge single token invert mean sentence reason contrastive nlp pretraining methods contrast input label pair rather input input pair use methods metric learn energy base model survey summarize recent self supervise supervise contrastive nlp pretraining methods describe use improve language model zero shoot learn pretraining data efficiency specific nlp end task introduce key contrastive learn concepts lessons learn prior research structure work applications cross field relations finally point open challenge future directions contrastive nlp encourage bring contrastive nlp pretraining closer recent successes image representation pretraining
ability perform arithmetic task remarkable trait human intelligence might form critical component complex reason task work investigate surface form number influence sequence sequence language model learn simple arithmetic task addition subtraction across wide range value find number represent surface form strong influence model accuracy particular model fail learn addition five digit number use subwords eg thirty-two struggle learn character level representations eg three two introduce position tokens eg three 10e1 two model learn accurately add subtract number sixty digits conclude modern pretrained language model easily learn arithmetic examples long use proper surface representation result bolster evidence subword tokenizers positional encode components current transformer design might need improvement moreover show regardless number parameters train examples model learn addition rule independent length number see train code reproduce experiment available https githubcom castorini transformers arithmetic
deep neural network achieve state art result various vision language task despite use large train datasets model train iterate single input output pair discard remain examples current prediction work actively exploit train data use information nearest train examples aid prediction train test specifically approach use target similar train example initialize memory state lstm model guide attention mechanisms apply approach image caption sentiment analysis respectively image text retrieval result confirm effectiveness propose approach two task widely use flickr8 imdb datasets code publicly available http githubcom ritaramo retrieval augmentation nn
interspeech two thousand and twenty-one computational paralinguistics challenge address four different problems first time research competition well define condition covid nineteen cough covid nineteen speech sub challenge binary classification covid nineteen infection make base cough sound speech escalation subchallenge three way assessment level escalation dialogue feature primates sub challenge four species vs background need classify describe sub challenge baseline feature extraction classifiers base usual compare boaw feature well deep unsupervised representation learn use audeep toolkit deep feature extraction pre train cnns use deep spectrum toolkit addition add deep end end sequential model partially linguistic analysis
dialogue state track dst pivotal component task orient dialogue systems relatively easy dst model capture belief state short conversations task dst become challenge length dialogue increase due injection distract contexts paper aim improve overall performance dst special focus handle longer dialogues tackle problem three perspectives one model design enable hierarchical slot status prediction two balance train procedure generic task specific language understand three data perturbation enhance model ability handle longer conversations conduct experiment multiwoz benchmark demonstrate effectiveness component via set ablation test especially longer conversations
understand public sentiment perception healthcare crisis essential develop appropriate crisis management techniques study use twitter data predictive model covid nineteen fine grain sentiment analysis opinion people social media pandemic yet do study perform depth fine grain sentiment analysis tweet covid nineteen purpose perform supervise train four transformer language model downstream task multi label classification tweet seven tone class confident anger fear joy sadness analytical tentative achieve lrap label rank average precision score nine thousand, two hundred and sixty-seven roberta train transformer model able correctly predict high accuracy tone tweet leverage model predict tone two hundred thousand tweet covid nineteen perform country wise analysis tone tweet extract useful indicators psychological condition people pandemic
speech sound disorder common communication impairment childhood speech disorder negatively affect live development children clinical intervention often recommend help diagnosis treatment clinicians use instrument methods spectrograms ultrasound tongue image analyse speech articulations analysis methods laborious clinicians therefore grow interest automation paper investigate contribution ultrasound tongue image automatic detection speech articulation errors systems train typically develop child speech augment database adult speech use audio ultrasound evaluation typically develop speech indicate pre train adult speech jointly use ultrasound audio give best result accuracy eight hundred and sixty-nine evaluate disorder speech collect pronunciation score experience speech language therapists focus case velar front glide r score show good inter annotator agreement velar front glide errors automatic velar front error detection best result obtain jointly use ultrasound audio best system correctly detect eight hundred and sixty-six errors identify experience clinicians segment identify errors best system seven hundred and thirty-two match errors identify clinicians result automatic glide detection harder interpret due poor inter annotator agreement appear promise overall find suggest automatic detection speech articulation errors potential integrate ultrasound intervention software automatically quantify progress speech therapy
investigate multi speaker speech recognition ultrasound image tongue video image lips train systems image data modal speech evaluate match test set two speak modes silent modal speech observe silent speech recognition image data underperform compare modal speech recognition likely due speak mode mismatch train test improve silent speech recognition performance use techniques address domain mismatch fmllr unsupervised model adaptation also analyse properties silent modal speech term utterance duration size articulatory space estimate articulatory space compute convex hull tongue splines extract ultrasound tongue image overall observe duration silent speech longer modal speech silent speech cover smaller articulatory space modal speech although two properties statistically significant across speak modes directly correlate word error rat speech recognition
establish good information retrieval system popular mediums entertainment quickly grow area investigation company researchers alike delve domain information retrieval podcast spotify podcast challenge give user query description find relevant short segment give dataset podcast previous techniques include solely classical information retrieval ir techniques perform poorly descriptive query present hand model exclusively rely large neural network tend perform better downside technique considerable amount time compute power require infer result experiment two hybrid model first filter best podcast base user query classical ir technique perform rank shortlist document base detail description use transformer base model
article describe efficient train method online stream attention base encoder decoder aed automatic speech recognition asr systems aed model achieve competitive performance offline scenarios jointly optimize components recently extend online stream framework via model monotonic chunkwise attention mocha however elaborate attention calculation process robust long form speech utterances moreover sequence level train objective time restrict stream encoder nonnegligible delay token emission inference address problems propose ctc synchronous train ctc st ctc alignments leverage reference token boundaries enable mocha model learn optimal monotonic input output alignments formulate purely end end train objective synchronize boundaries mocha ctc ctc model share encoder mocha model enhance encoder representation moreover propose method provide alignment information learn ctc branch attention base decoder therefore ctc st regard self distillation alignment knowledge ctc mocha experimental evaluations variety benchmark datasets show propose method significantly reduce recognition errors emission latency simultaneously especially long form noisy speech also compare ctc st several methods distill alignment knowledge hybrid asr system show ctc st achieve comparable tradeoff accuracy latency without rely external alignment information best mocha system show performance comparable rnn transducer rnn
topic model successful technique text analysis almost twenty years topic model meet deep neural network emerge new increasingly popular research area neural topic model hundred model develop wide range applications neural language understand text generation summarisation language model need summarise research developments discuss open problems future directions paper provide focus yet comprehensive overview neural topic model interest researchers ai community facilitate navigate innovate fast grow research area best knowledge first review focus specific topic
development democratic systems crucial task confirm selection one millennium sustainable development goals unite nations article report progress project aim address barriers one information overload achieve effective direct citizen participation democratic decision make process main objectives explore application natural language process nlp machine learn improve citizens experience digital citizen participation platforms take case study decide madrid consul platform enable citizens post proposals policies would like see adopt city council use nlp machine learn provide new ways suggest citizens proposals might wish support b group citizens interest easily interact c summarise comment post response proposals assist citizens aggregate develop proposals evaluation result confirm nlp machine learn role play address barriers users platforms consul currently experience
well define joke divide neatly setup punchline work humor today talk joke whole idea generate punchlines setup applications conversational humor funny remark usually occur non funny context thus paper base around two core concepts classification generation punchline particular setup base incongruity theory first implement feature base machine learn model classify humor humor generation use neural model merge classical rule base approach neural approach create hybrid model idea behind combine insights gain task setup punchline model thus apply exist text generation approach use compare model human write joke help human evaluators double blind study
many adversarial attack natural language process systems vast majority achieve success modify individual document tokens call textittoken modification attack token modification attack define specific combination fundamental textitcomponents constraint adversary particular search algorithm motivate observation survey exist token modification attack extract components use attack independent framework structure survey result effective categorisation field easy comparison components hope survey guide new researchers field spark research individual attack components
compare traditional visual question answer video ground dialogues require additional reason dialogue context answer question multi turn set previous approach video ground dialogues mostly use dialogue context simple text input without model inherent information flow turn level paper propose novel framework reason paths dialogue context pdc pdc model discover information flow among dialogue turn semantic graph construct base lexical components question answer pdc model learn predict reason paths semantic graph path prediction model predict path current turn past dialogue turn contain additional visual cue answer current question reason model sequentially process visual textual information reason path propagate feature use generate answer experimental result demonstrate effectiveness method provide additional insights model use semantic dependencies dialogue context retrieve visual cue
custom voice specific text speech tts service commercial speech platforms aim adapt source tts model synthesize personal voice target speaker use speech data custom voice present two unique challenge tts adaptation one support diverse customers adaptation model need handle diverse acoustic condition could different source speech data two support large number customers adaptation parameters need small enough target speaker reduce memory usage maintain high voice quality work propose adaspeech adaptive tts system high quality efficient customization new voice design several techniques adaspeech address two challenge custom voice one handle different acoustic condition use two acoustic encoders extract utterance level vector sequence phoneme level vectors target speech train inference extract utterance level vector reference speech use acoustic predictor predict phoneme level vectors two better trade adaptation parameters voice quality introduce conditional layer normalization mel spectrogram decoder adaspeech fine tune part addition speaker embed adaptation pre train source tts model libritts datasets fine tune vctk ljspeech datasets different acoustic condition libritts adaptation data eg twenty sentence one minute speech experiment result show adaspeech achieve much better adaptation quality baseline methods 5k specific parameters speaker demonstrate effectiveness custom voice audio sample available https speechresearchgithubio adaspeech
ability comprehend speech remain date unrivaled deep learn model feat could result brain ability fine tune generic sound representations speech specific process test hypothesis compare five type deep neural network ii human brain responses elicit speak sentence record one hundred and two dutch subject use functional magnetic resonance image fmri network either train acoustics scene classification speech text task base bengali english dutch train similarity model brain assess correlate respective activations optimal linear projection differences brain similarity across network reveal three main result first speech representations brain account random deep network second learn classify acoustic scenes lead deep net increase brain similarity third learn process phonetically relate speech input ie dutch vs english lead deep net reach higher level brain similarity learn process phonetically distant speech input ie dutch vs bengali together result suggest human brain fine tune heavily train auditory hierarchy learn process speech
paper propose omnidirectional representations transformers omninet omninet instead maintain strictly horizontal receptive field token allow attend tokens entire network process also interpret form extreme intensive attention mechanism receptive field entire width depth network end omnidirectional attention learn via meta learner essentially another self attention base model order mitigate computationally expensive cost full receptive field attention leverage efficient self attention model kernel base choromanski et al low rank attention wang et al big bird zaheer et al meta learner extensive experiment conduct autoregressive language model lm1b c4 machine translation long range arena lra image recognition experiment show omninet achieve considerable improvements across task include achieve state art performance lm1b wmt fourteen en de en fr long range arena moreover use omnidirectional representation vision transformers lead significant improvements image recognition task shoot learn fine tune setups
since uptake social media researchers mine online discussions track outbreak evolution specific diseases chronic condition influenza depression broaden set diseases study develop deep learn tool natural language process extract mention virtually medical condition disease unstructured social media text tool hand process reddit twitter post analyze cluster two result co occurrence network condition discover correspond well define categories medical condition result creation first comprehensive taxonomy medical condition automatically derive online discussions validate structure taxonomy official international statistical classification diseases relate health problems icd eleven find match cluster twenty official categories twenty-two base mention taxonomy sub categories reddit post geo reference yous able compute disease specific health score oppose count disease mention count knowledge taxonomy structure find disease specific health score causally link officially report prevalence eighteen condition
introduce gansformer novel efficient type transformer explore task visual generative model network employ bipartite structure enable long range interactions across image maintain computation linearly efficiency readily scale high resolution synthesis iteratively propagate information set latent variables evolve visual feature vice versa support refinement light encourage emergence compositional representations object scenes contrast classic transformer architecture utilize multiplicative integration allow flexible region base modulation thus see generalization successful stylegan network demonstrate model strength robustness careful evaluation range datasets simulate multi object environments rich real world indoor outdoor scenes show achieve state art result term image quality diversity enjoy fast learn better data efficiency qualitative quantitative experiment offer us insight model inner work reveal improve interpretability stronger disentanglement illustrate benefit efficacy approach implementation model available https githubcom dorarad gansformer
current nlp datasets target ambiguity solve native speaker relative ease present cryptonite large scale dataset base cryptic crosswords linguistically complex naturally source example cryptonite cryptic clue short phrase sentence mislead surface read whose solve require disambiguate semantic syntactic phonetic wordplays well world knowledge cryptic clue pose challenge even experience solvers though top tier experts solve almost one hundred accuracy cryptonite challenge task current model fine tune t5 large 470k cryptic clue achieve seventy-six accuracy par accuracy rule base clue solver eighty-six
name conventions important concern large verification project use proof assistants coq particular lemma name use proof engineer effectively understand modify coq code however provide accurate informative lemma name complex task currently often carry manually even lemma name automate use rule base tool generate name may fail adhere important conventions specify explicitly demonstrate toolchain dub roosterize automatically suggest lemma name coq project roosterize leverage neural network model train exist coq code thus avoid manual specification name conventions allow proof engineer conveniently access suggestions roosterize coq project development integrate toolchain popular visual studio code editor evaluation show roosterize substantially outperform strong baselines suggest lemma name useful practice demo video roosterize view https youtube hz5ac7q14rc
contrastive explanations clarify event occur contrast another inherently intuitive humans produce comprehend propose methodology produce contrastive explanations classification model modify representation disregard non contrastive information modify model behavior base contrastive reason method base project model representation latent space capture feature useful model differentiate two potential decisions demonstrate value contrastive explanations analyze two different scenarios use high level abstract concept attribution low level input token span attribution two widely use text classification task specifically produce explanations answer label alternative label aspect input useful aspects input useful particular decisions overall find would light ability label contrastive explanations provide accurate finer grain interpretability model decision
task emotion pair extraction ecpe aim extract potential clause pair emotions correspond cause document unlike well study task emotion extraction ece ecpe require emotion clauses provide annotations previous work ecpe either follow multi stage approach emotion extraction extraction pair do independently use complex architectures resolve limitations paper propose end end model ecpe task due unavailability english language ecpe corpus adapt ntcir thirteen ece corpus establish baseline ecpe task dataset dataset propose method produce significant performance improvements sixty-five increase f1 score multi stage approach achieve comparable performance state art methods
paper present finmatcher system result finsim two thousand and twenty-one share task co locate workshop financial technology web finweb conjunction web conference finsim two share task consist set concept label financial service domain goal find relevant top level concept give set concepts finmatcher system exploit three publicly available knowledge graph namely wordnet wikidata webisalod graph use generate explicit feature well latent feature feed neural classifier predict closest hypernym
activations language transformers like gpt2 show linearly map onto brain activity speech comprehension however nature activations remain largely unknown presumably conflate distinct linguistic class propose taxonomy factorize high dimensional activations language model four combinatorial class lexical compositional syntactic semantic representations introduce statistical method decompose lens gpt2 activations brain activity three hundred and forty-five subject record functional magnetic resonance image fmri listen forty-six hours narrate text result highlight two find first compositional representations recruit widespread cortical network lexical ones encompass bilateral temporal parietal prefrontal cortices second contrary previous claim syntax semantics associate separate modules instead appear share common distribute neural substrate overall study introduce general framework isolate distribute representations linguistic construct generate naturalistic settings
use end end automatic speech recognition e2e asr system real world applications voice activity detection vad system usually need improve performance reduce computational cost discard non speech part audio paper present novel end end e2e multi task learn mtl framework integrate asr vad one model propose system refer long run speech recognizer lr sr learn asr vad jointly two seperate task specific datasets train stage assistance vad asr performance improve connectionist temporal classification ctc loss function leverage vad alignment information inference stage lr sr system remove non speech part low computational cost recognize speech part high robustness experimental result segment speech data show propose mtl framework outperform baseline single task learn stl framework asr task unsegmented speech data find lr sr system outperform baseline asr systems build extra gmm base dnn base voice activity detector
natural language semantics recently seek combine complementary strengths formal distributional approach mean specifically proposals put forward augment formal semantic machinery distributional mean representations thereby introduce notion semantic similarity formal semantics define distributional systems aim incorporate formal notions entailment compositionality however give fundamentally different representational currency underlie formal distributional approach model world versus linguistic co occurrence unification prove extremely difficult define distributional formal semantics integrate distributionality formal semantic system level formal model approach offer probabilistic distribute mean representations also inherently compositional naturally capture fundamental semantic notions quantification entailment furthermore show probabilistic nature representations allow probabilistic inference information theoretic notion information measure term entropy surprisal naturally follow finally illustrate mean representations derive incrementally linguistic input use recurrent neural network model resultant incremental semantic construction procedure intuitively capture key semantic phenomena include negation presupposition anaphoricity
advance deep learn lead promise progress infer graphics program de render computer generate image however current methods explore decode methods lead better inductive bias infer graphics program work first explore effectiveness lstm rnn versus transformer network decoders order independent graphics program since sequence model must choose order object graphics program likelihood train find lstm performance highly sensitive sequence order random order vs pattern base order transformer performance roughly independent sequence order present policy gradient base reinforcement learn approach better inductive bias decoder via multiple diverse reward base graphics program specification render image also explore combination complementary reward achieve state art result two graphics program generation datasets
propose multimodal sing language classification model use audio content textual metadata lrid net propose model take audio signal language probability vector estimate metadata output probabilities target languages optionally lrid net facilitate modality dropouts handle miss modality experiment train several lrid net vary modality dropout configuration test various combinations input modalities experiment result demonstrate use multimodal input improve performance result also suggest adopt modality dropout degrade performance model full modality input enable model handle miss modality case extent
paper explore audiovisual emotion recognition noisy acoustic condition focus speech feature attempt answer follow research question speech emotion recognition perform noisy data ii extend multimodal approach improve accuracy compensate potential performance degradation different noise level present analytical investigation two emotion datasets superimpose noise different signal noise ratios compare three type acoustic feature visual feature incorporate hybrid fusion approach first neural network layer separate modality specific ones follow least one share layer final prediction result show significant performance decrease model train clean audio apply noisy data addition visual feature alleviate effect
milestone improvements bring deep representation learn pre train techniques lead large performance gain across downstream nlp ir vision task multimodal model techniques aim leverage large high quality visio linguistic datasets learn complementary information across image text modalities paper introduce wikipedia base image text wit dataset https githubcom google research datasets wit better facilitate multimodal multilingual learn wit compose curated set three hundred and seventy-six million entity rich image text examples one hundred and fifteen million unique image across one hundred and eight wikipedia languages size enable wit use pretraining dataset multimodal model show apply downstream task image text retrieval wit four main unique advantage first wit largest multimodal dataset number image text examples 3x time write second wit massively multilingual first kind coverage one hundred languages least 12k examples provide cross lingual texts many image third wit represent diverse set concepts real world entities relative previous datasets cover lastly wit provide challenge real world test set empirically illustrate use image text retrieval task example
recent progress audio source separation lead deep learn enable many neural network model provide robust solutions fundamental estimation problem study provide family efficient neural network architectures general purpose audio source separation focus multiple computational aspects hinder application neural network real world scenarios backbone structure convolutional network successive downsampling resampling multi resolution feature sudorm rf well aggregation perform simple one dimensional convolutions mechanism enable model obtain high fidelity signal separation wide variety settings variable number source present limit computational resources eg float point operations memory footprint number parameters latency experiment show sudorm rf model perform comparably even surpass several state art benchmarks significantly higher computational resource requirements causal variation sudorm rf able obtain competitive performance real time speech separation around 10db scale invariant signal distortion ratio improvement si sdri remain twenty time faster real time laptop device
malware classification important challenge problem information security modern malware classification techniques rely machine learn model train feature opcode sequence api call byte n grams among many others research consider opcode feature implement hybrid machine learn techniques engineer feature vectors train hide markov model technique refer hmm2vec word2vec embeddings opcode sequence result hmm2vec word2vec embed vectors use feature classification algorithms specifically consider support vector machine svm k nearest neighbor k nn random forest rf convolutional neural network cnn classifiers conduct substantial experiment variety malware families experiment extend well beyond previous work field
argue attempt build morality machine subject call interpretation problem whereby rule give machine open infinite interpretation ways might morally disapprove interpretation problem artificial intelligence illustration wittgenstein general claim rule contain criteria application use game example attempt define structure normative space argue rule follow within normative space guide value external space represent rule light problem analyse type mistake artificial moral agent could make make suggestions build morality machine get interpret rule give accordance external value explicit moral reason presence structure value adjustment causal power assign agent interaction human agents machine develop virtuous character impact interpretation problem minimise
hierarchical model text classification leak sensitive confidential train data information adversaries due train data memorization use differential privacy model train mitigate leakage attack train model perturb train optimizer however hierarchical text classification multiplicity model architectures available unclear whether architectures yield better trade remain model accuracy model leakage differentially private train perturbation others use white box membership inference attack assess information leakage three widely use neural network architectures hierarchical text classification differential privacy show relatively weak differential privacy guarantee already suffice completely mitigate membership inference attack thus result moderate decrease utility specifically large datasets long texts observe transformer base model achieve overall favorable privacy utility trade smaller datasets shorter texts cnns preferable
two popular type machine translation mt phrase base neural machine translation systems type systems compose multiple complex model layer model layer learn different linguistic aspects source language however model layer clear linguistic phenomena learn information learn phrase base mt systems often clear information learn model question rather information learn especially phrase reorder model neural machine translation systems situation even complex since many case exactly clear information learn learn would light linguistic phenomena capture mt systems analyze behavior important model phrase base neural mt systems consider phrase reorder model phrase base mt systems investigate word inside phrase biggest impact define phrase reorder behavior additionally contribute interpretability neural mt systems study behavior attention model key component neural mt systems closest model functionality phrase reorder model phrase base systems attention model together encoder hide state representations form main components encode source side linguistic information neural mt end also analyze information capture encoder hide state representations neural mt system investigate extent syntactic lexical semantic information source side capture hide state representations different neural mt architectures
train machine understand natural language interact humans elusive essential task field artificial intelligence recent years diversity dialogue systems design rapid development deep learn research especially recent pre train language model among study fundamental yet challenge part dialogue comprehension whose role teach machine read comprehend dialogue context respond paper review previous methods perspective dialogue model summarize characteristics challenge dialogue comprehension contrast plain text read comprehension discuss three typical pattern dialogue model widely use dialogue comprehension task response selection conversation question answer well dialogue relate language model techniques enhance prlms dialogue scenarios finally highlight technical advance recent years point lessons learn empirical analysis prospect towards new frontier research
consider task personalize asr model constrain fix budget record speaker specific utterances give speaker asr model propose method identify sentence speaker utterances likely harder give asr model recognize assume tiny amount speaker specific data learn phoneme level error model help us select sentence show speaker utterances sentence select use error model indeed larger error rat compare speaker utterances randomly select sentence find fine tune asr model sentence utterances select help error model yield higher wer improvements comparison fine tune equal number randomly select sentence utterances thus method provide efficient way collect speaker utterances budget constraints personalize asr model
active research pertain affective phenomenon empathy distress invaluable improve human machine interaction predict intensities complex emotions textual data difficult construct deeply root psychological theory consequently better prediction become imperative take account ancillary factor psychological test score demographic feature underlie latent primitive emotions along text undertone psychological complexity paper proffer team pvg solution wassa two thousand and twenty-one share task predict empathy emotion reaction news stories leverage textual data demographic feature psychological test score intrinsic interdependencies primitive emotions empathy propose multi input multi task framework task empathy score prediction empathy score prediction consider primary task emotion empathy classification consider secondary auxiliary task distress score prediction task system boost addition lexical feature submission rank 1st base average correlation five hundred and forty-five well distress correlation five hundred and seventy-four 2nd empathy pearson correlation five hundred and seventeen
current state art large scale conversational ai intelligent digital assistant systems industry comprise set components automatic speech recognition asr natural language understand nlu systems leverage share nlu ontology eg centralize intent slot schema exist separate skill rout component correctly route request appropriate skill either first party third party application actually execute user request skill rout component need thousands skills either subscribe intent subscribe intent specific contextual condition eg device screen ensure model robustness resilience skill rout component important problem since skills may dynamically change subscription ontology skill rout model deploy production show different model design choices impact model robustness context skill rout state art commercial conversational ai system specifically choices around data augmentation model architecture optimization method show apply data augmentation effective practical way drastically improve model robustness
present multilingual end end text speech framework map byte input spectrograms thus allow arbitrary input script besides strong result forty languages framework demonstrate capabilities adapt various new languages extreme low resource even shoot scenarios merely 40s transcribe record without need lexicon extra corpus auxiliary model particular linguistic expertise retain satisfactory intelligibility naturalness match rich resource model exhaustive comparative study perform reveal potential framework low resource application impact various factor contributory adaptation furthermore propose novel method extract language specific sub network better understand mechanism multilingual model
establish speech affect recognition low resource languages difficult task present transfer learn base speech affect recognition approach pre train model high resource language affect recognition task fine tune parameters low resource language use deep residual network use standard four data set demonstrate transfer learn solve problem data scarcity affect recognition task demonstrate approach efficient achieve seven hundred and forty-seven percent uar ravdess source urdu data set target ablation study identify pre train model add feature information improvement result solve less data issue use knowledge also experiment savee emo db data set set urdu target language four hundred utterances data available approach achieve high unweighted average recall uar compare exist algorithms
intersectional bias bias cause overlap multiple social factor like gender sexuality race disability religion etc recent study show word embed model lade bias intersectional group like african american females etc first step towards tackle intersectional bias identify however discover bias different intersectional group remain challenge task work present wordbias interactive visual tool design explore bias intersectional group encode static word embeddings give pretrained static word embed wordbias compute association word along different group base race age etc visualize use novel interactive interface use case study demonstrate wordbias help uncover bias intersectional group like black muslim males poor females etc encode word embed addition also evaluate tool use qualitative feedback expert interview source code tool publicly access reproducibility githubcom bhavyaghai wordbias
introduce method determine certain capability help achieve accurate model give data view label generate input program compose subroutines different capabilities posit subroutine useful minimal program invoke shorter one since minimum program length uncomputable instead estimate label minimum description length mdl proxy give us theoretically ground method analyze dataset characteristics call method rissanen data analysis rda father mdl showcase applicability wide variety settings nlp range evaluate utility generate subquestions answer question analyze value rationales explanations investigate importance different part speech uncover dataset gender bias
many intellectual endeavor require mathematical problem solve skill remain beyond capabilities computers measure ability machine learn model introduce math new dataset twelve thousand, five hundred challenge competition mathematics problems problem math full step step solution use teach model generate answer derivations explanations facilitate future research increase accuracy math also contribute large auxiliary pretraining dataset help teach model fundamentals mathematics even though able increase accuracy math result show accuracy remain relatively low even enormous transformer model moreover find simply increase budget model parameter count impractical achieve strong mathematical reason scale trend continue scale transformers automatically solve text base task scale currently solve math traction mathematical problem solve likely need new algorithmic advancements broader research community
effectively recognise apply emotions interactions highly desirable trait social robots implicitly understand subject experience different kinds action object world crucial natural hri interactions possibility perform positive action avoid negative action paper utilize nico robot appearance capabilities give nico ability model coherent affective association perceive auditory stimulus temporally asynchronous emotion expression do combine evaluations emotional valence vision language nico use information make decisions extend conversations order accrue affective information representation association coherent primary contribution provide nico robot ability learn affective associations perceive auditory stimulus emotional expression nico able individual subject specific stimuli aid emotion drive dialogue system rectify emotional expression incoherences robot able use information determine subject enjoyment perceive auditory stimuli real hri scenario
design natural language process nlp systems learn human feedback grow research body human loop hitl nlp frameworks continuously integrate human feedback improve model hitl nlp research nascent multifarious solve various nlp problems collect diverse feedback different people apply different methods learn collect feedback present survey hitl nlp work machine learn ml human computer interaction hci communities highlight short yet inspire history thoroughly summarize recent frameworks focus task goals human interactions feedback learn methods finally discuss future directions integrate human feedback nlp development loop
entity link el seek align entity mention text entries knowledge base usually comprise two phase candidate generation candidate rank methods focus latter candidate generation phase set upper bind time accuracy performance overall el system work contribution significant improvement candidate generation thus raise performance threshold el generate candidates include gold entity least candidate set top k propose simple approach efficiently embed mention entity pair dense space bert base bi encoder specifically extend wu et al two thousand and twenty introduce new pool function incorporate entity type side information achieve new state art eight thousand, four hundred and twenty-eight accuracy top fifty candidates zeshel dataset compare previous eight thousand, two hundred and six top sixty-four wu et al two thousand and twenty report result extensive experimentation use propose model see unseen entity datasets result suggest method could useful complement exist el approach
translate languages certain feature mark morphologically one absent mark contextually important test case machine translation translate english mark indefiniteness morphologically yorub use bare nouns mark feature contextually ambiguities arise work perform fine grain analysis smt system compare two nmt systems bilstm transformer translate bare nouns yorub english investigate systems extent identify bns correctly translate compare human translation pattern also analyze type errors model make provide linguistic description errors glean insights evaluate model performance low resource settings translate bare nouns result show transformer model outperform smt bilstm model four categories bilstm outperform smt model three categories smt outperform nmt model one category
paper present agnes first information retrieval system archaeological grey literature allow full text search long archaeological document search system web interface allow archaeology professionals scholars search collection sixty thousand dutch excavation report total three hundred and sixty-one million word conduct user study evaluation agnes search interface small diverse user group evaluation do screen capture think aloud protocol combine user interface feedback questionnaire evaluation cover control use completion pre define task well free use completion freely choose task free use allow us study information need archaeologists well interactions search system conclude one information need archaeologists typically recall orient often require list items answer two users prefer use free text query metadata filter confirm value free text search system three compilation diverse user group contribute collection diverse issue feedback improve system currently refine agnes user interface improve precision archaeological entities agnes help archaeologists answer research question effectively efficiently lead coherent narrative past
article bring together theories multimodal communication computational methods study primary school science diagram combine multiple expressive resources position work within field digital humanities show annotations inform multimodality research target expressive resources discourse structure allow impose structure output computational methods illustrate approach analyse two multimodal diagram corpora first corpus intend support research automatic diagram process whereas second orient towards study diagram mode communication result show multimodally inform annotations bring structural pattern diagram also extend across diagram deal different topics
paper propose parallel computation strategy posterior base lattice expansion algorithm efficient lattice rescoring neural language model lms automatic speech recognition first lattices first pass decode expand propose posterior base lattice expansion algorithm second expand lattice convert minimal list hypotheses cover every arc hypothesis constrain best path least one arc include lattice neural lm score minimal list compute parallel integrate back lattice rescoring stage experiment switchboard dataset show propose rescoring strategy obtain comparable recognition performance generate compact lattices competitive baseline method furthermore parallel rescoring method offer flexibility simplify integration pytorch train neural lms lattice rescoring kaldi
real time image caption along adequate precision main challenge research field present work multiple transformers self attention mechanism mtsm utilize multiple transformers address problems propose algorithm mtsm acquire region proposals use transformer detector detr consequently mtsm achieve self attention mechanism transfer region proposals visual geometrical feature another transformer learn object local global interconnections qualitative quantitative result propose algorithm mtsm show mscoco dataset
paper describe submissions team hwr dravidian language identification dli share task organize vardial two thousand and twenty-one workshop dli train set include sixteen thousand, six hundred and seventy-four youtube comment write roman script contain code mix text english one three south dravidian languages kannada malayalam tamil submit result generate use two model naive bay classifier adaptive language model show obtain competitive performance many language dialect identification task transformer base model widely regard state art number nlp task first submission send close submission track use train set provide share task organisers whereas second submission consider open use pretrained model train external data team attain share second position share task submission base naive bay result reinforce idea deep learn methods competitive language identification relate task many text classification task
since inception transformer base language model lead impressive performance gain across multiple natural language process task arabic current state art result datasets achieve arabert language model notwithstanding recent advancements sarcasm sentiment detection persist challenge task arabic give language rich morphology linguistic disparity dialectal variations paper proffer team sppu aasm submission wanlp arsarcasm share task two thousand and twenty-one center around sarcasm sentiment polarity detection arabic tweet study propose hybrid model combine sentence representations arabert static word vectors train arabic social media corpora propose system achieve f1 sarcastic score sixty-two f pn score seven hundred and fifteen sarcasm sentiment detection task respectively simulation result show propose system outperform multiple exist approach task suggest amalgamation context free context dependent text representations help capture complementary facets word mean arabic system rank second tenth respective sub task sarcasm detection sentiment identification
build agents capable understand language instructions critical effective robust human ai collaboration recent work focus train instruction follow agents via reinforcement learn environments synthetic language however instructions often define long horizon sparse reward task learn policies require many episodes experience end introduce ella exploration learn language abstraction reward shape approach correlate high level instructions simpler low level instructions enrich sparse reward afford environment ella two key elements one termination classifier identify agents complete low level instructions two relevance classifier correlate low level instructions success high level task learn termination classifier offline pair instructions terminal state notably departure prior work language abstraction learn relevance classifier online without rely explicit decomposition high level instructions low level instructions suite complex grid world environments vary instruction complexities reward sparsity ella show significant gain sample efficiency across several environments compare competitive language base reward shape shape methods
combine deep learn conditional probabilistic context free grammars cpcfg create end end system extract structure information complex document class document create cpcfg describe structure information extract conditional probabilities model deep neural network use grammar parse two document directly produce structure record contain extract information system train end end document record pair apply approach extract information scan invoice achieve state art result
semantically meaningful information content perceptual signal usually unevenly distribute speech signal example often many silence speed pronunciation vary considerably work propose slow autoencoders slowaes unsupervised learn high level variable rate discrete representations sequence apply speech show result event base representations automatically grow shrink depend density salient information input signal still allow faithful signal reconstruction develop run length transformers rlts event base representation model use construct language model speech domain able generate grammatical semantically coherent utterances continuations
social scientists show fifty content post news article relation journalistic content study propose classification algorithm categorize user comment post new article base alignment content alignment seek match user comment article base similarity content entities discussion topic propose bertac baert base approach learn jointly article comment embeddings infer relevance class comment introduce ordinal classification loss penalize difference predict true label conduct thorough study show influence propose loss learn process result five representative news outlets show approach learn comment class thirty-six average accuracy improvement compere baselines twenty-five compere ba bc model ba bc approach consist two model aim capture dis jointly formal language news article informal language comment also conduct user study evaluate human label performance understand difficulty classification task user agreement comment article alignment moderate per krippendorff alpha score suggest classification task difficult
deep neural network become dominant approach natural language process nlp however recent years become apparent shortcomings systematicity limit performance data efficiency deep learn nlp shortcomings clearly show lower level artificial task mostly synthetic data abstract pattern best know examples hard problem neural network term generalisation unseen data define relations items equality rather value argue low level problems demonstrate inability neural network learn systematically study propose embed relation base pattern erbp novel way create relational inductive bias encourage learn equality distance base relations abstract pattern erbp base relation base pattern rbp model bayesian prior network weight implement regularisation term otherwise standard network learn erbp easy integrate standard neural network affect learn capacity experiment erbp priors lead almost perfect generalisation learn abstract pattern synthetic noise free sequence erbp also improve natural language model word character level pitch prediction melodies rnn gru lstm network also find improvements complex task learn graph edit distance compositional sentence entailment erbp consistently improve rbp standard network show enable abstract pattern learn contribute performance natural language task
last years show rapid developments field multimodal machine learn combine eg vision text speech position paper explain field use outdated definitions multimodality prove unfit machine learn era propose new task relative definition multimodality context multimodal machine learn focus representations information relevant give machine learn task new definition multimodality aim provide miss foundation multimodal research important component language ground crucial milestone towards nlu
obtain high quality parallel corpora paramount importance train nmt systems however many language pair lack adequate gold standard train data popular approach mine call pseudo parallel sentence pair document two languages paper outline problems current methods propose computationally economical solutions problems demonstrate success novel methods tatoeba similarity search benchmark downstream task namely nmt uncover effect resource relate factor ie much monolingual bilingual data available give language optimal choice bitext mine approach echo problems oft use bucc dataset observe others make code data use experiment publicly available
recent success reinforcement learn rl solve complex task often attribute capacity explore exploit environment train sample efficiency usually issue since cheap simulators available sample data policy hand task orient dialogues usually learn offline data collect use human demonstrations collect diverse demonstrations annotate expensive unfortunately use rl methods train policy data prone issue bias generalization exacerbate stochasticity human response non markovian belief state dialogue management system end propose batch rl framework task orient dialogue policy learn causal aware safe policy improvement caspi method give guarantee dialogue policy performance also learn shape reward accord intentions behind human responses rather mimic demonstration data couple batch rl help overall sample efficiency framework demonstrate effectiveness framework dialogue context text generation end end dialogue task multiwoz20 dataset propose method outperform current state art metrics case end end case method train ten data able perform current state three four evaluation metrics
present neural network base handwritten text recognition htr model architecture train recognize full page handwritten print text without image segmentation base image sequence architecture extract text present image sequence correctly without impose constraints regard orientation layout size text non text also train generate auxiliary markup relate format layout content use character level vocabulary thereby enable language terminology subject model achieve new state art paragraph level recognition iam dataset evaluate scan real world handwritten free form test answer beset curve slant line draw table math chemistry symbols perform better commercially available htr cloud apis deploy production part commercial web application
study problem word level confidence estimation subword base end end e2e model automatic speech recognition asr although prior work propose train auxiliary confidence model asr systems extend naturally systems operate word piece wp vocabulary particular grind truth wp correctness label need train confidence model non unique tokenization word wp cause inaccurate label generate paper propose study two confidence model increase complexity solve problem final model use self attention directly learn word level confidence without need subword tokenization exploit full context feature multiple hypotheses improve confidence accuracy experiment voice search long tail test set show standard metrics eg nce auc rmse improve substantially propose confidence module also enable model selection approach combine device e2e model hybrid model server address rare word recognition problem e2e model
paper outline perspective future ai discuss directions machine model human like intelligence explain developmental evolutionary theories human cognition inform artificial intelligence emphasize role ecological niches sculpt intelligent behavior particular human intelligence fundamentally shape adapt constantly change socio cultural environment argue major limit current work ai miss perspective theoretically experimentally finally discuss promise approach developmental artificial intelligence model infant development multi scale interaction intrinsically motivate learn embodiment fastly change socio cultural environment paper take form interview pierre yves oudeyer mandred eppe organize within context ki kyounstliche intelligenz special issue developmental robotics
standard algorithm levenshtein distance treat trail whitespace letter symbol however humans compare two string implicitly assume string pad infinite trail whitespace inform expectations cost insertion deletion replacement violation expectations result non intuitive edit distance value account specific human intuition naive approach consider possible substrings trail whitespace would yield ofn3 algorithm work provide efficient ofn2 algorithm compute keywords imagine infinite trail whitespace human friendly intuitive edit distance table detection table alignment
recent study demonstrate perceivable improvement performance neural machine translation apply cross lingual language model pretraining lample conneau two thousand and nineteen especially translation language model tlm alleviate need expensive parallel corpora tlm work incorporate translation information dictionaries pretraining process propose novel bilingual dictionary base language model bdlm evaluate bdlm chinese english romanian chinese english obtain five hundred and fifty bleu wmt news19 tiedemann two thousand and twelve two hundred and forty-three bleu wmt20 news commentary outperform vanilla transformer vaswani et al two thousand and seventeen eighty-four bleu twenty-three bleu respectively accord result bdlm also advantage convergence speed predict rare word increase bleu wmt16 romanian english also show effectiveness low resources language translation
propose unsupervised solution authorship verification task utilize pre train deep language model compute new metric call dv distance propose metric measure difference two author compare pre train language model design address problem non comparability authorship verification frequently encounter small cross domain corpora best knowledge paper first one introduce method design non comparability mind grind rather indirectly also one first use deep language model set approach intuitive easy understand interpret visualization experiment four datasets show methods match surpass current state art strong baselines task
rapid development speech assistants adapt server intend automatic speech recognition asr solutions direct device become crucial researchers industry prefer use end end asr systems device speech recognition task end end systems make resource efficient maintain higher quality compare hybrid systems however build end end model require significant amount speech data another challenge task associate speech assistants personalization mainly lie handle vocabulary oov word work consider build effective end end asr system low resource setups high oov rate embody babel turkish babel georgian task address aforementioned problems propose method dynamic acoustic unit augmentation base bpe dropout technique non deterministically tokenizes utterances extend token contexts regularize distribution model recognition unseen word also reduce need optimal subword vocabulary size search technique provide steady improvement regular personalize oov orient speech recognition task least six relative wer twenty-five relative f score additional computational cost owe use bpe dropout monolingual turkish conformer establish competitive result two hundred and twenty-two character error rate cer three hundred and eighty-nine word error rate wer close best publish multilingual system
abolitionist movement nineteenth century unite state remain among significant social political movements us history abolitionist newspapers play crucial role spread information shape public opinion around range issue relate abolition slavery newspapers also serve primary source information movement scholars today result powerful new account movement leaders paper supplement recent qualitative work role women abolition vanguard well role black press quantitative text model approach use diachronic word embeddings identify newspapers tend lead lexical semantic innovations introduction new usages specific word newspapers tend follow aggregate evidence across hundreds change weight network newspapers nod direct edge weight represent frequency newspaper lead adoption lexical semantic change analysis network reveal pathways lexical semantic influence distinguish leaders followers well others stand apart semantic change sweep period specifically find two newspapers edit women provincial freeman lily lead large number semantic change corpus lend additional credence argument multiracial coalition women lead abolitionist movement term think action also contribute additional complexity scholarship seek tease apart relation abolitionist movement women suffrage movement vex racial politics characterize relation
paper present novel natural gradient hessian free nghf optimisation framework neural network train operate efficiently distribute manner rely linear conjugate gradient cg algorithm combine natural gradient ng method local curvature information hessian free hf second order methods solution numerical issue cg allow effective parameter update generate far fewer cg iterations usually use eg five eight instead two hundred work also present novel precondition approach improve progress make individual cg iterations model share parameters although applicable train losses model structure nghf investigate paper lattice base discriminative sequence train hybrid hide markov model acoustic model use standard recurrent neural network long short term memory time delay neural network model output probability calculation automatic speech recognition experiment report multi genre broadcast data set range different acoustic model type experiment show nghf achieve larger word error rate reductions standard stochastic gradient descent adam require order magnitude fewer parameter update
neural language model know high capacity memorization train sample may serious privacy implications train model user content email correspondence differential privacy dp popular choice train model privacy guarantee come significant cost term utility degradation disparate impact subgroups users work introduce two privacy preserve regularization methods train language model enable joint optimization utility privacy one use discriminator two inclusion triplet loss term compare methods dp extensive evaluation show advantage regularizers favorable utility privacy trade faster train ability tap exist optimization approach ensure uniform treatment represent subgroups
relation extraction play important role extract knowledge unstructured text require large amount label corpus reduce expensive annotation efforts semisupervised learn aim leverage label unlabeled data paper review compare three typical methods semi supervise deep learn meta learn self ensembling force consistent perturbations may confront insufficient supervision self train iteratively generate pseudo label retrain enlarge label set dual learn leverage primal task dual task give mutual feedback mean teacher tarvainen valpola two thousand and seventeen lst li et al two thousand and nineteen dualre lin et al two thousand and nineteen elaborate representatives alleviate weakness three methods respectively
one advantage use natural language process nlp technology music fully exploit embed base representation learn paradigm easily handle classical task semantic similarity however recent research reveal poor performance issue common baseline methods semantic similarity nlp show simple embed calibration methods easily promote performance semantic similarity without extra train hence ready use nevertheless still unclear best combination calibration methods much improve performance methods importantly previous work base auto encoder transformer hence performance auto regressive model music unclear render follow open question embed base semantic similarity also apply auto regressive music model poor baseline issue semantic similarity also exist unexplored embed calibration methods better promote performance music semantic similarity paper answer question explore different combination embed calibration auto regressive language model symbolic music result show music semantic similarity work auto regressive model also suffer poor baseline issue like nlp furthermore provide optimal combination embed calibration explore previous research result show combination embed calibration greatly improve music semantic similarity without train task
language inherent compulsory human communication whether express write speak way ensure understand people different regions grow awareness effort include low resourced languages nlp research african languages recently major subject research machine translation text base areas nlp however still little comparable research speech recognition african languages interestingly unique properties african languages affect nlp like diacritical tonal complexities major root speech suggest careful speech interpretation could provide intuition deal linguistic complexities african languages text base nlp okwugb e step towards build speech recognition systems african low resourced languages use fon igbo case study conduct comprehensive linguistic analysis language describe creation end end deep neural network base speech recognition model languages present state art asr model fon well benchmark asr model result igbo linguistic analyse fon igbo provide valuable insights guidance creation speech recognition model african low resourced languages well guide future nlp research fon igbo fon igbo model source code make publicly available
icd cod international standard capture report health condition diagnosis revenue cycle management healthcare manually assign icd cod prone human error due large code vocabulary similarities cod since machine learn base approach require grind truth train data inconsistency among human coders manifest noise label make train evaluation icd classifiers difficult presence noise paper investigate characteristics noise manually assign icd ten cod furthermore propose method train robust icd ten classifiers presence label noise research conclude nature noise systematic exist methods handle label noise assume noise completely random independent feature label case icd data therefore develop new method train robust classifiers presence systematic noise first identify icd ten cod human coders tend misuse confuse base cod locations icd ten hierarchy type cod baseline classifier prediction behaviors develop novel train strategy account noise compare method baseline handle label noise baseline methods assume random noise demonstrate propose method outperform baselines evaluate expert validate label
automatic code summarization free software developers heavy burden manual comment benefit software development maintenance abstract syntax tree ast depict source code syntactic structure incorporate guide generation code summaries however exist ast base methods suffer difficulty train generate inadequate code summaries paper present block wise abstract syntax tree split method basts short fully utilize rich tree form syntax structure asts improve code summarization basts split code method base block dominator tree control flow graph generate split ast code split split ast model tree lstm use pre train strategy capture local non linear syntax encode learn syntax encode combine code encode feed transformer generate high quality code summaries comprehensive experiment benchmarks demonstrate basts significantly outperform state art approach term various evaluation metrics facilitate reproducibility implementation available https githubcom xmudm basts
article describe research claim verification carry use multiple gin base model propose model consist three pair generators discriminators generator discriminator pair responsible generate synthetic data support refute claim claim label theoretical discussion propose model provide validate equilibrium state model propose model apply fever dataset pre train language model use input text data synthetically generate data help gain information help model perform better state art model standard classifiers
paper propose weakly supervise multilingual representation learn framework call cross lingual self train xlst xlst able utilize small amount annotate data high resource languages improve representation learn multilingual un annotate data specifically xlst use supervise train model produce initial representations another model learn maximize similarity output embeddings two model furthermore move average mechanism multi view data augmentation employ experimentally show crucial xlst comprehensive experiment conduct commonvoice corpus evaluate effectiveness xlst result five downstream low resource asr task show multilingual pretrained model achieve relatively one hundred and eighty-six per reduction state art self supervise method leverage additional one hundred hours annotate english data
typical fact verification model use retrieve write evidence verify claim evidence source however often change time information gather revise order adapt model must sensitive subtle differences support evidence present vitaminc benchmark infuse challenge case require fact verification model discern adjust slight factual change collect one hundred thousand wikipedia revisions modify underlie fact leverage revisions together additional synthetically construct ones create total four hundred thousand claim evidence pair unlike previous resources examples vitaminc contrastive ie contain evidence pair nearly identical language content exception one support give claim show train use design increase robustness improve accuracy ten adversarial fact verification six adversarial natural language inference nli moreover structure vitaminc lead us define additional task fact check resources tag relevant word evidence verify claim identify factual revisions provide automatic edit via factually consistent text generation
hate speech social media grow concern automate methods far sub par reliably detect major challenge lie potentially evasive nature hate speech due ambiguity fast evolution natural language tackle introduce vectorisation base crowd source continuously update dictionary hate word propose fuse approach standard word embed order improve classification performance cnn model train test model use merge two establish datasets one hundred and ten thousand, seven hundred and forty-eight tweet total add dictionary enhance input able increase cnn model predictive power increase f1 macro score seven percentage point
paper describe result informal collaboration launch african master machine intelligence ammi june two thousand and twenty series lecture labs speech data collection use mobile applications self supervise representation learn speech small group students lecturer continue work automatic speech recognition asr project three languages wolof ga somali paper describe data collect asr systems develop small amount 1h transcribe speech train data low resource condition pre train model large amount raw speech fundamental efficiency asr systems develop
previous work neural text speech tts address limit speed train inference time robustness difficult synthesis condition expressiveness controllability although several approach resolve limitations attempt solve weaknesses paper propose styler expressive controllable tts framework high speed robust synthesis novel audio text align method call mel calibrator exclude autoregressive decode enable rapid train inference robust synthesis unseen data also disentangle style factor model supervision enlarge controllability synthesize process lead expressive tts top novel noise model pipeline use domain adversarial train residual decode empower noise robust style transfer decompose noise without additional label various experiment demonstrate styler effective speed robustness expressive tts autoregressive decode expressive controllable read style non autoregressive tts synthesis sample experiment result provide via demo page code available publicly
code completion become essential component integrate development environments contemporary code completion methods rely abstract syntax tree ast generate syntactically correct code however fully capture sequential repetitive pattern write code structural information ast alleviate problems propose new code completion approach name ccag model flatten sequence partial ast ast graph ccag use propose ast graph attention block capture different dependencies ast graph representation learn code completion sub task code completion optimize via multi task learn ccag task balance automatically achieve use uncertainty without need tune task weight experimental result show ccag superior performance state art approach able provide intelligent code completion
pretrained use large amount data autoregressive language model able generate high quality sequence however model perform well hard lexical constraints lack fine control content generation process progressive insertion base transformers overcome limitation efficiently generate sequence parallel give input tokens constraint transformers however may fail support hard lexical constraints generation process likely terminate prematurely paper analyse early termination problems propose entity constrain insertion transformer enconter new insertion transformer address pitfall without compromise much generation efficiency introduce new train strategy consider predefined hard lexical constraints eg entities include generate sequence experiment show enconter outperform baseline model several performance metrics render suitable practical applications code available https githubcom larc cmu smu enconter
fake news severe problem social media paper present empirical study visual textual multimodal model task claim claim check worthiness conspiracy detection relate fake news detection recent work suggest image influential text often appear alongside fake text end several multimodal model propose recent years use image along text detect fake news social media sit like twitter however role image well understand claim detection specifically use transformer base textual multimodal model investigate state art model image text transformer base multimodal information four different datasets across two languages understand role image task claim conspiracy detection
investigate set techniques rnn transducers rnn ts instrumental lower word error rate three different task switchboard three hundred hours conversational spanish seven hundred and eighty hours conversational italian nine hundred hours techniques pertain architectural change speaker adaptation language model fusion model combination general train recipe first introduce novel multiplicative integration encoder prediction network vectors joint network oppose additive second discuss applicability vector speaker adaptation rnn ts conjunction data perturbation third explore effectiveness recently propose density ratio language model fusion task last least describe components train recipe effect recognition performance report fifty-nine one hundred and twenty-five word error rate switchboard callhome test set nist hub5 two thousand evaluation one hundred and twenty-seven wer mozilla commonvoice italian test set
article propose novel method estimate frequency distribution linguistic variables control statistical non independence due share ancestry unlike previous approach technique use available data language families large small well isolate control different degrees relatedness continuous scale estimate data approach involve three step first distributions phylogenies infer lexical data second phylogenies use part statistical model statistically estimate transition rat parameter state finally long term equilibrium result markov process compute case study investigate series potential word order correlations across languages world
various type pretraining architectures include autoregressive model eg gpt autoencoding model eg bert encoder decoder model eg t5 hand nlp task different nature three main categories classification unconditional generation conditional generation however none pretraining frameworks perform best task introduce inconvenience model development selection propose novel pretraining framework glm general language model address challenge compare previous work architecture three major benefit one perform well classification unconditional generation conditional generation task one single pretrained model two outperform bert like model classification due improve pretrain finetune consistency three naturally handle variable length blank fill crucial many downstream task empirically glm substantially outperform bert superglue natural language understand benchmark amount pre train data moreover glm 125x parameters bert large achieve best performance nlu conditional unconditional generation time demonstrate generalizability different downstream task
open ai publish clip model contrastive language image pre train multi modal neural network provide accessible model combine read visual recognition network offer novel ways probe dual abilities read text classify visual object paper demonstrate several new categories adversarial attack span basic typographical conceptual iconographic input generate fool model make false absurd classifications demonstrate contradictory text image signal confuse model choose false visual options like previous author show example clip model tend read first look later phenomenon describe read believe
direct decode task orient dialogue know suffer explain away effect manifest model prefer short generic responses argue use bay theorem factorize dialogue task two model distribution context give response prior response approach instantiation noisy channel model mitigate explain away effect allow principled incorporation large pretrained model response prior present extensive experiment show noisy channel model decode better responses compare direct decode two stage pretraining strategy employ open domain task orient dialogue data improve randomly initialize model
code comment help program comprehension consider important artifacts help developers software maintenance however comment mostly miss outdated specially complex software project result several automatic comment generation model develop solution recent model explore integration external knowledge resources unify model language class diagram improve generate comment paper propose api2com model leverage application program interface documentations api docs knowledge resource comment generation api docs include description methods detail therefore provide better context generate comment api docs use along code snippets abstract syntax tree model apply model large java dataset one hundred and thirty thousand methods evaluate use transformer rnn base architectures interestingly api docs use performance increase negligible therefore run different experiment reason result methods contain one api add api docs improve result four bleu score average bleu score automatic evaluation metric use machine translation however number apis use method increase performance model generate comment decrease due long documentations use input result confirm api docs useful generate better comment new techniques require identify informative ones method rather use documentations simultaneously
large scale pre train language model demonstrate strong capabilities generate realistic text however remain challenge control generation result previous approach prompt far sufficient limit usage language model tackle challenge propose innovative method inverse prompt better control text generation core idea inverse prompt use generate text inversely predict prompt beam search enhance relevance prompt generate text provide better controllability empirically pre train large scale chinese language model perform systematic study use human evaluation task open domain poem generation open domain long form question answer result show propose method substantially outperform baselines generation quality close human performance task narrators try poem generation demo https pretrainaminercn apps poetryhtml qa demo find https pretrainaminercn app qa researchers code provide https githubcom thudm inverseprompting
stack overflow one popular program community base question answer pcqa websites attract users recent years users raise inquire question stack overflow provide relate question help solve problems although many approach base deep learn automatically predict relatedness question approach limit since interaction information two question may lose paper adopt deep learn technique propose attention base sentence pair interaction model asim predict relatedness question stack overflow automatically adopt attention mechanism capture semantic interaction information question besides pre train release word embeddings specific software engineer domain task may also help relate task experiment result demonstrate asim make significant improvement baseline approach precision recall micro f1 evaluation metrics achieve state art performance task model also perform well duplicate question detection task askubuntu similar different task prove generalization robustness
cardiac signal electrocardiogram convey significant amount information health status patient typically summarize clinician form clinical report cumbersome process prone errors streamline routine process propose deep neural network capable caption cardiac signal receive cardiac signal input generate clinical report output extend generate multilingual report end create make publicly available multilingual clinical report dataset absence sufficient label data deep neural network benefit warm start pre train procedure parameters first learn arbitrary task propose task form discriminative multilingual pre train tokens clinical report randomly replace languages network task predict language tokens show method perform par state art pre train methods mlm electra marge simultaneously generate diverse plausible clinical report also demonstrate multilingual model outperform monolingual counterparts informally term beneficial phenomenon bless multilinguality
embeddings word concepts capture syntactic semantic regularities language however see limit use tool study characteristics different corpora relate one another introduce textessence interactive system design enable comparative analysis corpora use embeddings textessence include visual neighbor base similarity base modes embed analysis lightweight web base interface propose new measure embed confidence base nearest neighborhood overlap assist identify high quality embeddings corpus analysis case study covid nineteen scientific literature illustrate utility system textessence available https githubcom drgriffis text essence
introduce grey box adversarial attack defence framework sentiment classification address issue differentiability label preservation input reconstruction adversarial attack defence one unify framework result show train attack model capable generate high quality adversarial examples substantially faster one order magnitude less time state art attack methods examples also preserve original sentiment accord human evaluation additionally framework produce improve classifier robust defend multiple adversarial attack methods code available https githubcom ibm aur nlp adv def text dist
machine learn task evaluate model give data population measure population level metric fsm examples evaluation metric f include precision recall binary recognition f1 score multi class classification bleu metric language generation hand model train optimize sample level loss gstm learn step st subset aka mini batch popular choices g include cross entropy loss dice loss sentence level bleu score fundamental assumption behind paradigm mean value sample level loss g average possible sample effectively represent population level metric f task mathbbe gstm approx fsm paper systematically investigate assumption several nlp task show theoretically experimentally popular design sample level loss g may inconsistent true population level metric f task model train optimize former substantially sub optimal latter phenomenon call simpson bias due deep connections classic paradox know simpson reversal paradox statistics social sciences
early january two thousand and twenty china report first case new coronavirus sars cov two city wuhan unreliable fully accurate information start spread faster virus alongside pandemic people experience parallel infodemic ie overabundance information mislead even harmful widely spread around globe although social media increasingly use information source web search engines like google yahoo still represent powerful trustworthy resource find information web due capability capture largest amount information help users quickly identify relevant useful although always reliable result search query study aim detect potential mislead fake content capture analyse textual information flow search engines use real world dataset associate recent covid nineteen pandemic first apply sample techniques class imbalance use exist machine learn algorithms classification reliable news extract lexical host base feature associate uniform resource locators urls news article show propose methods common phishing malicious urls detection improve efficiency performance classifiers base find think usage textual urls feature improve effectiveness fake news detection methods
employ paraphrase tool conceal plagiarize text severe threat academic integrity enable detection machine paraphrase text evaluate effectiveness five pre train word embed model combine machine learn classifiers state art neural language model analyze preprints research paper graduation theses wikipedia article paraphrase use different configurations tool spinbot spinnerchief best perform technique longformer achieve average f1 score eight thousand and ninety-nine f19968 spinbot f17164 spinnerchief case human evaluators achieve f1784 spinbot f1656 spinnerchief case show automate classification alleviate shortcomings widely use text match systems turnitin plagscan facilitate future research data code two web applications showcasing contributions openly available
review describe application one popular deep learn base language model bert paper describe mechanism operation model main areas application task text analytics comparisons similar model task well description proprietary model prepare review data several dozen original scientific article publish past years attract attention scientific community systematize survey useful students researchers want get acquaint latest advance field natural language text analysis
recently pre train language model lms achieve strong performance fine tune difficult benchmarks like superglue however performance suffer label examples available fine tune pattern exploit train pet recent approach leverage pattern shoot learn however pet use task specific unlabeled data paper focus shoot learn without unlabeled data introduce adapet modify pet objective provide denser supervision fine tune result adapet outperform pet superglue without task specific unlabeled data code find https githubcom rrmenon10 adapet
controllable image caption cic generate image descriptions follow designate control signal receive unprecedented attention last years emulate human ability control caption generation current cic study focus exclusively control signal concern objective properties content interest descriptive pattern however argue almost exist objective control signal overlook two indispensable characteristics ideal control signal one event compatible visual content refer single sentence compatible describe activity two sample suitable control signal suitable specific image sample end propose new control signal cic verb specific semantic roles vsr vsr consist verb semantic roles represent target activity roles entities involve activity give designate vsr first train ground semantic role label gsrl model identify grind entities role propose semantic structure planner ssp learn human like descriptive semantic structure lastly use role shift caption model generate caption extensive experiment ablations demonstrate framework achieve better controllability several strong baselines two challenge cic benchmarks besides generate multi level diverse caption easily code available https githubcom mad red vsr guide cic
rise language model bert allow high quality text paraphrase problem academic integrity difficult differentiate original machine generate content propose benchmark consist paraphrase article use recent language model rely transformer architecture contribution foster future research paraphrase detection systems offer large collection align original paraphrase document study regard structure classification experiment state art systems make find publicly available
present mgenre sequence sequence system multilingual entity link mel problem task resolve language specific mention multilingual knowledge base kb mention give language mgenre predict name target entity leave right token token autoregressive fashion autoregressive formulation allow us effectively cross encode mention string entity name capture interactions standard dot product mention entity vectors also enable fast search within large kb even mention appear mention table need large scale vector indices prior mel work use single representation entity match entity name many languages possible allow exploit language connections source input target name moreover zero shoot set languages train data mgenre treat target language latent variable marginalize prediction time lead fifty improvements average accuracy show efficacy approach extensive evaluation include experiment three popular mel benchmarks mgenre establish new state art result code pre train model https githubcom facebookresearch genre
pangea panoramic graph environment annotation toolkit lightweight toolkit collect speech text annotations photo realistic 3d environments pangea immerse annotators web base simulation allow move around easily speak listen include database cloud storage integration plus utilities automatically align record speech manual transcriptions virtual pose annotators box pangea support two task collect navigation instructions navigation instruction follow could easily adapt annotate walk tour find label landmarks object similar task share best practice learn use pangea twenty thousand hour annotation effort collect room across room dataset hope open source annotation toolkit insights expedite future data collection efforts spur innovation kinds ground language task environments support
customer support via chat require agents resolve customer query minimum wait time maximum customer satisfaction give agents well customers vary level literacy overall quality responses provide agents tend poor predefined use static responses lead customer detraction customers tend feel longer interact human hence vital variations static responses reduce monotonicity responses however maintain list variations expensive give conversation context agent response propose unsupervised frame work generate contextual paraphrase use autoregressive model also propose automate metric base semantic similarity textual entailment expression diversity fluency evaluate quality contextual paraphrase demonstrate performance improvement reinforcement learn rl fine tune use automate metric reward function
decompilation procedure transform binary program high level representation source code human analysts examine modern decompilers reconstruct recover much information discard compilation infer variable name still extremely difficult inspire recent advance natural language process propose novel solution infer variable name decompiled code base mask language model byte pair encode neural architectures transformers bert solution take textitraw decompiler output less semantically meaningful code input enrich use propose textitfinetuning technique constrain mask language model use constrain mask language model introduce challenge predict number mask tokens original variable name address textitcount token prediction challenge post process algorithm compare state art approach train varbert model simpler much better performance evaluate model exist large scale data set one hundred and sixty-four thousand, six hundred and thirty-two binaries show predict variable name identical ones present original source code eight thousand, four hundred and fifteen time
paper propose beginnings formal framework model narrative textitqua narrative framework afford ability discuss key qualities stories communication include flow information narrator reader evolution reader story model time reader uncertainty demonstrate applicability computational narratology give explicit algorithms measure accuracy information convey reader two novel measurements story coherence
cognitive grammar suggest acquisition language grammar ground within visual structure grammar essential representation natural language also exist ubiquitously vision represent hierarchical part whole structure work study ground grammar induction vision language joint learn framework specifically present vlgrammar method use compound probabilistic context free grammars compound pcfgs induce language grammar image grammar simultaneously propose novel contrastive learn framework guide joint learn modules provide benchmark ground grammar induction task collect large scale dataset textscpartit contain human write sentence describe part level semantics 3d object experiment textscpartit dataset show vlgrammar outperform baselines image grammar induction language grammar induction learn vlgrammar naturally benefit relate downstream task specifically improve image unsupervised cluster accuracy thirty perform well image retrieval text retrieval notably induce grammar show superior generalizability easily generalize unseen categories
nlp systems rarely give special consideration number find text starkly contrast consensus neuroscience brain number represent differently word arrange recent nlp work numeracy comprehensive taxonomy task methods break subjective notion numeracy seven subtasks arrange along two dimension granularity exact vs approximate units abstract vs ground analyze myriad representational choices make eighteen previously publish number encoders decoders synthesize best practice represent number text articulate vision holistic numeracy nlp comprise design trade off unify evaluation
mixture expert moe present strong potential enlarge size language model trillions parameters however train trillion scale moe require algorithm system co design well tune high performance distribute train system unfortunately exist platform meet requirements strongly depend google hardware tpu software mesh tensorflow stack open available public especially gpu pytorch communities paper present fastmoe distribute moe train system base pytorch common accelerators system provide hierarchical interface flexible model design easy adaption different applications transformer xl megatron lm different direct implementation moe model use pytorch train speed highly optimize fastmoe sophisticate high performance acceleration skills system support place different experts multiple gpus across multiple nod enable enlarge number experts linearly number gpus source fastmoe available https githubcom laekov fastmoe apache two license
adversarial train end end e2e asr systems use generative adversarial network gin recently explore low resource asr corpora gans help learn true data representation two player min max game however train e2e asr model use large asr corpus gin framework never explore might take excessively long time due high variance gradient update face convergence issue paper introduce novel framework fine tune pre train asr model use gin objective asr model act generator discriminator try distinguish asr output real data since asr model pre train hypothesize asr model output soft distribution vectors help get higher score discriminator make task discriminator harder within gin framework turn improve performance asr model fine tune stage pre train asr model fine tune adversarially discriminator use additional adversarial loss experiment full librispeech dataset show propose approach outperform baselines conventional gin base adversarial model
descriptive code comment essential support code comprehension maintenance propose task automatically generate comment override methods formulate novel framework accommodate unique contextual linguistic reason require perform task approach feature one incorporate context class hierarchy two condition learn latent representations specificity generate comment capture specialize behavior override method three unlikelihood train discourage predictions conform invariant characteristics comment correspond override method experiment show propose approach able generate comment override methods higher quality compare prevail comment generation techniques
legal system judgment consistency regard one important manifestations fairness however due complexity factual elements impact sentence real world scenarios work do quantitatively measure judgment consistency towards real world data paper propose evaluation metric judgment inconsistency legal inconsistency coefficient linco aim evaluate inconsistency data group divide specific feature eg gender region race propose simulate judge different group legal judgment prediction ljp model measure judicial inconsistency disagreement judgment result give ljp model train different group experimental result synthetic data verify effectiveness linco employ linco explore inconsistency real case come follow observations one regional gender inconsistency exist legal system gender inconsistency much less regional inconsistency two level regional inconsistency vary little across different time periods three general judicial inconsistency negatively correlate severity criminal charge besides use linco evaluate performance several de bias methods adversarial learn find mechanisms effectively help ljp model avoid suffer data bias
recent coronavirus disease two thousand and nineteen covid nineteen outbreak microblogging service twitter widely use share opinions reactions events italy one first european countries severely affect outbreak establish lockdown stay home order potentially lead country reputation damage resort sentiment analysis investigate change opinions italy report twitter covid nineteen outbreak use different lexicons base methods find breakpoint correspond date first establish case covid nineteen italy cause relevant change sentiment score use proxy country reputation next demonstrate sentiment score italy strongly associate level ftse mib index italian stock exchange main index serve early detection signal change value ftse mib finally make content base classification tweet positive negative use two machine learn classifiers validate assign polarity tweet post outbreak
end end model auto regressive decoders show impressive result automatic speech recognition asr model formulate sequence level probability product conditional probabilities individual tokens give histories however performance locally normalise model sub optimal factor exposure bias consequently model distribution differ underlie data distribution paper residual energy base model r ebm propose complement auto regressive asr model close gap two distributions meanwhile r ebms also regard utterance level confidence estimators may benefit many downstream task experiment 100hr librispeech dataset show r ebms reduce word error rat wers eighty-two sixty-seven improve areas precision recall curve confidence score one hundred and twenty-six two hundred and eighty-four test clean test set furthermore state art model use self supervise learn wav2vec twenty r ebms still significantly improve wer confidence estimation performance
gin base neural vocoders parallel wavegan melgan attract great interest due lightweight parallel structure enable generate high fidelity waveform real time manner paper inspire relativistic gin introduce novel variant lsgan framework context waveform synthesis name pointwise relativistic lsgan prlsgan approach take truism score distribution consideration combine original mse loss propose pointwise relative discrepancy loss increase difficulty generator fool discriminator lead improve generation quality moreover prlsgan general purpose framework combine gin base neural vocoder enhance generation quality experiment show consistent performance boost base parallel wavegan melgan demonstrate effectiveness strong generalization ability propose prlsgan neural vocoders
despite feature real time decode monotonic multihead attention mma show comparable performance state art offline methods machine translation automatic speech recognition asr task however latency mma still major issue asr combine technique reduce test latency inference time head synchronous beam search decode force non activate head activate small fix delay first head activation paper remove discrepancy train test phase consider train mma interactions across multiple head occur test time specifically derive expect alignments monotonic attention consider boundaries head reflect learn process validate propose method two standard benchmark datasets asr show approach mma mutually constrain head train stage provide better performance baselines
theory discrete stochastic systems initiate work shannon von neumann shannon consider memory less communication channel generalization introduce state von neumann study synthesis reliable systems unreliable components fundamental work rabin scott deterministic finite state automata lead two generalizations first generalization transition function conditional distributions study carlyle starke turn lead generalization time discrete markov chain chain govern one transition probability matrix second generalization regular set introduce stochastic automata describe rabin stochastic automata well investigate report provide short introduction stochastic automata base valuable book claus include basic topics theory stochastic automata equivalence minimization reduction cover observability determinism stochastic versions mealy moore automata study finally stochastic language acceptors consider generalization nondeterministic finite state acceptors
train multi speaker text speech tts model scratch computationally expensive add new speakers dataset require model train naive solution sequential fine tune model new speakers model poor performance older speakers phenomenon know catastrophic forget paper look tts model continual learn perspective goal add new speakers without forget previous speakers therefore first propose experimental setup show serial fine tune new speakers result forget previous speakers exploit two well know techniques continual learn namely experience replay weight regularization reveal one mitigate effect degradation speech synthesis diversity sequential train new speakers use methods finally present simple extension improve result extreme setups
languages insufficient resources train speech recognition systems query example speak term detection qbe std offer way access untranscribed speech corpus help identify regions speak query term occur yet retrieval performance poor query corpus speak different speakers produce different record condition use data select variety speakers record condition seven australian aboriginal languages regional variety dutch endanger vulnerable evaluate whether qbe std performance languages could improve leverage representations extract pre train english wav2vec twenty model compare use mel frequency cepstral coefficients bottleneck feature find representations middle layer wav2vec twenty transformer offer large gain task performance fifty-six eighty-six feature extract use pre train english model yield improve detection evaluation languages better detection performance associate evaluation language phonological similarity english
large pre train transformer base model perform well across wide variety nlp task recent research suggest key may lie multi head attention mechanism ability learn represent linguistic information understand model represent syntactic semantic knowledge vital investigate succeed fail learn improve present dodrio open source interactive visualization tool help nlp researchers practitioners analyze attention mechanisms transformer base model linguistic knowledge dodrio tightly integrate overview summarize roles different attention head detail view help users compare attention weight syntactic structure semantic information input text facilitate visual comparison attention weight linguistic knowledge dodrio apply different graph visualization techniques represent attention weight scalable longer input text case study highlight dodrio provide insights understand attention mechanism transformer base model dodrio available https poloclubgithubio dodrio
paper present new large scale japanese speech corpus train automatic speech recognition asr systems corpus contain two thousand hours speech transcripts build japanese tv record subtitle develop herein iterative workflow extract match audio subtitle segment tv record base conventional method lightly supervise audio text alignment evaluate model train corpus use evaluation dataset build japanese tedx presentation videos confirm performance better train corpus spontaneous japanese csj experiment result show usefulness corpus train asr systems corpus make public research community along kaldi script train model report paper
unclear exist interpretations deep neural network model respond effectively need users paper summarize common form explanations feature attribution decision rule probe use two hundred recent paper natural language process nlp compare user question collect xai question bank find although users interest explanations road take namely model choose one result well define seemly similar legitimate counterpart model interpretations answer question
paper introduce png bert new encoder model neural tts model augment original bert model take phoneme grapheme representations text input well word level alignment pre train large text corpus self supervise manner fine tune tts task experimental result show neural tts model use pre train png bert encoder yield natural prosody accurate pronunciation baseline model use phoneme input pre train subjective side side preference evaluations show raters statistically significant preference speech synthesize use png bert grind truth record professional speakers
automatic speech recognition asr systems promise deliver objective interpretation human speech practice recent evidence suggest state art sota asrs struggle large variation speech due eg gender age speech impairment race accent many factor bias asr system overarch goal uncover bias asr systems work towards proactive bias mitigation asr paper first step towards goal systematically quantify bias dutch sota asr system gender age regional accent non native accent word error rat compare depth phoneme level error analysis conduct understand bias occur primarily focus bias due articulation differences dataset base find suggest bias mitigation strategies asr development
investigate hypothesis within combination number concept plus substantive concept eleven animals identity indistinguishability present level concepts ie eleven animals identical indistinguishable give rise statistical structure bose einstein type similar bose einstein statistics present identical indistinguishable quantum particles proceed identify evidence hypothesis extract statistical data world wide web utilize google search tool use kullback leibler divergence method compare obtain distribution maxwell boltzmann well bose einstein distributions show bose einstein provide better fit compare maxwell boltzmanns
pre train model bert achieve great success many natural language process task however obtain better sentence representation pre train model still worthy exploit previous work show anisotropy problem critical bottleneck bert base sentence representation hinder model fully utilize underlie semantic feature therefore attempt boost isotropy sentence distribution flow base model apply sentence representations achieve improvement paper find whiten operation traditional machine learn similarly enhance isotropy sentence representations achieve competitive result furthermore whiten technique also capable reduce dimensionality sentence representation experimental result show achieve promise performance also significantly reduce storage cost accelerate model retrieval speed
automate systems negotiate humans broad applications pedagogy conversational ai advance development practical negotiation systems present casino novel corpus thousand negotiation dialogues english participants take role campsite neighbor negotiate food water firewood package upcoming trip design result diverse linguistically rich negotiations maintain tractable close domain environment inspire literature human human negotiations annotate persuasion strategies perform correlation analysis understand dialogue behaviors associate negotiation performance propose evaluate multi task framework recognize strategies give utterance find multi task learn substantially improve performance strategy label especially ones skew release dataset annotations code propel future work human machine negotiations https githubcom kushalchawla casino
self attention sa encode vector sequence accord pairwise similarity widely use speech recognition due strong context model ability however apply long sequence data accuracy reduce cause fact weight average operator may lead dispersion attention distribution result relationship adjacent signal ignore address issue paper introduce relative position awareness self attention rpsa maintain global range dependency model ability self attention also improve localness model ability local window length original rpsa fix sensitive different test data propose gaussian base self attention gsa whose window length learnable adaptive test data automatically generalize gsa new residual gaussian self attention resgsa performance improvement apply rpsa gsa resgsa transformer base speech recognition respectively experimental result aishell one mandarin speech recognition corpus demonstrate effectiveness propose methods example resgsa transformer achieve character error rate cer five hundred and eighty-six test set relative seventy-eight lower sa transformer although performance propose resgsa transformer slightly better rpsa transformer tune window length manually
paper present production semi supervise learn ssl pipeline base student teacher framework leverage millions unlabeled examples improve natural language understand nlu task investigate two question relate use unlabeled data production ssl context one select sample huge unlabeled data pool beneficial ssl train two select data affect performance different state art ssl techniques compare four widely use ssl techniques pseudo label pl knowledge distillation kd virtual adversarial train vat cross view train cvt conjunction two data selection methods include committee base selection submodular optimization base selection examine benefit drawbacks techniques apply intent classification ic name entity recognition ner task provide guidelines specify methods might beneficial improve large scale nlu systems
monitor social discourse covid nineteen vaccines key understand large populations perceive vaccination campaign focus four thousand, seven hundred and sixty-five unique popular tweet english italian covid nineteen vaccines twelve two thousand and twenty three two thousand and twenty-one one popular english tweet like four hundred and ninety-five thousand time stress popular tweet affect cognitively massive populations investigate text multimedia tweet build knowledge graph syntactic semantic associations message include visual feature indicate online users frame social discourse mostly around logistics vaccine distribution english semantic frame vaccine highly polarise trust anticipation towards vaccine scientific asset save live anger sadness mention critical issue dose administer semantic associations vaccine hoax conspiratorial jargon indicate persistence conspiracy theories vaccines massively read english post absent italian message image analysis find popular tweet image people wear face mask use language lack trust joy find tweet show people mask indicate negative affect attribute face cover social discourse behavioural analysis reveal tendency users share content elicit joy sadness disgust like less sad message highlight interplay emotions content diffusion beyond sentiment astrazeneca vaccine suspend mid march two thousand and twenty-one astrazeneca associate trustful language drive experts popular italian tweet frame vaccine crucially replace earlier level trust deep sadness result stress cognitive network innovative multimedia process open new ways reconstruct online perceptions vaccines trust
knowledge capture form entities relationships store knowledge graph knowledge graph enhance capabilities applications many different areas include web search recommendation natural language understand mainly entities enable machine understand things go beyond simple tokens many modern algorithms use learn entity embeddings structure representations however build knowledge graph take time effort hence costly nontrivial hand many web source describe entities structure format therefore find ways get useful entity knowledge advantageous propose approach process entity centric textual knowledge source learn entity embeddings turn avoid need traditional knowledge graph first extract triple new representation format use traditional complex triple extraction methods define pre determine relationship label learn entity embeddings new type triple show embeddings learn approach high quality comparable know knowledge graph base embeddings use improve ii better contextual language model base entity embeddings iii easy compute versatile domain specific applications knowledge graph readily available
linear chain conditional random field crfs combine contextual word embeddings achieve state art performance sequence label task many task identity neighbor word often useful contextual information predict label give word however contextual embeddings usually train task agnostic manner mean although may encode information neighbor word guarantee therefore beneficial design sequence label architecture directly extract information embeddings propose locally contextual nonlinear crfs sequence label approach directly incorporate information neighbor embeddings predict label give word parametrizes potential function use deep neural network model serve drop replacement linear chain crf consistently outperform ablation study variety task result competitive best publish methods particular outperform previous state art chunk conll two thousand name entity recognition ontonotes fifty english
last decade political debate progressively shift social media rhetorical devices employ online actors factions operate debate arenas capture analyse conduct statistical read societal controversies argumentation dynamics paper propose five step methodology extract categorize explore latent argumentation structure online debate use twitter data deal brexit focus expect effect case materialisation event first extract effect claim contain tweet use regex exploit verbs relate creation destruction causation second categorise extract deal effect use structural topic model estimate unigrams bigrams third select controversial effect topics explore within topic argumentation differences self declare partisan user factions hence type topics use estimate covariate effect topic propensities use topics correlation network study topological structure debate identify coherent topical constellations finally analyse debate time dynamics infer lead follow relations among factions result show propose methodology employ perform statistical rhetorics analysis debate map architecture controversies across time particular deal brexit debate show assortative argumentation structure heavily characterize factional constellations arguments well polarize narrative frame invoke verbs relate creation destruction find highlight benefit implement systemic approach analysis debate allow unveil topical factional dependencies arguments employ online debate
chatbot literature focus improve fluency coherence chatbot dedicate make chatbots human like however little work delve really separate humans chatbots humans intrinsically understand effect responses interlocutor often respond intention propose optimistic view make interlocutor feel better paper propose innovative framework train chatbots possess human like intentions framework include guide chatbot interlocutor model play role humans guide chatbot assign intention learn induce interlocutor reply responses match intention example long responses joyful responses responses specific word etc examine framework use three experimental setups evaluate guide chatbot four different metrics demonstrate flexibility performance advantage additionally perform trials human interlocutors substantiate guide chatbot effectiveness influence responses humans certain extent code make available public
vision language navigation vln multimodal task agent follow natural language instructions navigate visual environments multiple setups propose researchers apply new model architectures train techniques boost navigation performance however recent study witness slow performance improvements indoor outdoor vln task agents inner mechanisms make navigation decisions remain unclear best knowledge way agents perceive multimodal input study clearly need investigations work conduct series diagnostic experiment unveil agents focus navigation result show indoor navigation agents refer object tokens direction tokens instruction make decisions contrast outdoor navigation agents heavily rely direction tokens poor understand object tokens furthermore instead merely star surround object indoor navigation agents set sight object current viewpoint come vision language alignments many model claim able align object tokens certain visual target cast doubt reliability alignments
recently unprecedented data growth originate different online platforms contribute big data term volume velocity variety veracity 4vs give nature big data unstructured perform analytics extract meaningful information currently great challenge big data analytics collect analyze unstructured textual data allow decision makers study escalation comment post social media platforms hence need automatic big data analysis overcome noise non reliability unstructured dataset digital media platforms however current machine learn algorithms use performance drive focus classification prediction accuracy base know properties learn train sample learn task large dataset machine learn model know require high computational cost eventually lead computational complexity work two supervise machine learn algorithms combine text mine techniques produce hybrid model consist nai bay support vector machine svm increase efficiency accuracy result obtain also reduce computational cost complexity system also provide open platform group persons common interest share comment message comment classify automatically legal illegal improve quality conversation among users hybrid model develop use weka tool java program language result show hybrid model give nine thousand, six hundred and seventy-six accuracy six thousand, one hundred and forty-five six thousand, nine hundred and twenty-one nai bay svm model respectively
design speech intent s2i agent map users speak command agents desire task action challenge due diverse grammatical lexical preference different users remedy discuss user teach s2i system paper user teach system learn scratch users speak input action demonstration ensure fully match users way formulate intents articulation habit main issue scarce train data due user effort involve exist state art approach set base non negative matrix factorization nmf capsule network paper combine encoder end end asr system prior nmf capsule network base user teach decoder investigate whether pre train methodology reduce train data requirements nmf capsule network experimental result show pre train asr nmf framework significantly outperform model also discuss limitations pre train different type command controlcandc applications
end end model reach state art performance speech recognition global soft attention monotonic might lead convergence problems instability bad generalisation use online stream also inefficient calculation monotonicity potentially fix several ad hoc solutions heuristics introduce monotonicity principled introduction rarely find literature far paper present mathematically clean solution introduce monotonicity introduce new latent variable represent audio position segment boundaries compare several monotonic latent model global soft attention baseline hard attention model local windowed soft attention model segmental soft attention model show monotonic model perform good global soft attention model perform experiment switchboard 300h carefully outline detail train release code configs
project task create model give speaker id chat history utterance query predict response utterance conversation model personalize speaker task useful tool build speech bots talk human like manner live conversation succeed use dense vector encode cluster able retrieve relevant historical dialogue context useful strategy overcome input limitations neural base model predictions require longer term reference dialogue history paper implement state art model use pre train fine tune techniques build transformer architecture multi head attention block switchboard corpus also show efficient vector cluster algorithms use real time utterance predictions require train therefore work offline encrypt message histories
transcribe meet contain overlap speech single distant microphone sdm one challenge problems automatic speech recognition asr various approach propose previous study monaural overlap speech recognition problem base either simulation data small scale real data paper extensively investigate two step approach first pre train serialize output train sot base multi talker asr use large scale simulation data fine tune model small amount real meet data experiment conduct utilize seventy-five thousand k hours internal single talker record simulate total 900k hours multi talker audio segment supervise pre train fine tune seventy hours ami sdm train data sot asr model achieve word error rate wer two hundred and twelve ami sdm evaluation set automatically count speakers test segment result significantly better previous state art wer three hundred and sixty-four oracle utterance boundary information also better result similarly fine tune single talker asr model apply beamformed audio
end end neural network model achieve improve performance various automatic speech recognition asr task however model perform poorly edge hardware due large memory computation requirements quantize model weight activations low precision promise solution previous research quantize asr model limit quantization approach use float point arithmetic inference thus fully exploit integer process units use less power float point counterparts moreover require train validation data quantization finetuning calibration however data may available due security privacy concern address limitations propose q asr integer zero shoot quantization scheme asr model particular generate synthetic data whose runtime statistics resemble real data use calibrate model quantization apply q asr quantize quartznet 15x5 jasperdr 10x5 without train data show negligible wer change compare full precision baseline model int8 quantization observe modest wer degradation twenty-nine achieve 244x speedup t4 gpu furthermore q asr exhibit large compression rate 4x small wer degradation
inspire ability stylegan generate highly realistic image variety domains much recent work focus understand use latent space stylegan manipulate generate real image however discover semantically meaningful latent manipulations typically involve painstaking human examination many degrees freedom annotate collection image desire manipulation work explore leverage power recently introduce contrastive language image pre train clip model order develop text base interface stylegan image manipulation require manual effort first introduce optimization scheme utilize clip base loss modify input latent vector response user provide text prompt next describe latent mapper infer text guide latent manipulation step give input image allow faster stable text base manipulation finally present method map text prompt input agnostic directions stylegan style space enable interactive text drive image manipulation extensive result comparisons demonstrate effectiveness approach
work focus compare different solutions machine translation low resource language pair namely zero shoot transfer learn unsupervised machine translation discuss data size affect performance unsupervised mt transfer learn additionally also look domain data affect result unsupervised mt code experiment perform project accessible github
tackle challenge visual question answer multi image set isvqa dataset traditional vqa task focus single image set target answer generate single image image set vqa however comprise set image require find connection image relate object across image base connections generate unify answer report work four approach bid improve performance task analyse compare result three baseline model lxmert hme videoqa visualbert show approach provide slight improvement baselines specific try improve spatial awareness model help model identify color use enhance pre train reduce language dependence use adversarial regularization improve count use regression loss graph base deduplication delve depth analysis language bias isvqa dataset show model train isvqa implicitly learn associate language strongly final answer
stream fusion also know system combination common technique automatic speech recognition traditional hybrid hide markov model approach yet mostly unexplored modern deep neural network end end model architectures investigate various fusion techniques attention base encoder decoder architecture know transformer strive achieve optimal fusion investigate different fusion level example single microphone set fusion standard magnitude phase feature introduce novel multi encoder learn method perform weight combination two encoder decoder multi head attention output train employ magnitude feature encoder inference able show consistent improvement wall street journal wsj language model librispeech without increase runtime parameters combine two multi encoder train model simple late fusion inference achieve state art performance transformer base model wsj significant wer reduction nineteen relative compare current benchmark approach
seven thousand languages world translation research efforts target high resource languages commercial translation systems support one hundred languages fewer make model available transfer low resource languages work present useful tool machine translation research mtdata nlcodec rtg demonstrate usefulness create multilingual neural machine translation model capable translate five hundred source languages english make multilingual model readily downloadable usable service parent model transfer learn even lower resource languages
work aim empirically clarify recently discover perspective label smooth incompatible knowledge distillation begin introduce motivation behind incompatibility raise ie label smooth erase relative information teacher logits provide novel connection label smooth affect distributions semantically similar dissimilar class propose metric quantitatively measure degree erase information sample representation study one sidedness imperfection incompatibility view massive analyse visualizations comprehensive experiment image classification binary network neural machine translation finally broadly discuss several circumstances wherein label smooth indeed lose effectiveness project page http zhiqiangshencom project lsandkd indexhtml
special purpose learn system assume knowledge admissible task design time adapt system unforeseen task require architecture manipulation add output head new task dataset work propose task agnostic vision language system accept image natural language task description output bound box confidences text system support wide range vision task classification localization question answer caption evaluate system ability learn multiple skills simultaneously perform task novel skill concept combinations learn new skills efficiently without forget
transformer architecture successful across many domains include natural language process computer vision speech recognition keyword spot self attention primarily use top convolutional recurrent encoders investigate range ways adapt transformer architecture keyword spot introduce keyword transformer kwt fully self attentional architecture exceed state art performance across multiple task without pre train additional data surprisingly simple architecture outperform complex model mix convolutional recurrent attentive layer kwt use drop replacement model set two new benchmark record google speech command dataset nine hundred and eighty-six nine hundred and seventy-seven accuracy twelve thirty-five command task respectively
paper present context summarizer tool take arbitrary public news article context summarize coherently fit either liberal conservative lean agenda context summarizer also suggest hashtag keywords bolster polarization summary case one incline take twitter parler platforms troll context summarizer achieve seventy-nine precision ninety-nine recall summarize covid nineteen article ninety-three precision ninety-three recall summarize politically center article eighty-seven precision eighty-eight recall take liberally bias article context summarize valid source instead synthesize fake text context summarizer could fairly pass adversarial disclosure test take easy route paper instead use context summarizer push debate potential misuse automate text generation beyond boilerplate text responsible disclosure adversarial language model
grow interest asr systems recognize phone language independent fashion additionally interest build language technologies low resource endanger languages however paucity realistic data use test systems technologies paper present publicly available phonetically transcribe corpus two thousand, two hundred and fifty-five utterances word short phrase endanger tangkhulic language east tusom iso six hundred and thirty-nine three code tibeto burman language variety speak mostly india dataset transcribe term phone rather phonemes better match universal phone recognition systems many larger phonemically transcribe datasets paper describe dataset methodology use produce present basic benchmarks state art universal phone recognition systems dataset baselines future experiment
paper tackle automatically discover phone like acoustic units aud unlabeled speech data past study usually propose single step approach propose two stage approach first stage learn subword discriminative feature representation second stage apply cluster learn representation obtain phone like cluster discover acoustic units first stage recently propose method task unsupervised subword model improve replace monolingual domain ood asr system multilingual one create subword discriminative representation language independent second stage segment level k mean adopt two methods represent variable length speech segment fix dimension feature vectors compare experiment low resource mboshi language corpus show approach outperform state art aud normalize mutual information nmi f score multilingual asr improve upon monolingual asr provide ood phone label estimate phone boundaries comparison systems without know grind truth phone boundaries show sixteen nmi performance gap suggest current approach significantly benefit improve phone boundary estimation
self supervise learn speech representations active research area work focus single domain read audio book exist large quantities label unlabeled data paper explore general setups domain unlabeled data pre train data differ domain label data fine tune turn may differ test data domain experiment show use target domain data pre train lead large performance improvements across variety setups large scale competitive setup show pre train unlabeled domain data reduce gap model train domain domain label data sixty-six seventy-three obvious practical implications since much easier obtain unlabeled target domain data label data moreover find pre train multiple domains improve generalization performance domains see train code model make available https githubcom pytorch fairseq
paper propose new methodology study sequential corpora implement two stage algorithm learn time base topics respect scale document position introduce concept topic scale rank learn topics within document scale first stage rank document use wordfish poisson base document scale method estimate document position serve second stage dependent variable learn relevant topics via supervise latent dirichlet allocation novelty bring two innovations text mine explain document position whose scale latent variable rank infer topics document scale match occurrences within corpus track evolution test yous state union two party address inductive approach reveal party dominate one end learn scale interchangeable transition follow party term office besides demonstrate high accuracy predict sample document position topic score method reveal hide topics differentiate similar document increase number learn topics unfold potential nest hierarchical topic structure compare popular topic model topic scale learn topics respect document similarities without specify time frequency learn topic evolution thus capture broader topic pattern dynamic topic model yield interpretable output plain latent dirichlet allocation
infer semantic type entity mention within text document important asset many downstream nlp task semantic role label entity disambiguation knowledge base question answer etc prior work mostly focus supervise solutions generally operate relatively small medium size type systems work describe two systems aim predict type information follow two task namely typesuggest module unsupervised system design predict type set user enter query term answer type prediction module provide solution task determine correct type answer expect give query systems generalize arbitrary type systems size thereby make highly appeal solution extract type information granularity
auto regressive sequence sequence model attention mechanisms achieve state art performance various task include text speech tts neural machine translation nmt standard train approach teacher force guide model reference output history inference stage generate output history must use mismatch impact performance however highly challenge train model use generate output several approach propose address problem normally selectively use generate output history make train stable approach often require heuristic schedule auxiliary classifier paper introduce attention force nmt approach guide model generate output history reference attention reduce train inference mismatch without schedule classifier attention force successful tts application nmt challenge due discrete multi modal nature output space tackle problem paper add selection scheme vanilla attention force automatically select suitable train approach pair train data experiment show attention force improve overall translation quality diversity translations
paper introduce new open source speech corpus name speechocean762 design pronunciation assessment use consist five thousand english utterances two hundred and fifty non native speakers half speakers children five experts annotate utterances sentence level word level phoneme level baseline system release open source illustrate phoneme level pronunciation assessment workflow corpus corpus allow use freely commercial non commercial purpose available free download openslr correspond baseline system publish kaldi speech recognition toolkit
image medical domain fundamentally different general domain image consequently infeasible directly employ general domain visual question answer vqa model medical domain additionally medical image annotation costly time consume process overcome limitations propose solution inspire self supervise pretraining transformer style architectures nlp vision language task method involve learn richer medical image text semantic representations use mask language model mlm image feature pretext task large medical imagecaption dataset propose solution achieve new state art performance two vqa datasets radiology image vqa med two thousand and nineteen vqa rad outperform even ensemble model previous best solutions moreover solution provide attention map help model interpretability code available https githubcom virajbagal mmbert
medical knowledge base kbs distil biomedical literature regulatory action expect provide high quality information facilitate clinical decision make entity disambiguation also refer entity link consider essential task unlock wealth medical kbs however exist medical entity disambiguation methods adequate due word discrepancies entities kb text snippets source document recently graph neural network gnns prove effective provide state art result many real world applications graph structure data paper introduce ed gnn base three representative gnns graphsage r gcn magnn medical entity disambiguation develop two optimization techniques fine tune improve ed gnn first introduce novel strategy represent entities mention text snippets query graph second design effective negative sample strategy identify hard negative sample improve model disambiguation capability compare best perform state art solutions ed gnn offer average improvement seventy-three term f1 score five real world datasets
dietary supplement ds widely use consumers information around efficacy safety ds disparate incomplete thus create barriers consumers find information effectively conversational agent ca systems apply healthcare domain system answer consumers regard ds use although widespread use ds study develop first ca system ds use
intelligent robots design interact humans real scenarios need able refer entities actively natural language spatial refer expression generation ambiguity unavoidable due diversity reference frame lead understand gap humans robots narrow gap paper propose novel perspective correct spatial refer expression generation pcsreg approach human robot interaction consider selection reference frame task refer expression generation simplify process generate diverse spatial relation units first pick landmarks spatial relation units accord entropy preference allow update stack model possible refer expressions generate accord different reference frame strategies finally evaluate every expression use probabilistic refer expression resolution model find best expression satisfy appropriateness effectiveness implement propose approach robot system empirical experiment show approach generate effective spatial refer expressions practical applications
paper describe system task four semeval two thousand and twenty-one read comprehension abstract mean recam participate subtasks main goal predict abstract word miss statement fine tune pre train mask language model namely bert albert use ensemble submit system subtask one recam imperceptibility subtask two recam nonspecificity subtask three recam intersection submit albert model give best result try multiple approach find mask language modelingmlm base approach work best
interest physical therapy individual exercise yoga dance increase alongside well trend however exercise hard follow without expert guidance impossible scale personalize feedback every trainee remotely thus automate pose correction systems require ever introduce new caption dataset name fixmypose address need collect descriptions correct current pose look like target pose english hindi collect descriptions interest linguistic properties egocentric relations environment object analogous reference etc require understand spatial relations commonsense knowledge posture avoid ml bias maintain balance across character diverse demographics perform variety movements several interior environments eg home offices dataset introduce pose correctional caption task reverse target pose retrieval task correctional caption task model must generate descriptions move current target pose image whereas retrieval task model select correct target pose give initial pose correctional description present strong cross attention baseline model uni multimodal rl multilingual also show baselines competitive model evaluate image difference datasets also propose new task specific metrics object match body part match direction match conduct human evaluation reliable evaluation demonstrate large human model performance gap suggest room promise future work verify sim real transfer fixmypose dataset collect set real image show promise performance image
paper show stargan vc spectral envelope transformation method non parallel many many voice conversion vc capable emotional vc evc although stargan vc show enable speaker identity conversion capability evc japanese phrase clarify paper describe direct application stargan vc evc task minimal fundamental frequency aperiodicity process subjective evaluation experiment evaluate performance stargan evc system term ability achieve evc japanese phrase subjective evaluation conduct term subjective classification mean opinion score neutrality similarity addition interdependence source target emotional domains investigate perspective quality evc
speech base image retrieval study proxy joint representation learn usually without emphasis retrieval unclear well speech base retrieval work practice absolute sense versus alternative strategies combine automatic speech recognition asr strong text encoders work extensively study expand choices encoder architectures train methodology include unimodal multimodal pretraining factor experiment cover different type speech three datasets flickr audio place audio localize narratives best model configuration achieve large gain state art eg push recall one two hundred and eighteen three hundred and thirty-two flickr audio two hundred and seventy-six five hundred and thirty-four place audio also show best speech base model match exceed cascade asr text encode speech spontaneous accent otherwise hard automatically transcribe
student mobility academic mobility involve students move institutions post secondary education one challenge task process assess transfer credit offer incoming student general process involve domain experts compare learn outcomes course decide offer transfer credit incoming students manual implementation labor intensive also influence undue bias administrative complexity propose research article focus identify model exploit advancements field natural language process nlp effectively automate process give unique structure domain specificity complexity learn outcomes los need design tailor make model arise propose model use cluster inspire methodology base knowledge base semantic similarity measure assess taxonomic similarity los transformer base semantic similarity model assess semantic similarity los similarity los aggregate form course course similarity due lack quality benchmark datasets new benchmark dataset contain seven course course similarity measure propose understand inherent need flexibility decision make process aggregation part model offer tunable parameters accommodate different scenarios provide efficient model assess similarity course exist resources research work steer future research attempt apply nlp field articulation ideal direction highlight persist research gap
paper present recent effort end end speaker attribute automatic speech recognition jointly perform speaker count speech recognition speaker identification monaural multi talker audio firstly thoroughly update model architecture previously design base long short term memory lstm base attention encoder decoder apply transformer architectures secondly propose speaker deduplication mechanism reduce speaker identification errors highly overlap regions experimental result librispeechmix dataset show transformer base architecture especially good count speakers propose model reduce speaker attribute word error rate forty-seven lstm base baseline furthermore libricss dataset consist real record overlap speech propose model achieve concatenate minimum permutation word error rat one hundred and nineteen one hundred and sixty-three without target speaker profile respectively state art result libricss monaural set
leverage dynamic contextual information end end speech recognition remain active research area previous solutions problem either design specialize use case generalize well open domain scenarios scale large bias list underperform rare long tail word address limitations propose novel solution combine shallow fusion trie base deep bias neural network language model contextualization techniques result significant one hundred and ninety-five relative word error rate improvement exist contextual bias approach fifty-four ninety-three improvement compare strong hybrid baseline open domain constrain contextualization task target consist mostly rare long tail word final system remain lightweight modular allow quick modification without model train
speech enable devices smartphones smart speakers become increasingly ubiquitous grow interest build automatic speech recognition asr systems run directly device end end e2e speech recognition model recurrent neural network transducers variants recently emerge prime candidates task apart accurate compact systems need decode speech low user perceive latency upl produce word soon speak work examine impact various techniques model architectures train criteria decode hyperparameters endpointer parameters upl analyse suggest measure model size parameters input chunk size measure computation eg flop rtf reflect model ability process input frame always strongly correlate observe upl thus conventional algorithmic latency measurements might inadequate accurately capture latency observe model deploy embed devices instead find factor affect token emission latency endpointing behavior significantly impact upl achieve best trade latency word error rate perform asr jointly endpointing use recently propose alignment regularization
often storage computational constraints embeddeddevices demand single device asr model serve multiple use case domains paper propose aflexibletransducerflexit device automatic speech recognition flexibly deal multiple use case domains different accuracy latency requirements specifically use single compact model flexit provide fast response voice command accurate transcription latency dictation order achieve flexible better accuracy latency trade off follow techniques use firstly propose use domain specific alter segment size emformer encoder enable flexit achieve flexible de cod secondly use alignment restrict rnnt loss achieve flexible fine grain control token emission latency different domains finally add domain indicator vector additional input flexit model use combination techniques show single model use improve wers real time factor dictation scenarios maintain optimal latency voice command use case
detect domain ood utterance crucial robust dialog system dialog systems train pool annotate ood data achieve goal however collect annotate ood data give domain expensive process mitigate issue previous work propose generative adversarial network gin base model generate ood data give domain automatically however propose model work directly text work text latent space instead enforce model include components responsible encode text latent space decode back auto encoder components increase model complexity make difficult train propose oodgan sequential generative adversarial network seqgan base model ood data generation propose model work directly text hence eliminate need include auto encoder ood data generate use oodgan model outperform state art ood detection metrics rostd sixty-seven relative improvement fpr ninety-five osq datasets twenty-eight relative improvement fpr ninety-five zheng et al two thousand and twenty
topic model structural topic model stm estimate latent topical cluster within text important step many topic model applications explore relationships discover topical structure metadata associate text document methods use estimate relationships must take account topical structure directly observe instead estimate author stm instance perform repeat ols regressions sample topic proportion metadata covariates use monte carlo sample technique know method composition paper propose two improvements first replace ols appropriate beta regression second suggest fully bayesian approach instead current blend frequentist bayesian methods demonstrate improve methodology explore relationships twitter post german members parliament mps different metadata covariates
neural network base language model commonly use rescoring approach improve quality modern automatic speech recognition asr systems exist methods computationally expensive since use autoregressive language model propose novel rescoring approach process entire lattice single call model key feature rescoring policy novel non autoregressive lattice transformer language model lt lm model take whole lattice input predict new language score arc additionally propose artificial lattices generation approach incorporate large amount text data lt lm train process single shoot rescoring perform order magnitude faster rescoring methods experiment three hundred time faster prune rnnlm lattice rescoring n best rescoring slightly inferior term wer
many real world applications mismatch distributions train data source test data target significantly degrade performance machine learn algorithms speech data cause mismatch include different acoustic environments speaker characteristics paper address issue challenge context dysarthric speech multi source domain speaker adaptation msda mssa specifically propose use optimal transport base approach call msda via weight joint optimal transport msda wdjot confront mismatch problem dysarthria detection propose approach outperform baseline state art msda model improve detection accuracy nine best competitor method employ msda wjdot dysarthric speaker adaptation command speech recognition provide command error rate relative reduction sixteen seven baseline best competitor model respectively interestingly msda wjdot provide similarity score source target ie speakers case leverage similarity measure define dysarthric healthy score target speaker diagnose dysarthria accuracy ninety-five
paper propose method relax conditional independence assumption connectionist temporal classification ctc base automatic speech recognition asr model train ctc base asr model auxiliary ctc losses intermediate layer addition original ctc loss last layer train inference generate prediction intermediate layer sum input next layer condition prediction last layer intermediate predictions method easy implement retain merit ctc base asr simple model architecture fast decode speed conduct experiment three different asr corpora propose method improve standard ctc model significantly eg twenty relative word error rate reduction wsj corpus little computational overhead moreover tedlium2 corpus aishell one corpus achieve comparable performance strong autoregressive model beam search decode speed least thirty time faster
clark et al two thousand and twenty claim electra approach highly efficient nlp performances relative computation budget reproducibility study focus claim summarize follow question use electra achieve close sota performances nlp low resource settings term compute cost
transducer base model rnn transducer transformer transducer achieve great success speech recognition typical transducer model decode output sequence condition current acoustic state previously predict tokens step step statistically number blank tokens prediction result account nearly ninety tokens take lot computation time predict blank tokens non blank tokens appear final output sequence therefore propose method name fast skip regularization try align blank position predict transducer predict ctc model inference transducer model predict blank tokens advance simple ctc project layer without many complicate forward calculations transducer decoder skip reduce computation improve inference speed greatly experiment conduct public chinese mandarin dataset aishell one result show fast skip regularization indeed help transducer model learn blank position alignments besides inference fast skip speed nearly four time little performance degradation
relation extraction important task knowledge acquisition text understand exist work mainly focus improve relation extraction extract effective feature design reasonable model structure however work focus validate correct result generate exist relation extraction model argue validation important promise direction improve performance relation extraction paper explore possibility use question answer validation specifically propose novel question answer base framework validate result relation extraction model propose framework easily apply exist relation classifiers without additional information conduct extensive experiment popular nyt dataset evaluate propose framework observe consistent improvements five strong baselines
present transducer model librispeech study variants include external language model lm shallow fusion subtract estimate internal lm justify bayesian interpretation transducer model prior give estimate internal lm subtraction internal lm give us fourteen relative improvement normal shallow fusion transducer separate probability distribution non blank label allow easier combination external lm easier estimation internal lm additionally take care include end sentence eos probability external lm last blank probability improve performance code setups publish
increasingly consider human speech perception production rely articulatory representations paper investigate whether type representation could improve performances deep generative model variational autoencoder train encode decode acoustic speech feature first develop articulatory model able associate articulatory parameters describe jaw tongue lips velum configurations vocal tract shape spectral feature incorporate articulatory parameters variational autoencoder apply spectral feature use regularization technique constraints part latent space follow articulatory trajectories show articulatory constraint improve model train decrease time convergence reconstruction loss convergence yield better performance speech denoising task
many information extraction applications entity link el emerge crucial task allow leverage information name entities knowledge base paper address task multimodal entity link mel emerge research field textual visual information use map ambiguous mention entity knowledge base kb first propose method build fully annotate twitter dataset mel entities define twitter kb propose model jointly learn representation mention entities textual visual contexts demonstrate effectiveness propose model evaluate propose dataset highlight importance leverage visual information available
combine recent advancements end end speech recognition non autoregressive automatic speech recognition push limit non autoregressive state art result multiple datasets librispeech fisherswitchboard wall street journal key recipe leverage ctc giant conformer neural network architectures specaugment wav2vec2 pre train achieve eighteen thirty-six wer librispeech test test set fifty-one ninety-eight wer switchboard thirty-four wall street journal without language model
paper introduce multi scale speech style model method end end expressive speech synthesis propose method employ multi scale reference encoder extract global scale utterance level local scale quasi phoneme level style feature target speech feed speech synthesis model extension input phoneme sequence train time multi scale style model could jointly train speech synthesis model end end fashion apply propose method style transfer task experimental result indicate controllability multi scale speech style model expressiveness synthesize speech greatly improve moreover assign different reference speeches extraction style scale flexibility propose method reveal
recently attention base encoder decoder aed end end e2e model draw attention field automatic speech recognition asr aed model however still drawbacks deploy commercial applications autoregressive beam search decode make inefficient high concurrency applications also inconvenient integrate external word level language model important thing aed model difficult stream recognition due global attention mechanism paper propose novel framework namely wnars use hybrid ctc attention aed model weight finite state transducers wfst solve problems together switch autoregressive beam search ctc branch decode perform first pass decode wfst chunk wise stream way decoder branch perform second pass rescoring generate hypotheses non autoregressively aishell one task wnars achieve character error rate five hundred and twenty-two 640ms latency best knowledge state art performance online asr experiment ten thousand hour mandarin task show propose method achieve twenty improvements fifty latency compare strong tdnn blstm lattice free mmi baseline
diverse promise datasets design hold back development fake audio detection asvspoof databases however previous datasets ignore attack situation hacker hide small fake clip real speech audio pose serious threat since difficult distinguish small fake clip whole speech utterance therefore paper develop dataset half truth audio detection partially fake audio dataset involve change word utterancethe audio word generate latest state art speech synthesis technology detect fake uttrances also localize manipulate regions speech use dataset benchmark result present dataset result show partially fake audio present much challenge fully fake audio fake audio detection
air traffic management specifically air traffic control atc rely mostly voice communications air traffic controllers atcos pilot case voice communications follow well define grammar could leverage automatic speech recognition asr technologies callsign use address airplane essential part atco pilot communications propose two step approach add contextual knowledge semi supervise train reduce asr system error rat recognize part utterance contain callsign initially represent wfst contextual knowledge ie air surveillance data atco pilot communication semi supervise learn ssl contextual knowledge add second pass decode ie lattice score result show unseen domains eg data airports present supervise train data aid contextual ssl compare standalone ssl task introduce callsign word error rate ca wer evaluation metric assess asr performance speak callsign utterance obtain three hundred and twenty-one ca wer relative improvement apply ssl additional one hundred and seventy-five ca wer improvement add contextual knowledge ssl challenge atc base test set gather liveatc
key issue system design lack communication hardware software domain expert recent research work show progress automatic hw sw co design flow neural accelerators seem make kind communication obsolete real world systems however composition multiple process units communication network memories hw sw co design process reconfigurable neural accelerators therefore important sub problem towards common co design methodology ultimate challenge define constraints design space exploration system level task require deep knowledge understand hardware architectures map workloads onto hardware application domain eg artificial intelligence project skills distribute among several people even different team one major reason establish end end development methodology digital systems position paper discuss possibilities establish methodology systems include reconfigurable dedicate accelerators outline central role languages tool play process
machine speech chain integrate end end e2e automatic speech recognition asr text speech tts one circle joint train prove effective data augmentation leverage large amount unpaired data paper explore tts asr pipeline speech chain domain adaptation neural tts e2e asr model text data target domain conduct experiment adapt audiobook domain librispeech presentation domain ted lium relative word error rate wer reduction ten e2e asr model ted lium test set relative wer reduction five hundred and fifteen synthetic speech generate neural tts presentation domain apply shoot speaker adaptation e2e asr use utterances target speakers unsupervised way result additional gain
present comprehensive study build adapt rnn transducer rnn model speak language understandingslu end end e2e model construct three practical settings case verbatim transcripts available constrain case available annotations slu label value restrictive case transcripts available correspond audio show rnn slu model develop start pre train automatic speech recognition asr systems follow slu adaptation step settings real audio data available artificially synthesize speech use successfully adapt various slu model evaluate two slu data set atis corpus customer call center data set propose model closely track performance e2e model achieve state art result
self supervise cluster methods achieve increase accuracy recent years yet perform well supervise classification methods contrast situation feature learn self supervise feature recently surpass performance supervise feature several important task hypothesize performance gap due difficulty specify without supervision feature correspond class differences semantic humans reduce performance gap introduce single noun prior state semantic cluster tend correspond concepts humans label single noun utilize pre train network map image sentence common space impose prior obtain constrain optimization task show formulation special case facility location problem introduce simple yet effective approach solve optimization task scale test approach several commonly report image cluster datasets obtain significant accuracy gain best exist approach
data exploration important step every data science machine learn project include involve textual data provide python library grasp exist algorithm draw pattern textual data library equip web base interface empower human users conveniently explore data extract pattern also demonstrate use library two settings spam detection argument mine discuss future deployments library eg beyond textual data exploration
describe plug play controllable language generation framework plug blend allow human user input multiple control cod topics context automate story generation allow human user lose fine grain control topics appear generate story even allow overlap blend topics show framework work different generation model control generation towards give continuous weight control cod keep generate sentence fluent demonstrate strong blend capability
neural sequence sequence text speech synthesis tts tacotron two transform text high quality speech however generate speech natural prosody still remain challenge yasuda et al show unlike natural speech tacotron two encoder fully represent prosodic feature eg syllable stress english character result flat fundamental frequency variations work propose novel carefully design strategy condition tacotron two two fundamental prosodic feature english stress syllable pitch accent help achieve natural prosody end use classifier learn feature end end fashion apply feature condition three part tacotron two text mel spectrogram pre encoder post encoder intra decoder show jointly condition feature pre encoder intra decoder stag result prosodically natural synthesize speech vs tacotron two allow model produce speech accurate pitch accent stress pattern quantitative evaluations show formulation achieve higher fundamental frequency contour correlation lower mel cepstral distortion measure synthesize natural speech subjective evaluation show propose method mean opinion score four hundred and fourteen fair higher baseline tacotron two three hundred and ninety-one compare natural speech ljspeech corpus four hundred and twenty-eight
grapheme phoneme g2p model convert word phonetic pronunciations classic g2p methods include rule base systems pronunciation dictionaries modern g2p systems incorporate learn lstm transformer base attention model usually dictionary base methods require significant manual effort build limit adaptivity unseen word transformer base model require significant train data generalize well especially dialects limit data propose novel use transformer base attention model adapt unseen dialects english language use small dictionary show method potential applications accent transfer text speech build robust g2p model dialects limit pronunciation dictionary size experiment two english dialects indian british model train scratch use one thousand word british english dictionary fourteen thousand, two hundred and eleven word hold lead phoneme error rate per twenty-six thousand, eight hundred and seventy-seven test set generate use full dictionary model pretrained cmudict american english dictionary fine tune dataset lead per two thousand, four hundred and sixty-nine test set
suicide 10th lead death yous one thousand, nine hundred and ninety-nine two thousand and nineteen however predict someone attempt suicide nearly impossible modern world many individuals suffer mental illness seek emotional support advice well know easily accessible social media platforms reddit prior artificial intelligence research demonstrate ability extract valuable information social media suicidal thoughts behaviors efforts consider severity temporality risk insights make possible access data enormous clinical potential dramatically envision trigger employ timely target interventions ie voluntary involuntary psychiatric hospitalization save live work address knowledge gap develop deep learn algorithms assess suicide risk term severity temporality reddit data base columbia suicide severity rat scale c ssrs particular employ two deep learn approach time variant time invariant model user level suicide risk assessment evaluate performance clinician adjudicate gold standard reddit corpus annotate base c ssrs result suggest time variant approach outperform time invariant method assessment suicide relate ideations supportive behaviors auc078 time invariant model perform better predict suicide relate behaviors suicide attempt auc064 propose approach integrate clinical diagnostic interview improve suicide risk assessments
reason tabular information present unique challenge modern nlp approach largely rely pre train contextualized embeddings text paper study challenge problem tabular natural language inference propose easy effective modifications information present model task show via systematic experiment strategies substantially improve tabular inference performance
acoustic model raw waveform learn feature extractors part neural network classifier goal many study area automatic speech recognition asr recently one line research focus frameworks pre train audio data unsupervised fashion aim improve downstream asr task work investigate usefulness one front end frameworks namely wav2vec hybrid asr systems addition deploy pre train feature extractor explore make use exist acoustic model train task different feature well another neural front end train together supervise asr loss well traditional gammatone feature apply comparison moreover show retrofit vectors speaker adaptation finally describe feature combine order advance performance final best system obtain relative improvement four six previous best model librispeech test clean test set
stream process speech audio require many contemporary practical speech recognition task even large corpora manually transcribe speech data available today impossible corpora cover adequately long tail linguistic content important task open end dictation voice search seek address stream tail recognition challenge use language model lm train unpaired text data enhance end end e2e model extend shallow fusion cold fusion approach stream recurrent neural network transducer rnnt also propose two new competitive fusion approach enhance rnnt architecture result multiple languages vary train set size show fusion methods improve stream rnnt performance introduce extra linguistic feature cold fusion work consistently better stream rnnt eighty-five wer improvement
introduce lookup table language model lookuplm method scale size rnn language model constant increase float point operations increase expressivity embed table particular instantiate additional embed table embed previous n gram token sequence rather single token allow embed table scale arbitrarily commensurate increase performance without change token vocabulary since embeddings sparsely retrieve table via lookup increase size table add neither extra operations forward pass extra parameters need store limit gpu tpu memory explore scale n gram embed table nearly billion parameters train three billion sentence corpus find lookuplm improve long tail log perplexity two hundred and forty-four long tail wer two hundred and thirty-four downstream speech recognition task standard rnn language model baseline improvement comparable scale baseline 62x number float point operations
recent years significant effort invest verify reproducibility robustness research claim social behavioral sciences sbs much involve resource intensive replication project paper investigate prediction reproducibility sbs paper use machine learn methods base set feature propose framework extract five type feature scholarly work use support assessments reproducibility publish research claim bibliometric feature venue feature author feature collect public apis extract use open source machine learn libraries customize parsers statistical feature p value extract recognize pattern body text semantic feature fund information obtain public apis extract use natural language process model analyze pairwise correlations individual feature importance predict set human assess grind truth label identify subset nine top feature play relatively important roles predict reproducibility sbs paper corpus result verify compare performances ten supervise predictive classifiers train different set feature
generation scientific visualization analytical natural language text challenge task paper propose text2chart multi stag chart generator method text2chart take natural language text input produce visualization two dimensional chart text2chart approach problem three stag firstly identify axis elements chart give text know x entities find map x entities correspond entities next generate chart type suitable give text bar line pie combination three stag capable generate visualization give analytical text also construct dataset problem experiment show text2chart achieve best performances bert base encode lstm model first stage label x entities random forest classifier map stage fasttext embed lstm chart type prediction experiment stag show satisfactory result effectiveness consider formation chart analytical text achieve commendable overall performance
knowledge base often consist facts harvest variety source many noisy conflict result level uncertainty triple knowledge base also often incomplete prompt use embed methods generalize know facts however exist embed methods model triple level uncertainty reason result lack global consistency address shortcomings propose beurre novel uncertain knowledge graph embed method calibrate probabilistic semantics beurre model entity box ie axis align hyperrectangle relations two entities affine transform head tail entity box geometry box allow efficient calculation intersections volumes endow model calibrate probabilistic semantics facilitate incorporation relational constraints extensive experiment two benchmark datasets show beurre consistently outperform baselines confidence prediction fact rank due probabilistic calibration ability capture high order dependencies among facts
improvements make automatic speech recognition performance last several years machine continue significantly lower performance accent speech humans addition significant improvements accent speech primarily arise overwhelm problem hundreds even thousands hours data humans typically require much less data adapt new accent paper explore methods inspire human perception evaluate possible performance improvements recognition accent speech specific focus recognize speech novel accent relative train data experiment run small accessible datasets available research community explore four methodologies pre exposure multiple accent grapheme phoneme base pronunciations dropout improve generalization novel accent identification layer neural network specifically associate accent model result indicate methods base human perception promise reduce wer understand accent speech model neural network novel accent
recent years widespread use social media lead increase generation toxic offensive content online platforms response social media platforms work develop automatic detection methods employ human moderators cope deluge offensive content various state art statistical model apply detect toxic post study focus detect word expressions make post offensive motivate organization semeval two thousand and twenty-one task five toxic span detection competition provide participants dataset contain toxic span annotation english post paper present wlv rit entry semeval two thousand and twenty-one task five best perform neural transformer model achieve sixty-eight f1 score furthermore develop open source framework multilingual detection offensive span ie mud base neural transformers detect toxic span texts
identify whether word carry mean different mean two contexts important research area natural language process play significant role many applications question answer document summarisation information retrieval information extraction previous work area rely language specific resources make difficult generalise across languages consider limitation approach semeval two thousand and twenty-one task two base pretrained transformer model use language specific process resources despite best model achieve ninety accuracy english english subtask compatible compare best result subtask ninety-three accuracy approach also achieve satisfactory result monolingual cross lingual language pair well
knowledge graph embeddings kges intensively explore recent years due promise wide range applications however exist study focus improve final model performance without acknowledge computational cost propose approach term execution time environmental impact paper propose simple yet effective kge framework reduce train time carbon footprint order magnitudes compare state art approach produce competitive performance highlight three technical innovations full batch learn via relational matrices close form orthogonal procrustes analysis kges non negative sample train addition first kge method whose entity embeddings also store full relation information train model encode rich semantics highly interpretable comprehensive experiment ablation study involve thirteen strong baselines two standard datasets verify effectiveness efficiency algorithm
relation extraction essential task knowledge acquisition representation new generate relations common real world less effort make predict unseen relations observe train stage paper formulate zero shoot relation extraction problem incorporate text description see unseen relations propose novel multi task learn model zero shoot bert zs bert directly predict unseen relations without hand craft attribute label multiple pairwise classifications give train instance consist input sentence descriptions relations zs bert learn two function project sentence relation descriptions embed space jointly minimize distance classify see relations generate embeddings unseen relations new come sentence base two function use nearest neighbor search obtain prediction unseen relations experiment conduct two well know datasets exhibit zs bert outperform exist methods least one thousand, three hundred and fifty-four improvement f1 score
paper describe system semeval two thousand and twenty-one task five toxic span detection develop ensemble model use bert base neural architectures post process combine tokens span evaluate several pre train language model use various ensemble techniques toxic span identification achieve sizable improvements baseline fine tune bert model finally system obtain f1 score six thousand, seven hundred and fifty-five test data
task orient dialog systems train reinforcement learn rl base dialog management module suffer low sample efficiency slow convergence speed due sparse reward rlto solve problem many strategies propose give proper reward train rl reward lack interpretability accurately estimate distribution state action pair real dialogs paper propose multi level reward model approach factorize reward three level hierarchy domain act slot base inverse adversarial reinforcement learn design reward model provide accurate explainable reward signal state action pairsextensive evaluations show approach apply wide range reinforcement learn base dialog systems significantly improve performance speed convergence
transformer base model lead significant innovation various classic practical subject include speech process natural language process computer vision top transformer attention base end end automatic speech recognition asr model become popular fashion recent years specifically non autoregressive model achieve fast inference speed comparable performance compare conventional autoregressive methods emergent research topic context natural language process bidirectional encoder representations transformers bert model receive widespread attention partially due ability infer contextualized word representations obtain superior performances downstream task perform simple fine tune order inherit advantage non autoregressive asr model also receive benefit pre train language model eg bert non autoregressive transformer base end end asr model base bert present paper series experiment conduct aishell one dataset demonstrate competitive superior result propose model compare state art asr systems
explosion user generate content ugc eg social media post comment review motivate development nlp applications tailor type informal texts prevalent among applications sentiment analysis machine translation mt ground observation ugc feature highly idiomatic sentiment charge language propose decoder side approach incorporate automatic sentiment score mt candidate selection process train separate english spanish sentiment classifiers use n best candidates generate baseline mt model beam search select candidate minimize absolute difference sentiment score source sentence translation perform human evaluation assess produce translations unlike previous work select minimally divergent translation consider sentiment score source sentence translation continuous interval rather use eg binary classification allow fine grain selection translation candidates result human evaluations show comparison open source mt baseline model top sentiment base pipeline build pipeline produce accurate translations colloquial sentiment heavy source texts
paper introduce new toolbox construct speech datasets long audio record raw reference texts develop tool step speech dataset construction pipeline include data preprocessing audio text alignment data post process filter propose pipeline also support human loop address text audio mismatch issue remove sample satisfy quality requirements demonstrate toolbox efficiency build russian librispeech corpus ruls librivox audiobooks toolbox opne source nemo framework ruls corpus release openslr
cross lingual word embeddings clwes encode word two languages share high dimensional space vectors represent word similar mean regardless language closely locate exist methods build high quality clwes learn mappings minimise ell2 norm loss function however optimisation objective demonstrate sensitive outliers base robust manhattan norm aka ell1 norm goodness fit criterion paper propose simple post process step improve clwes advantage approach fully agnostic train process original clwes therefore apply widely extensive experiment perform involve ten diverse languages embeddings train different corpora evaluation result base bilingual lexicon induction cross lingual transfer natural language inference task show ell1 refinement substantially outperform four state art baselines supervise unsupervised settings therefore recommend strategy adopt standard clwe methods
recently bidirectional encoder representations transformers bert propose achieve impressive success many natural language process nlp task question answer language understand due mainly effective pre train fine tune paradigm well strong local contextual model ability view paper present novel instantiation bert base contextualized language model lms use reranking n best hypotheses produce automatic speech recognition asr end frame n best hypothesis reranking bert prediction problem aim predict oracle hypothesis lowest word error rate wer give n best hypotheses denote pbert particular also explore capitalize task specific global topic information unsupervised manner assist pbert n best hypothesis reranking denote tpbert extensive experiment conduct ami benchmark corpus demonstrate effectiveness feasibility methods comparison conventional autoregressive model like recurrent neural network rnn recently propose method employ bert compute pseudo log likelihood pll score n best hypothesis reranking
inverse text normalization itn convert speak domain automatic speech recognition asr output write domain text improve readability asr output many state art itn systems use hand write weight finite state transducerwfst grammars since task extremely low tolerance unrecoverable errors introduce open source python wfst base library itn enable seamless path development production describe specification itn grammar rule english library adapt languages also use write speak text normalization evaluate nemo itn library use modify version google text normalization dataset
text style transfer aim controllably generate text target stylistic change maintain core mean source sentence constant many exist style transfer benchmarks primarily focus individual high level semantic change eg positive negative enable controllability high level offer fine grain control involve sentence structure emphasis content sentence paper introduce large scale benchmark styleptb one pair sentence undergo twenty-one fine grain stylistic change span atomic lexical syntactic semantic thematic transfer text well two compositions multiple transfer allow model fine grain stylistic change build block complex high level transfer benchmarking exist methods styleptb find struggle model fine grain change even difficult time compose multiple style result styleptb bring novel challenge hope encourage future research controllable text style transfer compositional model learn disentangle representations solve challenge would present important step towards controllable text generation
robustness counterfactual bias usually evaluate test dataset however evaluations robust test dataset perturb slightly evaluation result keep paper propose double perturbation framework uncover model weaknesses beyond test dataset framework first perturb test dataset construct abundant natural sentence similar test data diagnose prediction change regard single word substitution apply framework study two perturbation base approach use analyze model robustness counterfactual bias english one robustness focus synonym substitutions identify vulnerable examples prediction alter propose attack attain high success rat nine hundred and sixty nine hundred and ninety-eight find vulnerable examples original robustly train cnns transformers two counterfactual bias focus substitute demographic tokens eg gender race measure shift expect prediction among construct sentence method able reveal hide model bias directly show test dataset code available https githubcom chong z nlp second order attack
paper introduce sead framework simplify process design describe autonomous vehicle platooning manoeuvre although large body research formulate platooning manoeuvre still challenge design describe read understand difficulty largely arise miss formalisation fill gap analyse exist ways describe manoeuvre derive cause difficulty design framework simplify manoeuvre design process alongside manoeuvre design language develop structurally describe manoeuvre machine readable format unlike state art manoeuvre descriptions require one state machine every participate vehicle sead framework allow describe manoeuvre single perspective platoon leader proof concept propose framework implement mix traffic simulation environment behave autonomous highway scenario use framework implement several manoeuvre describe literature demonstrate applicability framework experiment perform evaluate execution time performance multiple alternatives join middle manoeuvre proof concept experiment reveal manoeuvre execution time reduce twenty-eight parallelising various step without considerable secondary effect hope sead framework pave way research area new manoeuvre design optimisation largely simplify unify platooning manoeuvre representation
fake tweet observe ever increase demand immediate countermeasures combat spread covid nineteen tweet misinformation flag neutralize early stag mitigate damage exist methods early detection fake news assume enough propagation information large label tweet may ideal set case like covid nineteen aspects largely absent work present endemic novel early detection model leverage exogenous endogenous signal relate tweet learn limit label data first develop novel dataset call ctf early covid nineteen twitter fake news additional behavioral test set validate early detection build heterogeneous graph follower followee user tweet tweet retweet connections train graph embed model aggregate propagation information graph embeddings contextual feature constitute endogenous time relative web scrap information constitute exogenous signal endemic train semi supervise fashion overcome challenge limit label data propose co attention mechanism fuse signal representations optimally experimental result ectf politifact gossipcop show endemic highly reliable detect early fake tweet outperform nine state art methods significantly
expressive read consider define attribute oral read fluency comprise prosodic realization phrase prominence context evaluate oral read help establish speaker comprehension text consider label dataset children read record speaker independent detection prominent word use acoustic prosodic lexico syntactic feature previous well tune random forest ensemble predictor replace rnn sequence classifier exploit potential context dependency across longer utterance deep learn apply obtain word level feature low level acoustic contour fundamental frequency intensity spectral shape end end fashion performance comparisons present across different feature type across different feature learn architectures prominent word prediction draw insights wherever possible
develop nlp model traditionally involve two stag train application retention information acquire train application time architecturally limit size model context window case transformers practical difficulties associate long sequence case rnns paper propose novel transformer base updater extractor architecture train procedure work sequence arbitrary length refine knowledge world base linguistic input explicitly train model incorporate incoming information world state representation obtain strong inductive generalization ability handle extremely long range dependencies prove lemma provide theoretical basis approach result also provide insight success failure modes model train variants truncate back propagation time transformer xl empirically investigate model performance three different task demonstrate promise preprint still work progress present focus easily interpretable task leave application propose ideas practical nlp applications future
although automatic speech recognition asr systems achieve significantly improvements recent years speak language recognition error occur easily spot human be various language model techniques develop post recognition task like semantic correction paper propose transformer base semantic correction method pretrained bart initialization experiment ten thousand hours mandarin speech dataset show character error rate cer effectively reduce two hundred and seventeen relatively compare baseline asr system expert evaluation demonstrate actual improvement model surpass cer indicate
attention base encoder decoder aed model learn implicit internal language model ilm train transcriptions integration external lm train much unpaired text usually lead better performance bayesian interpretation hybrid autoregressive transducer hat suggest divide prior discriminative acoustic model correspond implicit lm similarly hybrid hide markov model approach implicit lm calculate efficiently general yet unclear best methods estimate work compare different approach literature propose several novel methods estimate ilm directly aed model propose methods outperform previous approach also investigate methods suppress ilm mainly decrease capacity aed model limit label context also train aed model together pre exist lm
recent years researchers explore use reinforcement learn rl algorithms key components solution various natural language process task instance algorithms leverage deep neural learn find way conversational systems paper review state art rl methods possible use different problems natural language process focus primarily conversational systems mainly due grow relevance provide detail descriptions problems well discussions rl well suit solve also analyze advantage limitations methods finally elaborate promise research directions natural language process might benefit reinforcement learn
study mask predict tokens unsupervised fashion give rise linguistic structure downstream performance gain recent theories suggest pretrained language model acquire useful inductive bias mask implicitly act cloze reductions downstream task appeal show success random mask strategy use practice explain cloze like mask alone construct cloze like mask use task specific lexicons three different classification datasets show majority pretrained performance gain come generic mask associate lexicon explain empirical success generic mask demonstrate correspondence mask language model mlm objective exist methods learn statistical dependencies graphical model use derive method extract learn statistical dependencies mlms show dependencies encode useful inductive bias form syntactic structure unsupervised parse evaluation simply form minimum span tree imply statistical dependence structure outperform classic method unsupervised parse five thousand, eight hundred and seventy-four vs five thousand, five hundred and ninety-one uuas
traditional corpus level evaluation metrics machine translation mt correlate well fluency struggle reflect adequacy model base mt metrics train segment level human judgments emerge attractive replacement due strong correlation result model however require potentially expensive train new domains languages furthermore decisions inherently non transparent appear reflect unwelcome bias explore simple type base classifier metric macrof1 study applicability mt evaluation find macrof1 competitive direct assessment outperform others indicate downstream cross lingual information retrieval task performance show macrof1 use effectively compare supervise unsupervised neural machine translation reveal significant qualitative differences methods output
major focus recent research speak language understand slu end end approach single model predict intents directly speech input without intermediate transcripts however approach present challenge first since speech consider personally identifiable information case automatic speech recognition asr transcripts accessible second intent label speech data scarce address first challenge propose novel system predict intents flexible type input speech asr transcripts demonstrate strong performance either modality separately speech asr transcripts available system combination achieve better result use single input modality address second challenge leverage semantically robust pre train bert model adopt cross modal system co train text embeddings acoustic embeddings share latent space enhance system utilize acoustic module pre train librispeech domain adapt text module target datasets experiment show significant advantage pre train fine tune strategies result system achieve competitive intent classification performance snip slu fluent speech command datasets
probe diagnostic classification become popular strategy investigate whether give set intermediate feature present representations neural model naive probe study may mislead result various recent work suggest reliable methodologies compensate possible pitfalls probe however best practice numerous fast evolve simplify process run set probe experiment line suggest methodologies introduce probe ably extendable probe framework support automate application probe methods user input
relational knowledge base kbs establish tool world knowledge representation machine advantageous precision interpretability usually sacrifice data model flexibility advantage adhere manually engineer schema review take natural language process perspective limitations kbs examine may address part train neural contextual language model lms internalize express relational knowledge free text form propose novel taxonomy relational knowledge representation contextual lms base level kb supervision provide consider work probe lms implicit relational knowledge acquire self supervise pretraining unstructured text alone work explicitly supervise lms level kb entities relations conclude lms kbs complementary representation tool kbs provide high standard factual precision turn flexibly expressively model lms provide suggestions future research direction
grow deluge scientific publications demand text analysis tool help scientists policy makers navigate forecast beneficially guide scientific research recent advance natural language understand drive deep transformer network offer new possibilities map science surface text take multiple sometimes contradictory specialize sense across distinct research communities sensitivity context critical infometric applications transformer embed model bert capture shade association connotation vary across different linguistic contexts particular word span text report procedure encode scientific document tool measure improvement static word embeddings nearest neighbor retrieval task find discriminability contextual representations strongly influence choice pool strategy summarize high dimensional network activations importantly note fundamentals domain match train data important state art nlp tool yet state art model offer significant gain best approach investigate combine domain match pretraining sound pool state art deep transformer network encoders finally goal leverage contextual representations deep encoders present range measurements understand forecast research communities science
pronunciation one fundamentals language learn consider primary factor speak language come understand understand others persistent presence high error rat speech recognition domains result mispronunciations motivate us find alternative techniques handle mispronunciations study develop mispronunciation assessment system check pronunciation non native english speakers identify commonly mispronounce phonemes italian learners english present evaluation non native pronunciation observe phonetically annotate speech corpora work detect mispronunciations use phone base asr implement use kaldi use two non native english label corpora corpus italian adults contain five thousand, eight hundred and sixty-seven utterances forty-six speakers ii corpus italian children consist five thousand, two hundred and sixty-eight utterances seventy-eight children result show select error model discriminate correct sound incorrect sound native nonnative speech therefore use detect pronunciation errors non native speech phone error rat show improvement use error language model asr system show better accuracy apply error model select corpora
answer complex question people seamlessly combine information visual textual tabular source interest model reason multiple piece evidence surge recent years relatively little work question answer model reason across multiple modalities paper present multimodalqammqa challenge question answer dataset require joint reason text table image create mmqa use new framework generate complex multi modal question scale harvest table wikipedia attach image text paragraph use entities appear table define formal language allow us take question answer single modality combine generate cross modal question last crowdsourcing workers take automatically generate question rephrase fluent language create twenty-nine thousand, nine hundred and eighteen question procedure empirically demonstrate necessity multi modal multi hop approach solve task multi hop model implicitdecomp achieve average f1of five hundred and seventeen cross modal question substantially outperform strong baseline achieve three hundred and eighty-two f1 still lag significantly behind human performance nine hundred and one f1
thesis address data scarcity limitations linguistic theory propose language agnostic multi task train methods first introduce meta learn base approach meta transfer learn information judiciously extract high resource monolingual speech data code switch domain meta transfer learn quickly adapt model code switch task number monolingual task learn learn multi task learn fashion second propose novel multilingual meta embeddings approach effectively represent code switch data acquire useful knowledge learn languages learn commonalities closely relate languages leverage lexical composition method far efficient compare contextualized pre train multilingual model third introduce multi task learn integrate syntactic information transfer learn strategy language model learn code switch alleviate aforementioned issue propose data augmentation method use pointer gen neural network use copy mechanism teach model code switch point monolingual parallel sentence disentangle need linguistic theory model capture code switch point attend input word align parallel word without require word alignments constituency parsers importantly model effectively use languages syntactically different outperform linguistic theory base model
conventional approach improve performance end end speech translation e2e st model leverage source transcription via pre train joint train automatic speech recognition asr neural machine translation nmt task however since input modalities different difficult leverage source language text successfully work focus sequence level knowledge distillation seqkd external text base nmt model leverage full potential source language information propose backward seqkd seqkd target source backward nmt model end train bilingual e2e st model predict paraphrase transcriptions auxiliary task single decoder paraphrase generate translations bitext via back translation propose bidirectional seqkd seqkd forward backward nmt model combine experimental evaluations autoregressive non autoregressive model show seqkd direction consistently improve translation performance effectiveness complementary regardless model capacity
assess effectiveness medical intervention researchers must conduct time intensive highly manual literature review nlp systems help automate assist part expensive process support goal release ms2 multi document summarization medical study dataset 470k document 20k summaries derive scientific literature dataset facilitate development systems assess aggregate contradictory evidence across multiple study first large scale publicly available multi document summarization dataset biomedical domain experiment summarization system base bart promise early result formulate summarization input target free text structure form modify recently propose metric assess quality system generate summaries data model available https githubcom allenai ms2
use conversational assistants search information become increasingly popular among general public push research towards advance sophisticate techniques last years particular interest conversational search increase generalization conversational assistants also conversational search step forward allow natural interaction system work focus explore context present conversation via historical utterances respective embeddings aim develop conversational search system help people search information natural way particular system must able understand context question pose track current state conversation detect mention previous question answer achieve use context track component base neural query rewrite model another crucial aspect system provide relevant answer give question conversational history achieve objective use transformer base rank method expand architecture use conversational context result obtain system develop show advantage use context present natural language utterances neural embeddings generate throughout conversation
work study hallucinations neural machine translation nmt lie extreme end spectrum nmt pathologies firstly connect phenomenon hallucinations source perturbation long tail theory feldman two thousand and twenty present empirically validate hypothesis explain hallucinations source perturbation secondly consider hallucinations corpus level noise without source perturbation demonstrate two prominent type natural hallucinations detach oscillatory output could generate explain specific corpus level noise pattern finally elucidate phenomenon hallucination amplification popular data generation process backtranslation sequence level knowledge distillation
paper propose novel voice conversion vc method base non autoregressive sequence sequence nar s2s model inspire great success nar s2s model fastspeech text speech tts extend fastspeech2 model vc problem introduce convolution augment transformer conformer instead transformer make possible capture local global context information input sequence furthermore extend variance predictors variance converters explicitly convert source speaker prosody components pitch energy target speaker experimental evaluation japanese speaker dataset consist male female speakers one thousand utterances demonstrate propose model enable us perform stable faster better conversion autoregressive s2s ar s2s model tacotron2 transformer
semantic information sentence crucial improve expressiveness text speech tts system well learn limit train tts dataset virtue nowadays encoder structure large scale pre train text representation develop bidirectional encoder representations transformers bert prove embody text context semantic information apply tts additional input however bert explicitly associate semantic tokens point dependency relations sentence paper enhance expressiveness propose semantic representation learn method base graph neural network consider dependency relations sentence dependency graph input text compose edge dependency tree structure consider forward reverse directions semantic representations extract word level relational gate graph network rggn feed feature bert nod input upsampled semantic representations character level embeddings concatenate serve encoder input tacotron two experimental result show propose method outperform baseline use vanilla bert feature ljspeech blizzard challenge two thousand and thirteen datasets semantic representations learn reverse direction effective enhance expressiveness
counterfactual statements describe events take place consider problem counterfactual detection cfd product review purpose annotate multilingual cfd dataset amazon product review cover counterfactual statements write english german japanese languages dataset unique contain counterfactuals multiple languages cover new application area e commerce review provide high quality professional annotations train cfd model use different text representation methods classifiers find model robust selectional bias introduce due cue phrase base sentence selection moreover cfd dataset compatible prior datasets merge learn accurate cfd model apply machine translation english counterfactual examples create multilingual data perform poorly demonstrate language specificity problem ignore far
tsetlin machine tm interpretable pattern recognition algorithm base propositional logic algorithm demonstrate competitive performance many natural language process nlp task include sentiment analysis text classification word sense disambiguation wsd obtain human level interpretability legacy tm employ boolean input feature bag word bow however bow representation make difficult use pre train information instance word2vec glove word representations restriction constrain performance tm compare deep neural network dnns nlp reduce performance gap paper propose novel way use pre train word representations tm approach significantly enhance tm performance maintain interpretability time achieve extract semantically relate word pre train word representations input feature tm experiment show accuracy propose approach significantly higher previous bow base tm reach level dnn base model
number databases well size complexity increase create barrier use especially non experts come grip nature data way represent database specific query languages user interfaces data access difficulties worsen research settings common work many different databases one approach improve situation allow users pose query natural language work describe machine learn framework polyglotter general way support map natural language search database query importantly require creation manually annotate data train therefore apply easily multiple domains framework polyglot sense support multiple different database engines access variety query languages include sql cypher furthermore polyglotter also support multi class query result indicate framework perform well synthetic real databases may provide opportunities database maintainers improve accessibility resources
understand narrative text require capture character motivations goals mental state paper propose entity base narrative graph eng model internal state character story explicitly model entities interactions context appear learn rich representations experiment different task adaptive pre train objectives domain train symbolic inference capture dependencies different decisions output space evaluate model two narrative understand task predict character mental state desire fulfillment conduct qualitative analysis
intelligent agents communicate accomplish share goals goals shape agents language study dynamics learn latent language policies llps instructor agents generate natural language subgoal descriptions executor agents map descriptions low level action llps solve challenge long horizon reinforcement learn problems provide rich model study task orient language use previous work find llp train prone semantic drift use message ways inconsistent original natural language mean demonstrate theoretically empirically multitask train effective counter problem prove multitask train eliminate semantic drift well study family signal game show multitask train neural llps complex strategy game reduce drift improve sample efficiency
introduce techtrack new dataset track entities technical procedures dataset prepare annotate open domain article wikihow consist one thousand, three hundred and fifty-one procedures eg connect printer identify one thousand, two hundred unique entities average forty-seven entities per procedure evaluate performance state art model entity track task find well human annotation performance describe techtrack use take forward research understand procedures temporal texts
track entities throughout procedure describe text challenge due dynamic nature world describe process firstly propose formulate task question answer problem enable us use pre train transformer base language model qa benchmarks adapt procedural text understand secondly since transformer base language model encode flow events propose time stamp language modeltslm model encode event information lms architecture introduce timestamp encode model evaluate propara dataset show improvements publish state art result thirty-one increase f1 score moreover model yield better result location prediction task npn cook dataset result indicate approach effective procedural text understand general
natural languages commonly display trade among different strategies convey constituent roles similar trade however observe recent simulations iterate language learn neural network base agents chaabouni et al 2019b work evaluate result light two important factor namely lack effort base pressure agents lack variability initial input language
multilingual transformer improve parameter efficiency crosslingual transfer effectively train multilingual model well study use multilingual machine translation testbed study optimization challenge loss landscape parameter plasticity perspectives find imbalanced train data pose task interference high low resource languages characterize nearly orthogonal gradients major parameters optimization trajectory mostly dominate high resource show local curvature loss surface affect degree interference exist heuristics data subsampling implicitly reduce sharpness although still face trade high low resource languages propose principled multi objective optimization algorithm curvature aware task scale cat improve optimization generalization especially low resource experiment ted wmt opus one hundred benchmarks demonstrate cat advance pareto front accuracy efficient apply massive multilingual settings scale one hundred languages
large language model la bert use ubiquitously nlp pretraining consider luxury well fund industry labs afford one train model modest budget present recipe pretraining mask language model twenty-four hours use eight low range 12gb gpus demonstrate combination software optimizations design choices hyperparameter tune possible produce model competitive bert base glue task fraction original pretraining cost
large transformers pretrained clinical note electronic health record ehr afford substantial gain performance predictive clinical task cost train model necessity data access couple utility motivate parameter share ie release pretrained model clinicalbert efforts use deidentified ehr many researchers access large set sensitive non deidentified ehr might train bert model similar would safe release weight model work design battery approach intend recover personal health information phi train bert specifically attempt recover patient name condition associate find simple probe methods able meaningfully extract sensitive information bert train mimic iii corpus ehr however sophisticate attack may succeed facilitate research make experimental setup baseline probe model available https githubcom elehman16 exposingpatientdatarelease
personalization active learn key aspects successful learn aspects important address intelligent educational applications help systems adapt close gap students vary abilities become increasingly important context online distance learn run comparative head head study learn outcomes two popular online learn platforms platform follow traditional model deliver content series lecture videos multiple choice quiz platform b create personalize learn environment provide problem solve exercise personalize feedback report result study use pre post assessment quiz participants take course introductory data science topic two platforms observe statistically significant increase learn outcomes platform b highlight impact well design well engineer technology support active learn problem base learn online education moreover result self assessment questionnaire participants report perceive learn gain suggest participants use platform b improve metacognition
health outcome measurement observation use capture assess effect treatment automatic detection health outcomes text would undoubtedly speed access evidence necessary healthcare decision make prior work outcome detection model task either sequence label task goal detect text span describe health outcomes b classification task goal classify text pre define set categories depend outcome mention somewhere text however decouple span detection classification problematic model perspective ignore global structural correspondences sentence level word level information present give text propose method use word level sentence level information simultaneously perform outcome span detection outcome type classification addition inject contextual information hide vectors use label attention appropriately weight word level sentence level information experimental result several benchmark datasets health outcome detection show model consistently outperform decouple methods report competitive result
recent work show commonly available machine read comprehension mrc datasets use train high performance neural information retrieval ir systems however evaluation neural ir far limit standard supervise learn settings outperform traditional term match baselines conduct domain domain evaluations neural ir seek improve robustness across different scenarios include zero shoot settings show synthetic train examples generate use sequence sequence generator effective towards goal experiment pre train synthetic examples improve retrieval performance domain domain evaluation five different test set
grow polarization news media blame fan disagreement controversy even violence early identification polarize topics thus urgent matter help mitigate conflict however accurate measurement polarization still open research challenge address gap propose partisanship aware contextualized topic embeddings pacte method automatically detect polarize topics partisan news source specifically represent ideology news source topic corpus contextualized topic embed utilize language model finetuned recognize partisanship news article measure polarization source use cosine similarity apply method corpus news covid nineteen pandemic extensive experiment different news source topics demonstrate effectiveness method precisely capture topical polarization alignment different news source help clarify validate result explain polarization use moral foundation theory
end end automatic speech recognition asr model commonly train speak utterances use optimization methods like stochastic gradient descent sgd distribute settings like federate learn model train require transmission gradients network work design first method reveal identity speaker train utterance access gradient propose hessian free gradients match input reconstruction technique operate without second derivatives loss function require prior work expensive compute show effectiveness method use deepspeech model architecture demonstrate possible reveal speaker identity thirty-four top one accuracy fifty-one top five accuracy librispeech dataset study effect two well know techniques differentially private sgd dropout success method show dropout rate two reduce speaker identity accuracy zero top one five top five
neural module network nmn achieve success image ground task visual question answer vqa synthetic image however limit work nmn study video ground language task task extend complexity traditional visual task additional visual temporal variance motivate recent nmn approach image ground task introduce video ground neural module network vgnmn model information retrieval process video ground language task pipeline neural modules vgnmn first decompose language components explicitly resolve entity reference detect correspond action base input question detect entities action use parameters instantiate neural module network extract visual cue video experiment show vgnmn achieve promise performance two video ground language task video qa video ground dialogues
pretrained mask language model mlms revolutionise nlp recent years however previous work indicate shelf mlms effective universal lexical sentence encoders without task specific fine tune nli sentence similarity paraphrase task use annotate task data work demonstrate possible turn mlms effective universal lexical sentence encoders even without additional data without supervision propose extremely simple fast effective contrastive learn technique term mirror bert convert mlms eg bert roberta encoders less minute without additional external knowledge mirror bert rely fully identical slightly modify string pair positive ie synonymous fine tune examples aim maximise similarity identity fine tune report huge gain shelf mlms mirror bert lexical level sentence level task across different domains different languages notably standard sentence semantic similarity sts task self supervise mirror bert model even match performance task tune sentence bert model prior work finally delve deeper inner work mlms suggest evidence simple approach yield effective univeral lexical sentence encoders
contextualized entity representations learn state art deep learn model bert gpt t5 etc leverage attention mechanism learn data context however model still blind leverage knowledge context present knowledge graph knowledge context understand semantics entities relationship neighbor entities knowledge graph propose novel effective technique infuse knowledge context knowledge graph conceptual ambiguous entities model base transformer architecture novel technique project knowledge graph embed homogeneous vector space introduce new token type entities align entity position ids selective attention mechanism take bert baseline model implement knowledgeinfused bert infuse knowledge context conceptnet wordnet significantly outperform bert wide range nlp task eight different glue datasets ki bert base model even outperform bert large domain specific task like scitail academic subsets qqp qnli mnli
factual knowledge acquire pretraining store parameters language model lm useful downstream task eg question answer textual inference however facts incorrectly induce become obsolete time present knowledgeeditor method use edit knowledge thus fix bug unexpected predictions without need expensive train fine tune besides computationally efficient knowledgeeditor require modifications lm pre train eg use meta learn approach train hyper network constrain optimization modify fact without affect rest knowledge train hyper network use predict weight update test time show knowledgeeditor efficacy two popular architectures knowledge intensive task bert model fine tune fact check ii sequence sequence bart model question answer method change prediction specific word query tend result consistent change predictions also paraphrase show encourage exploit eg automatically generate paraphrase train interestingly hyper network regard probe reveal components model need change manipulate factual knowledge analysis show update tend concentrate small subset components code https githubcom nicola decao knowledgeeditor
twitter view data source natural language process nlp task continuously update data stream twitter make challenge trace real time topic evolution paper propose framework model fuzzy transition topic cluster extend previous work crisp cluster transition incorporate fuzzy logic order enrich underlie structure identify framework apply methodology computer generate cluster nouns tweet human tweet annotations obtain fuzzy transition compare crisp transition computer generate cluster human label topic set
humans ability rapidly understand rich combinatorial concepts limit data investigate ability context auditory signal evolve cultural transmission experiment study emergence combinatorial structure language propose neuro symbolic generative model combine strengths previous approach concept learn model perform fast inference draw neural network methods still retain interpretability generalization limit data see structure generative approach model outperform purely neural network base approach classification evaluate grind truth human experimental classification preferences produce superior reproductions observe signal well result demonstrate power flexible combine neural symbolic architectures human like generalization raw perceptual domains offer step towards develop precise computational model inductive bias language evolution
vision model train multimodal datasets benefit wide availability large image caption datasets recent model clip find generalize well zero shoot transfer learn settings could imply linguistic semantic ground confer additional generalization abilities visual feature space systematically evaluate various multimodal architectures vision model term unsupervised cluster shoot learn transfer learn adversarial robustness set multimodal train produce additional generalization capability compare standard supervise visual train conclude work still require semantic ground help improve vision model
document layout comprise structural visual eg font size information vital often ignore machine learn model exist model use layout information consider textual content overlook existence content modalities image additionally spatial interactions present content layout never really fully exploit bridge gap parse document content block eg text table image propose novel layout aware multimodal hierarchical framework lampret model block whole document lampret encode block multimodal transformer lower level aggregate block level representations connections utilize specifically design transformer higher level design hierarchical pretraining objectives lower level model train similarly multimodal ground model higher level model train propose novel layout aware objectives evaluate propose model two layout aware task text block fill image suggestion show effectiveness propose hierarchical architecture well pretraining techniques
paper introduce high quality open source speech synthesis dataset kazakh low resource language speak thirteen million people worldwide dataset consist ninety-three hours transcribe audio record speak two professional speakers female male first publicly available large scale dataset develop promote kazakh text speech tts applications academia industry paper share experience describe dataset development procedures face challenge discuss important future directions demonstrate reliability dataset build baseline end end tts model evaluate use subjective mean opinion score mos measure evaluation result show best tts model train dataset achieve mos four speakers make applicable practical use dataset train recipe pretrained tts model freely available
propose promise neural network model acquire ground representation robot action linguistic descriptions thereof properly respond various linguistic expressions include polysemous word important ability robots interact people via linguistic dialogue previous study show robots use word include action description pair datasets use pre train word embeddings however word embeddings train distributional hypothesis ground derive purely text corpus letter transform pre train word embeddings embody ones use robot sensory motor experience extend bidirectional translation model action descriptions incorporate non linear layer retrofit word embeddings train retrofit layer bidirectional translation model alternately propose model able transform pre train word embeddings adapt pair action description dataset result demonstrate embeddings synonyms form semantic cluster reflect experience action environments robot embeddings allow robot properly generate action unseen word pair action dataset
though word embeddings topics complementary representations several past work use pretrained word embeddings neural topic model address data sparsity short text small collection document work present novel neural topic model framework use multi view embed space one pretrained topic embeddings two pretrained word embeddings context insensitive glove context sensitive bert model jointly one many source improve topic quality better deal polysemy first build respective pool pretrained topic ie topicpool word embeddings ie wordpool identify one relevant source domains transfer knowledge guide meaningful learn sparse target domain within neural topic model quantify quality topics document representations via generalization perplexity interpretability topic coherence information retrieval ir use short text long text small large document collections news medical domains introduce multi source multi view embed space show state art neural topic model use six source high resource five target low resource corpora
code switch communication phenomenon speakers switch different languages conversation widespread adoption conversational agents chat platforms code switch become integral part write conversations many multi lingual communities worldwide make essential develop techniques summarize understand conversations towards objective introduce abstractive summarization hindi english code switch conversations develop first code switch conversation summarization dataset gupshup contain six thousand, eight hundred and thirty-one conversations hindi english correspond human annotate summaries english hindi english present detail account entire data collection annotation process analyze dataset use various code switch statistics train state art abstractive summarization model report performances use automate metrics human evaluation result show multi lingual mbart multi view seq2seq model obtain best performances new dataset
natural language process nlp information retrieval ir judicial domain essential task advent availability domain specific data electronic form aid different artificial intelligence ai technologies automate language process become comfortable hence become feasible researchers developers provide various automate tool legal community reduce human burden competition legal information extraction entailment coliee two thousand and nineteen run association international conference artificial intelligence law icail two thousand and nineteen come challenge task share define four sub task ie task1 task2 task3 task4 able provide automate systems judicial system paper present work note experiment carry part participation sub task define share task make use different information retrievalir deep learn base approach tackle problems obtain encourage result four sub task
neural ir model often study homogeneous narrow settings considerably limit insights generalization capabilities address allow researchers broadly establish effectiveness model introduce beir benchmarking ir heterogeneous benchmark information retrieval leverage careful selection seventeen datasets evaluation span diverse retrieval task include open domain datasets well narrow expert domains study effectiveness nine state art retrieval model zero shoot evaluation setup beir find perform well consistently across datasets challenge result show bm25 robust baseline reranking base model overall achieve best zero shoot performances however high computational cost contrast dense retrieval model computationally efficient often underperform approach highlight considerable room improvement generalization capabilities work extensively analyze different retrieval model provide several suggestions believe may useful future work beir datasets code available https githubcom ukplab beir
introduce distribute nli new nlu task goal predict distribution human judgements natural language inference show model capture human judgement distribution apply additional distribution estimation methods namely monte carlo mc dropout deep ensemble calibration distribution distillation four methods substantially outperform softmax baseline show mc dropout able achieve decent performance without distribution annotations calibration give substantial improvements extra distribution annotations provide suggest value multiple annotations example model distribution human judgements moreover mc dropout calibration achieve decent transfer performance domain data despite improvements best result still far estimate human upper bind indicate task predict distribution human judgements still open challenge problem large room future improvements showcase common errors mc dropout calibration finally give guidelines usage methods different level data availability encourage future work model human opinion distribution language reason
relationship linguistic dependencies statistical dependence build earlier work nlp cognitive science study question introduce contextualized version pointwise mutual information cpmi use pretrained language model estimate probabilities word context extract dependency tree maximize cpmi compare result structure gold dependencies overall find maximum cpmi tree correspond linguistic dependencies often tree extract non contextual pmi estimate roughly often simple baseline form connect adjacent word also provide evidence extent two kinds dependency align explain distance word category dependency relation finally analysis shed light differences large pretrained language model specifically kinds inductive bias encode
entity link important problem many applications previous solutions design settings annotate train data available however case numerous domains propose light weight scalable entity link method eigenthemes rely solely availability entity name referent knowledge base eigenthemes exploit fact entities truly mention document gold entities tend form semantically dense subset set candidate entities document geometrically speak represent entities vectors via give embed gold entities tend lie low rank subspace full embed space eigenthemes identify subspace use singular value decomposition score candidate entities accord proximity subspace empirical front introduce multiple strong baselines compare favorably exist state art extensive experiment benchmark datasets variety real world domains showcase effectiveness approach
introduce data set call dch two contain four thousand, three hundred and ninety real customer helpdesk dialogues chinese english translations dch two also contain dialogue level annotations turn level annotations obtain independently either nineteen twenty annotators data set build effort organisers ntcir fourteen short text conversation ntcir fifteen dialogue evaluation task help researchers understand constitute effective customer helpdesk dialogue thereby build efficient helpful helpdesk systems available customers time addition dch two may utilise purpose example repository retrieval base dialogue systems parallel corpus machine translation helpdesk domain
often challenge system solve new complex problem scratch much easier system access similar problems description solutions paradigm know case base reason cbr propose neuro symbolic cbr approach question answer large knowledge base cbr kbqa idea cbr tempt compose solution case nontrivial individual case contain partial logic full solution resolve cbr kbqa consist two modules non parametric memory store case question logical form parametric model generate logical form retrieve relevant case memory experiment show cbr kbqa effectively derive novel combination relations present case memory require answer compositional question several kbqa datasets test compositional generalization cbr kbqa achieve competitive performance example challenge complexwebquestions dataset cbr kbqa outperform current state art eleven accuracy furthermore show cbr kbqa capable use new case emphwithout train incorporate human label examples non parametric case memory cbr kbqa able successfully generate query contain unseen kb relations
enable nlp model appropriately respond instructional prompt consequently generalize new task study question leverage exist nlp datasets instructions use crowdsource create natural instructions dataset instructions task specific input output data dataset consist sixty-one distinct language instructions 600k task instance use evaluate exist state art language model lms address new task shoot prompt gpt3 fine tune bart analysis indicate exist model indeed benefit instructions hence show improve generalization new task b model like gpt three generally benefit instructions extent gain vary across different field instructions also depend task solve c generalization unseen task natural instructions remain far perfect state art indicate significant room progress direction
rumor often associate newly emerge events thus ability deal unseen rumor crucial rumor veracity classification model previous work address issue improve model generalizability assumption model stay unchanged even new outbreak event work propose alternative solution continuously update model accordance dynamics rumor domain creations biggest technical challenge associate new approach catastrophic forget previous learn due new learn adopt continual learn strategies control new learn avoid catastrophic forget propose additional strategy jointly use strengthen forget alleviation
paper propose new domain adaptation method call textitback train superior alternative self train self train result synthetic train data form quality input align noisy output back train result noisy input align quality output experimental result unsupervised domain adaptation question generation passage retrieval model textitnatural question domain machine learn domain show back train outperform self train large margin ninety-three bleu one point generation seventy-nine accuracy point top one retrieval release textitmlquestions domain adaptation dataset machine learn domain contain 50k unaligned passages 35k unaligned question 3k align passage question pair data code available https githubcom mcgill nlp mlquestions
develop novel approach confidently accelerate inference large expensive multilayer transformers ubiquitous natural language process nlp amortize approximate computational methods increase efficiency come unpredictable performance cost work present cat confident adaptive transformers simultaneously increase computational efficiency guarantee specifiable degree consistency original model high confidence method train additional prediction head top intermediate layer dynamically decide stop allocate computational effort input use meta consistency classifier calibrate early prediction stop rule formulate unique extension conformal prediction demonstrate effectiveness approach four classification regression task
knowledge graph completion kgc predict miss facts incomplete knowledge graph almost exist kgc research applicable one kg time one language however different language speakers may maintain separate kgs language individual kg expect complete moreover common entities relations kgs different surface form ids lead id proliferation entity alignment ea relation alignment ra task resolve recognize pair entity relation ids different kgs represent entity relation help prediction miss facts since knowledge one kg likely benefit completion another high confidence predictions may also add valuable information alignment task response study novel task jointly train multilingual kgc relation alignment entity alignment model present alignkgc use seed alignments jointly optimize three kgc ea ra losses key component alignkgc embed base soft notion asymmetric overlap define subject object set signatures relations aid better predict relations equivalent imply relations extensive experiment dbpedia five languages establish benefit joint train task achieve ten thirty-two mrr improvements alignkgc strong state art single kgc system completion model monolingual kg alignkgc achieve reasonable gain ea ra task vanilla completion model kg combine facts without alignment underscore value joint train task
determine coreference concept mention across multiple document fundamental natural language understand work cross document coreference resolution cdcr typically consider mention events news often involve abstract technical concepts prevalent science technology complex concepts take diverse ambiguous form many hierarchical level granularity eg task subtasks pose challenge cdcr present new task hierarchical cdcr concepts scientific paper goal jointly infer coreference cluster hierarchy create scico expert annotate dataset task 3x larger prominent ecb resource find tackle coreference hierarchy outperform disjoint model hope spur development joint model scico
increase concern regulations data privacy necessitate study privacy preserve methods natural language process nlp applications federate learn fl provide promise methods large number clients ie personal devices organizations collaboratively learn share global model benefit clients allow users keep data locally facilitate fl research nlp present fednlp research platform federate learn nlp fednlp support various popular task formulations nlp text classification sequence tag question answer seq2seq generation language model also implement interface transformer language model eg bert fl methods eg fedavg fedopt etc distribute train evaluation protocol interface support comprehensive collection non iid partition strategies preliminary experiment fednlp reveal exist large performance gap learn decentralize centralize datasets open intrigue excite future research directions aim develop fl methods suit nlp task
increase polarization online political discourse call computational tool able automatically detect monitor ideological divide social media introduce minimally supervise method directly leverage network structure online discussion forums specifically reddit detect polarize concepts model polarization along dimension agenda set frame draw upon insights moral psychology architecture propose combine graph neural network structure sparsity result representations concepts subreddits capture phenomena ideological radicalization subreddit hijack also create new dataset political discourse cover twelve years six hundred online group different ideologies
sparse regression recently apply enable transfer learn limit data study extension approach unsupervised learn particular learn word embeddings unstructured text corpora use low rank matrix factorization intuitively transfer word embeddings new domain expect embeddings change small number word eg ones novel mean domain propose novel group sparse penalty exploit sparsity perform transfer learn little text data available target domain eg single article text prove generalization bound algorithm furthermore empirically evaluate effectiveness term prediction accuracy downstream task well interpretability result
trend deploy digital systems numerous industries induce hike record digital information health sector observe large adoption digital devices systems generate large volumes personal medical health record electronic health record contain valuable information retrospective prospective analysis often entirely exploit dense information storage crude purpose condense health record select information hold characteristics original document base report disease summaries may boost diagnosis extend doctor interaction time patient high workload situation like covid nineteen pandemic paper propose multi head attention base mechanism perform extractive summarization meaningful phrase clinical note method find major sentence summary correlate tokens segment positional embeddings model output attention score statistically transform extract key phrase use projection heat map tool visual human use
answer sentence selection as2 model require annotate data ie hand label question answer pair present strategy collect weakly supervise answer question base reference improve as2 model specifically introduce reference base weak supervision rws fully automatic large scale data pipeline harvest high quality weakly supervise answer abundant web data require question reference pair input study efficacy robustness rws set tanda recent state art fine tune approach specialize as2 experiment indicate produce data consistently bolster tanda achieve state art term p1 nine hundred and one map nine hundred and twenty-nine wikiqa
paper study importance context predict citation worthiness sentence scholarly article formulate problem sequence label task solve use hierarchical bilstm model contribute new benchmark dataset contain two million sentence correspond label preserve sentence order dataset perform document level train test split importantly allow incorporate contextual information model process evaluate propose approach three benchmark datasets result quantify benefit use context contextual embeddings citation worthiness lastly error analysis provide insights case context play essential role predict citation worthiness
topic model successfully use analyze text document however exist topic model many document require train paper propose neural network base shoot learn method learn topic model document neural network model take small number document input output topic model priors propose method train neural network expect test likelihood improve topic model parameters estimate maximize posterior probability use priors base algorithm since step algorithm differentiable propose method backpropagate loss algorithm train neural network expect test likelihood maximize stochastic gradient descent method use set multiple text corpora episodic train framework experiment demonstrate propose method achieve better perplexity exist methods use three real world text document set
build success adress challenge interspeech two thousand and twenty attract participation thirty-four team across world adresso challenge target three difficult automatic prediction problems societal medical relevance namely detection alzheimer dementia inference cognitive test score prediction cognitive decline paper present prediction task detail describe datasets use report result baseline classification regression model develop task combination acoustic linguistic feature extract directly audio record without human intervention yield baseline accuracy seven thousand, eight hundred and eighty-seven ad classification task mmse prediction root mean square rmse error five hundred and twenty-eight six thousand, eight hundred and seventy-five accuracy cognitive decline prediction task
paper present technique interpret visualize intermediate layer cnns train raw speech data unsupervised manner show average feature map relu activation convolutional layer yield interpretable time series data propose technique enable acoustic analysis intermediate convolutional layer uncover meaningful representation speech get encode intermediate layer cnns manipulate individual latent variables marginal level outside train range train probe internal representations two model bare wavegan architecture ciwgan extension force generator output informative data result emergence linguistically meaningful representations interpretation visualization perform three basic acoustic properties speech periodic vibration correspond vowels aperiodic noise vibration correspond fricatives silence correspond stop also argue propose technique allow acoustic analysis intermediate layer parallel acoustic analysis human speech data extract f0 intensity duration formants acoustic properties intermediate layer order test cnns encode various type information model train two speech process different degrees complexity simple presence computationally complex presence reduplication copy material observe causal effect interpolation result change intermediate layer reveal individual variables get transform spike activation intermediate layer use propose technique analyze linguistically meaningful units speech get encode different convolutional layer
pre train language model recently advance abstractive summarization model fine tune human write reference summary generation test time work propose first application transductive learn summarization paradigm model learn test set input inference perform transduction propose utilize input document summarize sentence construct reference learn test time sentence often compress fuse form abstractive summaries provide omit detail additional context reader show approach yield state art result cnn dm nyt datasets instance achieve one rouge l point improvement cnn dm show benefit transduction older recent news finally human automatic evaluation show summaries become abstractive coherent
latent alignment objectives ctc axe significantly improve non autoregressive machine translation model improve autoregressive model well explore possibility train autoregressive machine translation model latent alignment objectives observe practice approach result degenerate model provide theoretical explanation empirical result prove latent alignment objectives incompatible teacher force
introduce two methods improve performance agents meet first time accomplish communicative task methods one message mutation generation communication protocol two random permutations communication channel proposals test use simple two player game involve teacher generate communication protocol send message student interpret message train multiple agents via self play analyse performance agents match stranger ie zero shoot communication performance find message mutation channel permutation positively influence performance discuss effect
communication cooperative effort require reach mutual understand among participants humans use commonsense reason implicitly produce natural logically coherent responses step towards fluid human ai communication study response generation rg model emulate human reason process use common sense help produce better quality responses aim tackle two research question formalize conversational common sense examine rg model capability use common sense first propose task cedar causal common sense dialogue response generation concretize common sense textual explanations might lead response evaluate rg model behavior compare model loss give valid explanation invalid one introduce process automatically generate explanations ask humans verify finally design two probe settings rg model target two reason capabilities use verify explanations find rg model hard time determine logical validity explanations identify grammatical naturalness explanation easily
vision language navigation task require agent navigate 3d environment base natural language instructions one key challenge task grind instructions current visual information agent perceive exist work employ soft attention individual word locate instruction require next action however different word different function sentence eg modifiers convey attribute verbs convey action syntax information like dependencies phrase structure aid agent locate important part instruction hence paper propose navigation agent utilize syntax information derive dependency tree enhance alignment instruction current visual scenes empirically agent outperform baseline model use syntax information room room dataset especially unseen environment besides agent achieve new state art room across room dataset contain instructions three languages english hindi telugu also show agent better align instructions current visual information via qualitative visualizations code model https githubcom jialuli luka syntaxvln
major depressive disorder mdd prevalent psychiatric disorder associate significant healthcare burden worldwide phenotyping mdd help early diagnosis consequently may significant advantage patient management prior research mdd phenotypes extract structure electronic health record ehr use electroencephalographic eeg data traditional machine learn model predict mdd phenotypes however mdd phenotypic information also document free text ehr data clinical note clinical note may provide accurate phenotyping information natural language process nlp algorithms must develop abstract information recent advancements nlp result state art neural language model bidirectional encoder representations transformers bert model transformer base model pre train corpus unsupervised text data fine tune specific task however neural language model underutilized clinical nlp task due lack large train datasets literature researchers utilize distant supervision paradigm train machine learn model clinical text classification task mitigate issue lack annotate train data still unknown whether paradigm effective neural language model paper propose leverage neural language model distant supervision paradigm identify mdd phenotypes clinical note experimental result indicate propose approach effective identify mdd phenotypes bio clinical bert specific bert model clinical data achieve best performance comparison conventional machine learn model
present skweak versatile python base software toolkit enable nlp developers apply weak supervision wide range nlp task weak supervision emerge machine learn paradigm base simple idea instead label data point hand use label function derive domain knowledge automatically obtain annotations give dataset result label aggregate generative model estimate accuracy possible confusions label function skweak toolkit make easy implement large spectrum label function heuristics gazetteers neural model linguistic constraints text data apply corpus aggregate result fully unsupervised fashion skweak especially design facilitate use weak supervision nlp task text classification sequence label illustrate use skweak ner sentiment analysis skweak release open source license available https githubcom norskregnesentral skweak
transformer architecture deeply change natural language process outperform previous state art model however well know transformer model like bert roberta gpt two require huge compute budget create high quality contextualised representation paper study several efficient pre train objectives transformers base model test objectives different task determine electra model new feature relevant confirm transformers pre train improve input contain mask tokens usage whole output compute loss reduce train time moreover inspire electra study model compose two block discriminator simple generator base statistical model impact computational performances besides prove eliminate mask token consider whole output loss computation essential choices improve performance furthermore show possible efficiently train bert like model use discriminative approach electra without complex generator expensive finally show electra benefit heavily state art hyper parameters search
text speech tts widely use synthesize personal voice target speaker well train source tts model fine tune pair adaptation data speech transcripts target speaker however many scenarios untranscribed speech data available adaptation bring challenge previous tts adaptation pipelines eg adaspeech paper develop adaspeech two adaptive tts system leverage untranscribed speech data adaptation specifically introduce mel spectrogram encoder well train tts model conduct speech reconstruction time constrain output sequence mel spectrogram encoder close original phoneme encoder adaptation use untranscribed speech data speech reconstruction fine tune tts decoder adaspeech two two advantage one pluggable system easily apply exist train tts model without train two effective system achieve par voice quality transcribe tts adaptation eg adaspeech amount untranscribed data achieve better voice quality previous untranscribed adaptation methods synthesize speech sample find https speechresearchgithubio adaspeech2
position encode transformer architecture provide supervision dependency model elements different position sequence investigate various methods encode positional information transformer base language model propose novel implementation name rotary position embeddingrope propose rope encode absolute positional information rotation matrix naturally incorporate explicit relative position dependency self attention formulation notably rope come valuable properties flexibility expand sequence lengths decay inter token dependency increase relative distance capability equip linear self attention relative position encode result enhance transformer rotary position embed roformer achieve superior performance task long texts release theoretical analysis along preliminary experiment result chinese data undergo experiment english benchmark soon update
paper propose strategy assess robustness different machine learn model involve natural language process nlp overall approach rely upon search semantically replace strategy consist two step one search identify important part text two semantically replace find replacements important part constrain replace tokens semantically similar word introduce different type search semantically replace methods design specifically particular type machine learn model also investigate effectiveness strategy provide general framework assess variety machine learn model finally empirical comparison provide robustness performance among three different model type different text representation
indispensable part modern human computer interaction system speech synthesis technology help users get output intelligent machine easily intuitively thus attract attention due limitations high complexity low efficiency traditional speech synthesis technology current research focus deep learn base end end speech synthesis technology powerful model ability simpler pipeline mainly consist three modules text front end acoustic model vocoder paper review research status three part classify compare various methods accord emphasis moreover paper also summarize open source speech corpus english chinese languages use speech synthesis task introduce commonly use subjective objective speech quality evaluation method finally attractive future research directions point
text encode automatic speech recognition asr transcripts audio representations show promise speech emotion recognition ser ever since yet challenge explain effect information stream ser systems clarification require analyse impact asr word error rate wer linguistic emotion recognition per se context fusion acoustic information exploitation age deep asr systems order tackle issue create transcripts original speech apply three modern asr systems include end end model train recurrent neural network transducer loss model connectionist temporal classification loss wav2vec framework self supervise learn afterwards use pre train textual model extract text representations asr output gold standard extraction learn acoustic speech feature utilise opensmile openxbow deepspectrum audeep finally conduct decision level fusion information stream acoustics linguistics use best development configuration achieve state art unweighted average recall value seven hundred and thirty-six seven hundred and thirty-eight speaker independent development test partition iemocap respectively
languages emerge change time population level though interactions individual speakers however hard directly observe single speaker linguistic innovation precipitate population wide change language many theoretical proposals exist introduce general mathematical model encompass wide variety individual level linguistic behaviours provide statistical predictions population level change result model allow us compare likelihood empirically attest change definite indefinite article multiple languages different assumptions way individuals learn use language find account language change appeal primarily errors childhood language acquisition weakly support historical data whereas allow speakers change incrementally across lifespan plausible particularly combine social network effect
natural language process nlp evolve significantly last decade paper highlight important milestones period try pinpoint contribution individual model algorithm overall progress furthermore focus issue still remain solve emphasize groundbreaking proposals transformers bert similar attention base model
recent work entity coreference resolution cr follow current trend deep learn apply embeddings relatively simple task relate feature sota model make use hierarchical representations discourse structure work leverage automatically construct discourse parse tree within neural approach demonstrate significant improvement two benchmark entity coreference resolution datasets explore impact vary depend upon type mention
work create web application highlight output nlp model train parse label discourse segment law text system build primarily journalists legal interpreters mind focus state level law use yous census population number allocate resources organize government system expose corpus collect six thousand state level laws pertain yous census use twenty-five scrapers build crawl state law websites release also build novel flexible annotation framework handle span tag relation tag arbitrary input text document embed simply webpage framework allow journalists researchers add annotation database correct tag new data
introduce theoretical framework understand predict complexity sequence classification task use novel extension theory boolean function sensitivity sensitivity function give distribution input sequence quantify number disjoint subsets input sequence individually change change output argue standard sequence classification methods bias towards learn low sensitivity function task require high sensitivity difficult end show analytically simple lexical classifiers express function bound sensitivity show empirically low sensitivity function easier learn lstms estimate sensitivity fifteen nlp task find sensitivity higher challenge task collect glue simple text classification task sensitivity predict performance simple lexical classifiers vanilla bilstms without pretrained contextualized embeddings within task sensitivity predict input hard simple model result suggest success massively pretrained contextual representations stem part provide representations information extract low sensitivity decoders
zero shoot learn aim recognize unseen object use semantic representations exist work use visual attribute label humans suitable large scale applications paper revisit use document semantic representations argue document like wikipedia page contain rich visual information however easily bury vast amount non visual sentence address issue propose semi automatic mechanism visual sentence extraction leverage document section headers cluster structure visual sentence extract visual sentence novel weight scheme distinguish similar class essentially form semantic representations like visual attribute need much less human effort imagenet dataset ten thousand unseen class representations lead sixty-four relative improvement commonly use ones
overwhelm popularity knowledge graph kgs researchers pour attention link prediction fill miss facts long time however mainly focus link prediction binary relational data facts usually represent triple form head entity relation tail entity practice n ary relational facts also ubiquitous encounter facts exist study usually decompose triple introduce multitude auxiliary virtual entities additional triple conversions result complexity carry link prediction n ary relational data even prove may loss structure information overcome problems paper represent n ary relational fact set role role value pair propose method call nalp conduct link prediction n ary relational data explicitly model relatedness role role value pair n ary relational fact extend nalp introduce type constraints roles role value without external type specific supervision propose reasonable negative sample mechanism experimental result validate effectiveness merit propose methods
foundation research summarization czech language lay work straka et al two thousand and eighteen publish sumeczech large czech news base summarization dataset propose several baseline approach however clear achieve result large space improvement work focus impact name entities summarization czech news article first annotate sumeczech name entities propose new metric rougene measure overlap name entities true generate summaries show still challenge summarization systems reach high score propose extractive summarization approach name entity density select sentence highest ratio number entities length sentence summary article experiment show propose approach reach result close solid baseline domain news article select first sentence moreover demonstrate select sentence reflect style report concisely identify happen propose summary beneficial combination first sentence article voice applications present news article propose two abstractive summarization approach base seq2seq architecture first approach use tokens article second approach access name entity annotations experiment show approach exceed state art result previously report straka et al two thousand and eighteen latter achieve slightly better result sumeczech domain test set
vocabulary size modern word base language model become ever larger many sample base train criteria propose investigate essence sample methods softmax relate traversal entire vocabulary simplify give speedups compare baseline problem notice current landscape sample methods lack systematic comparison myths prefer one another work consider monte carlo sample importance sample novel method call compensate partial summation noise contrastive estimation link back three traditional criteria namely mean square error binary cross entropy cross entropy derive theoretical solutions train problems contrary common belief show sample methods perform equally well long correct intend class posterior probabilities experimental result language model automatic speech recognition switchboard librispeech support claim sample base methods show similar perplexities word error rat give expect speedups
automatic speech recognition asr systems generalize poorly accent speech phonetic linguistic variability accent present hard challenge asr systems today data collection model strategies result bias asr performance across accent come cost users providers asr present survey current promise approach accent speech recognition highlight key challenge space approach mostly focus single model generalization accent feature engineer among challenge lack standard benchmark make research comparison especially difficult
manually determine concepts present group question challenge time consume process however process essential step model virtual learn environment since map concepts question use mastery level assessment recommendation engines require investigate unsupervised semantic model know topic model techniques assist computer science teachers task propose method transform computer science one teacher provide code solutions representative text document include code structure information apply non negative matrix factorization latent dirichlet allocation techniques extract underlie relationship question validate result use external dataset consider interpretability learn concepts use fourteen university professors data result confirm six semantically coherent cluster use current dataset moreover six topics comprise main concepts present test dataset achieve seventy-five normalize pointwise mutual information metric metric correlate human rat make propose method useful provide semantics large amount unannotated code
work describe encoder pre train procedure use frame wise label improve train stream recurrent neural network transducer rnn model stream rnn train scratch usually perform worse high latency although common address issue pre train components rnn criteria frame wise alignment guidance alignment easily available end end manner work frame wise alignment use pre train stream rnn encoder generate without use hmm base system therefore neural framework equip hmm free encoder pre train construct achieve expand spike ctc model leave right blank frame two expand strategies propose best knowledge first work simulate hmm base frame wise label use ctc model experiment conduct librispeech mls english task show propose pre train procedure compare random initialization reduce wer relatively five hundred and eleven emission latency sixty ms besides method lexicon free friendly new languages without manually design lexicon
build dialogue system communicate naturally humans challenge yet interest problem agent base compute rapid growth area usually hinder long stand problem data scarcity systems expect learn syntax grammar decision make reason insufficient amount task specific dataset recently introduce pre train language model potential address issue data scarcity bring considerable advantage generate contextualized word embeddings model consider counterpart imagenet nlp demonstrate capture different facets language hierarchical relations long term dependency sentiment short survey paper discuss recent progress make field pre train language model also deliberate strengths language model leverage design engage eloquent conversational agents paper therefore intend establish whether pre train model overcome challenge pertinent dialogue systems architecture could exploit order overcome challenge open challenge field dialogue systems also deliberate
score function measure plausibility triplets knowledge graph kgs key ensure excellent performance kg embed design also important problem literature automate machine learn automl techniques recently introduce kg design task aware score function achieve state art performance kg embed however effectiveness search score function still good desire paper observe exist score function exhibit distinct performance different semantic pattern motivate explore semantics search relation aware score function relation aware search require much larger search space previous one hence propose encode space supernet propose efficient alternative minimization algorithm search supernet one shoot manner finally experimental result benchmark datasets demonstrate propose method efficiently search relation aware score function achieve better embed performance state art methods
transformer encode network prove powerful tool understand natural languages play critical role native ads service facilitate recommendation appropriate ads base user web browse history sake efficient recommendation conventional methods would generate user advertisement embeddings independently siamese transformer encoder approximate nearest neighbour search ann leverage give underlie semantic user ad complicate independently generate embeddings prone information loss lead inferior recommendation quality although another encode strategy cross encoder much accurate lead huge run cost become infeasible realtime service like native ads recommendation work propose hybrid encoder make efficient precise native ads recommendation two consecutive step retrieval rank retrieval step user ad encode siamese component enable relevant candidates retrieve via ann search rank step represent ad disentangle embeddings user ad relate embeddings contribute fine grain selection high quality ads candidate set step light weight thank pre compute cache intermedia result optimize hybrid encoder performance two stage workflow progressive train pipeline develop build model capability retrieval rank task step step hybrid encoder effectiveness experimentally verify little additional cost outperform siamese encoder significantly achieve comparable recommendation quality cross encoder
emergence voice assistant devices usher delightful user experience smart home front also diverse educational environments classrooms personalize learn tutor however use voice interaction modality also could result exposure user identity hinder broader adoption voice interfaces especially important environments children present voice privacy need protect end build state art techniques propose literature design evaluate practical efficient framework voice privacy source approach combine speaker identification sid speech conversion methods randomly disguise identity users right device record speech ensure transform utterances users still successfully transcribe automatic speech recognition asr solutions evaluate asr performance conversion term word error rate show promise framework preserve content input speech
neural language model nlm train evaluate context span multiple utterances show consistently outperform conventional n gram language model nlms use limit context paper investigate various techniques incorporate turn base context history recurrent lstm transformer xl base nlms recurrent base nlms explore context carry mechanism feature base augmentation incorporate form contextual information bot response system dialogue act classify natural language understand nlu model mitigate sharp nearby fuzzy far away problem contextual nlm propose use attention layer lexical metadata improve feature base augmentation additionally adapt contextual nlm towards user provide fly speech pattern leverage encode large pre train mask language model perform fusion transformer xl base nlm test propose model use n best rescoring asr hypotheses task orient dialogues also evaluate downstream nlu task intent classification slot label best perform model show relative wer sixteen ninety-one slot label f1 score improvement four non contextual baselines
adaption end end speech recognition systems new task know challenge number solutions propose apply external language model various fusion methods possibly combination two pass decode also tts systems use generate adaptation data end end model paper show rnn transducer model effectively adapt new domains use small amount textual data take advantage model inherent structure prediction network interpret language model apply fast adaptation model adapt model avoid need complicate decode time fusions external language model use appropriate regularization prediction network adapt new domains still retain good generalization capabilities show multiple asr evaluation task method provide relative gain ten forty-five target task wer also share insights rnn transducer prediction network perform language model
commonly use speech corpora inadequately challenge academic commercial asr systems particular speech corpora lack metadata need detail analysis wer measurement response present earn twenty-one thirty-nine hour corpus earn call contain entity dense speech nine different financial sectors corpus intend benchmark asr systems wild special attention towards name entity recognition benchmark four commercial asr model two internal model build open source tool open source librispeech model discuss differences performance earn twenty-one use recently release fstalign tool provide candid analysis model recognition capabilities different partition analysis find asr accuracy certain ner categories poor present significant impediment transcript comprehension usage earn twenty-one bridge academic commercial asr system evaluation enable research entity model wer real world audio
query categorization essential part query intent understand e commerce search common query categorization task select relevant fine grain product categories product taxonomy frequent query rich customer behavior eg click data use infer relevant product categories however rare query cover large volume search traffic rely solely customer behavior may suffice due lack signal improve categorization rare query adapt pseudo relevance feedback prf approach utilize latent knowledge embed semantically lexically similar product document enrich representation rare query end propose novel deep neural model name attentive pseudo relevance feedback network aprf net enhance representation rare query query categorization demonstrate effectiveness approach collect search query large commercial search engine compare aprf net state art deep learn model text classification result show aprf net significantly improve query categorization fifty-nine f11 score baselines increase eighty-two improvement rare tail query find paper leverage improvements search query representation understand
self supervise learn ssl use huge unlabeled data successfully explore image natural language process recent work also investigate ssl speech notably successful improve performance downstream task automatic speech recognition asr work suggest possible reduce dependence label data build efficient speech systems evaluation mostly make asr use multiple heterogeneous experimental settings english render difficult objective comparison ssl approach evaluation impact build speech systems paper propose lebenchmark reproducible framework assess ssl speech include asr high low resource task also speak language understand speech translation emotion recognition also target speech technologies language different english french ssl model different size train carefully source document datasets experiment show ssl beneficial task confirm need exhaustive reliable benchmarks evaluate real impact lebenchmark share scientific community reproducible research ssl speech
silent speech interfaces ssi aim reconstruct speech signal record articulatory movement ultrasound video tongue currently deep neural network successful technology task efficient solution require methods simply process single image able extract tongue movement information sequence video frame one option apply recurrent neural structure long short term memory network lstm combination 2d convolutional neural network cnns experiment another approach extend cnn perform 3d convolution extra dimension correspond time particular apply spatial temporal convolutions decompose form prove successful recently video action recognition find experimentally 3d network outperform cnnlstm model indicate 3d cnns may feasible alternative cnnlstm network ssi systems
multimodal affect recognition constitute important aspect enhance interpersonal relationships human computer interaction however relevant data hard come notably costly annotate pose challenge barrier build robust multimodal affect recognition systems model train relatively small datasets tend overfit improvement gain use complex state art model marginal compare simple baselines meanwhile many different multimodal affect recognition datasets though may small paper propose leverage datasets use weakly supervise multi task learn improve generalization performance specifically explore three multimodal affect recognition task one emotion recognition two sentiment analysis three sarcasm recognition experimental result show multi task benefit task achieve improvement twenty-nine accuracy thirty-three f1 score furthermore method also help improve stability model performance addition analysis suggest weak supervision provide comparable contribution strong supervision task highly correlate
audio segmentation mismatch train data see run time major problem direct speech translation indeed systems usually train manually segment corpora real use case often present continuous audio require automatic sub optimal segmentation compare exist techniques vad base fix length hybrid segmentation methods paper propose enhance hybrid solutions produce better result without sacrifice latency experiment different domains language pair show methods outperform techniques reduce least thirty gap traditional vad base approach optimal manual segmentation
map search query set relevant categories product taxonomy significant challenge e commerce search two reason one train data exhibit severe class imbalance problem due bias click behavior two query little customer feedback eg tail query well represent train set difficulties query understand address problems propose deep learn model deepcat learn joint word category representations enhance query understand process believe learn category interactions help improve performance category map minority class tail torso query deepcat contain novel word category representation model train category representations base word category co occurrences train set category representation leverage introduce new loss function estimate category category co occurrences refine joint word category embeddings demonstrate model effectiveness minority categories tail query conduct two set experiment result show deepcat reach ten improvement minority class seventy-one improvement tail query state art label embed model find suggest promise direction improve e commerce search semantic model taxonomy hierarchies
large scale transformer base pre train recently revolutionize vision language vl research model lxmert vilbert uniter significantly lift state art wide range vl task however large number parameters model hinder application practice parallel work lottery ticket hypothesis show deep neural network contain small match subnetworks achieve par even better performance dense network train isolation work perform first empirical study assess whether trainable subnetworks also exist pre train vl model use uniter one best perform vl model testbed consolidate seven representative vl task experiment include visual question answer visual commonsense reason visual entailment refer expression comprehension image text retrieval gqa nlvr2 comprehensive analysis summarize main find follow difficult find subnetworks ie ticket strictly match performance full uniter model however encourage confirm find relax win ticket fifty seventy sparsity maintain ninety-nine full accuracy ii subnetworks find task specific prune transfer reasonably well task find pre train task sixty seventy sparsity transfer universally match ninety-eight ninety-six full accuracy average task iii adversarial train use enhance performance find lottery ticket
many real world scenarios extrinsic reward agent extremely sparse curiosity emerge useful concept provide intrinsic reward enable agent explore environment acquire information achieve goals despite strong performance many sparse reward task exist curiosity approach rely overly holistic view state transition allow structure understand specific aspects environment paper formulate curiosity base ground question answer encourage agent ask question environment curious answer question change show natural language question encourage agent uncover specific knowledge environment physical properties object well spatial relationships object serve valuable curiosity reward solve sparse reward task efficiently
content base music information retrieval see rapid progress adoption deep learn current approach high level music description typically make use classification model auto tag genre mood classification work propose address music description via audio caption define task generate natural language description music audio content human like manner end present first music audio caption model muscaps consist encoder decoder temporal attention method combine convolutional recurrent neural network architectures jointly process audio text input multimodal encoder leverage pre train audio data obtain representations effectively capture summarise musical feature input evaluation generate caption automatic metrics show method outperform baseline design non music audio caption ablation study unveil performance boost mainly attribute pre train audio encoder design choices modality fusion decode strategy use attention contribute marginally model represent shift away classification base music description combine task require auditory linguistic understand bridge semantic gap music information retrieval
massive spread false information social media become global risk especially global pandemic situation like covid nineteen false information detection thus become surge research topic recent months nlp4if two thousand and twenty-one share task fight covid nineteen infodemic organise strengthen research false information detection participants ask predict seven different binary label regard false information tweet share task organise three languages arabic bulgarian english paper present approach tackle task objective use transformers overall approach achieve seven hundred and seven mean f1 score arabic five hundred and seventy-eight mean f1 score bulgarian eight hundred and sixty-four mean f1 score english rank 4th place languages
create data drive model train large dataset unstructured dialogs crucial step develop retrieval base chatbot systems paper present long short term memory lstm base architecture learn unstructured multi turn dialogs provide result task select best response collection give responses ubuntu dialog corpus version two use corpus train show model achieve eight ten three higher accuracy recall1 recall2 recall5 respectively benchmark model also show result experiment perform use several similarity function model hyper parameters word embeddings propose architecture
extract multiple relations text sentence still challenge current open relation extraction open task paper develop several open model base bidirectional lstm crf bilstm crf neural network different contextualized word embed methods also propose new tag scheme solve overlap problems enhance model performance evaluation result comparisons model select best combination tag scheme word embedder bilstm crf network achieve open model remarkable extract ability multiple relation sentence
propose novel phrase break prediction method combine implicit feature extract pre train large language model aka bert explicit feature extract bilstm linguistic feature conventional bilstm base methods word representations sentence representations use independent components propose method take account representations extract latent semantics capture previous methods objective evaluation result show propose method obtain absolute improvement thirty-two point f1 score compare bilstm base conventional methods use linguistic feature moreover perceptual listen test result verify tts system apply propose method achieve mean opinion score four hundred and thirty-nine prosody naturalness highly competitive score four hundred and thirty-seven synthesize speech grind truth phrase break
traditional video summarization methods generate fix video representations regardless user interest therefore methods limit users expectations content search exploration scenarios multi modal video summarization one methods utilize address problem multi modal video summarization use help video exploration text base query consider one main drivers video summary generation user define thus encode text base query video effectively important task multi modal video summarization work new method propose use specialize attention network contextualized word representations tackle task propose model consist contextualized video summary controller multi modal attention mechanisms interactive attention network video summary generator base evaluation exist multi modal video summarization benchmark experimental result show propose model effective increase five hundred and eighty-eight accuracy four hundred and six increase f1 score compare state art method
telehealth help facilitate access medical professionals enable remote medical service patients service become gradually popular years advent necessary technological infrastructure benefit telehealth even apparent since begin covid nineteen crisis people become less incline visit doctor person pandemic paper focus facilitate chat sessions doctor patient note quality efficiency chat experience critical demand telehealth service increase accordingly develop smart auto response generation mechanism medical conversations help doctor respond consultation request efficiently particularly busy sessions explore nine hundred thousand anonymous historical online message doctor patients collect nine months implement cluster algorithms identify frequent responses doctor manually label data accordingly train machine learn algorithms use preprocessed data generate responses consider algorithm two step filter ie trigger model filter infeasible patient message response generator suggest top three doctor responses ones successfully pass trigger phase method provide accuracy eight thousand, three hundred and twenty-eight precision3 show robustness parameters
multi modal reason systems rely pre train object detector extract regions interest image however crucial module typically use black box train independently downstream task fix vocabulary object attribute make challenge systems capture long tail visual concepts express free form text paper propose mdetr end end modulate detector detect object image condition raw text query like caption question use transformer base architecture reason jointly text image fuse two modalities early stage model pre train network 13m text image pair mine pre exist multi modal datasets explicit alignment phrase text object image fine tune several downstream task phrase ground refer expression comprehension segmentation achieve state art result popular benchmarks also investigate utility model object detector give label set fine tune shoot set show pre train approach provide way handle long tail object categories label instance approach easily extend visual question answer achieve competitive performance gqa clevr code model available https githubcom ashkamath mdetr
analysis semantics word use text scientific paper predict future impact measure citations study detail examples automate text classification achieve eighty success rate distinguish highly cite little cite article automate intelligent systems allow identification promise work could become influential scientific community problems quantify mean texts representation human language clear since inception natural language process paper present novel method vector representation text mean base information theory show informational semantics use text classification basis leicester scientific corpus describe experimental framework use evaluate impact scientific article informational semantics interest citation classification discover important semantics texts predict citation count propose semantics texts important factor citation prediction article system extract abstract paper represent word abstract vectors mean space automatically analyse distribution scientific categories web science categories within text abstract classify paper accord citation count highly cite little cite show informational approach represent mean text offer way effectively predict scientific impact research paper
confidence score useful downstream applications automatic speech recognition asr systems recent work propose use neural network learn word utterance confidence score end end asr study word confidence model deletions utterance confidence take advantage word level train signal paper propose jointly learn word confidence word deletion utterance confidence empirical result show multi task learn three objectives improve confidence metrics nce auc rmse without need increase model size confidence estimation module use utterance level confidence rescoring also decrease word error rat google voice search long tail map datasets three five relative without need dedicate neural rescorer
recently grow interest study adversarial examples natural language model black box set methods attack natural language classifiers perturb certain important word classifier label change order find important word methods rank word importance query target model word word input sentence result high query inefficiency new interest approach introduce address problem interpretable learn learn word rank instead previous expensive search main advantage use approach achieve comparable attack rat state art methods yet faster fewer query fewer query desirable avoid suspicion towards attack agent nonetheless approach sacrifice useful information could leverage target classifier sake query efficiency paper study effect leverage target model output data attack rat average number query show improve limit overhead additional query
generate realistic sequence central task many machine learn applications considerable recent progress build deep generative model sequence generation task however issue mode collapse remain main issue current model paper propose gin base generic framework address problem mode collapse principled approach change standard gin objective maximize variational lower bind log likelihood minimize jensen shanon divergence data model distributions experiment model text generation task show generate realistic text high diversity
propose first general purpose gradient base attack transformer model instead search single adversarial example search distribution adversarial examples parameterized continuous value matrix hence enable gradient base optimization empirically demonstrate white box attack attain state art attack performance variety natural language task furthermore show powerful black box transfer attack enable sample adversarial distribution match exceed exist methods require hard label output
work look dataset one hundred and fourteen thousands suspicious message collect popular close message platform taiwan january july two thousand and twenty propose hybrid algorithm could efficiently cluster large number text message accord topics narratives obtain group message within limit content alterations within employ algorithm dataset able look content alterations temporal dynamics particular rumor time qualitative case study three covid nineteen relate rumor find key authoritative figure often misquote false information effective measure increase popularity one false information addition fact check effective stop misinformation get attention fact popularity one false information often influence major societal events effective content alterations
predictive performance modern statistical dependency parsers rely heavily availability expensive expert annotate treebank data annotations contribute equally train parsers paper attempt reduce number label examples need train strong dependency parser use batch active learn al particular investigate whether enforce diversity sample batch use determinantal point process dpps improve diversity agnostic counterparts simulation experiment english newswire corpus show select diverse batch dpps superior strong selection strategies enforce batch diversity especially initial stag learn process additionally diversityaware strategy robust corpus duplication set diversity agnostic sample strategies exhibit significant degradation
neuromorphic compute non von neumann compute paradigm perform computation emulate human brain neuromorphic systems extremely energy efficient know consume thousands time less power cpus gpus potential drive critical use case autonomous vehicles edge compute internet things future reason seek indispensable part future compute landscape neuromorphic systems mainly use spike base machine learn applications although non machine learn applications graph theory differential equations spike base simulations applications suggest neuromorphic compute might capable general purpose compute however general purpose computability neuromorphic compute establish yet work prove neuromorphic compute turing complete therefore capable general purpose compute specifically present model neuromorphic compute two neuron parameters threshold leak two synaptic parameters weight delay devise neuromorphic circuit compute mu recursive function ie constant successor projection function mu recursive operators ie composition primitive recursion minimization operators give mu recursive function operators precisely ones compute use turing machine work establish turing completeness neuromorphic compute
stream end end automatic speech recognition asr systems widely use everyday applications require transcribe speech text real time minimal latency make suitable task unlike non stream counterparts stream model constrain causal future context suffer higher word error rat wer improve stream model recent study one propose distill non stream teacher model unsupervised utterances train stream student use teachers predictions however performance gap teacher student wers remain high paper aim close gap use diversify set non stream teacher model combine use recognizer output vote error reduction rover particular show despite weaker rnn model ctc model remarkable teachers fuse rnn ctc model together build strongest teachers result student model drastically improve upon stream model previous work one wer decrease forty-one spanish twenty-seven portuguese thirteen french
study finite first order satisfiability fsat constructive set dependent type theory employ synthetic account enumerability decidability give full classification fsat depend first order signature non logical symbols one hand development focus trakhtenbrot theorem state fsat undecidable soon signature contain least binary relation symbol proof proceed many one reduction chain start post correspondence problem hand establish decidability fsat monadic first order logic ie signature contain unary function relation symbols well enumerability fsat arbitrary enumerable signatures showcase application trakthenbrot theorem continue reduction chain many one reduction fsat separation logic result mechanise framework grow coq library synthetic undecidability proof
boost simultaneous translation share task iwslt two thousand and twenty promise end end online speech translation approach recently propose consist incrementally encode speech input source language decode correspond text target language best possible trade latency translation quality paper investigate two key aspects end end simultaneous speech translation encode efficiently continuous speech flow b segment speech flow order alternate optimally read r encode input write w decode output operations extend previously propose end end online decode strategy show replace blstm ulstm encode degrade performance offline mode actually improve efficiency performance online mode also measure impact different methods segment speech signal use fix interval boundaries oracle word boundaries randomly set boundaries show best end end online decode strategy surprisingly one alternate r w operations fix size block english german speech translation setup
human evaluation modern high quality machine translation systems difficult problem increase evidence inadequate evaluation procedures lead erroneous conclusions considerable research human evaluation field still lack commonly accept standard procedure step toward goal propose evaluation methodology ground explicit error analysis base multidimensional quality metrics mqm framework carry largest mqm research study date score output top systems wmt two thousand and twenty share task two language pair use annotations provide professional translators access full document context analyze result data extensively find among result substantially different rank evaluate systems one establish wmt crowd workers exhibit clear preference human machine output surprisingly also find automatic metrics base pre train embeddings outperform human crowd workers make corpus publicly available research
rapid spread covid nineteen already affect human live throughout globe governments different countries take various measure affect people live clear study rule base machine learn base model apply answer question use public tweet japan usa uk australia two polarity timeseries meanpol pnratio two events namely lockdown emergency lead economic support package esp consider study statistical test sub series around lead esp events show positive impact people uk australia usa uk respectively unlike japanese people show opposite effect manual validation relevant tweet show agreement statistical result case study japanese tweet use supervise logistic regression classify tweet heath worry economy worry class eight thousand, three hundred and eleven accuracy predict tweet around events confirm statistical outcomes
time delay neural network tdnns widely use dnn hmm base hybrid speech recognition systems recent end end systems nevertheless receptive field tdnns limit fix desirable task like speech recognition temporal dynamics speech vary affect many factor paper propose use deformable tdnns adaptive temporal dynamics model end end speech recognition inspire deformable convnets deformable tdnns augment temporal sample locations additional offset learn offset automatically base asr criterion without additional supervision experiment show deformable tdnns obtain state art result wsj benchmarks one hundred and forty-two three hundred and forty-five wer wsj eval92 dev93 respectively outperform standard tdnns significantly furthermore propose latency control mechanism deformable tdnns enable deformable tdnns stream asr without accuracy degradation
build asr model across many language families challenge multi task learn problem due large language variations heavily unbalance data exist work show positive transfer high resource low resource languages however degradations high resource languages commonly observe due interference heterogeneous multilingual data reduction per language capacity conduct capacity study fifteen language task amount data per language vary 77k 547k hours adopt gshard one efficiently scale 10b parameters empirically find one scale number model parameters effective way solve capacity bottleneck 500m param model already better monolingual baselines scale 1b 10b bring quality gain two larger model data efficient also efficient term train cost measure tpu days 1b param model reach accuracy thirty-four train time 500m param model three give fix capacity budget add depth usually work better width large encoders tend better large decoders
paper address problem learn low dimension representation entities relational databases consist multiple table embeddings help capture semantics encode database use variety settings like auto completion table fully neural query process relational join query seamlessly handle miss value current work restrict work single table use pretrained embeddings external corpus make unsuitable use real world databases work look ways use attention base model learn embeddings entities relational database inspire bert style pretraining methods interest observe extend representation learn structure databases evaluate approach autocompletion relational databases achieve improvement standard baselines
fix bug program need locate bug understand cause problem patch code accordingly process become harder program train machine learn model even harder opaque deep learn model survey review paper exploit explanations enable humans debug nlp model call problem explanation base human debug ebhd particular categorize discuss exist work along three main dimension ebhd bug context workflow experimental set compile find ebhd components affect human debuggers highlight open problems could future research directions
present full size russian complexly ner label corpus internet user review along evaluation accuracy level reach corpus set advance deep learn neural network extract pharmacologically meaningful entities russian texts corpus annotation include mention follow entities medication thirty-three thousand and five mention adverse drug reaction one thousand, seven hundred and seventy-eight disease seventeen thousand, four hundred and three note four thousand, four hundred and ninety two medication disease comprise set attribute part corpus coreference annotation one thousand, five hundred and sixty coreference chain three hundred document special multi label model base language model set feature develop appropriate present corpus label influence choice different modifications model word vector representations type language model pre train russian text normalization style preliminary process analyze sufficient size corpus allow study effect particularities corpus label balance entities corpus result state art pharmacological entity extraction problem russian establish full size label corpus case adverse drug reaction adr recognition six hundred and eleven f1 exact metric analysis show par accuracy level language corpora similar characteristics adr representativnes evaluate baseline precision coreference relation extraction corpus seventy-one higher result reach russian corpora
many nlp paper task pipelines assume raw clean texts many texts encounter wild clean many visually structure document vsds pdfs conventional preprocessing tool vsds mainly focus word segmentation coarse layout analysis fine grain logical structure analysis identify paragraph boundaries hierarchies vsds underexplored end propose formulate task prediction transition label text fragment map fragment tree develop feature base machine learn system fuse visual textual semantic cue system significantly outperform baselines identify different structure vsds example system obtain paragraph boundary detection f1 score nine hundred and fifty-one significantly better popular pdf text tool f1 score seven hundred and thirty-nine
follow success speak dialogue systems sds smartphone assistants smart speakers number communicative robots develop commercialize compare conventional sdss design human machine interface interaction robots expect closer manner talk human anthropomorphism physical presence goal task dialogue may information retrieval conversation order realize human level long deep conversation develop intelligent conversational android erica set several social interaction task erica include attentive listen job interview speed date allow spontaneous incremental multiple utterances robust turn take model implement base trp transition relevance place prediction variety backchannels generate base time frame wise prediction instead ipu base prediction realize open domain attentive listen system partial repeat elaborate question focus word well assessment responses evaluate forty senior people engage conversation five seven minutes without conversation breakdown also compare woz set also realize job interview system set base question follow dynamic generation elaborate question also evaluate student subject show promise result
data mine project managers benefit use standard data mine process model benefit use standard process model data mine de facto popular cross industry standard process model data mine crisp dm reduce cost time also standard model facilitate knowledge transfer reuse best practice minimize knowledge requirements hand unlock potential ever grow textual data publications patent social media data document various form digital innovation increasingly need furthermore introduction cut edge machine learn tool techniques enable elicitation ideas process unstructured textual data generate new useful ideas refer idea mine exist literature idea mine merely overlook utilization standard data mine process model therefore purpose paper propose reusable model generate ideas crisp dm idea mine crisp i design development crisp i do follow design science approach crisp i facilitate idea generation use dynamic topic model dtm unsupervised machine learn subsequent statistical analysis dataset scholarly article adapt crisp i use guide process identify trend use scholarly literature datasets temporally organize patent textual dataset domain elicit ideas ex post evaluation crisp i leave future study
transformer base self supervise model train feature extractors empower many downstream speech task achieve state art performance however train inference process model may encounter prohibitively high computational cost large parameter budget although parameter share strategy pss propose albert pave way parameter reduction computation require remain interestingly find experiment distributions feature embeddings different transformer layer similar pss integrate property term layer consistency lc paper give similarity feature distributions assume feature embeddings different layer would similar represent power work layer consistency enable us adopt transformer base model efficient manner number conformer layer train iteration could uniformly sample shallow layer inference sli could apply reduce number layer inference stage experiment model train librispeech dataset evaluate phone classification speech recognition task experimentally achieve 78x parameter reduction four hundred and nineteen train speedup three hundred and seventy-seven inference speedup maintain comparable performance conventional bert like self supervise methods
recent advance neural architectures transformer couple emergence large scale pre train model bert revolutionize field natural language process nlp push state art number nlp task rich family variations model propose roberta albert xlnet fundamentally remain limit ability model certain kinds information cope certain information source easy pre exist model thus aim would light important theoretical limitations pre train bert style model inherent general transformer architecture first demonstrate practice two general type task segmentation segment label four datasets limitations indeed harmful address even simple naive ways yield sizable improvements vanilla roberta xlnet offer general discussion desiderata future additions transformer architecture would increase expressiveness hope could help design next generation deep nlp architectures
paper present method apply bert freedom operate patent analysis patent search accord method bert fine tune train patent descriptions independent claim description represent invention protect correspond claim train bert could able identify order freedom operate relevant patent base short description invention product test method train bert patent class g06t1 zero apply train bert five inventions classify g06t1 sixty describe via docdb abstract docdb abstract available espacenet european patent office
paper explore topic transportability sub area generalisability propose utilisation metrics base well establish statistics able estimate change performance nlp model new contexts define new measure transportability may allow better estimation nlp system performance new domains crucial assess performance nlp systems new task domains several instance increase complexity demonstrate lightweight domain similarity measure use estimators transportability nlp applications propose transportability measure evaluate context name entity recognition natural language inference task
paper survey several recent abstract summarization methods t5 pegasus prophetnet implement systems two languages english indonesian languages investigate impact pre train model one t5 three pegasuses three prophetnets several wikipedia datasets english indonesian language compare result wikipedia systems summaries t5 large pegasus xsum prophetnet cnndm provide best summarization significant factor influence rouge performance coverage density compression higher score better summary factor influence rouge score pre train goal dataset characteristics dataset use test pre train model cross lingual function several suggestions improve paper limitation one assure dataset use pre train model must sufficiently large contain adequate instance handle cross lingual purpose two advance process finetuning shall reasonable recommend use large dataset consist comprehensive coverage topics many languages implement advance process train infer train procedure zero shoot translation train stage pre train model
transformer base pretrained language model plms start new era modern natural language process nlp model combine power transformers transfer learn self supervise learn ssl follow success model general domain biomedical research community develop various domain plms start biobert latest biomegatron coderbert model strongly believe need survey paper provide comprehensive survey various transformer base biomedical pretrained language model bplms survey start brief overview foundational concepts like self supervise learn embed layer transformer encoder layer discuss core concepts transformer base plms like pretraining methods pretraining task fine tune methods various embed type specific biomedical domain introduce taxonomy transformer base bplms discuss model discuss various challenge present possible solutions conclude highlight open issue drive research community improve transformer base bplms
large scale contextual representation model significantly advance nlp recent years understand semantics text degree never see however need process large amount data achieve high quality result join access data multiple source extremely challenge due privacy regulatory reason federate learn solve limitations train model distribute fashion take advantage hardware devices generate data show viability train nlp model specifically word2vec federate learn protocol particular focus scenario small number organizations hold relatively large corpus result show neither quality result convergence time federate word2vec deteriorate compare centralise word2vec
paper several work propose address practical challenge deploy rnn transducer rnn base speech recognition system challenge adapt well train rnn model new domain without collect audio data obtain time stamp confidence score word level first challenge solve splice data method concatenate speech segment extract source domain data get time stamp phone prediction branch add rnn model share encoder purpose force alignment finally obtain word level confidence score utilize several type feature calculate decode confusion network evaluate microsoft production data splice data adaptation method improve baseline adaption text speech method five thousand, eight hundred and three one thousand, five hundred and twenty-five relative word error rate reduction respectively propose time stamp method get less 50ms word time difference average maintain recognition accuracy rnn model also obtain high confidence annotation performance limit computation cost
multi party dialogues common enterprise social media technical well non technical topics outcome conversation may positive negative important analyze dialogue end particular sentiment point view conflict analysis well future collaboration design propose explainable time series mine algorithm analysis dialogue represent attribute time series occurrences keywords empath categories infer sentiments various point progress special decision tree decision metrics take account temporal relationships dialogue events use predict outcome sentiment interpretable rule mine classifier use explain prediction experimental result present enterprise social media post large company
previous work demonstrate single head attention encoder decoder model able reach state art result conversational speech recognition paper improve result switchboard three hundred two thousand use improve optimizer speaker vector embeddings alternative speech representations reduce recognition errors lstm system switchboard three hundred four relative compensation decoder model probability ratio approach allow efficient integration external language model report fifty-nine one hundred and fifteen wer swb chm part hub5 zero simple lstm model study also consider recently propose conformer advance self attention base language model overall conformer show similar performance lstm nevertheless combination decode improve lm reach new record switchboard three hundred fifty one hundred wer swb chm find also confirm switchboard two thousand new state art report practically reach limit benchmark
paper describe solution shanda innovations team task one kdd cup two thousand and twelve novel approach call multifaceted factorization model propose incorporate great variety feature social network social relationships action users integrate implicit feedbacks improve recommendation accuracy keywords tag profile time feature also utilize model user interest addition user behaviors model durations recommendation record context aware ensemble framework apply combine multiple predictors produce final recommendation result propose approach obtain forty-three thousand, nine hundred and fifty-nine public score forty-one thousand, eight hundred and seventy-four private score test dataset achieve 2nd place kdd cup competition
self supervise learn ssl prove vital advance research natural language process nlp computer vision cv paradigm pretrains share model large volumes unlabeled data achieve state art sota various task minimal adaptation however speech process community lack similar setup systematically explore paradigm bridge gap introduce speech process universal performance benchmark superb superb leaderboard benchmark performance share model across wide range speech process task minimal architecture change label data among multiple usages share model especially focus extract representation learn ssl due preferable usability present simple framework solve superb task learn task specialize lightweight prediction head top freeze share model result demonstrate framework promise ssl representations show competitive generalizability accessibility across superb task release superb challenge leaderboard benchmark toolkit fuel research representation learn general speech process
paper envision multi agent system detect presence hate speech online social media platforms twitter facebook introduce novel framework employ deep learn techniques coordinate channel textual i age process experimental result aim demonstrate effectiveness methods classify online content train propose neural network model effectively detect hateful instance input conclude discussion system may use provide recommendations users manage online social network showcasing immense potential intelligent multi agent systems towards deliver social good
protection human right one important problems world paper aim provide dataset cover one significant human right contradiction recent months affect whole world george floyd incident propose label dataset topic detection contain seventeen million tweet tweet collect twenty-five may two thousand and twenty twenty-one august two thousand and twenty cover eighty-nine days start incident label dataset monitor trend news topics global local newspapers apart present two baselines tf idf lda evaluate result two methods three different k value metrics precision recall f1 score collect dataset available https githubcom meysamasgaric blmt
task natural language table retrieval nltr seek retrieve semantically relevant table base natural language query exist learn systems task often treat table plain text base assumption table structure dataframes however table complex layouts indicate diverse dependencies subtable structure nest headers result query may refer different span relevant content distribute across structure moreover systems fail generalize novel scenarios beyond see train set prior methods still distant generalizable solution nltr problem fall short handle complex table layouts query multiple granularities address issue propose graph base table retrieval gtr generalizable nltr framework multi granular graph representation learn framework table first convert tabular graph cell nod row nod column nod capture content different granularities tabular graph input graph transformer model capture table cell content layout structure enhance robustness generalizability model incorporate self supervise pre train task base graph context match experimental result two benchmarks show method lead significant improvements current state art systems experiment demonstrate promise performance method cross dataset generalization enhance capability handle complex table fulfil diverse query intents code data available https githubcom feiwang96 gtr
discover speaker independent acoustic units purely speak input know hard problem work propose unsupervised speaker normalization technique prior unit discovery base separate speaker relate content induce variations speech signal adversarial contrastive predictive cod approach technique neither require transcribe speech speaker label furthermore train multilingual fashion thus achieve speaker normalization even unlabeled data available target language speaker normalization do map utterances medoid style representative whole database demonstrate effectiveness approach conduct acoustic unit discovery hide markov model variational autoencoder note however propose speaker normalization serve front end unit discovery system experiment english yoruba mboshi show improvements compare use non normalize input
compile commonsense knowledge traditionally ai topic approach manual labor recent advance web data process enable automate approach demonstration showcase three systems automate commonsense knowledge base construction highlight time one aspect specific interest data management community use quasimodo illustrate knowledge extraction systems engineer ii dice illustrate role schema constraints play clean fuzzy commonsense knowledge iii ascent illustrate relevance conceptual model demo available online https quasimodor2enstfr https dicempi infmpgde ascentmpi infmpgde
graph base semantic representations valuable natural language process often simple effective represent linguistic concepts nod relations edge several attempt make find generative device sufficiently powerful represent languages semantic graph allow efficient parse add line work introduce graph extension grammar consist algebra graph together regular tree grammar generate expressions operations algebra due design operations grammars generate graph non structural reentrancies type node share excessively common formalisms abstract mean representation exist devices offer little support provide parse algorithm graph extension grammars prove correct run polynomial time
cultural diversity encode within languages world risk many languages become endanger last decades context grow globalization preserve diversity first necessary understand drive language extinction mechanisms might enable coexistence consider process work language shift conjunction theoretical data drive perspectives large scale empirical study spatial pattern languages multilingual societies use twitter census data yield wide diversity range almost complete mix language speakers include multilinguals segregation neat separation linguistic domains multilinguals mainly boundaries understand different state emerge especially become stable propose model coexistence languages may reach learn language facilitate bilinguals favor use endanger language simulations carry metapopulation framework highlight importance spatial interactions arise people mobility explain stability mix state presence boundary two linguistic regions change parameters regulate relation languages destabilize system undergo global transition accord model evolution system undergo transition highly history dependent easy change status quo go back previous state may simple even possible
link text representation critical many intelligent web applications online advertisement recommender systems recent breakthroughs pretrained language model graph neural network facilitate development correspond techniques however exist work mainly rely cascade model structure texts independently encode language model first textual embeddings aggregate graph neural network argue neighbourhood information insufficiently utilize within process restrict representation quality work propose graphformers graph neural network nest alongside transformer layer language model top architecture link texts iteratively extract neighbourhood information enhancement semantics iterative workflow give rise effective utilization neighbourhood information contribute representation quality introduce adaptation call unidirectional graphformers much efficient comparably effective leverage pretraining strategy call neighbourhood aware mask language model enhance train effect perform extensive experiment study three large scale link text datasets whose result verify effectiveness propose methods
explainable deep learn model advantageous many situations prior work mostly provide unimodal explanations post hoc approach part original system design explanation mechanisms also ignore useful textual information present image paper propose mtxnet end end trainable multimodal architecture generate multimodal explanations focus text image curate novel dataset textvqa x contain grind truth visual multi reference textual explanations leverage train evaluation quantitatively show train multimodal explanations complement model performance surpass unimodal baselines seven cider score two iou importantly demonstrate multimodal explanations consistent human interpretations help justify model decision provide useful insights help diagnose incorrect prediction finally describe real world e commerce application use generate multimodal explanations
keep mind necessity intelligent system educational sector paper propose text analysis base automate approach automatic evaluation descriptive answer examination particular research focus use intelligent concepts natural language process data mine computer aid examination evaluation system paper present architecture fair evaluation answer sheet architecture examiner create sample answer sheet give set question use concept text summarization text semantics keywords summarization final score answer calculate text similarity model base siamese manhattan lstm malstm result research compare manually grade assignments exist system approach find efficient order implement institution university
end end multilingual speech recognition involve use single model train compositional speech corpus include many languages result single neural network handle transcribe different languages due fact language train data different characteristics share network may struggle optimize various languages simultaneously paper propose novel multilingual architecture target core operation neural network linear transformation function key idea method assign fast weight matrices language decompose weight matrix share component language dependent component latter factorize vectors use rank one assumptions reduce number parameters per language efficient factorization scheme prove effective two multilingual settings seven twenty-seven languages reduce word error rat twenty-six twenty-seven rel two popular architectures lstm transformer respectively
recently mixture experts moe base transformer show promise result many domains largely due follow advantage architecture firstly moe base transformer increase model capacity without computational cost increase train inference time besides moe base transformer dynamic network adapt vary complexity input instance realworld applications work explore moe base model speech recognition name speechmoe control sparsity router activation improve diversity gate value propose sparsity l1 loss mean importance loss respectively addition new router architecture use speechmoe simultaneously utilize information share embed network hierarchical representation different moe layer experimental result show speechmoe achieve lower character error rate cer comparable computation cost traditional static network provide seventy two hundred and thirty relative cer improvements four evaluation datasets
wide variety speech process task range extract content information speech signal generate speech signal different task model network usually design tune separately universal model perform multiple speech process task task might improve relate abilities learn task multi task learn wide variety speech process task universal model study paper propose universal modularized model speechnet treat speech process task speech text input speech text output format select five essential speech process task multi task learn experiment speechnet show speechnet learn task analyze task improve task speechnet modularized flexible incorporate modules task train approach future release code experimental settings facilitate research modularized universal model multi task learn speech process task
data augmentation recently see increase interest nlp due work low resource domains new task popularity large scale neural network require large amount train data despite recent upsurge area still relatively underexplored perhaps due challenge pose discrete nature language data paper present comprehensive unify survey data augmentation nlp summarize literature structure manner first introduce motivate data augmentation nlp discuss major methodologically representative approach next highlight techniques use popular nlp applications task conclude outline current challenge directions future research overall paper aim clarify landscape exist literature data augmentation nlp motivate additional work area also present github repository paper list continuously update https githubcom styfeng dataaug4nlp
work train first monolingual lithuanian transformer model relatively large corpus lithuanian news article compare various output decode algorithms abstractive news summarization generate summaries coherent look impressive first glance however contain mislead information easy spot describe technical detail share train model accompany code online open source repository well characteristic sample generate summaries
growth natural language process techniques demand improve software engineer efficiency emerge interest translate intention human languages program languages survey paper attempt provide overview grow body research space begin review natural language semantic parse techniques draw parallel program synthesis efforts consider semantic parse work evolutionary perspective specific analyse neuro symbolic methods architecture supervision analyze advancements frameworks semantic parse code generation close present believe emerge open challenge domain
study present large scale benchmarking cloud base speech text systems google cloud speech text microsoft azure cognitive service amazon transcribe ibm watson speech text systems forty thousand, one hundred and fifty-eight clean noisy speech file one hundred and one hours test effect background noise stt quality also evaluate five different signal noise ratios 40db 0db result show microsoft azure provide lowest transcription error rate nine hundred and nine clean speech high robustness noisy environment google cloud amazon transcribe give similar performance latter limit time constraint usage though ibm watson could work correctly quiet condition highly sensible noisy speech could strongly limit application real life situations
recent advance name entity recognition ner show document level contexts significantly improve model performance many application scenarios however contexts available paper propose find external contexts sentence retrieve select set semantically relevant texts search engine original sentence query find empirically contextual representations compute retrieval base input view construct concatenation sentence external contexts achieve significantly improve performance compare original input view base sentence furthermore improve model performance input view cooperative learn train method encourage two input view produce similar contextual representations output label distributions experiment show approach achieve new state art performance eight ner data set across five domains
intent model become important part modern dialogue systems rapid expansion practical dialogue systems virtual assistants amazon alexa apple siri google assistant interest increase however recently focus detect fix discrete number see intents recent years see work do unseen intent detection context zero shoot learn paper continue prior work propose novel model intents continuous point place specialist intent space yield several advantage first continuous representation enable investigate relationships see intents second allow unseen intent reliably represent give limit quantities data finally paper show propose model augment unseen intents without retrain see ones experiment show model reliably add unseen intents high accuracy retain high performance see intents
recently increase number work introduce model capable generate natural language explanations nles predictions vision language vl task model appeal provide human friendly comprehensive explanations however still lack unify evaluation approach explanations generate model moreover currently datasets nles vl task work introduce e vil benchmark explainable vision language task establish unify evaluation framework provide first comprehensive comparison exist approach generate nles vl task e vil span four model three datasets automatic metrics human evaluation use assess model generate explanations also introduce e snli largest exist vl dataset nles 430k instance finally propose new model combine uniter learn joint embeddings image text gpt two pre train language model well suit text generation surpass previous state art large margin across datasets
error correction techniques use refine output sentence automatic speech recognition asr model achieve lower word error rate wer original asr output previous work usually use sequence sequence model correct asr output sentence autoregressively cause large latency deploy online asr service straightforward solution reduce latency inspire non autoregressive nar neural machine translation use nar sequence generation model asr error correction however come cost significantly increase asr error rate paper observe distinctive error pattern correction operations ie insertion deletion substitution asr propose fastcorrect novel nar error correction model base edit alignment train fastcorrect align source token asr output sentence target tokens correspond grind truth sentence base edit distance source target sentence extract number target tokens correspond source token edition correction use train length predictor adjust source tokens match length target sentence parallel generation inference token number predict length predictor use adjust source tokens target sequence generation experiment public aishell one dataset internal industrial scale asr dataset show effectiveness fastcorrect asr error correction one speed inference six nine time maintain accuracy eight fourteen wer reduction compare autoregressive correction model two outperform popular nar model adopt neural machine translation text edition large margin
mathematical compute become democratize high level languages high performance symbolic numeric systems necessary domain scientists engineer get best performance machine without deep knowledge code optimization naturally users need different term type either different algebraic properties use efficient data structure end develop symbolicsjl extendable symbolic system use dynamic multiple dispatch change behavior depend domain need work detail underlie abstract term interface allow speed without sacrifice generality show formalize generic api action independent implementation retroactively add optimize data structure system without change pre exist term rewriters showcase use optimize term construction give 113x acceleration general symbolic transformations show generic api allow complementary term rewrite implementations demonstrate ability swap classical term rewrite simplifiers e graph base term rewrite simplifiers showcase e graph ruleset minimize number cpu cycle expression evaluation demonstrate simplify real world reaction network simulation halve runtime additionally show reaction diffusion partial differential equation solver able automatically convert symbolic expressions via multiple dispatch trace subsequently accelerate parallelize give 157x simulation speedup together present symbolicsjl next generation symbolic numeric compute environment gear towards model simulation
call center human operators attend clients use textual chat common modern e commerce train enough skilled operators able provide good service challenge suggest algorithm method train implement assist agent provide line advice operators attend clients agent domain independent introduce new domains without major efforts design train organize structure knowledge professional discipline demonstrate applicability system experiment realize full life cycle specific domain analyze capabilities
wikipedia largest online encyclopedia use algorithms web users central hub reliable information web quality reliability wikipedia content maintain community volunteer editors machine learn information retrieval algorithms could help scale editors manual efforts around wikipedia content reliability however lack large scale data support development research fill gap paper propose wiki reliability first dataset english wikipedia article annotate wide set content reliability issue build dataset rely wikipedia templates templates tag use expert wikipedia editors indicate content issue presence non neutral point view contradictory article serve strong signal detect reliability issue revision select ten popular reliability relate templates wikipedia propose effective method label almost 1m sample wikipedia article revisions positive negative respect template positive negative example dataset come full article text twenty feature revision metadata provide overview possible downstream task enable data show wiki reliability use train large scale model content reliability prediction release data code public use
geometry problem solve attract much attention nlp community recently task challenge require abstract problem understand symbolic reason axiomatic knowledge however current datasets either small scale publicly available thus construct new large scale benchmark geometry3k consist three thousand and two geometry problems dense annotation formal language propose novel geometry solve approach formal language symbolic reason call interpretable geometry problem solver inter gps inter gps first parse problem text diagram formal language automatically via rule base text parse neural object detect respectively unlike implicit learn exist methods inter gps incorporate theorem knowledge conditional rule perform symbolic reason step step also theorem predictor design infer theorem application sequence feed symbolic solver efficient reasonable search path extensive experiment geometry3k geos datasets demonstrate inter gps achieve significant improvements exist methods project code data available https lupantechgithubio inter gps
dialogue systems popular natural language process nlp task promise real life applications also complicate task since many nlp task deserve study involve result multitude novel work task carry deep learn base due outstanding performance survey mainly focus deep learn base dialogue systems comprehensively review state art research outcomes dialogue systems analyze two angle model type system type specifically angle model type discuss principles characteristics applications different model widely use dialogue systems help researchers acquaint model see apply state art frameworks rather helpful design new dialogue system angle system type discuss task orient open domain dialogue systems two stream research provide insight hot topics relate furthermore comprehensively review evaluation methods datasets dialogue systems pave way future research finally possible research trend identify base recent research outcomes best knowledge survey comprehensive date one present area dialogue systems dialogue relate task extensively cover popular frameworks topics datasets keywords dialogue systems chatbots conversational ai task orient open domain chit chat question answer artificial intelligence natural language process information retrieval deep learn neural network cnn rnn hierarchical recurrent encoder decoder memory network attention transformer pointer net copynet reinforcement learn gans knowledge graph survey review
state art variational auto encoders vaes learn disentangle latent representations give impressive result discover feature like pitch pause duration accent speech data lead highly controllable text speech tts synthesis however lstm base vaes fail learn latent cluster speaker attribute train either limit noisy datasets different latent variables start encode feature limit control expressiveness speech synthesis resolve issue propose rti vae reorder transformer information reduction vae minimize mutual information different latent variables devise modify transformer architecture layer reorder learn controllable latent representations speech data show rti vae reduce cluster overlap speaker attribute least thirty lstm vae least seven vanilla transformer vae
technical report describe methods result three week sprint produce deployable speech recognition model thirty-one serve languages common voice project outline preprocessing step hyperparameter selection result accuracy official test set addition evaluate model multiple task close vocabulary speech recognition pre transcription force alignment key word spot follow experiment use coqui stt toolkit train deployment neural speech text model
recent research novelty detection focus mainly document level classification employ deep neural network dnn however black box nature dnns make difficult extract exact explanation document consider novel addition deal novelty word level crucial provide fine grain analysis available document level work propose tsetlin machine tm base architecture score individual word accord contribution novelty approach encode description novel document use linguistic pattern capture tm clauses adopt description measure much word contribute make document novel experimental result demonstrate approach break novelty interpretable phrase successfully measure novelty
contrast conventional pipeline speak language understand slu consist automatic speech recognition asr natural language understand nlu end end slu infer semantic mean directly speech overcome error propagation cause asr end end slot fill sf speech essential component end end slu usually regard sequence sequence generation problem heavily rely performance language model asr however hard generate correct slot slot vovabulary oov train data especially slot anti linguistic entity without grammatical rule inspire object detection computer vision detect object image consider sf task slot detection speech paper formulate sf task match task propose end end knowledge base sf model name speech slot speech2slot leverage knowledge detect boundary slot speech also release large scale dataset chinese speech slot fill contain eight hundred and thirty thousand sample experiment show approach markedly superior conventional pipeline slu approach outperform state art end end sf approach one thousand, two hundred and fifty-one accuracy improvement
propose fedenhance unsupervised federate learn fl approach speech enhancement separation non iid distribute data across multiple clients simulate real world scenario client access noisy record limit disjoint number speakers hence non iid client train model isolation use mixture invariant train periodically provide update central server experiment show approach achieve competitive enhancement performance compare iid train single device facilitate convergence speed overall performance use transfer learn server side moreover show effectively combine update clients train locally supervise unsupervised losses also release new dataset librifsd50k creation recipe order facilitate fl research source separation problems
conversational recommender systems crss revolutionize conventional recommendation paradigm embrace dialogue agents dynamically capture fine grain user preference typical conversational recommendation scenario crs firstly generate question let user clarify demand make suitable recommendations hence ability generate suitable clarify question key timely trace users dynamic preferences achieve successful recommendations however exist crss fall short ask high quality question one system generate responses heavily depend performance dialogue policy agent train huge conversation corpus cover circumstances two current crss fully utilize learn latent user profile generate appropriate personalize responses mitigate issue propose knowledge base question generation system kbqg novel framework conversational recommendation distinct previous conversational recommender systems kbqg model user preference finer granularity identify relevant relations structure knowledge graph kg condition vary importance different relations generate clarify question could perform better impel users provide detail preferences finially accurate recommendations generate fewer conversational turn furthermore propose kbqg outperform baselines experiment two real world datasets
large transformer base language model pre train corpora vary size different number step different batch size time fundamental components pre train objective architectural hyperparameters modify total therefore difficult ascribe change performance specific factor since search hyperparameter space full systems costly pre train scale versions several popular transformer base architectures common pre train corpus benchmark subset glue task wang et al two thousand and eighteen specifically systematically compare three pre train objectives different shape parameters model size also vary number pre train step batch size experiment mlm nsp bert style consistently outperform mlm roberta style well standard lm objective furthermore find additional compute mainly allocate increase model size train step inefficient base observations final step attempt scale several systems use compound scale tan le two thousand and nineteen adapt transformer base language model
article study impact online news social economic consumer perceptions application semantic network analysis use almost thirteen million online article italian media cover period four years assess incremental predictive power economic relate keywords consumer confidence index transform news network co occur word calculate semantic importance specific keywords see word appear article could anticipate consumers judgements economic situation result show economic relate keywords stronger predictive power consider current households national situation predictive power less significant regard expectations future indicator semantic importance offer complementary approach estimate consumer confidence lessen limitations traditional survey base methods
reduce prediction delay stream end end asr model minimal performance regression challenge problem constrain alignment well know exist approach penalize predict word boundaries use external low latency acoustic model contrary recently propose fastemit sequence level delay regularization scheme encourage vocabulary tokens blank without reference alignments although scheme successful reduce delay asr word error rate wer often severely degrade apply delay constrain scheme paper propose novel delay constrain method name self alignment self alignment require external alignment model instead utilize viterbi force alignments train model find lower latency alignment direction librispeech evaluation self alignment outperform exist scheme twenty-five fifty-six less delay compare fastemit constrain alignment similar word error rate voice search evaluation12 twenty-five delay reductions achieve compare fastemit constrain alignment two wer improvements
nowadays research speech technologies get lot thank recently create public domain corpora contain thousands record hours large amount data helpful train new complex model base deep learn technologies however lack dialectal diversity corpus know performance bias speech systems mainly underrepresented dialects work propose evaluate state art automatic speech recognition asr deep learn base model use unseen data corpus wide variety label english accent different countries around world model train 445k hours english speech open access corpus call multilingual librispeech show remarkable result popular benchmarks test accuracy asr sample extract another public corpus continuously grow common voice dataset present graphically accuracy term word error rate different english include accent show indeed accuracy bias term accentual variety favor accent prevalent train corpus
sign languages primary mean communication many deaf hard hear individuals since sign languages exhibit fundamental linguistic properties natural language believe tool theories natural language process nlp crucial towards model however exist research sign language process slp seldom attempt explore leverage linguistic organization sign languages position paper call nlp community include sign languages research area high social scientific impact first discuss linguistic properties sign languages consider model review limitations current slp model identify open challenge extend nlp sign languages finally urge one adoption efficient tokenization method two development linguistically inform model three collection real world sign language data four inclusion local sign language communities active lead voice direction research
introduce nlp toolkit base object orient knowledge base multi level grammar base toolkit focus semantic parse also abilities discover new knowledge grammar automatically new discover knowledge grammar identify human use update knowledge base grammar base process iterate many time improve toolkit continuously
recent literature underscore importance dataset documentation work machine learn part work involve address documentation debt datasets use widely document sparsely paper aim help address documentation debt bookcorpus popular text dataset train large language model notably researchers use bookcorpus train openai gpt n model google bert model even though little documentation exist dataset motivation composition collection process etc offer preliminary datasheet provide key context information bookcorpus highlight several notable deficiencies particular find evidence one bookcorpus likely violate copyright restrictions many book two bookcorpus contain thousands duplicate book three bookcorpus exhibit significant skew genre representation also find hint potential deficiencies call future research include problematic content potential skew religious representation lopsided author contributions work remain initial effort provide datasheet bookcorpus add grow literature urge careful systematic documentation machine learn datasets
accurate terminology translation crucial ensure practicality reliability neural machine translation nmt systems address lexically constrain nmt explore various methods ensure pre specify word phrase appear translation output however many case methods study general domain corpora term mostly uni bi grams ninety-eight paper instead tackle challenge setup consist domain specific corpora much longer n gram highly specialize term inspire recent success mask span prediction model propose simple effective train strategy achieve consistent improvements terminology sentence level translation three domain specific corpora two language pair
virtual assistants amazon alexa apple siri google home microsoft cortana become ubiquitous daily live successfully help users various daily task make phone call play music yet still struggle playful utterances mean interpret literally examples include joke absurd request question afraid dark let dog order zillion gummy bear today virtual assistants often return irrelevant answer utterances except hard cod ones address can reply address challenge automatically detect playful utterances first characterize different type playful human virtual assistant interaction introduce taxonomy playful request root theories humor refine analyze real world traffic alexa focus one node personification users refer virtual assistant person fun conjecture understand utterances improve user experience virtual assistants conduct wizard oz user study show endow virtual assistant ability identify humorous opportunities indeed potential increase user satisfaction hope work contribute understand landscape problem inspire novel ideas techniques towards vision give virtual assistants sense humor
distribute continuous representations use neural network odds representations employ linguistics typically symbolic vector quantization propose way induce discrete neural representations closer nature linguistic counterparts however clear metrics best suit analyze discrete representations compare merit four commonly use metrics context weakly supervise model speak language perform systematic analysis impact architectural choices ii learn objective train dataset iii evaluation metric find different evaluation metrics give inconsistent result particular find use minimal pair phoneme triple stimuli evaluation disadvantage larger embeddings unlike metrics apply complete utterances
knowledge graph kg alignment discover mappings ie equivalent entities relations others two kgs exist methods divide embed base model conventional reason lexical match base systems former compute similarity entities via cross kg embeddings usually rely ideal supervise learn set good performance lack appropriate reason avoid logically wrong mappings latter address reason issue poor utilize kg graph structure entity contexts study aim combine two solutions thus propose iterative framework name prase base probabilistic reason semantic embed learn kg embeddings via entity mappings probabilistic reason system name paris feed resultant entity mappings embeddings back paris augmentation prase framework compatible different embed base model experiment multiple datasets demonstrate state art performance
describe single submission task one coliee two thousand and twenty-one vanilla bm25 get second place well median submissions code available https githubcom neuralmind ai coliee
propose framework model operational conversational negation apply worldly context prior knowledge logical negation compositional distributional semantics give word framework create negation similar humans perceive negation framework correct logical negation weight mean closer entailment hierarchy mean apart propose framework flexible accommodate different choices logical negations compositions worldly context generation particular propose motivate new logical negation use matrix inverse validate sensibility conversational negation framework perform experiment leverage density matrices encode grade entailment information conclude combination subtraction negation phaser basis negate word yield highest pearson correlation six hundred and thirty-five human rat
encoder pre train promise end end speech translation st give fact speech translation data scarce st encoders simple instance automatic speech recognition asr machine translation mt encoders example find asr encoders lack global context representation necessary translation whereas mt encoders design deal long locally attentive acoustic sequence work propose stack acoustic textual encode sate method speech translation encoder begin process acoustic sequence usual later behave like mt encoder global representation input sequence way straightforward incorporate pre train model system also develop adaptor module alleviate representation inconsistency pre train asr encoder mt encoder multi teacher knowledge distillation method preserve pre train knowledge experimental result librispeech en fr must c en de show method achieve state art performance one hundred and eighty-three two hundred and fifty-two bleu point knowledge first develop end end st system achieve comparable even better bleu performance cascade st counterpart large scale asr mt data available
study use semantic brand score novel measure brand importance big textual data forecast elections base online news thirty-five thousand online news article transform network co occur word analyze combine methods tool social network analysis text mine forecast make four vote events italy provide consistent result across different vote systems general election referendum municipal election two round work contribute research electoral forecast focus predictions base online big data offer new perspectives regard textual analysis online news methodology relatively fast easy apply study also suggest existence link brand importance political candidates party electoral result
semantic brand score sbs new measure brand importance calculate text data combine methods social network semantic analysis metric flexible use different contexts across products market languages applicable brand also multiple set word sbs describe together three dimension brand prevalence diversity connectivity represent contribution research brand equity word co occurrence network use support decision make process within company example apply forecast company stock price assess brand importance respect competitors one side sbs relate familiar construct brand equity offer new perspectives effective strategic management brand era big data
children acquire knowledge language morphology invariably discover productive process generalize new word morphological learn make challenge fact even fully productive rule exceptions well know case english past tense verbs feature ed rule irregular verbs tolerance principle recent proposal provide precise threshold exceptions productive rule withstand empirical application far however require researcher fully specify rule define set word propose greedy search model automatically hypothesize rule evaluate productivity vocabulary search broader productivity fail model recursively subdivide vocabulary continue search productivity narrower rule train psychologically realistic data child direct input model display developmental pattern observe child morphology acquisition include notoriously complex case german noun pluralization also produce responses nonce word despite receive fraction train data similar human subject current neural network model responses
although board game video game study decades artificial intelligence research challenge word game remain relatively unexplored word game constrain game like chess poker instead word game strategy define players understand way word relate word game codenames provide unique opportunity investigate common sense understand relationships word important open challenge propose algorithm generate codenames clue language graph babelnet several embed methods word2vec glove fasttext bert introduce new score function measure quality clue propose weight term call detect incorporate dictionary base word representations document frequency improve clue selection develop babelnet word selection framework babelnet wsf improve babelnet clue quality overcome computational barriers previously prevent leverage language graph codenames extensive experiment human evaluators demonstrate propose innovations yield state art performance one thousand and twenty-eight improvement precision2 case overall work advance formal study word game approach common sense language understand
present novel method perform spell correction short input string search query individual word core lie procedure generate artificial typos closely follow error pattern manifest humans procedure use train production spell correction model base transformer architecture model currently serve hubspot product search show approach typo generation superior widespread practice add noise ignore human pattern also demonstrate approach may extend resource scarce settings train spell correction model arabic greek russian setswana languages without use label data
offensive content pervasive social media reason concern company government organizations several study recently publish investigate methods detect various form content eg hate speech cyberbullying cyberaggression clear majority study deal english partially annotate datasets available contain english data paper take advantage available english datasets apply cross lingual contextual word embeddings transfer learn make predictions low resource languages project predictions comparable data arabic bengali danish greek hindi spanish turkish report result eight thousand, four hundred and fifteen f1 macro bengali trac two share task eight thousand, five hundred and thirty-two f1 macro danish eight thousand, seven hundred and one f1 macro greek offenseval two thousand and twenty eight thousand, five hundred and sixty-eight f1 macro hindi hasoc two thousand and nineteen share task seven thousand, five hundred and thirteen f1 macro spanish semeval two thousand and nineteen task five hateval show approach compare favourably best systems submit recent share task three languages additionally report competitive performance arabic turkish use train development set offenseval two thousand and twenty share task result languages confirm robustness cross lingual contextual embeddings transfer learn task
larger language model higher accuracy average better every single instance datapoint work suggest larger model higher distribution robustness work suggest lower accuracy rare subgroups understand differences investigate model level individual instance however one major challenge individual predictions highly sensitive noise randomness train develop statistically rigorous methods address account pretraining finetuning noise find bert large worse bert mini least one four instance across mnli sst two qqp compare overall accuracy improvement two ten also find finetuning noise increase model size instance level accuracy momentum improvement bert mini bert medium correlate improvement bert medium bert large find suggest instance level predictions provide rich source information therefore recommend researchers supplement model weight model predictions
give collection untrimmed unsegmented videos video corpus moment retrieval vcmr retrieve temporal moment ie fraction video semantically correspond give text query video text two distinct feature space two general approach address vcmr separately encode modality representations align two modality representations query process ii adopt fine grain cross modal interaction learn multi modal representations query process second approach often lead better retrieval accuracy first approach far efficient paper propose retrieval localization network contrastive learn reloclnet vcmr adopt first approach introduce two contrastive learn objectives refine video encoder text encoder learn video text representations separately better alignment vcmr video contrastive learn videocl maximize mutual information query candidate video video level frame contrastive learn framecl aim highlight moment region correspond query frame level within video experimental result show although reloclnet encode text video separately efficiency retrieval accuracy comparable baselines adopt cross modal interaction learn
recently denoising diffusion probabilistic model generative score match show high potential model complex data distributions stochastic calculus provide unify point view techniques allow flexible inference scheme paper introduce grad tts novel text speech model score base decoder produce mel spectrograms gradually transform noise predict encoder align text input mean monotonic alignment search framework stochastic differential equations help us generalize conventional diffusion probabilistic model case reconstruct data noise different parameters allow make reconstruction flexible explicitly control trade sound quality inference speed subjective human evaluation show grad tts competitive state art text speech approach term mean opinion score make code publicly available shortly
effective method cross lingual transfer fine tune bilingual multilingual model supervise dataset one language evaluate another language zero shoot manner translate examples train time inference time also viable alternatives however cost associate methods rarely address literature work analyze cross lingual methods term effectiveness eg accuracy development deployment cost well latencies inference time experiment three task indicate best cross lingual method highly task dependent finally combine zero shoot translation methods achieve state art two three datasets use work base result question need manually label train data target language code model translate datasets available https githubcom unicamp dl cross lingual analysis
paper introduce qaconv new question answer qa dataset use conversations knowledge source focus informative conversations include business email panel discussions work channel unlike open domain task orient dialogues conversations usually long complex asynchronous involve strong domain knowledge total collect thirty-four thousand, two hundred and four qa pair include span base free form unanswerable question ten thousand, two hundred and fifty-nine select conversations human write machine generate question segment long conversations chunk use question generator dialogue summarizer auxiliary tool collect multi hop question dataset two test scenarios chunk mode full mode depend whether ground chunk provide retrieve large conversational pool experimental result show state art qa systems train exist qa datasets limit zero shoot ability tend predict question unanswerable fine tune systems corpus achieve significant improvement two hundred and thirty-six one hundred and thirty-six chunk mode full mode respectively
context aware machine translation model design leverage contextual information often fail result inaccurately disambiguate pronouns polysemous word require context resolution paper ask several question contexts human translators use resolve ambiguous word model pay large amount attention context explicitly train answer question introduce scat support context ambiguous translations new english french dataset comprise support context word 14k translations professional translators find useful pronoun disambiguation use scat perform depth analysis context use disambiguate examine positional lexical characteristics support word furthermore measure degree alignment model attention score support context scat apply guide attention strategy encourage agreement two
comprehend overall intent utterance help listener recognize individual word speak inspire fact perform novel study impact explicitly incorporate intent representations additional information improve recurrent neural network transducer rnn base automatic speech recognition asr system audio intent a2i model encode intent utterance form embeddings posteriors use auxiliary input rnn train inference experiment 50k hour far field english speech corpus study show run system non stream mode intent representation extract entire utterance use bias stream rnn search start provide five hundred and fifty-six relative word error rate reduction werr hand stream system use per frame intent posteriors extra input rnn asr system yield three hundred and thirty-three relative werr detail analysis stream system indicate propose method bring especially good gain media play relate intents eg nine hundred and twelve relative werr playmusicintent
skip connection widely use technique improve performance convergence deep neural network believe relieve difficulty optimization due non linearity propagate linear component neural network layer however another point view also see modulate mechanism input output input scale pre define value one work investigate scale factor effectiveness skip connection reveal trivial adjustment scale lead spurious gradient explode vanish line deepness model could address normalization particular layer normalization induce consistent improvements plain skip connection inspire find propose adaptively adjust scale input recursively apply skip connection layer normalization promote performance substantially generalize well across diverse task include machine translation image classification datasets
consider task link social media account belong author automate fashion basis content metadata correspond document stream focus learn embed map variable size sample user activity range single post entire months activity vector space sample author map nearby point approach require human annotate data train purpose allow us leverage large amount social media content propose model outperform several competitive baselines novel evaluation framework model establish recognition benchmarks domains method achieve high link accuracy even small sample account see train time prerequisite practical applications propose link framework
paper describe systems submit iwslt two thousand and twenty-one volctrans team participate offline speech translation text text simultaneous translation track offline speech translation best end end model achieve eighty-one bleu improvements benchmark must c test set even approach result strong cascade solution text text simultaneous translation explore best practice optimize wait k model result final submit systems exceed benchmark around seven bleu latency regime publish code model facilitate future research work industrial applications
recently considerable literature grow around theme shoot name entity recognition ner little publish benchmark data specifically focus practical challenge task current approach collect exist supervise ner datasets organize shoot set empirical study strategies conventionally aim recognize coarse grain entity type examples practice unseen entity type fine grain paper present nerd large scale human annotate shoot ner dataset hierarchy eight coarse grain sixty-six fine grain entity type nerd consist one hundred and eighty-eight thousand, two hundred and thirty-eight sentence wikipedia four million, six hundred and one thousand, one hundred and sixty word include annotate context part two level entity type best knowledge first shoot ner dataset largest human craft ner dataset construct benchmark task different emphases comprehensively assess generalization capability model extensive empirical result analysis show nerd challenge problem require research make nerd public https ningding97githubio fewnerd
typically information extraction ie require pipeline approach first sequence label model train manually annotate document extract relevant span new document arrive model predict span post process standardize convert information database entry replace labor intensive workflow transformer language model train exist database record directly generate structure json solution remove workload associate produce token level annotations take advantage data source generally quite plentiful eg database record long document common information extraction task use gradient checkpointing chunk encode apply method sequence thirty-two thousand tokens single gpu doc2dict approach competitive complex hand engineer pipelines offer simple effective baseline document level information extraction release doc2dict model code reproduce experiment facilitate future work
semantic embed widely investigate align knowledge graph kg entities current methods explore utilize graph structure entity name attribute ignore ontology ontological schema contain critical meta information class membership relationships entities paper propose ontology guide entity alignment method name ontoea kgs ontologies jointly embed class hierarchy class disjointness utilize avoid false mappings extensive experiment seven public industrial benchmarks demonstrate state art performance ontoea effectiveness ontologies
fact check model automatic fake news detection base reason give claim associate evidence model aim estimate claim veracity base support refute content within evidence model perform well generally assume due model learn reason evidence regard claim paper investigate assumption reason explore relationship importance claim evidence surprisingly find political fact check datasets often highest effectiveness obtain utilize evidence impact include claim either negligible harmful effectiveness highlight important problem constitute evidence exist approach automatic fake news detection
forecast tourism demand important implications policy makers company operate tourism industry research apply methods tool social network semantic analysis study user generate content retrieve online communities interact tripadvisor travel forum analyze forums seven major european capital cities period ten years collect two million, six hundred and sixty thousand post write one hundred and forty-seven thousand users present new methodology analysis tourism relate big data set variables could integrate traditional forecast model implement factor augment autoregressive bridge model social network semantic variables often lead better forecast performance univariate model model base google trend data forum language complexity centralization communication network ie presence eminent contributors variables contribute forecast international airport arrivals
article show discuss experience apply flip classroom method teach conditional random field natural language process course present activities develop together relationship cognitive complexity model bloom taxonomy provide reflections expectations model base evaluation get students seem students learn topic also method reward students additionally discuss shortcomings propose possible solutions conclude paper possible future work
recent advance artificial intelligence ai achieve human scale speed accuracy classification task turn capabilities make ai viable replacement many human activities core involve classification basic mechanical analytical task low level service job current systems need conscious recognize pattern classify however ai progress complicate task require intuition empathy must develop capabilities metathinking creativity empathy akin human self awareness consciousness contend paradigm shift possible fundamental shift state artificial intelligence toward consciousness shift similar take place humans process natural selection evolution paper aim theoretically explore requirements emergence consciousness ai also provide principled understand conscious ai detect might manifest contrast dominant paradigm seek ultimately create machine linguistically indistinguishable humans
transformers become one important architectural innovations deep learn enable many breakthroughs past years propose simple network architecture gmlp base mlps gate show perform well transformers key language vision applications comparisons show self attention critical vision transformers gmlp achieve accuracy bert model achieve parity transformers pretraining perplexity better downstream nlp task finetuning task gmlp perform worse make gmlp model substantially larger close gap transformers general experiment show gmlp scale well transformers increase data compute
capability automatically detect human stress benefit artificial intelligent agents involve affective compute human computer interaction stress emotion human affective state stress prove important implications regulation expression emotion although series methods establish multimodal stress detection limit step take explore underlie inter dependence stress emotion work investigate value emotion recognition auxiliary task improve stress detection propose muser transformer base model architecture novel multi task learn algorithm speed base dynamic sample strategy evaluations multimodal stress emotion muse dataset show model effective stress detection internal external auxiliary task achieve state art result
online social media provide channel monitor people social behaviors mental distress due restrictions impose covid nineteen people increasingly use online social network express feel consequently significant amount diverse user generate social media content however covid nineteen pandemic change way live study socialize recreate affect well mental health problems grow research leverage online social media analysis detect assess user mental status paper survey literature social media analysis mental disorder detection special focus study conduct context covid nineteen two thousand and twenty two thousand and twenty-one firstly classify survey study term feature extraction type vary language usage pattern aesthetic preferences online behaviors secondly explore detection methods use mental disorder detection include machine learn deep learn detection methods finally discuss challenge mental disorder detection use social media data include privacy ethical concern well technical challenge scale deploy systems large scale discuss learn lessons last years
longstanding question cognitive science concern learn mechanisms underlie compositionality human cognition humans infer structure relationships eg grammatical rule implicit sensory observations eg auditory speech use knowledge guide composition simpler mean complex wholes recent progress artificial neural network show large model train enough linguistic data grammatical structure emerge representations extend work domain mathematical reason possible formulate precise hypotheses mean eg quantities correspond numerals compose accord structure rule eg order operations work show neural network able infer something structure relationships implicit train data also deploy knowledge guide composition individual mean composite wholes
proliferation fake news ie news intentionally spread misinformation pose threat individuals society despite various fact check websites politifact robust detection techniques require deal increase fake news several deep learn model show promise result fake news classification however black box nature make difficult explain classification decisions quality assure model address problem propose novel interpretable fake news detection framework base recently introduce tsetlin machine tm brief utilize conjunctive clauses tm capture lexical semantic properties true fake news text use clause ensembles calculate credibility fake news evaluation conduct experiment two publicly available datasets politifact gossipcop demonstrate tm framework significantly outperform previously publish baselines least five term accuracy add benefit interpretable logic base representation approach provide higher f1 score bert xlnet however obtain slightly lower accuracy finally present case study model explainability demonstrate decompose meaningful word negations
study test robustness three communication network extract online forums include intranet platforms three large company company analyze communication among employees term network structure content language use period eight months analyze fifty-two thousand message post approximately twelve thousand employees specifically test network robustness stability set structural semantic metrics apply several different node removal strategies remove forum moderators spammers overly connect nod nod lie network periphery also test different combinations selections result indicate remove spammers peripheral nod relatively low impact strategy context accordingly could use clean noise generate type social actor reduce computation complexity analysis hand removal moderators seem significant impact network connectivity share content affect variables closeness centrality contribution index also find removal overly connect nod significantly change network structure lastly compare behavior moderators users find distinctive characteristics moderators identify list unknown find help online community managers understand role moderators within intranet forums useful social network analysts interest evaluate effect graph simplification techniques
computer aid tabular data extraction always challenge error prone task demand spectral spatial sanity data paper discuss approach tabular data extraction realm document comprehension give different kinds tabular format often find across various document discuss novel approach use computer vision extraction tabular data image vector pdfs convert image
study look signal economic awareness online social media test significance economic predictions study analyse period two years relationship west texas intermediate daily crude oil price multiple predictors extract twitter google trend wikipedia global data events language tone database gdelt semantic analysis apply study sentiment emotionality complexity language use autoregressive integrate move average explanatory variable arimax model use make predictions confirm value study variables result show combine analysis four media platforms carry valuable information make financial forecast twitter language complexity gdelt number article wikipedia page read highest predictive power study also allow comparison different fore sight abilities platform term many days ahead platform predict price movement happen comparison previous work media source dimension interaction language use combine joint analysis
study propose method base e mail social network analysis compare communication behavior managers voluntarily quit job managers decide stay collect eighteen months e mail analyze communication behavior eight hundred and sixty-six managers one hundred and eleven leave large global service company compare differences communication pattern compute social network metrics betweenness closeness centrality content analysis indicators emotionality complexity language use study emergence managers disengagement make distinction base period e mail data examine observe communications months five four managers leave find significant variations network structure use language result indicate average managers quit lower closeness centrality less engage conversations addition managers choose quit tend shift communication behavior start five months leave increase degree closeness centrality complexity language well oscillations betweenness centrality number nudge need send peer get answer
describe semeval two thousand and twenty-one task six detection persuasion techniques texts image data annotation guidelines evaluation setup result participate systems task focus memes three subtasks detect techniques text ii detect text span techniques use iii detect techniques entire meme ie text image popular task attract seventy-one registrations twenty-two team eventually make official submission test set evaluation result third subtask confirm importance modalities text image moreover team report benefit combine two modalities eg use early late fusion rather model interaction joint model
two thousand and nineteen center medicare medicaid service cms launch artificial intelligence ai health outcomes challenge seek solutions predict risk value base care incorporation cms innovation center payment service delivery model recently modern language model play key roles number health relate task paper present best knowledge first application model patient readmission prediction facilitate create dataset twelve million medical history sample derive limit dataset lds issue cms moreover propose comprehensive model solution center deep learn framework data demonstrate framework train attention base transformer learn medicare semantics support perform downstream prediction task thereby achieve ninety-one auc ninety-one recall readmission classification also introduce novel data pre process pipeline discuss pertinent deployment considerations surround model explainability bias
recent study construct direct interactions claim single user response comment relevant article capture evidence show remarkable success interpretable claim verification owe different single responses convey different cognition individual users ie audiences capture evidence belong perspective individual cognition however individuals cognition social things always able truly reflect objective may one side bias semantics opinions claim capture evidence correspondingly contain unobjective bias evidence fragment deteriorate task performance paper propose dual view model base view collective individual cognition cicd interpretable claim verification view collective cognition capture word level semantics base individual users also focus sentence level semantics ie overall responses among users adjust proportion generate global evidence view individual cognition select top k article high degree difference interact claim explore local key evidence fragment weaken bias individual cognition view evidence devise inconsistent loss suppress divergence global local evidence strengthen consistent share evidence experiment three benchmark datasets confirm cicd achieve state art performance
investigate impact novel method call virtual mirror promote employee self reflection impact customer satisfaction method base measure communication pattern social network semantic analysis mirror back individual goal demonstrate self reflection trigger change communication behaviors lead increase customer satisfaction illustrate test approach analyze e mail large global service company compare change customer satisfaction associate team leaders expose virtual mirror experimental group find increase customer satisfaction experimental group decrease control group team leaders involve virtual mirror process regard individual communication indicators find customer satisfaction higher employees responsive use simpler language embed less centralize communication network show stable leadership pattern
evaluate performance automatic speech recognition model usually word error rate within certain dataset use special care must take understand dataset order report realistic performance number argue many performance number report probably underestimate expect error rate conduct experiment control selection bias gender well overlap train test data content voice record condition find content overlap biggest impact factor like gender also play role
paper present novel high fidelity low latency universal neural vocoder framework base multiband wavernn data drive linear prediction discrete waveform model mwdlp mwdlp employ coarse fine bite wavernn architecture ten bite mu law waveform model sparse gate recurrent unit relatively large size hide units utilize multiband model deploy achieve real time low latency usage novel technique data drive linear prediction lp discrete waveform model propose lp coefficients estimate data drive manner moreover novel loss function use short time fourier transform stft discrete waveform model gumbel approximation also propose experimental result demonstrate propose mwdlp framework generate high fidelity synthetic speech see unseen speakers language three hundred speakers train data include clean noisy reverberant condition number train utterances limit sixty per speaker allow real time low latency process use single core sim twenty-one 27ghz cpu sim fifty-seven sixty-four real time factor include input output feature extraction
paper present low latency real time llrt non parallel voice conversion vc framework base cyclic variational autoencoder cyclevae multiband wavernn data drive linear prediction mwdlp cyclevae robust non parallel multispeaker spectral model utilize speaker independent latent space speaker dependent code generate reconstruct convert spectral feature give spectral feature input speaker hand mwdlp efficient high quality neural vocoder handle multispeaker data generate speech waveform llrt applications cpu accommodate llrt constraint cpu propose novel cyclevae framework utilize mel spectrogram spectral feature build sparse network architecture improve model performance also propose novel fine tune procedure refine frame rate cyclevae network utilize waveform loss mwdlp network experimental result demonstrate propose framework achieve high performance vc allow llrt usage single core twenty-one 27ghz cpu real time factor eighty-seven ninety-five include input output feature extraction frame shift ten ms window length two hundred and seventy-five ms two lookup frame
online search query come voice automatic speech recognition become key component deliver relevant search result errors introduce automatic speech recognition asr lead irrelevant search result return user thus cause user dissatisfaction paper introduce approach mondegreen correct voice query text space without depend audio signal may always available due system constraints privacy bandwidth example asr systems run device considerations focus voice query transcribe via several proprietary commercial asr systems query come users make internet online service search query first present analysis show different language distribution come user voice query traditional text corpora use train shelf asr systems demonstrate mondegreen achieve significant improvements increase user interaction correct user voice query one largest search systems google finally see mondegreen complement exist highly optimize production asr systems may frequently retrain thus lag behind due vocabulary drift
program one broadly applicable skills modern society modern machine learn model still code solutions basic problems despite importance surprisingly little work evaluate code generation difficult accurately assess code generation performance rigorously meet challenge introduce apps benchmark code generation unlike prior work restrict settings benchmark measure ability model take arbitrary natural language specification generate satisfactory python code similar company assess candidate software developers evaluate model check generate code test case benchmark include ten thousand problems range simple one line solutions substantial algorithmic challenge fine tune large language model github train set find prevalence syntax errors decrease exponentially model improve recent model gpt neo pass approximately twenty test case introductory problems find machine learn model begin learn code social significance automatic code generation increase come years benchmark provide important measure track advancements
datasets induce emotion label scarce utmost importance many nlp task present new automate method collect texts along induce reaction label method exploit online use reaction gifs capture complex affective state show augment data induce emotion induce sentiment label use method create publish reactiongif first kind affective dataset 30k tweet provide baselines three new task include induce sentiment prediction multilabel classification induce emotions method dataset open new research opportunities emotion detection affective compute
story visualization explore task fall intersection many important research directions computer vision natural language process task give series natural language caption compose story agent must generate sequence image correspond caption prior work introduce recurrent generative model outperform text image synthesis model task however room improvement generate image term visual quality coherence relevance present number improvements prior model approach include one addition dual learn framework utilize video caption reinforce semantic alignment story generate image two copy transform mechanism sequentially consistent story visualization three mart base transformers model complex interactions frame present ablation study demonstrate effect techniques generative power model individual image well entire narrative furthermore due complexity generative nature task standard evaluation metrics accurately reflect performance therefore also provide exploration evaluation metrics model focus aspects generate frame presence quality generate character relevance caption diversity generate image also present correlation experiment propose automate metrics human evaluations code data available https githubcom adymaharana storyviz
end end speak language understand slu recently attract increase interest compare conventional tandem base approach combine speech recognition language understand separate modules new approach extract users intentions directly speech signal result joint optimization low latency approach however typically design process one intention time lead users take multiple round fulfill requirements interact dialogue system paper propose stream end end framework process multiple intentions online incremental way backbone framework unidirectional rnn train connectionist temporal classification ctc criterion design intention identify sufficient evidence accumulate multiple intentions identify sequentially evaluate solution fluent speech command fsc dataset intent detection accuracy ninety-seven multi intent settings result comparable performance state art non stream model achieve online incremental way also employ model keyword spot task use google speech command dataset result also highly promise
topic model widely use study social phenomena conduct comparative study examine state art neural versus non neural topic model perform rigorous quantitative qualitative assessment dataset tweet covid nineteen pandemic result show neural topic model outperform classical counterparts standard evaluation metrics also produce coherent topics great benefit study complex social problems also propose novel regularization term neural topic model design address well document problem mode collapse demonstrate effectiveness
paper investigate research question senders large amount irrelevant unsolicited information commonly call spammers distort network structure social network two large social network analyze first extract twitter discourse big telecommunication company second obtain three years email communication two hundred managers work large multinational company work compare network robustness stability centrality interaction metrics well use language remove spammers least connect nod result show spammers significantly alter structure information carry network social indicators author additionally investigate correlation e mail subject line content track language sentiment emotionality complexity address case collect email body permit privacy reason find extend research robustness stability social network metrics application graph simplification strategies result practical implication network analysts company managers rely network analytics apply company email social media data support decision make process
dialogue system pipeline natural language generation nlg unit convert dialogue direction content correspond natural language realization recent trend dialogue systems first pre train large datasets fine tune supervise manner use datasets annotate application specific feature though novel behaviours learn custom annotation require effort severely bound quantity train set application specific nature limit reuse light recent success data drive approach propose novel future bridge nlg fbnlg concept dialogue systems simulators critical step fbnlg accept future user system utterance bridge present context towards future bridge enable self supervise train annotation free datasets decouple train nlg rest system fbnlg pre train massive datasets expect apply classical new dialogue scenarios minimal adaptation effort evaluate prototype fbnlg show future bridge viable approach universal shoot nlg task orient chit chat dialogues
twitter currently popular online social media platform allow users share user generate content publicly generate user data also crucial healthcare technologies discover pattern would hugely benefit several ways one applications automatically discover mental health problems eg depression previous study automatically detect depress user online social media largely rely upon user behaviour linguistic pattern include user social interactions downside model train several irrelevant content might crucial towards detect depress user besides content negative impact overall efficiency effectiveness model overcome shortcomings exist automatic depression detection methods propose novel computational framework automatic depression detection initially select relevant content hybrid extractive abstractive summarization strategy sequence user tweet lead fine grain relevant content content go novel deep learn framework comprise unify learn machinery comprise convolutional neural network cnn couple attention enhance gate recurrent units gru model lead better empirical performance exist strong baselines
advance pre train model eg bert xlnet etc largely revolutionize predictive performance various modern natural language process task allow corporations provide machine learn service mlaas encapsulate fine tune bert base model commercial apis however previous work discover series vulnerabilities bert base apis example bert base apis vulnerable model extraction attack adversarial example transferrability attack however due high capacity bert base apis fine tune model easy overlearned kind information leak extract model remain unknown lack bridge gap work first present effective model extraction attack adversary practically steal bert base api target victim model query limit number query develop effective attribute inference attack expose sensitive attribute train data use bert base apis extensive experiment benchmark datasets various realistic settings demonstrate potential vulnerabilities bert base apis
scientific document understand challenge data highly domain specific diverse however datasets task scientific text require expensive manual annotation tend small limit one field time scientific document contain many potential train signal citations use build large label datasets give present depth study cite worthiness detection english sentence label whether cite external source accomplish introduce citeworth large contextualized rigorously clean label dataset cite worthiness detection build massive corpus extract plain text scientific document show citeworth high quality challenge suitable study problems domain adaptation best perform cite worthiness detection model paragraph level contextualized sentence label model base longformer exhibit five f1 point improvement scibert consider individual sentence finally demonstrate language model fine tune cite worthiness secondary task lead improve performance downstream scientific document understand task
paper introduce new task controllable text edition take input long text question target answer output minimally modify text fit target answer task important many situations change condition consequences properties legal document change key information event news text challenge hard obtain parallel corpus train need first find text position change decide change construct new dataset wikibiocte task base exist dataset wikibio originally create table text generation use wikibiocte train manually label test set test also propose novel evaluation metrics novel method solve new task experimental result test set show propose method good fit novel nlp task
despite rapid progress recent past current speech recognition systems still require label train data limit technology small fraction languages speak around globe paper describe wav2vec short wav2vec unsupervised method train speech recognition model without label data leverage self supervise speech representations segment unlabeled audio learn map representations phonemes via adversarial train right representations key success method compare best previous unsupervised work wav2vec reduce phoneme error rate timit benchmark two hundred and sixty-one one hundred and thirteen larger english librispeech benchmark wav2vec achieve word error rate fifty-nine test rival best publish systems train nine hundred and sixty hours label data two years ago also experiment nine languages include low resource languages kyrgyz swahili tatar
despite impressive performance nlp self attention network recently prove limit process formal languages hierarchical structure mathsfdyckk language consist well nest parentheses k type suggest natural language approximate well model weak formal languages role hierarchy recursion natural language might limit qualify implication prove self attention network process mathsfdyckk subset mathsfdyckk depth bound arguably better capture bound hierarchical structure natural language specifically construct hard attention network d1 layer oflog k memory size per token per layer recognize mathsfdyckk soft attention network two layer oflog k memory size generate mathsfdyckk experiment show self attention network train mathsfdyckk generalize longer input near perfect accuracy also verify theoretical memory advantage self attention network recurrent network
pretrained language model lms perform well many task even learn examples prior work use many hold examples tune various aspects learn hyperparameters train objectives natural language templates prompt evaluate shoot ability lms hold examples unavailable set call true shoot learn test two model selection criteria cross validation minimum description length choose lm prompt hyperparameters true shoot set average marginally outperform random selection greatly underperform selection base hold examples moreover selection criteria often prefer model perform significantly worse randomly select ones find similar result even take account uncertainty model true performance selection well vary amount computation number examples use selection overall find suggest prior work significantly overestimate true shoot ability lms give difficulty shoot model selection
well know sufficiently young children encounter new word tend attach mean word yet lexicon previous research strategy show optimal information theoretic standpoint however information theoretic model employ neither explain weaken vocabulary learn bias older children polylinguals reproduce zipf mean frequency law namely non linear relationship number mean word frequency consider generalization model channel reproduce law analysis new model reveal regions phase space bias disappear consistently weaken loss bias older children polylinguals deep learn era model transparent low dimensional tool future experimental research illustrate predictive power theoretical framework originally design would light origins zipf rank frequency law
recent grid base document representations like bertgrid allow simultaneous encode textual layout information document 2d feature map state art image segmentation object detection model straightforwardly leverage extract key information document however methods achieve comparable performance state art sequence graph base methods layoutlm pick yet paper propose new multi modal backbone network concatenate bertgrid intermediate layer cnn model input cnn document image bertgrid grid word embeddings generate powerful grid base document representation name vibertgrid unlike bertgrid parameters bert cnn multimodal backbone network train jointly experimental result demonstrate joint train strategy improve significantly representation ability vibertgrid consequently vibertgrid base key information extraction approach achieve state art performance real world datasets
study look employees communication offer novel metrics help predict company stock price study intranet forum large italian company explore interactions use language eight thousand employees build network link word include general discourse network focus position node represent company brand find lower sentiment higher betweenness centrality company brand denser word co occurrence network equally distribute centrality score employees lower group betweenness centrality significant predictors higher stock price find offer new metrics helpful scholars company managers professional investors could integrate exist forecast model improve accuracy lastly contribute research word co occurrence network extend field application
data centric approach natural language process nlp predict personality type base mbti introspective self assessment questionnaire indicate different psychological preferences people perceive world make decisions systematic enrichment text representation base domain area generation feature base three type analysis sentimental grammatical aspects experimentation robust baseline stack model premature optimization hyperparameters grid search gradual feedback four classifiers dichotomies mbti result show attention data iteration loop focus quality explanatory power representativeness abstraction relevant important resources study phenomenon make possible improve evaluation metrics result quickly less costly complex model lstm state art ones bert well importance result comparisons make various perspectives addition study demonstrate broad spectrum evolution deepen task possible approach greater extension abstraction personality type
shoot learn propose rapidly emerge viable mean complete various task many shoot model widely use relation learn task however model shortage capture certain aspect semantic feature example cnn long range dependencies part transformer local feature difficult single model adapt various relation learn result high variance problem ensemble strategy could competitive improve accuracy shoot relation extraction mitigate high variance risk paper explore ensemble approach reduce variance introduce fine tune feature attention strategies calibrate relation level feature result several shoot relation learn task show model significantly outperform previous state art model
cross lingual speech adaptation aim solve problem leverage multiple rich resource languages build model low resource target language since low resource language limit train data speech recognition model easily overfit paper propose use adapters investigate performance multiple adapters parameter efficient cross lingual speech adaptation base previous metaadapter implicitly leverage adapters propose novel algorithms call simadapter explicitly learn knowledge adapters algorithm leverage adapters easily integrate transformer structuremetaadapter leverage meta learn transfer general knowledge train data test language simadapter aim learn similarities source target languages fine tune use adapters conduct extensive experiment five low resource languages common voice dataset result demonstrate metaadapter simadapter methods reduce wer two hundred and ninety-eight two hundred and fifty-five twenty-five one hundred and fifty-five trainable parameters compare strong full model fine tune baseline moreover also show two novel algorithms integrate better performance three hundred and fifty-five relative wer reduction
order face complexity business environments detect priorities trigger contingency strategies propose new methodological approach combine text mine social network big data analytics assessment stakeholders attitudes towards company core value approach apply case study consider twitter discourse core value italy collect ninety-four thousand tweet relate core value firm list fortune rank world admire company two thousand and thirteen two thousand and seventeen italian scenario find three predominant core value orientations customers employees excellence basis business strategy three latent ones economic financial growth citizenship social responsibility need periodic attention contribution mostly methodological extend research text mine online big data analytics apply complex business contexts
propose predict interpolate pi simple algorithm learn correlations stable across environments algorithm follow intuition use classifier train one environment make predictions examples another environment mistake informative correlations unstable work prove interpolate distributions correct predictions wrong predictions uncover oracle distribution unstable correlation vanish since oracle interpolation coefficients accessible use group distributionally robust optimization minimize worst case risk across interpolations evaluate method text classification image classification empirical result demonstrate algorithm able learn robust classifiers outperform irm two thousand, three hundred and eighty-five synthetic environments one thousand, two hundred and forty-one natural environments code data available https githubcom yujiabao predict interpolate
purpose purpose paper identify factor influence growth healthcare virtual communities practice vcops seven year longitudinal study conduct use metrics social network semantic analysis study online communication along three dimension social interactions connectivity interactivity language use author aim provide vcop managers valuable insights improve success communities design methodology approach communications period seven years april two thousand and eight april two thousand and fifteen fourteen thousand members sixteen different healthcare vcops coexist web platform analyse multilevel regression model use reveal main determinants community growth time independent variables derive social network semantic analysis measure find result show structural content base variables predict growth community progressively people join community structure centralise leaders dynamic rotate language use post less complex research limitations implications available data set include one web platform limit number control variables consolidate find present study experiment replicate healthcare vcops originality value study provide useful recommendations set nurture growth professional communities consider time interaction pattern among community members dynamic evolution interactions use language new analytical tool present together use innovative interaction metrics significantly influence community growth rotate leadership
loanwords anglicisms challenge german speech recognition due irregular pronunciation compare native german word automatically generate pronunciation dictionaries often include faulty phoneme sequence anglicisms work propose multitask sequence sequence approach grapheme phoneme conversion improve phonetization anglicisms extend grapheme phoneme model classifier distinguish anglicisms native german word approach model learn generate pronunciations differently depend classification result use model create supplementary anglicism pronunciation dictionaries add exist german speech recognition model test dedicate anglicism evaluation set improve recognition anglicisms compare baseline model reduce word error rate one anglicism error rate three show multitask learn help solve challenge loanwords german speech recognition
paper introduce event drive trade strategy predict stock movements detect corporate events news article unlike exist model utilize textual feature eg bag word sentiments directly make stock predictions consider corporate events drive force behind stock movements aim profit temporary stock mispricing may occur corporate events take place core propose strategy bi level event detection model low level event detector identify events existences token high level event detector incorporate entire article representation low level detect result discover events article level also develop elaborately annotate dataset edt corporate event detection news base stock prediction benchmark edt include nine thousand, seven hundred and twenty-one news article token level event label well three hundred and three thousand, eight hundred and ninety-three news article minute level timestamps comprehensive stock price label experiment edt indicate propose strategy outperform baselines win rate excess return market average return transaction
recent research consider shoot intent detection meta learn problem model learn learn consecutive set small task name episodes work propose protaugment meta learn algorithm short texts classification intent detection task protaugment novel extension prototypical network limit overfitting bias introduce shots classification objective episode rely diverse paraphrase conditional language model first fine tune paraphrase diversity later introduce decode stage meta learn episode diverse paraphrase unsupervised apply unlabelled data fuel prototypical network train objective consistency loss protaugment state art method intent detection meta learn extra label efforts without need fine tune conditional language model give application domain
information economy individuals work performance closely associate digital communication strategies study combine social network semantic analysis develop method identify top performers base email communication review exist literature identify indicators quantify email communication measurable dimension empirically examine predictive power propose indicators collect two million email archive five hundred and seventy-eight executives international service company panel regression employ derive interpretable association email indicators top performance result suggest top performers tend assume central network position high responsiveness email email content top performers use positive complex language low emotionality rich influential word probably reuse co workers better explore predictive power email indicators employ adaboost machine learn model achieve eight thousand, three hundred and fifty-six accuracy identify top performers cluster analysis find three categories top performers networkers central network position influencers influential ideas positivists positive sentiments find suggest top performers distinctive email communication pattern lay foundation ground email communication competence theory propose email analysis method also provide tool evaluate different type individual communication style
recently opinion summarization generation summary multiple review conduct self supervise manner consider sample review pseudo summary however non text data image metadata relate review consider less often use abundant information contain non text data propose self supervise multimodal opinion summarization framework call multimodalsum framework obtain representation modality use separate encoder modality text decoder generate summary resolve inherent heterogeneity multimodal data propose multimodal train pipeline first pretrain text encoder decoder base solely text modality data subsequently pretrain non text modality encoders consider pretrained text decoder pivot homogeneous representation multimodal data finally fuse multimodal representations train entire framework end end manner demonstrate superiority multimodalsum conduct experiment yelp amazon datasets
machine learn model train organizational communication data email enterprise carry unique risk breach confidentiality even model intend internal use work show confidentiality distinct privacy enterprise context aim formulate approach preserve confidentiality leverage principles differential privacy goal perform machine learn task learn language model perform topic analysis use interpersonal communications organization learn confidential information share organization work apply differential privacy techniques natural language process task usually assume independently distribute data overlook potential correlation among record ignore correlation result fictional promise privacy naively extend differential privacy techniques focus group privacy instead record level privacy straightforward approach mitigate issue approach although provide realistic privacy guarantee cautious severely impact model utility show gap two extreme measure privacy two language task introduce middle grind solution propose model capture correlation social network graph incorporate correlation privacy calculations pufferfish privacy principles
sing voice conversion svc one promise technique enrich way human computer interaction endow computer ability produce high fidelity expressive sing voice paper propose diffsvc svc system base denoising diffusion probabilistic model diffsvc use phonetic posteriorgrams ppgs content feature denoising module train diffsvc take destroy mel spectrogram produce diffusion forward process correspond step information input predict add gaussian noise use ppgs fundamental frequency feature loudness feature auxiliary input assist denoising process experiment show diffsvc achieve superior conversion performance term naturalness voice similarity current state art svc approach
recent explorations large scale pre train language model plms gpt three reveal power plms huge amount parameters set wave train ever larger plms however train large scale plm require tremendous amount computational resources time consume expensive addition exist large scale plms mainly train scratch individually ignore availability many exist well train plms end explore question previously train plms benefit train larger plms future specifically introduce novel pre train framework name knowledge inheritance ki combine self learn teacher guide learn efficiently train larger plms sufficient experimental result demonstrate feasibility ki framework also conduct empirical analyse explore effect teacher plms pre train settings include model architecture pre train data etc finally show ki well support lifelong learn knowledge transfer
paper introduce target aware weight train tawt weight train algorithm cross task learn base minimize representation base task distance source target task show tawt easy implement computationally efficient require little hyperparameter tune enjoy non asymptotic learn theoretic guarantee effectiveness tawt corroborate extensive experiment bert four sequence tag task natural language process nlp include part speech pos tag chunk predicate detection name entity recognition ner byproduct propose representation base task distance allow one reason theoretically principled way several critical aspects cross task learn choice source data impact fine tune
introduce attention free transformer aft efficient variant transformers eliminate need dot product self attention aft layer key value first combine set learn position bias result multiply query element wise fashion new operation memory complexity linear wrt context size dimension feature make compatible large input model size also introduce aft local aft conv two model variants take advantage idea locality spatial weight share maintain global connectivity conduct extensive experiment two autoregressive model task cifar10 enwik8 well image recognition task imagenet 1k classification show aft demonstrate competitive performance benchmarks provide excellent efficiency time
help individuals express better quotation recommendation receive grow attention nevertheless prior efforts focus model quotations query separately ignore relationship quotations query work introduce transformation matrix directly map query representations quotation representations better learn map relationship employ map loss minimize distance two semantic space one quotation another map query furthermore explore use word history query interpret figurative language quotations quotation aware attention apply top history query highlight indicator word experiment two datasets english chinese show model outperform previous state art model
measure public opinion key focus democratic elections enable candidates gauge popularity alter campaign strategies accordingly traditional survey poll remain popular estimation technique despite cost time intensity measurement errors lack real time capabilities lag representation public opinion recent years twitter opinion mine attempt combat issue despite achieve promise result experience set shortcomings unrepresentative sample population lack long term stability paper aim merge data techniques use bayesian data assimilation arrive accurate estimate true public opinion brexit referendum paper demonstrate effectiveness propose approach use twitter opinion data survey data trust pollsters firstly possible existence time gap sixteen days two data set identify gap subsequently incorporate propose assimilation architecture method find adequately incorporate information source measure strong upward trend leave support lead brexit referendum propose technique provide useful estimate true opinion essential future opinion measurement forecast research
follow procedural texts write natural languages challenge must read whole text identify relevant information identify instruction flow complete task prone failures texts structure readily visualize instruction flow reason infer particular step even build automate systems help novice agents achieve goal however structure recovery task challenge texts diverse nature paper propose identify relevant information texts generate information flow sentence build large annotate procedural text dataset ctfw cybersecurity domain three thousand, one hundred and fifty-four document dataset contain valuable instructions regard software vulnerability analysis experience perform extensive experiment ctfw lm gnn model variants multiple settings show generalizability task method also experiment procedural texts two domains maintenance manual cook substantially different cybersecurity experiment show graph convolution network bert sentence embeddings outperform bert three domains
inject external domain specific knowledge eg umls pretrained language model lms advance capability handle specialise domain task biomedical entity link bel however abundant expert knowledge available handful languages eg english work propose novel cross lingual biomedical entity link task xl bel establish new xl bel benchmark span ten typologically diverse languages first investigate ability standard knowledge agnostic well knowledge enhance monolingual multilingual lms beyond standard monolingual english bel task score indicate large gap english performance address challenge transfer domain specific knowledge resource rich languages resource poor ones end propose evaluate series cross lingual transfer methods xl bel task demonstrate general domain bitext help propagate available english knowledge languages little domain data remarkably show propose domain specific transfer methods yield consistent gain across target languages sometimes twenty precision1 point without domain knowledge target language without domain parallel data
word mover distance wmd fundamental technique measure similarity two document crux wmd take advantage underlie geometry word space employ optimal transport formulation original study wmd report wmd outperform classical baselines bag word bow tf idf significant margins various datasets paper point evaluation original study could mislead evaluate performances wmd classical baselines find classical baselines competitive wmd employ appropriate preprocessing ie l1 normalization however result intuitive wmd superior bow wmd take underlie geometry account whereas bow analysis show due high dimensional nature underlie metric find wmd high dimensional space behave similarly bow low dimensional space due curse dimensionality
pre train language model eg bert achieve impressive result different natural language process task large number parameters suffer big computational memory cost make difficult real world deployment therefore model compression necessary reduce computation memory cost pre train model work aim compress bert address follow two challenge practical issue one compression algorithm able output multiple compress model different size latencies order support devices different memory latency limitations two algorithm downstream task agnostic compress model generally applicable different downstream task leverage techniques neural architecture search nas propose nas bert efficient method bert compression nas bert train big supernet search space contain variety architectures output multiple compress model adaptive size latency furthermore train nas bert conduct standard self supervise pre train task eg mask language model depend specific downstream task thus compress model use across various downstream task technical challenge nas bert train big supernet pre train task extremely costly employ several techniques include block wise search search space prune performance approximation improve search efficiency accuracy extensive experiment glue squad benchmark datasets demonstrate nas bert find lightweight model better accuracy previous approach directly apply different downstream task adaptive model size different requirements memory latency
automatically generate medical report retinal image one promise ways help ophthalmologists reduce workload improve work efficiency work propose new context drive encode network automatically generate medical report retinal image propose model mainly compose multi modal input encoder fuse feature decoder experimental result show propose method capable effectively leverage interactive information input image context ie keywords case propose method create accurate meaningful report retinal image baseline model achieve state art performance performance show several commonly use metrics medical report generation task bleu avg sixteen cider one hundred and two rouge eighty-six
speech disorder often occur early stage parkinson disease pd speech impairments could indicators disorder early diagnosis motor symptoms obvious study construct new speech corpus mandarin chinese address classification patients pd implement classical machine learn methods rank algorithms feature selection convolutional recurrent deep network end end system classification accuracy significantly surpass state art study result suggest free talk stronger classification power standard speech task could help design future speech task efficient early diagnosis disease base exist classification methods natural speech study automatic detection pd daily conversation could accessible majority clinical population
advent globalization increase demand multilingual automatic speech recognition asr handle language dialectal variation speak content recent study show efficacy monolingual systems study design large multilingual end end asr use self attention base conformer architecture train system use arabic ar english en french fr languages evaluate system performance handle monolingual ar en fr ii multi dialectal modern standard arabic along dialectal variation egyptian moroccan iii code switch cross lingual ar en fr dialectal msa egyptian dialect test case compare current state art systems furthermore investigate influence different embed character representations include character vs word piece share vs distinct input symbol per language find demonstrate strength model outperform state art monolingual dialectal arabic code switch arabic asr
modern neural machine translation nmt model achieve competitive performance standard benchmarks wmt however still exist significant issue robustness domain generalization etc paper study nmt model perspective compositional generalization build benchmark dataset cognition consist 216k clean consistent sentence pair quantitatively analyze effect various factor use compound translation error rate demonstrate nmt model fail badly compositional generalization although perform remarkably well traditional metrics
present annotation approach capture emotional cognitive empathy student write peer review business model german propose annotation scheme allow us model emotional cognitive empathy score base three type review components also conduct annotation study three annotators base ninety-two student essay evaluate annotation scheme obtain inter rater agreement alpha079 components multi pi041 empathy score indicate propose annotation scheme successfully guide annotators substantial moderate agreement moreover train predictive model detect annotate empathy structure embed adaptive write support system students receive individual empathy feedback independent instructor time location evaluate tool peer learn exercise fifty-eight students find promise result perceive empathy skill learn perceive feedback accuracy intention use finally present freely available corpus five hundred empathy annotate student write peer review business model annotation guidelines encourage future research design development empathy support systems
bangla language seventh speak language two hundred and sixty-five million native non native speakers worldwide however english predominant language online resources technical knowledge journals documentation consequently many bangla speak people limit command english face hurdle utilize english resources bridge gap limit support increase demand researchers conduct many experiment develop valuable tool techniques create process bangla language materials many efforts also ongoing make easy use bangla language online technical domains review paper understand past previous future bangla natural language process bnlp trend study mainly concentrate specific domains bnlp sentiment analysis speech recognition optical character recognition text summarization apparent scarcity resources contain comprehensive study recent bnlp tool methods therefore paper present thorough review seventy-one bnlp research paper categorize eleven categories namely information extraction machine translation name entity recognition parse part speech tag question answer system sentiment analysis spam fake detection text summarization word sense disambiguation speech process recognition study article publish one thousand, nine hundred and ninety-nine two thousand and twenty-one fifty paper publish two thousand and fifteen discuss classical machine learn deep learn approach different datasets address limitations current future trend bnlp
recent study analysis multilingual representations focus identify whether emergence language independent representations whether multilingual model partition weight among different languages work conduct black box manner paper aim analyze individual components multilingual neural translation nmt model particular look encoder self attention encoder decoder attention head many one nmt model specific translation certain language pair others one employ metrics quantify aspects attention weight variance confidence two systematically rank importance attention head respect translation quality experimental result show surprisingly set important attention head similar across language pair possible remove nearly one third less important head without hurt translation quality greatly
crowdsourcing regard one prospective solution effective supervise learn aim build large scale annotate train data crowd workers previous study focus reduce influence noise crowdsourced annotations supervise model take different point work regard crowdsourced annotations gold standard respect individual annotators way find crowdsourcing could highly similar domain adaptation recent advance cross domain methods almost directly apply crowdsourcing take name entity recognition ner study case suggest annotator aware representation learn model inspire domain adaptation methods attempt capture effective domain aware feature investigate unsupervised supervise crowdsourcing learn assume small scale expert annotations available experimental result benchmark crowdsourced ner dataset show method highly effective lead new state art performance addition supervise set achieve impressive performance gain small scale expert annotations
voice conversion task convert speak utterance source speaker appear say different target speaker retain linguistic content utterance recent advance lead major improvements quality voice conversion systems however useful wider range contexts voice conversion systems would need trainable without access parallel data ii work zero shoot set source target speakers unseen train iii run real time faster recent techniques fulfil one two requirements three paper extend recent voice conversion model base generative adversarial network gans satisfy three condition specifically extend recent stargan vc model condition speaker embed potentially unseen speaker allow model use zero shoot set therefore call stargan zsvc compare stargan zsvc voice conversion techniques low resource set use small nine minute train set compare autovc another recent neural zero shoot approach observe stargan zsvc give small improvements zero shoot set show real time zero shoot voice conversion possible even model train little data work require see whether scale stargan zsvc also improve zero shoot voice conversion quality high resource contexts
memo describe ntr tsu win submission low resource asr challenge dialog2021 conference language identification track speak language identification lid important step multilingual automate speech recognition asr system pipeline traditionally asr task require large volumes label data unattainable world languages include languages russia memo show convolutional neural network self attentive pool layer show promise result low resource set language identification task set sota low resource asr challenge dataset additionally compare structure confusion matrices significantly diverse voxforge dataset state substantiate hypothesis whenever dataset diverse enough classification factor like gender age etc well average confusion matrix lid system bear language similarity measure
study word level quality estimation qe machine translation focus language specific model obvious disadvantage approach need label data language pair high cost require maintain several language specific model overcome problems explore different approach multilingual word level qe show qe model perform par current language specific model case zero shoot shoot qe demonstrate possible accurately predict word level quality give new language pair model train language pair find suggest word level qe model base powerful pre train transformers propose paper generalise well across languages make useful real world scenarios
paper describe system submit iwslt two thousand and twenty-one multilingual speech translation multist task huawei noah ark lab use unify transformer architecture multist model data different modalities ie speech text different task ie speech recognition machine translation speech translation exploit enhance model ability specifically speech text input firstly feed different feature extractors extract acoustic textual feature respectively feature process share encoder decoder architecture apply several train techniques improve performance include multi task learn task level curriculum learn data augmentation etc final system achieve significantly better result bilingual baselines supervise language pair yield reasonable result zero shoot language pair
current pandemic force people globally remain isolation practice social distance create need system combat result loneliness negative emotions paper propose nora virtual coach platform design utilize natural language understand dialogue system suggest recommendations base user interactions intend provide assistance companionship people undergo self quarantine work home routines nora help users gauge well detect record user emotion sentiment stress nora also recommend various workout meditation yoga exercise users support develop healthy daily routine addition provide social community inside nora users connect share experience others undergo similar isolation procedure nora access anywhere via web link support english mandarin
nowadays videos internet prevail precise depth understand videos difficult valuable problem platforms researchers exist video understand model well object recognition task currently still understand abstract contextual feature like highlight humor frame comedy videos current industrial work also mainly focus basic category classification task base appearances object feature detection methods abstract category remain blank data structure include information video frame audio spectrum texts provide new direction explore multimodal model propose make depth video understand mission possible paper analyze difficulties abstract understand videos propose multimodal structure obtain state art performance field select several benchmarks multimodal video understand apply suitable model find best performance last evaluate overall spotlight drawbacks model methods paper point possible directions improvements
commonsense inference understand explain human language fundamental research problem natural language process explain human conversations pose great challenge require contextual understand plan inference several aspects reason include causal temporal commonsense reason work introduce cider manually curated dataset contain dyadic dialogue explanations form implicit explicit knowledge triplets infer use contextual commonsense inference extract rich explanations conversations conducive improve several downstream applications annotate triplets categorize type commonsense knowledge present eg causal conditional temporal set three different task condition annotate dataset dialogue level natural language inference span extraction multi choice span selection baseline result obtain transformer base model reveal task difficult pave way promise future research dataset baseline implementations publicly available https githubcom declare lab cider
effectively model text rich fresh content news article document level challenge problem ensure content base model generalize well broad range applications critical train dataset large beyond scale human label achieve desire quality work address two challenge propose novel approach mine semantically relevant fresh document topic label little human supervision meanwhile design multitask model call newsembed alternatively train contrastive learn multi label classification derive universal document encoder show propose approach provide billions high quality organic train examples naturally extend multilingual set texts different languages encode semantic space experimentally demonstrate newsembed competitive performance across multiple natural language understand task supervise unsupervised
past decade much discussion issue bias report clinical research despite attention limit tool develop systematic assessment qualitative statements make clinical research study assess qualitative statements rely use manual expert raters limit size also previous attempt develop larger scale tool use natural language process limit accuracy number categories use classification find limitations mind study goal develop classification algorithm suitably accurate finely grain apply large scale assess qualitative sentiment express clinical trial abstract additionally study seek compare performance propose algorithm gin biobert previous study well expert manual rat clinical trial abstract study develop three class sentiment classification algorithm clinical trial abstract use semi supervise natural language process model base bidirectional encoder representation transformers bert model series clinical trial abstract annotate group experts academic medicine result use algorithm find classification accuracy nine hundred and thirteen macro f1 score ninety-two significant improvement accuracy compare previous methods expert rat also make sentiment classification finer grain previous study propose algorithm gin biobert suitable classification model large scale assessment qualitative statements clinical trial literature provide accurate reproducible tool large scale study clinical publication trend
research paper elaborate method evaluate machine translation model base performance underlie syntactical phenomena english arabic languages method especially important neural machine learn hard fine tune change thus find way evaluate easily diversely would greatly help task better
feature importance fi estimate popular form explanation commonly create evaluate compute change model confidence cause remove certain input feature test time example standard sufficiency metric top k important tokens keep paper study several explore dimension fi base explanations provide conceptual empirical improvements form explanation first advance new argument problematic remove feature input create evaluate explanations fact counterfactual input distribution ood model imply result explanations socially misalign crux problem model prior random weight initialization influence explanations explanation metrics unintended ways resolve issue propose simple alteration model train process result socially align explanations metrics second compare among five approach remove feature model input find methods produce ood counterfactuals others make recommendations select feature replacement function finally introduce four search base methods identify fi explanations compare strong baselines include lime integrate gradients random search experiment six diverse text classification datasets find method consistently outperform random search parallel local search introduce improvements second best method large fifty-four point sufficiency seventeen point comprehensiveness support code publicly available https githubcom peterbhase explanationsearch
crowdsourcing widely use create data common natural language understand task despite importance datasets measure refine model understand language little focus crowdsourcing methods use collect datasets paper compare efficacy interventions propose prior work ways improve data quality use multiple choice question answer testbed run randomize trial assign crowdworkers write question one four different data collection protocols find ask workers write explanations examples ineffective stand alone strategy boost nlu example difficulty however find train crowdworkers use iterative process collect data send feedback qualify workers base expert judgments effective mean collect challenge data use crowdsourced instead expert judgments qualify workers send feedback prove effective observe data iterative protocol expert assessments challenge several measure notably human model gap unanimous agreement portion data average twice large gap baseline protocol data
adversarial data collection adc human workforce interact model real time attempt produce examples elicit incorrect predictions researchers hope model train challenge datasets rely less superficial pattern thus less brittle however despite adc intuitive appeal remain unclear train adversarial datasets produce robust model paper conduct large scale control study focus question answer assign workers random compose question either adversarially model loop ii standard fashion without model across variety model datasets find model train adversarial data usually perform better adversarial datasets worse diverse collection domain evaluation set finally provide qualitative analysis adversarial vs standard data identify key differences offer guidance future research
question answer qa systems provide way query information available various format include limit unstructured structure data natural languages constitute considerable part conversational artificial intelligence ai lead introduction special research topic conversational question answer cqa wherein system require understand give context engage multi turn qa satisfy user information need whilst focus exist research work subject single turn qa field multi turn qa recently grasp attention prominence owe availability large scale multi turn qa datasets development pre train language model good amount model research paper add literature every year recently dire need arrange present relate work unify manner streamline future research survey therefore effort present comprehensive review state art research trend cqa primarily base review paper two thousand and sixteen two thousand and twenty-one find show trend shift single turn multi turn qa empower field conversational ai different perspectives survey intend provide epitome research community hope lay strong foundation field cqa
successfully negotiate deal enough communicate fluently pragmatic plan persuasive negotiation strategies essential modern dialogue agents excel generate fluent sentence still lack pragmatic ground reason strategically present dialograph negotiation system incorporate pragmatic strategies negotiation dialogue use graph neural network dialograph explicitly incorporate dependencies sequence strategies enable improve interpretable prediction next optimal strategies give dialogue context graph base method outperform prior state art negotiation model accuracy strategy dialogue act prediction quality downstream dialogue response generation qualitatively show benefit learn strategy graph provide explicit associations effective negotiation strategies course dialogue lead interpretable strategic dialogues
self train prove effective improve nmt performance augment model train synthetic parallel data common practice construct synthetic data base randomly sample subset large scale monolingual data empirically show sub optimal work propose improve sample procedure select informative monolingual sentence complement parallel data end compute uncertainty monolingual sentence use bilingual dictionary extract parallel data intuitively monolingual sentence lower uncertainty generally correspond easy translate pattern may provide additional gain accordingly design uncertainty base sample strategy efficiently exploit monolingual data self train monolingual sentence higher uncertainty would sample higher probability experimental result large scale wmt englishrightarrowgerman englishrightarrowchinese datasets demonstrate effectiveness propose approach extensive analyse suggest emphasize learn uncertain monolingual sentence approach improve translation quality high uncertainty sentence also benefit prediction low frequency word target side
partial label learn pll generally focus induce noise tolerant multi class classifier train overly annotate sample annotate set label one valid label basic promise exist pll solutions sufficient partial label pl sample train however common pl sample hand deal new task furthermore exist shoot learn algorithms assume precise label support set irrelevant label may seriously mislead meta learner thus lead compromise performance enable pll shoot learn set important problem yet well study paper introduce approach call fspll shoot pll fspll first perform adaptive distance metric learn embed network rectify prototypes task previously encounter next calculate prototype class new task embed network unseen example classify via distance prototype experimental result widely use shoot datasets omniglot miniimagenet demonstrate fspll achieve superior performance state art methods across different settings need fewer sample quickly adapt new task
understand blame support news text critical research question computational social science traditional methods datasets sentiment analysis however suitable domain political text consider direction sentiments express entities paper propose novel nlp task identify direct sentiment relationship political entities give news document call direct sentiment extraction million scale news corpus construct dataset news sentence sentiment relations political entities manually annotate present simple effective approach utilize pretrained transformer infer target class predict multiple question answer task combine outcomes demonstrate utility propose method social science research question analyze positive negative opinions political entities two major events two thousand and sixteen yous presidential election covid nineteen newly propose problem data method facilitate future study interdisciplinary nlp methods applications
paper introduce task factual error correction perform edit claim generate rewrite better support evidence extend well study task fact verification provide mechanism correct write texts refute partially support evidence demonstrate feasible train factual error correction systems exist fact check datasets contain label claim accompany evidence correction achieve employ two stage distant supervision approach incorporate evidence mask claim generate corrections approach base t5 transformer use retrieve evidence achieve better result exist work use pointer copy network gold evidence produce accurate factual error corrections 5x instance human evaluation one hundred and twenty-five increase sari score evaluation conduct dataset sixty-five thousand instance base recent fact verification share task release enable work task
neural model show impressive performance gain answer query natural language text however exist work unable support database query list count female athletes bear 20th century require reason set relevant facts operations join filter aggregation show state art transformer model perform well small databases exhibit limitations process noisy data numerical operations query aggregate facts propose modular architecture answer database style query multiple span text aggregate scale evaluate architecture use wikinldb novel dataset explore query architecture scale databases contain thousands facts whereas contemporary model limit many facts encode direct comparison small databases approach increase overall answer accuracy eighty-five ninety larger databases approach retain accuracy whereas transformer baselines could encode context
semi supervise train sst common approach leverage untranscribed unlabeled speech data improve automatic speech recognition performance low resource languages however available unlabeled speech mismatch target domain sst effective many case perform worse original system paper address issue low resource asr untranscribed domain speech data readily available target language specifically look improve performance conversational telephony speech target domain use web resources particular youtube data closely resemble news topical broadcast data leverage sst show case simply pool domain data train data lower word error rate wer case see improvements train first domain data fine tune result model original train data use two thousand hours speed perturb youtube audio target language semi supervise transcripts show improvements multiple languages data set one hundred and sixty-three relative improvement wer baseline systems seventy-four relative improvement wer system simply pool domain data train data
abstractive summarization task generate concise summary input document require one reason source document determine salient piece information scatter across long document two compose cohesive text reconstruct salient facts shorter summary faithfully reflect complex relations connect facts paper adapt tp transformer schlag et al two thousand and nineteen architecture enrich original transformer vaswani et al two thousand and seventeen explicitly compositional tensor product representation tpr task abstractive summarization key feature model structural bias introduce encode two separate representations token represent syntactic structure role vectors semantic content filler vectors separately model bind role filler vectors tpr layer output argue structure intermediate representations enable model take better control content salient facts structure syntax connect facts generate summary empirically show tp transformer outperform transformer original tp transformer significantly several abstractive summarization datasets base automatic human evaluations several syntactic semantic probe task demonstrate emergent structural information role vectors improve syntactic interpretability tpr layer output code model available https githubcom jiangyctarheel tpt summ
focus type linguistic formal reason goal reason explicit knowledge form natural language facts rule clark et al two thousand and twenty recent work name prover saha et al two thousand and twenty perform reason answer question also generate proof graph explain answer however compositional reason always unique may multiple ways reach correct answer thus work address new challenge problem generate multiple proof graph reason natural language rule base proof provide different rationale answer thereby improve interpretability reason systems order jointly learn proof graph exploit correlations multiple proof question pose task set generation problem structure output space proof represent direct graph propose two variants proof set generation model multiprover first model multilabel multiprover generate set proof via multi label classification implicit condition proof second model iterative multiprover generate proof iteratively explicitly condition previously generate proof experiment multiple synthetic zero shoot human paraphrase datasets reveal multiprover model significantly outperform prover datasets contain multiple gold proof iterative multiprover obtain state art proof f1 zero shoot scenarios examples single correct proof also generalize better question require higher depths reason multiple proof frequent code model publicly available https githubcom swarnahub multiprover
propose new paradigm maintain speaker identity dysarthric voice conversion dvc poor quality dysarthric speech greatly improve statistical vc normal speech utterances dysarthria patient nearly impossible collect previous work fail recover individuality patient light suggest novel two stage approach dvc highly flexible normal speech patient require first powerful parallel sequence sequence model convert input dysarthric speech normal speech reference speaker intermediate product nonparallel frame wise vc model realize variational autoencoder convert speaker identity reference speech back patient assume capable preserve enhance quality investigate several design options experimental evaluation result demonstrate potential approach improve quality dysarthric speech maintain speaker identity
possible use natural language intervene model behavior alter prediction desire way investigate effectiveness natural language interventions read comprehension systems study context social stereotype specifically propose new language understand task linguistic ethical interventions lei goal amend question answer qa model unethical behavior communicate context specific principles ethics equity end build upon recent methods quantify system social stereotype augment different kinds ethical interventions desire model behavior interventions zero shoot evaluation find even today powerful neural language model extremely poor ethical advice takers respond surprisingly little ethical interventions even though interventions state simple sentence shoot learn improve model behavior remain far desire outcome especially evaluate various type generalization new task thus pose novel language understand challenge community
mental health challenge think afflict around ten global population year many go untreated due stigma limit access service explore trend word phrase relate mental health collection one two three grams parse data stream roughly ten english tweet since two thousand and twelve examine temporal dynamics mental health language find popularity phrase mental health increase nearly two order magnitude two thousand and twelve two thousand and eighteen observe mention mental health spike annually reliably due mental health awareness campaign well unpredictably response mass shoot celebrities die suicide popular fictional stories portray suicide find level positivity message contain mental health stable growth period decline recently finally use ratio original tweet retweets quantify fraction appearances mental health language due social amplification since two thousand and fifteen mention mental health become increasingly due retweets suggest stigma associate discussion mental health twitter diminish time
paper study performance generalizability three approach ad detection speech recent adresso challenge dataset one use conventional acoustic feature two use novel pre train acoustic embeddings three combine acoustic feature embeddings find feature base approach higher precision classification approach rely combination embeddings feature prove higher balance performance across multiple metrics performance best model use combine approach outperform acoustic baseline challenge twenty-eight
despite recent advancement nlp research cross lingual transfer natural language generation relatively understudy work transfer supervision high resource language hrl multiple low resource languages lrls natural language generation nlg consider four nlg task text summarization question generation news headline generation distractor generation three syntactically diverse languages ie english hindi japanese propose unsupervised cross lingual language generation framework call zmbart use parallel pseudo parallel back translate data framework pre train mbart sequence sequence denoising auto encoder model auxiliary task use monolingual data three languages objective function auxiliary task close target task enrich multi lingual latent representation mbart provide good initialization target task model fine tune task specific supervise english data directly evaluate low resource languages zero shoot set overcome catastrophic forget spurious correlation issue apply freeze model component data argumentation approach respectively simple model approach give us promise resultswe experiment shoot train one thousand supervise data point boost model performance perform several ablations cross lingual transferability analyse demonstrate robustness zmbart
human activities see sequence events crucial understand societies disproportional event distribution different demographic group manifest amplify social stereotype potentially jeopardize ability members group pursue certain goals paper present first event centric study gender bias wikipedia corpus facilitate study curate corpus career personal life descriptions demographic information consist seven thousand, eight hundred and fifty-four fragment ten thousand, four hundred and twelve celebrities detect events state art event detection model calibrate result use strategically generate templates extract events asymmetric associations genders study discover wikipedia page tend intermingle personal life events professional events females males call awareness wikipedia community formalize guidelines train editors mind implicit bias contributors carry work also lay foundation future work quantify discover event bias corpus level
train reinforcement learn agent carry natural language instructions limit available supervision ie know instruction carry adapt clevr visual question answer dataset generate complex natural language navigation instructions accompany scene graph yield environment agnostic supervise dataset demonstrate use data set map scenes vizdoom environment use architecture citetgatedattention train agent carry complex language instructions
study use innovative measure semantic brand score assess interest stakeholders different company core value among others focus corporate social responsibility csr core value statements attention receive five categories stakeholders customers company communication team employees associations media combine big data methods tool social network analysis text mine analyze fifty-eight thousand italian tweet find different stakeholders different prevail interest csr get much less attention expect core value relate customers employees foreground
concern ability language model lms generate high quality synthetic text misuse launch spam disinformation propaganda therefore research community actively work develop approach detect whether give text organic synthetic useful first step important able fingerprint author lm attribute origin prior work fingerprint lms limit attribute synthetic text generate handful usually ten pre train lms however lms gpt2 commonly fine tune myriad ways eg domain specific text corpus use generate synthetic text challenge fingerprint fine tune lms universe fine tune lms much larger realistic scenarios address challenge study problem large scale fingerprint fine tune lms wild use real world dataset synthetic text generate one hundred and eight different fine tune lms conduct comprehensive experiment demonstrate limitations exist fingerprint approach result show fine tune effective attribute synthetic text generate fine tune lms
latent knowledge emotions opinions individuals manifest via social network crucial numerous applications include social management dynamical process public security affective compute interdisciplinary research field link artificial intelligence cognitive inference capable exploit emotion orient knowledge brief content textual content convey hide information personality cognition correspond author determine correlations variations users emotion recognition brief content embrace contrast author differences personality cognition trace within emotional expressions tackle challenge devise framework one hand infer latent individual aspects brief content hand present novel ensemble classifier equip dynamic dropout convnets extract emotions textual context categorize short text content propose method conjointly leverage cognitive factor exploit hide information utilize outcome vectors novel embed model foster emotion pertinent feature collectively assemble lexicon inductions experimental result show compare competitors propose model achieve higher performance recognize emotion noisy content
document level relation extraction attract much attention recent years usually formulate classification problem predict relations entity pair document however previous work indiscriminately represent intra inter sentential relations way confound different pattern predict besides create document graph use paths entities graph clue logical reason however entity pair connect path correct logical reason paths graph thus many case logical reason cover paper propose effective architecture sire represent intra inter sentential relations different ways design new straightforward form logical reason module cover logical reason chain experiment public datasets show sire outperform previous state art methods analysis show predictions reliable explainable code available https githubcom dreaminvoker sire
vision language pre train vlp large scale image text pair achieve huge success cross modal downstream task exist pre train methods mainly adopt two step train procedure firstly employ pre train object detector extract region base visual feature concatenate image representation text embed input transformer train however methods face problems use task specific visual representation specific object detector generic cross modal understand computation inefficiency two stage pipeline paper propose first end end vision language pre train model vl understand generation namely e2e vlp build unify transformer framework jointly learn visual representation semantic alignments image text incorporate task object detection image caption pre train unify transformer encoder decoder architecture enhance visual learn extensive set experiment conduct well establish vision language downstream task demonstrate effectiveness novel vlp paradigm
paper present improve model voice silent speech audio synthesize facial electromyography emg signal give model greater flexibility learn input feature directly use emg signal input place hand design feature use prior work model use convolutional layer extract feature signal transformer layer propagate information across longer distance provide better signal learn also introduce auxiliary task predict phoneme label addition predict speech audio feature open vocabulary intelligibility evaluation model improve state art task absolute two hundred and fifty-eight
mechanisms encode positional information central transformer base language model paper analyze position embeddings exist language model find strong evidence translation invariance embeddings effect self attention degree translation invariance increase train correlate positively model performance find lead us propose translation invariant self attention tisa account relative position tokens interpretable fashion without need conventional position embeddings proposal several theoretical advantage exist position representation approach experiment show improve regular albert glue task add order magnitude less positional parameters
recent advance supervise semi supervise self supervise deep learn algorithms show significant improvement performance automatic speech recognitionasr systems state art systems achieve word error rate wer less five however past researchers argue non suitability wer metric evaluation asr systems downstream task speak language understand slu information retrieval reason wer work surface level include syntactic semantic knowledgethe current work propose semantic wer swer metric evaluate asr transcripts downstream applications general swer easily customize stream task
detect classify instance hate social media text problem interest natural language process recent years work leverage state art transformer language model identify hate speech multilingual set capture intent post comment social media involve careful evaluation language style semantic content additional pointers hashtags emojis paper look problem identify whether twitter post hateful offensive discriminate detect toxic content one follow three class hate speech hate b offensive offn c profane prfn pre train multilingual transformer base text encoder base able successfully identify classify hate speech multiple languages provide test corpora achieve macro f1 score nine thousand and twenty-nine eight thousand, one hundred and eighty-seven seven thousand, five hundred and forty english german hindi respectively perform hate speech detection six thousand and seventy five thousand, three hundred and twenty-eight four thousand, nine hundred and seventy-four fine grain classification experiment show efficacy perspective api feature hate speech classification effect exploit multilingual train scheme feature selection study provide illustrate impact specific feature upon architecture classification head
mushroom body fruit fly brain one best study systems neuroscience core consist population kenyon cells receive input multiple sensory modalities cells inhibit anterior pair lateral neuron thus create sparse high dimensional representation input work study mathematical formalization network motif apply learn correlational structure word context corpus unstructured text common natural language process nlp task show network learn semantic representations word generate static context dependent word embeddings unlike conventional methods eg bert glove use dense representations word embed algorithm encode semantic mean word context form sparse binary hash cod quality learn representations evaluate word similarity analysis word sense disambiguation document classification show fruit fly network motif achieve performance comparable exist methods nlp additionally use fraction computational resources shorter train time smaller memory footprint
efficacy external language model lm integration exist end end e2e automatic speech recognition asr systems improve significantly use internal language model estimation ilme method method internal lm score subtract score obtain interpolate e2e score external lm score inference improve ilme base inference propose internal lm train ilmt method minimize additional internal lm loss update e2e model components affect internal lm estimation ilmt encourage e2e model form standalone lm inside exist components without sacrifice asr accuracy ilmt modular e2e model match train inference criteria enable thorough elimination source domain internal lm therefore lead effective integration target domain external lm experiment 30k hour train recurrent neural network transducer attention base encoder decoder model ilmt ilme base inference achieve three hundred and fifteen one hundred and fourteen relative word error rate reductions standard e2e train shallow fusion domain librispeech domain microsoft production test set respectively
real world knowledge graph often characterize low frequency relations challenge prompt increase interest shoot link prediction methods methods perform link prediction set new relations unseen train give example facts relation test time work perform systematic study spectrum model derive generalize current state art shoot link prediction goal probe limit learn shoot set find simple zero shoot baseline ignore relation specific information achieve surprisingly strong performance moreover experiment carefully craft synthetic datasets show examples relation fundamentally limit model use fine grain structural information allow exploit coarse grain positional information entities together find challenge implicit assumptions inductive bias prior work highlight new directions research area
order achieve high accuracy machine learn ml applications essential employ model large number parameters certain applications automatic speech recognition asr however require real time interactions users hence compel model low latency possible deploy large scale ml applications thus necessitate model quantization compression especially run ml model resource constrain devices example force model weight value zero possible apply zero weight compression reduce model size model read time memory literature methods refer sparse prune fundamental question weight force zero ie prune work propose compress sense base prune csp approach effectively address question reformulate sparse prune sparsity induce compression error reduction dual problem introduce classic compress sense process ml model train process use asr task example show csp consistently outperform exist approach literature
grow desire create computer systems communicate effectively collaborate humans complex open end activities assess systems present significant challenge describe framework evaluate systems engage open end complex scenarios evaluators luxury compare performance single right answer framework use evaluate human machine creative collaborations across story music generation interactive block build exploration molecular mechanisms cancer activities fundamentally different constrain task perform contemporary personal assistants generally open end single correct solution often obvious completion criteria identify key properties must exhibit successful systems identify hallmarks success capabilities feature evaluators observe would indicative progress toward achieve key property addition framework assessment key properties hallmarks intend serve goals guide research direction
transformer architecture revolutionize field natural language process nlp transformers base model eg bert power many important web service search translation question answer etc enormous research attention pay train model relatively little efforts make improve inference performance paper come address gap present empirical analysis scalability performance inferencing transformer base model cpus focus highly popular bert model identify key components transformer architecture bulk computation happen propose three optimizations speed optimizations evaluate use inference benchmark huggingface show achieve speedup x237 consider optimizations require change implementation model affect accuracy
paper study controllability expressive tts system train dataset continuous control dataset blizzard two thousand and thirteen dataset base audiobooks read female speaker contain great variability style expressiveness controllability evaluate objective subjective experiment objective assessment base measure correlation acoustic feature dimension latent space represent expressiveness subjective assessment base perceptual experiment users show interface controllable expressive tts ask retrieve synthetic utterance whose expressiveness subjectively correspond reference utterance
deep learn powerful tool natural language process nlp problems successful solutions problems rely heavily large amount annotate sample however manually annotate data expensive time consume active learn al strategies reduce need huge volumes label data iteratively select small number examples manual annotation base estimate utility train give model paper argue since al strategies choose examples independently may potentially select similar examples may contribute significantly learn process propose approach activemathbf2 learn amathbf2l actively adapt deep learn model train eliminate redundant examples choose al strategy show amathbf2l widely applicable use conjunction several different al strategies nlp task empirically demonstrate propose approach able reduce data requirements state art al strategies absolute percentage reduction approxmathbf3 twenty-five multiple nlp task achieve performance additional computation overhead
report analysis current events around globe expand professional editor lead journalism way citizen journalism nowadays politicians key players enjoy direct access audiences social media bypass filter official cable traditional media however multiple advantage free speech direct communication dim misuse media spread inaccurate mislead claim phenomena lead modern incarnation fact checker professional whose main aim examine claim use available evidence assess veracity text forensics task amount information available make work fact checker difficult mind start perspective professional fact checker survey available intelligent technologies support human expert different step fact check endeavor include identify claim worth fact check detect relevant previously fact check claim retrieve relevant evidence fact check claim actually verify claim case pay attention challenge future work potential impact real world fact check
multilingual model demonstrate impressive cross lingual transfer performance however test set like xnli monolingual example level multilingual communities common polyglots code mix converse inspire phenomenon present two strong black box adversarial attack one word level one phrase level multilingual model push ability handle code mix sentence limit former use bilingual dictionaries propose perturbations translations clean example sense disambiguation latter directly align clean example translations extract phrase perturbations phrase level attack success rate eight thousand, nine hundred and seventy-five xlm r large bring average accuracy seven thousand, nine hundred and eighty-five eight hundred and eighteen xnli finally propose efficient adversarial train scheme train number step original model show improve model accuracy
study problem dynamic visual reason raw videos challenge problem currently state art model often require dense supervision physical object properties events simulation impractical obtain real life paper present dynamic concept learner dcl unify framework ground physical object events video language dcl first adopt trajectory extractor track object time represent latent object centric feature vector build upon object centric representation dcl learn approximate dynamic interaction among object use graph network dcl incorporate semantic parser parse question semantic program finally program executor run program answer question lever learn dynamics model train dcl detect associate object across frame grind visual properties physical events understand causal relationship events make future counterfactual predictions leverage extract presentations answer query dcl achieve state art performance clevrer challenge causal video reason dataset even without use grind truth attribute collision label simulations train test dcl newly propose video retrieval event localization dataset derive clevrer show strong generalization capacity
currently grow number mature natural language process applications make people life convenient applications build source code language software engineer however applications understand source code language ease software engineer process research simultaneously transformer model especially combination transfer learn prove powerful technique natural language process task breakthroughs point promise direction process source code crack software engineer task paper describe codetrans encoder decoder transformer model task software engineer domain explore effectiveness encoder decoder transformer model six software engineer task include thirteen sub task moreover investigate effect different train strategies include single task learn transfer learn multi task learn multi task learn fine tune codetrans outperform state art model task expedite future work software engineer domain publish pre train model codetrans https githubcom agemagician codetrans
social network widely use information consumption dissemination especially time critical events natural disasters despite significantly large volume social media content often noisy direct use application therefore important filter categorize concisely summarize available content facilitate effective consumption decision make address issue automatic classification systems develop use supervise model approach thank earlier efforts create label datasets however exist datasets limit different aspects eg size contain duplicate less suitable support advance data hungry deep learn model paper present new large scale dataset 77k human label tweet sample pool twenty-four million tweet across nineteen disaster events happen two thousand and sixteen two thousand and nineteen moreover propose data collection sample pipeline important social media data sample human annotation report multiclass classification result use classic deep learn fasttext transformer base model set grind future study dataset associate resources publicly available https crisisnlpqcriorg humaiddatasethtml
multiview representation learn data help construct coherent contextualized users representations social media paper suggest joint embed model incorporate users social textual information learn contextualized user representations use understand lifestyle choices apply model tweet relate two lifestyle activities yoga keto diet use analyze users activity type motivation explain data collection annotation process detail provide depth analysis users different class base twitter content experiment show model result performance improvements domains
procedural events often think high level goal compose sequence step infer sub sequence step goal help artificial intelligence systems reason human activities past work nlp examine task goal step inference text introduce visual analogue propose visual goal step inference vgsi task model give textual goal must choose plausible step towards goal among four candidate image task challenge state art muitimodal model introduce novel dataset harvest wikihow consist seven hundred and seventy-two thousand, two hundred and ninety-four image represent human action show knowledge learn data effectively transfer datasets like howto100m increase multiple choice accuracy fifteen twenty task facilitate multi modal reason procedural events
paper develop natural language agent base model argumentation abma artificial deliberative agents adas construct help call neural language model recently develop ai computational linguistics adas equip minimalist belief system may generate submit novel contributions conversation natural language abma allow us simulate collective deliberation english ie arguments reason claim rather mathematical representations formal model paper use natural language abma test robustness formal reason balance model argumentation maes flache two thousand and thirteen singer et al two thousand and nineteen first long adas remain passive confirmation bias homophily update trigger polarization consistent result formal model however adas start actively generate new contributions evolution conservation dominate properties agents author suggest creation new arguments reason claim critically affect conversation pivotal importance understand dynamics collective deliberation paper close point fruitful applications model challenge future research
end end e2e slu network leverage pre train asr network still lack capability understand semantics utterances crucial slu task solve recently propose study use pre train nlu network however trivial fully utilize pre train network many solutions propose knowledge distillation cross modal share embed network integration interface propose simple robust integration method e2e slu network novel interface continuous token interface cti junctional representation asr nlu network network pre train vocabulary difference noise level directly fee asr network output nlu network thus train slu network e2e manner without additional modules gumbel softmax evaluate model use slurp challenge slu dataset achieve state art score intent classification slot fill task also verify nlu network pre train mask language model utilize noisy textual representation cti moreover show model train multi task learn heterogeneous data even integration cti
recent years see proliferation disinformation misinformation online thank freedom expression internet rise social media two solutions propose address problem manual fact check accurate credible slow non scalable ii automatic fact check fast scalable lack explainability credibility accumulation enough manually fact check claim middle grind approach emerge check whether give claim previously fact check make automatically thus fast also offer credibility explainability thank human fact check explanations associate fact check article relatively new understudy research direction focus claim make political debate context really matter thus study impact model context claim source side ie debate well target side ie fact check explanation document model local context global context well mean co reference resolution reason target text use transformer xh experimental result show represent valuable information source model source side context important yield ten point absolute improvement
assess ai agent converse human language understand visual content challenge generation metrics bleu score favor correct syntax semantics hence discriminative approach often use agent rank set candidate options mean reciprocal rank mrr metric evaluate model performance take account rank single human derive answer approach however raise new challenge ambiguity synonymy answer instance semantic equivalence eg yeah yes address normalize discount cumulative gain ndcg metric use capture relevance correct answer via dense annotations however ndcg metric favor usually applicable uncertain answer know craft model excel mrr ndcg metrics challenge ideally ai agent answer human like reply validate correctness answer address issue describe two step non parametric rank approach merge strong mrr ndcg model use approach manage keep mrr state art performance seven thousand and forty-one vs seven thousand, one hundred and twenty-four ndcg state art performance seven thousand, two hundred and sixteen vs seven thousand, five hundred and thirty-five moreover approach recent visual dialog two thousand and twenty challenge source code available https githubcom idansc mrr ndcg
online chat body language vocal characteristics part communication mechanism make challenge facilitate accurate interpretation feel emotions attitudes use emojis express emotional feel alternative approach type communication project focus model customer emotion online message session chatbot use affect control theory act predict emotional change interaction let customer use emojis also extend affective dictionaries use act purpose map emoji2vec embed affective space framework find emotional change message customer reaction change accordingly
paper present new objective prediction model synthetic speech naturalness use evaluate text speech voice conversion systems work language independently model train end end base cnn lstm network previously show give good result speech quality estimation train test model sixteen different datasets blizzard challenge voice conversion challenge show reliability deep learn base naturalness prediction improve transfer learn speech quality prediction model train objective polqa score propose model make publicly available example use evaluate different tts system configurations
medical image caption automatically generate medical description describe content give medical image traditional medical image caption model create medical description base single medical image input hence abstract medical description concept hard generate base traditional approach method limit effectiveness medical image caption multi modal medical image caption one approach utilize address problem multi modal medical image caption textual input eg expert define keywords consider one main drivers medical description generation thus encode textual input medical image effectively important task multi modal medical image caption work new end end deep multi modal medical image caption model propose contextualized keyword representations textual feature reinforcement mask self attention use develop propose approach base evaluation exist multi modal medical image caption dataset experimental result show propose model effective increase five hundred and thirty-two bleu avg one hundred and eighty-six cider compare state art method
survey provide overview evolution visually ground model speak language last twenty years model inspire observation children pick language rely wide range indirect noisy clue crucially include signal visual modality co occur speak utterances several field make important contributions approach model mimic process learn language machine learn natural language speech process computer vision cognitive science current paper bring together contributions order provide useful introduction overview practitioners areas discuss central research question address timeline developments datasets enable much work summarize main model architectures offer exhaustive overview evaluation metrics analysis techniques
factorize layer operations parameterized products two matrices occur variety deep learn contexts include compress model train certain type knowledge distillation multi head self attention architectures study initialize regularize deep net contain layer examine two simple understudy scheme spectral initialization frobenius decay improve performance guide insight design optimization routines network close possible well tune non decompose counterparts back intuition analysis initialization regularization scheme impact train gradient descent draw modern attempt understand interplay weight decay batch normalization empirically highlight benefit spectral initialization frobenius decay across variety settings model compression show enable low rank methods significantly outperform unstructured sparsity tensor methods task train low memory residual network analogs scheme also improve performance tensor decomposition techniques knowledge distillation frobenius decay enable simple overcomplete baseline yield compact model parameterized train without require retrain prune teacher network finally show scheme apply multi head attention lead improve performance translation unsupervised pre train
question fairness robustness transparency paramount address deploy nlp systems central concern question reliability nlp systems reliably treat different demographics fairly function correctly diverse noisy environments address argue need reliability test contextualize among exist work improve accountability show adversarial attack reframed goal via framework develop reliability test argue reliability test emphasis interdisciplinary collaboration enable rigorous target test aid enactment enforcement industry standards
spider nickname special frobenius algebras fundamental structure mathematics physics computer science pregroups fundamental structure linguistics pregroups spiders use together natural language process one syntax semantics turn pregroups characterize point spiders category preordered relations naturally arise grammars way around preordered spider algebras general characterize unions pregroups extend characterization relational spider algebras disjoint unions group compositional framework emerge result suggest new ways understand apply basis structure machine learn data analysis
paper present technical report submission 4th task semeval two thousand and twenty-one title read comprehension abstract mean task want predict correct answer base question give context usually contexts lengthy require large receptive field model thus common contextualized language model like bert miss fine representation performance due limit capacity input tokens tackle problem use longformer model better process sequence furthermore utilize method propose longformer benchmark wikihop dataset improve accuracy task data two thousand, three hundred and one two thousand, two hundred and ninety-five achieve baselines subtask one two respectively seven thousand and thirty six thousand, four hundred and thirty-eight
people observe events able abstract key information build concise summaries happen summaries include contextual semantic information describe important high level detail observe event exclude background information deem unimportant observer mind descriptions people generate videos different dynamic events greatly improve understand key information interest video descriptions capture caption provide expand attribute video label eg action object scenes sentiment etc allow us gain new insight people find important necessary summarize specific events exist caption datasets video understand either small scale restrict specific domain address present speak moments mit dataset 500k speak caption attribute unique short video depict broad range different events collect descriptions use audio record ensure remain natural concise possible allow us scale size large classification dataset order utilize propose dataset present novel adaptive mean margin amm approach contrastive learn evaluate model video caption retrieval multiple datasets show amm approach consistently improve result model train speak moments dataset generalize better train video caption datasets
paper explore association brand importance growth museum visitors analyze ten years online forum discussions apply semantic brand score sbs assess brand importance five european museums naive bay regression model indicate variations combine dimension sbs prevalence diversity connectivity align change museum visitors result suggest order attract visitors museum brand managers focus increase volume online post richness information generate users around brand rather control post overall positivity negativity
recent advance transformer model allow unprecedented sequence lengths due linear space time complexity meantime relative positional encode rpe propose beneficial classical transformers consist exploit lag instead absolute position inference still rpe available recent linear variants transformer require explicit computation attention matrix precisely avoid methods paper bridge gap present stochastic positional encode way generate pe use replacement classical additive sinusoidal pe provably behave like rpe main theoretical contribution make connection positional encode cross covariance structure correlate gaussian process illustrate performance approach long range arena benchmark music generation
interactive robots navigate photo realistic environments face challenge underlie vision language navigation vln addition need train handle dynamic nature dialogue however research cooperative vision dialog navigation cvdn navigator interact guide natural language order reach goal treat dialogue history vln style static instruction paper present visitron navigator better suit interactive regime inherent cvdn train identify associate object level concepts semantics environment dialogue history ii identify interact vs navigate via imitation learn binary classification head perform extensive ablations visitron gain empirical insights improve performance cvdn visitron competitive model static cvdn leaderboard also propose generalize interactive regime fine tune evaluate visitron future model pre train guide adaptability
ultrasound tongue image use visualise intra oral articulators speech production utilise range applications include speech language therapy phonetics research ultrasound speech audio record simultaneously order correctly use data two modalities correctly synchronise synchronisation achieve use specialise hardware record time approach fail practice result data limit usability paper address problem automatically synchronise ultrasound audio data collection first investigate tolerance expert ultrasound users synchronisation errors order find thresholds error detection use thresholds define accuracy score boundaries evaluate system describe approach automatic synchronisation drive self supervise neural network exploit correlation two signal synchronise train model data multiple domains different speaker characteristics different equipment different record environments achieve accuracy nine hundred and twenty-four hold domain data finally introduce novel resource cleave dataset gather new clinical subgroup hardware synchronisation prove unreliable apply model domain data evaluate performance subjectively expert users result show users prefer model output original hardware output seven hundred and ninety-three time result demonstrate strength approach ability generalise data new domains
describe real time system receive live audio stream jam session generate lyric line congruent live music play two novel approach propose align learn latent space audio text representations allow system generate novel lyric line match live instrumental music one approach base adversarial alignment latent representations audio lyric approach learn transfer topology music latent space lyric latent space user study music artists use system show system useful lyric composition also encourage artists improvise find new musical expressions another user study demonstrate users prefer line generate use propose methods line generate baseline model
estimate position multiple speakers helpful task like automatic speech recognition speaker diarization applications benefit know speaker position instance apply beamforming assign unique speaker identities recently several approach utilize acoustic signal augment visual data propose task however acoustic visual modality may corrupt specific spatial regions instance due poor light condition presence background noise paper propose novel audiovisual data fusion framework speaker localization assign individual dynamic stream weight specific regions localization space fusion achieve via neural network combine predictions individual audio video trackers base time location dependent reliability performance evaluation use audiovisual record yield promise result propose fusion approach outperform baseline model
paper present method concatenate patent claim description apply method bert train suitable descriptions claim train bert claim description bert could able identify novelty relevant descriptions patent addition introduce new score scheme relevance score novelty score process output bert meaningful way test method patent applications train bert first claim patent correspond descriptions bert output process accord relevance score result compare cite x document search report test show bert score cite x document highly relevant
recent years witness proliferation fake news propaganda misinformation disinformation online initially mostly textual content time image videos gain popularity much easier consume attract much attention spread simple text result researchers start target different modalities combinations thereof different modalities study different research communities insufficient interaction offer survey explore state art multimodal disinformation detection cover various combinations modalities text image audio video network structure temporal information moreover study focus factuality others investigate harmful content two components definition disinformation factuality ii harmfulness equally important typically study isolation thus argue need tackle disinformation detection take account multiple modalities well factuality harmfulness framework finally discuss current challenge future research directions
past decade witness groundbreaking rise machine learn human language analysis current methods capable automatically accurately recover various aspects syntax semantics include sentence structure ground word mean large data collections recent research show promise tool analyze acoustic communication nonhuman species posit machine learn cornerstone future collection process analysis multimodal stream data animal communication study include bioacoustic behavioral biological environmental data cetaceans unique non human model species possess sophisticate acoustic communications utilize different encode system evolve aquatic rather terrestrial medium sperm whale particular highly develop neuroanatomical feature cognitive abilities social structure discrete click base encode make excellent start point advance machine learn tool apply animals future paper detail roadmap toward goal base currently exist technology multidisciplinary scientific community effort outline key elements require collection process massive bioacoustic data sperm whale detect basic communication units language like higher level structure validate model interactive playback experiment technological capabilities develop undertake likely yield cross applications advancements broader communities investigate non human communication animal behavioral research
peaky behavior ctc model well know experimentally however understand peaky behavior occur miss whether good property provide formal analysis peaky behavior gradient descent convergence properties ctc loss relate train criteria analysis provide deep understand peaky behavior occur suboptimal simple example trivial learn model prove fee forward neural network train ctc uniform initialization converge towards peaky behavior one hundred error rate analysis explain ctc work well together blank label demonstrate peaky behavior occur relate losses include label prior model improve convergence